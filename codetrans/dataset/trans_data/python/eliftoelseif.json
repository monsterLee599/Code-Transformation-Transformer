{"7": "\ndef detect_os ( ) : \n    syst = system ( ) . lower ( ) \n    os = 'unknown' \n    if 'cygwin' in syst : \n        os = 'cygwin' \n    else : \n        if 'darwin' in syst : \n            os = 'mac' \n        else : \n            if 'linux' in syst : \n                os = 'linux' \n                try : \n                    with open ( '/proc/version' , 'r' ) as f : \n                        if 'microsoft' in f . read ( ) . lower ( ) : \n                            os = 'wsl' \n                except : \n                    pass \n            else : \n                if 'windows' in syst : \n                    os = 'windows' \n                else : \n                    if 'bsd' in syst : \n                        os = 'bsd' \n    return os "}
{"11": "\ndef get_mgtv_real_url ( url ) : \n    content = loads ( get_content ( url ) ) \n    m3u_url = content [ 'info' ] \n    split = urlsplit ( m3u_url ) \n    base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) \n    content = get_content ( content [ 'info' ] ) \n    segment_list = [ ] \n    segments_size = 0 \n    for i in content . split ( ) : \n        if not i . startswith ( '#' ) : \n            segment_list . append ( base_url + i ) \n        else : \n            if i . startswith ( '#EXT-MGTV-File-SIZE:' ) : \n                segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) \n    return m3u_url , segments_size , segment_list "}
{"14": "\ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    else : \n        if 'info_only' in kwargs and kwargs [ 'info_only' ] : \n            if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n                stream_id = kwargs [ 'stream_id' ] \n                if 'index' not in kwargs : \n                    self . p ( stream_id ) \n                else : \n                    self . p_i ( stream_id ) \n            else : \n                if 'index' not in kwargs : \n                    self . p ( [ ] ) \n                else : \n                    stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n                    self . p_i ( stream_id ) \n        else : \n            if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n                stream_id = kwargs [ 'stream_id' ] \n            else : \n                stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n            if stream_id in self . streams : \n                urls = self . streams [ stream_id ] [ 'src' ] \n                ext = self . streams [ stream_id ] [ 'container' ] \n                total_size = self . streams [ stream_id ] [ 'size' ] \n            else : \n                urls = self . dash_streams [ stream_id ] [ 'src' ] \n                ext = self . dash_streams [ stream_id ] [ 'container' ] \n                total_size = self . dash_streams [ stream_id ] [ 'size' ] \n            if not urls : \n                log . wtf ( '[Failed] Cannot extract video source.' ) \n            download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) \n            if not kwargs [ 'caption' ] : \n                print ( 'Skipping captions.' ) \n                return \n            for lang in self . caption_tracks : \n                filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n                print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) \n                srt = self . caption_tracks [ lang ] \n                with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                    x . write ( srt ) \n                print ( 'Done.' ) "}
{"15": "\ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if sourceType == 'sina' : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    else : \n        if sourceType == 'youku' : \n            youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n        else : \n            if sourceType == 'tudou' : \n                tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n            else : \n                if sourceType == 'qq' : \n                    qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) \n                else : \n                    if sourceType == 'letv' : \n                        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n                    else : \n                        if sourceType == 'zhuzhan' : \n                            url = 'http://www.acfun.cn/v/ac' + vid \n                            yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n                            seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n                            for t in seq : \n                                if yk_streams . get ( t ) : \n                                    preferred = yk_streams [ t ] \n                                    break \n                            size = 0 \n                            for url in preferred [ 0 ] : \n                                _ , _ , seg_size = url_info ( url ) \n                                size += seg_size \n                            if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : \n                                ext = 'flv' \n                            else : \n                                ext = 'mp4' \n                            print_info ( site_info , title , ext , size ) \n                            if not info_only : \n                                download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) \n                        else : \n                            raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping danmaku.' ) \n            return \n        try : \n            title = get_filename ( title ) \n            print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"18": "\ndef get_content ( url , headers = { } , decoded = True ) : \n    logging . debug ( 'get_content: %s' % url ) \n    req = request . Request ( url , headers = headers ) \n    if cookies : \n        cookies . add_cookie_header ( req ) \n        req . headers . update ( req . unredirected_hdrs ) \n    response = urlopen_with_retry ( req ) \n    data = response . read ( ) \n    content_encoding = response . getheader ( 'Content-Encoding' ) \n    if content_encoding == 'gzip' : \n        data = ungzip ( data ) \n    else : \n        if content_encoding == 'deflate' : \n            data = undeflate ( data ) \n    if decoded : \n        charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) \n        if charset is not None : \n            data = data . decode ( charset , 'ignore' ) \n        else : \n            data = data . decode ( 'utf-8' , 'ignore' ) \n    return data "}
{"19": "\ndef post_content ( url , headers = { } , post_data = { } , decoded = True , ** kwargs ) : \n    if kwargs . get ( 'post_data_raw' ) : \n        logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) \n    else : \n        logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) \n    req = request . Request ( url , headers = headers ) \n    if cookies : \n        cookies . add_cookie_header ( req ) \n        req . headers . update ( req . unredirected_hdrs ) \n    if kwargs . get ( 'post_data_raw' ) : \n        post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) \n    else : \n        post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) \n    response = urlopen_with_retry ( req , data = post_data_enc ) \n    data = response . read ( ) \n    content_encoding = response . getheader ( 'Content-Encoding' ) \n    if content_encoding == 'gzip' : \n        data = ungzip ( data ) \n    else : \n        if content_encoding == 'deflate' : \n            data = undeflate ( data ) \n    if decoded : \n        charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) \n        if charset is not None : \n            data = data . decode ( charset ) \n        else : \n            data = data . decode ( 'utf-8' ) \n    return data "}
{"34": "\ndef get_conn ( self ) : \n    conn = self . get_connection ( self . mysql_conn_id ) \n    conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } \n    if not conn . port : \n        conn_config [ \"port\" ] = 3306 \n    else : \n        conn_config [ \"port\" ] = int ( conn . port ) \n    if conn . extra_dejson . get ( 'charset' , False ) : \n        conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] \n        if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : \n            conn_config [ \"use_unicode\" ] = True \n    if conn . extra_dejson . get ( 'cursor' , False ) : \n        if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor \n        else : \n            if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : \n                conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor \n            else : \n                if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : \n                    conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor \n    local_infile = conn . extra_dejson . get ( 'local_infile' , False ) \n    if conn . extra_dejson . get ( 'ssl' , False ) : \n        dejson_ssl = conn . extra_dejson [ 'ssl' ] \n        if isinstance ( dejson_ssl , six . string_types ) : \n            dejson_ssl = json . loads ( dejson_ssl ) \n        conn_config [ 'ssl' ] = dejson_ssl \n    if conn . extra_dejson . get ( 'unix_socket' ) : \n        conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] \n    if local_infile : \n        conn_config [ \"local_infile\" ] = 1 \n    conn = MySQLdb . connect ( ** conn_config ) \n    return conn "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += 1 \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_ready_workers_running < num_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            else : \n                if num_workers_running > num_workers_expected : \n                    excess = num_workers_running - num_workers_expected \n                    log . debug ( '%s killing %s workers' , state , excess ) \n                    for _ in range ( excess ) : \n                        gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                        excess -= 1 \n                        wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n                else : \n                    if num_workers_running == num_workers_expected : \n                        refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                        log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                        time . sleep ( refresh_interval ) \n                        start_refresh ( gunicorn_master_proc ) \n                    else : \n                        log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                        time . sleep ( 10 ) \n                        if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : \n                            start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"69": "\ndef make_aware ( value , timezone = None ) : \n    if timezone is None : \n        timezone = TIMEZONE \n    if is_localized ( value ) : \n        raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) \n    if hasattr ( value , 'fold' ) : \n        value = value . replace ( fold = 1 ) \n    if hasattr ( timezone , 'localize' ) : \n        return timezone . localize ( value ) \n    else : \n        if hasattr ( timezone , 'convert' ) : \n            return timezone . convert ( value ) \n        else : \n            return value . replace ( tzinfo = timezone ) "}
{"74": "\ndef run ( self , endpoint , data = None , headers = None , extra_options = None ) : \n    extra_options = extra_options or { } \n    session = self . get_conn ( headers ) \n    if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : \n        url = self . base_url + '/' + endpoint \n    else : \n        url = ( self . base_url or '' ) + ( endpoint or '' ) \n    req = None \n    if self . method == 'GET' : \n        req = requests . Request ( self . method , url , params = data , headers = headers ) \n    else : \n        if self . method == 'HEAD' : \n            req = requests . Request ( self . method , url , headers = headers ) \n        else : \n            req = requests . Request ( self . method , url , data = data , headers = headers ) \n    prepped_request = session . prepare_request ( req ) \n    self . log . info ( \"Sending '%s' to url: %s\" , self . method , url ) \n    return self . run_and_check ( session , prepped_request , extra_options ) "}
{"104": "\ndef _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : \n    config = configparser . ConfigParser ( ) \n    if config . read ( config_file_name ) : \n        sections = config . sections ( ) \n    else : \n        raise AirflowException ( \"Couldn't read {0}\" . format ( config_file_name ) ) \n    if config_format is None : \n        config_format = 'boto' \n    conf_format = config_format . lower ( ) \n    if conf_format == 'boto' : \n        if profile is not None and 'profile ' + profile in sections : \n            cred_section = 'profile ' + profile \n        else : \n            cred_section = 'Credentials' \n    else : \n        if conf_format == 'aws' and profile is not None : \n            cred_section = profile \n        else : \n            cred_section = 'default' \n    if conf_format in ( 'boto' , 'aws' ) : \n        key_id_option = 'aws_access_key_id' \n        secret_key_option = 'aws_secret_access_key' \n    else : \n        key_id_option = 'access_key' \n        secret_key_option = 'secret_key' \n    if cred_section not in sections : \n        raise AirflowException ( \"This config file format is not recognized\" ) \n    else : \n        try : \n            access_key = config . get ( cred_section , key_id_option ) \n            secret_key = config . get ( cred_section , secret_key_option ) \n        except Exception : \n            logging . warning ( \"Option Error in parsing s3 config file\" ) \n            raise \n        return access_key , secret_key "}
{"109": "\ndef list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : \n    if include_examples is None : \n        include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) \n    file_paths = [ ] \n    if directory is None : \n        return [ ] \n    else : \n        if os . path . isfile ( directory ) : \n            return [ directory ] \n        else : \n            if os . path . isdir ( directory ) : \n                patterns_by_dir = { } \n                for root , dirs , files in os . walk ( directory , followlinks = True ) : \n                    patterns = patterns_by_dir . get ( root , [ ] ) \n                    ignore_file = os . path . join ( root , '.airflowignore' ) \n                    if os . path . isfile ( ignore_file ) : \n                        with open ( ignore_file , 'r' ) as f : \n                            patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] \n                    dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] \n                    for d in dirs : \n                        patterns_by_dir [ os . path . join ( root , d ) ] = patterns \n                    for f in files : \n                        try : \n                            file_path = os . path . join ( root , f ) \n                            if not os . path . isfile ( file_path ) : \n                                continue \n                            mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) \n                            if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : \n                                continue \n                            if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : \n                                continue \n                            might_contain_dag = True \n                            if safe_mode and not zipfile . is_zipfile ( file_path ) : \n                                with open ( file_path , 'rb' ) as fp : \n                                    content = fp . read ( ) \n                                    might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) \n                            if not might_contain_dag : \n                                continue \n                            file_paths . append ( file_path ) \n                        except Exception : \n                            log = LoggingMixin ( ) . log \n                            log . exception ( \"Error while examining %s\" , f ) \n    if include_examples : \n        import airflow . example_dags \n        example_dag_folder = airflow . example_dags . __path__ [ 0 ] \n        file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) \n    return file_paths "}
{"115": "\ndef start_in_async ( self ) : \n    while True : \n        loop_start_time = time . time ( ) \n        if self . _signal_conn . poll ( ) : \n            agent_signal = self . _signal_conn . recv ( ) \n            if agent_signal == DagParsingSignal . TERMINATE_MANAGER : \n                self . terminate ( ) \n                break \n            else : \n                if agent_signal == DagParsingSignal . END_MANAGER : \n                    self . end ( ) \n                    sys . exit ( os . EX_OK ) \n        self . _refresh_dag_dir ( ) \n        simple_dags = self . heartbeat ( ) \n        for simple_dag in simple_dags : \n            self . _result_queue . put ( simple_dag ) \n        self . _print_stat ( ) \n        all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n        max_runs_reached = self . max_runs_reached ( ) \n        dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) \n        self . _stat_queue . put ( dag_parsing_stat ) \n        if max_runs_reached : \n            self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n            break \n        loop_duration = time . time ( ) - loop_start_time \n        if loop_duration < 1 : \n            sleep_length = 1 - loop_duration \n            self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep_length ) \n            time . sleep ( sleep_length ) "}
{"116": "\ndef start_in_sync ( self ) : \n    while True : \n        agent_signal = self . _signal_conn . recv ( ) \n        if agent_signal == DagParsingSignal . TERMINATE_MANAGER : \n            self . terminate ( ) \n            break \n        else : \n            if agent_signal == DagParsingSignal . END_MANAGER : \n                self . end ( ) \n                sys . exit ( os . EX_OK ) \n            else : \n                if agent_signal == DagParsingSignal . AGENT_HEARTBEAT : \n                    self . _refresh_dag_dir ( ) \n                    simple_dags = self . heartbeat ( ) \n                    for simple_dag in simple_dags : \n                        self . _result_queue . put ( simple_dag ) \n                    self . _print_stat ( ) \n                    all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n                    max_runs_reached = self . max_runs_reached ( ) \n                    dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) \n                    self . _stat_queue . put ( dag_parsing_stat ) \n                    self . wait_until_finished ( ) \n                    self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) \n                    if max_runs_reached : \n                        self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n                        self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) \n                        break "}
{"139": "\ndef getsection ( self , section ) : \n    if ( section not in self . _sections and section not in self . airflow_defaults . _sections ) : \n        return None \n    _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) \n    if section in self . _sections : \n        _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) \n    section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) \n    for env_var in sorted ( os . environ . keys ( ) ) : \n        if env_var . startswith ( section_prefix ) : \n            key = env_var . replace ( section_prefix , '' ) . lower ( ) \n            _section [ key ] = self . _get_env_var_option ( section , key ) \n    for key , val in iteritems ( _section ) : \n        try : \n            val = int ( val ) \n        except ValueError : \n            try : \n                val = float ( val ) \n            except ValueError : \n                if val . lower ( ) in ( 't' , 'true' ) : \n                    val = True \n                else : \n                    if val . lower ( ) in ( 'f' , 'false' ) : \n                        val = False \n        _section [ key ] = val \n    return _section "}
{"171": "\ndef _deep_string_coerce ( content , json_path = 'json' ) : \n    c = _deep_string_coerce \n    if isinstance ( content , six . string_types ) : \n        return content \n    else : \n        if isinstance ( content , six . integer_types + ( float , ) ) : \n            return str ( content ) \n        else : \n            if isinstance ( content , ( list , tuple ) ) : \n                return [ c ( e , '{0}[{1}]' . format ( json_path , i ) ) for i , e in enumerate ( content ) ] \n            else : \n                if isinstance ( content , dict ) : \n                    return { k : c ( v , '{0}[{1}]' . format ( json_path , k ) ) for k , v in list ( content . items ( ) ) } \n                else : \n                    param_type = type ( content ) \n                    msg = 'Type {0} used for parameter {1} is not a number or a string' . format ( param_type , json_path ) \n                    raise AirflowException ( msg ) "}
{"191": "\ndef _get_executor ( executor_name ) : \n    if executor_name == Executors . LocalExecutor : \n        return LocalExecutor ( ) \n    else : \n        if executor_name == Executors . SequentialExecutor : \n            return SequentialExecutor ( ) \n        else : \n            if executor_name == Executors . CeleryExecutor : \n                from airflow . executors . celery_executor import CeleryExecutor \n                return CeleryExecutor ( ) \n            else : \n                if executor_name == Executors . DaskExecutor : \n                    from airflow . executors . dask_executor import DaskExecutor \n                    return DaskExecutor ( ) \n                else : \n                    if executor_name == Executors . KubernetesExecutor : \n                        from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor \n                        return KubernetesExecutor ( ) \n                    else : \n                        _integrate_plugins ( ) \n                        executor_path = executor_name . split ( '.' ) \n                        if len ( executor_path ) != 2 : \n                            raise AirflowException ( \"Executor {0} not supported: \" \"please specify in format plugin_module.executor\" . format ( executor_name ) ) \n                        if executor_path [ 0 ] in globals ( ) : \n                            return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) \n                        else : \n                            raise AirflowException ( \"Executor {0} not supported.\" . format ( executor_name ) ) "}
{"214": "\ndef update_state ( self , session = None ) : \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    self . log . debug ( \"Updating state for %s considering %s task(s)\" , self , len ( tis ) ) \n    for ti in list ( tis ) : \n        if ti . state == State . REMOVED : \n            tis . remove ( ti ) \n        else : \n            ti . task = dag . get_task ( ti . task_id ) \n    start_dttm = timezone . utcnow ( ) \n    unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) \n    none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) \n    none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) \n    if unfinished_tasks and none_depends_on_past and none_task_concurrency : \n        no_dependencies_met = True \n        for ut in unfinished_tasks : \n            old_state = ut . state \n            deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True , ignore_in_retry_period = True , ignore_in_reschedule_period = True ) , session = session ) \n            if deps_met or old_state != ut . current_state ( session = session ) : \n                no_dependencies_met = False \n                break \n    duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 \n    Stats . timing ( \"dagrun.dependency-check.{}\" . format ( self . dag_id ) , duration ) \n    root_ids = [ t . task_id for t in dag . roots ] \n    roots = [ t for t in tis if t . task_id in root_ids ] \n    if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : \n        self . log . info ( 'Marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = False , reason = 'task_failure' , session = session ) \n    else : \n        if not unfinished_tasks and all ( r . state in ( State . SUCCESS , State . SKIPPED ) for r in roots ) : \n            self . log . info ( 'Marking run %s successful' , self ) \n            self . set_state ( State . SUCCESS ) \n            dag . handle_callback ( self , success = True , reason = 'success' , session = session ) \n        else : \n            if ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : \n                self . log . info ( 'Deadlock; marking run %s failed' , self ) \n                self . set_state ( State . FAILED ) \n                dag . handle_callback ( self , success = False , reason = 'all_tasks_deadlocked' , session = session ) \n            else : \n                self . set_state ( State . RUNNING ) \n    self . _emit_duration_stats_for_finished_state ( ) \n    session . merge ( self ) \n    session . commit ( ) \n    return self . state "}
{"215": "\ndef verify_integrity ( self , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    task_ids = [ ] \n    for ti in tis : \n        task_ids . append ( ti . task_id ) \n        task = None \n        try : \n            task = dag . get_task ( ti . task_id ) \n        except AirflowException : \n            if ti . state == State . REMOVED : \n                pass \n            else : \n                if self . state is not State . RUNNING and not dag . partial : \n                    self . log . warning ( \"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\" . format ( ti , dag ) ) \n                    Stats . incr ( \"task_removed_from_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) \n                    ti . state = State . REMOVED \n        is_task_in_dag = task is not None \n        should_restore_task = is_task_in_dag and ti . state == State . REMOVED \n        if should_restore_task : \n            self . log . info ( \"Restoring task '{}' which was previously \" \"removed from DAG '{}'\" . format ( ti , dag ) ) \n            Stats . incr ( \"task_restored_to_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) \n            ti . state = State . NONE \n    for task in six . itervalues ( dag . task_dict ) : \n        if task . start_date > self . execution_date and not self . is_backfill : \n            continue \n        if task . task_id not in task_ids : \n            Stats . incr ( \"task_instance_created-{}\" . format ( task . __class__ . __name__ ) , 1 , 1 ) \n            ti = TaskInstance ( task , self . execution_date ) \n            session . add ( ti ) \n    session . commit ( ) "}
{"216": "\ndef jenkins_request_with_headers ( jenkins_server , req ) : \n    try : \n        response = jenkins_server . jenkins_request ( req ) \n        response_body = response . content \n        response_headers = response . headers \n        if response_body is None : \n            raise jenkins . EmptyResponseException ( \"Error communicating with server[%s]: \" \"empty response\" % jenkins_server . server ) \n        return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } \n    except HTTPError as e : \n        if e . code in [ 401 , 403 , 500 ] : \n            raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) \n        else : \n            if e . code == 404 : \n                raise jenkins . NotFoundException ( 'Requested item could not be found' ) \n            else : \n                raise \n    except socket . timeout as e : \n        raise jenkins . TimeoutException ( 'Error in request: %s' % e ) \n    except URLError as e : \n        if str ( e . reason ) == \"timed out\" : \n            raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) \n        raise JenkinsException ( 'Error in request: %s' % e . reason ) "}
{"221": "\ndef get_dag ( self , dag_id ) : \n    from airflow . models . dag import DagModel \n    root_dag_id = dag_id \n    if dag_id in self . dags : \n        dag = self . dags [ dag_id ] \n        if dag . is_subdag : \n            root_dag_id = dag . parent_dag . dag_id \n    orm_dag = DagModel . get_current ( root_dag_id ) \n    if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : \n        found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = False ) \n        if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : \n            return self . dags [ dag_id ] \n        else : \n            if dag_id in self . dags : \n                del self . dags [ dag_id ] \n    return self . dags . get ( dag_id ) "}
{"284": "\ndef execute ( self , context ) : \n    self . hook = self . get_hook ( ) \n    self . hook . get_conn ( ) \n    self . query_execution_context [ 'Database' ] = self . database \n    self . result_configuration [ 'OutputLocation' ] = self . output_location \n    self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) \n    query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) \n    if query_status in AWSAthenaHook . FAILURE_STATES : \n        raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) \n    else : \n        if not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : \n            raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) "}
{"285": "\ndef uncompress_file ( input_file_name , file_extension , dest_dir ) : \n    if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : \n        raise NotImplementedError ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file_extension ) ) \n    if file_extension . lower ( ) == '.gz' : \n        fmodule = gzip . GzipFile \n    else : \n        if file_extension . lower ( ) == '.bz2' : \n            fmodule = bz2 . BZ2File \n    with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = False ) as f_uncompressed : \n        shutil . copyfileobj ( f_compressed , f_uncompressed ) \n    return f_uncompressed . name "}
{"292": "\ndef _do_api_call ( self , endpoint_info , json ) : \n    method , endpoint = endpoint_info \n    url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) \n    if 'token' in self . databricks_conn . extra_dejson : \n        self . log . info ( 'Using token auth.' ) \n        auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) \n    else : \n        self . log . info ( 'Using basic auth.' ) \n        auth = ( self . databricks_conn . login , self . databricks_conn . password ) \n    if method == 'GET' : \n        request_func = requests . get \n    else : \n        if method == 'POST' : \n            request_func = requests . post \n        else : \n            raise AirflowException ( 'Unexpected HTTP Method: ' + method ) \n    attempt_num = 1 \n    while True : \n        try : \n            response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) \n            response . raise_for_status ( ) \n            return response . json ( ) \n        except requests_exceptions . RequestException as e : \n            if not _retryable_error ( e ) : \n                raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) \n            self . _log_request_error ( attempt_num , e ) \n        if attempt_num == self . retry_limit : \n            raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) \n        attempt_num += 1 \n        sleep ( self . retry_delay ) "}
{"299": "\ndef write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = False , record_time_added = False ) : \n    fmt = fmt . lower ( ) \n    if fmt not in [ 'csv' , 'json' , 'ndjson' ] : \n        raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) \n    df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) \n    df . columns = [ column . lower ( ) for column in df . columns ] \n    if coerce_to_timestamp and df . shape [ 0 ] > 0 : \n        object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] \n        self . log . info ( \"Coercing timestamps for: %s\" , object_name ) \n        schema = self . describe_object ( object_name ) \n        possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] \n        df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) \n    if record_time_added : \n        fetched_time = time . time ( ) \n        df [ \"time_fetched_from_salesforce\" ] = fetched_time \n    if fmt == \"csv\" : \n        self . log . info ( \"Cleaning data and writing to CSV\" ) \n        possible_strings = df . columns [ df . dtypes == \"object\" ] \n        df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) \n        df . to_csv ( filename , index = False ) \n    else : \n        if fmt == \"json\" : \n            df . to_json ( filename , \"records\" , date_unit = \"s\" ) \n        else : \n            if fmt == \"ndjson\" : \n                df . to_json ( filename , \"records\" , lines = True , date_unit = \"s\" ) \n    return df "}
{"317": "\ndef delete_file ( self , container_name , blob_name , is_prefix = False , ignore_if_missing = False , ** kwargs ) : \n    if is_prefix : \n        blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , ** kwargs ) ] \n    else : \n        if self . check_for_blob ( container_name , blob_name ) : \n            blobs_to_delete = [ blob_name ] \n        else : \n            blobs_to_delete = [ ] \n    if not ignore_if_missing and len ( blobs_to_delete ) == 0 : \n        raise AirflowException ( 'Blob(s) not found: {}' . format ( blob_name ) ) \n    for blob_uri in blobs_to_delete : \n        self . log . info ( \"Deleting blob: \" + blob_uri ) \n        self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , ** kwargs ) "}
{"352": "\ndef create_training_job ( self , config , wait_for_completion = True , print_log = True , check_interval = 30 , max_ingestion_time = None ) : \n    self . check_training_config ( config ) \n    response = self . get_conn ( ) . create_training_job ( ** config ) \n    if print_log : \n        self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) \n    else : \n        if wait_for_completion : \n            describe_response = self . check_status ( config [ 'TrainingJobName' ] , 'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) \n            billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] \n            self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) \n    return response "}
{"356": "\ndef describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : \n    log_group = '/aws/sagemaker/TrainingJobs' \n    if len ( stream_names ) < instance_count : \n        logs_conn = self . get_log_conn ( ) \n        try : \n            streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) \n            stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] \n            positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) \n        except logs_conn . exceptions . ResourceNotFoundException : \n            pass \n    if len ( stream_names ) > 0 : \n        for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : \n            self . log . info ( event [ 'message' ] ) \n            ts , count = positions [ stream_names [ idx ] ] \n            if event [ 'timestamp' ] == ts : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) \n            else : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) \n    if state == LogState . COMPLETE : \n        return state , last_description , last_describe_job_call \n    if state == LogState . JOB_COMPLETE : \n        state = LogState . COMPLETE \n    else : \n        if time . time ( ) - last_describe_job_call >= 30 : \n            description = self . describe_training_job ( job_name ) \n            last_describe_job_call = time . time ( ) \n            if secondary_training_status_changed ( description , last_description ) : \n                self . log . info ( secondary_training_status_message ( description , last_description ) ) \n                last_description = description \n            status = description [ 'TrainingJobStatus' ] \n            if status not in self . non_terminal_states : \n                state = LogState . JOB_COMPLETE \n    return state , last_description , last_describe_job_call "}
{"357": "\ndef check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : \n    if not non_terminal_states : \n        non_terminal_states = self . non_terminal_states \n    sec = 0 \n    running = True \n    while running : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        try : \n            response = describe_function ( job_name ) \n            status = response [ key ] \n            self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) \n        except KeyError : \n            raise AirflowException ( 'Could not get status of the SageMaker job' ) \n        except ClientError : \n            raise AirflowException ( 'AWS request failed, check logs for more info' ) \n        if status in non_terminal_states : \n            running = True \n        else : \n            if status in self . failed_states : \n                raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) \n            else : \n                running = False \n        if max_ingestion_time and sec > max_ingestion_time : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    self . log . info ( 'SageMaker Job Compeleted' ) \n    response = describe_function ( job_name ) \n    return response "}
{"367": "\ndef _prepare_cli_cmd ( self ) : \n    conn = self . conn \n    hive_bin = 'hive' \n    cmd_extra = [ ] \n    if self . use_beeline : \n        hive_bin = 'beeline' \n        jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) \n        if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : \n            template = conn . extra_dejson . get ( 'principal' , \"hive/_HOST@EXAMPLE.COM\" ) \n            if \"_HOST\" in template : \n                template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) \n            proxy_user = \"\" \n            if conn . extra_dejson . get ( 'proxy_user' ) == \"login\" and conn . login : \n                proxy_user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) \n            else : \n                if conn . extra_dejson . get ( 'proxy_user' ) == \"owner\" and self . run_as : \n                    proxy_user = \"hive.server2.proxy.user={0}\" . format ( self . run_as ) \n            jdbc_url += \";principal={template};{proxy_user}\" . format ( template = template , proxy_user = proxy_user ) \n        else : \n            if self . auth : \n                jdbc_url += \";auth=\" + self . auth \n        jdbc_url = '\"{}\"' . format ( jdbc_url ) \n        cmd_extra += [ '-u' , jdbc_url ] \n        if conn . login : \n            cmd_extra += [ '-n' , conn . login ] \n        if conn . password : \n            cmd_extra += [ '-p' , conn . password ] \n    hive_params_list = self . hive_cli_params . split ( ) \n    return [ hive_bin ] + cmd_extra + hive_params_list "}
{"382": "\ndef _bind_parameters ( operation , parameters ) : \n    string_parameters = { } \n    for ( name , value ) in iteritems ( parameters ) : \n        if value is None : \n            string_parameters [ name ] = 'NULL' \n        else : \n            if isinstance ( value , basestring ) : \n                string_parameters [ name ] = \"'\" + _escape ( value ) + \"'\" \n            else : \n                string_parameters [ name ] = str ( value ) \n    return operation % string_parameters "}
{"384": "\ndef _bq_cast ( string_field , bq_type ) : \n    if string_field is None : \n        return None \n    else : \n        if bq_type == 'INTEGER' : \n            return int ( string_field ) \n        else : \n            if bq_type == 'FLOAT' or bq_type == 'TIMESTAMP' : \n                return float ( string_field ) \n            else : \n                if bq_type == 'BOOLEAN' : \n                    if string_field not in [ 'true' , 'false' ] : \n                        raise ValueError ( \"{} must have value 'true' or 'false'\" . format ( string_field ) ) \n                    return string_field == 'true' \n                else : \n                    return string_field "}
{"391": "\ndef cancel_query ( self ) : \n    jobs = self . service . jobs ( ) \n    if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : \n        self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) \n        if self . location : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) \n        else : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) \n    else : \n        self . log . info ( 'No running BigQuery jobs to cancel.' ) \n        return \n    max_polling_attempts = 12 \n    polling_attempts = 0 \n    job_complete = False \n    while polling_attempts < max_polling_attempts and not job_complete : \n        polling_attempts = polling_attempts + 1 \n        job_complete = self . poll_job_complete ( self . running_job_id ) \n        if job_complete : \n            self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) \n        else : \n            if polling_attempts == max_polling_attempts : \n                self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) \n            else : \n                self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) \n                time . sleep ( 5 ) "}
{"411": "\ndef create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : \n    axis = { } \n    if custom_format and format : \n        axis [ 'tickFormat' ] = format \n    else : \n        if format : \n            if format == 'AM_PM' : \n                axis [ 'tickFormat' ] = \"function(d) { return get_am_pm(parseInt(d)); }\" \n            else : \n                axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format \n    if label : \n        axis [ 'axisLabel' ] = \"'\" + label + \"'\" \n    if date : \n        self . dateformat = format \n        axis [ 'tickFormat' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parseInt(d))) }\\n\" \"\" % self . dateformat ) \n        if name [ 0 ] == 'x' : \n            self . x_axis_date = True \n    self . axislist [ name ] = axis \n    if name == \"xAxis\" and self . focus_enable : \n        self . axislist [ 'x2Axis' ] = axis "}
{"412": "\ndef create_y_axis ( self , name , label = None , format = None , custom_format = False ) : \n    axis = { } \n    if custom_format and format : \n        axis [ 'tickFormat' ] = format \n    else : \n        if format : \n            axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format \n    if label : \n        axis [ 'axisLabel' ] = \"'\" + label + \"'\" \n    self . axislist [ name ] = axis "}
{"432": "\ndef poll_query_status ( self , query_execution_id , max_tries = None ) : \n    try_number = 1 \n    final_query_state = None \n    while True : \n        query_state = self . check_query_status ( query_execution_id ) \n        if query_state is None : \n            self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) \n        else : \n            if query_state in self . INTERMEDIATE_STATES : \n                self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) \n            else : \n                self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) \n                final_query_state = query_state \n                break \n        if max_tries and try_number >= max_tries : \n            final_query_state = query_state \n            break \n        try_number += 1 \n        sleep ( self . sleep_time ) \n    return final_query_state "}
{"456": "\ndef create_cluster ( self , cluster , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : \n    if isinstance ( cluster , dict ) : \n        cluster_proto = Cluster ( ) \n        cluster = self . _dict_to_proto ( py_dict = cluster , proto = cluster_proto ) \n    else : \n        if not isinstance ( cluster , Cluster ) : \n            raise AirflowException ( \"cluster is not instance of Cluster proto or python dict\" ) \n    self . _append_label ( cluster , 'airflow-version' , 'v' + version . version ) \n    self . log . info ( \"Creating (project_id=%s, zone=%s, cluster_name=%s)\" , self . project_id , self . location , cluster . name ) \n    try : \n        op = self . get_client ( ) . create_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster = cluster , retry = retry , timeout = timeout ) \n        op = self . wait_for_operation ( op ) \n        return op . target_link \n    except AlreadyExists as error : \n        self . log . info ( 'Assuming Success: %s' , error . message ) \n        return self . get_cluster ( name = cluster . name ) . self_link "}
{"458": "\ndef _get_webhook_endpoint ( self , http_conn_id , webhook_endpoint ) : \n    if webhook_endpoint : \n        endpoint = webhook_endpoint \n    else : \n        if http_conn_id : \n            conn = self . get_connection ( http_conn_id ) \n            extra = conn . extra_dejson \n            endpoint = extra . get ( 'webhook_endpoint' , '' ) \n        else : \n            raise AirflowException ( 'Cannot get webhook endpoint: No valid Discord ' 'webhook endpoint or http_conn_id supplied.' ) \n    if not re . match ( '^webhooks/[0-9]+/[a-zA-Z0-9_-]+$' , endpoint ) : \n        raise AirflowException ( 'Expected Discord webhook endpoint in the form ' 'of \"webhooks/{webhook.id}/{webhook.token}\".' ) \n    return endpoint "}
{"488": "\ndef _update_counters ( self , ti_status ) : \n    for key , ti in list ( ti_status . running . items ( ) ) : \n        ti . refresh_from_db ( ) \n        if ti . state == State . SUCCESS : \n            ti_status . succeeded . add ( key ) \n            self . log . debug ( \"Task instance %s succeeded. Don't rerun.\" , ti ) \n            ti_status . running . pop ( key ) \n            continue \n        else : \n            if ti . state == State . SKIPPED : \n                ti_status . skipped . add ( key ) \n                self . log . debug ( \"Task instance %s skipped. Don't rerun.\" , ti ) \n                ti_status . running . pop ( key ) \n                continue \n            else : \n                if ti . state == State . FAILED : \n                    self . log . error ( \"Task instance %s failed\" , ti ) \n                    ti_status . failed . add ( key ) \n                    ti_status . running . pop ( key ) \n                    continue \n                else : \n                    if ti . state == State . UP_FOR_RETRY : \n                        self . log . warning ( \"Task instance %s is up for retry\" , ti ) \n                        ti_status . running . pop ( key ) \n                        ti_status . to_run [ key ] = ti \n                    else : \n                        if ti . state == State . UP_FOR_RESCHEDULE : \n                            self . log . warning ( \"Task instance %s is up for reschedule\" , ti ) \n                            ti_status . running . pop ( key ) \n                            ti_status . to_run [ key ] = ti \n                        else : \n                            if ti . state == State . NONE : \n                                self . log . warning ( \"FIXME: task instance %s state was set to none externally or \" \"reaching concurrency limits. Re-adding task to queue.\" , ti ) \n                                ti . set_state ( State . SCHEDULED ) \n                                ti_status . running . pop ( key ) \n                                ti_status . to_run [ key ] = ti "}
{"495": "\ndef heartbeat_callback ( self , session = None ) : \n    if self . terminating : \n        self . task_runner . terminate ( ) \n        return \n    self . task_instance . refresh_from_db ( ) \n    ti = self . task_instance \n    fqdn = get_hostname ( ) \n    same_hostname = fqdn == ti . hostname \n    same_process = ti . pid == os . getpid ( ) \n    if ti . state == State . RUNNING : \n        if not same_hostname : \n            self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) \n            raise AirflowException ( \"Hostname of job runner does not match\" ) \n        else : \n            if not same_process : \n                current_pid = os . getpid ( ) \n                self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current_pid ) \n                raise AirflowException ( \"PID of job runner does not match\" ) \n    else : \n        if ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : \n            self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) \n            self . task_runner . terminate ( ) \n            self . terminating = True "}
{"513": "\ndef _process_spark_submit_log ( self , itr ) : \n    for line in itr : \n        line = line . strip ( ) \n        if self . _is_yarn and self . _connection [ 'deploy_mode' ] == 'cluster' : \n            match = re . search ( '(application[0-9_]+)' , line ) \n            if match : \n                self . _yarn_application_id = match . groups ( ) [ 0 ] \n                self . log . info ( \"Identified spark driver id: %s\" , self . _yarn_application_id ) \n        else : \n            if self . _is_kubernetes : \n                match = re . search ( r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)' , line ) \n                if match : \n                    self . _kubernetes_driver_pod = match . groups ( ) [ 0 ] \n                    self . log . info ( \"Identified spark driver pod: %s\" , self . _kubernetes_driver_pod ) \n                match_exit_code = re . search ( r'\\s*Exit code: (\\d+)' , line ) \n                if match_exit_code : \n                    self . _spark_exit_code = int ( match_exit_code . groups ( ) [ 0 ] ) \n            else : \n                if self . _should_track_driver_status and not self . _driver_id : \n                    match_driver_id = re . search ( r'(driver-[0-9\\-]+)' , line ) \n                    if match_driver_id : \n                        self . _driver_id = match_driver_id . groups ( ) [ 0 ] \n                        self . log . info ( \"identified spark driver id: {}\" . format ( self . _driver_id ) ) \n                else : \n                    self . log . info ( line ) \n        self . log . debug ( \"spark submit log: {}\" . format ( line ) ) "}
{"515": "\ndef get_task_runner ( local_task_job ) : \n    if _TASK_RUNNER == \"StandardTaskRunner\" : \n        return StandardTaskRunner ( local_task_job ) \n    else : \n        if _TASK_RUNNER == \"CgroupTaskRunner\" : \n            from airflow . contrib . task_runner . cgroup_task_runner import CgroupTaskRunner \n            return CgroupTaskRunner ( local_task_job ) \n        else : \n            raise AirflowException ( \"Unknown task runner type {}\" . format ( _TASK_RUNNER ) ) "}
{"519": "\ndef _write_local_schema_file ( self , cursor ) : \n    schema_str = None \n    schema_file_mime_type = 'application/json' \n    tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) \n    if self . schema is not None and isinstance ( self . schema , string_types ) : \n        schema_str = self . schema . encode ( 'utf-8' ) \n    else : \n        if self . schema is not None and isinstance ( self . schema , list ) : \n            schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) \n        else : \n            schema = [ ] \n            for field in cursor . description : \n                field_name = field [ 0 ] \n                field_type = self . type_map ( field [ 1 ] ) \n                if field [ 6 ] or field_type == 'TIMESTAMP' : \n                    field_mode = 'NULLABLE' \n                else : \n                    field_mode = 'REQUIRED' \n                schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) \n            schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) \n    tmp_schema_file_handle . write ( schema_str ) \n    self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) \n    schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } \n    return schema_file_to_upload "}
{"520": "\ndef _get_col_type_dict ( self ) : \n    schema = [ ] \n    if isinstance ( self . schema , string_types ) : \n        schema = json . loads ( self . schema ) \n    else : \n        if isinstance ( self . schema , list ) : \n            schema = self . schema \n        else : \n            if self . schema is not None : \n                self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) \n    col_type_dict = { } \n    try : \n        col_type_dict = { col [ 'name' ] : col [ 'type' ] for col in schema } \n    except KeyError : \n        self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying_a_json_schema_file' ) \n    return col_type_dict "}
{"522": "\ndef execute ( self , context ) : \n    self . hook = SqoopHook ( conn_id = self . conn_id , verbose = self . verbose , num_mappers = self . num_mappers , hcatalog_database = self . hcatalog_database , hcatalog_table = self . hcatalog_table , properties = self . properties ) \n    if self . cmd_type == 'export' : \n        self . hook . export_table ( table = self . table , export_dir = self . export_dir , input_null_string = self . input_null_string , input_null_non_string = self . input_null_non_string , staging_table = self . staging_table , clear_staging_table = self . clear_staging_table , enclosed_by = self . enclosed_by , escaped_by = self . escaped_by , input_fields_terminated_by = self . input_fields_terminated_by , input_lines_terminated_by = self . input_lines_terminated_by , input_optionally_enclosed_by = self . input_optionally_enclosed_by , batch = self . batch , relaxed_isolation = self . relaxed_isolation , extra_export_options = self . extra_export_options ) \n    else : \n        if self . cmd_type == 'import' : \n            if self . create_hcatalog_table : \n                self . extra_import_options [ 'create-hcatalog-table' ] = '' \n            if self . table and self . query : \n                raise AirflowException ( 'Cannot specify query and table together. Need to specify either or.' ) \n            if self . table : \n                self . hook . import_table ( table = self . table , target_dir = self . target_dir , append = self . append , file_type = self . file_type , columns = self . columns , split_by = self . split_by , where = self . where , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) \n            else : \n                if self . query : \n                    self . hook . import_query ( query = self . query , target_dir = self . target_dir , append = self . append , file_type = self . file_type , split_by = self . split_by , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) \n                else : \n                    raise AirflowException ( \"Provide query or table parameter to import using Sqoop\" ) \n        else : \n            raise AirflowException ( \"cmd_type should be 'import' or 'export'\" ) "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    else : \n        if isinstance ( delta , timedelta ) : \n            delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > 0 : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if num > 0 : \n                    start_date += delta \n                else : \n                    start_date -= delta \n    return sorted ( dates ) "}
{"526": "\ndef scale_time_units ( time_seconds_arr , unit ) : \n    if unit == 'minutes' : \n        return list ( map ( lambda x : x * 1.0 / 60 , time_seconds_arr ) ) \n    else : \n        if unit == 'hours' : \n            return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time_seconds_arr ) ) \n        else : \n            if unit == 'days' : \n                return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time_seconds_arr ) ) \n    return time_seconds_arr "}
{"543": "\ndef get_conn ( self ) : \n    effective_user = self . proxy_user \n    autoconfig = self . autoconfig \n    use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' \n    try : \n        connections = self . get_connections ( self . hdfs_conn_id ) \n        if not effective_user : \n            effective_user = connections [ 0 ] . login \n        if not autoconfig : \n            autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) \n        hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) \n    except AirflowException : \n        if not autoconfig : \n            raise \n    if autoconfig : \n        client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) \n    else : \n        if len ( connections ) == 1 : \n            client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n        else : \n            if len ( connections ) > 1 : \n                nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] \n                client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n            else : \n                raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) \n    return client "}
{"561": "\ndef _get_credentials ( self ) : \n    key_path = self . _get_field ( 'key_path' , False ) \n    keyfile_dict = self . _get_field ( 'keyfile_dict' , False ) \n    scope = self . _get_field ( 'scope' , None ) \n    if scope : \n        scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] \n    else : \n        scopes = _DEFAULT_SCOPES \n    if not key_path and not keyfile_dict : \n        self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) \n        credentials , _ = google . auth . default ( scopes = scopes ) \n    else : \n        if key_path : \n            if key_path . endswith ( '.json' ) : \n                self . log . debug ( 'Getting connection using JSON key file %s' % key_path ) \n                credentials = ( google . oauth2 . service_account . Credentials . from_service_account_file ( key_path , scopes = scopes ) ) \n            else : \n                if key_path . endswith ( '.p12' ) : \n                    raise AirflowException ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) \n                else : \n                    raise AirflowException ( 'Unrecognised extension for key file.' ) \n        else : \n            try : \n                keyfile_dict = json . loads ( keyfile_dict ) \n                keyfile_dict [ 'private_key' ] = keyfile_dict [ 'private_key' ] . replace ( '\\\\n' , '\\n' ) \n                credentials = ( google . oauth2 . service_account . Credentials . from_service_account_info ( keyfile_dict , scopes = scopes ) ) \n            except json . decoder . JSONDecodeError : \n                raise AirflowException ( 'Invalid key JSON.' ) \n    return credentials . with_subject ( self . delegate_to ) if self . delegate_to else credentials "}
{"567": "\ndef to_tensor ( pic ) : \n    if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : \n        raise TypeError ( 'pic should be PIL Image or ndarray. Got {}' . format ( type ( pic ) ) ) \n    if isinstance ( pic , np . ndarray ) : \n        if pic . ndim == 2 : \n            pic = pic [ : , : , None ] \n        img = torch . from_numpy ( pic . transpose ( ( 2 , 0 , 1 ) ) ) \n        if isinstance ( img , torch . ByteTensor ) : \n            return img . float ( ) . div ( 255 ) \n        else : \n            return img \n    if accimage is not None and isinstance ( pic , accimage . Image ) : \n        nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) \n        pic . copyto ( nppic ) \n        return torch . from_numpy ( nppic ) \n    if pic . mode == 'I' : \n        img = torch . from_numpy ( np . array ( pic , np . int32 , copy = False ) ) \n    else : \n        if pic . mode == 'I;16' : \n            img = torch . from_numpy ( np . array ( pic , np . int16 , copy = False ) ) \n        else : \n            if pic . mode == 'F' : \n                img = torch . from_numpy ( np . array ( pic , np . float32 , copy = False ) ) \n            else : \n                if pic . mode == '1' : \n                    img = 255 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = False ) ) \n                else : \n                    img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) \n    if pic . mode == 'YCbCr' : \n        nchannel = 3 \n    else : \n        if pic . mode == 'I;16' : \n            nchannel = 1 \n        else : \n            nchannel = len ( pic . mode ) \n    img = img . view ( pic . size [ 1 ] , pic . size [ 0 ] , nchannel ) \n    img = img . transpose ( 0 , 1 ) . transpose ( 0 , 2 ) . contiguous ( ) \n    if isinstance ( img , torch . ByteTensor ) : \n        return img . float ( ) . div ( 255 ) \n    else : \n        return img "}
{"584": "\ndef to_grayscale ( img , num_output_channels = 1 ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if num_output_channels == 1 : \n        img = img . convert ( 'L' ) \n    else : \n        if num_output_channels == 3 : \n            img = img . convert ( 'L' ) \n            np_img = np . array ( img , dtype = np . uint8 ) \n            np_img = np . dstack ( [ np_img , np_img , np_img ] ) \n            img = Image . fromarray ( np_img , 'RGB' ) \n        else : \n            raise ValueError ( 'num_output_channels should be either 1 or 3' ) \n    return img "}
{"598": "\ndef get_params ( img , scale , ratio ) : \n    area = img . size [ 0 ] * img . size [ 1 ] \n    for attempt in range ( 10 ) : \n        target_area = random . uniform ( * scale ) * area \n        log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) \n        aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) \n        w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) \n        h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) \n        if w <= img . size [ 0 ] and h <= img . size [ 1 ] : \n            i = random . randint ( 0 , img . size [ 1 ] - h ) \n            j = random . randint ( 0 , img . size [ 0 ] - w ) \n            return i , j , h , w \n    in_ratio = img . size [ 0 ] / img . size [ 1 ] \n    if ( in_ratio < min ( ratio ) ) : \n        w = img . size [ 0 ] \n        h = w / min ( ratio ) \n    else : \n        if ( in_ratio > max ( ratio ) ) : \n            h = img . size [ 1 ] \n            w = h * max ( ratio ) \n        else : \n            w = img . size [ 0 ] \n            h = img . size [ 1 ] \n    i = ( img . size [ 1 ] - h ) // 2 \n    j = ( img . size [ 0 ] - w ) // 2 \n    return i , j , h , w "}
{"608": "\ndef searx_bang ( full_query ) : \n    if len ( full_query . getSearchQuery ( ) ) == 0 : \n        return [ ] \n    results = [ ] \n    first_char = full_query . getSearchQuery ( ) [ 0 ] \n    if first_char == '!' or first_char == '?' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( first_char + \"images\" ) \n            results . append ( first_char + \"wikipedia\" ) \n            results . append ( first_char + \"osm\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for categorie in categories : \n                if categorie . startswith ( engine_query ) : \n                    results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) \n            for engine in engines : \n                if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) \n            for engine_shortcut in engine_shortcuts : \n                if engine_shortcut . startswith ( engine_query ) : \n                    results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) \n    else : \n        if first_char == ':' : \n            if len ( full_query . getSearchQuery ( ) ) == 1 : \n                results . append ( \":en\" ) \n                results . append ( \":en_us\" ) \n                results . append ( \":english\" ) \n                results . append ( \":united_kingdom\" ) \n            else : \n                engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n                for lc in language_codes : \n                    lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) \n                    if lang_id . startswith ( engine_query ) : \n                        if len ( engine_query ) <= 2 : \n                            results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) \n                        else : \n                            results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) \n                    if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : \n                        results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) \n                    if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                        results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) \n    result_set = set ( results ) \n    for query_part in full_query . query_parts : \n        if query_part in result_set : \n            result_set . remove ( query_part ) \n    return list ( result_set ) "}
{"621": "\ndef _max_mask_non_finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : \n    m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) \n    needs_masking = ~ np . isfinite ( m ) \n    if needs_masking . ndim > 0 : \n        m [ needs_masking ] = mask \n    else : \n        if needs_masking : \n            m = mask \n    return m "}
{"634": "\ndef _maybe_check_valid_map_values ( map_values , validate_args ) : \n    assertions = [ ] \n    message = 'Rank of map_values must be 1.' \n    if tensorshape_util . rank ( map_values . shape ) is not None : \n        if tensorshape_util . rank ( map_values . shape ) != 1 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) \n    message = 'Size of map_values must be greater than 0.' \n    if tensorshape_util . num_elements ( map_values . shape ) is not None : \n        if tensorshape_util . num_elements ( map_values . shape ) == 0 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) \n    return assertions "}
{"639": "\ndef transform_log_prob_fn ( log_prob_fn : PotentialFn , bijector : BijectorNest , init_state : State = None ) -> Union [ PotentialFn , Tuple [ PotentialFn , State ] ] : \n    def wrapper ( * args ) : \n        bijector_ = bijector \n        args = tf . nest . map_structure ( lambda x : 0. + x , args ) \n        if len ( args ) == 1 : \n            args = args [ 0 ] \n        else : \n            if isinstance ( bijector_ , list ) : \n                bijector_ = tuple ( bijector_ ) \n        original_space_args = tf . nest . map_structure ( lambda b , x : b . forward ( x ) , bijector_ , args ) \n        original_space_args = original_space_args \n        original_space_log_prob , extra = call_fn ( log_prob_fn , original_space_args ) \n        event_ndims = tf . nest . map_structure ( lambda x : tf . rank ( x ) - tf . rank ( original_space_log_prob ) , args ) \n        return original_space_log_prob + sum ( tf . nest . flatten ( tf . nest . map_structure ( lambda b , x , e : b . forward_log_det_jacobian ( x , event_ndims = e ) , bijector_ , args , event_ndims ) ) ) , [ original_space_args , extra ] \n    if init_state is None : \n        return wrapper \n    else : \n        return wrapper , tf . nest . map_structure ( lambda b , s : b . inverse ( s ) , bijector , init_state ) "}
{"652": "\ndef _maybe_validate_args ( outcomes , logits , probs , validate_args ) : \n    assertions = [ ] \n    def validate_equal_last_dim ( tensor_a , tensor_b , message ) : \n        if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : \n            if tensor_a . shape [ - 1 ] != tensor_b . shape [ - 1 ] : \n                raise ValueError ( message ) \n        else : \n            if validate_args : \n                assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - 1 ] , tf . shape ( input = tensor_b ) [ - 1 ] , message = message ) ) \n    if logits is not None : \n        validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) \n    if probs is not None : \n        validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) \n    message = 'Rank of outcomes must be 1.' \n    if outcomes . shape . ndims is not None : \n        if outcomes . shape . ndims != 1 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) \n    message = 'Size of outcomes must be greater than 0.' \n    if outcomes . shape . num_elements ( ) is not None : \n        if outcomes . shape . num_elements ( ) == 0 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) \n    return assertions "}
{"680": "\ndef maybe_check_quadrature_param ( param , name , validate_args ) : \n    with tf . name_scope ( \"check_\" + name ) : \n        assertions = [ ] \n        if tensorshape_util . rank ( param . shape ) is not None : \n            if tensorshape_util . rank ( param . shape ) == 0 : \n                raise ValueError ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape_util . rank ( param . shape ) ) ) \n        else : \n            if validate_args : \n                assertions . append ( assert_util . assert_rank_at_least ( param , 1 , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) \n        if tensorshape_util . with_rank_at_least ( param . shape , 1 ) [ - 1 ] is not None : \n            if tf . compat . dimension_value ( param . shape [ - 1 ] ) != 1 : \n                raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension_value ( param . shape [ - 1 ] ) ) ) \n        else : \n            if validate_args : \n                assertions . append ( assert_util . assert_equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) \n        if assertions : \n            return distribution_util . with_dependencies ( assertions , param ) \n        return param "}
{"682": "\ndef interpolate_loc ( grid , loc ) : \n    if len ( loc ) != 2 : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_loc\" ) : \n        if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : \n            return [ None ] * deg \n        w = grid [ ... , tf . newaxis , : , : ] \n        loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] \n        if loc [ 0 ] is None : \n            x = w [ ... , 1 , : ] * loc [ 1 ] \n        else : \n            if loc [ 1 ] is None : \n                x = w [ ... , 0 , : ] * loc [ 0 ] \n            else : \n                delta = loc [ 0 ] - loc [ 1 ] \n                x = w [ ... , 0 , : ] * delta + loc [ 1 ] \n        return [ x [ ... , k ] for k in range ( deg ) ] "}
{"739": "\ndef _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : \n    if direction == 'ASCENDING' : \n        pass \n    else : \n        if direction == 'DESCENDING' : \n            values = np . negative ( values ) \n        else : \n            raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' ) "}
{"740": "\ndef _sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : \n    if direction == 'ASCENDING' : \n        pass \n    else : \n        if direction == 'DESCENDING' : \n            values = np . negative ( values ) \n        else : \n            raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) \n    if direction == 'DESCENDING' : \n        return np . negative ( result ) \n    return result "}
{"744": "\ndef log_ndtr ( x , series_order = 3 , name = \"log_ndtr\" ) : \n    if not isinstance ( series_order , int ) : \n        raise TypeError ( \"series_order must be a Python integer.\" ) \n    if series_order < 0 : \n        raise ValueError ( \"series_order must be non-negative.\" ) \n    if series_order > 30 : \n        raise ValueError ( \"series_order must be <= 30.\" ) \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        if dtype_util . base_equal ( x . dtype , tf . float64 ) : \n            lower_segment = LOGNDTR_FLOAT64_LOWER \n            upper_segment = LOGNDTR_FLOAT64_UPPER \n        else : \n            if dtype_util . base_equal ( x . dtype , tf . float32 ) : \n                lower_segment = LOGNDTR_FLOAT32_LOWER \n                upper_segment = LOGNDTR_FLOAT32_UPPER \n            else : \n                raise TypeError ( \"x.dtype=%s is not supported.\" % x . dtype ) \n        return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) ) "}
{"777": "\ndef common_dtype ( args_list , preferred_dtype = None ) : \n    dtype = None \n    preferred_dtype = ( None if preferred_dtype is None else tf . as_dtype ( preferred_dtype ) ) \n    for a in tf . nest . flatten ( args_list ) : \n        if hasattr ( a , 'dtype' ) : \n            dt = tf . as_dtype ( a . dtype ) \n        else : \n            continue \n        if dtype is None : \n            dtype = dt \n        else : \n            if dtype != dt : \n                raise TypeError ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) \n    if dtype is None and preferred_dtype is None : \n        return None \n    return ( preferred_dtype if dtype is None else dtype ) . as_numpy_dtype "}
{"788": "\ndef merge ( self , x = None , y = None , ildj = None , kwargs = None , mapping = None ) : \n    if mapping is None : \n        mapping = _Mapping ( x = x , y = y , ildj = ildj , kwargs = kwargs ) \n    else : \n        if any ( arg is not None for arg in [ x , y , ildj , kwargs ] ) : \n            raise ValueError ( \"Cannot simultaneously specify mapping and individual \" \"arguments.\" ) \n    return _Mapping ( x = self . _merge ( self . x , mapping . x ) , y = self . _merge ( self . y , mapping . y ) , ildj = self . _merge ( self . ildj , mapping . ildj ) , kwargs = self . _merge ( self . kwargs , mapping . kwargs , use_equals = True ) ) "}
{"791": "\ndef _deep_tuple ( self , x ) : \n    if isinstance ( x , dict ) : \n        return self . _deep_tuple ( tuple ( sorted ( x . items ( ) ) ) ) \n    else : \n        if isinstance ( x , ( list , tuple ) ) : \n            return tuple ( map ( self . _deep_tuple , x ) ) \n    return x "}
{"810": "\ndef deserialize_function ( serial , function_type ) : \n    if function_type == 'function' : \n        function = tf . keras . utils . deserialize_keras_object ( serial ) \n    else : \n        if function_type == 'lambda' : \n            function = generic_utils . func_load ( serial ) \n        else : \n            raise TypeError ( 'Unknown function type:' , function_type ) \n    return function "}
{"815": "\ndef call_fn ( fn , args ) : \n    if expand_as_args ( args ) : \n        return fn ( * args ) \n    else : \n        if _expand_as_kwargs ( args ) : \n            return fn ( ** args ) \n        else : \n            return fn ( args ) "}
{"821": "\ndef _validate_block_sizes ( block_sizes , bijectors , validate_args ) : \n    block_sizes_shape = block_sizes . shape \n    if tensorshape_util . is_fully_defined ( block_sizes_shape ) : \n        if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : \n            raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) \n        return block_sizes \n    else : \n        if validate_args : \n            message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) \n            with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : \n                return tf . identity ( block_sizes ) \n        else : \n            return block_sizes "}
{"856": "\ndef _assert_same_base_type ( items , expected_type = None ) : \n    original_expected_type = expected_type \n    mismatch = False \n    for item in items : \n        if item is not None : \n            item_type = base_dtype ( item . dtype ) \n            if not expected_type : \n                expected_type = item_type \n            else : \n                if expected_type != item_type : \n                    mismatch = True \n                    break \n    if mismatch : \n        expected_type = original_expected_type \n        original_item_str = None \n        get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) \n        for item in items : \n            if item is not None : \n                item_type = base_dtype ( item . dtype ) \n                if not expected_type : \n                    expected_type = item_type \n                    original_item_str = get_name ( item ) \n                else : \n                    if expected_type != item_type : \n                        raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) \n        return expected_type \n    else : \n        return expected_type "}
{"857": "\ndef assert_same_float_dtype ( tensors = None , dtype = None ) : \n    if tensors : \n        dtype = _assert_same_base_type ( tensors , dtype ) \n    if not dtype : \n        dtype = tf . float32 \n    else : \n        if not is_floating ( dtype ) : \n            raise ValueError ( 'Expected floating point type, got {}.' . format ( dtype ) ) \n    return dtype "}
{"898": "\ndef _get_static_predicate ( pred ) : \n    if pred in { 0 , 1 } : \n        pred_value = bool ( pred ) \n    else : \n        if isinstance ( pred , bool ) : \n            pred_value = pred \n        else : \n            if isinstance ( pred , tf . Tensor ) : \n                pred_value = tf . get_static_value ( pred ) \n                if pred_value is None : \n                    pred_value = c_api . TF_TryEvaluateConstant_wrapper ( pred . graph . _c_graph , pred . _as_tf_output ( ) ) \n            else : \n                raise TypeError ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) \n    return pred_value "}
{"905": "\ndef shapes_from_loc_and_scale ( loc , scale , name = \"shapes_from_loc_and_scale\" ) : \n    if loc is not None and tensorshape_util . rank ( loc . shape ) == 0 : \n        loc = None \n    with tf . name_scope ( name ) : \n        event_size = scale . range_dimension_tensor ( ) \n        event_size_ = tf . get_static_value ( event_size ) \n        loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - 1 ] ) ) \n        if event_size_ is not None and loc_event_size_ is not None : \n            if loc_event_size_ != 1 and loc_event_size_ != event_size_ : \n                raise ValueError ( \"Event size of 'scale' ({}) could not be broadcast up to that \" \"of 'loc' ({}).\" . format ( event_size_ , loc_event_size_ ) ) \n        else : \n            if loc_event_size_ is not None and loc_event_size_ != 1 : \n                event_size_ = loc_event_size_ \n        if event_size_ is None : \n            event_shape = event_size [ tf . newaxis ] \n        else : \n            event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ 1 ] ) , dtype = tf . int32 , name = \"event_shape\" ) \n        batch_shape = scale . batch_shape_tensor ( ) \n        if loc is not None : \n            loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , 1 ) [ : - 1 ] \n            if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : \n                loc_batch_shape = tf . shape ( input = loc ) [ : - 1 ] \n            else : \n                loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = \"loc_batch_shape\" ) \n            batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) \n            batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = \"batch_shape\" ) \n        return batch_shape , event_shape "}
{"907": "\ndef maybe_check_scalar_distribution ( distribution , expected_base_dtype , validate_args ) : \n    if distribution . dtype != expected_base_dtype : \n        raise TypeError ( \"dtype mismatch; \" \"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\" . format ( dtype_util . name ( distribution . dtype ) , dtype_util . name ( expected_base_dtype ) ) ) \n    if validate_args and ( distribution . reparameterization_type != reparameterization . FULLY_REPARAMETERIZED ) : \n        raise ValueError ( \"Base distribution should be reparameterized or be \" \"a function of non-trainable variables; \" \"distribution.reparameterization_type = \\\"{}\\\" \" \"!= \\\"FULLY_REPARAMETERIZED\\\".\" . format ( distribution . reparameterization_type ) ) \n    with tf . name_scope ( \"check_distribution\" ) : \n        assertions = [ ] \n        def check_is_scalar ( is_scalar , name ) : \n            is_scalar_ = tf . get_static_value ( is_scalar ) \n            if is_scalar_ is not None : \n                if not is_scalar_ : \n                    raise ValueError ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) \n            else : \n                if validate_args : \n                    assertions . append ( assert_util . assert_equal ( is_scalar , True , message = ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) ) ) \n        check_is_scalar ( distribution . is_scalar_event ( ) , \"is_scalar_event\" ) \n        check_is_scalar ( distribution . is_scalar_batch ( ) , \"is_scalar_batch\" ) \n        return assertions "}
{"925": "\ndef tridiag ( below = None , diag = None , above = None , name = None ) : \n    def _pad ( x ) : \n        shape = tf . concat ( [ tf . shape ( input = x ) [ : - 1 ] , [ 1 ] ] , axis = 0 ) \n        z = tf . zeros ( shape , dtype = x . dtype ) \n        return tf . concat ( [ z , x , z ] , axis = - 1 ) \n    def _add ( * x ) : \n        s = None \n        for y in x : \n            if y is None : \n                continue \n            else : \n                if s is None : \n                    s = y \n                else : \n                    s += y \n        if s is None : \n            raise ValueError ( \"Must specify at least one of `below`, `diag`, `above`.\" ) \n        return s \n    with tf . name_scope ( name or \"tridiag\" ) : \n        if below is not None : \n            below = tf . convert_to_tensor ( value = below , name = \"below\" ) \n            below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - 1 , 1 : ] \n        if diag is not None : \n            diag = tf . convert_to_tensor ( value = diag , name = \"diag\" ) \n            diag = tf . linalg . diag ( diag ) \n        if above is not None : \n            above = tf . convert_to_tensor ( value = above , name = \"above\" ) \n            above = tf . linalg . diag ( _pad ( above ) ) [ ... , 1 : , : - 1 ] \n        return _add ( below , diag , above ) "}
{"927": "\ndef process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : \n    with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : \n        if quadrature_grid_and_probs is None : \n            grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) \n            grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) \n            grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n            probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) \n            return grid , probs \n        grid , probs = tuple ( quadrature_grid_and_probs ) \n        grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) \n        probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = \"probs\" ) \n        def _static_event_size ( x ) : \n            return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) \n        m , n = _static_event_size ( probs ) , _static_event_size ( grid ) \n        if m is not None and n is not None : \n            if m != n : \n                raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) \n        else : \n            if validate_args : \n                assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] \n                with tf . control_dependencies ( assertions ) : \n                    grid = tf . identity ( grid ) \n                    probs = tf . identity ( probs ) \n        return grid , probs "}
{"929": "\ndef expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : \n    with tf . name_scope ( op_name or \"expand_to_vector\" ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is None : \n            if validate_args : \n                x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = \"Input is neither scalar nor vector.\" ) ] , x ) \n            ndims = tf . rank ( x ) \n            expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) \n            return tf . reshape ( x , expanded_shape ) \n        else : \n            if ndims == 0 : \n                x_const = tf . get_static_value ( x ) \n                if x_const is not None : \n                    return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) \n                else : \n                    return tf . reshape ( x , [ 1 ] ) \n            else : \n                if ndims != 1 : \n                    raise ValueError ( \"Input is neither scalar nor vector.\" ) \n        return x "}
{"931": "\ndef _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : \n            raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) \n        if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : \n            if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != 0 : \n                raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) \n        else : \n            if validate_args : \n                assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] \n        rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) \n        msg = '`rightmost_transposed_ndims` must be non-negative.' \n        if rightmost_transposed_ndims_ is not None : \n            if rightmost_transposed_ndims_ < 0 : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) \n        else : \n            if validate_args : \n                assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] \n        return assertions "}
{"932": "\ndef _maybe_validate_perm ( perm , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_perm' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( perm . dtype ) : \n            raise TypeError ( '`perm` must be integer type' ) \n        msg = '`perm` must be a vector.' \n        if tensorshape_util . rank ( perm . shape ) is not None : \n            if tensorshape_util . rank ( perm . shape ) != 1 : \n                raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) \n        else : \n            if validate_args : \n                assertions += [ assert_util . assert_rank ( perm , 1 , message = msg ) ] \n        perm_ = tf . get_static_value ( perm ) \n        msg = '`perm` must be a valid permutation vector.' \n        if perm_ is not None : \n            if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) \n        else : \n            if validate_args : \n                assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] \n        return assertions "}
{"975": "\ndef _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : \n    output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) \n    validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) \n    if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : \n        with tf . control_dependencies ( validation_dependencies ) : \n            output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) \n        return output_shape , output_tensorshape \n    with tf . control_dependencies ( validation_dependencies ) : \n        event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) \n        input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) \n    additional_assertions = [ ] \n    if is_validated : \n        pass \n    else : \n        if validate_args : \n            mask = event_shape_in >= 0 \n            explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) \n            explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) \n            additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) \n    with tf . control_dependencies ( additional_assertions ) : \n        output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) \n    return output_shape , output_tensorshape "}
{"977": "\ndef _maybe_check_valid_shape ( shape , validate_args ) : \n    if not dtype_util . is_integer ( shape . dtype ) : \n        raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) \n    assertions = [ ] \n    message = '`{}` rank should be <= 1.' \n    if tensorshape_util . rank ( shape . shape ) is not None : \n        if tensorshape_util . rank ( shape . shape ) > 1 : \n            raise ValueError ( message . format ( shape ) ) \n    else : \n        if validate_args : \n            assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) \n    shape_ = tf . get_static_value ( shape ) \n    message = '`{}` elements must have at most one `-1`.' \n    if shape_ is not None : \n        if sum ( shape_ == - 1 ) > 1 : \n            raise ValueError ( message . format ( shape ) ) \n    else : \n        if validate_args : \n            assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) \n    message = '`{}` elements must be either positive integers or `-1`.' \n    if shape_ is not None : \n        if np . any ( shape_ < - 1 ) : \n            raise ValueError ( message . format ( shape ) ) \n    else : \n        if validate_args : \n            assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) \n    return assertions "}
{"994": "\ndef val_where ( cond , tval , fval ) : \n    if isinstance ( tval , tf . Tensor ) : \n        return tf . where ( cond , tval , fval ) \n    else : \n        if isinstance ( tval , tuple ) : \n            cls = type ( tval ) \n            return cls ( * ( val_where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) \n        else : \n            raise Exception ( TypeError ) "}
{"1028": "\ndef _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : \n    assertions = [ ] \n    if not _is_iterable ( distributions ) or not distributions : \n        raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) \n    if dtype_override is None : \n        dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] \n        if dts [ 1 : ] != dts [ : - 1 ] : \n            raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) \n    for d in distributions : \n        if tensorshape_util . rank ( d . event_shape ) is not None : \n            if tensorshape_util . rank ( d . event_shape ) != 1 : \n                raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) \n        else : \n            if validate_args : \n                assertions . append ( assert_util . assert_equal ( 1 , tf . size ( input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) \n    batch_shapes = [ d . batch_shape for d in distributions ] \n    if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : \n        if batch_shapes [ 1 : ] != batch_shapes [ : - 1 ] : \n            raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) \n    else : \n        if validate_args : \n            batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] \n            assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) \n    return assertions "}
{"1042": "\ndef amari_alpha ( logu , alpha = 1. , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"amari_alpha\" , [ logu ] ) : \n        if alpha is None or tf . is_tensor ( alpha ) : \n            raise TypeError ( \"`alpha` cannot be `None` or `Tensor` type.\" ) \n        if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : \n            raise TypeError ( \"`self_normalized` cannot be `None` or `Tensor` type.\" ) \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        if alpha == 0. : \n            f = - logu \n        else : \n            if alpha == 1. : \n                f = tf . exp ( logu ) * logu \n            else : \n                f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) \n        if not self_normalized : \n            return f \n        if alpha == 0. : \n            return f + tf . math . expm1 ( logu ) \n        else : \n            if alpha == 1. : \n                return f - tf . math . expm1 ( logu ) \n            else : \n                return f - tf . math . expm1 ( logu ) / ( alpha - 1. ) "}
{"1054": "\ndef monte_carlo_csiszar_f_divergence ( f , p_log_prob , q , num_draws , use_reparametrization = None , seed = None , name = None ) : \n    reparameterization_types = tf . nest . flatten ( q . reparameterization_type ) \n    with tf . compat . v1 . name_scope ( name , \"monte_carlo_csiszar_f_divergence\" , [ num_draws ] ) : \n        if use_reparametrization is None : \n            use_reparametrization = all ( reparameterization_type == tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) \n        else : \n            if ( use_reparametrization and any ( reparameterization_type != tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) ) : \n                raise ValueError ( \"Distribution `q` must be reparameterized, i.e., a diffeomorphic \" \"transformation of a parameterless distribution. (Otherwise this \" \"function has a biased gradient.)\" ) \n        if not callable ( p_log_prob ) : \n            raise TypeError ( \"`p_log_prob` must be a Python `callable` function.\" ) \n        return monte_carlo . expectation ( f = lambda q_samples : f ( p_log_prob ( q_samples ) - q . log_prob ( q_samples ) ) , samples = q . sample ( num_draws , seed = seed ) , log_prob = q . log_prob , use_reparametrization = use_reparametrization ) "}
{"1058": "\ndef _broadcast_cat_event_and_params ( event , params , base_dtype ) : \n    if dtype_util . is_integer ( event . dtype ) : \n        pass \n    else : \n        if dtype_util . is_floating ( event . dtype ) : \n            event = tf . cast ( event , dtype = tf . int32 ) \n        else : \n            raise TypeError ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base_dtype ) ) \n    shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) \n    if not shape_known_statically or params . shape [ : - 1 ] != event . shape : \n        params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) \n        params_shape = tf . shape ( input = params ) [ : - 1 ] \n        event *= tf . ones ( params_shape , dtype = event . dtype ) \n        if tensorshape_util . rank ( params . shape ) is not None : \n            tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) \n    return event , params "}
{"1109": "\ndef _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : \n    assertions = [ ] \n    message = 'Input `lower_upper` must have at least 2 dimensions.' \n    if lower_upper . shape . ndims is not None : \n        if lower_upper . shape . ndims < 2 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) \n    message = '`rank(lower_upper)` must equal `rank(perm) + 1`' \n    if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : \n        if lower_upper . shape . ndims != perm . shape . ndims + 1 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) \n    message = '`lower_upper` must be square.' \n    if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : \n        if lower_upper . shape [ - 2 ] != lower_upper . shape [ - 1 ] : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) \n            assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) \n    return assertions "}
{"1110": "\ndef _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : \n    assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n    message = 'Input `rhs` must have at least 2 dimensions.' \n    if rhs . shape . ndims is not None : \n        if rhs . shape . ndims < 2 : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) \n    message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' \n    if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : \n        if lower_upper . shape [ - 1 ] != rhs . shape [ - 2 ] : \n            raise ValueError ( message ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) \n    return assertions "}
{"1112": "\ndef _maybe_validate_matrix ( a , validate_args ) : \n    assertions = [ ] \n    if not a . dtype . is_floating : \n        raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) \n    if a . shape . ndims is not None : \n        if a . shape . ndims < 2 : \n            raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) \n    else : \n        if validate_args : \n            assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) \n    return assertions "}
{"1118": "\ndef _create_input_order ( input_size , input_order = \"left-to-right\" ) : \n    if isinstance ( input_order , six . string_types ) : \n        if input_order == \"left-to-right\" : \n            return np . arange ( start = 1 , stop = input_size + 1 ) \n        else : \n            if input_order == \"right-to-left\" : \n                return np . arange ( start = input_size , stop = 0 , step = - 1 ) \n            else : \n                if input_order == \"random\" : \n                    ret = np . arange ( start = 1 , stop = input_size + 1 ) \n                    np . random . shuffle ( ret ) \n                    return ret \n    else : \n        if np . all ( np . sort ( input_order ) == np . arange ( 1 , input_size + 1 ) ) : \n            return np . array ( input_order ) \n    raise ValueError ( \"Invalid input order: '{}'.\" . format ( input_order ) ) "}
{"1119": "\ndef _create_degrees ( input_size , hidden_units = None , input_order = \"left-to-right\" , hidden_degrees = \"equal\" ) : \n    input_order = _create_input_order ( input_size , input_order ) \n    degrees = [ input_order ] \n    if hidden_units is None : \n        hidden_units = [ ] \n    for units in hidden_units : \n        if isinstance ( hidden_degrees , six . string_types ) : \n            if hidden_degrees == \"random\" : \n                degrees . append ( np . random . randint ( low = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) , high = input_size , size = units ) ) \n            else : \n                if hidden_degrees == \"equal\" : \n                    min_degree = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) \n                    degrees . append ( np . maximum ( min_degree , np . ceil ( np . arange ( 1 , units + 1 ) * ( input_size - 1 ) / float ( units + 1 ) ) . astype ( np . int32 ) ) ) \n        else : \n            raise ValueError ( 'Invalid hidden order: \"{}\".' . format ( hidden_degrees ) ) \n    return degrees "}
{"1147": "\ndef create ( model , training_set , criterion , end_trigger = None , batch_size = 32 , optim_method = None , cores = None , bigdl_type = \"float\" ) : \n    if not end_trigger : \n        end_trigger = MaxEpoch ( 1 ) \n    if not optim_method : \n        optim_method = SGD ( ) \n    if isinstance ( training_set , RDD ) or isinstance ( training_set , DataSet ) : \n        return DistriOptimizer ( model = model , training_rdd = training_set , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , bigdl_type = bigdl_type ) \n    else : \n        if isinstance ( training_set , tuple ) and len ( training_set ) == 2 : \n            x , y = training_set \n            return LocalOptimizer ( X = x , Y = y , model = model , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , cores = cores , bigdl_type = \"float\" ) \n        else : \n            raise Exception ( \"Not supported training set: %s\" % type ( training_set ) ) "}
{"1154": "\ndef fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , validation_data = None , distributed = True ) : \n    if distributed : \n        if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : \n            training_data = to_sample_rdd ( x , y ) \n            if validation_data : \n                validation_data = to_sample_rdd ( * validation_data ) \n        else : \n            if ( isinstance ( x , RDD ) or isinstance ( x , DataSet ) ) and not y : \n                training_data = x \n            else : \n                raise TypeError ( \"Unsupported training data type: %s\" % type ( x ) ) \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , training_data , batch_size , nb_epoch , validation_data ) \n    else : \n        if validation_data : \n            val_x = [ JTensor . from_ndarray ( x ) for x in to_list ( validation_data [ 0 ] ) ] \n            val_y = JTensor . from_ndarray ( validation_data [ 1 ] ) \n        else : \n            val_x , val_y = None , None \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , [ JTensor . from_ndarray ( x ) for x in to_list ( x ) ] , JTensor . from_ndarray ( y ) , batch_size , nb_epoch , val_x , val_y , multiprocessing . cpu_count ( ) ) "}
{"1155": "\ndef evaluate ( self , x , y = None , batch_size = 32 ) : \n    if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : \n        evaluation_data = to_sample_rdd ( x , y ) \n    else : \n        if isinstance ( x , RDD ) and not y : \n            evaluation_data = x \n        else : \n            raise TypeError ( \"Unsupported evaluation data type: %s\" % type ( x ) ) \n    return callBigDlFunc ( self . bigdl_type , \"evaluate\" , self . value , evaluation_data , batch_size ) "}
{"1156": "\ndef predict ( self , x , distributed = True ) : \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            features = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        else : \n            if isinstance ( x , RDD ) : \n                features = x \n            else : \n                raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) \n        return self . predict_distributed ( features ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . predict_local ( x ) \n        else : \n            raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) "}
{"1165": "\ndef _py2java ( gateway , obj ) : \n    if isinstance ( obj , RDD ) : \n        obj = _to_java_object_rdd ( obj ) \n    else : \n        if isinstance ( obj , DataFrame ) : \n            obj = obj . _jdf \n        else : \n            if isinstance ( obj , SparkContext ) : \n                obj = obj . _jsc \n            else : \n                if isinstance ( obj , ( list , tuple ) ) : \n                    obj = ListConverter ( ) . convert ( [ _py2java ( gateway , x ) for x in obj ] , gateway . _gateway_client ) \n                else : \n                    if isinstance ( obj , dict ) : \n                        result = { } \n                        for ( key , value ) in obj . items ( ) : \n                            result [ key ] = _py2java ( gateway , value ) \n                        obj = MapConverter ( ) . convert ( result , gateway . _gateway_client ) \n                    else : \n                        if isinstance ( obj , JavaValue ) : \n                            obj = obj . value \n                        else : \n                            if isinstance ( obj , JavaObject ) : \n                                pass \n                            else : \n                                if isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : \n                                    pass \n                                else : \n                                    data = bytearray ( PickleSerializer ( ) . dumps ( obj ) ) \n                                    obj = gateway . jvm . org . apache . spark . bigdl . api . python . BigDLSerDe . loads ( data ) \n    return obj "}
{"1166": "\ndef get_activation_by_name ( activation_name , activation_id = None ) : \n    import bigdl . nn . layer as BLayer \n    activation = None \n    activation_name = activation_name . lower ( ) \n    if activation_name == \"tanh\" : \n        activation = BLayer . Tanh ( ) \n    else : \n        if activation_name == \"sigmoid\" : \n            activation = BLayer . Sigmoid ( ) \n        else : \n            if activation_name == \"hard_sigmoid\" : \n                activation = BLayer . HardSigmoid ( ) \n            else : \n                if activation_name == \"relu\" : \n                    activation = BLayer . ReLU ( ) \n                else : \n                    if activation_name == \"softmax\" : \n                        activation = BLayer . SoftMax ( ) \n                    else : \n                        if activation_name == \"softplus\" : \n                            activation = BLayer . SoftPlus ( beta = 1.0 ) \n                        else : \n                            if activation_name == \"softsign\" : \n                                activation = BLayer . SoftSign ( ) \n                            else : \n                                if activation_name == \"linear\" : \n                                    activation = BLayer . Identity ( ) \n                                else : \n                                    raise Exception ( \"Unsupported activation type: %s\" % activation_name ) \n    if not activation_id : \n        activation . set_name ( activation_id ) \n    return activation "}
{"1175": "\ndef predict ( self , x , batch_size = None , verbose = None , is_distributed = False ) : \n    if batch_size or verbose : \n        raise Exception ( \"we don't support batch_size or verbose for now\" ) \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            input = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        else : \n            if isinstance ( x , RDD ) : \n                input = x \n        return self . bmodel . predict ( input ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . bmodel . predict_local ( x ) \n    raise Exception ( \"not supported type: %s\" % x ) "}
{"1186": "\ndef load_keras ( json_path = None , hdf5_path = None , by_name = False ) : \n    import os \n    try : \n        import tensorflow \n    except ImportError : \n        os . environ [ 'KERAS_BACKEND' ] = \"theano\" \n        try : \n            from theano import ifelse \n        except ImportError : \n            raise Exception ( \"No backend is found for Keras. \" \"Please install either tensorflow or theano.\" ) \n    from bigdl . keras . converter import DefinitionLoader , WeightLoader \n    if json_path and not hdf5_path : \n        return DefinitionLoader . from_json_path ( json_path ) \n    else : \n        if json_path and hdf5_path : \n            return WeightLoader . load_weights_from_json_hdf5 ( json_path , hdf5_path , by_name = by_name ) \n        else : \n            if hdf5_path and not json_path : \n                kmodel , bmodel = DefinitionLoader . from_hdf5_path ( hdf5_path ) \n                WeightLoader . load_weights_from_kmodel ( bmodel , kmodel ) \n                return bmodel "}
{"1205": "\ndef _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : \n    if isinstance ( utterance , str ) : \n        pass \n    else : \n        if isinstance ( utterance , RichMessage ) : \n            utterance = utterance . json ( ) \n        else : \n            if isinstance ( utterance , ( list , dict ) ) : \n                utterance = jsonify_data ( utterance ) \n            else : \n                utterance = str ( utterance ) \n    dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id \n    if self . log_file . tell ( ) >= self . log_max_size * 1024 : \n        self . log_file . close ( ) \n        self . log_file = self . _get_log_file ( ) \n    else : \n        try : \n            log_msg = { } \n            log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) \n            log_msg [ 'dialog_id' ] = dialog_id \n            log_msg [ 'direction' ] = direction \n            log_msg [ 'message' ] = utterance \n            log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) \n            self . log_file . write ( f'{log_str}\\n' ) \n        except IOError : \n            log . error ( 'Failed to write dialog log.' ) "}
{"1215": "\ndef get_momentum_variable ( self ) : \n    optimizer = self . get_optimizer ( ) \n    if hasattr ( optimizer , 'rho' ) : \n        return optimizer . rho \n    else : \n        if hasattr ( optimizer , 'beta_1' ) : \n            return optimizer . beta_1 \n    return None "}
{"1218": "\ndef process_word ( word : str , to_lower : bool = False , append_case : Optional [ str ] = None ) -> Tuple [ str ] : \n    if all ( x . isupper ( ) for x in word ) and len ( word ) > 1 : \n        uppercase = \"<ALL_UPPER>\" \n    else : \n        if word [ 0 ] . isupper ( ) : \n            uppercase = \"<FIRST_UPPER>\" \n        else : \n            uppercase = None \n    if to_lower : \n        word = word . lower ( ) \n    if word . isdigit ( ) : \n        answer = [ \"<DIGIT>\" ] \n    else : \n        if word . startswith ( \"http://\" ) or word . startswith ( \"www.\" ) : \n            answer = [ \"<HTTP>\" ] \n        else : \n            answer = list ( word ) \n    if to_lower and uppercase is not None : \n        if append_case == \"first\" : \n            answer = [ uppercase ] + answer \n        else : \n            if append_case == \"last\" : \n                answer = answer + [ uppercase ] \n    return tuple ( answer ) "}
{"1220": "\ndef bi_rnn ( units : tf . Tensor , n_hidden : List , cell_type = 'gru' , seq_lengths = None , trainable_initial_states = False , use_peepholes = False , name = 'Bi-' ) : \n    with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : \n        if cell_type == 'gru' : \n            forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) \n                initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        else : \n            if cell_type == 'lstm' : \n                forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n                backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n                if trainable_initial_states : \n                    initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_fw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) \n                    initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_bw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) \n                else : \n                    initial_state_fw = initial_state_bw = None \n            else : \n                raise RuntimeError ( 'cell_type must be either \"gru\" or \"lstm\"s' ) \n        ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths , initial_state_fw = initial_state_fw , initial_state_bw = initial_state_bw ) \n    kernels = [ var for var in forward_cell . trainable_variables + backward_cell . trainable_variables if 'kernel' in var . name ] \n    for kernel in kernels : \n        tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) \n    return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) "}
{"1221": "\ndef stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = False , name = 'RNN_layer' ) : \n    for n , n_hidden in enumerate ( n_hidden_list ) : \n        with tf . variable_scope ( name + '_' + str ( n ) ) : \n            if cell_type == 'gru' : \n                forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n                backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n            else : \n                if cell_type == 'lstm' : \n                    forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n                    backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n                else : \n                    raise RuntimeError ( 'cell_type must be either gru or lstm' ) \n            ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) \n            units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) \n            if cell_type == 'gru' : \n                last_units = tf . concat ( [ fw , bw ] , axis = 1 ) \n            else : \n                ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw \n                c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) \n                h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) \n                last_units = ( h , c ) \n    return units , last_units "}
{"1292": "\ndef download_decompress ( url : str , download_path : [ Path , str ] , extract_paths = None ) : \n    file_name = Path ( urlparse ( url ) . path ) . name \n    download_path = Path ( download_path ) \n    if extract_paths is None : \n        extract_paths = [ download_path ] \n    else : \n        if isinstance ( extract_paths , list ) : \n            extract_paths = [ Path ( path ) for path in extract_paths ] \n        else : \n            extract_paths = [ Path ( extract_paths ) ] \n    cache_dir = os . getenv ( 'DP_CACHE_DIR' ) \n    extracted = False \n    if cache_dir : \n        cache_dir = Path ( cache_dir ) \n        url_hash = md5 ( url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] \n        arch_file_path = cache_dir / url_hash \n        extracted_path = cache_dir / ( url_hash + '_extracted' ) \n        extracted = extracted_path . exists ( ) \n        if not extracted and not arch_file_path . exists ( ) : \n            simple_download ( url , arch_file_path ) \n    else : \n        arch_file_path = download_path / file_name \n        simple_download ( url , arch_file_path ) \n        extracted_path = extract_paths . pop ( ) \n    if not extracted : \n        log . info ( 'Extracting {} archive into {}' . format ( arch_file_path , extracted_path ) ) \n        extracted_path . mkdir ( parents = True , exist_ok = True ) \n        if file_name . endswith ( '.tar.gz' ) : \n            untar ( arch_file_path , extracted_path ) \n        else : \n            if file_name . endswith ( '.gz' ) : \n                ungzip ( arch_file_path , extracted_path / Path ( file_name ) . with_suffix ( '' ) . name ) \n            else : \n                if file_name . endswith ( '.zip' ) : \n                    with zipfile . ZipFile ( arch_file_path , 'r' ) as zip_ref : \n                        zip_ref . extractall ( extracted_path ) \n                else : \n                    raise RuntimeError ( f'Trying to extract an unknown type of archive {file_name}' ) \n        if not cache_dir : \n            arch_file_path . unlink ( ) \n    for extract_path in extract_paths : \n        for src in extracted_path . iterdir ( ) : \n            dest = extract_path / src . name \n            if src . is_dir ( ) : \n                copytree ( src , dest ) \n            else : \n                extract_path . mkdir ( parents = True , exist_ok = True ) \n                shutil . copy ( str ( src ) , str ( dest ) ) "}
{"1304": "\ndef _parse_config_property ( item : _T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> _T : \n    if isinstance ( item , str ) : \n        return item . format ( ** variables ) \n    else : \n        if isinstance ( item , list ) : \n            return [ _parse_config_property ( item , variables ) for item in item ] \n        else : \n            if isinstance ( item , dict ) : \n                return { k : _parse_config_property ( v , variables ) for k , v in item . items ( ) } \n            else : \n                return item "}
{"1307": "\ndef from_params ( params : Dict , mode : str = 'infer' , serialized : Any = None , ** kwargs ) -> Component : \n    config_params = { k : _resolve ( v ) for k , v in params . items ( ) } \n    if 'ref' in config_params : \n        try : \n            component = _refs [ config_params [ 'ref' ] ] \n            if serialized is not None : \n                component . deserialize ( serialized ) \n            return component \n        except KeyError : \n            e = ConfigError ( 'Component with id \"{id}\" was referenced but not initialized' . format ( id = config_params [ 'ref' ] ) ) \n            log . exception ( e ) \n            raise e \n    else : \n        if 'config_path' in config_params : \n            from deeppavlov . core . commands . infer import build_model \n            refs = _refs . copy ( ) \n            _refs . clear ( ) \n            config = parse_config ( expand_path ( config_params [ 'config_path' ] ) ) \n            model = build_model ( config , serialized = serialized ) \n            _refs . clear ( ) \n            _refs . update ( refs ) \n            try : \n                _refs [ config_params [ 'id' ] ] = model \n            except KeyError : \n                pass \n            return model \n    cls_name = config_params . pop ( 'class_name' , None ) \n    if not cls_name : \n        e = ConfigError ( 'Component config has no `class_name` nor `ref` fields' ) \n        log . exception ( e ) \n        raise e \n    cls = get_model ( cls_name ) \n    config_params = { k : _init_param ( v , mode ) for k , v in config_params . items ( ) } \n    try : \n        spec = inspect . getfullargspec ( cls ) \n        if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : \n            kwargs [ 'mode' ] = mode \n        component = cls ( ** dict ( config_params , ** kwargs ) ) \n        try : \n            _refs [ config_params [ 'id' ] ] = component \n        except KeyError : \n            pass \n    except Exception : \n        log . exception ( \"Exception in {}\" . format ( cls ) ) \n        raise \n    if serialized is not None : \n        component . deserialize ( serialized ) \n    return component "}
{"1334": "\ndef _tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10 ) : \n    if not self . is_valid ( ) : \n        self . fill ( rows = rows ) \n    d = collections . OrderedDict ( ) \n    if rollups : \n        col = next ( iter ( viewvalues ( self . _data ) ) ) \n        lrows = len ( col [ 'data' ] ) \n        d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) \n    for k , v in viewitems ( self . _data ) : \n        x = v [ 'data' ] \n        t = v [ \"type\" ] \n        if t == \"enum\" : \n            domain = v [ 'domain' ] \n            x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] \n        else : \n            if t == \"time\" : \n                x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] \n        if rollups : \n            mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None \n            maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None \n            if v [ 'type' ] == \"enum\" : \n                v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None \n            x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x \n        d [ k ] = x \n    return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt ) "}
{"1342": "\ndef _find_function_from_code ( frame , code ) : \n    def find_code ( iterable , depth = 0 ) : \n        if depth > 3 : \n            return \n        for item in iterable : \n            if item is None : \n                continue \n            found = None \n            if hasattr ( item , \"__code__\" ) and item . __code__ == code : \n                found = item \n            else : \n                if isinstance ( item , type ) or isinstance ( item , ModuleType ) : \n                    try : \n                        found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) \n                    except Exception : \n                        continue \n                else : \n                    if isinstance ( item , ( list , tuple , set ) ) : \n                        found = find_code ( item , depth + 1 ) \n                    else : \n                        if isinstance ( item , dict ) : \n                            found = find_code ( item . values ( ) , depth + 1 ) \n            if found : \n                return found \n    return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) ) "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2 ] \n        assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and fr . f_code . co_filename == this_filename : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                for i in range ( fr . f_lineno - 1 ) : \n                    next ( f ) \n                g = tokenize . generate_tokens ( f . readline ) \n                step = 0 \n                args_tokens = [ ] \n                level = 0 \n                for ttt in g : \n                    if step == 0 : \n                        if ttt [ 0 ] != tokenize . NAME : \n                            continue \n                        if not ttt [ 1 ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = 1 \n                    else : \n                        if step == 1 : \n                            assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \"(\" \n                            args_tokens . append ( [ ] ) \n                            step = 2 \n                        else : \n                            if step == 2 : \n                                if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \",\" : \n                                    args_tokens . append ( [ ] ) \n                                else : \n                                    if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \")\" : \n                                        break \n                                    else : \n                                        if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \"([{\" : \n                                            level += 1 \n                                        if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \")]}\" : \n                                            level -= 1 \n                                        assert level >= 0 , \"Parse error: parentheses level became negative\" \n                                        args_tokens [ - 1 ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1370": "\ndef _get_lambda_source_code ( lambda_fn , src ) : \n    def gen_lambdas ( ) : \n        def gen ( ) : \n            yield src + \"\\n\" \n        g = gen ( ) \n        step = 0 \n        tokens = [ ] \n        for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : \n            if step == 0 : \n                if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : \n                    step = 1 \n                    tokens = [ tok ] \n                    level = 0 \n            else : \n                if step == 1 : \n                    if tok [ 0 ] == tokenize . NAME : \n                        tokens . append ( tok ) \n                        step = 2 \n                    else : \n                        step = 0 \n                else : \n                    if step == 2 : \n                        if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : \n                            tokens . append ( tok ) \n                            step = 3 \n                        else : \n                            step = 0 \n                    else : \n                        if step == 3 : \n                            if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : \n                                yield tokenize . untokenize ( tokens ) . strip ( ) \n                                step = 0 \n                            else : \n                                tokens . append ( tok ) \n                                if tok [ 0 ] == tokenize . OP : \n                                    if tok [ 1 ] in \"[({\" : \n                                        level += 1 \n                                    if tok [ 1 ] in \"])}\" : \n                                        level -= 1 \n        assert not tokens \n    actual_code = lambda_fn . __code__ . co_code \n    for lambda_src in gen_lambdas ( ) : \n        try : \n            fn = eval ( lambda_src , globals ( ) , locals ( ) ) \n            if fn . __code__ . co_code == actual_code : \n                return lambda_src . split ( \":\" , 1 ) [ 1 ] . strip ( ) \n        except Exception : \n            pass \n    return \"<lambda>\" "}
{"1392": "\ndef summary ( self , return_data = False ) : \n    if not self . _has_content ( ) : \n        print ( \"This H2OFrame is empty and not initialized.\" ) \n        return self . _ex . _cache . _data ; \n    if not self . _ex . _cache . is_valid ( ) : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    if not return_data : \n        if self . nrows == 0 : \n            print ( \"This H2OFrame is empty.\" ) \n        else : \n            if H2ODisplay . _in_ipy ( ) : \n                import IPython . display \n                IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , True ) , raw = True ) \n            else : \n                print ( self . _ex . _cache . _tabulate ( \"simple\" , True ) ) \n    else : \n        return self . _ex . _cache . _data "}
{"1400": "\ndef rename ( self , columns = None ) : \n    assert_is_type ( columns , None , dict ) \n    new_names = self . names \n    ncols = self . ncols \n    for col , name in columns . items ( ) : \n        col_index = None \n        if is_type ( col , int ) and ( - ncols <= col < ncols ) : \n            col_index = ( col + ncols ) % ncols \n        else : \n            if is_type ( col , str ) and col in self . names : \n                col_index = self . names . index ( col ) \n        if col_index is not None : \n            new_names [ col_index ] = name \n    return self . set_names ( new_names ) "}
{"1402": "\ndef set_name ( self , col = None , name = None ) : \n    assert_is_type ( col , None , int , str ) \n    assert_is_type ( name , str ) \n    ncols = self . ncols \n    col_index = None \n    if is_type ( col , int ) : \n        if not ( - ncols <= col < ncols ) : \n            raise H2OValueError ( \"Index %d is out of bounds for a frame with %d columns\" % ( col , ncols ) ) \n        col_index = ( col + ncols ) % ncols \n    else : \n        if is_type ( col , str ) : \n            if col not in self . names : \n                raise H2OValueError ( \"Column %s doesn't exist in the frame.\" % col ) \n            col_index = self . names . index ( col ) \n        else : \n            assert col is None \n            if ncols != 1 : \n                raise H2OValueError ( \"The frame has %d columns; please specify which one to rename\" % ncols ) \n            col_index = 0 \n    if name != self . names [ col_index ] and name in self . types : \n        raise H2OValueError ( \"Column '%s' already exists in the frame\" % name ) \n    oldname = self . names [ col_index ] \n    old_cache = self . _ex . _cache \n    self . _ex = ExprNode ( \"colnames=\" , self , col_index , name ) \n    self . _ex . _cache . fill_from ( old_cache ) \n    if self . names is None : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    else : \n        self . _ex . _cache . _names = self . names [ : col_index ] + [ name ] + self . names [ col_index + 1 : ] \n        self . _ex . _cache . _types [ name ] = self . _ex . _cache . _types . pop ( oldname ) \n    return "}
{"1413": "\ndef split_frame ( self , ratios = None , destination_frames = None , seed = None ) : \n    assert_is_type ( ratios , [ numeric ] , None ) \n    assert_is_type ( destination_frames , [ str ] , None ) \n    assert_is_type ( seed , int , None ) \n    if ratios is None : \n        ratios = [ 0.75 ] \n    if not ratios : \n        raise ValueError ( \"Ratios array may not be empty\" ) \n    if destination_frames is not None : \n        if len ( ratios ) + 1 != len ( destination_frames ) : \n            raise ValueError ( \"The number of provided destination_frames must be one more \" \"than the number of provided ratios\" ) \n    num_slices = len ( ratios ) + 1 \n    boundaries = [ ] \n    last_boundary = 0 \n    i = 0 \n    while i < num_slices - 1 : \n        ratio = ratios [ i ] \n        if ratio < 0 : \n            raise ValueError ( \"Ratio must be greater than 0\" ) \n        boundary = last_boundary + ratio \n        if boundary >= 1.0 : \n            raise ValueError ( \"Ratios must add up to less than 1.0\" ) \n        boundaries . append ( boundary ) \n        last_boundary = boundary \n        i += 1 \n    splits = [ ] \n    tmp_runif = self . runif ( seed ) \n    tmp_runif . frame_id = \"%s_splitter\" % _py_tmp_key ( h2o . connection ( ) . session_id ) \n    i = 0 \n    while i < num_slices : \n        if i == 0 : \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( tmp_runif <= upper_boundary ) , : ] \n        else : \n            if i == num_slices - 1 : \n                lower_boundary = boundaries [ i - 1 ] \n                tmp_slice = self [ ( tmp_runif > lower_boundary ) , : ] \n            else : \n                lower_boundary = boundaries [ i - 1 ] \n                upper_boundary = boundaries [ i ] \n                tmp_slice = self [ ( ( tmp_runif > lower_boundary ) & ( tmp_runif <= upper_boundary ) ) , : ] \n        if destination_frames is None : \n            splits . append ( tmp_slice ) \n        else : \n            destination_frame_id = destination_frames [ i ] \n            tmp_slice . frame_id = destination_frame_id \n            splits . append ( tmp_slice ) \n        i += 1 \n    del tmp_runif \n    return splits "}
{"1454": "\ndef version_check ( ) : \n    from . __init__ import __version__ as ver_pkg \n    ci = h2oconn . cluster \n    if not ci : \n        raise H2OConnectionError ( \"Connection not initialized. Did you run h2o.connect()?\" ) \n    ver_h2o = ci . version \n    if ver_pkg == \"SUBST_PROJECT_VERSION\" : \n        ver_pkg = \"UNKNOWN\" \n    if str ( ver_h2o ) != str ( ver_pkg ) : \n        branch_name_h2o = ci . branch_name \n        build_number_h2o = ci . build_number \n        if build_number_h2o is None or build_number_h2o == \"unknown\" : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        else : \n            if build_number_h2o == \"99999\" : \n                raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver_h2o , ver_pkg ) ) \n            else : \n                raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) \n    if ci . build_too_old : \n        print ( \"Warning: Your H2O cluster version is too old ({})! Please download and install the latest \" \"version from http://h2o.ai/download/\" . format ( ci . build_age ) ) "}
{"1463": "\ndef get_model ( model_id ) : \n    assert_is_type ( model_id , str ) \n    model_json = api ( \"GET /3/Models/%s\" % model_id ) [ \"models\" ] [ 0 ] \n    algo = model_json [ \"algo\" ] \n    if algo == \"svd\" : \n        m = H2OSVD ( ) \n    else : \n        if algo == \"pca\" : \n            m = H2OPrincipalComponentAnalysisEstimator ( ) \n        else : \n            if algo == \"drf\" : \n                m = H2ORandomForestEstimator ( ) \n            else : \n                if algo == \"naivebayes\" : \n                    m = H2ONaiveBayesEstimator ( ) \n                else : \n                    if algo == \"kmeans\" : \n                        m = H2OKMeansEstimator ( ) \n                    else : \n                        if algo == \"glrm\" : \n                            m = H2OGeneralizedLowRankEstimator ( ) \n                        else : \n                            if algo == \"glm\" : \n                                m = H2OGeneralizedLinearEstimator ( ) \n                            else : \n                                if algo == \"gbm\" : \n                                    m = H2OGradientBoostingEstimator ( ) \n                                else : \n                                    if algo == \"deepwater\" : \n                                        m = H2ODeepWaterEstimator ( ) \n                                    else : \n                                        if algo == \"xgboost\" : \n                                            m = H2OXGBoostEstimator ( ) \n                                        else : \n                                            if algo == \"word2vec\" : \n                                                m = H2OWord2vecEstimator ( ) \n                                            else : \n                                                if algo == \"generic\" : \n                                                    m = H2OGenericEstimator ( ) \n                                                else : \n                                                    if algo == \"deeplearning\" : \n                                                        if model_json [ \"output\" ] [ \"model_category\" ] == \"AutoEncoder\" : \n                                                            m = H2OAutoEncoderEstimator ( ) \n                                                        else : \n                                                            m = H2ODeepLearningEstimator ( ) \n                                                    else : \n                                                        if algo == \"stackedensemble\" : \n                                                            m = H2OStackedEnsembleEstimator ( ) \n                                                        else : \n                                                            if algo == \"isolationforest\" : \n                                                                m = H2OIsolationForestEstimator ( ) \n                                                            else : \n                                                                raise ValueError ( \"Unknown algo type: \" + algo ) \n    m . _resolve_model ( model_id , model_json ) \n    return m "}
{"1493": "\ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) \n    if type == \"barplot\" : \n        plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) \n    else : \n        if type == \"lines\" : \n            plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1501": "\ndef _prepare_data_payload ( data ) : \n    if not data : \n        return None \n    res = { } \n    for key , value in viewitems ( data ) : \n        if value is None : \n            continue \n        if isinstance ( value , list ) : \n            value = stringify_list ( value ) \n        else : \n            if isinstance ( value , dict ) : \n                if \"__meta\" in value and value [ \"__meta\" ] [ \"schema_name\" ] . endswith ( \"KeyV3\" ) : \n                    value = value [ \"name\" ] \n                else : \n                    value = stringify_dict ( value ) \n            else : \n                value = str ( value ) \n        res [ key ] = value \n    return res "}
{"1522": "\ndef save_dict ( ) : \n    global g_test_root_dir \n    global g_output_filename_failed_tests \n    global g_output_filename_passed_tests \n    global g_output_pickle_filename \n    global g_failed_test_info_dict \n    if \"2.build_id\" not in g_failed_test_info_dict . keys ( ) : \n        g_failed_test_info_dict [ \"2.build_id\" ] = \"unknown\" \n    build_id = g_failed_test_info_dict [ \"2.build_id\" ] \n    g_output_filename_failed_tests = g_output_filename_failed_tests + '_build_' + build_id + '_failed_tests.log' \n    g_output_filename_passed_tests = g_output_filename_passed_tests + '_build_' + build_id + '_passed_tests.log' \n    g_output_pickle_filename = g_output_pickle_filename + '_build_' + build_id + '.pickle' \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    with open ( g_output_pickle_filename , 'wb' ) as test_file : \n        pickle . dump ( g_failed_test_info_dict , test_file ) \n    text_file_failed_tests = open ( g_output_filename_failed_tests , 'w' ) \n    text_file_passed_tests = None \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    write_passed_tests = False \n    if ( \"passed_tests_info *********\" in allKeys ) : \n        text_file_passed_tests = open ( g_output_filename_passed_tests , 'w' ) \n        write_passed_tests = True \n    for keyName in allKeys : \n        val = g_failed_test_info_dict [ keyName ] \n        if isinstance ( val , list ) : \n            if ( len ( val ) == 3 ) : \n                if keyName == \"failed_tests_info *********\" : \n                    write_test_java_message ( keyName , val , text_file_failed_tests ) \n                if keyName == \"passed_tests_info *********\" : \n                    write_test_java_message ( keyName , val , text_file_passed_tests ) \n            else : \n                if ( len ( val ) == 2 ) : \n                    write_java_message ( keyName , val , text_file_failed_tests ) \n                    if write_passed_tests : \n                        write_java_message ( keyName , val , text_file_passed_tests ) \n        else : \n            write_general_build_message ( keyName , val , text_file_failed_tests ) \n            if write_passed_tests : \n                write_general_build_message ( keyName , val , text_file_passed_tests ) \n    text_file_failed_tests . close ( ) \n    if write_passed_tests : \n        text_file_passed_tests . close ( ) "}
{"1554": "\ndef _jar_paths ( ) : \n    own_jar = os . getenv ( \"H2O_JAR_PATH\" , \"\" ) \n    if own_jar != \"\" : \n        if not os . path . isfile ( own_jar ) : \n            raise H2OStartupError ( \"Environment variable H2O_JAR_PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own_jar ) \n        yield own_jar \n    cwd_chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) \n    for i in range ( len ( cwd_chunks ) , 0 , - 1 ) : \n        if cwd_chunks [ i - 1 ] == \"h2o-3\" : \n            yield os . path . sep . join ( cwd_chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) \n    backend_dir = os . path . split ( os . path . realpath ( __file__ ) ) [ 0 ] \n    yield os . path . join ( backend_dir , \"bin\" , \"h2o.jar\" ) \n    prefix1 = prefix2 = sys . prefix \n    if prefix1 . startswith ( os . path . sep + \"Library\" ) : \n        prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) \n    else : \n        if prefix1 . startswith ( os . path . sep + \"System\" ) : \n            prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] \n    yield os . path . join ( prefix1 , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix1 , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( get_config_var ( \"userbase\" ) , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix2 , \"h2o_jar\" , \"h2o.jar\" ) "}
{"1557": "\ndef _uri2path ( self , uri ) : \n    if uri == self . package_name : \n        return os . path . join ( self . root_path , '__init__.py' ) \n    path = uri . replace ( '.' , os . path . sep ) \n    path = path . replace ( self . package_name + os . path . sep , '' ) \n    path = os . path . join ( self . root_path , path ) \n    if os . path . exists ( path + '.py' ) : \n        path += '.py' \n    else : \n        if os . path . exists ( os . path . join ( path , '__init__.py' ) ) : \n            path = os . path . join ( path , '__init__.py' ) \n        else : \n            return None \n    return path "}
{"1559": "\ndef _parse_lines ( self , linesource ) : \n    functions = [ ] \n    classes = [ ] \n    for line in linesource : \n        if line . startswith ( 'def ' ) and line . count ( '(' ) : \n            name = self . _get_object_name ( line ) \n            if not name . startswith ( '_' ) : \n                functions . append ( name ) \n        else : \n            if line . startswith ( 'class ' ) : \n                name = self . _get_object_name ( line ) \n                if not name . startswith ( '_' ) : \n                    classes . append ( name ) \n            else : \n                pass \n    functions . sort ( ) \n    classes . sort ( ) \n    return functions , classes "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if len ( argv ) < 2 : \n        usage ( ) \n    i = 1 \n    while ( i < len ( argv ) ) : \n        s = argv [ i ] \n        if ( s == \"--inputfileadd\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        else : \n            if ( s == \"--inputfilerm\" ) : \n                i += 1 \n                if ( i > len ( argv ) ) : \n                    usage ( ) \n                g_old_messages_to_remove = argv [ i ] \n            else : \n                if ( s == \"--loadjavamessage\" ) : \n                    i += 1 \n                    if i > len ( argv ) : \n                        usage ( ) \n                    g_load_java_message_filename = argv [ i ] \n                else : \n                    if ( s == \"--savejavamessage\" ) : \n                        i += 1 \n                        if ( i > len ( argv ) ) : \n                            usage ( ) \n                        g_save_java_message_filename = argv [ i ] \n                    else : \n                        if ( s == '--printjavamessage' ) : \n                            i += 1 \n                            g_print_java_messages = True \n                            g_load_java_message_filename = argv [ i ] \n                        else : \n                            if ( s == '--help' ) : \n                                usage ( ) \n                            else : \n                                unknown_arg ( s ) \n        i += 1 "}
{"1580": "\ndef confusion_matrix ( self , metrics = None , thresholds = None ) : \n    if metrics is None and thresholds is None : \n        metrics = [ 'f1' ] \n    if isinstance ( metrics , list ) : \n        metrics_list = metrics \n    else : \n        if metrics is None : \n            metrics_list = [ ] \n        else : \n            metrics_list = [ metrics ] \n    if isinstance ( thresholds , list ) : \n        thresholds_list = thresholds \n    else : \n        if thresholds is None : \n            thresholds_list = [ ] \n        else : \n            thresholds_list = [ thresholds ] \n    assert_is_type ( thresholds_list , [ numeric ] ) \n    assert_satisfies ( thresholds_list , all ( 0 <= t <= 1 for t in thresholds_list ) ) \n    if not all ( m . lower ( ) in H2OBinomialModelMetrics . max_metrics for m in metrics_list ) : \n        raise ValueError ( \"The only allowable metrics are {}\" , ', ' . join ( H2OBinomialModelMetrics . max_metrics ) ) \n    metrics_thresholds = [ self . find_threshold_by_max_metric ( m ) for m in metrics_list ] \n    for mt in metrics_thresholds : \n        thresholds_list . append ( mt ) \n    first_metrics_thresholds_offset = len ( thresholds_list ) - len ( metrics_thresholds ) \n    thresh2d = self . _metric_json [ 'thresholds_and_metric_scores' ] \n    actual_thresholds = [ float ( e [ 0 ] ) for i , e in enumerate ( thresh2d . cell_values ) ] \n    cms = [ ] \n    for i , t in enumerate ( thresholds_list ) : \n        idx = self . find_idx_by_threshold ( t ) \n        row = thresh2d . cell_values [ idx ] \n        tns = row [ 11 ] \n        fns = row [ 12 ] \n        fps = row [ 13 ] \n        tps = row [ 14 ] \n        p = tps + fns \n        n = tns + fps \n        c0 = n - fps \n        c1 = p - tps \n        if t in metrics_thresholds : \n            m = metrics_list [ i - first_metrics_thresholds_offset ] \n            table_header = \"Confusion Matrix (Act/Pred) for max {} @ threshold = {}\" . format ( m , actual_thresholds [ idx ] ) \n        else : \n            table_header = \"Confusion Matrix (Act/Pred) @ threshold = {}\" . format ( actual_thresholds [ idx ] ) \n        cms . append ( ConfusionMatrix ( cm = [ [ c0 , fps ] , [ c1 , tps ] ] , domains = self . _metric_json [ 'domain' ] , table_header = table_header ) ) \n    if len ( cms ) == 1 : \n        return cms [ 0 ] \n    else : \n        return cms "}
{"1587": "\ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while True : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if login == username . strip ( ) : \n                    return login , password \n        print ( \"Which account do you want to use? (Type number)\" ) \n        for ind , ( login , password ) in enumerate ( lines ) : \n            print ( \"%d: %s\" % ( ind + 1 , login ) ) \n        print ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) \n        print ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if ind == 0 : \n                add_credentials ( ) \n                continue \n            else : \n                if ind == - 1 : \n                    delete_credentials ( ) \n                    check_secret ( ) \n                    continue \n                else : \n                    if 0 <= ind - 1 < len ( lines ) : \n                        return lines [ ind - 1 ] \n        except Exception : \n            print ( \"Wrong input, enter the number of the account to use.\" ) "}
{"1594": "\ndef download ( self , resource_group_name , virtual_wan_name , vpn_sites = None , output_blob_sas_url = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _download_initial ( resource_group_name = resource_group_name , virtual_wan_name = virtual_wan_name , vpn_sites = vpn_sites , output_blob_sas_url = output_blob_sas_url , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1596": "\ndef update_command ( self , resource_group_name , node_name , session , pssession , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _update_command_initial ( resource_group_name = resource_group_name , node_name = node_name , session = session , pssession = pssession , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'PowerShellCommandResults' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1597": "\ndef delete_by_id ( self , application_definition_id , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _delete_by_id_initial ( application_definition_id = application_definition_id , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1598": "\ndef create_or_update_by_id ( self , application_definition_id , parameters , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _create_or_update_by_id_initial ( application_definition_id = application_definition_id , parameters = parameters , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'ApplicationDefinition' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1601": "\ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            print ( 'request:' ) \n            try : \n                print ( request . body ) \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        else : \n            if resp . length > 0 : \n                respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            print ( 'response:' ) \n            try : \n                print ( respbody ) \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if status == 307 : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if status >= 300 : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1602": "\ndef execute_script_actions ( self , resource_group_name , cluster_name , persist_on_success , script_actions = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _execute_script_actions_initial ( resource_group_name = resource_group_name , cluster_name = cluster_name , persist_on_success = persist_on_success , script_actions = script_actions , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1604": "\ndef purge_deleted ( self , vault_name , location , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _purge_deleted_initial ( vault_name = vault_name , location = location , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1629": "\ndef get_children_from_path ( node , * path ) : \n    cur = node \n    for index , child in enumerate ( path ) : \n        if isinstance ( child , _strtype ) : \n            next = _MinidomXmlToObject . get_child_nodes ( cur , child ) \n        else : \n            next = _MinidomXmlToObject . _get_child_nodesNS ( cur , * child ) \n        if index == len ( path ) - 1 : \n            return next \n        else : \n            if not next : \n                break \n        cur = next [ 0 ] \n    return [ ] "}
{"1635": "\ndef replace_content ( self , resource_group_name , automation_account_name , runbook_name , runbook_content , custom_headers = None , raw = False , callback = None , polling = True , ** operation_config ) : \n    raw_result = self . _replace_content_initial ( resource_group_name = resource_group_name , automation_account_name = automation_account_name , runbook_name = runbook_name , runbook_content = runbook_content , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        header_dict = { 'location' : 'str' , } \n        deserialized = self . _deserialize ( 'object' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            client_raw_response . add_headers ( header_dict ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1640": "\ndef import_data ( self , resource_group_name , name , files , format = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _import_data_initial ( resource_group_name = resource_group_name , name = name , files = files , format = format , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1641": "\ndef publish ( self , resource_group_name , automation_account_name , runbook_name , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _publish_initial ( resource_group_name = resource_group_name , automation_account_name = automation_account_name , runbook_name = runbook_name , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            client_raw_response . add_headers ( { 'location' : 'str' , } ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1718": "\ndef create_or_update ( self , resource_group_name , vm_scale_set_name , parameters , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _create_or_update_initial ( resource_group_name = resource_group_name , vm_scale_set_name = vm_scale_set_name , parameters = parameters , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'VirtualMachineScaleSet' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1736": "\ndef wait_for_operation_status ( self , request_id , wait_for_status = 'Succeeded' , timeout = 30 , sleep_interval = 5 , progress_callback = wait_for_operation_status_progress_default_callback , success_callback = wait_for_operation_status_success_default_callback , failure_callback = wait_for_operation_status_failure_default_callback ) : \n    loops = timeout // sleep_interval + 1 \n    start_time = time . time ( ) \n    for _ in range ( int ( loops ) ) : \n        result = self . get_operation_status ( request_id ) \n        elapsed = time . time ( ) - start_time \n        if result . status == wait_for_status : \n            if success_callback is not None : \n                success_callback ( elapsed ) \n            return result \n        else : \n            if result . error : \n                if failure_callback is not None : \n                    ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_FAILURE , result . status , result ) \n                    failure_callback ( elapsed , ex ) \n                return result \n            else : \n                if progress_callback is not None : \n                    progress_callback ( elapsed ) \n                time . sleep ( sleep_interval ) \n    if failure_callback is not None : \n        ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_TIMEOUT , result . status , result ) \n        failure_callback ( elapsed , ex ) \n    return result "}
{"1770": "\nasync def close ( self , exception = None ) : \n    self . running = False \n    if self . error : \n        return \n    if isinstance ( exception , ServiceBusError ) : \n        self . error = exception \n    else : \n        if exception : \n            self . error = ServiceBusError ( str ( exception ) ) \n        else : \n            self . error = ServiceBusError ( \"This message handler is now closed.\" ) \n    await self . _handler . close_async ( ) "}
{"1775": "\ndef merge ( self , reservation_order_id , sources = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _merge_initial ( reservation_order_id = reservation_order_id , sources = sources , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( '[ReservationResponse]' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1777": "\ndef purge ( self , resource_group_name , workspace_name , table , filters , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _purge_initial ( resource_group_name = resource_group_name , workspace_name = workspace_name , table = table , filters = filters , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'object' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1805": "\ndef reset_service_principal_profile ( self , resource_group_name , resource_name , client_id , secret = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _reset_service_principal_profile_initial ( resource_group_name = resource_group_name , resource_name = resource_name , client_id = client_id , secret = secret , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1806": "\ndef delete ( self ) : \n    if self . _queue_name : \n        self . service_bus_service . delete_queue_message ( self . _queue_name , self . broker_properties [ 'SequenceNumber' ] , self . broker_properties [ 'LockToken' ] ) \n    else : \n        if self . _topic_name and self . _subscription_name : \n            self . service_bus_service . delete_subscription_message ( self . _topic_name , self . _subscription_name , self . broker_properties [ 'SequenceNumber' ] , self . broker_properties [ 'LockToken' ] ) \n        else : \n            raise AzureServiceBusPeekLockError ( _ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_DELETE ) "}
{"1807": "\ndef unlock ( self ) : \n    if self . _queue_name : \n        self . service_bus_service . unlock_queue_message ( self . _queue_name , self . broker_properties [ 'SequenceNumber' ] , self . broker_properties [ 'LockToken' ] ) \n    else : \n        if self . _topic_name and self . _subscription_name : \n            self . service_bus_service . unlock_subscription_message ( self . _topic_name , self . _subscription_name , self . broker_properties [ 'SequenceNumber' ] , self . broker_properties [ 'LockToken' ] ) \n        else : \n            raise AzureServiceBusPeekLockError ( _ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_UNLOCK ) "}
{"1808": "\ndef renew_lock ( self ) : \n    if self . _queue_name : \n        self . service_bus_service . renew_lock_queue_message ( self . _queue_name , self . broker_properties [ 'SequenceNumber' ] , self . broker_properties [ 'LockToken' ] ) \n    else : \n        if self . _topic_name and self . _subscription_name : \n            self . service_bus_service . renew_lock_subscription_message ( self . _topic_name , self . _subscription_name , self . broker_properties [ 'SequenceNumber' ] , self . broker_properties [ 'LockToken' ] ) \n        else : \n            raise AzureServiceBusPeekLockError ( _ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_RENEW_LOCK ) "}
{"1817": "\ndef start_web_site_network_trace_operation ( self , resource_group_name , name , duration_in_seconds = None , max_frame_length = None , sas_url = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _start_web_site_network_trace_operation_initial ( resource_group_name = resource_group_name , name = name , duration_in_seconds = duration_in_seconds , max_frame_length = max_frame_length , sas_url = sas_url , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( '[NetworkTrace]' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1819": "\ndef swap_slot_slot ( self , resource_group_name , name , slot , target_slot , preserve_vnet , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _swap_slot_slot_initial ( resource_group_name = resource_group_name , name = name , slot = slot , target_slot = target_slot , preserve_vnet = preserve_vnet , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1823": "\ndef create_and_start_migration ( self , resource_group_name , namespace_name , target_namespace , post_migration_name , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _create_and_start_migration_initial ( resource_group_name = resource_group_name , namespace_name = namespace_name , target_namespace = target_namespace , post_migration_name = post_migration_name , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'MigrationConfigProperties' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1825": "\ndef move_resources ( self , source_resource_group_name , resources = None , target_resource_group = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _move_resources_initial ( source_resource_group_name = source_resource_group_name , resources = resources , target_resource_group = target_resource_group , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1836": "\ndef create ( self , resource_group_name , node_name , session , user_name = None , password = None , retention_period = None , credential_data_format = None , encryption_certificate_thumbprint = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _create_initial ( resource_group_name = resource_group_name , node_name = node_name , session = session , user_name = user_name , password = password , retention_period = retention_period , credential_data_format = credential_data_format , encryption_certificate_thumbprint = encryption_certificate_thumbprint , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'SessionResource' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1837": "\ndef create_subscription ( self , billing_account_name , invoice_section_name , body , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _create_subscription_initial ( billing_account_name = billing_account_name , invoice_section_name = invoice_section_name , body = body , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        header_dict = { 'Location' : 'str' , 'Retry-After' : 'int' , } \n        deserialized = self . _deserialize ( 'SubscriptionCreationResult' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            client_raw_response . add_headers ( header_dict ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1838": "\ndef export_request_rate_by_interval ( self , parameters , location , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _export_request_rate_by_interval_initial ( parameters = parameters , location = location , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'LogAnalyticsOperationResult' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , lro_options = { 'final-state-via' : 'azure-async-operation' } , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1840": "\ndef _bulk_add_tasks ( self , results_queue , chunk_tasks_to_add ) : \n    try : \n        add_collection_response = self . _original_add_collection ( self . _client , self . _job_id , chunk_tasks_to_add , self . _task_add_collection_options , self . _custom_headers , self . _raw ) \n    except BatchErrorException as e : \n        if e . error . code == \"RequestBodyTooLarge\" : \n            if len ( chunk_tasks_to_add ) == 1 : \n                failed_task = chunk_tasks_to_add . pop ( ) \n                self . errors . appendleft ( e ) \n                _LOGGER . error ( \"Failed to add task with ID %s due to the body\" \" exceeding the maximum request size\" , failed_task . id ) \n            else : \n                midpoint = int ( len ( chunk_tasks_to_add ) / 2 ) \n                with self . _max_tasks_lock : \n                    if midpoint < self . _max_tasks_per_request : \n                        self . _max_tasks_per_request = midpoint \n                        _LOGGER . info ( \"Amount of tasks per request reduced from %s to %s due to the\" \" request body being too large\" , str ( self . _max_tasks_per_request ) , str ( midpoint ) ) \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add [ midpoint : ] ) \n                self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add [ : midpoint ] ) \n        else : \n            if 500 <= e . response . status_code <= 599 : \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n            else : \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n                self . errors . appendleft ( e ) \n    except Exception as e : \n        self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        self . errors . appendleft ( e ) \n    else : \n        try : \n            add_collection_response = add_collection_response . output \n        except AttributeError : \n            pass \n        for task_result in add_collection_response . value : \n            if task_result . status == TaskAddStatus . server_error : \n                with self . _pending_queue_lock : \n                    for task in chunk_tasks_to_add : \n                        if task . id == task_result . task_id : \n                            self . tasks_to_add . appendleft ( task ) \n            else : \n                if ( task_result . status == TaskAddStatus . client_error and not task_result . error . code == \"TaskExists\" ) : \n                    self . failure_tasks . appendleft ( task_result ) \n                else : \n                    results_queue . appendleft ( task_result ) "}
{"1843": "\ndef reset_password ( self , user_name , reset_password_payload , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _reset_password_initial ( user_name = user_name , reset_password_payload = reset_password_payload , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1844": "\ndef start_environment ( self , user_name , environment_id , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _start_environment_initial ( user_name = user_name , environment_id = environment_id , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1845": "\ndef _create_message ( response , service_instance ) : \n    respbody = response . body \n    custom_properties = { } \n    broker_properties = None \n    message_type = None \n    message_location = None \n    for name , value in response . headers : \n        if name . lower ( ) == 'brokerproperties' : \n            broker_properties = json . loads ( value ) \n        else : \n            if name . lower ( ) == 'content-type' : \n                message_type = value \n            else : \n                if name . lower ( ) == 'location' : \n                    message_location = value \n                else : \n                    if name . lower ( ) not in [ 'transfer-encoding' , 'server' , 'date' , 'strict-transport-security' ] : \n                        if '\"' in value : \n                            value = value [ 1 : - 1 ] . replace ( '\\\\\"' , '\"' ) \n                            try : \n                                custom_properties [ name ] = datetime . strptime ( value , '%a, %d %b %Y %H:%M:%S GMT' ) \n                            except ValueError : \n                                custom_properties [ name ] = value \n                        else : \n                            if value . lower ( ) == 'true' : \n                                custom_properties [ name ] = True \n                            else : \n                                if value . lower ( ) == 'false' : \n                                    custom_properties [ name ] = False \n                                else : \n                                    try : \n                                        float_value = float ( value ) \n                                        if str ( int ( float_value ) ) == value : \n                                            custom_properties [ name ] = int ( value ) \n                                        else : \n                                            custom_properties [ name ] = float_value \n                                    except ValueError : \n                                        pass \n    if message_type is None : \n        message = Message ( respbody , service_instance , message_location , custom_properties , 'application/atom+xml;type=entry;charset=utf-8' , broker_properties ) \n    else : \n        message = Message ( respbody , service_instance , message_location , custom_properties , message_type , broker_properties ) \n    return message "}
{"1852": "\ndef get_client_from_cli_profile ( client_class , ** kwargs ) : \n    cloud = get_cli_active_cloud ( ) \n    parameters = { } \n    if 'credentials' not in kwargs or 'subscription_id' not in kwargs : \n        resource , _ = _client_resource ( client_class , cloud ) \n        credentials , subscription_id , tenant_id = get_azure_cli_credentials ( resource = resource , with_tenant = True ) \n        parameters . update ( { 'credentials' : kwargs . get ( 'credentials' , credentials ) , 'subscription_id' : kwargs . get ( 'subscription_id' , subscription_id ) } ) \n    args = get_arg_spec ( client_class . __init__ ) . args \n    if 'adla_job_dns_suffix' in args and 'adla_job_dns_suffix' not in kwargs : \n        parameters [ 'adla_job_dns_suffix' ] = cloud . suffixes . azure_datalake_analytics_catalog_and_job_endpoint \n    else : \n        if 'base_url' in args and 'base_url' not in kwargs : \n            _ , base_url = _client_resource ( client_class , cloud ) \n            if base_url : \n                parameters [ 'base_url' ] = base_url \n            else : \n                parameters [ 'base_url' ] = cloud . endpoints . resource_manager \n    if 'tenant_id' in args and 'tenant_id' not in kwargs : \n        parameters [ 'tenant_id' ] = tenant_id \n    parameters . update ( kwargs ) \n    return _instantiate_client ( client_class , ** parameters ) "}
{"1873": "\ndef update_policies ( self , resource_group_name , registry_name , quarantine_policy = None , trust_policy = None , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _update_policies_initial ( resource_group_name = resource_group_name , registry_name = registry_name , quarantine_policy = quarantine_policy , trust_policy = trust_policy , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'RegistryPolicies' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1877": "\ndef complete_restore ( self , location_name , operation_id , last_backup_name , custom_headers = None , raw = False , polling = True , ** operation_config ) : \n    raw_result = self . _complete_restore_initial ( location_name = location_name , operation_id = operation_id , last_backup_name = last_backup_name , custom_headers = custom_headers , raw = True , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is True : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    else : \n        if polling is False : \n            polling_method = NoPolling ( ) \n        else : \n            polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1891": "\ndef segments ( self , ** kwargs ) : \n    segmentBase = self . segmentBase or self . walk_back_get_attr ( \"segmentBase\" ) \n    segmentLists = self . segmentList or self . walk_back_get_attr ( \"segmentList\" ) \n    segmentTemplate = self . segmentTemplate or self . walk_back_get_attr ( \"segmentTemplate\" ) \n    if segmentTemplate : \n        for segment in segmentTemplate . segments ( RepresentationID = self . id , Bandwidth = int ( self . bandwidth * 1000 ) , ** kwargs ) : \n            if segment . init : \n                yield segment \n            else : \n                yield segment \n    else : \n        if segmentLists : \n            for segmentList in segmentLists : \n                for segment in segmentList . segments : \n                    yield segment \n        else : \n            yield Segment ( self . base_url , 0 , True , True ) "}
{"1899": "\ndef parse_xml ( data , name = \"XML\" , ignore_ns = False , exception = PluginError , schema = None , invalid_char_entities = False ) : \n    if is_py2 and isinstance ( data , unicode ) : \n        data = data . encode ( \"utf8\" ) \n    else : \n        if is_py3 and isinstance ( data , str ) : \n            data = bytearray ( data , \"utf8\" ) \n    if ignore_ns : \n        data = re . sub ( br\"[\\t ]xmlns=\\\"(.+?)\\\"\" , b\"\" , data ) \n    if invalid_char_entities : \n        data = re . sub ( br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)' , b'&amp;' , data ) \n    try : \n        tree = ET . fromstring ( data ) \n    except Exception as err : \n        snippet = repr ( data ) \n        if len ( snippet ) > 35 : \n            snippet = snippet [ : 35 ] + \" ...\" \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        tree = schema . validate ( tree , name = name , exception = exception ) \n    return tree "}
{"1901": "\ndef search_dict ( data , key ) : \n    if isinstance ( data , dict ) : \n        for dkey , value in data . items ( ) : \n            if dkey == key : \n                yield value \n            for result in search_dict ( value , key ) : \n                yield result \n    else : \n        if isinstance ( data , list ) : \n            for value in data : \n                for result in search_dict ( value , key ) : \n                    yield result "}
{"1904": "\ndef parse_manifest ( cls , session , url_or_manifest , ** args ) : \n    ret = { } \n    if url_or_manifest . startswith ( '<?xml' ) : \n        mpd = MPD ( parse_xml ( url_or_manifest , ignore_ns = True ) ) \n    else : \n        res = session . http . get ( url_or_manifest , ** args ) \n        url = res . url \n        urlp = list ( urlparse ( url ) ) \n        urlp [ 2 ] , _ = urlp [ 2 ] . rsplit ( \"/\" , 1 ) \n        mpd = MPD ( session . http . xml ( res , ignore_ns = True ) , base_url = urlunparse ( urlp ) , url = url ) \n    video , audio = [ ] , [ ] \n    for aset in mpd . periods [ 0 ] . adaptationSets : \n        if aset . contentProtection : \n            raise PluginError ( \"{} is protected by DRM\" . format ( url ) ) \n        for rep in aset . representations : \n            if rep . mimeType . startswith ( \"video\" ) : \n                video . append ( rep ) \n            else : \n                if rep . mimeType . startswith ( \"audio\" ) : \n                    audio . append ( rep ) \n    if not video : \n        video = [ None ] \n    if not audio : \n        audio = [ None ] \n    locale = session . localization \n    locale_lang = locale . language \n    lang = None \n    available_languages = set ( ) \n    for aud in audio : \n        if aud and aud . lang : \n            available_languages . add ( aud . lang ) \n            try : \n                if locale . explicit and aud . lang and Language . get ( aud . lang ) == locale_lang : \n                    lang = aud . lang \n            except LookupError : \n                continue \n    if not lang : \n        lang = audio [ 0 ] and audio [ 0 ] . lang \n    log . debug ( \"Available languages for DASH audio streams: {0} (using: {1})\" . format ( \", \" . join ( available_languages ) or \"NONE\" , lang or \"n/a\" ) ) \n    if len ( available_languages ) > 1 : \n        audio = list ( filter ( lambda a : a . lang is None or a . lang == lang , audio ) ) \n    for vid , aud in itertools . product ( video , audio ) : \n        stream = DASHStream ( session , mpd , vid , aud , ** args ) \n        stream_name = [ ] \n        if vid : \n            stream_name . append ( \"{:0.0f}{}\" . format ( vid . height or vid . bandwidth_rounded , \"p\" if vid . height else \"k\" ) ) \n        if audio and len ( audio ) > 1 : \n            stream_name . append ( \"a{:0.0f}k\" . format ( aud . bandwidth ) ) \n        ret [ '+' . join ( stream_name ) ] = stream \n    return ret "}
{"1905": "\ndef determine_json_encoding ( cls , sample ) : \n    nulls_at = [ i for i , j in enumerate ( bytearray ( sample [ : 4 ] ) ) if j == 0 ] \n    if nulls_at == [ 0 , 1 , 2 ] : \n        return \"UTF-32BE\" \n    else : \n        if nulls_at == [ 0 , 2 ] : \n            return \"UTF-16BE\" \n        else : \n            if nulls_at == [ 1 , 2 , 3 ] : \n                return \"UTF-32LE\" \n            else : \n                if nulls_at == [ 1 , 3 ] : \n                    return \"UTF-16LE\" \n                else : \n                    return \"UTF-8\" "}
{"1919": "\ndef create_output ( plugin ) : \n    if ( args . output or args . stdout ) and ( args . record or args . record_and_pipe ) : \n        console . exit ( \"Cannot use record options with other file output options.\" ) \n    if args . output : \n        if args . output == \"-\" : \n            out = FileOutput ( fd = stdout ) \n        else : \n            out = check_file_output ( args . output , args . force ) \n    else : \n        if args . stdout : \n            out = FileOutput ( fd = stdout ) \n        else : \n            if args . record_and_pipe : \n                record = check_file_output ( args . record_and_pipe , args . force ) \n                out = FileOutput ( fd = stdout , record = record ) \n            else : \n                http = namedpipe = record = None \n                if not args . player : \n                    console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) \n                if args . player_fifo : \n                    pipename = \"streamlinkpipe-{0}\" . format ( os . getpid ( ) ) \n                    log . info ( \"Creating pipe {0}\" , pipename ) \n                    try : \n                        namedpipe = NamedPipe ( pipename ) \n                    except IOError as err : \n                        console . exit ( \"Failed to create pipe: {0}\" , err ) \n                else : \n                    if args . player_http : \n                        http = create_http_server ( ) \n                title = create_title ( plugin ) \n                if args . record : \n                    record = check_file_output ( args . record , args . force ) \n                log . info ( \"Starting player: {0}\" , args . player ) \n                out = PlayerOutput ( args . player , args = args . player_args , quiet = not args . verbose_player , kill = not args . player_no_close , namedpipe = namedpipe , http = http , record = record , title = title ) \n    return out "}
{"1926": "\ndef read_stream ( stream , output , prebuffer , chunk_size = 8192 ) : \n    is_player = isinstance ( output , PlayerOutput ) \n    is_http = isinstance ( output , HTTPServer ) \n    is_fifo = is_player and output . namedpipe \n    show_progress = isinstance ( output , FileOutput ) and output . fd is not stdout and sys . stdout . isatty ( ) \n    show_record_progress = hasattr ( output , \"record\" ) and isinstance ( output . record , FileOutput ) and output . record . fd is not stdout and sys . stdout . isatty ( ) \n    stream_iterator = chain ( [ prebuffer ] , iter ( partial ( stream . read , chunk_size ) , b\"\" ) ) \n    if show_progress : \n        stream_iterator = progress ( stream_iterator , prefix = os . path . basename ( args . output ) ) \n    else : \n        if show_record_progress : \n            stream_iterator = progress ( stream_iterator , prefix = os . path . basename ( args . record ) ) \n    try : \n        for data in stream_iterator : \n            if is_win32 and is_fifo : \n                output . player . poll ( ) \n                if output . player . returncode is not None : \n                    log . info ( \"Player closed\" ) \n                    break \n            try : \n                output . write ( data ) \n            except IOError as err : \n                if is_player and err . errno in ACCEPTABLE_ERRNO : \n                    log . info ( \"Player closed\" ) \n                else : \n                    if is_http and err . errno in ACCEPTABLE_ERRNO : \n                        log . info ( \"HTTP connection closed\" ) \n                    else : \n                        console . exit ( \"Error when writing to output: {0}, exiting\" , err ) \n                break \n    except IOError as err : \n        console . exit ( \"Error when reading from stream: {0}, exiting\" , err ) \n    finally : \n        stream . close ( ) \n        log . info ( \"Stream ended\" ) "}
{"1927": "\ndef handle_stream ( plugin , streams , stream_name ) : \n    stream_name = resolve_stream_name ( streams , stream_name ) \n    stream = streams [ stream_name ] \n    if args . subprocess_cmdline : \n        if isinstance ( stream , StreamProcess ) : \n            try : \n                cmdline = stream . cmdline ( ) \n            except StreamError as err : \n                console . exit ( \"{0}\" , err ) \n            console . msg ( \"{0}\" , cmdline ) \n        else : \n            console . exit ( \"The stream specified cannot be translated to a command\" ) \n    else : \n        if console . json : \n            console . msg_json ( stream ) \n        else : \n            if args . stream_url : \n                try : \n                    console . msg ( \"{0}\" , stream . to_url ( ) ) \n                except TypeError : \n                    console . exit ( \"The stream specified cannot be translated to a URL\" ) \n            else : \n                alt_streams = list ( filter ( lambda k : stream_name + \"_alt\" in k , sorted ( streams . keys ( ) ) ) ) \n                file_output = args . output or args . stdout \n                for stream_name in [ stream_name ] + alt_streams : \n                    stream = streams [ stream_name ] \n                    stream_type = type ( stream ) . shortname ( ) \n                    if stream_type in args . player_passthrough and not file_output : \n                        log . info ( \"Opening stream: {0} ({1})\" , stream_name , stream_type ) \n                        success = output_stream_passthrough ( plugin , stream ) \n                    else : \n                        if args . player_external_http : \n                            return output_stream_http ( plugin , streams , external = True , port = args . player_external_http_port ) \n                        else : \n                            if args . player_continuous_http and not file_output : \n                                return output_stream_http ( plugin , streams ) \n                            else : \n                                log . info ( \"Opening stream: {0} ({1})\" , stream_name , stream_type ) \n                                success = output_stream ( plugin , stream ) \n                    if success : \n                        break "}
{"1941": "\ndef log_current_versions ( ) : \n    if logger . root . isEnabledFor ( logging . DEBUG ) : \n        if sys . platform == \"darwin\" : \n            os_version = \"macOS {0}\" . format ( platform . mac_ver ( ) [ 0 ] ) \n        else : \n            if sys . platform . startswith ( \"win\" ) : \n                os_version = \"{0} {1}\" . format ( platform . system ( ) , platform . release ( ) ) \n            else : \n                os_version = platform . platform ( ) \n        log . debug ( \"OS:         {0}\" . format ( os_version ) ) \n        log . debug ( \"Python:     {0}\" . format ( platform . python_version ( ) ) ) \n        log . debug ( \"Streamlink: {0}\" . format ( streamlink_version ) ) \n        log . debug ( \"Requests({0}), Socks({1}), Websocket({2})\" . format ( requests . __version__ , socks_version , websocket_version ) ) "}
{"1944": "\ndef set_option ( self , key , value ) : \n    if key == \"rtmpdump\" : \n        key = \"rtmp-rtmpdump\" \n    else : \n        if key == \"rtmpdump-proxy\" : \n            key = \"rtmp-proxy\" \n        else : \n            if key == \"errorlog\" : \n                key = \"subprocess-errorlog\" \n            else : \n                if key == \"errorlog-path\" : \n                    key = \"subprocess-errorlog-path\" \n    if key == \"http-proxy\" : \n        self . http . proxies [ \"http\" ] = update_scheme ( \"http://\" , value ) \n    else : \n        if key == \"https-proxy\" : \n            self . http . proxies [ \"https\" ] = update_scheme ( \"https://\" , value ) \n        else : \n            if key == \"http-cookies\" : \n                if isinstance ( value , dict ) : \n                    self . http . cookies . update ( value ) \n                else : \n                    self . http . parse_cookies ( value ) \n            else : \n                if key == \"http-headers\" : \n                    if isinstance ( value , dict ) : \n                        self . http . headers . update ( value ) \n                    else : \n                        self . http . parse_headers ( value ) \n                else : \n                    if key == \"http-query-params\" : \n                        if isinstance ( value , dict ) : \n                            self . http . params . update ( value ) \n                        else : \n                            self . http . parse_query_params ( value ) \n                    else : \n                        if key == \"http-trust-env\" : \n                            self . http . trust_env = value \n                        else : \n                            if key == \"http-ssl-verify\" : \n                                self . http . verify = value \n                            else : \n                                if key == \"http-disable-dh\" : \n                                    if value : \n                                        requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS += ':!DH' \n                                        try : \n                                            requests . packages . urllib3 . contrib . pyopenssl . DEFAULT_SSL_CIPHER_LIST = requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS . encode ( \"ascii\" ) \n                                        except AttributeError : \n                                            pass \n                                else : \n                                    if key == \"http-ssl-cert\" : \n                                        self . http . cert = value \n                                    else : \n                                        if key == \"http-timeout\" : \n                                            self . http . timeout = value \n                                        else : \n                                            self . options . set ( key , value ) "}
{"1945": "\ndef get_option ( self , key ) : \n    if key == \"rtmpdump\" : \n        key = \"rtmp-rtmpdump\" \n    else : \n        if key == \"rtmpdump-proxy\" : \n            key = \"rtmp-proxy\" \n        else : \n            if key == \"errorlog\" : \n                key = \"subprocess-errorlog\" \n    if key == \"http-proxy\" : \n        return self . http . proxies . get ( \"http\" ) \n    else : \n        if key == \"https-proxy\" : \n            return self . http . proxies . get ( \"https\" ) \n        else : \n            if key == \"http-cookies\" : \n                return self . http . cookies \n            else : \n                if key == \"http-headers\" : \n                    return self . http . headers \n                else : \n                    if key == \"http-query-params\" : \n                        return self . http . params \n                    else : \n                        if key == \"http-trust-env\" : \n                            return self . http . trust_env \n                        else : \n                            if key == \"http-ssl-verify\" : \n                                return self . http . verify \n                            else : \n                                if key == \"http-ssl-cert\" : \n                                    return self . http . cert \n                                else : \n                                    if key == \"http-timeout\" : \n                                        return self . http . timeout \n                                    else : \n                                        return self . options . get ( key ) "}
{"1963": "\ndef dologin ( self , email , password , emailauth = \"\" , emailsteamid = \"\" , captchagid = \"-1\" , captcha_text = \"\" , twofactorcode = \"\" ) : \n    epassword , rsatimestamp = self . encrypt_password ( email , password ) \n    login_data = { 'username' : email , \"password\" : epassword , \"emailauth\" : emailauth , \"loginfriendlyname\" : \"Streamlink\" , \"captchagid\" : captchagid , \"captcha_text\" : captcha_text , \"emailsteamid\" : emailsteamid , \"rsatimestamp\" : rsatimestamp , \"remember_login\" : True , \"donotcache\" : self . donotcache , \"twofactorcode\" : twofactorcode } \n    res = self . session . http . post ( self . _dologin_url , data = login_data ) \n    resp = self . session . http . json ( res , schema = self . _dologin_schema ) \n    if not resp [ u\"success\" ] : \n        if resp . get ( u\"captcha_needed\" ) : \n            captchagid = resp [ u\"captcha_gid\" ] \n            log . error ( \"Captcha result required, open this URL to see the captcha: {}\" . format ( self . _captcha_url . format ( captchagid ) ) ) \n            try : \n                captcha_text = self . input_ask ( \"Captcha text\" ) \n            except FatalPluginError : \n                captcha_text = None \n            if not captcha_text : \n                return False \n        else : \n            if resp . get ( u\"emailauth_needed\" ) : \n                if not emailauth : \n                    try : \n                        emailauth = self . input_ask ( \"Email auth code required\" ) \n                    except FatalPluginError : \n                        emailauth = None \n                    if not emailauth : \n                        return False \n                else : \n                    raise SteamLoginFailed ( \"Email auth key error\" ) \n            if resp . get ( u\"requires_twofactor\" ) : \n                try : \n                    twofactorcode = self . input_ask ( \"Two factor auth code required\" ) \n                except FatalPluginError : \n                    twofactorcode = None \n                if not twofactorcode : \n                    return False \n            if resp . get ( u\"message\" ) : \n                raise SteamLoginFailed ( resp [ u\"message\" ] ) \n        return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u\"emailsteamid\" , u\"\" ) , captcha_text = captcha_text , captchagid = captchagid , twofactorcode = twofactorcode ) \n    else : \n        if resp . get ( \"login_complete\" ) : \n            return True \n        else : \n            log . error ( \"Something when wrong when logging in to Steam\" ) \n            return False "}
{"1971": "\ndef _create_api ( self ) : \n    if self . options . get ( \"purge_credentials\" ) : \n        self . cache . set ( \"session_id\" , None , 0 ) \n        self . cache . set ( \"auth\" , None , 0 ) \n        self . cache . set ( \"session_id\" , None , 0 ) \n    locale = self . get_option ( \"locale\" ) or self . session . localization . language_code \n    api = CrunchyrollAPI ( self . cache , self . session , session_id = self . get_option ( \"session_id\" ) , locale = locale ) \n    if not self . get_option ( \"session_id\" ) : \n        self . logger . debug ( \"Creating session with locale: {0}\" , locale ) \n        api . start_session ( ) \n        if api . auth : \n            self . logger . debug ( \"Using saved credentials\" ) \n            login = api . authenticate ( ) \n            self . logger . info ( \"Successfully logged in as '{0}'\" , login [ \"user\" ] [ \"username\" ] or login [ \"user\" ] [ \"email\" ] ) \n        else : \n            if self . options . get ( \"username\" ) : \n                try : \n                    self . logger . debug ( \"Attempting to login using username and password\" ) \n                    api . login ( self . options . get ( \"username\" ) , self . options . get ( \"password\" ) ) \n                    login = api . authenticate ( ) \n                    self . logger . info ( \"Logged in as '{0}'\" , login [ \"user\" ] [ \"username\" ] or login [ \"user\" ] [ \"email\" ] ) \n                except CrunchyrollAPIError as err : \n                    raise PluginError ( u\"Authentication error: {0}\" . format ( err . msg ) ) \n            else : \n                self . logger . warning ( \"No authentication provided, you won't be able to access \" \"premium restricted content\" ) \n    return api "}
{"1973": "\ndef outputCharFormatter ( c ) : \n    if 32 < c < 127 : \n        return chr ( c ) \n    else : \n        if c == 10 : \n            return '\\\\n' \n        else : \n            if c == 13 : \n                return '\\\\r' \n            else : \n                if c == 32 : \n                    return '\" \"' \n                else : \n                    return '\\\\x{:02x}' . format ( c ) "}
{"1982": "\ndef explanation ( self , index , extra = None ) : \n    extraBits = 0 if extra is None else self . extraBits ( index ) \n    if not hasattr ( self , 'extraTable' ) : \n        formatString = '{0}{3}' \n        lo = hi = value = self . value ( index , extra ) \n    else : \n        if extraBits == 0 : \n            formatString = '{0}{2}: {3}' \n            lo , hi = self . span ( index ) \n            value = lo \n        else : \n            formatString = '{0}{1} {2}: {3}-{4}; {3}+{5}={6}' \n            lo , hi = self . span ( index ) \n            value = lo + extra \n    return formatString . format ( self . description and self . description + ': ' , 'x' * extraBits , self . bitPattern ( index ) , lo , hi , extra , value , ) "}
{"1996": "\ndef readPrefixArray ( self , kind , numberOfTrees ) : \n    prefixes = [ ] \n    for i in range ( numberOfTrees ) : \n        if kind == L : \n            alphabet = LiteralAlphabet ( i ) \n        else : \n            if kind == I : \n                alphabet = InsertAndCopyAlphabet ( i ) \n            else : \n                if kind == D : \n                    alphabet = DistanceAlphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) \n        self . readPrefixCode ( alphabet ) \n        prefixes . append ( alphabet ) \n    self . prefixCodes [ kind ] = prefixes "}
{"2022": "\ndef _task ( self , task , progressbar = False ) : \n    if self . delay : \n        return self . executor . schedule ( task ) \n    else : \n        import vaex . utils \n        callback = None \n        try : \n            if progressbar == True : \n                def update ( fraction ) : \n                    bar . update ( fraction ) \n                    return True \n                bar = vaex . utils . progressbar ( task . name ) \n                callback = self . executor . signal_progress . connect ( update ) \n            else : \n                if progressbar : \n                    callback = self . executor . signal_progress . connect ( progressbar ) \n            result = self . executor . run ( task ) \n            if progressbar == True : \n                bar . finish ( ) \n                sys . stdout . write ( '\\n' ) \n            return result \n        finally : \n            if callback : \n                self . executor . signal_progress . disconnect ( callback ) "}
{"2043": "\ndef dtype ( self , expression , internal = False ) : \n    expression = _ensure_string_from_expression ( expression ) \n    if expression in self . variables : \n        return np . float64 ( 1 ) . dtype \n    else : \n        if expression in self . columns . keys ( ) : \n            column = self . columns [ expression ] \n            data = column [ 0 : 1 ] \n            dtype = data . dtype \n        else : \n            data = self . evaluate ( expression , 0 , 1 , filtered = False ) \n            dtype = data . dtype \n    if not internal : \n        if dtype != str_type : \n            if dtype . kind in 'US' : \n                return str_type \n            if dtype . kind == 'O' : \n                if isinstance ( data [ 0 ] , six . string_types ) : \n                    return str_type \n    return dtype "}
{"2050": "\ndef subspaces ( self , expressions_list = None , dimensions = None , exclude = None , ** kwargs ) : \n    if dimensions is not None : \n        expressions_list = list ( itertools . combinations ( self . get_column_names ( ) , dimensions ) ) \n        if exclude is not None : \n            import six \n            def excluded ( expressions ) : \n                if callable ( exclude ) : \n                    return exclude ( expressions ) \n                else : \n                    if isinstance ( exclude , six . string_types ) : \n                        return exclude in expressions \n                    else : \n                        if isinstance ( exclude , ( list , tuple ) ) : \n                            for e in exclude : \n                                if isinstance ( e , six . string_types ) : \n                                    if e in expressions : \n                                        return True \n                                else : \n                                    if isinstance ( e , ( list , tuple ) ) : \n                                        if set ( e ) . issubset ( expressions ) : \n                                            return True \n                                    else : \n                                        raise ValueError ( \"elements of exclude should contain a string or a sequence of strings\" ) \n                        else : \n                            raise ValueError ( \"exclude should contain a string, a sequence of strings, or should be a callable\" ) \n                return False \n            expressions_list = [ expr for expr in expressions_list if not excluded ( expr ) ] \n        logger . debug ( \"expression list generated: %r\" , expressions_list ) \n    import vaex . legacy \n    return vaex . legacy . Subspaces ( [ self ( * expressions , ** kwargs ) for expressions in expressions_list ] ) "}
{"2081": "\ndef sample ( self , n = None , frac = None , replace = False , weights = None , random_state = None ) : \n    self = self . extract ( ) \n    if type ( random_state ) == int or random_state is None : \n        random_state = np . random . RandomState ( seed = random_state ) \n    if n is None and frac is None : \n        n = 1 \n    else : \n        if frac is not None : \n            n = int ( round ( frac * len ( self ) ) ) \n    weights_values = None \n    if weights is not None : \n        weights_values = self . evaluate ( weights ) \n        weights_values = weights_values / self . sum ( weights ) \n    indices = random_state . choice ( len ( self ) , n , replace = replace , p = weights_values ) \n    return self . take ( indices ) "}
{"2113": "\ndef fillna ( ar , value , fill_nan = True , fill_masked = True ) : \n    ar = ar if not isinstance ( ar , column . Column ) else ar . to_numpy ( ) \n    if ar . dtype . kind in 'O' and fill_nan : \n        strings = ar . astype ( str ) \n        mask = strings == 'nan' \n        ar = ar . copy ( ) \n        ar [ mask ] = value \n    else : \n        if ar . dtype . kind in 'f' and fill_nan : \n            mask = np . isnan ( ar ) \n            if np . any ( mask ) : \n                ar = ar . copy ( ) \n                ar [ mask ] = value \n    if fill_masked and np . ma . isMaskedArray ( ar ) : \n        mask = ar . mask \n        if np . any ( mask ) : \n            ar = ar . data . copy ( ) \n            ar [ mask ] = value \n    return ar "}
{"2150": "\ndef iter_properties ( fh , comments = False ) : \n    for line in _property_lines ( fh ) : \n        key , value = _split_key_value ( line ) \n        if key is not COMMENT : \n            key = _unescape ( key ) \n        else : \n            if not comments : \n                continue \n        yield key , _unescape ( value ) "}
{"2161": "\ndef hz_to_mel ( frequencies , htk = False ) : \n    frequencies = np . asanyarray ( frequencies ) \n    if htk : \n        return 2595.0 * np . log10 ( 1.0 + frequencies / 700.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    mels = ( frequencies - f_min ) / f_sp \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if frequencies . ndim : \n        log_t = ( frequencies >= min_log_hz ) \n        mels [ log_t ] = min_log_mel + np . log ( frequencies [ log_t ] / min_log_hz ) / logstep \n    else : \n        if frequencies >= min_log_hz : \n            mels = min_log_mel + np . log ( frequencies / min_log_hz ) / logstep \n    return mels "}
{"2162": "\ndef mel_to_hz ( mels , htk = False ) : \n    mels = np . asanyarray ( mels ) \n    if htk : \n        return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    freqs = f_min + f_sp * mels \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if mels . ndim : \n        log_t = ( mels >= min_log_mel ) \n        freqs [ log_t ] = min_log_hz * np . exp ( logstep * ( mels [ log_t ] - min_log_mel ) ) \n    else : \n        if mels >= min_log_mel : \n            freqs = min_log_hz * np . exp ( logstep * ( mels - min_log_mel ) ) \n    return freqs "}
{"2180": "\ndef viterbi_discriminative ( prob , transition , p_state = None , p_init = None , return_logp = False ) : \n    n_states , n_steps = prob . shape \n    if transition . shape != ( n_states , n_states ) : \n        raise ParameterError ( 'transition.shape={}, must be ' '(n_states, n_states)={}' . format ( transition . shape , ( n_states , n_states ) ) ) \n    if np . any ( transition < 0 ) or not np . allclose ( transition . sum ( axis = 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid transition matrix: must be non-negative ' 'and sum to 1 on each row.' ) \n    if np . any ( prob < 0 ) or not np . allclose ( prob . sum ( axis = 0 ) , 1 ) : \n        raise ParameterError ( 'Invalid probability values: each column must ' 'sum to 1 and be non-negative' ) \n    states = np . zeros ( n_steps , dtype = int ) \n    values = np . zeros ( ( n_steps , n_states ) , dtype = float ) \n    ptr = np . zeros ( ( n_steps , n_states ) , dtype = int ) \n    epsilon = np . finfo ( prob . dtype ) . tiny \n    if p_state is None : \n        p_state = np . empty ( n_states ) \n        p_state . fill ( 1. / n_states ) \n    else : \n        if p_state . shape != ( n_states , ) : \n            raise ParameterError ( 'Marginal distribution p_state must have shape (n_states,). ' 'Got p_state.shape={}' . format ( p_state . shape ) ) \n        else : \n            if np . any ( p_state < 0 ) or not np . allclose ( p_state . sum ( axis = - 1 ) , 1 ) : \n                raise ParameterError ( 'Invalid marginal state distribution: ' 'p_state={}' . format ( p_state ) ) \n    log_trans = np . log ( transition + epsilon ) \n    log_marginal = np . log ( p_state + epsilon ) \n    log_prob = np . log ( prob . T + epsilon ) - log_marginal \n    if p_init is None : \n        p_init = np . empty ( n_states ) \n        p_init . fill ( 1. / n_states ) \n    else : \n        if np . any ( p_init < 0 ) or not np . allclose ( p_init . sum ( ) , 1 ) : \n            raise ParameterError ( 'Invalid initial state distribution: ' 'p_init={}' . format ( p_init ) ) \n    log_p_init = np . log ( p_init + epsilon ) \n    _viterbi ( log_prob , log_trans , log_p_init , states , values , ptr ) \n    if return_logp : \n        return states , values [ - 1 , states [ - 1 ] ] \n    return states "}
{"2185": "\ndef onset_detect ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , backtrack = False , energy = None , units = 'frames' , ** kwargs ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) \n    onset_envelope -= onset_envelope . min ( ) \n    if not onset_envelope . any ( ) : \n        return np . array ( [ ] , dtype = np . int ) \n    onset_envelope /= onset_envelope . max ( ) \n    kwargs . setdefault ( 'pre_max' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'post_max' , 0.00 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'pre_avg' , 0.10 * sr // hop_length ) \n    kwargs . setdefault ( 'post_avg' , 0.10 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'wait' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'delta' , 0.07 ) \n    onsets = util . peak_pick ( onset_envelope , ** kwargs ) \n    if backtrack : \n        if energy is None : \n            energy = onset_envelope \n        onsets = onset_backtrack ( onsets , energy ) \n    if units == 'frames' : \n        pass \n    else : \n        if units == 'samples' : \n            onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) \n        else : \n            if units == 'time' : \n                onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) \n            else : \n                raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return onsets "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = False , center = True , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if lag < 1 or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if max_size < 1 or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048 ) \n    hop_length = kwargs . get ( 'hop_length' , 512 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if max_size == 1 : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) \n    else : \n        if ref . shape != S . shape : \n            raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = True \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = False \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = 0 ) \n    pad_width = lag \n    if center : \n        pad_width += n_fft // ( 2 * hop_length ) \n    onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - 1 ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ 1 ] ] \n    return onset_env "}
{"2192": "\ndef waveplot ( y , sr = 22050 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , max_sr = 1000 , ax = None , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( isinstance ( max_sr , int ) and max_sr > 0 ) : \n        raise ParameterError ( 'max_sr must be a non-negative integer' ) \n    target_sr = sr \n    hop_length = 1 \n    if max_points is not None : \n        if max_points <= 0 : \n            raise ParameterError ( 'max_points must be strictly positive' ) \n        if max_points < y . shape [ - 1 ] : \n            target_sr = min ( max_sr , ( sr * y . shape [ - 1 ] ) // max_points ) \n        hop_length = sr // target_sr \n        if y . ndim == 1 : \n            y = __envelope ( y , hop_length ) \n        else : \n            y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) \n    if y . ndim > 1 : \n        y_top = y [ 0 ] \n        y_bottom = - y [ 1 ] \n    else : \n        y_top = y \n        y_bottom = - y \n    axes = __check_axes ( ax ) \n    kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) \n    locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , sr = sr , hop_length = hop_length ) \n    out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) \n    axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) \n    if x_axis == 'time' : \n        axes . xaxis . set_major_formatter ( TimeFormatter ( lag = False ) ) \n        axes . xaxis . set_label_text ( 'Time' ) \n    else : \n        if x_axis is None or x_axis in [ 'off' , 'none' ] : \n            axes . set_xticks ( [ ] ) \n        else : \n            raise ParameterError ( 'Unknown x_axis value: {}' . format ( x_axis ) ) \n    return out "}
{"2195": "\ndef __check_axes ( axes ) : \n    if axes is None : \n        import matplotlib . pyplot as plt \n        axes = plt . gca ( ) \n    else : \n        if not isinstance ( axes , Axes ) : \n            raise ValueError ( \"`axes` must be an instance of matplotlib.axes.Axes. \" \"Found type(axes)={}\" . format ( type ( axes ) ) ) \n    return axes "}
{"2196": "\ndef __scale_axes ( axes , ax_type , which ) : \n    kwargs = dict ( ) \n    if which == 'x' : \n        thresh = 'linthreshx' \n        base = 'basex' \n        scale = 'linscalex' \n        scaler = axes . set_xscale \n        limit = axes . set_xlim \n    else : \n        thresh = 'linthreshy' \n        base = 'basey' \n        scale = 'linscaley' \n        scaler = axes . set_yscale \n        limit = axes . set_ylim \n    if ax_type == 'mel' : \n        mode = 'symlog' \n        kwargs [ thresh ] = 1000.0 \n        kwargs [ base ] = 2 \n    else : \n        if ax_type == 'log' : \n            mode = 'symlog' \n            kwargs [ base ] = 2 \n            kwargs [ thresh ] = core . note_to_hz ( 'C2' ) \n            kwargs [ scale ] = 0.5 \n        else : \n            if ax_type in [ 'cqt' , 'cqt_hz' , 'cqt_note' ] : \n                mode = 'log' \n                kwargs [ base ] = 2 \n            else : \n                if ax_type == 'tempo' : \n                    mode = 'log' \n                    kwargs [ base ] = 2 \n                    limit ( 16 , 480 ) \n                else : \n                    return \n    scaler ( mode , ** kwargs ) "}
{"2218": "\ndef nn_filter ( S , rec = None , aggregate = None , axis = - 1 , ** kwargs ) : \n    if aggregate is None : \n        aggregate = np . mean \n    if rec is None : \n        kwargs = dict ( kwargs ) \n        kwargs [ 'sparse' ] = True \n        rec = segment . recurrence_matrix ( S , axis = axis , ** kwargs ) \n    else : \n        if not scipy . sparse . issparse ( rec ) : \n            rec = scipy . sparse . csr_matrix ( rec ) \n    if rec . shape [ 0 ] != S . shape [ axis ] or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'Invalid self-similarity matrix shape ' 'rec.shape={} for S.shape={}' . format ( rec . shape , S . shape ) ) \n    return __nn_filter_helper ( rec . data , rec . indices , rec . indptr , S . swapaxes ( 0 , axis ) , aggregate ) . swapaxes ( 0 , axis ) "}
{"2227": "\ndef get_window ( window , Nx , fftbins = True ) : \n    if six . callable ( window ) : \n        return window ( Nx ) \n    else : \n        if ( isinstance ( window , ( six . string_types , tuple ) ) or np . isscalar ( window ) ) : \n            return scipy . signal . get_window ( window , Nx , fftbins = fftbins ) \n        else : \n            if isinstance ( window , ( np . ndarray , list ) ) : \n                if len ( window ) == Nx : \n                    return np . asarray ( window ) \n                raise ParameterError ( 'Window size mismatch: ' '{:d} != {:d}' . format ( len ( window ) , Nx ) ) \n            else : \n                raise ParameterError ( 'Invalid window specification: {}' . format ( window ) ) "}
{"2233": "\ndef spectral_centroid ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , freq = None , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with real-valued input' ) \n    else : \n        if np . any ( S < 0 ) : \n            raise ParameterError ( 'Spectral centroid is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    return np . sum ( freq * util . normalize ( S , norm = 1 , axis = 0 ) , axis = 0 , keepdims = True ) "}
{"2234": "\ndef spectral_rolloff ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , freq = None , roll_percent = 0.85 ) : \n    if not 0.0 < roll_percent < 1.0 : \n        raise ParameterError ( 'roll_percent must lie in the range (0, 1)' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with real-valued input' ) \n    else : \n        if np . any ( S < 0 ) : \n            raise ParameterError ( 'Spectral rolloff is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    total_energy = np . cumsum ( S , axis = 0 ) \n    threshold = roll_percent * total_energy [ - 1 ] \n    ind = np . where ( total_energy < threshold , np . nan , 1 ) \n    return np . nanmin ( ind * freq , axis = 0 , keepdims = True ) "}
{"2235": "\ndef spectral_flatness ( y = None , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , amin = 1e-10 , power = 2.0 ) : \n    if amin <= 0 : \n        raise ParameterError ( 'amin must be strictly positive' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 1. , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with real-valued input' ) \n    else : \n        if np . any ( S < 0 ) : \n            raise ParameterError ( 'Spectral flatness is only defined ' 'with non-negative energies' ) \n    S_thresh = np . maximum ( amin , S ** power ) \n    gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = 0 , keepdims = True ) ) \n    amean = np . mean ( S_thresh , axis = 0 , keepdims = True ) \n    return gmean / amean "}
{"2243": "\ndef __match_intervals ( intervals_from , intervals_to , strict = True ) : \n    start_index = np . argsort ( intervals_to [ : , 0 ] ) \n    end_index = np . argsort ( intervals_to [ : , 1 ] ) \n    start_sorted = intervals_to [ start_index , 0 ] \n    end_sorted = intervals_to [ end_index , 1 ] \n    search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) \n    search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) \n    output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) \n    for i in range ( len ( intervals_from ) ) : \n        query = intervals_from [ i ] \n        after_query = search_ends [ i ] \n        before_query = search_starts [ i ] \n        candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) \n        if len ( candidates ) > 0 : \n            output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) \n        else : \n            if strict : \n                raise ParameterError \n            else : \n                dist_before = np . inf \n                dist_after = np . inf \n                if search_starts [ i ] > 0 : \n                    dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] \n                if search_ends [ i ] + 1 < len ( intervals_to ) : \n                    dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] \n                if dist_before < dist_after : \n                    output [ i ] = end_index [ search_starts [ i ] - 1 ] \n                else : \n                    output [ i ] = start_index [ search_ends [ i ] + 1 ] \n    return output "}
{"2247": "\ndef interp_harmonics ( x , freqs , h_range , kind = 'linear' , fill_value = 0 , axis = 0 ) : \n    out_shape = [ len ( h_range ) ] \n    out_shape . extend ( x . shape ) \n    x_out = np . zeros ( out_shape , dtype = x . dtype ) \n    if freqs . ndim == 1 and len ( freqs ) == x . shape [ axis ] : \n        harmonics_1d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    else : \n        if freqs . ndim == 2 and freqs . shape == x . shape : \n            harmonics_2d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n        else : \n            raise ParameterError ( 'freqs.shape={} does not match ' 'input shape={}' . format ( freqs . shape , x . shape ) ) \n    return x_out "}
{"2253": "\ndef resample ( y , orig_sr , target_sr , res_type = 'kaiser_best' , fix = True , scale = False , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if orig_sr == target_sr : \n        return y \n    ratio = float ( target_sr ) / orig_sr \n    n_samples = int ( np . ceil ( y . shape [ - 1 ] * ratio ) ) \n    if res_type in ( 'scipy' , 'fft' ) : \n        y_hat = scipy . signal . resample ( y , n_samples , axis = - 1 ) \n    else : \n        if res_type == 'polyphase' : \n            if int ( orig_sr ) != orig_sr or int ( target_sr ) != target_sr : \n                raise ParameterError ( 'polyphase resampling is only supported for integer-valued sampling rates.' ) \n            orig_sr = int ( orig_sr ) \n            target_sr = int ( target_sr ) \n            gcd = np . gcd ( orig_sr , target_sr ) \n            y_hat = scipy . signal . resample_poly ( y , target_sr // gcd , orig_sr // gcd , axis = - 1 ) \n        else : \n            y_hat = resampy . resample ( y , orig_sr , target_sr , filter = res_type , axis = - 1 ) \n    if fix : \n        y_hat = util . fix_length ( y_hat , n_samples , ** kwargs ) \n    if scale : \n        y_hat /= np . sqrt ( ratio ) \n    return np . ascontiguousarray ( y_hat , dtype = y . dtype ) "}
{"2263": "\ndef beat_track ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , start_bpm = 120.0 , tightness = 100 , trim = True , bpm = None , units = 'frames' ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset . onset_strength ( y = y , sr = sr , hop_length = hop_length , aggregate = np . median ) \n    if not onset_envelope . any ( ) : \n        return ( 0 , np . array ( [ ] , dtype = int ) ) \n    if bpm is None : \n        bpm = tempo ( onset_envelope = onset_envelope , sr = sr , hop_length = hop_length , start_bpm = start_bpm ) [ 0 ] \n    beats = __beat_tracker ( onset_envelope , bpm , float ( sr ) / hop_length , tightness , trim ) \n    if units == 'frames' : \n        pass \n    else : \n        if units == 'samples' : \n            beats = core . frames_to_samples ( beats , hop_length = hop_length ) \n        else : \n            if units == 'time' : \n                beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) \n            else : \n                raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return ( bpm , beats ) "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = True , axis = - 1 ) : \n    axis = np . abs ( axis ) \n    if rec . ndim != 2 or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = 1 - axis \n        lag_format = rec . format \n        if axis == 0 : \n            rec = rec . tocsc ( ) \n        else : \n            if axis in ( - 1 , 1 ) : \n                rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    else : \n        if pad : \n            padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] \n            padding [ ( 1 - axis ) ] = ( 0 , t ) \n            lag = np . pad ( rec , padding , mode = 'constant' ) \n        else : \n            lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2273": "\ndef path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7 , zero_mean = False , clip = True , ** kwargs ) : \n    if min_ratio is None : \n        min_ratio = 1. / max_ratio \n    else : \n        if min_ratio > max_ratio : \n            raise ParameterError ( 'min_ratio={} cannot exceed max_ratio={}' . format ( min_ratio , max_ratio ) ) \n    R_smooth = None \n    for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2 ) : \n        kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) \n        if R_smooth is None : \n            R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) \n        else : \n            np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , out = R_smooth ) \n    if clip : \n        np . clip ( R_smooth , 0 , None , out = R_smooth ) \n    return R_smooth "}
{"2276": "\ndef valid_audio ( y , mono = True ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and y . ndim != 1 : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    else : \n        if y . ndim > 2 or y . ndim == 0 : \n            raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return True "}
{"2278": "\ndef fix_length ( data , size , axis = - 1 , ** kwargs ) : \n    kwargs . setdefault ( 'mode' , 'constant' ) \n    n = data . shape [ axis ] \n    if n > size : \n        slices = [ slice ( None ) ] * data . ndim \n        slices [ axis ] = slice ( 0 , size ) \n        return data [ tuple ( slices ) ] \n    else : \n        if n < size : \n            lengths = [ ( 0 , 0 ) ] * data . ndim \n            lengths [ axis ] = ( 0 , size - n ) \n            return np . pad ( data , lengths , ** kwargs ) \n    return data "}
{"2280": "\ndef normalize ( S , norm = np . inf , axis = 0 , threshold = None , fill = None ) : \n    if threshold is None : \n        threshold = tiny ( S ) \n    else : \n        if threshold <= 0 : \n            raise ParameterError ( 'threshold={} must be strictly ' 'positive' . format ( threshold ) ) \n    if fill not in [ None , False , True ] : \n        raise ParameterError ( 'fill={} must be None or boolean' . format ( fill ) ) \n    if not np . all ( np . isfinite ( S ) ) : \n        raise ParameterError ( 'Input must be finite' ) \n    mag = np . abs ( S ) . astype ( np . float ) \n    fill_norm = 1 \n    if norm == np . inf : \n        length = np . max ( mag , axis = axis , keepdims = True ) \n    else : \n        if norm == - np . inf : \n            length = np . min ( mag , axis = axis , keepdims = True ) \n        else : \n            if norm == 0 : \n                if fill is True : \n                    raise ParameterError ( 'Cannot normalize with norm=0 and fill=True' ) \n                length = np . sum ( mag > 0 , axis = axis , keepdims = True , dtype = mag . dtype ) \n            else : \n                if np . issubdtype ( type ( norm ) , np . number ) and norm > 0 : \n                    length = np . sum ( mag ** norm , axis = axis , keepdims = True ) ** ( 1. / norm ) \n                    if axis is None : \n                        fill_norm = mag . size ** ( - 1. / norm ) \n                    else : \n                        fill_norm = mag . shape [ axis ] ** ( - 1. / norm ) \n                else : \n                    if norm is None : \n                        return S \n                    else : \n                        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    small_idx = length < threshold \n    Snorm = np . empty_like ( S ) \n    if fill is None : \n        length [ small_idx ] = 1.0 \n        Snorm [ : ] = S / length \n    else : \n        if fill : \n            length [ small_idx ] = np . nan \n            Snorm [ : ] = S / length \n            Snorm [ np . isnan ( Snorm ) ] = fill_norm \n        else : \n            length [ small_idx ] = np . inf \n            Snorm [ : ] = S / length \n    return Snorm "}
{"2283": "\ndef sparsify_rows ( x , quantile = 0.01 ) : \n    if x . ndim == 1 : \n        x = x . reshape ( ( 1 , - 1 ) ) \n    else : \n        if x . ndim > 2 : \n            raise ParameterError ( 'Input must have 2 or fewer dimensions. ' 'Provided x.shape={}.' . format ( x . shape ) ) \n    if not 0.0 <= quantile < 1 : \n        raise ParameterError ( 'Invalid quantile {:.2f}' . format ( quantile ) ) \n    x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    mags = np . abs ( x ) \n    norms = np . sum ( mags , axis = 1 , keepdims = True ) \n    mag_sort = np . sort ( mags , axis = 1 ) \n    cumulative_mag = np . cumsum ( mag_sort / norms , axis = 1 ) \n    threshold_idx = np . argmin ( cumulative_mag < quantile , axis = 1 ) \n    for i , j in enumerate ( threshold_idx ) : \n        idx = np . where ( mags [ i ] >= mag_sort [ i , j ] ) \n        x_sparse [ i , idx ] = x [ i , idx ] \n    return x_sparse . tocsr ( ) "}
{"2284": "\ndef roll_sparse ( x , shift , axis = 0 ) : \n    if not scipy . sparse . isspmatrix ( x ) : \n        return np . roll ( x , shift , axis = axis ) \n    if axis not in [ 0 , 1 , - 1 ] : \n        raise ParameterError ( 'axis must be one of (0, 1, -1)' ) \n    shift = np . mod ( shift , x . shape [ axis ] ) \n    if shift == 0 : \n        return x . copy ( ) \n    fmt = x . format \n    if axis == 0 : \n        x = x . tocsc ( ) \n    else : \n        if axis in ( - 1 , 1 ) : \n            x = x . tocsr ( ) \n    x_r = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    idx_in = [ slice ( None ) ] * x . ndim \n    idx_out = [ slice ( None ) ] * x_r . ndim \n    idx_in [ axis ] = slice ( 0 , - shift ) \n    idx_out [ axis ] = slice ( shift , None ) \n    x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] \n    idx_out [ axis ] = slice ( 0 , shift ) \n    idx_in [ axis ] = slice ( - shift , None ) \n    x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] \n    return x_r . asformat ( fmt ) "}
{"2287": "\ndef sync ( data , idx , aggregate = None , pad = True , axis = - 1 ) : \n    if aggregate is None : \n        aggregate = np . mean \n    shape = list ( data . shape ) \n    if np . all ( [ isinstance ( _ , slice ) for _ in idx ] ) : \n        slices = idx \n    else : \n        if np . all ( [ np . issubdtype ( type ( _ ) , np . integer ) for _ in idx ] ) : \n            slices = index_to_slice ( np . asarray ( idx ) , 0 , shape [ axis ] , pad = pad ) \n        else : \n            raise ParameterError ( 'Invalid index set: {}' . format ( idx ) ) \n    agg_shape = list ( shape ) \n    agg_shape [ axis ] = len ( slices ) \n    data_agg = np . empty ( agg_shape , order = 'F' if np . isfortran ( data ) else 'C' , dtype = data . dtype ) \n    idx_in = [ slice ( None ) ] * data . ndim \n    idx_agg = [ slice ( None ) ] * data_agg . ndim \n    for ( i , segment ) in enumerate ( slices ) : \n        idx_in [ axis ] = segment \n        idx_agg [ axis ] = i \n        data_agg [ tuple ( idx_agg ) ] = aggregate ( data [ tuple ( idx_in ) ] , axis = axis ) \n    return data_agg "}
{"2294": "\ndef track_progress ( func , tasks , bar_width = 50 , ** kwargs ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    else : \n        if isinstance ( tasks , collections_abc . Iterable ) : \n            task_num = len ( tasks ) \n        else : \n            raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    prog_bar = ProgressBar ( task_num , bar_width ) \n    results = [ ] \n    for task in tasks : \n        results . append ( func ( task , ** kwargs ) ) \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    return results "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50 , chunksize = 1 , skip_first = False , keep_order = True ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    else : \n        if isinstance ( tasks , collections_abc . Iterable ) : \n            task_num = len ( tasks ) \n        else : \n            raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num -= nproc * chunksize * int ( skip_first ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if len ( results ) < nproc * chunksize : \n                continue \n            else : \n                if len ( results ) == nproc * chunksize : \n                    prog_bar . start ( ) \n                    continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2306": "\ndef imrescale ( img , scale , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    if isinstance ( scale , ( float , int ) ) : \n        if scale <= 0 : \n            raise ValueError ( 'Invalid scale {}, must be positive.' . format ( scale ) ) \n        scale_factor = scale \n    else : \n        if isinstance ( scale , tuple ) : \n            max_long_edge = max ( scale ) \n            max_short_edge = min ( scale ) \n            scale_factor = min ( max_long_edge / max ( h , w ) , max_short_edge / min ( h , w ) ) \n        else : \n            raise TypeError ( 'Scale must be a number or tuple of int, but got {}' . format ( type ( scale ) ) ) \n    new_size = _scale_size ( ( w , h ) , scale_factor ) \n    rescaled_img = imresize ( img , new_size , interpolation = interpolation ) \n    if return_scale : \n        return rescaled_img , scale_factor \n    else : \n        return rescaled_img "}
{"2308": "\ndef get_priority ( priority ) : \n    if isinstance ( priority , int ) : \n        if priority < 0 or priority > 100 : \n            raise ValueError ( 'priority must be between 0 and 100' ) \n        return priority \n    else : \n        if isinstance ( priority , Priority ) : \n            return priority . value \n        else : \n            if isinstance ( priority , str ) : \n                return Priority [ priority . upper ( ) ] . value \n            else : \n                raise TypeError ( 'priority must be an integer or Priority enum value' ) "}
{"2312": "\ndef flowread ( flow_or_path , quantize = False , concat_axis = 0 , * args , ** kwargs ) : \n    if isinstance ( flow_or_path , np . ndarray ) : \n        if ( flow_or_path . ndim != 3 ) or ( flow_or_path . shape [ - 1 ] != 2 ) : \n            raise ValueError ( 'Invalid flow with shape {}' . format ( flow_or_path . shape ) ) \n        return flow_or_path \n    else : \n        if not is_str ( flow_or_path ) : \n            raise TypeError ( '\"flow_or_path\" must be a filename or numpy array, not {}' . format ( type ( flow_or_path ) ) ) \n    if not quantize : \n        with open ( flow_or_path , 'rb' ) as f : \n            try : \n                header = f . read ( 4 ) . decode ( 'utf-8' ) \n            except Exception : \n                raise IOError ( 'Invalid flow file: {}' . format ( flow_or_path ) ) \n            else : \n                if header != 'PIEH' : \n                    raise IOError ( 'Invalid flow file: {}, header does not contain PIEH' . format ( flow_or_path ) ) \n            w = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            h = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            flow = np . fromfile ( f , np . float32 , w * h * 2 ) . reshape ( ( h , w , 2 ) ) \n    else : \n        assert concat_axis in [ 0 , 1 ] \n        cat_flow = imread ( flow_or_path , flag = 'unchanged' ) \n        if cat_flow . ndim != 2 : \n            raise IOError ( '{} is not a valid quantized flow file, its dimension is {}.' . format ( flow_or_path , cat_flow . ndim ) ) \n        assert cat_flow . shape [ concat_axis ] % 2 == 0 \n        dx , dy = np . split ( cat_flow , 2 , axis = concat_axis ) \n        flow = dequantize_flow ( dx , dy , * args , ** kwargs ) \n    return flow . astype ( np . float32 ) "}
{"2315": "\ndef load_state_dict ( module , state_dict , strict = False , logger = None ) : \n    unexpected_keys = [ ] \n    own_state = module . state_dict ( ) \n    for name , param in state_dict . items ( ) : \n        if name not in own_state : \n            unexpected_keys . append ( name ) \n            continue \n        if isinstance ( param , torch . nn . Parameter ) : \n            param = param . data \n        try : \n            own_state [ name ] . copy_ ( param ) \n        except Exception : \n            raise RuntimeError ( 'While copying the parameter named {}, ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {}.' . format ( name , own_state [ name ] . size ( ) , param . size ( ) ) ) \n    missing_keys = set ( own_state . keys ( ) ) - set ( state_dict . keys ( ) ) \n    err_msg = [ ] \n    if unexpected_keys : \n        err_msg . append ( 'unexpected key in source state_dict: {}\\n' . format ( ', ' . join ( unexpected_keys ) ) ) \n    if missing_keys : \n        err_msg . append ( 'missing keys in source state_dict: {}\\n' . format ( ', ' . join ( missing_keys ) ) ) \n    err_msg = '\\n' . join ( err_msg ) \n    if err_msg : \n        if strict : \n            raise RuntimeError ( err_msg ) \n        else : \n            if logger is not None : \n                logger . warn ( err_msg ) \n            else : \n                print ( err_msg ) "}
{"2316": "\ndef load_checkpoint ( model , filename , map_location = None , strict = False , logger = None ) : \n    if filename . startswith ( 'modelzoo://' ) : \n        import torchvision \n        model_urls = dict ( ) \n        for _ , name , ispkg in pkgutil . walk_packages ( torchvision . models . __path__ ) : \n            if not ispkg : \n                _zoo = import_module ( 'torchvision.models.{}' . format ( name ) ) \n                _urls = getattr ( _zoo , 'model_urls' ) \n                model_urls . update ( _urls ) \n        model_name = filename [ 11 : ] \n        checkpoint = model_zoo . load_url ( model_urls [ model_name ] ) \n    else : \n        if filename . startswith ( 'open-mmlab://' ) : \n            model_name = filename [ 13 : ] \n            checkpoint = model_zoo . load_url ( open_mmlab_model_urls [ model_name ] ) \n        else : \n            if filename . startswith ( ( 'http://' , 'https://' ) ) : \n                checkpoint = model_zoo . load_url ( filename ) \n            else : \n                if not osp . isfile ( filename ) : \n                    raise IOError ( '{} is not a checkpoint file' . format ( filename ) ) \n                checkpoint = torch . load ( filename , map_location = map_location ) \n    if isinstance ( checkpoint , OrderedDict ) : \n        state_dict = checkpoint \n    else : \n        if isinstance ( checkpoint , dict ) and 'state_dict' in checkpoint : \n            state_dict = checkpoint [ 'state_dict' ] \n        else : \n            raise RuntimeError ( 'No state_dict found in checkpoint file {}' . format ( filename ) ) \n    if list ( state_dict . keys ( ) ) [ 0 ] . startswith ( 'module.' ) : \n        state_dict = { k [ 7 : ] : v for k , v in checkpoint [ 'state_dict' ] . items ( ) } \n    if hasattr ( model , 'module' ) : \n        load_state_dict ( model . module , state_dict , strict , logger ) \n    else : \n        load_state_dict ( model , state_dict , strict , logger ) \n    return checkpoint "}
{"2318": "\ndef save_checkpoint ( model , filename , optimizer = None , meta = None ) : \n    if meta is None : \n        meta = { } \n    else : \n        if not isinstance ( meta , dict ) : \n            raise TypeError ( 'meta must be a dict or None, but got {}' . format ( type ( meta ) ) ) \n    meta . update ( mmcv_version = mmcv . __version__ , time = time . asctime ( ) ) \n    mmcv . mkdir_or_exist ( osp . dirname ( filename ) ) \n    if hasattr ( model , 'module' ) : \n        model = model . module \n    checkpoint = { 'meta' : meta , 'state_dict' : weights_to_cpu ( model . state_dict ( ) ) } \n    if optimizer is not None : \n        checkpoint [ 'optimizer' ] = optimizer . state_dict ( ) \n    torch . save ( checkpoint , filename ) "}
{"2319": "\ndef init_optimizer ( self , optimizer ) : \n    if isinstance ( optimizer , dict ) : \n        optimizer = obj_from_dict ( optimizer , torch . optim , dict ( params = self . model . parameters ( ) ) ) \n    else : \n        if not isinstance ( optimizer , torch . optim . Optimizer ) : \n            raise TypeError ( 'optimizer must be either an Optimizer object or a dict, ' 'but got {}' . format ( type ( optimizer ) ) ) \n    return optimizer "}
{"2323": "\ndef run ( self , data_loaders , workflow , max_epochs , ** kwargs ) : \n    assert isinstance ( data_loaders , list ) \n    assert mmcv . is_list_of ( workflow , tuple ) \n    assert len ( data_loaders ) == len ( workflow ) \n    self . _max_epochs = max_epochs \n    work_dir = self . work_dir if self . work_dir is not None else 'NONE' \n    self . logger . info ( 'Start running, host: %s, work_dir: %s' , get_host_info ( ) , work_dir ) \n    self . logger . info ( 'workflow: %s, max: %d epochs' , workflow , max_epochs ) \n    self . call_hook ( 'before_run' ) \n    while self . epoch < max_epochs : \n        for i , flow in enumerate ( workflow ) : \n            mode , epochs = flow \n            if isinstance ( mode , str ) : \n                if not hasattr ( self , mode ) : \n                    raise ValueError ( 'runner has no method named \"{}\" to run an epoch' . format ( mode ) ) \n                epoch_runner = getattr ( self , mode ) \n            else : \n                if callable ( mode ) : \n                    epoch_runner = mode \n                else : \n                    raise TypeError ( 'mode in workflow must be a str or ' 'callable function, not {}' . format ( type ( mode ) ) ) \n            for _ in range ( epochs ) : \n                if mode == 'train' and self . epoch >= max_epochs : \n                    return \n                epoch_runner ( data_loaders [ i ] , ** kwargs ) \n    time . sleep ( 1 ) \n    self . call_hook ( 'after_run' ) "}
{"2325": "\ndef convert_video ( in_file , out_file , print_cmd = False , pre_options = '' , ** kwargs ) : \n    options = [ ] \n    for k , v in kwargs . items ( ) : \n        if isinstance ( v , bool ) : \n            if v : \n                options . append ( '-{}' . format ( k ) ) \n        else : \n            if k == 'log_level' : \n                assert v in [ 'quiet' , 'panic' , 'fatal' , 'error' , 'warning' , 'info' , 'verbose' , 'debug' , 'trace' ] \n                options . append ( '-loglevel {}' . format ( v ) ) \n            else : \n                options . append ( '-{} {}' . format ( k , v ) ) \n    cmd = 'ffmpeg -y {} -i {} {} {}' . format ( pre_options , in_file , ' ' . join ( options ) , out_file ) \n    if print_cmd : \n        print ( cmd ) \n    subprocess . call ( cmd , shell = True ) "}
{"2326": "\ndef resize_video ( in_file , out_file , size = None , ratio = None , keep_ar = False , log_level = 'info' , print_cmd = False , ** kwargs ) : \n    if size is None and ratio is None : \n        raise ValueError ( 'expected size or ratio must be specified' ) \n    else : \n        if size is not None and ratio is not None : \n            raise ValueError ( 'size and ratio cannot be specified at the same time' ) \n    options = { 'log_level' : log_level } \n    if size : \n        if not keep_ar : \n            options [ 'vf' ] = 'scale={}:{}' . format ( size [ 0 ] , size [ 1 ] ) \n        else : \n            options [ 'vf' ] = ( 'scale=w={}:h={}:force_original_aspect_ratio' '=decrease' . format ( size [ 0 ] , size [ 1 ] ) ) \n    else : \n        if not isinstance ( ratio , tuple ) : \n            ratio = ( ratio , ratio ) \n        options [ 'vf' ] = 'scale=\"trunc(iw*{}):trunc(ih*{})\"' . format ( ratio [ 0 ] , ratio [ 1 ] ) \n    convert_video ( in_file , out_file , print_cmd , ** options ) "}
{"2332": "\ndef obj_from_dict ( info , parent = None , default_args = None ) : \n    assert isinstance ( info , dict ) and 'type' in info \n    assert isinstance ( default_args , dict ) or default_args is None \n    args = info . copy ( ) \n    obj_type = args . pop ( 'type' ) \n    if mmcv . is_str ( obj_type ) : \n        if parent is not None : \n            obj_type = getattr ( parent , obj_type ) \n        else : \n            obj_type = sys . modules [ obj_type ] \n    else : \n        if not isinstance ( obj_type , type ) : \n            raise TypeError ( 'type must be a str or valid type, but got {}' . format ( type ( obj_type ) ) ) \n    if default_args is not None : \n        for name , value in default_args . items ( ) : \n            args . setdefault ( name , value ) \n    return obj_type ( ** args ) "}
{"2333": "\ndef imread ( img_or_path , flag = 'color' ) : \n    if isinstance ( img_or_path , np . ndarray ) : \n        return img_or_path \n    else : \n        if is_str ( img_or_path ) : \n            flag = imread_flags [ flag ] if is_str ( flag ) else flag \n            check_file_exist ( img_or_path , 'img file does not exist: {}' . format ( img_or_path ) ) \n            return cv2 . imread ( img_or_path , flag ) \n        else : \n            raise TypeError ( '\"img\" must be a numpy array or a filename' ) "}
{"2340": "\ndef slice_list ( in_list , lens ) : \n    if not isinstance ( lens , list ) : \n        raise TypeError ( '\"indices\" must be a list of integers' ) \n    else : \n        if sum ( lens ) != len ( in_list ) : \n            raise ValueError ( 'sum of lens and list length does not match: {} != {}' . format ( sum ( lens ) , len ( in_list ) ) ) \n    out_list = [ ] \n    idx = 0 \n    for i in range ( len ( lens ) ) : \n        out_list . append ( in_list [ idx : idx + lens [ i ] ] ) \n        idx += lens [ i ] \n    return out_list "}
{"2343": "\ndef scatter ( input , devices , streams = None ) : \n    if streams is None : \n        streams = [ None ] * len ( devices ) \n    if isinstance ( input , list ) : \n        chunk_size = ( len ( input ) - 1 ) // len ( devices ) + 1 \n        outputs = [ scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) ] \n        return outputs \n    else : \n        if isinstance ( input , torch . Tensor ) : \n            output = input . contiguous ( ) \n            stream = streams [ 0 ] if output . numel ( ) > 0 else None \n            with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : \n                output = output . cuda ( devices [ 0 ] , non_blocking = True ) \n            return output \n        else : \n            raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) ) "}
{"2344": "\ndef color_val ( color ) : \n    if is_str ( color ) : \n        return Color [ color ] . value \n    else : \n        if isinstance ( color , Color ) : \n            return color . value \n        else : \n            if isinstance ( color , tuple ) : \n                assert len ( color ) == 3 \n                for channel in color : \n                    assert channel >= 0 and channel <= 255 \n                return color \n            else : \n                if isinstance ( color , int ) : \n                    assert color >= 0 and color <= 255 \n                    return color , color , color \n                else : \n                    if isinstance ( color , np . ndarray ) : \n                        assert color . ndim == 1 and color . size == 3 \n                        assert np . all ( ( color >= 0 ) & ( color <= 255 ) ) \n                        color = color . astype ( np . uint8 ) \n                        return tuple ( color ) \n                    else : \n                        raise TypeError ( 'Invalid type for color: {}' . format ( type ( color ) ) ) "}
{"2354": "\ndef scatter_kwargs ( inputs , kwargs , target_gpus , dim = 0 ) : \n    inputs = scatter ( inputs , target_gpus , dim ) if inputs else [ ] \n    kwargs = scatter ( kwargs , target_gpus , dim ) if kwargs else [ ] \n    if len ( inputs ) < len ( kwargs ) : \n        inputs . extend ( [ ( ) for _ in range ( len ( kwargs ) - len ( inputs ) ) ] ) \n    else : \n        if len ( kwargs ) < len ( inputs ) : \n            kwargs . extend ( [ { } for _ in range ( len ( inputs ) - len ( kwargs ) ) ] ) \n    inputs = tuple ( inputs ) \n    kwargs = tuple ( kwargs ) \n    return inputs , kwargs "}
{"2367": "\ndef egg2dist ( self , egginfo_path , distinfo_path ) : \n    def adios ( p ) : \n        if os . path . exists ( p ) and not os . path . islink ( p ) and os . path . isdir ( p ) : \n            shutil . rmtree ( p ) \n        else : \n            if os . path . exists ( p ) : \n                os . unlink ( p ) \n    adios ( distinfo_path ) \n    if not os . path . exists ( egginfo_path ) : \n        import glob \n        pat = os . path . join ( os . path . dirname ( egginfo_path ) , '*.egg-info' ) \n        possible = glob . glob ( pat ) \n        err = \"Egg metadata expected at %s but not found\" % ( egginfo_path , ) \n        if possible : \n            alt = os . path . basename ( possible [ 0 ] ) \n            err += \" (%s found - possible misnamed archive file?)\" % ( alt , ) \n        raise ValueError ( err ) \n    if os . path . isfile ( egginfo_path ) : \n        pkginfo_path = egginfo_path \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , egginfo_path ) \n        os . mkdir ( distinfo_path ) \n    else : \n        pkginfo_path = os . path . join ( egginfo_path , 'PKG-INFO' ) \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , pkginfo_path ) \n        shutil . copytree ( egginfo_path , distinfo_path , ignore = lambda x , y : set ( ( 'PKG-INFO' , 'requires.txt' , 'SOURCES.txt' , 'not-zip-safe' , ) ) ) \n        dependency_links_path = os . path . join ( distinfo_path , 'dependency_links.txt' ) \n        with open ( dependency_links_path , 'r' ) as dependency_links_file : \n            dependency_links = dependency_links_file . read ( ) . strip ( ) \n        if not dependency_links : \n            adios ( dependency_links_path ) \n    write_pkg_info ( os . path . join ( distinfo_path , 'METADATA' ) , pkg_info ) \n    metadata_path = os . path . join ( distinfo_path , 'METADATA' ) \n    self . add_requirements ( metadata_path ) \n    metadata_json_path = os . path . join ( distinfo_path , 'metadata.json' ) \n    pymeta = pkginfo_to_dict ( metadata_path , distribution = self . distribution ) \n    if 'description' in pymeta : \n        description_filename = 'DESCRIPTION.rst' \n        description_text = pymeta . pop ( 'description' ) \n        description_path = os . path . join ( distinfo_path , description_filename ) \n        with open ( description_path , \"wb\" ) as description_file : \n            description_file . write ( description_text . encode ( 'utf-8' ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'description' ] = description_filename \n    license = self . license_file ( ) \n    if license : \n        license_filename = 'LICENSE.txt' \n        shutil . copy ( license , os . path . join ( self . distinfo_dir , license_filename ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'license' ] = license_filename \n    with open ( metadata_json_path , \"w\" ) as metadata_json : \n        json . dump ( pymeta , metadata_json , sort_keys = True ) \n    adios ( egginfo_path ) "}
{"2376": "\nasync def write ( self , changes : Dict [ str , StoreItem ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for ( key , change ) in changes . items ( ) : \n            e_tag = change . e_tag \n            doc = { 'id' : self . __sanitize_key ( key ) , 'realId' : key , 'document' : self . __create_dict ( change ) } \n            if ( e_tag == '*' or not e_tag ) : \n                self . client . UpsertItem ( database_or_Container_link = self . __container_link , document = doc , options = { 'disableAutomaticIdGeneration' : True } ) \n            else : \n                if ( len ( e_tag ) > 0 ) : \n                    access_condition = { 'type' : 'IfMatch' , 'condition' : e_tag } \n                    self . client . ReplaceItem ( document_link = self . __item_link ( self . __sanitize_key ( key ) ) , new_document = doc , options = { 'accessCondition' : access_condition } ) \n                else : \n                    raise Exception ( 'cosmosdb_storage.write(): etag missing' ) \n    except Exception as e : \n        raise e "}
{"2400": "\ndef _do_pass ( self , pass_ , dag , options ) : \n    if not options [ \"ignore_requires\" ] : \n        for required_pass in pass_ . requires : \n            dag = self . _do_pass ( required_pass , dag , options ) \n    if pass_ not in self . valid_passes : \n        if pass_ . is_transformation_pass : \n            pass_ . property_set = self . fenced_property_set \n            new_dag = pass_ . run ( dag ) \n            if not isinstance ( new_dag , DAGCircuit ) : \n                raise TranspilerError ( \"Transformation passes should return a transformed dag.\" \"The pass %s is returning a %s\" % ( type ( pass_ ) . __name__ , type ( new_dag ) ) ) \n            dag = new_dag \n        else : \n            if pass_ . is_analysis_pass : \n                pass_ . property_set = self . property_set \n                pass_ . run ( FencedDAGCircuit ( dag ) ) \n            else : \n                raise TranspilerError ( \"I dont know how to handle this type of pass\" ) \n        self . _update_valid_passes ( pass_ , options [ 'ignore_preserves' ] ) \n    return dag "}
{"2405": "\ndef single_gate_params ( gate , params = None ) : \n    if gate in ( 'U' , 'u3' ) : \n        return params [ 0 ] , params [ 1 ] , params [ 2 ] \n    else : \n        if gate == 'u2' : \n            return np . pi / 2 , params [ 0 ] , params [ 1 ] \n        else : \n            if gate == 'u1' : \n                return 0 , 0 , params [ 0 ] \n            else : \n                if gate == 'id' : \n                    return 0 , 0 , 0 \n    raise QiskitError ( 'Gate is not among the valid types: %s' % gate ) "}
{"2427": "\ndef _process_bit_id ( self , node ) : \n    reg = None \n    if node . name in self . dag . qregs : \n        reg = self . dag . qregs [ node . name ] \n    else : \n        if node . name in self . dag . cregs : \n            reg = self . dag . cregs [ node . name ] \n        else : \n            raise QiskitError ( \"expected qreg or creg name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    if node . type == \"indexed_id\" : \n        return [ ( reg , node . index ) ] \n    else : \n        if node . type == \"id\" : \n            if not self . bit_stack [ - 1 ] : \n                return [ ( reg , j ) for j in range ( reg . size ) ] \n            else : \n                if node . name in self . bit_stack [ - 1 ] : \n                    return [ self . bit_stack [ - 1 ] [ node . name ] ] \n                raise QiskitError ( \"expected local bit name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    return None "}
{"2430": "\ndef _process_cnot ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ 0 ] ) \n    id1 = self . _process_bit_id ( node . children [ 1 ] ) \n    if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : \n        raise QiskitError ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) \n    for idx in range ( maxidx ) : \n        if len ( id0 ) > 1 and len ( id1 ) > 1 : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) \n        else : \n            if len ( id0 ) > 1 : \n                self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) \n            else : \n                self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition ) "}
{"2433": "\ndef _create_dag_op ( self , name , params , qargs ) : \n    if name == \"u0\" : \n        op_class = U0Gate \n    else : \n        if name == \"u1\" : \n            op_class = U1Gate \n        else : \n            if name == \"u2\" : \n                op_class = U2Gate \n            else : \n                if name == \"u3\" : \n                    op_class = U3Gate \n                else : \n                    if name == \"x\" : \n                        op_class = XGate \n                    else : \n                        if name == \"y\" : \n                            op_class = YGate \n                        else : \n                            if name == \"z\" : \n                                op_class = ZGate \n                            else : \n                                if name == \"t\" : \n                                    op_class = TGate \n                                else : \n                                    if name == \"tdg\" : \n                                        op_class = TdgGate \n                                    else : \n                                        if name == \"s\" : \n                                            op_class = SGate \n                                        else : \n                                            if name == \"sdg\" : \n                                                op_class = SdgGate \n                                            else : \n                                                if name == \"swap\" : \n                                                    op_class = SwapGate \n                                                else : \n                                                    if name == \"rx\" : \n                                                        op_class = RXGate \n                                                    else : \n                                                        if name == \"ry\" : \n                                                            op_class = RYGate \n                                                        else : \n                                                            if name == \"rz\" : \n                                                                op_class = RZGate \n                                                            else : \n                                                                if name == \"rzz\" : \n                                                                    op_class = RZZGate \n                                                                else : \n                                                                    if name == \"id\" : \n                                                                        op_class = IdGate \n                                                                    else : \n                                                                        if name == \"h\" : \n                                                                            op_class = HGate \n                                                                        else : \n                                                                            if name == \"cx\" : \n                                                                                op_class = CnotGate \n                                                                            else : \n                                                                                if name == \"cy\" : \n                                                                                    op_class = CyGate \n                                                                                else : \n                                                                                    if name == \"cz\" : \n                                                                                        op_class = CzGate \n                                                                                    else : \n                                                                                        if name == \"ch\" : \n                                                                                            op_class = CHGate \n                                                                                        else : \n                                                                                            if name == \"crz\" : \n                                                                                                op_class = CrzGate \n                                                                                            else : \n                                                                                                if name == \"cu1\" : \n                                                                                                    op_class = Cu1Gate \n                                                                                                else : \n                                                                                                    if name == \"cu3\" : \n                                                                                                        op_class = Cu3Gate \n                                                                                                    else : \n                                                                                                        if name == \"ccx\" : \n                                                                                                            op_class = ToffoliGate \n                                                                                                        else : \n                                                                                                            if name == \"cswap\" : \n                                                                                                                op_class = FredkinGate \n                                                                                                            else : \n                                                                                                                raise QiskitError ( \"unknown operation for ast node name %s\" % name ) \n    op = op_class ( * params ) \n    self . dag . apply_operation_back ( op , qargs , [ ] , condition = self . condition ) "}
{"2448": "\ndef vectorize ( density_matrix , method = 'col' ) : \n    density_matrix = np . array ( density_matrix ) \n    if method == 'col' : \n        return density_matrix . flatten ( order = 'F' ) \n    else : \n        if method == 'row' : \n            return density_matrix . flatten ( order = 'C' ) \n        else : \n            if method in [ 'pauli' , 'pauli_weights' ] : \n                num = int ( np . log2 ( len ( density_matrix ) ) ) \n                if len ( density_matrix ) != 2 ** num : \n                    raise Exception ( 'Input state must be n-qubit state' ) \n                if method == 'pauli_weights' : \n                    pgroup = pauli_group ( num , case = 'weight' ) \n                else : \n                    pgroup = pauli_group ( num , case = 'tensor' ) \n                vals = [ np . trace ( np . dot ( p . to_matrix ( ) , density_matrix ) ) for p in pgroup ] \n                return np . array ( vals ) \n    return None "}
{"2449": "\ndef devectorize ( vectorized_mat , method = 'col' ) : \n    vectorized_mat = np . array ( vectorized_mat ) \n    dimension = int ( np . sqrt ( vectorized_mat . size ) ) \n    if len ( vectorized_mat ) != dimension * dimension : \n        raise Exception ( 'Input is not a vectorized square matrix' ) \n    if method == 'col' : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'F' ) \n    else : \n        if method == 'row' : \n            return vectorized_mat . reshape ( dimension , dimension , order = 'C' ) \n        else : \n            if method in [ 'pauli' , 'pauli_weights' ] : \n                num_qubits = int ( np . log2 ( dimension ) ) \n                if dimension != 2 ** num_qubits : \n                    raise Exception ( 'Input state must be n-qubit state' ) \n                if method == 'pauli_weights' : \n                    pgroup = pauli_group ( num_qubits , case = 'weight' ) \n                else : \n                    pgroup = pauli_group ( num_qubits , case = 'tensor' ) \n                pbasis = np . array ( [ p . to_matrix ( ) for p in pgroup ] ) / 2 ** num_qubits \n                return np . tensordot ( vectorized_mat , pbasis , axes = 1 ) \n    return None "}
{"2450": "\ndef choi_to_rauli ( choi , order = 1 ) : \n    if order == 0 : \n        order = 'weight' \n    else : \n        if order == 1 : \n            order = 'tensor' \n    num_qubits = int ( np . log2 ( np . sqrt ( len ( choi ) ) ) ) \n    pgp = pauli_group ( num_qubits , case = order ) \n    rauli = [ ] \n    for i in pgp : \n        for j in pgp : \n            pauliop = np . kron ( j . to_matrix ( ) . T , i . to_matrix ( ) ) \n            rauli += [ np . trace ( np . dot ( choi , pauliop ) ) ] \n    return np . array ( rauli ) . reshape ( 4 ** num_qubits , 4 ** num_qubits ) "}
{"2454": "\ndef shannon_entropy ( pvec , base = 2 ) : \n    if base == 2 : \n        def logfn ( x ) : \n            return - x * np . log2 ( x ) \n    else : \n        if base == np . e : \n            def logfn ( x ) : \n                return - x * np . log ( x ) \n        else : \n            def logfn ( x ) : \n                return - x * np . log ( x ) / np . log ( base ) \n    h = 0. \n    for x in pvec : \n        if 0 < x < 1 : \n            h += logfn ( x ) \n    return h "}
{"2457": "\ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if state . ndim == 2 and len ( state ) == 4 and d0 == 2 and d1 == 2 : \n        return __eof_qubit ( state ) \n    else : \n        if state . ndim == 1 : \n            if d0 < d1 : \n                tr = [ 1 ] \n            else : \n                tr = [ 0 ] \n            state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n            return entropy ( state ) \n        else : \n            print ( 'Input must be a state-vector or 2-qubit density matrix.' ) \n    return None "}
{"2469": "\ndef quaternion_from_axis_rotation ( angle , axis ) : \n    out = np . zeros ( 4 , dtype = float ) \n    if axis == 'x' : \n        out [ 1 ] = 1 \n    else : \n        if axis == 'y' : \n            out [ 2 ] = 1 \n        else : \n            if axis == 'z' : \n                out [ 3 ] = 1 \n            else : \n                raise ValueError ( 'Invalid axis input.' ) \n    out *= math . sin ( angle / 2.0 ) \n    out [ 0 ] = math . cos ( angle / 2.0 ) \n    return Quaternion ( out ) "}
{"2477": "\ndef check_range ( self , j ) : \n    if isinstance ( j , int ) : \n        if j < 0 or j >= self . size : \n            raise QiskitIndexError ( \"register index out of range\" ) \n        else : \n            if isinstance ( j , slice ) : \n                if j . start < 0 or j . stop >= self . size or ( j . step is not None and j . step <= 0 ) : \n                    raise QiskitIndexError ( \"register index slice out of range\" ) "}
{"2508": "\ndef set_label_convention ( self , convention ) : \n    ketex = \"$\\\\left.|%s\\\\right\\\\rangle$\" \n    if convention == \"original\" : \n        self . xlabel = [ '$x$' , '' ] \n        self . ylabel = [ '$y$' , '' ] \n        self . zlabel = [ '$\\\\left|0\\\\right>$' , '$\\\\left|1\\\\right>$' ] \n    else : \n        if convention == \"xyz\" : \n            self . xlabel = [ '$x$' , '' ] \n            self . ylabel = [ '$y$' , '' ] \n            self . zlabel = [ '$z$' , '' ] \n        else : \n            if convention == \"sx sy sz\" : \n                self . xlabel = [ '$s_x$' , '' ] \n                self . ylabel = [ '$s_y$' , '' ] \n                self . zlabel = [ '$s_z$' , '' ] \n            else : \n                if convention == \"01\" : \n                    self . xlabel = [ '' , '' ] \n                    self . ylabel = [ '' , '' ] \n                    self . zlabel = [ '$\\\\left|0\\\\right>$' , '$\\\\left|1\\\\right>$' ] \n                else : \n                    if convention == \"polarization jones\" : \n                        self . xlabel = [ ketex % \"\\\\nearrow\\\\hspace{-1.46}\\\\swarrow\" , ketex % \"\\\\nwarrow\\\\hspace{-1.46}\\\\searrow\" ] \n                        self . ylabel = [ ketex % \"\\\\circlearrowleft\" , ketex % \"\\\\circlearrowright\" ] \n                        self . zlabel = [ ketex % \"\\\\leftrightarrow\" , ketex % \"\\\\updownarrow\" ] \n                    else : \n                        if convention == \"polarization jones letters\" : \n                            self . xlabel = [ ketex % \"D\" , ketex % \"A\" ] \n                            self . ylabel = [ ketex % \"L\" , ketex % \"R\" ] \n                            self . zlabel = [ ketex % \"H\" , ketex % \"V\" ] \n                        else : \n                            if convention == \"polarization stokes\" : \n                                self . ylabel = [ \"$\\\\nearrow\\\\hspace{-1.46}\\\\swarrow$\" , \"$\\\\nwarrow\\\\hspace{-1.46}\\\\searrow$\" ] \n                                self . zlabel = [ \"$\\\\circlearrowleft$\" , \"$\\\\circlearrowright$\" ] \n                                self . xlabel = [ \"$\\\\leftrightarrow$\" , \"$\\\\updownarrow$\" ] \n                            else : \n                                raise Exception ( \"No such convention.\" ) "}
{"2538": "\ndef has_register ( self , register ) : \n    has_reg = False \n    if ( isinstance ( register , QuantumRegister ) and register in self . qregs ) : \n        has_reg = True \n    else : \n        if ( isinstance ( register , ClassicalRegister ) and register in self . cregs ) : \n            has_reg = True \n    return has_reg "}
{"2543": "\ndef add_register ( self , * regs ) : \n    if not regs : \n        return \n    if any ( [ isinstance ( reg , int ) for reg in regs ] ) : \n        if len ( regs ) == 1 and isinstance ( regs [ 0 ] , int ) : \n            regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ) \n        else : \n            if len ( regs ) == 2 and all ( [ isinstance ( reg , int ) for reg in regs ] ) : \n                regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ClassicalRegister ( regs [ 1 ] , 'c' ) ) \n            else : \n                raise QiskitError ( \"QuantumCircuit parameters can be Registers or Integers.\" \" If Integers, up to 2 arguments. QuantumCircuit was called\" \" with %s.\" % ( regs , ) ) \n    for register in regs : \n        if register in self . qregs or register in self . cregs : \n            raise QiskitError ( \"register name \\\"%s\\\" already exists\" % register . name ) \n        if isinstance ( register , QuantumRegister ) : \n            self . qregs . append ( register ) \n        else : \n            if isinstance ( register , ClassicalRegister ) : \n                self . cregs . append ( register ) \n            else : \n                raise QiskitError ( \"expected a register\" ) "}
{"2556": "\ndef pulse_drawer ( samples , duration , dt = None , interp_method = 'None' , filename = None , interactive = False , dpi = 150 , nop = 1000 , size = ( 6 , 5 ) ) : \n    try : \n        from matplotlib import pyplot as plt \n    except ImportError : \n        raise ImportError ( 'pulse_drawer need matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if dt : \n        _dt = dt \n    else : \n        _dt = 1 \n    re_y = np . real ( samples ) \n    im_y = np . imag ( samples ) \n    image = plt . figure ( figsize = size ) \n    ax0 = image . add_subplot ( 111 ) \n    if interp_method == 'CubicSpline' : \n        time = np . arange ( 0 , duration + 1 ) * _dt + 0.5 * _dt \n        cs_ry = CubicSpline ( time [ : - 1 ] , re_y ) \n        cs_iy = CubicSpline ( time [ : - 1 ] , im_y ) \n        _time = np . linspace ( 0 , duration * _dt , nop ) \n        _re_y = cs_ry ( _time ) \n        _im_y = cs_iy ( _time ) \n    else : \n        if interp_method == 'None' : \n            time = np . arange ( 0 , duration + 1 ) * _dt \n            _time = np . r_ [ time [ 0 ] , np . repeat ( time [ 1 : - 1 ] , 2 ) , time [ - 1 ] ] \n            _re_y = np . repeat ( re_y , 2 ) \n            _im_y = np . repeat ( im_y , 2 ) \n        else : \n            raise QiskitError ( 'Invalid interpolation method \"%s\"' % interp_method ) \n    ax0 . fill_between ( x = _time , y1 = _re_y , y2 = np . zeros_like ( _time ) , facecolor = 'red' , alpha = 0.3 , edgecolor = 'red' , linewidth = 1.5 , label = 'real part' ) \n    ax0 . fill_between ( x = _time , y1 = _im_y , y2 = np . zeros_like ( _time ) , facecolor = 'blue' , alpha = 0.3 , edgecolor = 'blue' , linewidth = 1.5 , label = 'imaginary part' ) \n    ax0 . set_xlim ( 0 , duration * _dt ) \n    ax0 . grid ( b = True , linestyle = '-' ) \n    ax0 . legend ( bbox_to_anchor = ( 0.5 , 1.00 ) , loc = 'lower center' , ncol = 2 , frameon = False , fontsize = 14 ) \n    if filename : \n        image . savefig ( filename , dpi = dpi , bbox_inches = 'tight' ) \n    plt . close ( image ) \n    if image and interactive : \n        plt . show ( image ) \n    return image "}
{"2558": "\ndef _map_free_gates ( layout , gates , coupling_map ) : \n    blocked_qubits = set ( ) \n    mapped_gates = [ ] \n    remaining_gates = [ ] \n    for gate in gates : \n        if not gate [ 'partition' ] : \n            qubits = [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] . qargs \n            if not qubits : \n                continue \n            if blocked_qubits . intersection ( qubits ) : \n                blocked_qubits . update ( qubits ) \n                remaining_gates . append ( gate ) \n            else : \n                mapped_gate = _transform_gate_for_layout ( gate , layout ) \n                mapped_gates . append ( mapped_gate ) \n            continue \n        qubits = gate [ 'partition' ] [ 0 ] \n        if blocked_qubits . intersection ( qubits ) : \n            blocked_qubits . update ( qubits ) \n            remaining_gates . append ( gate ) \n        else : \n            if len ( qubits ) == 1 : \n                mapped_gate = _transform_gate_for_layout ( gate , layout ) \n                mapped_gates . append ( mapped_gate ) \n            else : \n                if coupling_map . distance ( * [ layout [ q ] for q in qubits ] ) == 1 : \n                    mapped_gate = _transform_gate_for_layout ( gate , layout ) \n                    mapped_gates . append ( mapped_gate ) \n                else : \n                    blocked_qubits . update ( qubits ) \n                    remaining_gates . append ( gate ) \n    return mapped_gates , remaining_gates "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if number_of_qubits < 5 : \n        temp_set = [ ] \n        if case == 'weight' : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) \n        else : \n            if case == 'tensor' : \n                for k in range ( 4 ** number_of_qubits ) : \n                    z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                    x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                    for j in range ( number_of_qubits ) : \n                        element = ( k // ( 4 ** j ) ) % 4 \n                        if element == 1 : \n                            x [ j ] = True \n                        else : \n                            if element == 2 : \n                                z [ j ] = True \n                                x [ j ] = True \n                            else : \n                                if element == 3 : \n                                    z [ j ] = True \n                    temp_set . append ( Pauli ( z , x ) ) \n                return temp_set \n            else : \n                raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2596": "\ndef from_label ( cls , label ) : \n    z = np . zeros ( len ( label ) , dtype = np . bool ) \n    x = np . zeros ( len ( label ) , dtype = np . bool ) \n    for i , char in enumerate ( label ) : \n        if char == 'X' : \n            x [ - i - 1 ] = True \n        else : \n            if char == 'Z' : \n                z [ - i - 1 ] = True \n            else : \n                if char == 'Y' : \n                    z [ - i - 1 ] = True \n                    x [ - i - 1 ] = True \n                else : \n                    if char != 'I' : \n                        raise QiskitError ( \"Pauli string must be only consisted of 'I', 'X', \" \"'Y' or 'Z' but you have {}.\" . format ( char ) ) \n    return cls ( z = z , x = x ) "}
{"2615": "\ndef _validate_measure_sampling ( self , experiment ) : \n    if self . _shots <= 1 : \n        self . _sample_measure = False \n        return \n    if hasattr ( experiment . config , 'allows_measure_sampling' ) : \n        self . _sample_measure = experiment . config . allows_measure_sampling \n    else : \n        measure_flag = False \n        for instruction in experiment . instructions : \n            if instruction . name == \"reset\" : \n                self . _sample_measure = False \n                return \n            if measure_flag : \n                if instruction . name not in [ \"measure\" , \"barrier\" , \"id\" , \"u0\" ] : \n                    self . _sample_measure = False \n                    return \n            else : \n                if instruction . name == \"measure\" : \n                    measure_flag = True \n        self . _sample_measure = True "}
{"2618": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if n_qubits > max_qubits : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if experiment . config . memory_slots == 0 : \n            logger . warning ( 'No classical registers in circuit \"%s\", ' 'counts will be empty.' , name ) \n        else : \n            if 'measure' not in [ op . name for op in experiment . instructions ] : \n                logger . warning ( 'No measurements in circuit \"%s\", ' 'classical register will remain all zeros.' , name ) "}
{"2639": "\ndef _initialize_backend_prop ( self ) : \n    backend_prop = self . backend_prop \n    for ginfo in backend_prop . gates : \n        if ginfo . gate == 'cx' : \n            for item in ginfo . parameters : \n                if item . name == 'gate_error' : \n                    g_reliab = 1.0 - item . value \n                    break \n                else : \n                    g_reliab = 1.0 \n            swap_reliab = - math . log ( pow ( g_reliab , 3 ) ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap_reliab ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap_reliab ) \n            self . cx_errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g_reliab \n            self . gate_list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) \n    idx = 0 \n    for q in backend_prop . qubits : \n        for nduv in q : \n            if nduv . name == 'readout_error' : \n                self . readout_errors [ idx ] = 1.0 - nduv . value \n                self . available_hw_qubits . append ( idx ) \n        idx += 1 \n    for edge in self . cx_errors : \n        self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ 0 ] ] * self . readout_errors [ edge [ 1 ] ] \n    self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) \n    for i in swap_costs_temp : \n        self . swap_costs [ i ] = { } \n        for j in swap_costs_temp [ i ] : \n            if ( i , j ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] \n            else : \n                if ( j , i ) in self . cx_errors : \n                    self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] \n                else : \n                    best_reliab = 0.0 \n                    for n in self . swap_graph . neighbors ( j ) : \n                        if ( n , j ) in self . cx_errors : \n                            reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] \n                        else : \n                            reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] \n                        if reliab > best_reliab : \n                            best_reliab = reliab \n                    self . swap_costs [ i ] [ j ] = best_reliab "}
{"2644": "\ndef run ( self , dag ) : \n    self . _initialize_backend_prop ( ) \n    num_qubits = self . _create_program_graph ( dag ) \n    if num_qubits > len ( self . swap_graph ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : \n        self . pending_program_edges . append ( ( end1 , end2 ) ) \n    while self . pending_program_edges : \n        edge = self . _select_next_edge ( ) \n        q1_mapped = edge [ 0 ] in self . prog2hw \n        q2_mapped = edge [ 1 ] in self . prog2hw \n        if ( not q1_mapped ) and ( not q2_mapped ) : \n            best_hw_edge = self . _select_best_remaining_cx ( ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] \n            self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] \n            self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) \n            self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) \n        else : \n            if not q1_mapped : \n                best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) \n                self . prog2hw [ edge [ 0 ] ] = best_hw_qubit \n                self . available_hw_qubits . remove ( best_hw_qubit ) \n            else : \n                best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) \n                self . prog2hw [ edge [ 1 ] ] = best_hw_qubit \n                self . available_hw_qubits . remove ( best_hw_qubit ) \n        new_edges = [ x for x in self . pending_program_edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] \n        self . pending_program_edges = new_edges \n    for qid in self . qarg_to_id . values ( ) : \n        if qid not in self . prog2hw : \n            self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] \n            self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) \n    layout = Layout ( ) \n    for q in dag . qubits ( ) : \n        pid = self . _qarg_to_id ( q ) \n        hwid = self . prog2hw [ pid ] \n        layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid \n    self . property_set [ 'layout' ] = layout "}
{"2665": "\ndef status ( self ) : \n    if self . _future . running ( ) : \n        _status = JobStatus . RUNNING \n    else : \n        if self . _future . cancelled ( ) : \n            _status = JobStatus . CANCELLED \n        else : \n            if self . _future . done ( ) : \n                _status = JobStatus . DONE if self . _future . exception ( ) is None else JobStatus . ERROR \n            else : \n                _status = JobStatus . INITIALIZING \n    return _status "}
{"2690": "\ndef rename_register ( self , regname , newname ) : \n    if regname == newname : \n        return \n    if newname in self . qregs or newname in self . cregs : \n        raise DAGCircuitError ( \"duplicate register name %s\" % newname ) \n    if regname not in self . qregs and regname not in self . cregs : \n        raise DAGCircuitError ( \"no register named %s\" % regname ) \n    if regname in self . qregs : \n        reg = self . qregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    if regname in self . cregs : \n        reg = self . cregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    for node in self . _multi_graph . nodes ( ) : \n        if node . type == \"in\" or node . type == \"out\" : \n            if node . name and regname in node . name : \n                node . name = newname \n        else : \n            if node . type == \"op\" : \n                qa = [ ] \n                for a in node . qargs : \n                    if a [ 0 ] == regname : \n                        a = ( newname , a [ 1 ] ) \n                    qa . append ( a ) \n                node . qargs = qa \n                ca = [ ] \n                for a in node . cargs : \n                    if a [ 0 ] == regname : \n                        a = ( newname , a [ 1 ] ) \n                    ca . append ( a ) \n                node . cargs = ca \n                if node . condition is not None : \n                    if node . condition [ 0 ] == regname : \n                        node . condition = ( newname , node . condition [ 1 ] ) \n    for _ , _ , edge_data in self . _multi_graph . edges ( data = True ) : \n        if regname in edge_data [ 'name' ] : \n            edge_data [ 'name' ] = re . sub ( regname , newname , edge_data [ 'name' ] ) "}
{"2699": "\ndef _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = True ) : \n    add_regs = set ( ) \n    reg_frag_chk = { } \n    for v in keyregs . values ( ) : \n        reg_frag_chk [ v ] = { j : False for j in range ( len ( v ) ) } \n    for k in edge_map . keys ( ) : \n        if k [ 0 ] . name in keyregs : \n            reg_frag_chk [ k [ 0 ] ] [ k [ 1 ] ] = True \n    for k , v in reg_frag_chk . items ( ) : \n        s = set ( v . values ( ) ) \n        if len ( s ) == 2 : \n            raise DAGCircuitError ( \"edge_map fragments reg %s\" % k ) \n        else : \n            if s == set ( [ False ] ) : \n                if k in self . qregs . values ( ) or k in self . cregs . values ( ) : \n                    raise DAGCircuitError ( \"unmapped duplicate reg %s\" % k ) \n                else : \n                    add_regs . add ( k ) \n            else : \n                if valreg : \n                    if not edge_map [ ( k , 0 ) ] [ 0 ] . name in valregs : \n                        size = max ( map ( lambda x : x [ 1 ] , filter ( lambda x : x [ 0 ] == edge_map [ ( k , 0 ) ] [ 0 ] , edge_map . values ( ) ) ) ) \n                        qreg = QuantumRegister ( size + 1 , edge_map [ ( k , 0 ) ] [ 0 ] . name ) \n                        add_regs . add ( qreg ) \n    return add_regs "}
{"2703": "\ndef compose_back ( self , input_circuit , edge_map = None ) : \n    edge_map = edge_map or { } \n    if len ( set ( edge_map . values ( ) ) ) != len ( edge_map ) : \n        raise DAGCircuitError ( \"duplicates in wire_map\" ) \n    add_qregs = self . _check_edgemap_registers ( edge_map , input_circuit . qregs , self . qregs ) \n    for qreg in add_qregs : \n        self . add_qreg ( qreg ) \n    add_cregs = self . _check_edgemap_registers ( edge_map , input_circuit . cregs , self . cregs ) \n    for creg in add_cregs : \n        self . add_creg ( creg ) \n    self . _check_wiremap_validity ( edge_map , input_circuit . input_map , self . output_map ) \n    for nd in input_circuit . topological_nodes ( ) : \n        if nd . type == \"in\" : \n            m_wire = edge_map . get ( nd . wire , nd . wire ) \n            if m_wire not in self . output_map : \n                raise DAGCircuitError ( \"wire %s[%d] not in self\" % ( m_wire [ 0 ] . name , m_wire [ 1 ] ) ) \n            if nd . wire not in input_circuit . wires : \n                raise DAGCircuitError ( \"inconsistent wire type for %s[%d] in input_circuit\" % ( nd . wire [ 0 ] . name , nd . wire [ 1 ] ) ) \n        else : \n            if nd . type == \"out\" : \n                pass \n            else : \n                if nd . type == \"op\" : \n                    condition = self . _map_condition ( edge_map , nd . condition ) \n                    self . _check_condition ( nd . name , condition ) \n                    m_qargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . qargs ) ) \n                    m_cargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . cargs ) ) \n                    self . apply_operation_back ( nd . op , m_qargs , m_cargs , condition ) \n                else : \n                    raise DAGCircuitError ( \"bad node type %s\" % nd . type ) "}
{"2730": "\ndef __pauli_meas_gates ( circuit , qreg , op ) : \n    if op not in [ 'X' , 'Y' , 'Z' ] : \n        raise QiskitError ( \"There's no X, Y or Z basis for this Pauli \" \"measurement\" ) \n    if op == \"X\" : \n        circuit . u2 ( 0. , np . pi , qreg ) \n    else : \n        if op == \"Y\" : \n            circuit . u2 ( 0. , 0.5 * np . pi , qreg ) "}
{"2731": "\ndef tomography_set ( meas_qubits , meas_basis = 'Pauli' , prep_qubits = None , prep_basis = None ) : \n    if not isinstance ( meas_qubits , list ) : \n        raise QiskitError ( 'Qubits argument must be a list' ) \n    num_of_qubits = len ( meas_qubits ) \n    if prep_qubits is None : \n        prep_qubits = meas_qubits \n    if not isinstance ( prep_qubits , list ) : \n        raise QiskitError ( 'prep_qubits argument must be a list' ) \n    if len ( prep_qubits ) != len ( meas_qubits ) : \n        raise QiskitError ( 'meas_qubits and prep_qubitsare different length' ) \n    if isinstance ( meas_basis , str ) : \n        if meas_basis . lower ( ) == 'pauli' : \n            meas_basis = PAULI_BASIS \n    if isinstance ( prep_basis , str ) : \n        if prep_basis . lower ( ) == 'pauli' : \n            prep_basis = PAULI_BASIS \n        else : \n            if prep_basis . lower ( ) == 'sic' : \n                prep_basis = SIC_BASIS \n    circuits = [ ] \n    circuit_labels = [ ] \n    if prep_basis is None : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'meas' : meas } ) \n            label = '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ 0 ] , qubit ) \n            circuit_labels . append ( label ) \n        return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'meas_basis' : meas_basis } \n    num_of_s = len ( list ( prep_basis . values ( ) ) [ 0 ] ) \n    plst_single = [ ( b , s ) for b in prep_basis . keys ( ) for s in range ( num_of_s ) ] \n    for plst_product in product ( plst_single , repeat = num_of_qubits ) : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            prep = dict ( zip ( prep_qubits , plst_product ) ) \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'prep' : prep , 'meas' : meas } ) \n            label = '_prep_' \n            for qubit , op in prep . items ( ) : \n                label += '%s%d(%d)' % ( op [ 0 ] , op [ 1 ] , qubit ) \n            label += '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ 0 ] , qubit ) \n            circuit_labels . append ( label ) \n    return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'prep_basis' : prep_basis , 'meas_basis' : meas_basis } "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if len ( msg ) < msg_len : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        else : \n            if len ( msg ) > msg_len : \n                msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2750": "\ndef plot_job_history ( jobs , interval = 'year' ) : \n    def get_date ( job ) : \n        return datetime . datetime . strptime ( job . creation_date ( ) , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    current_time = datetime . datetime . now ( ) \n    if interval == 'year' : \n        bins = [ ( current_time - datetime . timedelta ( days = k * 365 / 12 ) ) for k in range ( 12 ) ] \n    else : \n        if interval == 'month' : \n            bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30 ) ] \n        else : \n            if interval == 'week' : \n                bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7 ) ] \n    binned_jobs = [ 0 ] * len ( bins ) \n    if interval == 'year' : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . month == dat . month : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    else : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . day == dat . day and date . month == dat . month : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    nz_bins = [ ] \n    nz_idx = [ ] \n    for ind , val in enumerate ( binned_jobs ) : \n        if val != 0 : \n            nz_idx . append ( ind ) \n            nz_bins . append ( val ) \n    total_jobs = sum ( binned_jobs ) \n    colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , '#7a5195' , '#ef5675' , '#bc5090' ] \n    if interval == 'year' : \n        labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2 : ] , bins [ b ] . month ) for b in nz_idx ] \n    else : \n        labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] \n    fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 ) ) \n    ax . pie ( nz_bins [ : : - 1 ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14 } , rotatelabels = True , counterclock = False ) \n    ax . add_artist ( Circle ( ( 0 , 0 ) , 0.7 , color = 'white' , zorder = 1 ) ) \n    ax . text ( 0 , 0 , total_jobs , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = 26 ) \n    fig . tight_layout ( ) \n    return fig "}
{"2755": "\ndef _transpile_circuit ( circuit_config_tuple ) : \n    circuit , transpile_config = circuit_config_tuple \n    if transpile_config . pass_manager : \n        pass_manager = transpile_config . pass_manager \n    else : \n        if transpile_config . coupling_map : \n            pass_manager = default_pass_manager ( transpile_config . basis_gates , transpile_config . coupling_map , transpile_config . initial_layout , transpile_config . seed_transpiler ) \n        else : \n            pass_manager = default_pass_manager_simulator ( transpile_config . basis_gates ) \n    return pass_manager . run ( circuit ) "}
{"2762": "\ndef assemble ( experiments , backend = None , qobj_id = None , qobj_header = None , shots = 1024 , memory = False , max_credits = None , seed_simulator = None , default_qubit_los = None , default_meas_los = None , schedule_los = None , meas_level = 2 , meas_return = 'avg' , memory_slots = None , memory_slot_size = 100 , rep_time = None , parameter_binds = None , config = None , seed = None , ** run_config ) : \n    if config : \n        warnings . warn ( 'config is not used anymore. Set all configs in ' 'run_config.' , DeprecationWarning ) \n        run_config = run_config or config \n    if seed : \n        warnings . warn ( 'seed is deprecated in favor of seed_simulator.' , DeprecationWarning ) \n        seed_simulator = seed_simulator or seed \n    experiments = experiments if isinstance ( experiments , list ) else [ experiments ] \n    qobj_id , qobj_header , run_config = _parse_run_args ( backend , qobj_id , qobj_header , shots , memory , max_credits , seed_simulator , default_qubit_los , default_meas_los , schedule_los , meas_level , meas_return , memory_slots , memory_slot_size , rep_time , parameter_binds , ** run_config ) \n    if all ( isinstance ( exp , QuantumCircuit ) for exp in experiments ) : \n        bound_experiments , run_config = _expand_parameters ( circuits = experiments , run_config = run_config ) \n        return assemble_circuits ( circuits = bound_experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) \n    else : \n        if all ( isinstance ( exp , Schedule ) for exp in experiments ) : \n            return assemble_schedules ( schedules = experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) \n        else : \n            raise QiskitError ( \"bad input to assemble() function; \" \"must be either circuits or schedules\" ) "}
{"2789": "\ndef parse_debug ( self , val ) : \n    if val is True : \n        self . parse_deb = True \n    else : \n        if val is False : \n            self . parse_deb = False \n        else : \n            raise QasmError ( \"Illegal debug value '\" + str ( val ) + \"' must be True or False.\" ) "}
{"2802": "\ndef get_backend ( self , name = None , ** kwargs ) : \n    backends = self . backends ( name , ** kwargs ) \n    if len ( backends ) > 1 : \n        raise QiskitBackendNotFoundError ( 'More than one backend matches the criteria' ) \n    else : \n        if not backends : \n            raise QiskitBackendNotFoundError ( 'No backend matches the criteria' ) \n    return backends [ 0 ] "}
{"2819": "\ndef random_density_matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : \n    if method == 'Hilbert-Schmidt' : \n        return __random_density_hs ( length , rank , seed ) \n    else : \n        if method == 'Bures' : \n            return __random_density_bures ( length , rank , seed ) \n        else : \n            raise QiskitError ( 'Error: unrecognized method {}' . format ( method ) ) "}
{"2841": "\ndef _automatic_dims ( cls , dims , size ) : \n    if dims is None : \n        dims = size \n    else : \n        if np . product ( dims ) != size : \n            raise QiskitError ( \"dimensions do not match size.\" ) \n    if isinstance ( dims , ( int , np . integer ) ) : \n        num_qubits = int ( np . log2 ( dims ) ) \n        if 2 ** num_qubits == size : \n            return num_qubits * ( 2 , ) \n        return ( dims , ) \n    return tuple ( dims ) "}
{"2846": "\ndef state_fidelity ( state1 , state2 ) : \n    s1 = np . array ( state1 ) \n    s2 = np . array ( state2 ) \n    if s1 . ndim == 1 and s2 . ndim == 1 : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) ) ** 2 \n    else : \n        if s1 . ndim == 1 : \n            return np . abs ( s1 . conj ( ) . dot ( s2 ) . dot ( s1 ) ) \n        else : \n            if s2 . ndim == 1 : \n                return np . abs ( s2 . conj ( ) . dot ( s1 ) . dot ( s2 ) ) \n    s1sq = _funm_svd ( s1 , np . sqrt ) \n    s2sq = _funm_svd ( s2 , np . sqrt ) \n    return np . linalg . norm ( s1sq . dot ( s2sq ) , ord = 'nuc' ) ** 2 "}
{"2865": "\ndef get_ammo_generator ( self ) : \n    af_readers = { 'phantom' : missile . AmmoFileReader , 'slowlog' : missile . SlowLogReader , 'line' : missile . LineReader , 'uri' : missile . UriReader , 'uripost' : missile . UriPostReader , 'access' : missile . AccessLogReader , 'caseline' : missile . CaseLineReader , } \n    if self . uris and self . ammo_file : \n        raise StepperConfigurationError ( 'Both uris and ammo file specified. You must specify only one of them' ) \n    else : \n        if self . uris : \n            ammo_gen = missile . UriStyleGenerator ( self . uris , self . headers , http_ver = self . http_ver ) \n        else : \n            if self . ammo_file : \n                if self . ammo_type in af_readers : \n                    if self . ammo_type == 'phantom' : \n                        opener = resource . get_opener ( self . ammo_file ) \n                        with opener ( self . use_cache ) as ammo : \n                            try : \n                                if not ammo . next ( ) [ 0 ] . isdigit ( ) : \n                                    self . ammo_type = 'uri' \n                                    self . log . info ( \"Setting ammo_type 'uri' because ammo is not started with digit and you did not specify ammo format\" ) \n                                else : \n                                    self . log . info ( \"Default ammo type ('phantom') used, use 'phantom.ammo_type' option to override it\" ) \n                            except StopIteration : \n                                self . log . exception ( \"Couldn't read first line of ammo file\" ) \n                                raise AmmoFileError ( \"Couldn't read first line of ammo file\" ) \n                else : \n                    raise NotImplementedError ( 'No such ammo type implemented: \"%s\"' % self . ammo_type ) \n                ammo_gen = af_readers [ self . ammo_type ] ( self . ammo_file , headers = self . headers , http_ver = self . http_ver , use_cache = self . use_cache ) \n            else : \n                raise StepperConfigurationError ( 'Ammo not found. Specify uris or ammo file' ) \n    self . log . info ( \"Using %s ammo reader\" % type ( ammo_gen ) . __name__ ) \n    return ammo_gen "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    else : \n        if self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n            self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if self . timeout > 120000 : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl' "}
{"2873": "\ndef expand_time ( str_time , default_unit = 's' , multiplier = 1 ) : \n    parser = re . compile ( r'(\\d+)([a-zA-Z]*)' ) \n    parts = parser . findall ( str_time ) \n    result = 0.0 \n    for value , unit in parts : \n        value = int ( value ) \n        unit = unit . lower ( ) \n        if unit == '' : \n            unit = default_unit \n        if unit == 'ms' : \n            result += value * 0.001 \n            continue \n        else : \n            if unit == 's' : \n                result += value \n                continue \n            else : \n                if unit == 'm' : \n                    result += value * 60 \n                    continue \n                else : \n                    if unit == 'h' : \n                        result += value * 60 * 60 \n                        continue \n                    else : \n                        if unit == 'd' : \n                            result += value * 60 * 60 * 24 \n                            continue \n                        else : \n                            if unit == 'w' : \n                                result += value * 60 * 60 * 24 * 7 \n                                continue \n                            else : \n                                raise ValueError ( \"String contains unsupported unit %s: %s\" % ( unit , str_time ) ) \n    return int ( result * multiplier ) "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ True if i == ' ' else False for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( True ) \n            else : \n                if is_space ( chunk ) : \n                    result . append ( True ) \n                else : \n                    result . append ( False ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result += chunk \n        else : \n            if left > 0 : \n                if len ( chunk ) <= left : \n                    result += chunk \n                    left -= len ( chunk ) \n                else : \n                    leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] \n                    was_cut = not is_empty ( leftover , markups ) \n                    if was_cut : \n                        result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' \n                    else : \n                        result += chunk [ : left ] \n                    left = 0 \n    return result "}
{"2915": "\ndef clean_len ( self , line ) : \n    if isinstance ( line , basestring ) : \n        return len ( self . screen . markup . clean_markup ( line ) ) \n    else : \n        if isinstance ( line , tuple ) or isinstance ( line , list ) : \n            markups = self . screen . markup . get_markup_vars ( ) \n            length = 0 \n            for i in line : \n                if i not in markups : \n                    length += len ( i ) \n            return length "}
{"2936": "\ndef __handle_data_items ( self , host , data ) : \n    for metric , value in data . iteritems ( ) : \n        if value == '' : \n            self . sign [ host ] [ metric ] = - 1 \n            self . data [ host ] [ metric ] = value \n        else : \n            if not self . data [ host ] . get ( metric , None ) : \n                self . sign [ host ] [ metric ] = 1 \n            else : \n                if float ( value ) > float ( self . data [ host ] [ metric ] ) : \n                    self . sign [ host ] [ metric ] = 1 \n                else : \n                    if float ( value ) < float ( self . data [ host ] [ metric ] ) : \n                        self . sign [ host ] [ metric ] = - 1 \n                    else : \n                        self . sign [ host ] [ metric ] = 0 \n            self . data [ host ] [ metric ] = \"%.2f\" % float ( value ) "}
{"2954": "\ndef format_config_for_graphql ( config ) : \n    def _format_config_subdict ( config , current_indent = 0 ) : \n        check . dict_param ( config , 'config' , key_type = str ) \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        printer . line ( '{' ) \n        n_elements = len ( config ) \n        for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : \n            value = config [ key ] \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{key}: {formatted_value}{comma}' . format ( key = key , formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' , ) ) \n        printer . line ( '}' ) \n        return printer . read ( ) \n    def _format_config_sublist ( config , current_indent = 0 ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        printer . line ( '[' ) \n        n_elements = len ( config ) \n        for i , value in enumerate ( config ) : \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{formatted_value}{comma}' . format ( formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' ) ) \n        printer . line ( ']' ) \n        return printer . read ( ) \n    def _format_config_item ( config , current_indent = 0 ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        if isinstance ( config , dict ) : \n            return _format_config_subdict ( config , printer . current_indent ) \n        else : \n            if isinstance ( config , list ) : \n                return _format_config_sublist ( config , printer . current_indent ) \n            else : \n                if isinstance ( config , bool ) : \n                    return repr ( config ) . lower ( ) \n                else : \n                    return repr ( config ) . replace ( '\\'' , '\"' ) \n    check . dict_param ( config , 'config' , key_type = str ) \n    if not isinstance ( config , dict ) : \n        check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) \n    return _format_config_subdict ( config ) "}
{"2976": "\ndef _create_context_free_log ( run_config , pipeline_def ) : \n    check . inst_param ( run_config , 'run_config' , RunConfig ) \n    check . inst_param ( pipeline_def , 'pipeline_def' , PipelineDefinition ) \n    loggers = [ define_colored_console_logger ( 'dagster' ) ] \n    if run_config . event_callback : \n        loggers += [ construct_event_logger ( run_config . event_callback ) ] \n    else : \n        if run_config . loggers : \n            loggers += run_config . loggers \n    return DagsterLog ( run_config . run_id , get_logging_tags ( None , run_config , pipeline_def ) , loggers ) "}
{"3013": "\ndef minhash ( self , v ) : \n    if not isinstance ( v , collections . Iterable ) : \n        raise TypeError ( \"Input vector must be an iterable\" ) \n    if not len ( v ) == self . dim : \n        raise ValueError ( \"Input dimension mismatch, expecting %d\" % self . dim ) \n    if not isinstance ( v , np . ndarray ) : \n        v = np . array ( v , dtype = np . float32 ) \n    else : \n        if v . dtype != np . float32 : \n            v = v . astype ( np . float32 ) \n    hashvalues = np . zeros ( ( self . sample_size , 2 ) , dtype = np . int ) \n    vzeros = ( v == 0 ) \n    if vzeros . all ( ) : \n        raise ValueError ( \"Input is all zeros\" ) \n    v [ vzeros ] = np . nan \n    vlog = np . log ( v ) \n    for i in range ( self . sample_size ) : \n        t = np . floor ( ( vlog / self . rs [ i ] ) + self . betas [ i ] ) \n        ln_y = ( t - self . betas [ i ] ) * self . rs [ i ] \n        ln_a = self . ln_cs [ i ] - ln_y - self . rs [ i ] \n        k = np . nanargmin ( ln_a ) \n        hashvalues [ i ] [ 0 ] , hashvalues [ i ] [ 1 ] = k , int ( t [ k ] ) \n    return WeightedMinHash ( self . seed , hashvalues ) "}
{"3030": "\ndef parse_statement ( self ) : \n    self . _skip_whitespace_and_comments ( ) \n    if self . _current_token . kind == tokenize . ENDMARKER : \n        return None \n    stmt_loc = self . _current_location ( ignore_char_num = True ) \n    binding_key_or_keyword = self . _parse_selector ( ) \n    statement = None \n    if self . _current_token . value != '=' : \n        if binding_key_or_keyword == 'import' : \n            module = self . _parse_selector ( scoped = False ) \n            statement = ImportStatement ( module , stmt_loc ) \n        else : \n            if binding_key_or_keyword == 'include' : \n                str_loc = self . _current_location ( ) \n                success , filename = self . _maybe_parse_basic_type ( ) \n                if not success or not isinstance ( filename , str ) : \n                    self . _raise_syntax_error ( 'Expected file path as string.' , str_loc ) \n                statement = IncludeStatement ( filename , stmt_loc ) \n            else : \n                self . _raise_syntax_error ( \"Expected '='.\" ) \n    else : \n        self . _advance_one_token ( ) \n        value = self . parse_value ( ) \n        scope , selector , arg_name = parse_binding_key ( binding_key_or_keyword ) \n        statement = BindingStatement ( scope , selector , arg_name , value , stmt_loc ) \n    assert statement , 'Internal parsing error.' \n    if ( self . _current_token . kind != tokenize . NEWLINE and self . _current_token . kind != tokenize . ENDMARKER ) : \n        self . _raise_syntax_error ( 'Expected newline.' ) \n    else : \n        if self . _current_token . kind == tokenize . NEWLINE : \n            self . _advance_one_token ( ) \n    return statement "}
{"3048": "\ndef config_scope ( name_or_scope ) : \n    try : \n        valid_value = True \n        if isinstance ( name_or_scope , list ) : \n            new_scope = name_or_scope \n        else : \n            if name_or_scope and isinstance ( name_or_scope , six . string_types ) : \n                new_scope = current_scope ( ) \n                new_scope . extend ( name_or_scope . split ( '/' ) ) \n            else : \n                valid_value = name_or_scope in ( None , '' ) \n                new_scope = [ ] \n        _ACTIVE_SCOPES . append ( new_scope ) \n        scopes_are_valid = map ( config_parser . MODULE_RE . match , new_scope ) \n        if not valid_value or not all ( scopes_are_valid ) : \n            err_str = 'Invalid value for `name_or_scope`: {}.' \n            raise ValueError ( err_str . format ( name_or_scope ) ) \n        yield new_scope \n    finally : \n        _ACTIVE_SCOPES . pop ( ) "}
{"3051": "\ndef parse_config ( bindings , skip_unknown = False ) : \n    if isinstance ( bindings , ( list , tuple ) ) : \n        bindings = '\\n' . join ( bindings ) \n    _validate_skip_unknown ( skip_unknown ) \n    if isinstance ( skip_unknown , ( list , tuple ) ) : \n        skip_unknown = set ( skip_unknown ) \n    parser = config_parser . ConfigParser ( bindings , ParserDelegate ( skip_unknown ) ) \n    for statement in parser : \n        if isinstance ( statement , config_parser . BindingStatement ) : \n            scope , selector , arg_name , value , location = statement \n            if not arg_name : \n                macro_name = '{}/{}' . format ( scope , selector ) if scope else selector \n                with utils . try_with_location ( location ) : \n                    bind_parameter ( ( macro_name , 'gin.macro' , 'value' ) , value ) \n                continue \n            if not _should_skip ( selector , skip_unknown ) : \n                with utils . try_with_location ( location ) : \n                    bind_parameter ( ( scope , selector , arg_name ) , value ) \n        else : \n            if isinstance ( statement , config_parser . ImportStatement ) : \n                if skip_unknown : \n                    try : \n                        __import__ ( statement . module ) \n                        _IMPORTED_MODULES . add ( statement . module ) \n                    except ImportError : \n                        log_str = 'Skipping import of unknown module `%s` (skip_unknown=%r).' \n                        logging . info ( log_str , statement . module , skip_unknown ) \n                else : \n                    with utils . try_with_location ( statement . location ) : \n                        __import__ ( statement . module ) \n                    _IMPORTED_MODULES . add ( statement . module ) \n            else : \n                if isinstance ( statement , config_parser . IncludeStatement ) : \n                    with utils . try_with_location ( statement . location ) : \n                        parse_config_file ( statement . filename , skip_unknown ) \n                else : \n                    raise AssertionError ( 'Unrecognized statement type {}.' . format ( statement ) ) "}
{"3052": "\ndef register_file_reader ( * args ) : \n    def do_registration ( file_reader_fn , is_readable_fn ) : \n        if file_reader_fn not in list ( zip ( * _FILE_READERS ) ) [ 0 ] : \n            _FILE_READERS . append ( ( file_reader_fn , is_readable_fn ) ) \n    if len ( args ) == 1 : \n        return functools . partial ( do_registration , is_readable_fn = args [ 0 ] ) \n    else : \n        if len ( args ) == 2 : \n            do_registration ( * args ) \n        else : \n            err_str = 'register_file_reader() takes 1 or 2 arguments ({} given)' \n            raise TypeError ( err_str . format ( len ( args ) ) ) "}
{"3064": "\ndef sp_search_query ( query ) : \n    result = [ ] \n    for ( field , values ) in query . items ( ) : \n        field = SEARCH_FIELD_MAP . get ( field , field ) \n        if field is None : \n            continue \n        for value in values : \n            if field == 'year' : \n                value = _transform_year ( value ) \n                if value is not None : \n                    result . append ( '%s:%d' % ( field , value ) ) \n            else : \n                if field == 'any' : \n                    result . append ( '\"%s\"' % value ) \n                else : \n                    result . append ( '%s:\"%s\"' % ( field , value ) ) \n    return ' ' . join ( result ) "}
{"3065": "\ndef _parse_retry_after ( self , response ) : \n    value = response . headers . get ( 'Retry-After' ) \n    if not value : \n        seconds = 0 \n    else : \n        if re . match ( r'^\\s*[0-9]+\\s*$' , value ) : \n            seconds = int ( value ) \n        else : \n            date_tuple = email . utils . parsedate ( value ) \n            if date_tuple is None : \n                seconds = 0 \n            else : \n                seconds = time . mktime ( date_tuple ) - time . time ( ) \n    return max ( 0 , seconds ) "}
{"3074": "\ndef on_message ( self , message ) : \n    try : \n        message = json . loads ( message ) \n    except ValueError : \n        try : \n            self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Parsing request failed' , } , } ) ) \n        except tornado . websocket . WebSocketClosedError : \n            pass \n        return \n    if 'messageType' not in message or 'data' not in message : \n        try : \n            self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Invalid message' , } , } ) ) \n        except tornado . websocket . WebSocketClosedError : \n            pass \n        return \n    msg_type = message [ 'messageType' ] \n    if msg_type == 'setProperty' : \n        for property_name , property_value in message [ 'data' ] . items ( ) : \n            try : \n                self . thing . set_property ( property_name , property_value ) \n            except PropertyError as e : \n                self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : str ( e ) , } , } ) ) \n    else : \n        if msg_type == 'requestAction' : \n            for action_name , action_params in message [ 'data' ] . items ( ) : \n                input_ = None \n                if 'input' in action_params : \n                    input_ = action_params [ 'input' ] \n                action = self . thing . perform_action ( action_name , input_ ) \n                if action : \n                    tornado . ioloop . IOLoop . current ( ) . spawn_callback ( perform_action , action , ) \n                else : \n                    self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Invalid action request' , 'request' : message , } , } ) ) \n        else : \n            if msg_type == 'addEventSubscription' : \n                for event_name in message [ 'data' ] . keys ( ) : \n                    self . thing . add_event_subscriber ( event_name , self ) \n            else : \n                try : \n                    self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Unknown messageType: ' + msg_type , 'request' : message , } , } ) ) \n                except tornado . websocket . WebSocketClosedError : \n                    pass "}
{"3083": "\ndef get_addresses ( ) : \n    addresses = set ( ) \n    for iface in ifaddr . get_adapters ( ) : \n        for addr in iface . ips : \n            if addr . is_IPv4 : \n                ip = addr . ip \n                if not ip . startswith ( '169.254.' ) : \n                    addresses . add ( ip ) \n            else : \n                if addr . is_IPv6 : \n                    ip = addr . ip [ 0 ] . split ( '%' ) [ 0 ] . lower ( ) \n                    if not ip . startswith ( 'fe80:' ) : \n                        addresses . add ( '[{}]' . format ( ip ) ) \n    return sorted ( list ( addresses ) ) "}
{"3089": "\ndef get_action_descriptions ( self , action_name = None ) : \n    descriptions = [ ] \n    if action_name is None : \n        for name in self . actions : \n            for action in self . actions [ name ] : \n                descriptions . append ( action . as_action_description ( ) ) \n    else : \n        if action_name in self . actions : \n            for action in self . actions [ action_name ] : \n                descriptions . append ( action . as_action_description ( ) ) \n    return descriptions "}
{"3115": "\ndef _get_upsert_fields ( self , kwargs ) : \n    model_instance = self . model ( ** kwargs ) \n    insert_fields = [ ] \n    update_fields = [ ] \n    for field in model_instance . _meta . local_concrete_fields : \n        has_default = field . default != NOT_PROVIDED \n        if ( field . name in kwargs or field . column in kwargs ) : \n            insert_fields . append ( field ) \n            update_fields . append ( field ) \n            continue \n        else : \n            if has_default : \n                insert_fields . append ( field ) \n                continue \n        if field . primary_key is True and 'pk' in kwargs : \n            insert_fields . append ( field ) \n            update_fields . append ( field ) \n            continue \n        if self . _is_magical_field ( model_instance , field , is_insert = True ) : \n            insert_fields . append ( field ) \n        if self . _is_magical_field ( model_instance , field , is_insert = False ) : \n            update_fields . append ( field ) \n    return insert_fields , update_fields "}
{"3120": "\ndef as_sql ( self , compiler , connection ) : \n    result = [ ] \n    for key , value in self . value . items ( ) : \n        if hasattr ( value , 'as_sql' ) : \n            sql , params = value . as_sql ( compiler , connection ) \n            result . append ( 'hstore(\\'%s\\', %s)' % ( key , sql % params ) ) \n        else : \n            if value is not None : \n                result . append ( 'hstore(\\'%s\\', \\'%s\\')' % ( ( key , value ) ) ) \n            else : \n                result . append ( 'hstore(\\'%s\\', NULL)' % key ) \n    return '%s' % ' || ' . join ( result ) , [ ] "}
{"3133": "\ndef get_prep_value ( self , value ) : \n    value = Field . get_prep_value ( self , value ) \n    if isinstance ( value , dict ) : \n        prep_value = { } \n        for key , val in value . items ( ) : \n            if isinstance ( val , Expression ) : \n                prep_value [ key ] = val \n            else : \n                if val is not None : \n                    prep_value [ key ] = str ( val ) \n                else : \n                    prep_value [ key ] = val \n        value = prep_value \n    if isinstance ( value , list ) : \n        value = [ str ( item ) for item in value ] \n    return value "}
{"3136": "\ndef _rewrite_insert ( self , sql , params , return_id = False ) : \n    returning = self . qn ( self . query . model . _meta . pk . attname ) if return_id else '*' \n    if self . query . conflict_action . value == 'UPDATE' : \n        return self . _rewrite_insert_update ( sql , params , returning ) \n    else : \n        if self . query . conflict_action . value == 'NOTHING' : \n            return self . _rewrite_insert_nothing ( sql , params , returning ) \n    raise SuspiciousOperation ( ( '%s is not a valid conflict action, specify ' 'ConflictAction.UPDATE or ConflictAction.NOTHING.' ) % str ( self . query . conflict_action ) ) "}
{"3157": "\ndef select ( self , board ) : \n    if self . unexplored : \n        i = random . randrange ( len ( self . unexplored ) ) \n        pos = self . unexplored [ i ] \n        self . unexplored [ i ] = self . unexplored [ len ( self . unexplored ) - 1 ] \n        self . unexplored . pop ( ) \n        return pos \n    else : \n        if self . bestchild : \n            return self . bestchild . pos \n        else : \n            return PASS "}
{"3191": "\ndef metadata_id ( item ) : \n    if 'user' in item : \n        nick = item [ 'user' ] \n    else : \n        if 'comment' in item : \n            nick = item [ 'comment' ] [ 'user' ] \n        else : \n            nick = item [ 'bot_id' ] \n    return item [ 'ts' ] + nick "}
{"3215": "\ndef get_comments ( self , post_id ) : \n    path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) \n    params = { 'post_id' : post_id , 'post_type' : 'answer' , 'avatar_size' : 0 } \n    headers = { 'X-Requested-With' : 'XMLHttpRequest' } \n    try : \n        response = self . fetch ( path , payload = params , headers = headers ) \n        raw = response . text \n    except requests . exceptions . HTTPError as ex : \n        if ex . response . status_code == 404 : \n            logger . debug ( \"Comments URL did not work. Using old URL schema.\" ) \n            self . _use_new_urls = False \n            path = urijoin ( self . base_url , self . COMMENTS_OLD ) \n            response = self . fetch ( path , payload = params , headers = headers ) \n            raw = response . text \n        else : \n            if ex . response . status_code == 500 : \n                logger . warning ( \"Comments not retrieved due to %s\" , ex ) \n                raw = '[]' \n            else : \n                raise ex \n    return raw "}
{"3222": "\ndef _fetch_gerrit28 ( self , from_date = DEFAULT_DATETIME ) : \n    from_ut = datetime_to_utc ( from_date ) \n    from_ut = from_ut . timestamp ( ) \n    filter_open = \"status:open\" \n    filter_closed = \"status:closed\" \n    last_item_open = self . client . next_retrieve_group_item ( ) \n    last_item_closed = self . client . next_retrieve_group_item ( ) \n    reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n    reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n    last_nreviews_open = len ( reviews_open ) \n    last_nreviews_closed = len ( reviews_closed ) \n    while reviews_open or reviews_closed : \n        if reviews_open and reviews_closed : \n            if reviews_open [ 0 ] [ 'lastUpdated' ] >= reviews_closed [ 0 ] [ 'lastUpdated' ] : \n                review_open = reviews_open . pop ( 0 ) \n                review = review_open \n            else : \n                review_closed = reviews_closed . pop ( 0 ) \n                review = review_closed \n        else : \n            if reviews_closed : \n                review_closed = reviews_closed . pop ( 0 ) \n                review = review_closed \n            else : \n                review_open = reviews_open . pop ( 0 ) \n                review = review_open \n        updated = review [ 'lastUpdated' ] \n        if updated <= from_ut : \n            logger . debug ( \"No more updates for %s\" % ( self . hostname ) ) \n            break \n        else : \n            yield review \n        if not reviews_open and last_nreviews_open >= self . max_reviews : \n            last_item_open = self . client . next_retrieve_group_item ( last_item_open , review_open ) \n            reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n            last_nreviews_open = len ( reviews_open ) \n        if not reviews_closed and last_nreviews_closed >= self . max_reviews : \n            last_item_closed = self . client . next_retrieve_group_item ( last_item_closed , review_closed ) \n            reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n            last_nreviews_closed = len ( reviews_closed ) "}
{"3225": "\ndef next_retrieve_group_item ( self , last_item = None , entry = None ) : \n    next_item = None \n    gerrit_version = self . version \n    if gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] > 9 : \n        if last_item is None : \n            next_item = 0 \n        else : \n            next_item = last_item \n    else : \n        if gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] == 9 : \n            cause = \"Gerrit 2.9.0 does not support pagination\" \n            raise BackendError ( cause = cause ) \n        else : \n            if entry is not None : \n                next_item = entry [ 'sortKey' ] \n    return next_item "}
{"3244": "\ndef uuid ( * args ) : \n    def check_value ( v ) : \n        if not isinstance ( v , str ) : \n            raise ValueError ( \"%s value is not a string instance\" % str ( v ) ) \n        else : \n            if not v : \n                raise ValueError ( \"value cannot be None or empty\" ) \n            else : \n                return v \n    s = ':' . join ( map ( check_value , args ) ) \n    sha1 = hashlib . sha1 ( s . encode ( 'utf-8' , errors = 'surrogateescape' ) ) \n    uuid_sha1 = sha1 . hexdigest ( ) \n    return uuid_sha1 "}
{"3256": "\ndef _initialize_archive ( self ) : \n    if 'archive_path' not in self . parsed_args : \n        manager = None \n    else : \n        if self . parsed_args . no_archive : \n            manager = None \n        else : \n            if not self . parsed_args . archive_path : \n                archive_path = os . path . expanduser ( ARCHIVES_DEFAULT_PATH ) \n            else : \n                archive_path = self . parsed_args . archive_path \n            manager = ArchiveManager ( archive_path ) \n    self . archive_manager = manager "}
{"3267": "\ndef _pre_init ( self ) : \n    if self . parsed_args . git_log : \n        git_path = self . parsed_args . git_log \n    else : \n        if not self . parsed_args . git_path : \n            base_path = os . path . expanduser ( '~/.perceval/repositories/' ) \n            processed_uri = self . parsed_args . uri . lstrip ( '/' ) \n            git_path = os . path . join ( base_path , processed_uri ) + '-git' \n        else : \n            git_path = self . parsed_args . git_path \n    setattr ( self . parsed_args , 'gitpath' , git_path ) "}
{"3275": "\ndef rev_list ( self , branches = None ) : \n    if self . is_empty ( ) : \n        logger . warning ( \"Git %s repository is empty; unable to get the rev-list\" , self . uri ) \n        raise EmptyRepositoryError ( repository = self . uri ) \n    cmd_rev_list = [ 'git' , 'rev-list' , '--topo-order' ] \n    if branches is None : \n        cmd_rev_list . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) \n    else : \n        if len ( branches ) == 0 : \n            cmd_rev_list . extend ( [ '--branches' , '--tags' , '--max-count=0' ] ) \n        else : \n            branches = [ 'refs/heads/' + branch for branch in branches ] \n            cmd_rev_list . extend ( branches ) \n    for line in self . _exec_nb ( cmd_rev_list , cwd = self . dirpath , env = self . gitenv ) : \n        yield line . rstrip ( '\\n' ) \n    logger . debug ( \"Git rev-list fetched from %s repository (%s)\" , self . uri , self . dirpath ) "}
{"3276": "\ndef log ( self , from_date = None , to_date = None , branches = None , encoding = 'utf-8' ) : \n    if self . is_empty ( ) : \n        logger . warning ( \"Git %s repository is empty; unable to get the log\" , self . uri ) \n        raise EmptyRepositoryError ( repository = self . uri ) \n    cmd_log = [ 'git' , 'log' , '--reverse' , '--topo-order' ] \n    cmd_log . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) \n    if from_date : \n        dt = from_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) \n        cmd_log . append ( '--since=' + dt ) \n    if to_date : \n        dt = to_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) \n        cmd_log . append ( '--until=' + dt ) \n    if branches is None : \n        cmd_log . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) \n    else : \n        if len ( branches ) == 0 : \n            cmd_log . append ( '--max-count=0' ) \n        else : \n            branches = [ 'refs/heads/' + branch for branch in branches ] \n            cmd_log . extend ( branches ) \n    for line in self . _exec_nb ( cmd_log , cwd = self . dirpath , env = self . gitenv ) : \n        yield line \n    logger . debug ( \"Git log fetched from %s repository (%s)\" , self . uri , self . dirpath ) "}
{"3280": "\ndef _update_references ( self , refs ) : \n    new_refs = [ ref . refname for ref in refs ] \n    for old_ref in self . _discover_refs ( ) : \n        if not old_ref . refname . startswith ( 'refs/heads/' ) : \n            continue \n        if old_ref . refname in new_refs : \n            continue \n        self . _update_ref ( old_ref , delete = True ) \n    for new_ref in refs : \n        refname = new_ref . refname \n        if refname . endswith ( '^{}' ) : \n            logger . debug ( \"Annotated tag %s ignored for updating in sync process\" , refname ) \n            continue \n        else : \n            if not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : \n                logger . debug ( \"Reference %s not needed; ignored for updating in sync process\" , refname ) \n                continue \n            else : \n                self . _update_ref ( new_ref ) \n    cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] \n    self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) "}
{"3295": "\ndef metadata_category ( item ) : \n    if \"base\" in item : \n        category = CATEGORY_PULL_REQUEST \n    else : \n        if \"forks_count\" in item : \n            category = CATEGORY_REPO \n        else : \n            category = CATEGORY_ISSUE \n    return category "}
{"3296": "\ndef __fetch_pull_requests ( self , from_date , to_date ) : \n    raw_pulls = self . client . pulls ( from_date = from_date ) \n    for raw_pull in raw_pulls : \n        pull = json . loads ( raw_pull ) \n        if str_to_datetime ( pull [ 'updated_at' ] ) > to_date : \n            return \n        self . __init_extra_pull_fields ( pull ) \n        for field in TARGET_PULL_FIELDS : \n            if not pull [ field ] : \n                continue \n            if field == 'user' : \n                pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) \n            else : \n                if field == 'merged_by' : \n                    pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) \n                else : \n                    if field == 'review_comments' : \n                        pull [ field + '_data' ] = self . __get_pull_review_comments ( pull [ 'number' ] ) \n                    else : \n                        if field == 'requested_reviewers' : \n                            pull [ field + '_data' ] = self . __get_pull_requested_reviewers ( pull [ 'number' ] ) \n                        else : \n                            if field == 'commits' : \n                                pull [ field + '_data' ] = self . __get_pull_commits ( pull [ 'number' ] ) \n        yield pull "}
{"3317": "\ndef _need_check_tokens ( self ) : \n    if self . n_tokens <= 1 or self . rate_limit is None : \n        return False \n    else : \n        if self . last_rate_limit_checked is None : \n            self . last_rate_limit_checked = self . rate_limit \n            return True \n    approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 \n    if self . rate_limit <= approaching_limit : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) \n    if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    else : \n        if ratio > 1.0 : \n            self . last_rate_limit_checked = self . rate_limit \n            return False \n        else : \n            return False "}
{"3372": "\ndef _fetch_from_remote ( self , method , args ) : \n    try : \n        if method == NNTTPClient . GROUP : \n            data = self . handler . group ( args ) \n        else : \n            if method == NNTTPClient . OVER : \n                data = self . handler . over ( args ) \n            else : \n                if method == NNTTPClient . ARTICLE : \n                    data = self . _fetch_article ( args ) \n    except nntplib . NNTPTemporaryError as e : \n        data = e \n        raise e \n    finally : \n        if self . archive : \n            self . archive . store ( method , args , None , data ) \n    return data "}
{"3381": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        self . nline += 1 \n        if self . SUPYBOT_EMPTY_REGEX . match ( line ) : \n            continue \n        ts , msg = self . _parse_supybot_timestamp ( line ) \n        if self . SUPYBOT_EMPTY_COMMENT_REGEX . match ( msg ) : \n            continue \n        else : \n            if self . SUPYBOT_EMPTY_COMMENT_ACTION_REGEX . match ( msg ) : \n                continue \n            else : \n                if self . SUPYBOT_EMPTY_BOT_REGEX . match ( msg ) : \n                    continue \n        itype , nick , body = self . _parse_supybot_msg ( msg ) \n        item = self . _build_item ( ts , itype , nick , body ) \n        yield item "}
{"3517": "\ndef get_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if os . path . isdir ( target ) : \n        for src in source : \n            self . get_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) ) \n    else : \n        if len ( source ) > 1 : \n            raise Failure ( 'Target \"%s\" is not a directory.' % target ) \n        else : \n            if len ( source ) == 1 : \n                self . get_single_file ( pool , source [ 0 ] , target ) \n            else : \n                pass \n    pool . join ( ) "}
{"3519": "\ndef cp_files ( self , source , target , delete_source = False ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if target [ - 1 ] == PATH_SEP : \n        for src in source : \n            self . cp_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) , delete_source ) \n    else : \n        if len ( source ) > 1 : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n        else : \n            if len ( source ) == 1 : \n                self . cp_single_file ( pool , source [ 0 ] , target , delete_source ) \n            else : \n                pass \n    pool . join ( ) "}
{"3522": "\ndef dsync_files ( self , source , target ) : \n    src_s3_url = S3URL . is_valid ( source ) \n    dst_s3_url = S3URL . is_valid ( target ) \n    source_list = self . relative_dir_walk ( source ) \n    if len ( source_list ) == 0 or '.' in source_list : \n        raise Failure ( 'Sync command need to sync directory to directory.' ) \n    sync_list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source_list ] \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    if src_s3_url and not dst_s3_url : \n        for src , dest in sync_list : \n            pool . download ( src , dest ) \n    else : \n        if not src_s3_url and dst_s3_url : \n            for src , dest in sync_list : \n                pool . upload ( src , dest ) \n        else : \n            if src_s3_url and dst_s3_url : \n                for src , dest in sync_list : \n                    pool . copy ( src , dest ) \n            else : \n                raise InvalidArgument ( 'Cannot sync two local directories.' ) \n    pool . join ( ) \n    if self . opt . delete_removed : \n        target_list = self . relative_dir_walk ( target ) \n        remove_list = [ os . path . join ( target , f ) for f in ( set ( target_list ) - set ( source_list ) ) ] \n        if S3URL . is_valid ( target ) : \n            pool = ThreadPool ( ThreadUtil , self . opt ) \n            pool . batch_delete ( remove_list ) \n            pool . join ( ) \n        else : \n            for f in remove_list : \n                try : \n                    os . unlink ( f ) \n                    message ( 'Delete %s' , f ) \n                except : \n                    pass "}
{"3533": "\ndef upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : \n    s3url = S3URL ( target ) \n    obj = self . lookup ( s3url ) \n    if not mpi : \n        fsize = os . path . getsize ( source ) \n        md5cache = LocalMD5Cache ( source ) \n        if self . opt . dry_run : \n            message ( '%s => %s' , source , target ) \n            return \n        else : \n            if self . opt . sync_check and self . sync_check ( md5cache , obj ) : \n                message ( '%s => %s (synced)' , source , target ) \n                return \n            else : \n                if not self . opt . force and obj : \n                    raise Failure ( 'File already exists: %s' % target ) \n        if fsize < self . opt . max_singlepart_upload_size : \n            data = self . read_file_chunk ( source , 0 , fsize ) \n            self . s3 . put_object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n            message ( '%s => %s' , source , target ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . upload ( * args ) \n        return \n    data = self . read_file_chunk ( source , pos , chunk ) \n    response = self . s3 . upload_part ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , Body = data , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            message ( '%s => %s' , source , target ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Upload failed: Unable to complete upload %s.' % source ) "}
{"3553": "\ndef match_delta ( self , value ) : \n    m = self . REGEX_DELTA . search ( value ) \n    delta = datetime . timedelta ( days = 0 ) \n    if m : \n        d = int ( m . group ( 1 ) ) \n        if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : \n            d = - d \n        if m . group ( 2 ) == 'minute' : \n            delta = datetime . timedelta ( minutes = d ) \n        else : \n            if m . group ( 2 ) == 'hour' : \n                delta = datetime . timedelta ( hours = d ) \n            else : \n                if m . group ( 2 ) == 'day' : \n                    delta = datetime . timedelta ( days = d ) \n                else : \n                    if m . group ( 2 ) == 'week' : \n                        delta = datetime . timedelta ( weeks = d ) \n        value = self . REGEX_DELTA . sub ( '' , value ) \n    return ( delta , value ) "}
{"3569": "\ndef _build_person_data ( request ) : \n    if hasattr ( request , 'rollbar_person' ) : \n        rollbar_person_prop = request . rollbar_person \n        try : \n            person = rollbar_person_prop ( ) \n        except TypeError : \n            person = rollbar_person_prop \n        if person and isinstance ( person , dict ) : \n            return person \n        else : \n            return None \n    if hasattr ( request , 'user' ) : \n        user_prop = request . user \n        try : \n            user = user_prop ( ) \n        except TypeError : \n            user = user_prop \n        if not user : \n            return None \n        else : \n            if isinstance ( user , dict ) : \n                return user \n            else : \n                retval = { } \n                if getattr ( user , 'id' , None ) : \n                    retval [ 'id' ] = text ( user . id ) \n                else : \n                    if getattr ( user , 'user_id' , None ) : \n                        retval [ 'id' ] = text ( user . user_id ) \n                if retval . get ( 'id' ) : \n                    username = getattr ( user , 'username' , None ) \n                    email = getattr ( user , 'email' , None ) \n                    retval . update ( { 'username' : username , 'email' : email } ) \n                return retval \n    if hasattr ( request , 'user_id' ) : \n        user_id_prop = request . user_id \n        try : \n            user_id = user_id_prop ( ) \n        except TypeError : \n            user_id = user_id_prop \n        if not user_id : \n            return None \n        return { 'id' : text ( user_id ) } "}
{"3578": "\ndef decompose ( hangul_letter ) : \n    from . import checker \n    if len ( hangul_letter ) < 1 : \n        raise NotLetterException ( '' ) \n    else : \n        if not checker . is_hangul ( hangul_letter ) : \n            raise NotHangulException ( '' ) \n    if hangul_letter in CHO : \n        return hangul_letter , '' , '' \n    if hangul_letter in JOONG : \n        return '' , hangul_letter , '' \n    if hangul_letter in JONG : \n        return '' , '' , hangul_letter \n    code = hangul_index ( hangul_letter ) \n    cho , joong , jong = decompose_index ( code ) \n    if cho < 0 : \n        cho = 0 \n    try : \n        return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] \n    except : \n        print ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) \n        print ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) \n        raise Exception ( ) "}
{"3615": "\ndef _check_imports_order ( self , _module_node ) : \n    std_imports = [ ] \n    third_party_imports = [ ] \n    first_party_imports = [ ] \n    external_imports = [ ] \n    local_imports = [ ] \n    third_party_not_ignored = [ ] \n    first_party_not_ignored = [ ] \n    local_not_ignored = [ ] \n    isort_obj = isort . SortImports ( file_contents = \"\" , known_third_party = self . config . known_third_party , known_standard_library = self . config . known_standard_library , ) \n    for node , modname in self . _imports_stack : \n        if modname . startswith ( \".\" ) : \n            package = \".\" + modname . split ( \".\" ) [ 1 ] \n        else : \n            package = modname . split ( \".\" ) [ 0 ] \n        nested = not isinstance ( node . parent , astroid . Module ) \n        ignore_for_import_order = not self . linter . is_message_enabled ( \"wrong-import-order\" , node . fromlineno ) \n        import_category = isort_obj . place_module ( package ) \n        node_and_package_import = ( node , package ) \n        if import_category in ( \"FUTURE\" , \"STDLIB\" ) : \n            std_imports . append ( node_and_package_import ) \n            wrong_import = ( third_party_not_ignored or first_party_not_ignored or local_not_ignored ) \n            if self . _is_fallback_import ( node , wrong_import ) : \n                continue \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'standard import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ 0 ] [ 0 ] . as_string ( ) , ) , ) \n        else : \n            if import_category == \"THIRDPARTY\" : \n                third_party_imports . append ( node_and_package_import ) \n                external_imports . append ( node_and_package_import ) \n                if not nested and not ignore_for_import_order : \n                    third_party_not_ignored . append ( node_and_package_import ) \n                wrong_import = first_party_not_ignored or local_not_ignored \n                if wrong_import and not nested : \n                    self . add_message ( \"wrong-import-order\" , node = node , args = ( 'third party import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ 0 ] [ 0 ] . as_string ( ) , ) , ) \n            else : \n                if import_category == \"FIRSTPARTY\" : \n                    first_party_imports . append ( node_and_package_import ) \n                    external_imports . append ( node_and_package_import ) \n                    if not nested and not ignore_for_import_order : \n                        first_party_not_ignored . append ( node_and_package_import ) \n                    wrong_import = local_not_ignored \n                    if wrong_import and not nested : \n                        self . add_message ( \"wrong-import-order\" , node = node , args = ( 'first party import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ 0 ] [ 0 ] . as_string ( ) , ) , ) \n                else : \n                    if import_category == \"LOCALFOLDER\" : \n                        local_imports . append ( ( node , package ) ) \n                        if not nested and not ignore_for_import_order : \n                            local_not_ignored . append ( ( node , package ) ) \n    return std_imports , external_imports , local_imports "}
{"3617": "\ndef _add_imported_module ( self , node , importedmodname ) : \n    module_file = node . root ( ) . file \n    context_name = node . root ( ) . name \n    base = os . path . splitext ( os . path . basename ( module_file ) ) [ 0 ] \n    try : \n        importedmodname = astroid . modutils . get_module_part ( importedmodname , module_file ) \n    except ImportError : \n        pass \n    if context_name == importedmodname : \n        self . add_message ( \"import-self\" , node = node ) \n    else : \n        if not astroid . modutils . is_standard_module ( importedmodname ) : \n            if base != \"__init__\" and context_name not in self . _module_pkg : \n                self . _module_pkg [ context_name ] = context_name . rsplit ( \".\" , 1 ) [ 0 ] \n            importedmodnames = self . stats [ \"dependencies\" ] . setdefault ( importedmodname , set ( ) ) \n            if context_name not in importedmodnames : \n                importedmodnames . add ( context_name ) \n            self . import_graph [ context_name ] . add ( importedmodname ) \n            if not self . linter . is_message_enabled ( \"cyclic-import\" , line = node . lineno ) : \n                self . _excluded_edges [ context_name ] . add ( importedmodname ) "}
{"3628": "\ndef visit_call ( self , node ) : \n    try : \n        for inferred in node . func . infer ( ) : \n            if inferred is astroid . Uninferable : \n                continue \n            else : \n                if inferred . root ( ) . name == OPEN_MODULE : \n                    if getattr ( node . func , \"name\" , None ) in OPEN_FILES : \n                        self . _check_open_mode ( node ) \n                else : \n                    if inferred . root ( ) . name == UNITTEST_CASE : \n                        self . _check_redundant_assert ( node , inferred ) \n                    else : \n                        if isinstance ( inferred , astroid . ClassDef ) : \n                            if inferred . qname ( ) == THREADING_THREAD : \n                                self . _check_bad_thread_instantiation ( node ) \n                            else : \n                                if inferred . qname ( ) == SUBPROCESS_POPEN : \n                                    self . _check_for_preexec_fn_in_popen ( node ) \n                        else : \n                            if isinstance ( inferred , astroid . FunctionDef ) : \n                                name = inferred . qname ( ) \n                                if name == COPY_COPY : \n                                    self . _check_shallow_copy_environ ( node ) \n                                else : \n                                    if name in ENV_GETTERS : \n                                        self . _check_env_function ( node , inferred ) \n                                    else : \n                                        if name == SUBPROCESS_RUN and PY35 : \n                                            self . _check_for_check_kw_in_run ( node ) \n            self . _check_deprecated_method ( node , inferred ) \n    except astroid . InferenceError : \n        return "}
{"3649": "\ndef _no_context_variadic ( node , variadic_name , variadic_type , variadics ) : \n    statement = node . statement ( ) \n    for name in statement . nodes_of_class ( astroid . Name ) : \n        if name . name != variadic_name : \n            continue \n        inferred = safe_infer ( name ) \n        if isinstance ( inferred , ( astroid . List , astroid . Tuple ) ) : \n            length = len ( inferred . elts ) \n        else : \n            if isinstance ( inferred , astroid . Dict ) : \n                length = len ( inferred . items ) \n            else : \n                continue \n        inferred_statement = inferred . statement ( ) \n        if not length and isinstance ( inferred_statement , astroid . FunctionDef ) : \n            is_in_starred_context = _has_parent_of_type ( node , variadic_type , statement ) \n            used_as_starred_argument = _is_name_used_as_variadic ( name , variadics ) \n            if is_in_starred_context or used_as_starred_argument : \n                return True \n    return False "}
{"3655": "\ndef project_from_files ( files , func_wrapper = _astroid_wrapper , project_name = \"no name\" , black_list = ( \"CVS\" , ) ) : \n    astroid_manager = manager . AstroidManager ( ) \n    project = Project ( project_name ) \n    for something in files : \n        if not os . path . exists ( something ) : \n            fpath = modutils . file_from_modpath ( something . split ( \".\" ) ) \n        else : \n            if os . path . isdir ( something ) : \n                fpath = os . path . join ( something , \"__init__.py\" ) \n            else : \n                fpath = something \n        ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) \n        if ast is None : \n            continue \n        project . path = project . path or ast . file \n        project . add_module ( ast ) \n        base_name = ast . name \n        if ast . package and something . find ( \"__init__\" ) == - 1 : \n            for fpath in modutils . get_module_files ( os . path . dirname ( ast . file ) , black_list ) : \n                ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) \n                if ast is None or ast . name == base_name : \n                    continue \n                project . add_module ( ast ) \n    return project "}
{"3658": "\ndef visit_assignname ( self , node ) : \n    if hasattr ( node , \"_handled\" ) : \n        return \n    node . _handled = True \n    if node . name in node . frame ( ) : \n        frame = node . frame ( ) \n    else : \n        frame = node . root ( ) \n    try : \n        if not hasattr ( frame , \"locals_type\" ) : \n            if isinstance ( frame , astroid . ClassDef ) : \n                self . visit_classdef ( frame ) \n            else : \n                if isinstance ( frame , astroid . FunctionDef ) : \n                    self . visit_functiondef ( frame ) \n                else : \n                    self . visit_module ( frame ) \n        current = frame . locals_type [ node . name ] \n        values = set ( node . infer ( ) ) \n        frame . locals_type [ node . name ] = list ( set ( current ) | values ) \n    except astroid . InferenceError : \n        pass "}
{"3673": "\ndef _check_new_format ( self , node , func ) : \n    if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : \n        return \n    if node . starargs or node . kwargs : \n        return \n    try : \n        strnode = next ( func . bound . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : \n        return \n    try : \n        call_site = CallSite . from_call ( node ) \n    except astroid . InferenceError : \n        return \n    try : \n        fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) \n    except utils . IncompleteFormatString : \n        self . add_message ( \"bad-format-string\" , node = node ) \n        return \n    positional_arguments = call_site . positional_arguments \n    named_arguments = call_site . keyword_arguments \n    named_fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } \n    if num_args and manual_pos : \n        self . add_message ( \"format-combined-specification\" , node = node ) \n        return \n    check_args = False \n    num_args += sum ( 1 for field in named_fields if field == \"\" ) \n    if named_fields : \n        for field in named_fields : \n            if field and field not in named_arguments : \n                self . add_message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) \n        for field in named_arguments : \n            if field not in named_fields : \n                self . add_message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) \n        num_args = num_args or manual_pos \n        if positional_arguments or num_args : \n            empty = any ( True for field in named_fields if field == \"\" ) \n            if named_arguments or empty : \n                check_args = True \n    else : \n        check_args = True \n    if check_args : \n        num_args = num_args or manual_pos \n        if len ( positional_arguments ) > num_args : \n            self . add_message ( \"too-many-format-args\" , node = node ) \n        else : \n            if len ( positional_arguments ) < num_args : \n                self . add_message ( \"too-few-format-args\" , node = node ) \n    self . _detect_vacuous_formatting ( node , positional_arguments ) \n    self . _check_new_format_specifiers ( node , fields , named_arguments ) "}
{"3674": "\ndef process_non_raw_string_token ( self , prefix , string_body , start_row ) : \n    i = 0 \n    while True : \n        i = string_body . find ( \"\\\\\" , i ) \n        if i == - 1 : \n            break \n        next_char = string_body [ i + 1 ] \n        match = string_body [ i : i + 2 ] \n        if next_char in self . UNICODE_ESCAPE_CHARACTERS : \n            if \"u\" in prefix : \n                pass \n            else : \n                if ( _PY3K or self . _unicode_literals ) and \"b\" not in prefix : \n                    pass \n                else : \n                    self . add_message ( \"anomalous-unicode-escape-in-string\" , line = start_row , args = ( match , ) , ) \n        else : \n            if next_char not in self . ESCAPE_CHARACTERS : \n                self . add_message ( \"anomalous-backslash-in-string\" , line = start_row , args = ( match , ) ) \n        i += 2 "}
{"3689": "\ndef builder_inited ( app ) : \n    base_path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n    ext_path = os . path . join ( base_path , \"pylint\" , \"extensions\" ) \n    modules = [ ] \n    doc_files = { } \n    for filename in os . listdir ( ext_path ) : \n        name , ext = os . path . splitext ( filename ) \n        if name [ 0 ] == \"_\" or name in DEPRECATED_MODULES : \n            continue \n        if ext == \".py\" : \n            modules . append ( \"pylint.extensions.%s\" % name ) \n        else : \n            if ext == \".rst\" : \n                doc_files [ \"pylint.extensions.\" + name ] = os . path . join ( ext_path , filename ) \n    modules . sort ( ) \n    if not modules : \n        sys . exit ( \"No Pylint extensions found?\" ) \n    linter = PyLinter ( ) \n    linter . load_plugin_modules ( modules ) \n    extensions_doc = os . path . join ( base_path , \"doc\" , \"technical_reference\" , \"extensions.rst\" ) \n    with open ( extensions_doc , \"w\" ) as stream : \n        stream . write ( \"Optional Pylint checkers in the extensions module\\n\" ) \n        stream . write ( \"=================================================\\n\\n\" ) \n        stream . write ( \"Pylint provides the following optional plugins:\\n\\n\" ) \n        for module in modules : \n            stream . write ( \"- :ref:`{}`\\n\" . format ( module ) ) \n        stream . write ( \"\\n\" ) \n        stream . write ( \"You can activate any or all of these extensions \" \"by adding a ``load-plugins`` line to the ``MASTER`` \" \"section of your ``.pylintrc``, for example::\\n\" ) \n        stream . write ( \"\\n    load-plugins=pylint.extensions.docparams,\" \"pylint.extensions.docstyle\\n\\n\" ) \n        by_module = get_plugins_info ( linter , doc_files ) \n        for module , info in sorted ( by_module . items ( ) ) : \n            linter . _print_checker_doc ( info [ \"name\" ] , info , stream = stream ) "}
{"3695": "\ndef set_option ( self , optname , value , action = None , optdict = None ) : \n    if optname in self . _options_methods or optname in self . _bw_options_methods : \n        if value : \n            try : \n                meth = self . _options_methods [ optname ] \n            except KeyError : \n                meth = self . _bw_options_methods [ optname ] \n                warnings . warn ( \"%s is deprecated, replace it by %s\" % ( optname , optname . split ( \"-\" ) [ 0 ] ) , DeprecationWarning , ) \n            value = utils . _check_csv ( value ) \n            if isinstance ( value , ( list , tuple ) ) : \n                for _id in value : \n                    meth ( _id , ignore_unknown = True ) \n            else : \n                meth ( value ) \n            return \n    else : \n        if optname == \"output-format\" : \n            self . _reporter_name = value \n            if self . _reporters : \n                self . _load_reporter ( ) \n    try : \n        checkers . BaseTokenChecker . set_option ( self , optname , value , action , optdict ) \n    except config . UnsupportedAction : \n        print ( \"option %s can't be read from config file\" % optname , file = sys . stderr ) "}
{"3716": "\ndef _format_option_value ( optdict , value ) : \n    if isinstance ( value , ( list , tuple ) ) : \n        value = \",\" . join ( _format_option_value ( optdict , item ) for item in value ) \n    else : \n        if isinstance ( value , dict ) : \n            value = \",\" . join ( \"%s:%s\" % ( k , v ) for k , v in value . items ( ) ) \n        else : \n            if hasattr ( value , \"match\" ) : \n                value = value . pattern \n            else : \n                if optdict . get ( \"type\" ) == \"yn\" : \n                    value = \"yes\" if value else \"no\" \n                else : \n                    if isinstance ( value , str ) and value . isspace ( ) : \n                        value = \"'%s'\" % value \n    return value "}
{"3733": "\ndef possible_exc_types ( node ) : \n    excs = [ ] \n    if isinstance ( node . exc , astroid . Name ) : \n        inferred = utils . safe_infer ( node . exc ) \n        if inferred : \n            excs = [ inferred . name ] \n    else : \n        if node . exc is None : \n            handler = node . parent \n            while handler and not isinstance ( handler , astroid . ExceptHandler ) : \n                handler = handler . parent \n            if handler and handler . type : \n                inferred_excs = astroid . unpack_infer ( handler . type ) \n                excs = ( exc . name for exc in inferred_excs if exc is not astroid . Uninferable ) \n        else : \n            target = _get_raise_target ( node ) \n            if isinstance ( target , astroid . ClassDef ) : \n                excs = [ target . name ] \n            else : \n                if isinstance ( target , astroid . FunctionDef ) : \n                    for ret in target . nodes_of_class ( astroid . Return ) : \n                        if ret . frame ( ) != target : \n                            continue \n                        val = utils . safe_infer ( ret . value ) \n                        if ( val and isinstance ( val , ( astroid . Instance , astroid . ClassDef ) ) and utils . inherit_from_std_ex ( val ) ) : \n                            excs . append ( val . name ) \n    try : \n        return { exc for exc in excs if not utils . node_ignores_exception ( node , exc ) } \n    except astroid . InferenceError : \n        return set ( ) "}
{"3740": "\ndef _get_unpacking_extra_info ( node , infered ) : \n    more = \"\" \n    infered_module = infered . root ( ) . name \n    if node . root ( ) . name == infered_module : \n        if node . lineno == infered . lineno : \n            more = \" %s\" % infered . as_string ( ) \n        else : \n            if infered . lineno : \n                more = \" defined at line %s\" % infered . lineno \n    else : \n        if infered . lineno : \n            more = \" defined at line %s of %s\" % ( infered . lineno , infered_module ) \n    return more "}
{"3741": "\ndef _detect_global_scope ( node , frame , defframe ) : \n    def_scope = scope = None \n    if frame and frame . parent : \n        scope = frame . parent . scope ( ) \n    if defframe and defframe . parent : \n        def_scope = defframe . parent . scope ( ) \n    if isinstance ( frame , astroid . FunctionDef ) : \n        if not isinstance ( node . parent , ( astroid . FunctionDef , astroid . Arguments ) ) : \n            return False \n    else : \n        if any ( not isinstance ( f , ( astroid . ClassDef , astroid . Module ) ) for f in ( frame , defframe ) ) : \n            return False \n    break_scopes = [ ] \n    for s in ( scope , def_scope ) : \n        parent_scope = s \n        while parent_scope : \n            if not isinstance ( parent_scope , ( astroid . ClassDef , astroid . Module ) ) : \n                break_scopes . append ( parent_scope ) \n                break \n            if parent_scope . parent : \n                parent_scope = parent_scope . parent . scope ( ) \n            else : \n                break \n    if break_scopes and len ( set ( break_scopes ) ) != 1 : \n        return False \n    return frame . lineno < defframe . lineno "}
{"3753": "\ndef Run ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    from getopt import getopt \n    s_opts = \"hdi\" \n    l_opts = ( \"help\" , \"duplicates=\" , \"ignore-comments\" , \"ignore-imports\" , \"ignore-docstrings\" , ) \n    min_lines = 4 \n    ignore_comments = False \n    ignore_docstrings = False \n    ignore_imports = False \n    opts , args = getopt ( argv , s_opts , l_opts ) \n    for opt , val in opts : \n        if opt in ( \"-d\" , \"--duplicates\" ) : \n            min_lines = int ( val ) \n        else : \n            if opt in ( \"-h\" , \"--help\" ) : \n                usage ( ) \n            else : \n                if opt in ( \"-i\" , \"--ignore-comments\" ) : \n                    ignore_comments = True \n                else : \n                    if opt in ( \"--ignore-docstrings\" , ) : \n                        ignore_docstrings = True \n                    else : \n                        if opt in ( \"--ignore-imports\" , ) : \n                            ignore_imports = True \n    if not args : \n        usage ( 1 ) \n    sim = Similar ( min_lines , ignore_comments , ignore_docstrings , ignore_imports ) \n    for filename in args : \n        with open ( filename ) as stream : \n            sim . append_stream ( filename , stream ) \n    sim . run ( ) \n    sys . exit ( 0 ) "}
{"3777": "\ndef _check_signature ( self , method1 , refmethod , class_type , cls ) : \n    if not ( isinstance ( method1 , astroid . FunctionDef ) and isinstance ( refmethod , astroid . FunctionDef ) ) : \n        self . add_message ( \"method-check-failed\" , args = ( method1 , refmethod ) , node = method1 ) \n        return \n    instance = cls . instantiate_class ( ) \n    method1 = function_to_method ( method1 , instance ) \n    refmethod = function_to_method ( refmethod , instance ) \n    if method1 . args . args is None or refmethod . args . args is None : \n        return \n    if is_attr_private ( method1 . name ) : \n        return \n    if method1 . decorators : \n        for decorator in method1 . decorators . nodes : \n            if ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname == \"setter\" ) : \n                return \n    if _different_parameters ( refmethod , method1 , dummy_parameter_regex = self . _dummy_rgx ) : \n        self . add_message ( \"arguments-differ\" , args = ( class_type , method1 . name ) , node = method1 ) \n    else : \n        if len ( method1 . args . defaults ) < len ( refmethod . args . defaults ) : \n            self . add_message ( \"signature-differs\" , args = ( class_type , method1 . name ) , node = method1 ) "}
{"3780": "\ndef _check_bad_exception_context ( self , node ) : \n    cause = utils . safe_infer ( node . cause ) \n    if cause in ( astroid . Uninferable , None ) : \n        return \n    if isinstance ( cause , astroid . Const ) : \n        if cause . value is not None : \n            self . add_message ( \"bad-exception-context\" , node = node ) \n    else : \n        if not isinstance ( cause , astroid . ClassDef ) and not utils . inherit_from_std_ex ( cause ) : \n            self . add_message ( \"bad-exception-context\" , node = node ) "}
{"3781": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    klass = node . parent . frame ( ) \n    for stmt in node . nodes_of_class ( astroid . Call ) : \n        if node_frame_class ( stmt ) != node_frame_class ( node ) : \n            continue \n        expr = stmt . func \n        if not isinstance ( expr , astroid . Attribute ) : \n            continue \n        call = expr . expr \n        if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == \"super\" ) : \n            continue \n        if not klass . newstyle and has_known_bases ( klass ) : \n            continue \n        else : \n            if not call . args : \n                if sys . version_info [ 0 ] == 3 : \n                    continue \n                else : \n                    self . add_message ( \"missing-super-argument\" , node = call ) \n                    continue \n            arg0 = call . args [ 0 ] \n            if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == \"type\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) \n                continue \n            if ( len ( call . args ) >= 2 and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == \"self\" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == \"__class__\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"self.__class__\" , ) ) \n                continue \n            try : \n                supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) \n            except astroid . InferenceError : \n                continue \n            if klass is not supcls : \n                name = None \n                if supcls : \n                    name = supcls . name \n                else : \n                    if call . args and hasattr ( call . args [ 0 ] , \"name\" ) : \n                        name = call . args [ 0 ] . name \n                if name : \n                    self . add_message ( \"bad-super-call\" , node = call , args = ( name , ) ) "}
{"3795": "\ndef _check_simplifiable_if ( self , node ) : \n    if self . _is_actual_elif ( node ) : \n        return \n    if len ( node . orelse ) != 1 or len ( node . body ) != 1 : \n        return \n    first_branch = node . body [ 0 ] \n    else_branch = node . orelse [ 0 ] \n    if isinstance ( first_branch , astroid . Return ) : \n        if not isinstance ( else_branch , astroid . Return ) : \n            return \n        first_branch_is_bool = self . _is_bool_const ( first_branch ) \n        else_branch_is_bool = self . _is_bool_const ( else_branch ) \n        reduced_to = \"'return bool(test)'\" \n    else : \n        if isinstance ( first_branch , astroid . Assign ) : \n            if not isinstance ( else_branch , astroid . Assign ) : \n                return \n            first_branch_targets = [ target . name for target in first_branch . targets if isinstance ( target , astroid . AssignName ) ] \n            else_branch_targets = [ target . name for target in else_branch . targets if isinstance ( target , astroid . AssignName ) ] \n            if not first_branch_targets or not else_branch_targets : \n                return \n            if sorted ( first_branch_targets ) != sorted ( else_branch_targets ) : \n                return \n            first_branch_is_bool = self . _is_bool_const ( first_branch ) \n            else_branch_is_bool = self . _is_bool_const ( else_branch ) \n            reduced_to = \"'var = bool(test)'\" \n        else : \n            return \n    if not first_branch_is_bool or not else_branch_is_bool : \n        return \n    if not first_branch . value . value : \n        return \n    self . add_message ( \"simplifiable-if-statement\" , node = node , args = ( reduced_to , ) ) "}
{"3802": "\ndef _check_chained_comparison ( self , node ) : \n    if node . op != \"and\" or len ( node . values ) < 2 : \n        return \n    def _find_lower_upper_bounds ( comparison_node , uses ) : \n        left_operand = comparison_node . left \n        for operator , right_operand in comparison_node . ops : \n            for operand in ( left_operand , right_operand ) : \n                value = None \n                if isinstance ( operand , astroid . Name ) : \n                    value = operand . name \n                else : \n                    if isinstance ( operand , astroid . Const ) : \n                        value = operand . value \n                if value is None : \n                    continue \n                if operator in ( \"<\" , \"<=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n                    else : \n                        if operand is right_operand : \n                            uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                else : \n                    if operator in ( \">\" , \">=\" ) : \n                        if operand is left_operand : \n                            uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                        else : \n                            if operand is right_operand : \n                                uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n            left_operand = right_operand \n    uses = collections . defaultdict ( lambda : { \"lower_bound\" : set ( ) , \"upper_bound\" : set ( ) } ) \n    for comparison_node in node . values : \n        if isinstance ( comparison_node , astroid . Compare ) : \n            _find_lower_upper_bounds ( comparison_node , uses ) \n    for _ , bounds in uses . items ( ) : \n        num_shared = len ( bounds [ \"lower_bound\" ] . intersection ( bounds [ \"upper_bound\" ] ) ) \n        num_lower_bounds = len ( bounds [ \"lower_bound\" ] ) \n        num_upper_bounds = len ( bounds [ \"upper_bound\" ] ) \n        if num_shared < num_lower_bounds and num_shared < num_upper_bounds : \n            self . add_message ( \"chained-comparison\" , node = node ) \n            break "}
{"3830": "\ndef _get_indent_length ( line ) : \n    result = 0 \n    for char in line : \n        if char == \" \" : \n            result += 1 \n        else : \n            if char == \"\\t\" : \n                result += _TAB_LENGTH \n            else : \n                break \n    return result "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and tokens [ start ] [ 1 ] == \"for\" : \n        self . _pop_token ( ) \n    if tokens [ start + 1 ] [ 1 ] != \"(\" : \n        return \n    found_and_or = False \n    depth = 0 \n    keyword_token = str ( tokens [ start ] [ 1 ] ) \n    line_num = tokens [ start ] [ 2 ] [ 0 ] \n    for i in range ( start , len ( tokens ) - 1 ) : \n        token = tokens [ i ] \n        if token [ 0 ] == tokenize . NL : \n            return \n        if token [ 1 ] == \"(\" : \n            depth += 1 \n        else : \n            if token [ 1 ] == \")\" : \n                depth -= 1 \n                if depth : \n                    continue \n                if tokens [ i + 1 ] [ 1 ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + 1 ] [ 0 ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                    if i == start + 2 : \n                        return \n                    if keyword_token == \"not\" : \n                        if not found_and_or : \n                            self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                    else : \n                        if keyword_token in ( \"return\" , \"yield\" ) : \n                            self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                        else : \n                            if keyword_token not in self . _keywords_with_parens : \n                                if not found_and_or : \n                                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                return \n            else : \n                if depth == 1 : \n                    if token [ 1 ] == \",\" : \n                        return \n                    if token [ 1 ] in ( \"and\" , \"or\" ) : \n                        found_and_or = True \n                    else : \n                        if token [ 1 ] == \"yield\" : \n                            return \n                        else : \n                            if token [ 1 ] == \"for\" : \n                                return "}
{"3840": "\ndef _has_valid_type_annotation ( self , tokens , i ) : \n    if not self . _inside_brackets ( \"(\" ) : \n        return False \n    bracket_level = 0 \n    for token in tokens [ i - 1 : : - 1 ] : \n        if token [ 1 ] == \":\" : \n            return True \n        if token [ 1 ] == \"(\" : \n            return False \n        if token [ 1 ] == \"]\" : \n            bracket_level += 1 \n        else : \n            if token [ 1 ] == \"[\" : \n                bracket_level -= 1 \n            else : \n                if token [ 1 ] == \",\" : \n                    if not bracket_level : \n                        return False \n                else : \n                    if token [ 1 ] in ( \".\" , \"...\" ) : \n                        continue \n                    else : \n                        if token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : \n                            return False \n    return False "}
{"3841": "\ndef _check_equals_spacing ( self , tokens , i ) : \n    if self . _has_valid_type_annotation ( tokens , i ) : \n        self . _check_space ( tokens , i , ( _MUST , _MUST ) ) \n    else : \n        if self . _inside_brackets ( \"(\" ) or self . _inside_brackets ( \"lambda\" ) : \n            self . _check_space ( tokens , i , ( _MUST_NOT , _MUST_NOT ) ) \n        else : \n            self . _check_space ( tokens , i , ( _MUST , _MUST ) ) "}
{"3845": "\ndef check_lines ( self , lines , i ) : \n    max_chars = self . config . max_line_length \n    ignore_long_line = self . config . ignore_long_lines \n    def check_line ( line , i ) : \n        if not line . endswith ( \"\\n\" ) : \n            self . add_message ( \"missing-final-newline\" , line = i ) \n        else : \n            stripped_line = line . rstrip ( \"\\t\\n\\r\\v \" ) \n            if not stripped_line and _EMPTY_LINE in self . config . no_space_check : \n                pass \n            else : \n                if line [ len ( stripped_line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : \n                    self . add_message ( \"trailing-whitespace\" , line = i , col_offset = len ( stripped_line ) ) \n            line = stripped_line \n        mobj = OPTION_RGX . search ( line ) \n        if mobj and \"=\" in line : \n            front_of_equal , _ , back_of_equal = mobj . group ( 1 ) . partition ( \"=\" ) \n            if front_of_equal . strip ( ) == \"disable\" : \n                if \"line-too-long\" in { _msg_id . strip ( ) for _msg_id in back_of_equal . split ( \",\" ) } : \n                    return None \n                line = line . rsplit ( \"#\" , 1 ) [ 0 ] . rstrip ( ) \n        if len ( line ) > max_chars and not ignore_long_line . search ( line ) : \n            self . add_message ( \"line-too-long\" , line = i , args = ( len ( line ) , max_chars ) ) \n        return i + 1 \n    unsplit_ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } \n    unsplit = [ ] \n    for line in lines . splitlines ( True ) : \n        if line [ - 1 ] in unsplit_ends : \n            unsplit . append ( line ) \n            continue \n        if unsplit : \n            unsplit . append ( line ) \n            line = \"\" . join ( unsplit ) \n            unsplit = [ ] \n        i = check_line ( line , i ) \n        if i is None : \n            break \n    if unsplit : \n        check_line ( \"\" . join ( unsplit ) , i ) "}
{"3847": "\ndef _in_iterating_context ( node ) : \n    parent = node . parent \n    if isinstance ( parent , astroid . For ) : \n        return True \n    if isinstance ( parent , astroid . Comprehension ) : \n        if parent . iter == node : \n            return True \n    else : \n        if isinstance ( parent , astroid . Call ) : \n            if isinstance ( parent . func , astroid . Name ) : \n                parent_scope = parent . func . lookup ( parent . func . name ) [ 0 ] \n                if _is_builtin ( parent_scope ) and parent . func . name in _ACCEPTS_ITERATOR : \n                    return True \n            else : \n                if isinstance ( parent . func , astroid . Attribute ) : \n                    if parent . func . attrname in ATTRIBUTES_ACCEPTS_ITERATOR : \n                        return True \n            inferred = utils . safe_infer ( parent . func ) \n            if inferred : \n                if inferred . qname ( ) in _BUILTIN_METHOD_ACCEPTS_ITERATOR : \n                    return True \n                root = inferred . root ( ) \n                if root and root . name == \"itertools\" : \n                    return True \n        else : \n            if isinstance ( parent , astroid . Assign ) and isinstance ( parent . targets [ 0 ] , ( astroid . List , astroid . Tuple ) ) : \n                if len ( parent . targets [ 0 ] . elts ) > 1 : \n                    return True \n            else : \n                if ( isinstance ( parent , astroid . Compare ) and len ( parent . ops ) == 1 and parent . ops [ 0 ] [ 0 ] == \"in\" ) : \n                    return True \n                else : \n                    if isinstance ( parent , astroid . YieldFrom ) : \n                        return True \n    if isinstance ( parent , astroid . Starred ) : \n        return True \n    return False "}
{"3876": "\ndef _check_format_string ( self , node , format_arg ) : \n    num_args = _count_supplied_tokens ( node . args [ format_arg + 1 : ] ) \n    if not num_args : \n        return \n    format_string = node . args [ format_arg ] . value \n    if not isinstance ( format_string , str ) : \n        required_num_args = 0 \n    else : \n        try : \n            if self . _format_style == \"old\" : \n                keyword_args , required_num_args , _ , _ = utils . parse_format_string ( format_string ) \n                if keyword_args : \n                    return \n            else : \n                if self . _format_style == \"new\" : \n                    keyword_arguments , implicit_pos_args , explicit_pos_args = utils . parse_format_method_string ( format_string ) \n                    keyword_args_cnt = len ( set ( k for k , l in keyword_arguments if not isinstance ( k , int ) ) ) \n                    required_num_args = ( keyword_args_cnt + implicit_pos_args + explicit_pos_args ) \n        except utils . UnsupportedFormatCharacter as ex : \n            char = format_string [ ex . index ] \n            self . add_message ( \"logging-unsupported-format\" , node = node , args = ( char , ord ( char ) , ex . index ) , ) \n            return \n        except utils . IncompleteFormatString : \n            self . add_message ( \"logging-format-truncated\" , node = node ) \n            return \n    if num_args > required_num_args : \n        self . add_message ( \"logging-too-many-args\" , node = node ) \n    else : \n        if num_args < required_num_args : \n            self . add_message ( \"logging-too-few-args\" , node = node ) "}
{"3881": "\ndef _determine_function_name_type ( node , config = None ) : \n    property_classes , property_names = _get_properties ( config ) \n    if not node . is_method ( ) : \n        return \"function\" \n    if node . decorators : \n        decorators = node . decorators . nodes \n    else : \n        decorators = [ ] \n    for decorator in decorators : \n        if isinstance ( decorator , astroid . Name ) or ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname in property_names ) : \n            infered = utils . safe_infer ( decorator ) \n            if infered and infered . qname ( ) in property_classes : \n                return \"attr\" \n        else : \n            if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"setter\" , \"deleter\" , ) : \n                return \"attr\" \n    return \"method\" "}
{"3892": "\ndef visit_lambda ( self , node ) : \n    if node . args . defaults : \n        return \n    call = node . body \n    if not isinstance ( call , astroid . Call ) : \n        return \n    if isinstance ( node . body . func , astroid . Attribute ) and isinstance ( node . body . func . expr , astroid . Call ) : \n        return \n    call_site = CallSite . from_call ( call ) \n    ordinary_args = list ( node . args . args ) \n    new_call_args = list ( self . _filter_vararg ( node , call . args ) ) \n    if node . args . kwarg : \n        if self . _has_variadic_argument ( call . kwargs , node . args . kwarg ) : \n            return \n    if node . args . vararg : \n        if self . _has_variadic_argument ( call . starargs , node . args . vararg ) : \n            return \n    else : \n        if call . starargs : \n            return \n    if call . keywords : \n        lambda_kwargs = { keyword . name for keyword in node . args . defaults } \n        if len ( lambda_kwargs ) != len ( call_site . keyword_arguments ) : \n            return \n        if set ( call_site . keyword_arguments ) . difference ( lambda_kwargs ) : \n            return \n    if len ( ordinary_args ) != len ( new_call_args ) : \n        return \n    for arg , passed_arg in zip ( ordinary_args , new_call_args ) : \n        if not isinstance ( passed_arg , astroid . Name ) : \n            return \n        if arg . name != passed_arg . name : \n            return \n    self . add_message ( \"unnecessary-lambda\" , line = node . fromlineno , node = node ) "}
{"3898": "\ndef visit_assignname ( self , node ) : \n    self . _check_assign_to_new_keyword_violation ( node . name , node ) \n    frame = node . frame ( ) \n    assign_type = node . assign_type ( ) \n    if isinstance ( assign_type , astroid . Comprehension ) : \n        self . _check_name ( \"inlinevar\" , node . name , node ) \n    else : \n        if isinstance ( frame , astroid . Module ) : \n            if isinstance ( assign_type , astroid . Assign ) and not in_loop ( assign_type ) : \n                if isinstance ( utils . safe_infer ( assign_type . value ) , astroid . ClassDef ) : \n                    self . _check_name ( \"class\" , node . name , node ) \n                else : \n                    if not _redefines_import ( node ) : \n                        self . _check_name ( \"const\" , node . name , node ) \n            else : \n                if isinstance ( assign_type , astroid . ExceptHandler ) : \n                    self . _check_name ( \"variable\" , node . name , node ) \n        else : \n            if isinstance ( frame , astroid . FunctionDef ) : \n                if node . name in frame and node . name not in frame . argnames ( ) : \n                    if not _redefines_import ( node ) : \n                        self . _check_name ( \"variable\" , node . name , node ) \n            else : \n                if isinstance ( frame , astroid . ClassDef ) : \n                    if not list ( frame . local_attr_ancestors ( node . name ) ) : \n                        self . _check_name ( \"class_attribute\" , node . name , node ) "}
{"3900": "\ndef _check_docstring ( self , node_type , node , report_missing = True , confidence = interfaces . HIGH ) : \n    docstring = node . doc \n    if docstring is None : \n        if not report_missing : \n            return \n        lines = utils . get_node_last_lineno ( node ) - node . lineno \n        if node_type == \"module\" and not lines : \n            return \n        max_lines = self . config . docstring_min_length \n        if node_type != \"module\" and max_lines > - 1 and lines < max_lines : \n            return \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : \n            func = utils . safe_infer ( node . body [ 0 ] . value . func ) \n            if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : \n                if PY3K and func . bound . name == \"str\" : \n                    return \n                if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : \n                    return \n        self . add_message ( \"missing-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) \n    else : \n        if not docstring . strip ( ) : \n            self . stats [ \"undocumented_\" + node_type ] += 1 \n            self . add_message ( \"empty-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) "}
{"3905": "\ndef add_checker ( self , checker ) : \n    vcids = set ( ) \n    lcids = set ( ) \n    visits = self . visit_events \n    leaves = self . leave_events \n    for member in dir ( checker ) : \n        cid = member [ 6 : ] \n        if cid == \"default\" : \n            continue \n        if member . startswith ( \"visit_\" ) : \n            v_meth = getattr ( checker , member ) \n            if self . _is_method_enabled ( v_meth ) : \n                visits [ cid ] . append ( v_meth ) \n                vcids . add ( cid ) \n        else : \n            if member . startswith ( \"leave_\" ) : \n                l_meth = getattr ( checker , member ) \n                if self . _is_method_enabled ( l_meth ) : \n                    leaves [ cid ] . append ( l_meth ) \n                    lcids . add ( cid ) \n    visit_default = getattr ( checker , \"visit_default\" , None ) \n    if visit_default : \n        for cls in nodes . ALL_NODE_CLASSES : \n            cid = cls . __name__ . lower ( ) \n            if cid not in vcids : \n                visits [ cid ] . append ( visit_default ) "}
{"3922": "\ndef get ( self , access_token = None , refresh_token = None ) : \n    if access_token : \n        return self . query . filter_by ( access_token = access_token ) . first ( ) \n    else : \n        if refresh_token : \n            return self . query . filter_by ( refresh_token = refresh_token ) . first ( ) \n    return None "}
{"3923": "\ndef set ( self , token , request , * args , ** kwargs ) : \n    if hasattr ( request , 'user' ) and request . user : \n        user = request . user \n    else : \n        if self . current_user : \n            user = self . current_user ( ) \n    client = request . client \n    tokens = self . query . filter_by ( client_id = client . client_id , user_id = user . id ) . all ( ) \n    if tokens : \n        for tk in tokens : \n            self . session . delete ( tk ) \n        self . session . commit ( ) \n    expires_in = token . get ( 'expires_in' ) \n    expires = datetime . utcnow ( ) + timedelta ( seconds = expires_in ) \n    tok = self . model ( ** token ) \n    tok . expires = expires \n    tok . client_id = client . client_id \n    tok . user_id = user . id \n    self . session . add ( tok ) \n    self . session . commit ( ) \n    return tok "}
{"3926": "\ndef prepare_request ( uri , headers = None , data = None , method = None ) : \n    if headers is None : \n        headers = { } \n    if data and not method : \n        method = 'POST' \n    else : \n        if not method : \n            method = 'GET' \n    if method == 'GET' and data : \n        uri = add_params_to_uri ( uri , data ) \n        data = None \n    return uri , headers , data , method "}
{"3932": "\ndef handle_oauth2_response ( self , args ) : \n    client = self . make_client ( ) \n    remote_args = { 'code' : args . get ( 'code' ) , 'client_secret' : self . consumer_secret , 'redirect_uri' : session . get ( '%s_oauthredir' % self . name ) } \n    log . debug ( 'Prepare oauth2 remote args %r' , remote_args ) \n    remote_args . update ( self . access_token_params ) \n    headers = copy ( self . _access_token_headers ) \n    if self . access_token_method == 'POST' : \n        headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) \n        body = client . prepare_request_body ( ** remote_args ) \n        resp , content = self . http_request ( self . expand_url ( self . access_token_url ) , headers = headers , data = to_bytes ( body , self . encoding ) , method = self . access_token_method , ) \n    else : \n        if self . access_token_method == 'GET' : \n            qs = client . prepare_request_body ( ** remote_args ) \n            url = self . expand_url ( self . access_token_url ) \n            url += ( '?' in url and '&' or '?' ) + qs \n            resp , content = self . http_request ( url , headers = headers , method = self . access_token_method , ) \n        else : \n            raise OAuthException ( 'Unsupported access_token_method: %s' % self . access_token_method ) \n    data = parse_response ( resp , content , content_type = self . content_type ) \n    if resp . code not in ( 200 , 201 ) : \n        raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) \n    return data "}
{"3933": "\ndef authorized_response ( self , args = None ) : \n    if args is None : \n        args = request . args \n    if 'oauth_verifier' in args : \n        data = self . handle_oauth1_response ( args ) \n    else : \n        if 'code' in args : \n            data = self . handle_oauth2_response ( args ) \n        else : \n            data = self . handle_unknown_response ( ) \n    session . pop ( '%s_oauthtok' % self . name , None ) \n    session . pop ( '%s_oauthredir' % self . name , None ) \n    return data "}
{"3935": "\ndef _hash_token ( application , token ) : \n    if isinstance ( token , dict ) : \n        hashed_token = tuple ( sorted ( token . items ( ) ) ) \n    else : \n        if isinstance ( token , tuple ) : \n            hashed_token = token \n        else : \n            raise TypeError ( '%r is unknown type of token' % token ) \n    return ( application . __class__ . __name__ , application . name , hashed_token ) "}
{"3974": "\ndef validate_bearer_token ( self , token , scopes , request ) : \n    log . debug ( 'Validate bearer token %r' , token ) \n    tok = self . _tokengetter ( access_token = token ) \n    if not tok : \n        msg = 'Bearer token not found.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    if tok . expires is not None and datetime . datetime . utcnow ( ) > tok . expires : \n        msg = 'Bearer token is expired.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    if scopes and not set ( tok . scopes ) & set ( scopes ) : \n        msg = 'Bearer token scope not valid.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    request . access_token = tok \n    request . user = tok . user \n    request . scopes = scopes \n    if hasattr ( tok , 'client' ) : \n        request . client = tok . client \n    else : \n        if hasattr ( tok , 'client_id' ) : \n            request . client = self . _clientgetter ( tok . client_id ) \n    return True "}
{"3994": "\ndef remote_app ( self , name , version = None , ** kwargs ) : \n    if version is None : \n        if 'request_token_url' in kwargs : \n            version = '1' \n        else : \n            version = '2' \n    if version == '1' : \n        remote_app = OAuth1Application ( name , clients = cached_clients ) \n    else : \n        if version == '2' : \n            remote_app = OAuth2Application ( name , clients = cached_clients ) \n        else : \n            raise ValueError ( 'unkonwn version %r' % version ) \n    return self . add_remote_app ( remote_app , ** kwargs ) "}
{"4011": "\ndef use_privatekey_file ( self , keyfile , filetype = _UNSPECIFIED ) : \n    keyfile = _path_string ( keyfile ) \n    if filetype is _UNSPECIFIED : \n        filetype = FILETYPE_PEM \n    else : \n        if not isinstance ( filetype , integer_types ) : \n            raise TypeError ( \"filetype must be an integer\" ) \n    use_result = _lib . SSL_CTX_use_PrivateKey_file ( self . _context , keyfile , filetype ) \n    if not use_result : \n        self . _raise_passphrase_exception ( ) "}
{"4029": "\ndef set_tlsext_host_name ( self , name ) : \n    if not isinstance ( name , bytes ) : \n        raise TypeError ( \"name must be a byte string\" ) \n    else : \n        if b\"\\0\" in name : \n            raise TypeError ( \"name must not contain NUL byte\" ) \n    _lib . SSL_set_tlsext_host_name ( self . _ssl , name ) "}
{"4034": "\ndef shutdown ( self ) : \n    result = _lib . SSL_shutdown ( self . _ssl ) \n    if result < 0 : \n        self . _raise_ssl_error ( self . _ssl , result ) \n    else : \n        if result > 0 : \n            return True \n        else : \n            return False "}
{"4053": "\ndef _get_asn1_time ( timestamp ) : \n    string_timestamp = _ffi . cast ( 'ASN1_STRING*' , timestamp ) \n    if _lib . ASN1_STRING_length ( string_timestamp ) == 0 : \n        return None \n    else : \n        if ( _lib . ASN1_STRING_type ( string_timestamp ) == _lib . V_ASN1_GENERALIZEDTIME ) : \n            return _ffi . string ( _lib . ASN1_STRING_data ( string_timestamp ) ) \n        else : \n            generalized_timestamp = _ffi . new ( \"ASN1_GENERALIZEDTIME**\" ) \n            _lib . ASN1_TIME_to_generalizedtime ( timestamp , generalized_timestamp ) \n            if generalized_timestamp [ 0 ] == _ffi . NULL : \n                _untested_error ( \"ASN1_TIME_to_generalizedtime\" ) \n            else : \n                string_timestamp = _ffi . cast ( \"ASN1_STRING*\" , generalized_timestamp [ 0 ] ) \n                string_data = _lib . ASN1_STRING_data ( string_timestamp ) \n                string_result = _ffi . string ( string_data ) \n                _lib . ASN1_GENERALIZEDTIME_free ( generalized_timestamp [ 0 ] ) \n                return string_result "}
{"4055": "\ndef dump_publickey ( type , pkey ) : \n    bio = _new_mem_buf ( ) \n    if type == FILETYPE_PEM : \n        write_bio = _lib . PEM_write_bio_PUBKEY \n    else : \n        if type == FILETYPE_ASN1 : \n            write_bio = _lib . i2d_PUBKEY_bio \n        else : \n            raise ValueError ( \"type argument must be FILETYPE_PEM or FILETYPE_ASN1\" ) \n    result_code = write_bio ( bio , pkey . _pkey ) \n    if result_code != 1 : \n        _raise_current_error ( ) \n    return _bio_to_string ( bio ) "}
{"4056": "\ndef load_publickey ( type , buffer ) : \n    if isinstance ( buffer , _text_type ) : \n        buffer = buffer . encode ( \"ascii\" ) \n    bio = _new_mem_buf ( buffer ) \n    if type == FILETYPE_PEM : \n        evp_pkey = _lib . PEM_read_bio_PUBKEY ( bio , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n    else : \n        if type == FILETYPE_ASN1 : \n            evp_pkey = _lib . d2i_PUBKEY_bio ( bio , _ffi . NULL ) \n        else : \n            raise ValueError ( \"type argument must be FILETYPE_PEM or FILETYPE_ASN1\" ) \n    if evp_pkey == _ffi . NULL : \n        _raise_current_error ( ) \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _ffi . gc ( evp_pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = True \n    return pkey "}
{"4059": "\ndef dump_crl ( type , crl ) : \n    bio = _new_mem_buf ( ) \n    if type == FILETYPE_PEM : \n        ret = _lib . PEM_write_bio_X509_CRL ( bio , crl . _crl ) \n    else : \n        if type == FILETYPE_ASN1 : \n            ret = _lib . i2d_X509_CRL_bio ( bio , crl . _crl ) \n        else : \n            if type == FILETYPE_TEXT : \n                ret = _lib . X509_CRL_print ( bio , crl . _crl ) \n            else : \n                raise ValueError ( \"type argument must be FILETYPE_PEM, FILETYPE_ASN1, or \" \"FILETYPE_TEXT\" ) \n    assert ret == 1 \n    return _bio_to_string ( bio ) "}
{"4061": "\ndef generate_key ( self , type , bits ) : \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if not isinstance ( bits , int ) : \n        raise TypeError ( \"bits must be an integer\" ) \n    if type == TYPE_RSA : \n        if bits <= 0 : \n            raise ValueError ( \"Invalid number of bits\" ) \n        exponent = _lib . BN_new ( ) \n        exponent = _ffi . gc ( exponent , _lib . BN_free ) \n        _lib . BN_set_word ( exponent , _lib . RSA_F4 ) \n        rsa = _lib . RSA_new ( ) \n        result = _lib . RSA_generate_key_ex ( rsa , bits , exponent , _ffi . NULL ) \n        _openssl_assert ( result == 1 ) \n        result = _lib . EVP_PKEY_assign_RSA ( self . _pkey , rsa ) \n        _openssl_assert ( result == 1 ) \n    else : \n        if type == TYPE_DSA : \n            dsa = _lib . DSA_new ( ) \n            _openssl_assert ( dsa != _ffi . NULL ) \n            dsa = _ffi . gc ( dsa , _lib . DSA_free ) \n            res = _lib . DSA_generate_parameters_ex ( dsa , bits , _ffi . NULL , 0 , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n            _openssl_assert ( res == 1 ) \n            _openssl_assert ( _lib . DSA_generate_key ( dsa ) == 1 ) \n            _openssl_assert ( _lib . EVP_PKEY_set1_DSA ( self . _pkey , dsa ) == 1 ) \n        else : \n            raise Error ( \"No such key type\" ) \n    self . _initialized = True "}
{"4103": "\ndef set_reason ( self , reason ) : \n    if reason is None : \n        self . _delete_reason ( ) \n    else : \n        if not isinstance ( reason , bytes ) : \n            raise TypeError ( \"reason must be None or a byte string\" ) \n        else : \n            reason = reason . lower ( ) . replace ( b' ' , b'' ) \n            reason_code = [ r . lower ( ) for r in self . _crl_reasons ] . index ( reason ) \n            new_reason_ext = _lib . ASN1_ENUMERATED_new ( ) \n            _openssl_assert ( new_reason_ext != _ffi . NULL ) \n            new_reason_ext = _ffi . gc ( new_reason_ext , _lib . ASN1_ENUMERATED_free ) \n            set_result = _lib . ASN1_ENUMERATED_set ( new_reason_ext , reason_code ) \n            _openssl_assert ( set_result != _ffi . NULL ) \n            self . _delete_reason ( ) \n            add_result = _lib . X509_REVOKED_add1_ext_i2d ( self . _revoked , _lib . NID_crl_reason , new_reason_ext , 0 , 0 ) \n            _openssl_assert ( add_result == 1 ) "}
{"4151": "\ndef calc_pvalues ( query , gene_sets , background = 20000 , ** kwargs ) : \n    k = len ( query ) \n    query = set ( query ) \n    vals = [ ] \n    if isinstance ( background , set ) : \n        bg = len ( background ) \n        query = query . intersection ( background ) \n    else : \n        if isinstance ( background , int ) : \n            bg = background \n        else : \n            raise ValueError ( \"background should be set or int object\" ) \n    subsets = sorted ( gene_sets . keys ( ) ) \n    for s in subsets : \n        category = gene_sets . get ( s ) \n        m = len ( category ) \n        hits = query . intersection ( set ( category ) ) \n        x = len ( hits ) \n        if x < 1 : \n            continue \n        vals . append ( ( s , hypergeom . sf ( x - 1 , bg , m , k ) , x , m , hits ) ) \n    return zip ( * vals ) "}
{"4161": "\ndef ranking_metric_tensor ( exprs , method , permutation_num , pos , neg , classes , ascending , rs = np . random . RandomState ( ) ) : \n    G , S = exprs . shape \n    expr_mat = exprs . values . T \n    perm_cor_tensor = np . tile ( expr_mat , ( permutation_num + 1 , 1 , 1 ) ) \n    for arr in perm_cor_tensor [ : - 1 ] : \n        rs . shuffle ( arr ) \n    classes = np . array ( classes ) \n    pos = classes == pos \n    neg = classes == neg \n    pos_cor_mean = perm_cor_tensor [ : , pos , : ] . mean ( axis = 1 ) \n    neg_cor_mean = perm_cor_tensor [ : , neg , : ] . mean ( axis = 1 ) \n    pos_cor_std = perm_cor_tensor [ : , pos , : ] . std ( axis = 1 , ddof = 1 ) \n    neg_cor_std = perm_cor_tensor [ : , neg , : ] . std ( axis = 1 , ddof = 1 ) \n    if method == 'signal_to_noise' : \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / ( pos_cor_std + neg_cor_std ) \n    else : \n        if method == 't_test' : \n            denom = 1.0 / G \n            cor_mat = ( pos_cor_mean - neg_cor_mean ) / np . sqrt ( denom * pos_cor_std ** 2 + denom * neg_cor_std ** 2 ) \n        else : \n            if method == 'ratio_of_classes' : \n                cor_mat = pos_cor_mean / neg_cor_mean \n            else : \n                if method == 'diff_of_classes' : \n                    cor_mat = pos_cor_mean - neg_cor_mean \n                else : \n                    if method == 'log2_ratio_of_classes' : \n                        cor_mat = np . log2 ( pos_cor_mean / neg_cor_mean ) \n                    else : \n                        logging . error ( \"Please provide correct method name!!!\" ) \n                        sys . exit ( 0 ) \n    cor_mat_ind = cor_mat . argsort ( ) \n    cor_mat . sort ( ) \n    if ascending : \n        return cor_mat_ind , cor_mat \n    return cor_mat_ind [ : , : : - 1 ] , cor_mat [ : , : : - 1 ] "}
{"4162": "\ndef ranking_metric ( df , method , pos , neg , classes , ascending ) : \n    df_mean = df . groupby ( by = classes , axis = 1 ) . mean ( ) \n    df_std = df . groupby ( by = classes , axis = 1 ) . std ( ) \n    if method == 'signal_to_noise' : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / ( df_std [ pos ] + df_std [ neg ] ) \n    else : \n        if method == 't_test' : \n            ser = ( df_mean [ pos ] - df_mean [ neg ] ) / np . sqrt ( df_std [ pos ] ** 2 / len ( df_std ) + df_std [ neg ] ** 2 / len ( df_std ) ) \n        else : \n            if method == 'ratio_of_classes' : \n                ser = df_mean [ pos ] / df_mean [ neg ] \n            else : \n                if method == 'diff_of_classes' : \n                    ser = df_mean [ pos ] - df_mean [ neg ] \n                else : \n                    if method == 'log2_ratio_of_classes' : \n                        ser = np . log2 ( df_mean [ pos ] / df_mean [ neg ] ) \n                    else : \n                        logging . error ( \"Please provide correct method name!!!\" ) \n                        sys . exit ( 0 ) \n    ser = ser . sort_values ( ascending = ascending ) \n    return ser "}
{"4174": "\ndef _set_cores ( self ) : \n    cpu_num = cpu_count ( ) - 1 \n    if self . _processes > cpu_num : \n        cores = cpu_num \n    else : \n        if self . _processes < 1 : \n            cores = 1 \n        else : \n            cores = self . _processes \n    self . _processes = int ( cores ) "}
{"4175": "\ndef load_gmt ( self , gene_list , gmt ) : \n    if isinstance ( gmt , dict ) : \n        genesets_dict = gmt \n    else : \n        if isinstance ( gmt , str ) : \n            genesets_dict = self . parse_gmt ( gmt ) \n        else : \n            raise Exception ( \"Error parsing gmt parameter for gene sets\" ) \n    subsets = list ( genesets_dict . keys ( ) ) \n    self . n_genesets = len ( subsets ) \n    for subset in subsets : \n        subset_list = genesets_dict . get ( subset ) \n        if isinstance ( subset_list , set ) : \n            subset_list = list ( subset_list ) \n            genesets_dict [ subset ] = subset_list \n        tag_indicator = np . in1d ( gene_list , subset_list , assume_unique = True ) \n        tag_len = tag_indicator . sum ( ) \n        if self . min_size <= tag_len <= self . max_size : \n            continue \n        del genesets_dict [ subset ] \n    filsets_num = len ( subsets ) - len ( genesets_dict ) \n    self . _logger . info ( \"%04d gene_sets have been filtered out when max_size=%s and min_size=%s\" % ( filsets_num , self . max_size , self . min_size ) ) \n    if filsets_num == len ( subsets ) : \n        self . _logger . error ( \"No gene sets passed through filtering condition!!!, try new parameters again!\\n\" + \"Note: check gene name, gmt file format, or filtering size.\" ) \n        sys . exit ( 0 ) \n    self . _gmtdct = genesets_dict \n    return genesets_dict "}
{"4179": "\ndef _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : \n    res = OrderedDict ( ) \n    for gs , gseale , ind , RES in zipdata : \n        rdict = OrderedDict ( ) \n        rdict [ 'es' ] = gseale [ 0 ] \n        rdict [ 'nes' ] = gseale [ 1 ] \n        rdict [ 'pval' ] = gseale [ 2 ] \n        rdict [ 'fdr' ] = gseale [ 3 ] \n        rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) \n        rdict [ 'matched_size' ] = len ( ind ) \n        _genes = rank_metric . index . values [ ind ] \n        rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) \n        if self . module != 'ssgsea' : \n            if rdict [ 'es' ] > 0 : \n                idx = RES . argmax ( ) \n                ldg_pos = list ( filter ( lambda x : x <= idx , ind ) ) \n            else : \n                if rdict [ 'es' ] < 0 : \n                    idx = RES . argmin ( ) \n                    ldg_pos = list ( filter ( lambda x : x >= idx , ind ) ) \n                else : \n                    ldg_pos = ind \n            rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) \n        rdict [ 'RES' ] = RES \n        rdict [ 'hits_indices' ] = ind \n        res [ gs ] = rdict \n    self . results = res \n    res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) \n    res_df . index . name = 'Term' \n    res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = True ) \n    res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) \n    self . res2d = res_df \n    if self . _outdir is None : \n        return \n    out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) \n    if self . module == 'ssgsea' : \n        out = out . replace ( \".csv\" , \".txt\" ) \n        with open ( out , 'a' ) as f : \n            f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\\n' ) \n            f . write ( \"# might not proper for publication\\n\" ) \n            res_df . to_csv ( f , sep = '\\t' ) \n    else : \n        res_df . to_csv ( out ) \n    return "}
{"4180": "\ndef load_data ( self , cls_vec ) : \n    if isinstance ( self . data , pd . DataFrame ) : \n        exprs = self . data . copy ( ) \n        if exprs . index . dtype == 'O' : \n            exprs = exprs . reset_index ( ) \n    else : \n        if os . path . isfile ( self . data ) : \n            if self . data . endswith ( \"gct\" ) : \n                exprs = pd . read_csv ( self . data , skiprows = 1 , comment = '#' , sep = \"\\t\" ) \n            else : \n                exprs = pd . read_csv ( self . data , comment = '#' , sep = \"\\t\" ) \n        else : \n            raise Exception ( 'Error parsing gene expression DataFrame!' ) \n    if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : \n        self . _logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) \n        exprs . drop_duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) \n    if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : \n        self . _logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) \n        exprs . dropna ( how = 'all' , inplace = True ) \n        exprs = exprs . fillna ( 0 ) \n    exprs . set_index ( keys = exprs . columns [ 0 ] , inplace = True ) \n    df = exprs . select_dtypes ( include = [ np . number ] ) \n    df_std = df . groupby ( by = cls_vec , axis = 1 ) . std ( ) \n    df = df [ ~ df_std . isin ( [ 0 ] ) . any ( axis = 1 ) ] \n    df = df + 0.00001 \n    return df "}
{"4188": "\ndef parse_genesets ( self ) : \n    enrichr_library = self . get_libraries ( ) \n    if isinstance ( self . gene_sets , list ) : \n        gss = self . gene_sets \n    else : \n        if isinstance ( self . gene_sets , str ) : \n            gss = [ g . strip ( ) for g in self . gene_sets . strip ( ) . split ( \",\" ) ] \n        else : \n            if isinstance ( self . gene_sets , dict ) : \n                gss = [ self . gene_sets ] \n            else : \n                raise Exception ( \"Error parsing enrichr libraries, please provided corrected one\" ) \n    gss_exist = [ ] \n    for g in gss : \n        if isinstance ( g , dict ) : \n            gss_exist . append ( g ) \n            continue \n        if isinstance ( g , str ) : \n            if g in enrichr_library : \n                gss_exist . append ( g ) \n                continue \n            if g . lower ( ) . endswith ( \".gmt\" ) and os . path . exists ( g ) : \n                self . _logger . info ( \"User Defined gene sets is given: %s\" % g ) \n                with open ( g ) as genesets : \n                    g_dict = { line . strip ( ) . split ( \"\\t\" ) [ 0 ] : line . strip ( ) . split ( \"\\t\" ) [ 2 : ] for line in genesets . readlines ( ) } \n                gss_exist . append ( g_dict ) \n    return gss_exist "}
{"4189": "\ndef parse_genelists ( self ) : \n    if isinstance ( self . gene_list , list ) : \n        genes = self . gene_list \n    else : \n        if isinstance ( self . gene_list , pd . DataFrame ) : \n            if self . gene_list . shape [ 1 ] >= 3 : \n                genes = self . gene_list . iloc [ : , : 3 ] . apply ( lambda x : \"\\t\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n            else : \n                if self . gene_list . shape [ 1 ] == 2 : \n                    genes = self . gene_list . apply ( lambda x : \",\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n                else : \n                    genes = self . gene_list . squeeze ( ) . tolist ( ) \n        else : \n            if isinstance ( self . gene_list , pd . Series ) : \n                genes = self . gene_list . squeeze ( ) . tolist ( ) \n            else : \n                genes = [ ] \n                with open ( self . gene_list ) as f : \n                    for gene in f : \n                        genes . append ( gene . strip ( ) ) \n    self . _isezid = all ( map ( self . _is_entrez_id , genes ) ) \n    if self . _isezid : \n        self . _gls = set ( map ( int , self . _gls ) ) \n    else : \n        self . _gls = genes \n    return '\\n' . join ( genes ) "}
{"4192": "\ndef get_background ( self ) : \n    if os . path . isfile ( self . background ) : \n        with open ( self . background ) as b : \n            bg2 = b . readlines ( ) \n        bg = [ g . strip ( ) for g in bg2 ] \n        return set ( bg ) \n    DB_FILE = resource_filename ( \"gseapy\" , \"data/{}.background.genes.txt\" . format ( self . background ) ) \n    filename = os . path . join ( DEFAULT_CACHE_PATH , \"{}.background.genes.txt\" . format ( self . background ) ) \n    if os . path . exists ( filename ) : \n        df = pd . read_csv ( filename , sep = \"\\t\" ) \n    else : \n        if os . path . exists ( DB_FILE ) : \n            df = pd . read_csv ( DB_FILE , sep = \"\\t\" ) \n        else : \n            self . _logger . warning ( \"Downloading %s for the first time. It might take a couple of miniutes.\" % self . background ) \n            bm = Biomart ( ) \n            df = bm . query ( dataset = self . background ) \n            df . dropna ( subset = [ 'go_id' ] , inplace = True ) \n    self . _logger . info ( \"using all annotated genes with GO_ID as background genes\" ) \n    df . dropna ( subset = [ 'entrezgene' ] , inplace = True ) \n    if self . _isezid : \n        bg = df [ 'entrezgene' ] . astype ( int ) \n    else : \n        bg = df [ 'external_gene_name' ] \n    return set ( bg ) "}
{"4202": "\ndef write_filter ( script , filter_xml ) : \n    if isinstance ( script , mlx . FilterScript ) : \n        script . filters . append ( filter_xml ) \n    else : \n        if isinstance ( script , str ) : \n            script_file = open ( script , 'a' ) \n            script_file . write ( filter_xml ) \n            script_file . close ( ) \n        else : \n            print ( filter_xml ) \n    return None "}
{"4209": "\ndef rotate ( script , axis = 'z' , angle = 0.0 ) : \n    angle = math . radians ( angle ) \n    if axis . lower ( ) == 'x' : \n        vert_function ( script , x_func = 'x' , y_func = 'y*cos({angle})-z*sin({angle})' . format ( angle = angle ) , z_func = 'y*sin({angle})+z*cos({angle})' . format ( angle = angle ) ) \n    else : \n        if axis . lower ( ) == 'y' : \n            vert_function ( script , x_func = 'z*sin({angle})+x*cos({angle})' . format ( angle = angle ) , y_func = 'y' , z_func = 'z*cos({angle})-x*sin({angle})' . format ( angle = angle ) ) \n        else : \n            if axis . lower ( ) == 'z' : \n                vert_function ( script , x_func = 'x*cos({angle})-y*sin({angle})' . format ( angle = angle ) , y_func = 'x*sin({angle})+y*cos({angle})' . format ( angle = angle ) , z_func = 'z' ) \n            else : \n                print ( 'Axis name is not valid; exiting ...' ) \n                sys . exit ( 1 ) \n    return None "}
{"4231": "\ndef handle_error ( program_name , cmd , log = None ) : \n    print ( '\\nHouston, we have a problem.' , '\\n%s did not finish successfully. Review the log' % program_name , 'file and the input file(s) to see what went wrong.' ) \n    print ( '%s command: \"%s\"' % ( program_name , cmd ) ) \n    if log is not None : \n        print ( 'log: \"%s\"' % log ) \n    print ( 'Where do we go from here?' ) \n    print ( ' r  - retry running %s (probably after' % program_name , 'you\\'ve fixed any problems with the input files)' ) \n    print ( ' c  - continue on with the script (probably after' , 'you\\'ve manually re-run and generated the desired' , 'output file(s)' ) \n    print ( ' x  - exit, keeping the TEMP3D files and log' ) \n    print ( ' xd - exit, deleting the TEMP3D files and log' ) \n    while True : \n        choice = input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if choice == 'x' : \n        print ( 'Exiting ...' ) \n        sys . exit ( 1 ) \n    else : \n        if choice == 'xd' : \n            print ( 'Deleting TEMP3D* and log files and exiting ...' ) \n            util . delete_all ( 'TEMP3D*' ) \n            if log is not None : \n                os . remove ( log ) \n            sys . exit ( 1 ) \n        else : \n            if choice == 'c' : \n                print ( 'Continuing on ...' ) \n                break_now = True \n            else : \n                if choice == 'r' : \n                    print ( 'Retrying %s cmd ...' % program_name ) \n                    break_now = False \n    return break_now "}
{"4232": "\ndef begin ( script = 'TEMP3D_default.mlx' , file_in = None , mlp_in = None ) : \n    script_file = open ( script , 'w' ) \n    script_file . write ( '' . join ( [ '<!DOCTYPE FilterScript>\\n' , '<FilterScript>\\n' ] ) ) \n    script_file . close ( ) \n    current_layer = - 1 \n    last_layer = - 1 \n    stl = False \n    if mlp_in is not None : \n        if not isinstance ( mlp_in , list ) : \n            mlp_in = [ mlp_in ] \n        for val in mlp_in : \n            tree = ET . parse ( val ) \n            for elem in tree . iter ( tag = 'MLMesh' ) : \n                filename = ( elem . attrib [ 'filename' ] ) \n                current_layer += 1 \n                last_layer += 1 \n                if os . path . splitext ( filename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : \n                    layers . change ( script , current_layer ) \n                    clean . merge_vert ( script ) \n                    stl = True \n    if file_in is not None : \n        if not isinstance ( file_in , list ) : \n            file_in = [ file_in ] \n        for val in file_in : \n            current_layer += 1 \n            last_layer += 1 \n            if os . path . splitext ( val ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : \n                layers . change ( script , current_layer ) \n                clean . merge_vert ( script ) \n                stl = True \n    if stl : \n        layers . change ( script , last_layer ) \n    else : \n        if last_layer == - 1 : \n            file_in = [ 'TEMP3D.xyz' ] \n            file_in_descriptor = open ( file_in [ 0 ] , 'w' ) \n            file_in_descriptor . write ( '0 0 0' ) \n            file_in_descriptor . close ( ) \n            layers . delete ( script ) \n    return current_layer , last_layer "}
{"4236": "\ndef run_script ( self , log = None , ml_log = None , mlp_out = None , overwrite = False , file_out = None , output_mask = None , script_file = None , print_meshlabserver_output = True ) : \n    temp_script = False \n    temp_ml_log = False \n    if self . __no_file_in : \n        temp_file_in_file = tempfile . NamedTemporaryFile ( delete = False , suffix = '.xyz' , dir = os . getcwd ( ) ) \n        temp_file_in_file . write ( b'0 0 0' ) \n        temp_file_in_file . close ( ) \n        self . file_in = [ temp_file_in_file . name ] \n    if not self . filters : \n        script_file = None \n    else : \n        if script_file is None : \n            temp_script = True \n            temp_script_file = tempfile . NamedTemporaryFile ( delete = False , suffix = '.mlx' ) \n            temp_script_file . close ( ) \n            self . save_to_file ( temp_script_file . name ) \n            script_file = temp_script_file . name \n    if ( self . parse_geometry or self . parse_topology or self . parse_hausdorff ) and ( ml_log is None ) : \n        temp_ml_log = True \n        ml_log_file = tempfile . NamedTemporaryFile ( delete = False , suffix = '.txt' ) \n        ml_log_file . close ( ) \n        ml_log = ml_log_file . name \n    if file_out is None : \n        file_out = self . file_out \n    run ( script = script_file , log = log , ml_log = ml_log , mlp_in = self . mlp_in , mlp_out = mlp_out , overwrite = overwrite , file_in = self . file_in , file_out = file_out , output_mask = output_mask , ml_version = self . ml_version , print_meshlabserver_output = print_meshlabserver_output ) \n    if self . parse_geometry : \n        self . geometry = compute . parse_geometry ( ml_log , log , print_output = print_meshlabserver_output ) \n    if self . parse_topology : \n        self . topology = compute . parse_topology ( ml_log , log , print_output = print_meshlabserver_output ) \n    if self . parse_hausdorff : \n        self . hausdorff_distance = compute . parse_hausdorff ( ml_log , log , print_output = print_meshlabserver_output ) \n    if self . __no_file_in : \n        os . remove ( temp_file_in_file . name ) \n    if temp_script : \n        os . remove ( temp_script_file . name ) \n    if temp_ml_log : \n        os . remove ( ml_log_file . name ) "}
{"4240": "\ndef mesh_element ( script , sample_num = 1000 , element = 'VERT' ) : \n    if element . lower ( ) == 'vert' : \n        element_num = 0 \n    else : \n        if element . lower ( ) == 'edge' : \n            element_num = 1 \n        else : \n            if element . lower ( ) == 'face' : \n                element_num = 2 \n    filter_xml = '' . join ( [ '  <filter name=\"Mesh Element Subsampling\">\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( element_num ) , 'description=\"Element to sample:\" ' , 'enum_val0=\"Vertex\" ' , 'enum_val1=\"Edge\" ' , 'enum_val2=\"Face\" ' , 'enum_cardinality=\"3\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Sampled Mesh' ) \n    return None "}
{"4241": "\ndef clustered_vert ( script , cell_size = 1.0 , strategy = 'AVERAGE' , selected = False ) : \n    if strategy . lower ( ) == 'average' : \n        strategy_num = 0 \n    else : \n        if strategy . lower ( ) == 'center' : \n            strategy_num = 1 \n    filter_xml = '' . join ( [ '  <filter name=\"Clustered Vertex Subsampling\">\\n' , '    <Param name=\"Threshold\" ' , 'value=\"{}\" ' . format ( cell_size ) , 'description=\"Cell Size\" ' , 'min=\"0\" ' , 'max=\"1000\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( strategy_num ) , 'description=\"Representative Strategy:\" ' , 'enum_val0=\"Average\" ' , 'enum_val1=\"Closest to center\" ' , 'enum_cardinality=\"2\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Selected\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cluster Samples' ) \n    return None "}
{"4246": "\ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : \n    topology = { 'manifold' : True , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = False \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2 ] \n                if topology [ 'genus' ] != 'undefined' : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2 ] \n                if topology [ 'hole_num' ] == 'a' : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        else : \n            if print_output : \n                print ( '{:16} = {}' . format ( key , value ) ) \n    return topology "}
{"4247": "\ndef parse_hausdorff ( ml_log , log = None , print_output = False ) : \n    hausdorff_distance = { \"min_distance\" : 0.0 , \"max_distance\" : 0.0 , \"mean_distance\" : 0.0 , \"rms_distance\" : 0.0 , \"number_points\" : 0 } \n    with open ( ml_log ) as fread : \n        result = fread . readlines ( ) \n        data = \"\" \n        for idx , line in enumerate ( result ) : \n            m = re . match ( r\"\\s*Sampled (\\d+) pts.*\" , line ) \n            if m is not None : \n                hausdorff_distance [ \"number_points\" ] = int ( m . group ( 1 ) ) \n            if 'Hausdorff Distance computed' in line : \n                data = result [ idx + 2 ] \n        m = re . match ( r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\" , data ) \n        hausdorff_distance [ \"min_distance\" ] = float ( m . group ( 1 ) ) \n        hausdorff_distance [ \"max_distance\" ] = float ( m . group ( 2 ) ) \n        hausdorff_distance [ \"mean_distance\" ] = float ( m . group ( 3 ) ) \n        hausdorff_distance [ \"rms_distance\" ] = float ( m . group ( 4 ) ) \n        for key , value in hausdorff_distance . items ( ) : \n            if log is not None : \n                log_file = open ( log , 'a' ) \n                log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n                log_file . close ( ) \n            else : \n                if print_output : \n                    print ( '{:16} = {}' . format ( key , value ) ) \n        return hausdorff_distance "}
{"4250": "\ndef cyclic_rainbow ( script , direction = 'sphere' , start_pt = ( 0 , 0 , 0 ) , amplitude = 255 / 2 , center = 255 / 2 , freq = 0.8 , phase = ( 0 , 120 , 240 , 0 ) , alpha = False ) : \n    start_pt = util . make_list ( start_pt , 3 ) \n    amplitude = util . make_list ( amplitude , 4 ) \n    center = util . make_list ( center , 4 ) \n    freq = util . make_list ( freq , 4 ) \n    phase = util . make_list ( phase , 4 ) \n    if direction . lower ( ) == 'sphere' : \n        increment = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)' . format ( start_pt [ 0 ] , start_pt [ 1 ] , start_pt [ 2 ] ) \n    else : \n        if direction . lower ( ) == 'x' : \n            increment = 'x - {}' . format ( start_pt [ 0 ] ) \n        else : \n            if direction . lower ( ) == 'y' : \n                increment = 'y - {}' . format ( start_pt [ 1 ] ) \n            else : \n                if direction . lower ( ) == 'z' : \n                    increment = 'z - {}' . format ( start_pt [ 2 ] ) \n                else : \n                    increment = direction \n    red_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 0 ] , i = increment , p = math . radians ( phase [ 0 ] ) , a = amplitude [ 0 ] , c = center [ 0 ] ) \n    green_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 1 ] , i = increment , p = math . radians ( phase [ 1 ] ) , a = amplitude [ 1 ] , c = center [ 1 ] ) \n    blue_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 2 ] , i = increment , p = math . radians ( phase [ 2 ] ) , a = amplitude [ 2 ] , c = center [ 2 ] ) \n    if alpha : \n        alpha_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 3 ] , i = increment , p = math . radians ( phase [ 3 ] ) , a = amplitude [ 3 ] , c = center [ 3 ] ) \n    else : \n        alpha_func = 255 \n    function ( script , red = red_func , green = green_func , blue = blue_func , alpha = alpha_func ) \n    return None "}
{"4259": "\ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) \n    if fext != 'obj' : \n        print ( 'Input file must be obj. Exiting ...' ) \n        sys . exit ( 1 ) \n    fread = open ( fbasename , 'r' ) \n    first = True \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if element == 'v' : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        else : \n            if element == 'l' : \n                p1 = x_co \n                p2 = y_co \n                line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4306": "\ndef profiler_handler ( uri ) : \n    if uri == 'main' : \n        runner . run ( show_guestbook , 'cmhp' ) \n    else : \n        if uri == 'add' : \n            runner . run ( add_entry , 'cmhp' ) \n    return flask . redirect ( '/' ) "}
{"4351": "\ndef block ( rdd , bsize = - 1 , dtype = None ) : \n    try : \n        entry = rdd . first ( ) \n    except IndexError : \n        return rdd \n    if isinstance ( entry , dict ) : \n        rdd = rdd . map ( lambda x : list ( x . values ( ) ) ) \n        return DictRDD ( rdd , list ( entry . keys ( ) ) , bsize , dtype ) \n    else : \n        if isinstance ( entry , tuple ) : \n            return DictRDD ( rdd , bsize = bsize , dtype = dtype ) \n        else : \n            if sp . issparse ( entry ) : \n                return SparseRDD ( rdd , bsize ) \n            else : \n                if isinstance ( entry , np . ndarray ) : \n                    return ArrayRDD ( rdd , bsize ) \n                else : \n                    return BlockRDD ( rdd , bsize , dtype ) "}
{"4352": "\ndef transform ( self , fn , dtype = None , * args , ** kwargs ) : \n    rdd = self . _rdd . map ( fn ) \n    if dtype is None : \n        return self . __class__ ( rdd , noblock = True , ** self . get_params ( ) ) \n    if dtype is np . ndarray : \n        return ArrayRDD ( rdd , bsize = self . bsize , noblock = True ) \n    else : \n        if dtype is sp . spmatrix : \n            return SparseRDD ( rdd , bsize = self . bsize , noblock = True ) \n        else : \n            return BlockRDD ( rdd , bsize = self . bsize , dtype = dtype , noblock = True ) "}
{"4355": "\ndef transform ( self , fn , column = None , dtype = None ) : \n    dtypes = self . dtype \n    if column is None : \n        indices = list ( range ( len ( self . columns ) ) ) \n    else : \n        if not type ( column ) in ( list , tuple ) : \n            column = [ column ] \n        indices = [ self . columns . index ( c ) for c in column ] \n    if dtype is not None : \n        if not type ( dtype ) in ( list , tuple ) : \n            dtype = [ dtype ] \n        dtypes = [ dtype [ indices . index ( i ) ] if i in indices else t for i , t in enumerate ( self . dtype ) ] \n    def mapper ( values ) : \n        result = fn ( * [ values [ i ] for i in indices ] ) \n        if len ( indices ) == 1 : \n            result = ( result , ) \n        else : \n            if not isinstance ( result , ( tuple , list ) ) : \n                raise ValueError ( \"Transformer function must return an\" \" iterable!\" ) \n            else : \n                if len ( result ) != len ( indices ) : \n                    raise ValueError ( \"Transformer result's length must be\" \" equal to the given columns length!\" ) \n        return tuple ( result [ indices . index ( i ) ] if i in indices else v for i , v in enumerate ( values ) ) \n    return DictRDD ( self . _rdd . map ( mapper ) , columns = self . columns , dtype = dtypes , bsize = self . bsize , noblock = True ) "}
{"4362": "\ndef execute_over_ssh ( cmd , ssh , cwd = None , shell = 'bash' ) : \n    port = None \n    parts = ssh . split ( ':' , 1 ) \n    if len ( parts ) > 1 and not parts [ 1 ] . isdigit ( ) : \n        raise InvalidConfig ( extra_body = 'Invalid port number on ssh config: {}' . format ( parts [ 1 ] ) ) \n    else : \n        if len ( parts ) > 1 : \n            port = parts [ 1 ] \n    quoted_cmd = ' ' . join ( [ x . replace ( \"'\" , \"\"\"'\"'\"'\"\"\" ) for x in cmd . split ( ' ' ) ] ) \n    remote_cmd = ' ' . join ( [ ' ' . join ( get_shell ( shell ) ) , ' ' . join ( [ EXECUTE_SHELL_PARAM , \"'\" , ' ' . join ( ( [ 'cd' , cwd , ';' ] if cwd else [ ] ) + [ quoted_cmd ] ) , \"'\" ] ) ] , ) \n    return [ 'ssh' , parts [ 0 ] ] + ( [ '-p' , port ] if port else [ ] ) + [ '-C' ] + [ remote_cmd ] "}
{"4368": "\ndef pkt_text ( pkt ) : \n    if pkt . src . upper ( ) in BANNED_DEVICES : \n        body = '' \n    else : \n        if pkt . src . upper ( ) [ : 8 ] in AMAZON_DEVICES : \n            body = '{} (Amazon Device)' . format ( pkt . src ) \n        else : \n            body = pkt . src \n    return body "}
{"4376": "\ndef convert ( self , txn ) : \n    ofxid = self . mk_ofxid ( txn . id ) \n    metadata = { } \n    posting_metadata = { \"ofxid\" : ofxid } \n    if isinstance ( txn , OfxTransaction ) : \n        posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) \n        return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) \n    else : \n        if isinstance ( txn , InvestmentTransaction ) : \n            acct1 = self . name \n            acct2 = self . name \n            posting1 = None \n            posting2 = None \n            security = self . maybe_get_ticker ( txn . security ) \n            if isinstance ( txn . type , str ) : \n                if re . match ( '^(buy|sell)' , txn . type ) : \n                    acct2 = self . unknownaccount or 'Assets:Unknown' \n                else : \n                    if txn . type == 'transfer' : \n                        acct2 = 'Transfer' \n                    else : \n                        if txn . type == 'reinvest' : \n                            acct2 = 'Income:Interest' \n                        else : \n                            if txn . type == 'income' and txn . income_type == 'DIV' : \n                                metadata [ 'dividend_from' ] = security \n                                acct2 = 'Income:Dividends' \n                                posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) \n                                posting2 = posting1 . clone_inverted ( acct2 ) \n                            else : \n                                pass \n            else : \n                if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : \n                    acct2 = self . unknownaccount or 'Assets:Unknown' \n                else : \n                    if ( txn . type == 2 ) : \n                        acct2 = 'Income:Interest' \n                    else : \n                        pass \n            aux_date = None \n            if txn . settleDate is not None and txn . settleDate != txn . tradeDate : \n                aux_date = txn . settleDate \n            if posting1 is None and posting2 is None : \n                posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = True ) , metadata = posting_metadata ) \n                posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = True ) ) \n            else : \n                pass \n            return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] ) "}
{"4377": "\ndef find_ledger_file ( ledgerrcpath = None ) : \n    if ledgerrcpath is None : \n        ledgerrcpath = os . path . abspath ( os . path . expanduser ( \"~/.ledgerrc\" ) ) \n    if \"LEDGER_FILE\" in os . environ : \n        return os . path . abspath ( os . path . expanduser ( os . environ [ \"LEDGER_FILE\" ] ) ) \n    else : \n        if os . path . exists ( ledgerrcpath ) : \n            ledgerrc = open ( ledgerrcpath ) \n            for line in ledgerrc . readlines ( ) : \n                md = re . match ( r\"--file\\s+([^\\s]+).*\" , line ) \n                if md is not None : \n                    return os . path . abspath ( os . path . expanduser ( md . group ( 1 ) ) ) \n        else : \n            return None "}
{"4381": "\ndef default ( cls ) -> 'PrecalculatedTextMeasurer' : \n    if cls . _default_cache is not None : \n        return cls . _default_cache \n    if pkg_resources . resource_exists ( __name__ , 'default-widths.json.xz' ) : \n        import lzma \n        with pkg_resources . resource_stream ( __name__ , 'default-widths.json.xz' ) as f : \n            with lzma . open ( f , \"rt\" ) as g : \n                cls . _default_cache = PrecalculatedTextMeasurer . from_json ( cast ( TextIO , g ) ) \n                return cls . _default_cache \n    else : \n        if pkg_resources . resource_exists ( __name__ , 'default-widths.json' ) : \n            with pkg_resources . resource_stream ( __name__ , 'default-widths.json' ) as f : \n                cls . _default_cache = PrecalculatedTextMeasurer . from_json ( io . TextIOWrapper ( f , encoding = 'utf-8' ) ) \n                return cls . _default_cache \n        else : \n            raise ValueError ( 'could not load default-widths.json' ) "}
{"4400": "\ndef _onIncomingMessageReceived ( self , conn , message ) : \n    if self . _syncObj . encryptor and not conn . sendRandKey : \n        conn . sendRandKey = message \n        conn . recvRandKey = os . urandom ( 32 ) \n        conn . send ( conn . recvRandKey ) \n        return \n    if isinstance ( message , list ) : \n        done = False \n        try : \n            if message [ 0 ] == 'status' : \n                conn . send ( self . _syncObj . getStatus ( ) ) \n                done = True \n            else : \n                if message [ 0 ] == 'add' : \n                    self . _syncObj . addNodeToCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'ADD' , arg = message [ 1 ] ) ) \n                    done = True \n                else : \n                    if message [ 0 ] == 'remove' : \n                        if message [ 1 ] == self . _selfNode . address : \n                            conn . send ( 'FAIL REMOVE ' + message [ 1 ] ) \n                        else : \n                            self . _syncObj . removeNodeFromCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'REMOVE' , arg = message [ 1 ] ) ) \n                        done = True \n                    else : \n                        if message [ 0 ] == 'set_version' : \n                            self . _syncObj . setCodeVersion ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'SET_VERSION' , arg = str ( message [ 1 ] ) ) ) \n                            done = True \n        except Exception as e : \n            conn . send ( str ( e ) ) \n            done = True \n        if done : \n            return \n    node = self . _nodeAddrToNode [ message ] if message in self . _nodeAddrToNode else None \n    if node is None and message != 'readonly' : \n        conn . disconnect ( ) \n        self . _unknownConnections . discard ( conn ) \n        return \n    readonly = node is None \n    if readonly : \n        nodeId = str ( self . _readonlyNodesCounter ) \n        node = Node ( nodeId ) \n        self . _readonlyNodes . add ( node ) \n        self . _readonlyNodesCounter += 1 \n    self . _unknownConnections . discard ( conn ) \n    self . _connections [ node ] = conn \n    conn . setOnMessageReceivedCallback ( functools . partial ( self . _onMessageReceived , node ) ) \n    if not readonly : \n        self . _onNodeConnected ( node ) \n    else : \n        self . _onReadonlyNodeConnected ( node ) "}
{"4418": "\ndef token_required ( view_func ) : \n    def _parse_auth_header ( auth_header ) : \n        reg = re . compile ( '(\\w+)[=] ?\"?([\\w-]+)\"?' ) \n        header_dict = dict ( reg . findall ( auth_header ) ) \n        return header_dict [ 'Token' ] \n    def _get_passed_token ( request ) : \n        try : \n            auth_header = request . META [ 'HTTP_AUTHORIZATION' ] \n            token = _parse_auth_header ( auth_header ) \n        except KeyError : \n            token = request . GET . get ( settings . WATCHMAN_TOKEN_NAME ) \n        return token \n    def _validate_token ( request ) : \n        if settings . WATCHMAN_TOKENS : \n            watchman_tokens = settings . WATCHMAN_TOKENS . split ( ',' ) \n        else : \n            if settings . WATCHMAN_TOKEN : \n                watchman_tokens = [ settings . WATCHMAN_TOKEN , ] \n            else : \n                return True \n        return _get_passed_token ( request ) in watchman_tokens \n    \n    @ csrf_exempt \n    @ wraps ( view_func ) \n    def _wrapped_view ( request , * args , ** kwargs ) : \n        if _validate_token ( request ) : \n            return view_func ( request , * args , ** kwargs ) \n        return HttpResponseForbidden ( ) \n    return _wrapped_view "}
{"4424": "\ndef extract_xml ( input_ ) : \n    if type ( input_ ) == str : \n        file_object = open ( input_ , \"rb\" ) \n    else : \n        if type ( input_ ) == bytes : \n            file_object = BytesIO ( input_ ) \n        else : \n            file_object = input_ \n    try : \n        header = file_object . read ( 6 ) \n        file_object . seek ( 0 ) \n        if header . startswith ( MAGIC_ZIP ) : \n            _zip = zipfile . ZipFile ( file_object ) \n            xml = _zip . open ( _zip . namelist ( ) [ 0 ] ) . read ( ) . decode ( ) \n        else : \n            if header . startswith ( MAGIC_GZIP ) : \n                xml = GzipFile ( fileobj = file_object ) . read ( ) . decode ( ) \n            else : \n                if header . startswith ( MAGIC_XML ) : \n                    xml = file_object . read ( ) . decode ( ) \n                else : \n                    file_object . close ( ) \n                    raise InvalidAggregateReport ( \"Not a valid zip, gzip, or xml file\" ) \n        file_object . close ( ) \n    except UnicodeDecodeError : \n        raise InvalidAggregateReport ( \"File objects must be opened in binary \" \"(rb) mode\" ) \n    except Exception as error : \n        raise InvalidAggregateReport ( \"Invalid archive file: {0}\" . format ( error . __str__ ( ) ) ) \n    return xml "}
{"4427": "\ndef parse_report_file ( input_ , nameservers = None , dns_timeout = 2.0 , strip_attachment_payloads = False , parallel = False ) : \n    if type ( input_ ) == str : \n        file_object = open ( input_ , \"rb\" ) \n    else : \n        if type ( input_ ) == bytes : \n            file_object = BytesIO ( input_ ) \n        else : \n            file_object = input_ \n    content = file_object . read ( ) \n    try : \n        report = parse_aggregate_report_file ( content , nameservers = nameservers , dns_timeout = dns_timeout , parallel = parallel ) \n        results = OrderedDict ( [ ( \"report_type\" , \"aggregate\" ) , ( \"report\" , report ) ] ) \n    except InvalidAggregateReport : \n        try : \n            sa = strip_attachment_payloads \n            results = parse_report_email ( content , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) \n        except InvalidDMARCReport : \n            raise InvalidDMARCReport ( \"Not a valid aggregate or forensic \" \"report\" ) \n    return results "}
{"4453": "\ndef _connect_command ( self ) : \n    options = { \"verbose\" : self . options [ \"verbose\" ] , \"pedantic\" : self . options [ \"pedantic\" ] , \"lang\" : __lang__ , \"version\" : __version__ , \"protocol\" : PROTOCOL } \n    if \"auth_required\" in self . _server_info : \n        if self . _server_info [ \"auth_required\" ] : \n            if self . options [ \"user\" ] is not None and self . options [ \"password\" ] is not None : \n                options [ \"user\" ] = self . options [ \"user\" ] \n                options [ \"pass\" ] = self . options [ \"password\" ] \n            else : \n                if self . options [ \"token\" ] is not None : \n                    options [ \"auth_token\" ] = self . options [ \"token\" ] \n                else : \n                    if self . _current_server . uri . password is None : \n                        options [ \"auth_token\" ] = self . _current_server . uri . username \n                    else : \n                        options [ \"user\" ] = self . _current_server . uri . username \n                        options [ \"pass\" ] = self . _current_server . uri . password \n    if self . options [ \"name\" ] is not None : \n        options [ \"name\" ] = self . options [ \"name\" ] \n    if self . options [ \"no_echo\" ] is not None : \n        options [ \"echo\" ] = not self . options [ \"no_echo\" ] \n    connect_opts = json . dumps ( options , sort_keys = True ) \n    return b'' . join ( [ CONNECT_OP + _SPC_ + connect_opts . encode ( ) + _CRLF_ ] ) "}
{"4457": "\ndef _process_connect_init ( self ) : \n    self . _status = Client . CONNECTING \n    connection_completed = self . _io_reader . readline ( ) \n    info_line = yield from asyncio . wait_for ( connection_completed , self . options [ \"connect_timeout\" ] ) \n    if INFO_OP not in info_line : \n        raise NatsError ( \"nats: empty response from server when expecting INFO message\" ) \n    _ , info = info_line . split ( INFO_OP + _SPC_ , 1 ) \n    try : \n        srv_info = json . loads ( info . decode ( ) ) \n    except : \n        raise NatsError ( \"nats: info message, json parse error\" ) \n    self . _process_info ( srv_info ) \n    self . _server_info = srv_info \n    if 'max_payload' in self . _server_info : \n        self . _max_payload = self . _server_info [ \"max_payload\" ] \n    if 'tls_required' in self . _server_info and self . _server_info [ 'tls_required' ] : \n        ssl_context = None \n        if \"tls\" in self . options : \n            ssl_context = self . options . get ( 'tls' ) \n        else : \n            if self . _current_server . uri . scheme == 'tls' : \n                ssl_context = ssl . create_default_context ( ) \n            else : \n                raise NatsError ( 'nats: no ssl context provided' ) \n        transport = self . _io_writer . transport \n        sock = transport . get_extra_info ( 'socket' ) \n        if not sock : \n            raise NatsError ( 'nats: unable to get socket' ) \n        yield from self . _io_writer . drain ( ) \n        self . _io_reader , self . _io_writer = yield from asyncio . open_connection ( loop = self . _loop , limit = DEFAULT_BUFFER_SIZE , sock = sock , ssl = ssl_context , server_hostname = self . _current_server . uri . hostname , ) \n    if self . is_reconnecting : \n        self . _ps . reset ( ) \n    connect_cmd = self . _connect_command ( ) \n    self . _io_writer . write ( connect_cmd ) \n    self . _io_writer . write ( PING_PROTO ) \n    yield from self . _io_writer . drain ( ) \n    next_op = yield from self . _io_reader . readline ( ) \n    if self . options [ \"verbose\" ] and OK_OP in next_op : \n        next_op = yield from self . _io_reader . readline ( ) \n    if ERR_OP in next_op : \n        err_line = next_op . decode ( ) \n        _ , err_msg = err_line . split ( \" \" , 1 ) \n        raise NatsError ( \"nats: \" + err_msg . rstrip ( '\\r\\n' ) ) \n    if PONG_PROTO in next_op : \n        self . _status = Client . CONNECTED \n    self . _reading_task = self . _loop . create_task ( self . _read_loop ( ) ) \n    self . _pongs = [ ] \n    self . _pings_outstanding = 0 \n    self . _ping_interval_task = self . _loop . create_task ( self . _ping_interval ( ) ) \n    self . _flusher_task = self . _loop . create_task ( self . _flusher ( ) ) "}
{"4461": "\ndef decode ( self , images , save = None , round = 4 , names = None , ** kwargs ) : \n    if isinstance ( images , string_types ) : \n        images = [ images ] \n    if isinstance ( images , list ) : \n        imgs_to_decode = imageutils . load_imgs ( images , self . masker ) \n    else : \n        imgs_to_decode = images \n    methods = { 'pearson' : self . _pearson_correlation , 'dot' : self . _dot_product , 'roi' : self . _roi_association } \n    result = np . around ( methods [ self . method ] ( imgs_to_decode , ** kwargs ) , round ) \n    if names is None : \n        if type ( images ) . __module__ == np . __name__ : \n            names = [ 'image_%d' % i for i in range ( images . shape [ 1 ] ) ] \n        else : \n            if self . method == 'roi' : \n                names = [ 'cluster_%d' % i for i in range ( result . shape [ 1 ] ) ] \n            else : \n                names = images \n    result = pd . DataFrame ( result , columns = names , index = self . feature_names ) \n    if save is not None : \n        result . to_csv ( save , index_label = 'Feature' ) \n    return result "}
{"4466": "\ndef feature_selection ( feat_select , X , y ) : \n    if re . match ( '.*-best' , feat_select ) is not None : \n        n = int ( feat_select . split ( '-' ) [ 0 ] ) \n        selector = SelectKBest ( k = n ) \n        import warnings \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( 'ignore' , category = UserWarning ) \n            features_selected = np . where ( selector . fit ( X , y ) . get_support ( ) is True ) [ 0 ] \n    else : \n        if re . match ( '.*-randombest' , feat_select ) is not None : \n            n = int ( feat_select . split ( '-' ) [ 0 ] ) \n            from random import shuffle \n            features = range ( 0 , X . shape [ 1 ] ) \n            shuffle ( features ) \n            features_selected = features [ : n ] \n    return features_selected "}
{"4470": "\ndef classify ( X , y , clf_method = 'ERF' , classifier = None , output = 'summary_clf' , cross_val = None , class_weight = None , regularization = None , param_grid = None , scoring = 'accuracy' , refit_all = True , feat_select = None ) : \n    clf = Classifier ( clf_method , classifier , param_grid ) \n    if cross_val is not None : \n        score = clf . cross_val_fit ( X , y , cross_val , scoring = scoring , feat_select = feat_select , class_weight = class_weight ) \n    else : \n        score = clf . fit ( X , y , class_weight = class_weight ) . score ( X , y ) \n    from collections import Counter \n    if output == 'clf' : \n        return clf \n    else : \n        if output == 'summary' : \n            output = { 'score' : score , 'n' : dict ( Counter ( y ) ) } \n        else : \n            if output == 'summary_clf' : \n                output = { 'score' : score , 'n' : dict ( Counter ( y ) ) , 'clf' : clf , 'features_selected' : clf . features_selected , 'predictions' : clf . predictions } \n        return output "}
{"4472": "\ndef set_class_weight ( self , class_weight = 'auto' , y = None ) : \n    if class_weight is None : \n        cw = None \n        try : \n            self . clf . set_params ( class_weight = cw ) \n        except ValueError : \n            pass \n    else : \n        if class_weight == 'auto' : \n            c = np . bincount ( y ) \n            ii = np . nonzero ( c ) [ 0 ] \n            c = c / float ( c . sum ( ) ) \n            cw = dict ( zip ( ii [ : : - 1 ] , c [ ii ] ) ) \n            try : \n                self . clf . set_params ( class_weight = cw ) \n            except ValueError : \n                import warnings \n                warnings . warn ( \"Tried to set class_weight, but failed. The classifier \" \"probably doesn't support it\" ) "}
{"4474": "\ndef fit_dataset ( self , dataset , y , features = None , feature_type = 'features' ) : \n    if feature_type == 'features' : \n        X = np . rot90 ( dataset . feature_table . data . toarray ( ) ) \n    else : \n        if feature_type == 'voxels' : \n            X = np . rot90 ( dataset . image_table . data . toarray ( ) ) \n    self . sk_classifier . fit ( X , y ) "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( prop_mask_active > activation_threshold ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( dists > 5.5 ) & ( dists < 6.5 ) ) [ 0 ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ dists <= r ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    else : \n        if return_type == 'data' : \n            return self . get_image_data ( ids ) "}
{"4500": "\ndef get_mask ( self , layers = None , output = 'vector' , in_global_mask = True ) : \n    if in_global_mask : \n        output = 'vector' \n    if layers is None : \n        layers = self . layers . keys ( ) \n    else : \n        if not isinstance ( layers , list ) : \n            layers = [ layers ] \n    layers = map ( lambda x : x if isinstance ( x , string_types ) else self . stack [ x ] , layers ) \n    layers = [ self . layers [ l ] for l in layers if l in self . layers ] \n    layers . append ( self . full ) \n    layers = np . vstack ( layers ) . T . astype ( bool ) \n    mask = layers . all ( axis = 1 ) \n    mask = self . get_image ( mask , output ) \n    return mask [ self . global_mask ] if in_global_mask else mask "}
{"4509": "\ndef get_ticker_price ( self , ticker , startDate = None , endDate = None , fmt = 'json' , frequency = 'daily' ) : \n    url = self . _get_url ( ticker , frequency ) \n    params = { 'format' : fmt if fmt != \"object\" else 'json' , 'resampleFreq' : frequency } \n    if startDate : \n        params [ 'startDate' ] = startDate \n    if endDate : \n        params [ 'endDate' ] = endDate \n    response = self . _request ( 'GET' , url , params = params ) \n    if fmt == \"json\" : \n        return response . json ( ) \n    else : \n        if fmt == \"object\" : \n            data = response . json ( ) \n            return [ dict_to_object ( item , \"TickerPrice\" ) for item in data ] \n        else : \n            return response . content . decode ( \"utf-8\" ) "}
{"4511": "\ndef get_bulk_news ( self , file_id = None , fmt = 'json' ) : \n    if file_id : \n        url = \"tiingo/news/bulk_download/{}\" . format ( file_id ) \n    else : \n        url = \"tiingo/news/bulk_download\" \n    response = self . _request ( 'GET' , url ) \n    data = response . json ( ) \n    if fmt == 'json' : \n        return data \n    else : \n        if fmt == 'object' : \n            return dict_to_object ( data , \"BulkNews\" ) "}
{"4513": "\nasync def get_bearer_info ( self ) : \n    if self . client_id is None : \n        raise SpotifyException ( _GET_BEARER_ERR % 'client_id' ) \n    else : \n        if self . client_secret is None : \n            raise SpotifyException ( _GET_BEARER_ERR % 'client_secret' ) \n    token = b64encode ( ':' . join ( ( self . client_id , self . client_secret ) ) . encode ( ) ) \n    kwargs = { 'url' : 'https://accounts.spotify.com/api/token' , 'data' : { 'grant_type' : 'client_credentials' } , 'headers' : { 'Authorization' : 'Basic ' + token . decode ( ) } } \n    async with self . _session . post ( ** kwargs ) as resp : \n        return json . loads ( await resp . text ( encoding = 'utf-8' ) ) "}
{"4514": "\nasync def request ( self , route , ** kwargs ) : \n    if isinstance ( route , tuple ) : \n        method , url = route \n    else : \n        method = route . method \n        url = route . url \n    if self . bearer_info is None : \n        self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n        access_token = bearer_info [ 'access_token' ] \n    else : \n        access_token = self . bearer_info [ 'access_token' ] \n    headers = { 'Authorization' : 'Bearer ' + access_token , 'Content-Type' : kwargs . get ( 'content_type' , 'application/json' ) , ** kwargs . pop ( 'headers' , { } ) } \n    for _ in range ( self . RETRY_AMOUNT ) : \n        r = await self . _session . request ( method , url , headers = headers , ** kwargs ) \n        try : \n            status = r . status \n            try : \n                data = json . loads ( await r . text ( encoding = 'utf-8' ) ) \n            except json . decoder . JSONDecodeError : \n                data = { } \n            if 300 > status >= 200 : \n                return data \n            if status == 401 : \n                self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n                headers [ 'Authorization' ] = 'Bearer ' + bearer_info [ 'access_token' ] \n                continue \n            if status == 429 : \n                amount = r . headers . get ( 'Retry-After' ) \n                await asyncio . sleep ( int ( amount ) , loop = self . loop ) \n                continue \n            if status in ( 502 , 503 ) : \n                continue \n            if status == 403 : \n                raise Forbidden ( r , data ) \n            else : \n                if status == 404 : \n                    raise NotFound ( r , data ) \n        finally : \n            await r . release ( ) \n    else : \n        raise HTTPException ( r , data ) "}
{"4549": "\nasync def search ( self , q : str , * , types : Optional [ Iterable [ str ] ] = [ 'track' , 'playlist' , 'artist' , 'album' ] , limit : Optional [ int ] = 20 , offset : Optional [ int ] = 0 , market : Optional [ str ] = None ) -> Dict [ str , List [ Union [ Track , Playlist , Artist , Album ] ] ] : \n    if not hasattr ( types , '__iter__' ) : \n        raise TypeError ( 'types must be an iterable.' ) \n    else : \n        if not isinstance ( types , list ) : \n            types = list ( item for item in types ) \n    types_ = set ( types ) \n    if not types_ . issubset ( _SEARCH_TYPES ) : \n        raise ValueError ( _SEARCH_TYPE_ERR % types_ . difference ( _SEARCH_TYPES ) . pop ( ) ) \n    kwargs = { 'q' : q . replace ( ' ' , '+' ) , 'queary_type' : ',' . join ( tp . strip ( ) for tp in types ) , 'market' : market , 'limit' : limit , 'offset' : offset } \n    data = await self . http . search ( ** kwargs ) \n    return { key : [ _TYPES [ obj [ 'type' ] ] ( self , obj ) for obj in value [ 'items' ] ] for key , value in data . items ( ) } "}
{"4560": "\nasync def from_href ( self ) : \n    if not hasattr ( self , 'href' ) : \n        raise TypeError ( 'Spotify object has no `href` attribute, therefore cannot be retrived' ) \n    else : \n        if hasattr ( self , 'http' ) : \n            return await self . http . request ( ( 'GET' , self . href ) ) \n        else : \n            cls = type ( self ) \n    try : \n        client = getattr ( self , '_{0}__client' . format ( cls . __name__ ) ) \n    except AttributeError : \n        raise TypeError ( 'Spotify object has no way to access a HTTPClient.' ) \n    else : \n        http = client . http \n    data = await http . request ( ( 'GET' , self . href ) ) \n    return cls ( client , data ) "}
{"4572": "\ndef _format_decoded ( self , to_format , result = None ) : \n    if not result : \n        result = [ ] \n    for data in List ( to_format ) . format ( ) : \n        if data : \n            if \"^\" in data : \n                return self . _format_decoded ( data . split ( \"^\" ) , result ) \n            if \"#\" in data : \n                return self . _format_decoded ( data . split ( \"#\" ) , result ) \n            if \",\" in data : \n                return self . _format_decoded ( data . split ( \",\" ) , result ) \n            if \"!\" in data : \n                return self . _format_decoded ( data . split ( \"!\" ) , result ) \n            if \"|\" in data : \n                return self . _format_decoded ( data . split ( \"|\" ) , result ) \n            if data : \n                data = self . _extract_base ( data ) \n                if data and ( self . checker . is_domain_valid ( data ) or self . checker . is_ip_valid ( data ) ) : \n                    result . append ( data ) \n                else : \n                    if data : \n                        url_base = self . checker . is_url_valid ( data , return_base = True ) \n                        if url_base : \n                            result . append ( url_base ) \n    return result "}
{"4588": "\ndef colorify_logo ( cls , home = False ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        else : \n            if PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] >= 50 : \n                for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                    to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n            else : \n                for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                    to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        print ( \"\\n\" . join ( to_print ) ) "}
{"4596": "\ndef _get_structure ( self ) : \n    structure_file = \"\" \n    req = \"\" \n    if PyFunceble . path . isfile ( self . structure ) : \n        structure_file = self . structure \n    else : \n        if PyFunceble . path . isfile ( self . base + \"dir_structure_production.json\" ) : \n            structure_file = self . base + \"dir_structure_production.json\" \n        else : \n            if \"dev\" not in PyFunceble . VERSION : \n                req = PyFunceble . requests . get ( PyFunceble . LINKS [ \"dir_structure\" ] . replace ( \"dev\" , \"master\" ) ) \n            else : \n                req = PyFunceble . requests . get ( PyFunceble . LINKS [ \"dir_structure\" ] . replace ( \"master\" , \"dev\" ) ) \n    if structure_file . endswith ( \"_production.json\" ) : \n        return self . _update_structure_from_config ( Dict ( ) . from_json ( File ( structure_file ) . read ( ) ) ) \n    if structure_file . endswith ( \".json\" ) : \n        return self . _update_structure_from_config ( Dict ( ) . from_json ( File ( structure_file ) . read ( ) ) ) \n    return self . _update_structure_from_config ( Dict ( ) . from_json ( req . text ) ) "}
{"4606": "\ndef _load ( self ) : \n    if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in PyFunceble . environ : \n        while True : \n            response = input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + \"A configuration key is missing.\\n\" + PyFunceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) \n            if isinstance ( response , str ) : \n                if response . lower ( ) == \"y\" : \n                    self . _merge_values ( ) \n                    self . _save ( ) \n                    print ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) \n                    break \n                else : \n                    if response . lower ( ) == \"n\" : \n                        raise Exception ( \"Configuration key still missing.\" ) \n    else : \n        self . _merge_values ( ) \n        self . _save ( ) "}
{"4608": "\ndef check_versions ( cls , local , upstream ) : \n    status = [ None , None , None ] \n    for index , version_number in enumerate ( local ) : \n        if int ( version_number ) < int ( upstream [ index ] ) : \n            status [ index ] = True \n        else : \n            if int ( version_number ) > int ( upstream [ index ] ) : \n                status [ index ] = False \n    if False in status : \n        return False \n    if True in status : \n        return True \n    return None "}
{"4611": "\ndef _analytic_host_file_directory ( self ) : \n    output_dir = ( self . output_parent_dir + PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"parent\" ] ) \n    if self . domain_status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"potentially_up\" ] : \n        output_dir += PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"potentially_up\" ] \n    else : \n        if ( self . domain_status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"potentially_down\" ] ) : \n            output_dir += PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"potentially_down\" ] \n        else : \n            if self . domain_status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"suspicious\" ] : \n                output_dir += PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"suspicious\" ] \n            else : \n                output_dir += PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"up\" ] \n    return output_dir "}
{"4623": "\ndef mine ( self ) : \n    if PyFunceble . CONFIGURATION [ \"mining\" ] : \n        try : \n            history = PyFunceble . requests . get ( self . to_get , timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] , headers = self . headers , ) . history \n            mined = { self . to_get_bare : [ ] } \n            for element in history : \n                element = element . url \n                if PyFunceble . INTERN [ \"to_test_type\" ] == \"url\" : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = False ) \n                else : \n                    if PyFunceble . INTERN [ \"to_test_type\" ] == \"domain\" : \n                        to_append = Check ( ) . is_url_valid ( element , return_base = True ) \n                    else : \n                        raise Exception ( \"Unknown tested.\" ) \n                if to_append : \n                    if to_append . endswith ( \":80\" ) : \n                        to_append = to_append [ : - 3 ] \n                    if to_append != self . to_get_bare : \n                        mined [ self . to_get_bare ] . append ( to_append ) \n            if mined [ self . to_get_bare ] : \n                return mined \n            return None \n        except ( PyFunceble . requests . ConnectionError , PyFunceble . requests . exceptions . Timeout , PyFunceble . requests . exceptions . InvalidURL , PyFunceble . socket . timeout , urllib3_exceptions . InvalidHeader , UnicodeDecodeError , ) : \n            return None \n    return None "}
{"4637": "\ndef header ( self , do_not_print = False ) : \n    if ( not PyFunceble . CONFIGURATION [ \"header_printed\" ] or self . template == \"Percentage\" or do_not_print ) : \n        if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template == \"Generic_File\" ) : \n            to_print = self . headers [ \"Generic\" ] \n            if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] and PyFunceble . HTTP_CODE [ \"active\" ] ) : \n                to_print = Dict ( to_print ) . remove_key ( \"Analyze Date\" ) \n        else : \n            if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] : \n                to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ] \n            else : \n                if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] : \n                    to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ] \n                else : \n                    if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n                        to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ] \n                    else : \n                        if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"invalid\" ] : \n                            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ] \n                        else : \n                            if ( self . template == \"Less\" or self . template == \"Percentage\" or self . template == \"HTTP\" ) : \n                                to_print = self . headers [ self . template ] \n                                if self . template == \"Less\" and not PyFunceble . HTTP_CODE [ \"active\" ] : \n                                    to_print [ \"Source\" ] = 10 \n        if not PyFunceble . HTTP_CODE [ \"active\" ] : \n            to_print = Dict ( to_print ) . remove_key ( \"HTTP Code\" ) \n        self . currently_used_header = to_print \n        if not do_not_print : \n            self . _before_header ( ) \n            for formatted_template in self . _header_constructor ( to_print ) : \n                if not self . only_on_file : \n                    print ( formatted_template ) \n                if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                    File ( self . output ) . write ( formatted_template + \"\\n\" ) "}
{"4640": "\ndef _colorify ( self , data ) : \n    if self . template in [ \"Generic\" , \"Less\" ] : \n        if ( self . data_to_print [ 1 ] . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or self . data_to_print [ 1 ] . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            data = PyFunceble . Fore . BLACK + PyFunceble . Back . GREEN + data \n        else : \n            if self . data_to_print [ 1 ] . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n                data = PyFunceble . Fore . BLACK + PyFunceble . Back . RED + data \n            else : \n                data = PyFunceble . Fore . BLACK + PyFunceble . Back . CYAN + data \n    return data "}
{"4642": "\ndef data ( self ) : \n    if isinstance ( self . data_to_print , list ) : \n        to_print = { } \n        to_print_size = [ ] \n        alone_cases = [ \"Percentage\" , \"HTTP\" ] \n        without_header = [ \"FullHosts\" , \"PlainDomain\" ] \n        if self . template . lower ( ) == \"json\" : \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                return self . _json_print ( ) \n            return None \n        if self . template not in alone_cases and self . template not in without_header : \n            self . header ( True ) \n            to_print_size = self . _size_from_header ( self . currently_used_header ) \n        else : \n            if self . template in without_header : \n                for data in self . data_to_print : \n                    to_print_size . append ( str ( len ( data ) ) ) \n            else : \n                to_print_size = self . _size_from_header ( self . headers [ self . template ] ) \n        to_print = self . _data_constructor ( to_print_size ) \n        self . _before_header ( ) \n        for data in self . _header_constructor ( to_print , False ) : \n            if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template in [ \"Less\" , \"Percentage\" ] : \n                if not self . only_on_file : \n                    colorified_data = self . _colorify ( data ) \n                    print ( colorified_data ) \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                File ( self . output ) . write ( data + \"\\n\" ) \n    else : \n        raise Exception ( \"Please review Prints().data()\" ) "}
{"4643": "\ndef _save ( self , last = False ) : \n    if ( self . _authorization ( ) and PyFunceble . CONFIGURATION [ \"logs\" ] and \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . file = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"execution_time\" ] ) \n        if PyFunceble . path . isfile ( self . file ) : \n            content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) \n        else : \n            content = { } \n        if self . action == \"start\" : \n            if \"final_total\" in content and content [ \"final_total\" ] : \n                del content [ \"final_total\" ] \n            if \"data\" in content : \n                content [ \"data\" ] . append ( [ PyFunceble . INTERN [ \"start\" ] ] ) \n            else : \n                content [ \"data\" ] = [ [ PyFunceble . INTERN [ \"start\" ] ] ] \n        else : \n            if self . action == \"stop\" : \n                try : \n                    content [ \"data\" ] [ - 1 ] . append ( PyFunceble . INTERN [ \"end\" ] ) \n                    start = content [ \"data\" ] [ 0 ] [ 0 ] \n                    end = content [ \"data\" ] [ - 1 ] [ - 1 ] \n                    content [ \"current_total\" ] = self . format_execution_time ( start , end ) \n                    if last : \n                        content [ \"final_total\" ] = content [ \"current_total\" ] \n                        print ( PyFunceble . Fore . MAGENTA + PyFunceble . Style . BRIGHT + \"Global execution time: \" + content [ \"final_total\" ] ) \n                except KeyError : \n                    pass \n        try : \n            Dict ( content ) . to_json ( self . file ) \n        except FileNotFoundError : \n            DirectoryStructure ( ) \n            Dict ( content ) . to_json ( self . file ) "}
{"4651": "\ndef get ( self ) : \n    result = { } \n    if self . algorithm in self . valid_algorithms : \n        if self . algorithm == \"all\" : \n            del self . valid_algorithms [ 0 ] \n            for algo in self . valid_algorithms : \n                if self . path and path . isfile ( self . path ) : \n                    result [ algo ] = self . _hash_file ( algo ) \n                else : \n                    if self . data : \n                        result [ algo ] = self . _hash_data ( algo ) \n                    else : \n                        return None \n        else : \n            if self . path and path . isfile ( self . path ) : \n                result [ self . algorithm ] = self . _hash_file ( self . algorithm ) \n            else : \n                if self . data : \n                    result [ self . algorithm ] = self . _hash_data ( self . algorithm ) \n                else : \n                    return None \n    else : \n        return None \n    if self . algorithm != \"all\" and self . only_hash : \n        return result [ self . algorithm ] \n    return result "}
{"4655": "\ndef merge ( self , to_merge , strict = True ) : \n    result = { } \n    for element in to_merge : \n        if element in self . main_dictionnary : \n            if isinstance ( to_merge [ element ] , dict ) and isinstance ( self . main_dictionnary [ element ] , dict ) : \n                result [ element ] = Dict ( self . main_dictionnary [ element ] ) . merge ( to_merge [ element ] ) \n            else : \n                if isinstance ( to_merge [ element ] , list ) and isinstance ( self . main_dictionnary [ element ] , list ) : \n                    result [ element ] = List ( self . main_dictionnary [ element ] ) . merge ( to_merge [ element ] , strict ) \n                else : \n                    result . update ( { element : to_merge [ element ] } ) \n        else : \n            result . update ( { element : to_merge [ element ] } ) \n    for element in self . main_dictionnary : \n        if element not in result : \n            result [ element ] = self . main_dictionnary [ element ] \n    return result "}
{"4658": "\ndef fix_path ( self , splited_path = None ) : \n    if not splited_path : \n        split_path = [ ] \n        if self . directory : \n            if \"/\" in self . directory : \n                split_path = self . directory . split ( \"/\" ) \n            else : \n                if \"\\\\\" in self . directory : \n                    split_path = self . directory . split ( \"\\\\\" ) \n                else : \n                    split_path = [ self . directory ] \n            return self . fix_path ( splited_path = [ x for x in split_path if x ] ) \n        return self . directory \n    return directory_separator . join ( splited_path ) + directory_separator "}
{"4662": "\ndef merge ( self , to_merge , strict = True ) : \n    result = [ ] \n    if strict : \n        for index , element in enumerate ( to_merge ) : \n            try : \n                if isinstance ( element , dict ) and isinstance ( self . main_list [ index ] , dict ) : \n                    result . append ( Dict ( self . main_list [ index ] ) . merge ( element ) ) \n                else : \n                    if isinstance ( element , list ) and isinstance ( self . main_list [ index ] , list ) : \n                        result . append ( List ( self . main_list [ index ] ) . merge ( element ) ) \n                    else : \n                        result . append ( element ) \n            except IndexError : \n                result . append ( element ) \n    else : \n        result = self . main_list \n        for element in to_merge : \n            if element not in result : \n                result . append ( element ) \n    return result "}
{"4666": "\ndef count ( self ) : \n    if self . status : \n        PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] += 1 \n        if ( self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] += 1 \n        else : \n            if self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n                PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] += 1 \n            else : \n                PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] += 1 "}
{"4668": "\ndef log ( self ) : \n    if ( PyFunceble . CONFIGURATION [ \"show_percentage\" ] and PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 ) : \n        output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) \n        File ( output ) . delete ( ) \n        self . _calculate ( ) \n        if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n            print ( \"\\n\" ) \n            Prints ( None , \"Percentage\" , output ) . header ( ) \n            lines_to_print = [ [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] \n            if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n                lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] \n                del lines_to_print [ 1 ] \n            for to_print in lines_to_print : \n                Prints ( to_print , \"Percentage\" , output ) . data ( ) \n    else : \n        if PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 : \n            self . _calculate ( ) "}
{"4669": "\ndef is_url_valid ( self , url = None , return_base = False , return_formatted = False ) : \n    initial_base = None \n    if url : \n        to_test = url \n    else : \n        if self . element : \n            to_test = self . element \n        else : \n            to_test = PyFunceble . INTERN [ \"to_test\" ] \n    if to_test . startswith ( \"http\" ) : \n        try : \n            regex = r\"(^(http:\\/\\/|https:\\/\\/)(.+?(?=\\/)|.+?$))\" \n            initial_base = base = Regex ( to_test , regex , return_data = True , rematch = True ) . match ( ) [ 2 ] \n            if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n                base = domain2idna ( base ) \n            domain_status = self . is_domain_valid ( base ) \n            ip_status = self . is_ip_valid ( base ) \n            if domain_status or ip_status : \n                if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] and return_formatted : \n                    return Regex ( to_test , initial_base , escape = True , return_data = True , replace_with = base , occurences = 1 , ) . replace ( ) \n                if return_formatted : \n                    return to_test \n                if return_base : \n                    return base \n                return True \n        except TypeError : \n            pass \n    if return_formatted : \n        return to_test \n    return False "}
{"4670": "\ndef is_domain_valid ( self , domain = None , subdomain_check = False ) : \n    regex_valid_domains = r\"^(?=.{0,253}$)(([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9](?:\\.)?|[a-z0-9](?:\\.)?))$\" \n    regex_valid_subdomains = r\"^(?=.{0,253}$)(([a-z0-9_][a-z0-9-_]{0,61}[a-z0-9_-]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9]))$\" \n    if domain : \n        to_test = domain \n    else : \n        if self . element : \n            to_test = self . element \n        else : \n            to_test = PyFunceble . INTERN [ \"to_test\" ] \n    try : \n        last_point_index = to_test . rindex ( \".\" ) \n        extension = to_test [ last_point_index + 1 : ] \n        if not extension and to_test . endswith ( \".\" ) : \n            try : \n                extension = [ x for x in to_test . split ( \".\" ) if x ] [ - 1 ] \n            except IndexError : \n                pass \n        if not extension or extension not in PyFunceble . INTERN [ \"iana_db\" ] : \n            return False \n        if ( Regex ( to_test , regex_valid_domains , return_data = False ) . match ( ) and not subdomain_check ) : \n            return True \n        if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n            for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                try : \n                    suffix_index = to_test . rindex ( \".\" + suffix ) \n                    to_check = to_test [ : suffix_index ] \n                    if \".\" not in to_check and subdomain_check : \n                        return False \n                    if \".\" in to_check and subdomain_check : \n                        return True \n                    if \".\" in to_check : \n                        return Regex ( to_check , regex_valid_subdomains , return_data = False ) . match ( ) \n                except ValueError : \n                    pass \n        to_check = to_test [ : last_point_index ] \n        if \".\" in to_check and subdomain_check : \n            return True \n        if \".\" in to_check : \n            return Regex ( to_check , regex_valid_subdomains , return_data = False ) . match ( ) \n    except ( ValueError , AttributeError ) : \n        pass \n    return False "}
{"4671": "\ndef is_subdomain ( self , domain = None ) : \n    if domain : \n        to_test = domain \n    else : \n        if self . element : \n            to_test = self . element \n        else : \n            to_test = PyFunceble . INTERN [ \"to_test\" ] \n    return self . is_domain_valid ( to_test , subdomain_check = True ) "}
{"4672": "\ndef get ( cls ) : \n    if PyFunceble . INTERN [ \"to_test_type\" ] == \"domain\" : \n        if Check ( ) . is_domain_valid ( ) or Check ( ) . is_ip_valid ( ) : \n            return SyntaxStatus ( PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ) . handle ( ) \n    else : \n        if PyFunceble . INTERN [ \"to_test_type\" ] == \"url\" : \n            if Check ( ) . is_url_valid ( ) : \n                return SyntaxStatus ( PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ) . handle ( ) \n        else : \n            raise Exception ( \"Unknow test type.\" ) \n    return SyntaxStatus ( PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ) . handle ( ) "}
{"4684": "\ndef add ( self ) : \n    if self . _authorization ( ) : \n        if self . epoch < int ( PyFunceble . time ( ) ) : \n            state = \"past\" \n        else : \n            state = \"future\" \n        if self . is_in_database ( ) : \n            if ( str ( self . epoch ) != PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration_date\" : self . expiration_date , } ) \n            else : \n                if self . is_time_older ( ) : \n                    if ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"state\" ] != \"past\" ) : \n                        PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"state\" : \"past\" } ) \n                else : \n                    if ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"state\" ] != \"future\" ) : \n                        PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"state\" : \"future\" } ) \n        else : \n            if ( not PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] = { } \n            PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] . update ( { PyFunceble . INTERN [ \"to_test\" ] : { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration_date\" : self . expiration_date , } } ) \n        self . _backup ( ) "}
{"4707": "\ndef filter_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False , ) : \n    imports = SAFE_IMPORTS \n    if additional_imports : \n        imports |= frozenset ( additional_imports ) \n    del additional_imports \n    messages = check ( source ) \n    if ignore_init_module_imports : \n        marked_import_line_numbers = frozenset ( ) \n    else : \n        marked_import_line_numbers = frozenset ( unused_import_line_numbers ( messages ) ) \n    marked_unused_module = collections . defaultdict ( lambda : [ ] ) \n    for line_number , module_name in unused_import_module_name ( messages ) : \n        marked_unused_module [ line_number ] . append ( module_name ) \n    if expand_star_imports and not ( re . search ( r'\\b__all__\\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : \n        marked_star_import_line_numbers = frozenset ( star_import_used_line_numbers ( messages ) ) \n        if len ( marked_star_import_line_numbers ) > 1 : \n            marked_star_import_line_numbers = frozenset ( ) \n        else : \n            undefined_names = [ ] \n            for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : \n                undefined_names . append ( undefined_name ) \n            if not undefined_names : \n                marked_star_import_line_numbers = frozenset ( ) \n    else : \n        marked_star_import_line_numbers = frozenset ( ) \n    if remove_unused_variables : \n        marked_variable_line_numbers = frozenset ( unused_variable_line_numbers ( messages ) ) \n    else : \n        marked_variable_line_numbers = frozenset ( ) \n    if remove_duplicate_keys : \n        marked_key_line_numbers = frozenset ( duplicate_key_line_numbers ( messages , source ) ) \n    else : \n        marked_key_line_numbers = frozenset ( ) \n    line_messages = get_messages_by_line ( messages ) \n    sio = io . StringIO ( source ) \n    previous_line = '' \n    for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : \n        if '#' in line : \n            yield line \n        else : \n            if line_number in marked_import_line_numbers : \n                yield filter_unused_import ( line , unused_module = marked_unused_module [ line_number ] , remove_all_unused_imports = remove_all_unused_imports , imports = imports , previous_line = previous_line ) \n            else : \n                if line_number in marked_variable_line_numbers : \n                    yield filter_unused_variable ( line ) \n                else : \n                    if line_number in marked_key_line_numbers : \n                        yield filter_duplicate_key ( line , line_messages [ line_number ] , line_number , marked_key_line_numbers , source ) \n                    else : \n                        if line_number in marked_star_import_line_numbers : \n                            yield filter_star_import ( line , undefined_names ) \n                        else : \n                            yield line \n        previous_line = line "}
{"4727": "\ndef create ( cls , name_value , name_type ) : \n    if isinstance ( name_value , Name . NameValue ) : \n        value = name_value \n    else : \n        if isinstance ( name_value , str ) : \n            value = cls . NameValue ( name_value ) \n        else : \n            name = 'Name' \n            msg = exceptions . ErrorStrings . BAD_EXP_RECV \n            member = 'name_value' \n            raise TypeError ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name_value' , type ( Name . NameValue ) , type ( name_value ) ) ) \n    if isinstance ( name_type , Name . NameType ) : \n        n_type = name_type \n    else : \n        if isinstance ( name_type , Enum ) : \n            n_type = cls . NameType ( name_type ) \n        else : \n            name = 'Name' \n            msg = exceptions . ErrorStrings . BAD_EXP_RECV \n            member = 'name_type' \n            raise TypeError ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name_type' , type ( Name . NameType ) , type ( name_type ) ) ) \n    return Name ( name_value = value , name_type = n_type ) "}
{"4747": "\ndef _get_attribute_from_managed_object ( self , managed_object , attr_name ) : \n    if attr_name == 'Unique Identifier' : \n        return str ( managed_object . unique_identifier ) \n    else : \n        if attr_name == 'Name' : \n            names = list ( ) \n            for name in managed_object . names : \n                name = attributes . Name ( attributes . Name . NameValue ( name ) , attributes . Name . NameType ( enums . NameType . UNINTERPRETED_TEXT_STRING ) ) \n                names . append ( name ) \n            return names \n        else : \n            if attr_name == 'Object Type' : \n                return managed_object . _object_type \n            else : \n                if attr_name == 'Cryptographic Algorithm' : \n                    return managed_object . cryptographic_algorithm \n                else : \n                    if attr_name == 'Cryptographic Length' : \n                        return managed_object . cryptographic_length \n                    else : \n                        if attr_name == 'Cryptographic Parameters' : \n                            return None \n                        else : \n                            if attr_name == 'Cryptographic Domain Parameters' : \n                                return None \n                            else : \n                                if attr_name == 'Certificate Type' : \n                                    return managed_object . certificate_type \n                                else : \n                                    if attr_name == 'Certificate Length' : \n                                        return None \n                                    else : \n                                        if attr_name == 'X.509 Certificate Identifier' : \n                                            return None \n                                        else : \n                                            if attr_name == 'X.509 Certificate Subject' : \n                                                return None \n                                            else : \n                                                if attr_name == 'X.509 Certificate Issuer' : \n                                                    return None \n                                                else : \n                                                    if attr_name == 'Certificate Identifier' : \n                                                        return None \n                                                    else : \n                                                        if attr_name == 'Certificate Subject' : \n                                                            return None \n                                                        else : \n                                                            if attr_name == 'Certificate Issuer' : \n                                                                return None \n                                                            else : \n                                                                if attr_name == 'Digital Signature Algorithm' : \n                                                                    return None \n                                                                else : \n                                                                    if attr_name == 'Digest' : \n                                                                        return None \n                                                                    else : \n                                                                        if attr_name == 'Operation Policy Name' : \n                                                                            return managed_object . operation_policy_name \n                                                                        else : \n                                                                            if attr_name == 'Cryptographic Usage Mask' : \n                                                                                return managed_object . cryptographic_usage_masks \n                                                                            else : \n                                                                                if attr_name == 'Lease Time' : \n                                                                                    return None \n                                                                                else : \n                                                                                    if attr_name == 'Usage Limits' : \n                                                                                        return None \n                                                                                    else : \n                                                                                        if attr_name == 'State' : \n                                                                                            return managed_object . state \n                                                                                        else : \n                                                                                            if attr_name == 'Initial Date' : \n                                                                                                return managed_object . initial_date \n                                                                                            else : \n                                                                                                if attr_name == 'Activation Date' : \n                                                                                                    return None \n                                                                                                else : \n                                                                                                    if attr_name == 'Process Start Date' : \n                                                                                                        return None \n                                                                                                    else : \n                                                                                                        if attr_name == 'Protect Stop Date' : \n                                                                                                            return None \n                                                                                                        else : \n                                                                                                            if attr_name == 'Deactivation Date' : \n                                                                                                                return None \n                                                                                                            else : \n                                                                                                                if attr_name == 'Destroy Date' : \n                                                                                                                    return None \n                                                                                                                else : \n                                                                                                                    if attr_name == 'Compromise Occurrence Date' : \n                                                                                                                        return None \n                                                                                                                    else : \n                                                                                                                        if attr_name == 'Compromise Date' : \n                                                                                                                            return None \n                                                                                                                        else : \n                                                                                                                            if attr_name == 'Revocation Reason' : \n                                                                                                                                return None \n                                                                                                                            else : \n                                                                                                                                if attr_name == 'Archive Date' : \n                                                                                                                                    return None \n                                                                                                                                else : \n                                                                                                                                    if attr_name == 'Object Group' : \n                                                                                                                                        return None \n                                                                                                                                    else : \n                                                                                                                                        if attr_name == 'Fresh' : \n                                                                                                                                            return None \n                                                                                                                                        else : \n                                                                                                                                            if attr_name == 'Link' : \n                                                                                                                                                return None \n                                                                                                                                            else : \n                                                                                                                                                if attr_name == 'Application Specific Information' : \n                                                                                                                                                    return None \n                                                                                                                                                else : \n                                                                                                                                                    if attr_name == 'Contact Information' : \n                                                                                                                                                        return None \n                                                                                                                                                    else : \n                                                                                                                                                        if attr_name == 'Last Change Date' : \n                                                                                                                                                            return None \n                                                                                                                                                        else : \n                                                                                                                                                            return None "}
{"4749": "\ndef _set_attribute_on_managed_object ( self , managed_object , attribute ) : \n    attribute_name = attribute [ 0 ] \n    attribute_value = attribute [ 1 ] \n    if self . _attribute_policy . is_attribute_multivalued ( attribute_name ) : \n        if attribute_name == 'Name' : \n            managed_object . names . extend ( [ x . name_value . value for x in attribute_value ] ) \n            for name in managed_object . names : \n                if managed_object . names . count ( name ) > 1 : \n                    raise exceptions . InvalidField ( \"Cannot set duplicate name values.\" ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) \n    else : \n        field = None \n        value = attribute_value . value \n        if attribute_name == 'Cryptographic Algorithm' : \n            field = 'cryptographic_algorithm' \n        else : \n            if attribute_name == 'Cryptographic Length' : \n                field = 'cryptographic_length' \n            else : \n                if attribute_name == 'Cryptographic Usage Mask' : \n                    field = 'cryptographic_usage_masks' \n                    value = list ( ) \n                    for e in enums . CryptographicUsageMask : \n                        if e . value & attribute_value . value : \n                            value . append ( e ) \n                else : \n                    if attribute_name == 'Operation Policy Name' : \n                        field = 'operation_policy_name' \n        if field : \n            existing_value = getattr ( managed_object , field ) \n            if existing_value : \n                if existing_value != value : \n                    raise exceptions . InvalidField ( \"Cannot overwrite the {0} attribute.\" . format ( attribute_name ) ) \n            else : \n                setattr ( managed_object , field , value ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) "}
{"4750": "\ndef is_allowed ( self , policy_name , session_user , session_group , object_owner , object_type , operation ) : \n    policy_section = self . get_relevant_policy_section ( policy_name , session_group ) \n    if policy_section is None : \n        return False \n    object_policy = policy_section . get ( object_type ) \n    if not object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} objects.\" . format ( policy_name , self . _get_enum_string ( object_type ) ) ) \n        return False \n    operation_object_policy = object_policy . get ( operation ) \n    if not operation_object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} operations on {2} \" \"objects.\" . format ( policy_name , self . _get_enum_string ( operation ) , self . _get_enum_string ( object_type ) ) ) \n        return False \n    if operation_object_policy == enums . Policy . ALLOW_ALL : \n        return True \n    else : \n        if operation_object_policy == enums . Policy . ALLOW_OWNER : \n            if session_user == object_owner : \n                return True \n            else : \n                return False \n        else : \n            if operation_object_policy == enums . Policy . DISALLOW_ALL : \n                return False \n            else : \n                return False "}
{"4752": "\ndef create ( self , secret_type , value = None ) : \n    if secret_type is ObjectType . CERTIFICATE : \n        return self . _create_certificate ( value ) \n    else : \n        if secret_type is ObjectType . SYMMETRIC_KEY : \n            return self . _create_symmetric_key ( value ) \n        else : \n            if secret_type is ObjectType . PUBLIC_KEY : \n                return self . _create_public_key ( value ) \n            else : \n                if secret_type is ObjectType . PRIVATE_KEY : \n                    return self . _create_private_key ( value ) \n                else : \n                    if secret_type is ObjectType . SPLIT_KEY : \n                        return self . _create_split_key ( value ) \n                    else : \n                        if secret_type is ObjectType . TEMPLATE : \n                            return self . _create_template ( value ) \n                        else : \n                            if secret_type is ObjectType . SECRET_DATA : \n                                return self . _create_secret_data ( value ) \n                            else : \n                                if secret_type is ObjectType . OPAQUE_DATA : \n                                    return self . _create_opaque_data ( value ) \n                                else : \n                                    raise TypeError ( \"Unrecognized secret type: {0}\" . format ( secret_type ) ) "}
{"4753": "\ndef set_setting ( self , setting , value ) : \n    if setting not in self . _expected_settings + self . _optional_settings : \n        raise exceptions . ConfigurationError ( \"Setting '{0}' is not supported.\" . format ( setting ) ) \n    if setting == 'hostname' : \n        self . _set_hostname ( value ) \n    else : \n        if setting == 'port' : \n            self . _set_port ( value ) \n        else : \n            if setting == 'certificate_path' : \n                self . _set_certificate_path ( value ) \n            else : \n                if setting == 'key_path' : \n                    self . _set_key_path ( value ) \n                else : \n                    if setting == 'ca_path' : \n                        self . _set_ca_path ( value ) \n                    else : \n                        if setting == 'auth_suite' : \n                            self . _set_auth_suite ( value ) \n                        else : \n                            if setting == 'policy_path' : \n                                self . _set_policy_path ( value ) \n                            else : \n                                if setting == 'enable_tls_client_auth' : \n                                    self . _set_enable_tls_client_auth ( value ) \n                                else : \n                                    if setting == 'tls_cipher_suites' : \n                                        self . _set_tls_cipher_suites ( value ) \n                                    else : \n                                        if setting == 'logging_level' : \n                                            self . _set_logging_level ( value ) \n                                        else : \n                                            self . _set_database_path ( value ) "}
{"4759": "\ndef validate ( self ) : \n    if self . value is not None : \n        if not isinstance ( self . value , six . integer_types ) : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        else : \n            if self . value > LongInteger . MAX : \n                raise ValueError ( 'long integer value greater than accepted max' ) \n            else : \n                if self . value < LongInteger . MIN : \n                    raise ValueError ( 'long integer value less than accepted min' ) "}
{"4763": "\ndef validate ( self ) : \n    if not isinstance ( self . enum , enumeration . EnumMeta ) : \n        raise TypeError ( 'enumeration type {0} must be of type EnumMeta' . format ( self . enum ) ) \n    if self . value is not None : \n        if not isinstance ( self . value , self . enum ) : \n            raise TypeError ( 'enumeration {0} must be of type {1}' . format ( self . value , self . enum ) ) \n        if type ( self . value . value ) not in six . integer_types : \n            raise TypeError ( 'enumeration value must be an int' ) \n        else : \n            if self . value . value > Enumeration . MAX : \n                raise ValueError ( 'enumeration value greater than accepted max' ) \n            else : \n                if self . value . value < Enumeration . MIN : \n                    raise ValueError ( 'enumeration value less than accepted min' ) "}
{"4764": "\ndef read_value ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    try : \n        value = unpack ( '!Q' , istream . read ( self . LENGTH ) ) [ 0 ] \n    except Exception : \n        self . logger . error ( \"Error reading boolean value from buffer\" ) \n        raise \n    if value == 1 : \n        self . value = True \n    else : \n        if value == 0 : \n            self . value = False \n        else : \n            raise ValueError ( \"expected: 0 or 1, observed: {0}\" . format ( value ) ) \n    self . validate ( ) "}
{"4769": "\ndef validate ( self ) : \n    if self . value is not None : \n        if type ( self . value ) not in six . integer_types : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        else : \n            if self . value > Interval . MAX : \n                raise ValueError ( 'interval value greater than accepted max' ) \n            else : \n                if self . value < Interval . MIN : \n                    raise ValueError ( 'interval value less than accepted min' ) "}
{"4771": "\ndef key_wrapping_data ( self , value ) : \n    if value is None : \n        value = { } \n    else : \n        if not isinstance ( value , dict ) : \n            raise TypeError ( \"Key wrapping data must be a dictionary.\" ) \n    self . _kdw_wrapping_method = value . get ( 'wrapping_method' ) \n    eki = value . get ( 'encryption_key_information' ) \n    if eki is None : \n        eki = { } \n    self . _kdw_eki_unique_identifier = eki . get ( 'unique_identifier' ) \n    eki_cp = eki . get ( 'cryptographic_parameters' ) \n    if eki_cp is None : \n        eki_cp = { } \n    self . _kdw_eki_cp_block_cipher_mode = eki_cp . get ( 'block_cipher_mode' ) \n    self . _kdw_eki_cp_padding_method = eki_cp . get ( 'padding_method' ) \n    self . _kdw_eki_cp_hashing_algorithm = eki_cp . get ( 'hashing_algorithm' ) \n    self . _kdw_eki_cp_key_role_type = eki_cp . get ( 'key_role_type' ) \n    self . _kdw_eki_cp_digital_signature_algorithm = eki_cp . get ( 'digital_signature_algorithm' ) \n    self . _kdw_eki_cp_cryptographic_algorithm = eki_cp . get ( 'cryptographic_algorithm' ) \n    self . _kdw_eki_cp_random_iv = eki_cp . get ( 'random_iv' ) \n    self . _kdw_eki_cp_iv_length = eki_cp . get ( 'iv_length' ) \n    self . _kdw_eki_cp_tag_length = eki_cp . get ( 'tag_length' ) \n    self . _kdw_eki_cp_fixed_field_length = eki_cp . get ( 'fixed_field_length' ) \n    self . _kdw_eki_cp_invocation_field_length = eki_cp . get ( 'invocation_field_length' ) \n    self . _kdw_eki_cp_counter_length = eki_cp . get ( 'counter_length' ) \n    self . _kdw_eki_cp_initial_counter_value = eki_cp . get ( 'initial_counter_value' ) \n    mski = value . get ( 'mac_signature_key_information' ) \n    if mski is None : \n        mski = { } \n    self . _kdw_mski_unique_identifier = mski . get ( 'unique_identifier' ) \n    mski_cp = mski . get ( 'cryptographic_parameters' ) \n    if mski_cp is None : \n        mski_cp = { } \n    self . _kdw_mski_cp_block_cipher_mode = mski_cp . get ( 'block_cipher_mode' ) \n    self . _kdw_mski_cp_padding_method = mski_cp . get ( 'padding_method' ) \n    self . _kdw_mski_cp_hashing_algorithm = mski_cp . get ( 'hashing_algorithm' ) \n    self . _kdw_mski_cp_key_role_type = mski_cp . get ( 'key_role_type' ) \n    self . _kdw_mski_cp_digital_signature_algorithm = mski_cp . get ( 'digital_signature_algorithm' ) \n    self . _kdw_mski_cp_cryptographic_algorithm = mski_cp . get ( 'cryptographic_algorithm' ) \n    self . _kdw_mski_cp_random_iv = mski_cp . get ( 'random_iv' ) \n    self . _kdw_mski_cp_iv_length = mski_cp . get ( 'iv_length' ) \n    self . _kdw_mski_cp_tag_length = mski_cp . get ( 'tag_length' ) \n    self . _kdw_mski_cp_fixed_field_length = mski_cp . get ( 'fixed_field_length' ) \n    self . _kdw_mski_cp_invocation_field_length = mski_cp . get ( 'invocation_field_length' ) \n    self . _kdw_mski_cp_counter_length = mski_cp . get ( 'counter_length' ) \n    self . _kdw_mski_cp_initial_counter_value = mski_cp . get ( 'initial_counter_value' ) \n    self . _kdw_mac_signature = value . get ( 'mac_signature' ) \n    self . _kdw_iv_counter_nonce = value . get ( 'iv_counter_nonce' ) \n    self . _kdw_encoding_option = value . get ( 'encoding_option' ) "}
{"4772": "\ndef validate ( self ) : \n    if not isinstance ( self . value , bytes ) : \n        raise TypeError ( \"key value must be bytes\" ) \n    else : \n        if not isinstance ( self . cryptographic_algorithm , enums . CryptographicAlgorithm ) : \n            raise TypeError ( \"key algorithm must be a CryptographicAlgorithm \" \"enumeration\" ) \n        else : \n            if not isinstance ( self . cryptographic_length , six . integer_types ) : \n                raise TypeError ( \"key length must be an integer\" ) \n            else : \n                if not isinstance ( self . key_format_type , enums . KeyFormatType ) : \n                    raise TypeError ( \"key format type must be a KeyFormatType \" \"enumeration\" ) \n                else : \n                    if self . key_format_type not in self . _valid_formats : \n                        raise ValueError ( \"key format type must be one of {0}\" . format ( self . _valid_formats ) ) \n    mask_count = len ( self . cryptographic_usage_masks ) \n    for i in range ( mask_count ) : \n        mask = self . cryptographic_usage_masks [ i ] \n        if not isinstance ( mask , enums . CryptographicUsageMask ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"key mask {0} must be a CryptographicUsageMask \" \"enumeration\" . format ( position ) ) \n    name_count = len ( self . names ) \n    for i in range ( name_count ) : \n        name = self . names [ i ] \n        if not isinstance ( name , six . string_types ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"key name {0} must be a string\" . format ( position ) ) "}
{"4773": "\ndef validate ( self ) : \n    if not isinstance ( self . value , bytes ) : \n        raise TypeError ( \"secret value must be bytes\" ) \n    else : \n        if not isinstance ( self . data_type , enums . SecretDataType ) : \n            raise TypeError ( \"secret data type must be a SecretDataType \" \"enumeration\" ) \n    mask_count = len ( self . cryptographic_usage_masks ) \n    for i in range ( mask_count ) : \n        mask = self . cryptographic_usage_masks [ i ] \n        if not isinstance ( mask , enums . CryptographicUsageMask ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"secret data mask {0} must be a CryptographicUsageMask \" \"enumeration\" . format ( position ) ) \n    name_count = len ( self . names ) \n    for i in range ( name_count ) : \n        name = self . names [ i ] \n        if not isinstance ( name , six . string_types ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"secret data name {0} must be a string\" . format ( position ) ) "}
{"4774": "\ndef validate ( self ) : \n    if not isinstance ( self . value , bytes ) : \n        raise TypeError ( \"opaque value must be bytes\" ) \n    else : \n        if not isinstance ( self . opaque_type , enums . OpaqueDataType ) : \n            raise TypeError ( \"opaque data type must be an OpaqueDataType \" \"enumeration\" ) \n    name_count = len ( self . names ) \n    for i in range ( name_count ) : \n        name = self . names [ i ] \n        if not isinstance ( name , six . string_types ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"opaque data name {0} must be a string\" . format ( position ) ) "}
{"4786": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributeListResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        self . _unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is missing \" \"the unique identifier.\" ) \n    names = list ( ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_NAME , local_buffer ) : \n            name = primitives . TextString ( tag = enums . Tags . ATTRIBUTE_NAME ) \n            name . read ( local_buffer , kmip_version = kmip_version ) \n            names . append ( name ) \n        if len ( names ) == 0 : \n            raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is \" \"missing the attribute names.\" ) \n        self . _attribute_names = names \n    else : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_REFERENCE , local_buffer ) : \n            if self . is_type_next ( enums . Types . STRUCTURE , local_buffer ) : \n                reference = objects . AttributeReference ( ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                names . append ( primitives . TextString ( value = reference . attribute_name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            else : \n                if self . is_type_next ( enums . Types . ENUMERATION , local_buffer ) : \n                    reference = primitives . Enumeration ( enums . Tags , tag = enums . Tags . ATTRIBUTE_REFERENCE ) \n                    reference . read ( local_buffer , kmip_version = kmip_version ) \n                    name = enums . convert_attribute_tag_to_name ( reference . value ) \n                    names . append ( primitives . TextString ( value = name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n                else : \n                    raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding \" \"contains an invalid AttributeReference type.\" ) \n        self . _attribute_names = names \n    self . is_oversized ( local_buffer ) "}
{"4799": "\ndef convert ( self , obj ) : \n    if isinstance ( obj , pobjects . SymmetricKey ) : \n        return self . _build_core_key ( obj , secrets . SymmetricKey ) \n    else : \n        if isinstance ( obj , secrets . SymmetricKey ) : \n            return self . _build_pie_key ( obj , pobjects . SymmetricKey ) \n        else : \n            if isinstance ( obj , pobjects . PublicKey ) : \n                return self . _build_core_key ( obj , secrets . PublicKey ) \n            else : \n                if isinstance ( obj , secrets . PublicKey ) : \n                    return self . _build_pie_key ( obj , pobjects . PublicKey ) \n                else : \n                    if isinstance ( obj , pobjects . PrivateKey ) : \n                        return self . _build_core_key ( obj , secrets . PrivateKey ) \n                    else : \n                        if isinstance ( obj , secrets . PrivateKey ) : \n                            return self . _build_pie_key ( obj , pobjects . PrivateKey ) \n                        else : \n                            if isinstance ( obj , pobjects . Certificate ) : \n                                return self . _build_core_certificate ( obj ) \n                            else : \n                                if isinstance ( obj , secrets . Certificate ) : \n                                    return self . _build_pie_certificate ( obj ) \n                                else : \n                                    if isinstance ( obj , pobjects . SecretData ) : \n                                        return self . _build_core_secret_data ( obj ) \n                                    else : \n                                        if isinstance ( obj , secrets . SecretData ) : \n                                            return self . _build_pie_secret_data ( obj ) \n                                        else : \n                                            if isinstance ( obj , pobjects . OpaqueObject ) : \n                                                return self . _build_core_opaque_object ( obj ) \n                                            else : \n                                                if isinstance ( obj , secrets . OpaqueObject ) : \n                                                    return self . _build_pie_opaque_object ( obj ) \n                                                else : \n                                                    raise TypeError ( \"object type unsupported and cannot be converted\" ) "}
{"4820": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( Credential , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . CREDENTIAL_TYPE , local_stream ) : \n        self . _credential_type = primitives . Enumeration ( enum = enums . CredentialType , tag = enums . Tags . CREDENTIAL_TYPE ) \n        self . _credential_type . read ( local_stream , kmip_version = kmip_version ) \n    else : \n        raise ValueError ( \"Credential encoding missing the credential type.\" ) \n    if self . is_tag_next ( enums . Tags . CREDENTIAL_VALUE , local_stream ) : \n        if self . credential_type == enums . CredentialType . USERNAME_AND_PASSWORD : \n            self . _credential_value = UsernamePasswordCredential ( ) \n        else : \n            if self . credential_type == enums . CredentialType . DEVICE : \n                self . _credential_value = DeviceCredential ( ) \n            else : \n                if self . credential_type == enums . CredentialType . ATTESTATION : \n                    self . _credential_value = AttestationCredential ( ) \n                else : \n                    raise ValueError ( \"Credential encoding includes unrecognized credential \" \"type.\" ) \n        self . _credential_value . read ( local_stream , kmip_version = kmip_version ) \n    else : \n        raise ValueError ( \"Credential encoding missing the credential value.\" ) \n    self . is_oversized ( local_stream ) "}
{"4853": "\ndef mac ( self , algorithm , key , data ) : \n    mac_data = None \n    if algorithm in self . _hash_algorithms . keys ( ) : \n        self . logger . info ( \"Generating a hash-based message authentication code using \" \"{0}\" . format ( algorithm . name ) ) \n        hash_algorithm = self . _hash_algorithms . get ( algorithm ) \n        try : \n            h = hmac . HMAC ( key , hash_algorithm ( ) , backend = default_backend ( ) ) \n            h . update ( data ) \n            mac_data = h . finalize ( ) \n        except Exception as e : \n            self . logger . exception ( e ) \n            raise exceptions . CryptographicFailure ( \"An error occurred while computing an HMAC. \" \"See the server log for more information.\" ) \n    else : \n        if algorithm in self . _symmetric_key_algorithms . keys ( ) : \n            self . logger . info ( \"Generating a cipher-based message authentication code using \" \"{0}\" . format ( algorithm . name ) ) \n            cipher_algorithm = self . _symmetric_key_algorithms . get ( algorithm ) \n            try : \n                c = cmac . CMAC ( cipher_algorithm ( key ) , backend = default_backend ( ) ) \n                c . update ( data ) \n                mac_data = c . finalize ( ) \n            except Exception as e : \n                raise exceptions . CryptographicFailure ( \"An error occurred while computing a CMAC. \" \"See the server log for more information.\" ) \n        else : \n            raise exceptions . InvalidField ( \"The cryptographic algorithm ({0}) is not a supported \" \"for a MAC operation.\" . format ( algorithm ) ) \n    return mac_data "}
{"4856": "\ndef _encrypt_asymmetric ( self , encryption_algorithm , encryption_key , plain_text , padding_method , hashing_algorithm = None ) : \n    if encryption_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . OAEP : \n            hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n            if hash_algorithm is None : \n                raise exceptions . InvalidField ( \"The hashing algorithm '{0}' is not supported for \" \"asymmetric encryption.\" . format ( hashing_algorithm ) ) \n            padding_method = asymmetric_padding . OAEP ( mgf = asymmetric_padding . MGF1 ( algorithm = hash_algorithm ( ) ) , algorithm = hash_algorithm ( ) , label = None ) \n        else : \n            if padding_method == enums . PaddingMethod . PKCS1v15 : \n                padding_method = asymmetric_padding . PKCS1v15 ( ) \n            else : \n                raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for asymmetric \" \"encryption.\" . format ( padding_method ) ) \n        backend = default_backend ( ) \n        try : \n            public_key = backend . load_der_public_key ( encryption_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( encryption_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The public key bytes could not be loaded.\" ) \n        cipher_text = public_key . encrypt ( plain_text , padding_method ) \n        return { 'cipher_text' : cipher_text } \n    else : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm '{0}' is not supported for \" \"asymmetric encryption.\" . format ( encryption_algorithm ) ) "}
{"4857": "\ndef _decrypt_asymmetric ( self , decryption_algorithm , decryption_key , cipher_text , padding_method , hashing_algorithm = None ) : \n    if decryption_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . OAEP : \n            hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n            if hash_algorithm is None : \n                raise exceptions . InvalidField ( \"The hashing algorithm '{0}' is not supported for \" \"asymmetric decryption.\" . format ( hashing_algorithm ) ) \n            padding_method = asymmetric_padding . OAEP ( mgf = asymmetric_padding . MGF1 ( algorithm = hash_algorithm ( ) ) , algorithm = hash_algorithm ( ) , label = None ) \n        else : \n            if padding_method == enums . PaddingMethod . PKCS1v15 : \n                padding_method = asymmetric_padding . PKCS1v15 ( ) \n            else : \n                raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for asymmetric \" \"decryption.\" . format ( padding_method ) ) \n        backend = default_backend ( ) \n        try : \n            private_key = backend . load_der_private_key ( decryption_key , None ) \n        except Exception : \n            try : \n                private_key = backend . load_pem_private_key ( decryption_key , None ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The private key bytes could not be loaded.\" ) \n        plain_text = private_key . decrypt ( cipher_text , padding_method ) \n        return plain_text \n    else : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm '{0}' is not supported for \" \"asymmetric decryption.\" . format ( decryption_algorithm ) ) "}
{"4859": "\ndef derive_key ( self , derivation_method , derivation_length , derivation_data = None , key_material = None , hash_algorithm = None , salt = None , iteration_count = None , encryption_algorithm = None , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    if derivation_method == enums . DerivationMethod . ENCRYPT : \n        result = self . encrypt ( encryption_algorithm = encryption_algorithm , encryption_key = key_material , plain_text = derivation_data , cipher_mode = cipher_mode , padding_method = padding_method , iv_nonce = iv_nonce ) \n        return result . get ( 'cipher_text' ) \n    else : \n        if hash_algorithm is None : \n            raise exceptions . InvalidField ( \"Hash algorithm is required.\" ) \n        hashing_algorithm = self . _encryption_hash_algorithms . get ( hash_algorithm , None ) \n        if hashing_algorithm is None : \n            raise exceptions . InvalidField ( \"Hash algorithm '{0}' is not a supported hashing \" \"algorithm.\" . format ( hash_algorithm ) ) \n        if derivation_method == enums . DerivationMethod . HMAC : \n            df = hkdf . HKDF ( algorithm = hashing_algorithm ( ) , length = derivation_length , salt = salt , info = derivation_data , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        else : \n            if derivation_method == enums . DerivationMethod . HASH : \n                if None not in [ derivation_data , key_material ] : \n                    raise exceptions . InvalidField ( \"For hash-based key derivation, specify only \" \"derivation data or key material, not both.\" ) \n                else : \n                    if derivation_data is not None : \n                        hashing_data = derivation_data \n                    else : \n                        if key_material is not None : \n                            hashing_data = key_material \n                        else : \n                            raise exceptions . InvalidField ( \"For hash-based key derivation, derivation data or \" \"key material must be specified.\" ) \n                df = hashes . Hash ( algorithm = hashing_algorithm ( ) , backend = default_backend ( ) ) \n                df . update ( hashing_data ) \n                derived_data = df . finalize ( ) \n                return derived_data \n            else : \n                if derivation_method == enums . DerivationMethod . PBKDF2 : \n                    if salt is None : \n                        raise exceptions . InvalidField ( \"For PBKDF2 key derivation, salt must be specified.\" ) \n                    if iteration_count is None : \n                        raise exceptions . InvalidField ( \"For PBKDF2 key derivation, iteration count must be \" \"specified.\" ) \n                    df = pbkdf2 . PBKDF2HMAC ( algorithm = hashing_algorithm ( ) , length = derivation_length , salt = salt , iterations = iteration_count , backend = default_backend ( ) ) \n                    derived_data = df . derive ( key_material ) \n                    return derived_data \n                else : \n                    if derivation_method == enums . DerivationMethod . NIST800_108_C : \n                        df = kbkdf . KBKDFHMAC ( algorithm = hashing_algorithm ( ) , mode = kbkdf . Mode . CounterMode , length = derivation_length , rlen = 4 , llen = None , location = kbkdf . CounterLocation . BeforeFixed , label = None , context = None , fixed = derivation_data , backend = default_backend ( ) ) \n                        derived_data = df . derive ( key_material ) \n                        return derived_data \n                    else : \n                        raise exceptions . InvalidField ( \"Derivation method '{0}' is not a supported key \" \"derivation method.\" . format ( derivation_method ) ) "}
{"4861": "\ndef verify_signature ( self , signing_key , message , signature , padding_method , signing_algorithm = None , hashing_algorithm = None , digital_signature_algorithm = None ) : \n    backend = default_backend ( ) \n    hash_algorithm = None \n    dsa_hash_algorithm = None \n    dsa_signing_algorithm = None \n    if hashing_algorithm : \n        hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n    if digital_signature_algorithm : \n        algorithm_pair = self . _digital_signature_algorithms . get ( digital_signature_algorithm ) \n        if algorithm_pair : \n            dsa_hash_algorithm = algorithm_pair [ 0 ] \n            dsa_signing_algorithm = algorithm_pair [ 1 ] \n    if dsa_hash_algorithm and dsa_signing_algorithm : \n        if hash_algorithm and ( hash_algorithm != dsa_hash_algorithm ) : \n            raise exceptions . InvalidField ( \"The hashing algorithm does not match the digital \" \"signature algorithm.\" ) \n        if ( signing_algorithm and ( signing_algorithm != dsa_signing_algorithm ) ) : \n            raise exceptions . InvalidField ( \"The signing algorithm does not match the digital \" \"signature algorithm.\" ) \n        signing_algorithm = dsa_signing_algorithm \n        hash_algorithm = dsa_hash_algorithm \n    if signing_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . PSS : \n            if hash_algorithm : \n                padding = asymmetric_padding . PSS ( mgf = asymmetric_padding . MGF1 ( hash_algorithm ( ) ) , salt_length = asymmetric_padding . PSS . MAX_LENGTH ) \n            else : \n                raise exceptions . InvalidField ( \"A hashing algorithm must be specified for PSS \" \"padding.\" ) \n        else : \n            if padding_method == enums . PaddingMethod . PKCS1v15 : \n                padding = asymmetric_padding . PKCS1v15 ( ) \n            else : \n                raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for signature \" \"verification.\" . format ( padding_method ) ) \n        try : \n            public_key = backend . load_der_public_key ( signing_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( signing_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The signing key bytes could not be loaded.\" ) \n        try : \n            public_key . verify ( signature , message , padding , hash_algorithm ( ) ) \n            return True \n        except errors . InvalidSignature : \n            return False \n        except Exception : \n            raise exceptions . CryptographicFailure ( \"The signature verification process failed.\" ) \n    else : \n        raise exceptions . InvalidField ( \"The signing algorithm '{0}' is not supported for \" \"signature verification.\" . format ( signing_algorithm ) ) "}
{"4865": "\ndef protocol_version_to_kmip_version ( value ) : \n    if not isinstance ( value , ProtocolVersion ) : \n        return None \n    if value . major == 1 : \n        if value . minor == 0 : \n            return enums . KMIPVersion . KMIP_1_0 \n        else : \n            if value . minor == 1 : \n                return enums . KMIPVersion . KMIP_1_1 \n            else : \n                if value . minor == 2 : \n                    return enums . KMIPVersion . KMIP_1_2 \n                else : \n                    if value . minor == 3 : \n                        return enums . KMIPVersion . KMIP_1_3 \n                    else : \n                        if value . minor == 4 : \n                            return enums . KMIPVersion . KMIP_1_4 \n                        else : \n                            return None \n    else : \n        return None "}
{"4886": "\ndef create ( self , algorithm , length , operation_policy_name = None , name = None , cryptographic_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    else : \n        if not isinstance ( length , six . integer_types ) or length <= 0 : \n            raise TypeError ( \"length must be a positive integer\" ) \n    if cryptographic_usage_mask is not None : \n        if not isinstance ( cryptographic_usage_mask , list ) or all ( isinstance ( item , enums . CryptographicUsageMask ) for item in cryptographic_usage_mask ) is False : \n            raise TypeError ( \"cryptographic_usage_mask must be a list of \" \"CryptographicUsageMask enumerations\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    key_attributes = self . _build_key_attributes ( algorithm , length , cryptographic_usage_mask ) \n    key_attributes . extend ( common_attributes ) \n    if name : \n        key_attributes . extend ( self . _build_name_attribute ( name ) ) \n    template = cobjects . TemplateAttribute ( attributes = key_attributes ) \n    result = self . proxy . create ( enums . ObjectType . SYMMETRIC_KEY , template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        return result . uuid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4887": "\ndef create_key_pair ( self , algorithm , length , operation_policy_name = None , public_name = None , public_usage_mask = None , private_name = None , private_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    else : \n        if not isinstance ( length , six . integer_types ) or length <= 0 : \n            raise TypeError ( \"length must be a positive integer\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    algorithm_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_ALGORITHM , algorithm ) \n    length_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_LENGTH , length ) \n    common_attributes . extend ( [ algorithm_attribute , length_attribute ] ) \n    template = cobjects . TemplateAttribute ( attributes = common_attributes , tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n    public_template = None \n    names = None \n    if public_name : \n        names = self . _build_name_attribute ( name = public_name ) \n    attrs = [ ] \n    if public_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , public_usage_mask ) ] \n    if names or attrs : \n        public_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n    private_template = None \n    names = None \n    if private_name : \n        names = self . _build_name_attribute ( name = private_name ) \n    attrs = [ ] \n    if private_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , private_usage_mask ) ] \n    if names or attrs : \n        private_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n    result = self . proxy . create_key_pair ( common_template_attribute = template , private_key_template_attribute = private_template , public_key_template_attribute = public_template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        public_uid = result . public_key_uuid \n        private_uid = result . private_key_uuid \n        return public_uid , private_uid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4898": "\ndef _build_cryptographic_parameters ( self , value ) : \n    if value is None : \n        return None \n    else : \n        if not isinstance ( value , dict ) : \n            raise TypeError ( \"Cryptographic parameters must be a dictionary.\" ) \n    cryptographic_parameters = CryptographicParameters ( block_cipher_mode = value . get ( 'block_cipher_mode' ) , padding_method = value . get ( 'padding_method' ) , hashing_algorithm = value . get ( 'hashing_algorithm' ) , key_role_type = value . get ( 'key_role_type' ) , digital_signature_algorithm = value . get ( 'digital_signature_algorithm' ) , cryptographic_algorithm = value . get ( 'cryptographic_algorithm' ) , random_iv = value . get ( 'random_iv' ) , iv_length = value . get ( 'iv_length' ) , tag_length = value . get ( 'tag_length' ) , fixed_field_length = value . get ( 'fixed_field_length' ) , invocation_field_length = value . get ( 'invocation_field_length' ) , counter_length = value . get ( 'counter_length' ) , initial_counter_value = value . get ( 'initial_counter_value' ) ) \n    return cryptographic_parameters "}
{"4923": "\ndef timesince ( value ) : \n    if not value : \n        return \"\" \n    if not isinstance ( value , datetime . date ) : \n        return value \n    now = datetime . datetime . now ( ) \n    delta = now - value \n    if value > now : \n        return \"right now\" \n    else : \n        if delta . days > 365 : \n            return '%d years ago' % ( delta . days / 365 ) \n        else : \n            if delta . days > 30 : \n                return '%d months ago' % ( delta . days / 30 ) \n            else : \n                if delta . days > 0 : \n                    return '%d days ago' % delta . days \n                else : \n                    if delta . seconds > 3600 : \n                        return '%d hours ago' % ( delta . seconds / 3600 ) \n                    else : \n                        if delta . seconds > 60 : \n                            return '%d minutes ago' % ( delta . seconds / 60 ) \n                        else : \n                            return 'right now' "}
{"4929": "\ndef load_config ( ) : \n    mode = os . environ . get ( 'MODE' ) \n    try : \n        if mode == 'PRODUCTION' : \n            from . production import ProductionConfig \n            return ProductionConfig \n        else : \n            if mode == 'TESTING' : \n                from . testing import TestingConfig \n                return TestingConfig \n            else : \n                from . development import DevelopmentConfig \n                return DevelopmentConfig \n    except ImportError : \n        from . default import Config \n        return Config "}
{"4983": "\ndef menu ( self , tree_alias , tree_branches , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return '' \n    tree_branches = self . resolve_var ( tree_branches ) \n    parent_isnull = False \n    parent_ids = [ ] \n    parent_aliases = [ ] \n    current_item = self . get_tree_current_item ( tree_alias ) \n    self . tree_climber ( tree_alias , current_item ) \n    for branch_id in tree_branches . split ( ',' ) : \n        branch_id = branch_id . strip ( ) \n        if branch_id == ALIAS_TRUNK : \n            parent_isnull = True \n        else : \n            if branch_id == ALIAS_THIS_CHILDREN and current_item is not None : \n                branch_id = current_item . id \n                parent_ids . append ( branch_id ) \n            else : \n                if branch_id == ALIAS_THIS_ANCESTOR_CHILDREN and current_item is not None : \n                    branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id \n                    parent_ids . append ( branch_id ) \n                else : \n                    if branch_id == ALIAS_THIS_SIBLINGS and current_item is not None and current_item . parent is not None : \n                        branch_id = current_item . parent . id \n                        parent_ids . append ( branch_id ) \n                    else : \n                        if branch_id == ALIAS_THIS_PARENT_SIBLINGS and current_item is not None : \n                            branch_id = self . get_ancestor_level ( current_item , depth = 2 ) . id \n                            parent_ids . append ( branch_id ) \n                        else : \n                            if branch_id . isdigit ( ) : \n                                parent_ids . append ( int ( branch_id ) ) \n                            else : \n                                parent_aliases . append ( branch_id ) \n    check_access = self . check_access \n    menu_items = [ ] \n    for item in sitetree_items : \n        if not item . hidden and item . inmenu and check_access ( item , context ) : \n            if item . parent is None : \n                if parent_isnull : \n                    menu_items . append ( item ) \n            else : \n                if item . parent . id in parent_ids or item . parent . alias in parent_aliases : \n                    menu_items . append ( item ) \n    menu_items = self . apply_hook ( menu_items , 'menu' ) \n    self . update_has_children ( tree_alias , menu_items , 'menu' ) \n    return menu_items "}
{"5002": "\ndef redirects_handler ( * args , ** kwargs ) : \n    path = args [ 0 ] . path \n    shift = '../' \n    if 'delete' in path : \n        shift += '../' \n    else : \n        if 'history' in path : \n            if 'item_id' not in kwargs : \n                shift += '../' \n    return HttpResponseRedirect ( path + shift ) "}
{"5003": "\ndef _redirect ( self , request , response ) : \n    if '_addanother' in request . POST : \n        return HttpResponseRedirect ( '../item_add/' ) \n    else : \n        if '_save' in request . POST : \n            return HttpResponseRedirect ( '../' ) \n        else : \n            if '_continue' in request . POST : \n                return response \n    return HttpResponseRedirect ( '' ) "}
{"5013": "\ndef item ( title , url , children = None , url_as_pattern = True , hint = '' , alias = '' , description = '' , in_menu = True , in_breadcrumbs = True , in_sitetree = True , access_loggedin = False , access_guest = False , access_by_perms = None , perms_mode_all = True , ** kwargs ) : \n    item_obj = get_tree_item_model ( ) ( title = title , url = url , urlaspattern = url_as_pattern , hint = hint , alias = alias , description = description , inmenu = in_menu , insitetree = in_sitetree , inbreadcrumbs = in_breadcrumbs , access_loggedin = access_loggedin , access_guest = access_guest , ** kwargs ) \n    item_obj . id = generate_id_for ( item_obj ) \n    item_obj . is_dynamic = True \n    item_obj . dynamic_children = [ ] \n    cleaned_permissions = [ ] \n    if access_by_perms : \n        if not isinstance ( access_by_perms , list ) : \n            access_by_perms = [ access_by_perms ] \n        for perm in access_by_perms : \n            if isinstance ( perm , six . string_types ) : \n                try : \n                    app , codename = perm . split ( '.' ) \n                except ValueError : \n                    raise ValueError ( 'Wrong permission string format: supplied - `%s`; ' 'expected - `<app_name>.<permission_name>`.' % perm ) \n                try : \n                    perm = Permission . objects . get ( codename = codename , content_type__app_label = app ) \n                except Permission . DoesNotExist : \n                    raise ValueError ( 'Permission `%s.%s` does not exist.' % ( app , codename ) ) \n            else : \n                if not isinstance ( perm , ( int , Permission ) ) : \n                    raise ValueError ( 'Permissions must be given as strings, ints, or `Permission` instances.' ) \n            cleaned_permissions . append ( perm ) \n    item_obj . permissions = cleaned_permissions or [ ] \n    item_obj . access_perm_type = item_obj . PERM_TYPE_ALL if perms_mode_all else item_obj . PERM_TYPE_ANY \n    if item_obj . permissions : \n        item_obj . access_restricted = True \n    if children is not None : \n        for child in children : \n            child . parent = item_obj \n            item_obj . dynamic_children . append ( child ) \n    return item_obj "}
{"5021": "\ndef create_http_headers_for_new_span ( context_stack = None , tracer = None ) : \n    if tracer : \n        zipkin_attrs = tracer . get_zipkin_attrs ( ) \n    else : \n        if context_stack : \n            zipkin_attrs = context_stack . get ( ) \n        else : \n            zipkin_attrs = get_default_tracer ( ) . get_zipkin_attrs ( ) \n    if not zipkin_attrs : \n        return { } \n    return { 'X-B3-TraceId' : zipkin_attrs . trace_id , 'X-B3-SpanId' : generate_random_64bit_string ( ) , 'X-B3-ParentSpanId' : zipkin_attrs . span_id , 'X-B3-Flags' : '0' , 'X-B3-Sampled' : '1' if zipkin_attrs . is_sampled else '0' , } "}
{"5022": "\ndef _get_current_context ( self ) : \n    if self . _is_local_root_span : \n        if self . sample_rate is not None : \n            if self . zipkin_attrs_override and not self . zipkin_attrs_override . is_sampled : \n                return True , create_attrs_for_span ( sample_rate = self . sample_rate , trace_id = self . zipkin_attrs_override . trace_id , ) \n            else : \n                if not self . zipkin_attrs_override : \n                    return True , create_attrs_for_span ( sample_rate = self . sample_rate , use_128bit_trace_id = self . use_128bit_trace_id , ) \n        if self . firehose_handler and not self . zipkin_attrs_override : \n            return True , create_attrs_for_span ( sample_rate = 0.0 , use_128bit_trace_id = self . use_128bit_trace_id , ) \n        return False , self . zipkin_attrs_override \n    else : \n        existing_zipkin_attrs = self . get_tracer ( ) . get_zipkin_attrs ( ) \n        if existing_zipkin_attrs : \n            return False , ZipkinAttrs ( trace_id = existing_zipkin_attrs . trace_id , span_id = generate_random_64bit_string ( ) , parent_span_id = existing_zipkin_attrs . span_id , flags = existing_zipkin_attrs . flags , is_sampled = existing_zipkin_attrs . is_sampled , ) \n    return False , None "}
{"5034": "\ndef _get_protobuf_kind ( kind ) : \n    if kind == Kind . CLIENT : \n        return zipkin_pb2 . Span . CLIENT \n    else : \n        if kind == Kind . SERVER : \n            return zipkin_pb2 . Span . SERVER \n        else : \n            if kind == Kind . PRODUCER : \n                return zipkin_pb2 . Span . PRODUCER \n            else : \n                if kind == Kind . CONSUMER : \n                    return zipkin_pb2 . Span . CONSUMER \n    return None "}
{"5046": "\ndef detect_span_version_and_encoding ( message ) : \n    if isinstance ( message , six . string_types ) : \n        if six . PY2 : \n            message = six . b ( message ) \n        else : \n            message = message . encode ( 'utf-8' ) \n    if len ( message ) < 2 : \n        raise ZipkinError ( \"Invalid span format. Message too short.\" ) \n    if six . byte2int ( message ) <= 16 : \n        if six . byte2int ( message ) == 10 and six . byte2int ( message [ 1 : 2 ] ) != 0 : \n            return Encoding . V2_PROTO3 \n        return Encoding . V1_THRIFT \n    str_msg = message . decode ( 'utf-8' ) \n    if str_msg [ 0 ] == '[' : \n        span_list = json . loads ( str_msg ) \n        if len ( span_list ) > 0 : \n            for span in span_list : \n                if any ( word in span for word in _V2_ATTRIBUTES ) : \n                    return Encoding . V2_JSON \n                else : \n                    if ( 'binaryAnnotations' in span or ( 'annotations' in span and 'endpoint' in span [ 'annotations' ] ) ) : \n                        return Encoding . V1_JSON \n            return Encoding . V2_JSON \n    raise ZipkinError ( \"Unknown or unsupported span encoding\" ) "}
{"5050": "\ndef _create_json_endpoint ( self , endpoint , is_v1 ) : \n    json_endpoint = { } \n    if endpoint . service_name : \n        json_endpoint [ 'serviceName' ] = endpoint . service_name \n    else : \n        if is_v1 : \n            json_endpoint [ 'serviceName' ] = \"\" \n    if endpoint . port and endpoint . port != 0 : \n        json_endpoint [ 'port' ] = endpoint . port \n    if endpoint . ipv4 is not None : \n        json_endpoint [ 'ipv4' ] = endpoint . ipv4 \n    if endpoint . ipv6 is not None : \n        json_endpoint [ 'ipv6' ] = endpoint . ipv6 \n    return json_endpoint "}
{"5054": "\ndef _decode_thrift_annotations ( self , thrift_annotations ) : \n    local_endpoint = None \n    kind = Kind . LOCAL \n    all_annotations = { } \n    timestamp = None \n    duration = None \n    for thrift_annotation in thrift_annotations : \n        all_annotations [ thrift_annotation . value ] = thrift_annotation . timestamp \n        if thrift_annotation . host : \n            local_endpoint = self . _convert_from_thrift_endpoint ( thrift_annotation . host , ) \n    if 'cs' in all_annotations and 'sr' not in all_annotations : \n        kind = Kind . CLIENT \n        timestamp = all_annotations [ 'cs' ] \n        duration = all_annotations [ 'cr' ] - all_annotations [ 'cs' ] \n    else : \n        if 'cs' not in all_annotations and 'sr' in all_annotations : \n            kind = Kind . SERVER \n            timestamp = all_annotations [ 'sr' ] \n            duration = all_annotations [ 'ss' ] - all_annotations [ 'sr' ] \n    annotations = { name : self . seconds ( ts ) for name , ts in all_annotations . items ( ) if name not in _DROP_ANNOTATIONS } \n    return annotations , local_endpoint , kind , timestamp , duration "}
{"5055": "\ndef _convert_from_thrift_binary_annotations ( self , thrift_binary_annotations ) : \n    tags = { } \n    local_endpoint = None \n    remote_endpoint = None \n    for binary_annotation in thrift_binary_annotations : \n        if binary_annotation . key == 'sa' : \n            remote_endpoint = self . _convert_from_thrift_endpoint ( thrift_endpoint = binary_annotation . host , ) \n        else : \n            key = binary_annotation . key \n            annotation_type = binary_annotation . annotation_type \n            value = binary_annotation . value \n            if annotation_type == zipkin_core . AnnotationType . BOOL : \n                tags [ key ] = \"true\" if value == 1 else \"false\" \n            else : \n                if annotation_type == zipkin_core . AnnotationType . STRING : \n                    tags [ key ] = str ( value ) \n                else : \n                    log . warning ( 'Only STRING and BOOL binary annotations are ' 'supported right now and can be properly decoded.' ) \n            if binary_annotation . host : \n                local_endpoint = self . _convert_from_thrift_endpoint ( thrift_endpoint = binary_annotation . host , ) \n    return tags , local_endpoint , remote_endpoint "}
{"5064": "\ndef parse ( self , data ) : \n    data = '\\n' . join ( self . strip ( data . split ( '\\n' ) ) ) \n    tag_re = re . compile ( r'^:\\n?(?P<full_tag>(?P<tag>[0-9]{2}|NS)(?P<sub_tag>[A-Z])?):' , re . MULTILINE ) \n    matches = list ( tag_re . finditer ( data ) ) \n    valid_matches = list ( self . sanatize_tag_id_matches ( matches ) ) \n    for i , match in enumerate ( valid_matches ) : \n        tag_id = self . normalize_tag_id ( match . group ( 'tag' ) ) \n        tag = self . tags . get ( match . group ( 'full_tag' ) ) or self . tags [ tag_id ] \n        if valid_matches [ i + 1 : ] : \n            tag_data = data [ match . end ( ) : valid_matches [ i + 1 ] . start ( ) ] . strip ( ) \n        else : \n            tag_data = data [ match . end ( ) : ] . strip ( ) \n        tag_dict = tag . parse ( self , tag_data ) \n        for processor in self . processors . get ( 'pre_%s' % tag . slug , [ ] ) : \n            tag_dict = processor ( self , tag , tag_dict ) \n        result = tag ( self , tag_dict ) \n        for processor in self . processors . get ( 'post_%s' % tag . slug , [ ] ) : \n            result = processor ( self , tag , tag_dict , result ) \n        if isinstance ( tag , mt940 . tags . Statement ) : \n            if not self . transactions : \n                transaction = Transaction ( self ) \n                self . transactions . append ( transaction ) \n            if transaction . data . get ( 'id' ) : \n                transaction = Transaction ( self , result ) \n                self . transactions . append ( transaction ) \n            else : \n                transaction . data . update ( result ) \n        else : \n            if issubclass ( tag . scope , Transaction ) and self . transactions : \n                for k , v in _compat . iteritems ( result ) : \n                    if k in transaction . data and hasattr ( v , 'strip' ) : \n                        transaction . data [ k ] += '\\n%s' % v . strip ( ) \n                    else : \n                        transaction . data [ k ] = v \n            else : \n                if issubclass ( tag . scope , Transactions ) : \n                    self . data . update ( result ) \n    return self . transactions "}
{"5065": "\ndef parse ( src , encoding = None ) : \n    def safe_is_file ( filename ) : \n        try : \n            return os . path . isfile ( src ) \n        except ValueError : \n            return False \n    if hasattr ( src , 'read' ) : \n        data = src . read ( ) \n    else : \n        if safe_is_file ( src ) : \n            with open ( src , 'rb' ) as fh : \n                data = fh . read ( ) \n        else : \n            data = src \n    if hasattr ( data , 'decode' ) : \n        exception = None \n        encodings = [ encoding , 'utf-8' , 'cp852' , 'iso8859-15' , 'latin1' ] \n        for encoding in encodings : \n            if not encoding : \n                continue \n            try : \n                data = data . decode ( encoding ) \n                break \n            except UnicodeDecodeError as e : \n                exception = e \n            except UnicodeEncodeError : \n                break \n        else : \n            raise exception \n    transactions = mt940 . models . Transactions ( ) \n    transactions . parse ( data ) \n    return transactions "}
{"5069": "\nasync def request ( self , method , url , ** kwargs ) : \n    rate_limiter = RateLimiter ( max_calls = 59 , period = 60 , callback = limited ) \n    async with rate_limiter : \n        if not self . token : \n            raise UnauthorizedDetected ( 'UnauthorizedDetected (status code: 401): No TOKEN provided' ) \n        headers = { 'User-Agent' : self . user_agent , 'Content-Type' : 'application/json' } \n        if 'json' in kwargs : \n            kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) \n        kwargs [ 'headers' ] = headers \n        headers [ 'Authorization' ] = self . token \n        for tries in range ( 5 ) : \n            async with self . session . request ( method , url , ** kwargs ) as resp : \n                log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) \n                data = await json_or_text ( resp ) \n                if 300 > resp . status >= 200 : \n                    return data \n                if resp . status == 429 : \n                    fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' \n                    retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) \n                    mins = retry_after / 60 \n                    log . warning ( fmt , retry_after , mins ) \n                    is_global = True \n                    if is_global : \n                        self . _global_over . clear ( ) \n                    await asyncio . sleep ( retry_after , loop = self . loop ) \n                    log . debug ( 'Done sleeping for the rate limit. Retrying...' ) \n                    if is_global : \n                        self . _global_over . set ( ) \n                        log . debug ( 'Global rate limit is now over.' ) \n                    continue \n                if resp . status == 400 : \n                    raise HTTPException ( resp , data ) \n                else : \n                    if resp . status == 401 : \n                        raise Unauthorized ( resp , data ) \n                    else : \n                        if resp . status == 403 : \n                            raise Forbidden ( resp , data ) \n                        else : \n                            if resp . status == 404 : \n                                raise NotFound ( resp , data ) \n                            else : \n                                raise HTTPException ( resp , data ) \n        raise HTTPException ( resp , data ) "}
{"5076": "\ndef encode ( term , compressed = False ) : \n    encoded_term = encode_term ( term ) \n    if compressed : \n        if compressed is True : \n            compressed = 6 \n        else : \n            if compressed < 0 or compressed > 9 : \n                raise ValueError ( \"invalid compression level: %r\" % ( compressed , ) ) \n        zlib_term = compress ( encoded_term , compressed ) \n        ln = len ( encoded_term ) \n        if len ( zlib_term ) + 5 <= ln : \n            return b\"\\x83P\" + _int4_pack ( ln ) + zlib_term \n    return b\"\\x83\" + encoded_term "}
{"5088": "\ndef save ( self , ** kwargs ) : \n    child_relation_names = [ rel . get_accessor_name ( ) for rel in get_all_child_relations ( self ) ] \n    child_m2m_field_names = [ field . name for field in get_all_child_m2m_relations ( self ) ] \n    update_fields = kwargs . pop ( 'update_fields' , None ) \n    if update_fields is None : \n        real_update_fields = None \n        relations_to_commit = child_relation_names \n        m2m_fields_to_commit = child_m2m_field_names \n    else : \n        real_update_fields = [ ] \n        relations_to_commit = [ ] \n        m2m_fields_to_commit = [ ] \n        for field in update_fields : \n            if field in child_relation_names : \n                relations_to_commit . append ( field ) \n            else : \n                if field in child_m2m_field_names : \n                    m2m_fields_to_commit . append ( field ) \n                else : \n                    real_update_fields . append ( field ) \n    super ( ClusterableModel , self ) . save ( update_fields = real_update_fields , ** kwargs ) \n    for relation in relations_to_commit : \n        getattr ( self , relation ) . commit ( ) \n    for field in m2m_fields_to_commit : \n        getattr ( self , field ) . commit ( ) "}
{"5125": "\ndef _interpret_response ( self , response , payload , expected_status ) : \n    raw_content = response . text \n    if not raw_content : \n        raise with_context ( exc = BadApiResponse ( 'Empty {status} response from node.' . format ( status = response . status_code , ) , ) , context = { 'request' : payload , } , ) \n    try : \n        decoded = json . loads ( raw_content ) \n    except ValueError : \n        raise with_context ( exc = BadApiResponse ( 'Non-JSON {status} response from node: ' '{raw_content}' . format ( status = response . status_code , raw_content = raw_content , ) ) , context = { 'request' : payload , 'raw_response' : raw_content , } , ) \n    if not isinstance ( decoded , dict ) : \n        raise with_context ( exc = BadApiResponse ( 'Malformed {status} response from node: {decoded!r}' . format ( status = response . status_code , decoded = decoded , ) , ) , context = { 'request' : payload , 'response' : decoded , } , ) \n    if response . status_code in expected_status : \n        return decoded \n    error = None \n    try : \n        if response . status_code == codes [ 'bad_request' ] : \n            error = decoded [ 'error' ] \n        else : \n            if response . status_code == codes [ 'internal_server_error' ] : \n                error = decoded [ 'exception' ] \n    except KeyError : \n        pass \n    raise with_context ( exc = BadApiResponse ( '{status} response from node: {error}' . format ( error = error or decoded , status = response . status_code , ) , ) , context = { 'request' : payload , 'response' : decoded , } , ) "}
{"5135": "\ndef check_trytes_codec ( encoding ) : \n    if encoding == AsciiTrytesCodec . name : \n        return AsciiTrytesCodec . get_codec_info ( ) \n    else : \n        if encoding == AsciiTrytesCodec . compat_name : \n            warn ( '\"{old_codec}\" codec will be removed in PyOTA v2.1. ' 'Use \"{new_codec}\" instead.' . format ( new_codec = AsciiTrytesCodec . name , old_codec = AsciiTrytesCodec . compat_name , ) , DeprecationWarning , ) \n            return AsciiTrytesCodec . get_codec_info ( ) \n    return None "}
{"5138": "\ndef decode ( self , input , errors = 'strict' ) : \n    if isinstance ( input , memoryview ) : \n        input = input . tobytes ( ) \n    if not isinstance ( input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) \n    if not isinstance ( input , bytearray ) : \n        input = bytearray ( input ) \n    bytes_ = bytearray ( ) \n    for i in range ( 0 , len ( input ) , 2 ) : \n        try : \n            first , second = input [ i : i + 2 ] \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) \n            else : \n                if errors == 'replace' : \n                    bytes_ += b'?' \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) \n            else : \n                if errors == 'replace' : \n                    bytes_ += b'?' \n    return binary_type ( bytes_ ) , len ( input ) "}
{"5162": "\ndef _repr_pretty_ ( self , p , cycle ) : \n    class_name = type ( self ) . __name__ \n    if cycle : \n        p . text ( '{cls}(...)' . format ( cls = class_name , ) ) \n    else : \n        with p . group ( len ( class_name ) + 1 , '{cls}(' . format ( cls = class_name ) , ')' , ) : \n            prepared = self . as_json_compatible ( ) \n            if isinstance ( prepared , Mapping ) : \n                p . text ( '**' ) \n            else : \n                if isinstance ( prepared , Iterable ) : \n                    p . text ( '*' ) \n            p . pretty ( prepared ) "}
{"5170": "\ndef finalize ( self ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not self : \n        raise ValueError ( 'Bundle has no transactions.' ) \n    balance = self . balance \n    if balance < 0 : \n        if self . change_address : \n            self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) \n        else : \n            raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) \n    else : \n        if balance > 0 : \n            raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) \n    while True : \n        sponge = Kerl ( ) \n        last_index = len ( self ) - 1 \n        for i , txn in enumerate ( self ) : \n            txn . current_index = i \n            txn . last_index = last_index \n            sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) \n        bundle_hash_trits = [ 0 ] * HASH_LENGTH \n        sponge . squeeze ( bundle_hash_trits ) \n        bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) \n        if any ( 13 in part for part in normalize ( bundle_hash ) ) : \n            tail_transaction = ( self . tail_transaction ) \n            tail_transaction . increment_legacy_tag ( ) \n        else : \n            break \n    for txn in self : \n        txn . bundle_hash = bundle_hash \n        txn . signature_message_fragment = Fragment ( txn . message or b'' ) "}
{"5189": "\ndef from_csv ( cls , filename = None , text = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise LegendError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    r = csv . DictReader ( f , skipinitialspace = True ) \n    list_of_Decors , components = [ ] , [ ] \n    kind = 'component' \n    for row in r : \n        d , component = { } , { } \n        for ( k , v ) in row . items ( ) : \n            if ( k in [ None , '' ] ) : \n                continue \n            if ( v in [ None , '' ] ) : \n                if k . lower ( ) not in [ 'color' , 'colour' ] : \n                    continue \n            if k [ : 4 ] . lower ( ) == 'comp' : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                if v . lower ( ) == 'true' : \n                    component [ prop ] = True \n                else : \n                    if v . lower ( ) == 'false' : \n                        component [ prop ] = False \n                    else : \n                        try : \n                            component [ prop ] = float ( v ) \n                        except ValueError : \n                            component [ prop ] = v . lower ( ) \n            else : \n                if k [ : 5 ] . lower ( ) == 'curve' : \n                    prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                    component [ prop ] = v . lower ( ) \n                    kind = 'curve' \n                else : \n                    try : \n                        d [ k ] = float ( v ) \n                    except ValueError : \n                        d [ k ] = v . lower ( ) \n        this_component = Component ( component ) \n        d [ kind ] = this_component \n        if this_component in components : \n            with warnings . catch_warnings ( ) : \n                warnings . simplefilter ( \"always\" ) \n                w = \"This legend contains duplicate components.\" \n                warnings . warn ( w ) \n        components . append ( this_component ) \n        list_of_Decors . append ( Decor ( d ) ) \n    return cls ( list_of_Decors ) "}
{"5213": "\ndef to_csv ( self , filename = None , as_text = True , use_descriptions = False , dlm = \",\" , header = True ) : \n    if ( filename is None ) : \n        if ( not as_text ) : \n            raise StriplogError ( \"You must provide a filename or set as_text to True.\" ) \n    else : \n        as_text = False \n    if as_text : \n        output = StringIO ( ) \n    else : \n        output = open ( filename , 'w' ) \n    fieldnames = [ 'Top' , 'Base' , 'Component' ] \n    writer = csv . DictWriter ( output , delimiter = dlm , fieldnames = fieldnames , quoting = csv . QUOTE_MINIMAL ) \n    if header : \n        writer . writeheader ( ) \n    for i in self . __list : \n        if use_descriptions and i . description : \n            text = i . description \n        else : \n            if i . primary : \n                text = i . primary . summary ( ) \n            else : \n                text = '' \n        data = { j : k for j , k in zip ( fieldnames , [ i . top . z , i . base . z , text ] ) } \n        writer . writerow ( data ) \n    if as_text : \n        return output . getvalue ( ) \n    else : \n        output . close \n        return None "}
{"5215": "\ndef plot_axis ( self , ax , legend , ladder = False , default_width = 1 , match_only = None , colour = None , colour_function = None , cmap = None , default = None , width_field = None , ** kwargs ) : \n    default_c = None \n    patches = [ ] \n    for iv in self . __list : \n        origin = ( 0 , iv . top . z ) \n        d = legend . get_decor ( iv . primary , match_only = match_only ) \n        thick = iv . base . z - iv . top . z \n        if ladder : \n            if width_field is not None : \n                w = iv . data . get ( width_field , 1 ) \n                w = default_width * w / self . max_field ( width_field ) \n                default_c = 'gray' \n            else : \n                if legend is not None : \n                    w = d . width or default_width \n                    try : \n                        w = default_width * w / legend . max_width \n                    except : \n                        w = default_width \n        else : \n            w = default_width \n        this_patch_kwargs = kwargs . copy ( ) \n        lw = this_patch_kwargs . pop ( 'lw' , 0 ) \n        ec = this_patch_kwargs . pop ( 'ec' , 'k' ) \n        fc = this_patch_kwargs . pop ( 'fc' , None ) or default_c or d . colour \n        if colour is None : \n            rect = mpl . patches . Rectangle ( origin , w , thick , fc = fc , lw = lw , hatch = d . hatch , ec = ec , ** this_patch_kwargs ) \n            ax . add_patch ( rect ) \n        else : \n            rect = mpl . patches . Rectangle ( origin , w , thick , lw = lw , ec = ec , ** this_patch_kwargs ) \n            patches . append ( rect ) \n    if colour is not None : \n        cmap = cmap or 'viridis' \n        p = mpl . collections . PatchCollection ( patches , cmap = cmap , lw = lw ) \n        p . set_array ( self . get_data ( colour , colour_function , default = default ) ) \n        ax . add_collection ( p ) \n        cb = plt . colorbar ( p ) \n        cb . outline . set_linewidth ( 0 ) \n    return ax "}
{"5218": "\ndef find ( self , search_term , index = False ) : \n    hits = [ ] \n    for i , iv in enumerate ( self ) : \n        try : \n            search_text = iv . description or iv . primary . summary ( ) \n            pattern = re . compile ( search_term , flags = re . IGNORECASE ) \n            if pattern . search ( search_text ) : \n                hits . append ( i ) \n        except TypeError : \n            if search_term in iv . components : \n                hits . append ( i ) \n    if hits and index : \n        return hits \n    else : \n        if hits : \n            return self [ hits ] \n        else : \n            return "}
{"5242": "\ndef drain_node_with_spec ( self , id , drain_spec , mark_eligible = None ) : \n    payload = { } \n    if drain_spec and mark_eligible is not None : \n        payload = { \"NodeID\" : id , \"DrainSpec\" : drain_spec , \"MarkEligible\" : mark_eligible } \n    else : \n        if drain_spec and mark_eligible is None : \n            payload = { \"NodeID\" : id , \"DrainSpec\" : drain_spec } \n        else : \n            if not drain_spec and mark_eligible is not None : \n                payload = { \"NodeID\" : id , \"DrainSpec\" : None , \"MarkEligible\" : mark_eligible } \n            else : \n                if not drain_spec and mark_eligible is None : \n                    payload = { \"NodeID\" : id , \"DrainSpec\" : None , } \n    return self . request ( id , \"drain\" , json = payload , method = \"post\" ) . json ( ) "}
{"5243": "\ndef eligible_node ( self , id , eligible = None , ineligible = None ) : \n    payload = { } \n    if eligible is not None and ineligible is not None : \n        raise nomad . api . exceptions . InvalidParameters \n    if eligible is None and ineligible is None : \n        raise nomad . api . exceptions . InvalidParameters \n    if eligible is not None and eligible : \n        payload = { \"Eligibility\" : \"eligible\" , \"NodeID\" : id } \n    else : \n        if eligible is not None and not eligible : \n            payload = { \"Eligibility\" : \"ineligible\" , \"NodeID\" : id } \n        else : \n            if ineligible is not None : \n                payload = { \"Eligibility\" : \"ineligible\" , \"NodeID\" : id } \n            else : \n                if ineligible is not None and not ineligible : \n                    payload = { \"Eligibility\" : \"eligible\" , \"NodeID\" : id } \n    return self . request ( id , \"eligibility\" , json = payload , method = \"post\" ) . json ( ) "}
{"5276": "\ndef start ( self ) : \n    from . pjf_worker import PJFWorker \n    worker = PJFWorker ( self ) \n    if self . update_pjf : \n        worker . update_library ( ) \n    else : \n        if self . browser_auto : \n            worker . browser_autopwn ( ) \n        else : \n            if self . fuzz_web : \n                worker . web_fuzzer ( ) \n            else : \n                if self . json : \n                    if not self . web_server and not self . ext_fuzz and not self . cmd_fuzz : \n                        worker . fuzz ( ) \n                    else : \n                        if self . ext_fuzz : \n                            if self . stdin : \n                                worker . fuzz_stdin ( ) \n                            else : \n                                worker . fuzz_command_line ( ) \n                        else : \n                            if self . cmd_fuzz : \n                                if self . stdin : \n                                    worker . fuzz_external ( True ) \n                                else : \n                                    worker . fuzz_external ( ) \n                            else : \n                                worker . start_http_server ( ) \n                else : \n                    if self . json_file : \n                        worker . start_file_fuzz ( ) \n                    else : \n                        if self . process_to_monitor : \n                            worker . start_process_monitor ( ) "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = True ) : \n            if type ( d ) == dict : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = False ) \n                    else : \n                        if type ( d [ k ] ) == str : \n                            if _decode : \n                                d [ k ] = decode ( d [ k ] ) \n                            else : \n                                d [ k ] = encode ( d [ k ] ) \n            else : \n                if type ( d ) == list : \n                    arr = [ ] \n                    for e in d : \n                        if type ( e ) == str : \n                            if _decode : \n                                arr . append ( decode ( e ) ) \n                            else : \n                                arr . append ( encode ( e ) ) \n                        else : \n                            if type ( e ) in [ dict , list ] : \n                                if _decode : \n                                    arr . append ( encode_decode_all ( e ) ) \n                                else : \n                                    arr . append ( encode_decode_all ( e , _decode = False ) ) \n                            else : \n                                arr . append ( e ) \n                    return arr \n                else : \n                    if _decode : \n                        return decode ( d ) \n                    else : \n                        return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if sys . version_info >= ( 3 , 0 ) : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if sys . version_info >= ( 3 , 0 ) : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5 ) ) , _decode = False ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = False ) \n    return func_wrapper "}
{"5281": "\ndef build ( self , pre = None , shortest = False ) : \n    res = super ( Q , self ) . build ( pre , shortest = shortest ) \n    if self . escape : \n        return repr ( res ) \n    else : \n        if self . html_js_escape : \n            return ( \"'\" + res . encode ( \"string_escape\" ) . replace ( \"<\" , \"\\\\x3c\" ) . replace ( \">\" , \"\\\\x3e\" ) + \"'\" ) \n        else : \n            return \"{q}{r}{q}\" . format ( q = self . quote , r = res ) "}
{"5285": "\ndef build ( self , pre = None , shortest = False ) : \n    if pre is None : \n        pre = [ ] \n    if shortest : \n        raise errors . OptGram \n    else : \n        if rand . maybe ( ) : \n            return super ( STAR , self ) . build ( pre , shortest = shortest ) \n        else : \n            raise errors . OptGram "}
{"5293": "\ndef fuzz_elements ( self , element ) : \n    try : \n        if type ( element ) == dict : \n            tmp_element = { } \n            for key in element : \n                if len ( self . config . parameters ) > 0 : \n                    if self . config . exclude_parameters : \n                        fuzz = key not in self . config . parameters \n                    else : \n                        fuzz = key in self . config . parameters \n                else : \n                    fuzz = True \n                if fuzz : \n                    if type ( element [ key ] ) == dict : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    else : \n                        if type ( element [ key ] ) == list : \n                            tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                        else : \n                            tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) \n                else : \n                    tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n            element = tmp_element \n            del tmp_element \n        else : \n            if type ( element ) == list : \n                arr = [ ] \n                for key in element : \n                    if type ( key ) == dict : \n                        arr . append ( self . fuzz_elements ( key ) ) \n                    else : \n                        if type ( key ) == list : \n                            arr . append ( self . fuzz_elements ( key ) ) \n                        else : \n                            if len ( self . config . parameters ) <= 0 : \n                                arr . append ( self . mutator . fuzz ( key ) ) \n                            else : \n                                arr . append ( key ) \n                element = arr \n                del arr \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) \n    return element "}
{"5319": "\ndef verifyToken ( self , auth ) : \n    if auth in ( self . Auth . SkypeToken , self . Auth . Authorize ) : \n        if \"skype\" not in self . tokenExpiry or datetime . now ( ) >= self . tokenExpiry [ \"skype\" ] : \n            if not hasattr ( self , \"getSkypeToken\" ) : \n                raise SkypeAuthException ( \"Skype token expired, and no password specified\" ) \n            self . getSkypeToken ( ) \n    else : \n        if auth == self . Auth . RegToken : \n            if \"reg\" not in self . tokenExpiry or datetime . now ( ) >= self . tokenExpiry [ \"reg\" ] : \n                self . getRegToken ( ) "}
{"5350": "\ndef _check_index ( self , key : Union [ slice , int ] ) -> ( int , int ) : \n    ss , se = self . _span \n    if isinstance ( key , int ) : \n        if key < 0 : \n            key += se - ss \n            if key < 0 : \n                raise IndexError ( 'index out of range' ) \n        else : \n            if key >= se - ss : \n                raise IndexError ( 'index out of range' ) \n        start = ss + key \n        return start , start + 1 \n    if key . step is not None : \n        raise NotImplementedError ( 'step is not implemented for string setter.' ) \n    start , stop = key . start or 0 , key . stop \n    if start < 0 : \n        start += se - ss \n        if start < 0 : \n            raise IndexError ( 'start index out of range' ) \n    if stop is None : \n        stop = se - ss \n    else : \n        if stop < 0 : \n            stop += se - ss \n    if start > stop : \n        raise IndexError ( 'stop index out of range or start is after the stop' ) \n    return start + ss , stop + ss "}
{"5351": "\ndef insert ( self , index : int , string : str ) -> None : \n    ss , se = self . _span \n    lststr = self . _lststr \n    lststr0 = lststr [ 0 ] \n    if index < 0 : \n        index += se - ss \n        if index < 0 : \n            index = 0 \n    else : \n        if index > se - ss : \n            index = se - ss \n    index += ss \n    lststr [ 0 ] = lststr0 [ : index ] + string + lststr0 [ index : ] \n    string_len = len ( string ) \n    self . _insert_update ( index = index , length = string_len ) \n    type_to_spans = self . _type_to_spans \n    for type_ , spans in parse_to_spans ( bytearray ( string , 'ascii' , 'replace' ) ) . items ( ) : \n        for s , e in spans : \n            insort ( type_to_spans [ type_ ] , [ index + s , index + e ] ) "}
{"5378": "\ndef rm_dup_args_safe ( self , tag : str = None ) -> None : \n    name_to_lastarg_vals = { } \n    for arg in reversed ( self . arguments ) : \n        name = arg . name . strip ( WS ) \n        if arg . positional : \n            val = arg . value \n        else : \n            val = arg . value . strip ( WS ) \n        if name in name_to_lastarg_vals : \n            if not val : \n                del arg [ 0 : len ( arg . string ) ] \n            else : \n                lastarg , dup_vals = name_to_lastarg_vals [ name ] \n                if val in dup_vals : \n                    del arg [ 0 : len ( arg . string ) ] \n                else : \n                    if '' in dup_vals : \n                        del lastarg [ 0 : len ( lastarg . string ) ] \n                        dup_vals . pop ( 0 ) \n                    else : \n                        dup_vals . append ( val ) \n                        if tag : \n                            arg . value += tag \n        else : \n            name_to_lastarg_vals [ name ] = ( arg , [ val ] ) "}
{"5379": "\ndef set_arg ( self , name : str , value : str , positional : bool = None , before : str = None , after : str = None , preserve_spacing : bool = True ) -> None : \n    args = list ( reversed ( self . arguments ) ) \n    arg = get_arg ( name , args ) \n    if arg : \n        if positional : \n            arg . positional = positional \n        if preserve_spacing : \n            val = arg . value \n            arg . value = val . replace ( val . strip ( WS ) , value ) \n        else : \n            arg . value = value \n        return \n    if not name and positional is None : \n        positional = True \n    if not positional and preserve_spacing and args : \n        before_names = [ ] \n        name_lengths = [ ] \n        before_values = [ ] \n        after_values = [ ] \n        for arg in args : \n            aname = arg . name \n            name_len = len ( aname ) \n            name_lengths . append ( name_len ) \n            before_names . append ( STARTING_WS_MATCH ( aname ) [ 0 ] ) \n            arg_value = arg . value \n            before_values . append ( STARTING_WS_MATCH ( arg_value ) [ 0 ] ) \n            after_values . append ( ENDING_WS_MATCH ( arg_value ) [ 0 ] ) \n        pre_name_ws_mode = mode ( before_names ) \n        name_length_mode = mode ( name_lengths ) \n        post_value_ws_mode = mode ( [ SPACE_AFTER_SEARCH ( self . string ) [ 0 ] ] + after_values [ 1 : ] ) \n        pre_value_ws_mode = mode ( before_values ) \n    else : \n        preserve_spacing = False \n    if positional : \n        addstring = '|' + value \n    else : \n        if preserve_spacing : \n            addstring = ( '|' + ( pre_name_ws_mode + name . strip ( WS ) ) . ljust ( name_length_mode ) + '=' + pre_value_ws_mode + value + post_value_ws_mode ) \n        else : \n            addstring = '|' + name + '=' + value \n    if before : \n        arg = get_arg ( before , args ) \n        arg . insert ( 0 , addstring ) \n    else : \n        if after : \n            arg = get_arg ( after , args ) \n            arg . insert ( len ( arg . string ) , addstring ) \n        else : \n            if args and not positional : \n                arg = args [ 0 ] \n                arg_string = arg . string \n                if preserve_spacing : \n                    arg [ 0 : len ( arg_string ) ] = ( arg . string . rstrip ( WS ) + post_value_ws_mode + addstring . rstrip ( WS ) + after_values [ 0 ] ) \n                else : \n                    arg . insert ( len ( arg_string ) , addstring ) \n            else : \n                self . insert ( - 2 , addstring ) "}
{"5386": "\ndef from_file ( filepath ) : \n    if filepath . endswith ( \".prj\" ) : \n        string = open ( filepath , \"r\" ) . read ( ) \n        return parse . from_unknown_wkt ( string ) \n    else : \n        if filepath . endswith ( ( \".geojson\" , \".json\" ) ) : \n            raw = open ( filepath ) . read ( ) \n            geoj = json . loads ( raw ) \n            if \"crs\" in geoj : \n                crsinfo = geoj [ \"crs\" ] \n                if crsinfo [ \"type\" ] == \"name\" : \n                    string = crsinfo [ \"properties\" ] [ \"name\" ] \n                    return parse . from_unknown_text ( string ) \n                else : \n                    if crsinfo [ \"type\" ] == \"link\" : \n                        url = crsinfo [ \"properties\" ] [ \"name\" ] \n                        type = crsinfo [ \"properties\" ] . get ( \"type\" ) \n                        return from_url ( url , format = type ) \n                    else : \n                        raise FormatError ( \"Invalid GeoJSON crs type: must be either 'name' or 'link'\" ) \n            else : \n                return parse . from_epsg_code ( \"4326\" ) "}
{"5390": "\ndef from_unknown_text ( text , strict = False ) : \n    if text . startswith ( \"+\" ) : \n        crs = from_proj4 ( text , strict ) \n    else : \n        if text . startswith ( ( \"PROJCS[\" , \"GEOGCS[\" ) ) : \n            crs = from_unknown_wkt ( text , strict ) \n        else : \n            if text . startswith ( \"EPSG:\" ) : \n                crs = from_epsg_code ( text . split ( \":\" ) [ 1 ] ) \n            else : \n                if text . startswith ( \"ESRI:\" ) : \n                    crs = from_esri_code ( text . split ( \":\" ) [ 1 ] ) \n                else : \n                    if text . startswith ( \"SR-ORG:\" ) : \n                        crs = from_sr_code ( text . split ( \":\" ) [ 1 ] ) \n                    else : \n                        raise FormatError ( \"Could not auto-detect the type of crs format, make sure it is one of the supported formats\" ) \n    return crs "}
{"5394": "\ndef parse_geo_tiff ( key_dir_vlr : GeoKeyDirectoryVlr , double_vlr : GeoDoubleParamsVlr , ascii_vlr : GeoAsciiParamsVlr , ) -> List [ GeoTiffKey ] : \n    geotiff_keys = [ ] \n    for k in key_dir_vlr . geo_keys : \n        if k . tiff_tag_location == 0 : \n            value = k . value_offset \n        else : \n            if k . tiff_tag_location == 34736 : \n                value = double_vlr . doubles [ k . value_offset ] \n            else : \n                if k . tiff_tag_location == 34737 : \n                    try : \n                        value = ascii_vlr . strings [ k . value_offset ] [ k . count : ] \n                    except IndexError : \n                        value = ascii_vlr . strings [ 0 ] [ k . value_offset : k . value_offset + k . count ] \n                else : \n                    logger . warning ( \"GeoTiffKey with unknown tiff tag location ({})\" . format ( k . tiff_tag_location ) ) \n                    continue \n        geotiff_keys . append ( GeoTiffKey ( k . id , value ) ) \n    return geotiff_keys "}
{"5395": "\ndef get_signedness_for_extra_dim ( type_index ) : \n    try : \n        t = _extra_dims_style_2 [ type_index ] \n        if \"uint\" in t : \n            return DimensionSignedness . UNSIGNED \n        else : \n            if \"int\" in t : \n                return DimensionSignedness . SIGNED \n            else : \n                return DimensionSignedness . FLOATING \n    except IndexError : \n        raise errors . UnknownExtraType ( type_index ) "}
{"5429": "\ndef open_las ( source , closefd = True ) : \n    if isinstance ( source , str ) : \n        stream = open ( source , mode = \"rb\" ) \n        if not closefd : \n            raise ValueError ( \"Cannot use closefd with filename\" ) \n    else : \n        if isinstance ( source , bytes ) : \n            stream = io . BytesIO ( source ) \n        else : \n            stream = source \n    return LasReader ( stream , closefd = closefd ) "}
{"5468": "\ndef guess_type ( self , path , allow_directory = True ) : \n    if path . endswith ( '.ipynb' ) : \n        return 'notebook' \n    else : \n        if allow_directory and self . dir_exists ( path ) : \n            return 'directory' \n        else : \n            return 'file' "}
{"5473": "\ndef _convert_file_records ( self , file_records ) : \n    for record in file_records : \n        type_ = self . guess_type ( record [ 'name' ] , allow_directory = False ) \n        if type_ == 'notebook' : \n            yield self . _notebook_model_from_db ( record , False ) \n        else : \n            if type_ == 'file' : \n                yield self . _file_model_from_db ( record , False , None ) \n            else : \n                self . do_500 ( \"Unknown file type %s\" % type_ ) "}
{"5478": "\ndef rename_file ( self , old_path , path ) : \n    with self . engine . begin ( ) as db : \n        try : \n            if self . file_exists ( old_path ) : \n                rename_file ( db , self . user_id , old_path , path ) \n            else : \n                if self . dir_exists ( old_path ) : \n                    rename_directory ( db , self . user_id , old_path , path ) \n                else : \n                    self . no_such_entity ( path ) \n        except ( FileExists , DirectoryExists ) : \n            self . already_exists ( path ) \n        except RenameRoot as e : \n            self . do_409 ( str ( e ) ) "}
{"5479": "\ndef delete_file ( self , path ) : \n    if self . file_exists ( path ) : \n        self . _delete_non_directory ( path ) \n    else : \n        if self . dir_exists ( path ) : \n            self . _delete_directory ( path ) \n        else : \n            self . no_such_entity ( path ) "}
{"5507": "\ndef _get_name ( column_like ) : \n    if isinstance ( column_like , Column ) : \n        return column_like . name \n    else : \n        if isinstance ( column_like , Cast ) : \n            return column_like . clause . name "}
{"5525": "\ndef normalize_api_path ( api_path ) : \n    normalized = posixpath . normpath ( api_path . strip ( '/' ) ) \n    if normalized == '.' : \n        normalized = '' \n    else : \n        if normalized . startswith ( '..' ) : \n            raise PathOutsideRoot ( normalized ) \n    return normalized "}
{"5549": "\ndef validate ( self , data ) : \n    from dispatch . theme import ThemeManager \n    errors = { } \n    if data . get ( 'widget' ) is not None : \n        try : \n            widget = ThemeManager . Widgets . get ( data [ 'widget' ] ) \n        except WidgetNotFound as e : \n            errors [ 'widget' ] = str ( e ) \n        else : \n            for field in widget . fields : \n                field_data = data [ 'data' ] . get ( field . name ) \n                if field_data is not None : \n                    try : \n                        field . validate ( field_data ) \n                    except InvalidField as e : \n                        errors [ field . name ] = str ( e ) \n                else : \n                    if field . required : \n                        errors [ field . name ] = '%s is required' % field . label \n    if errors : \n        raise ValidationError ( errors ) \n    return data "}
{"5584": "\ndef consume ( self , amt , request_token ) : \n    with self . _lock : \n        time_now = self . _time_utils . time ( ) \n        if self . _consumption_scheduler . is_scheduled ( request_token ) : \n            return self . _release_requested_amt_for_scheduled_request ( amt , request_token , time_now ) \n        else : \n            if self . _projected_to_exceed_max_rate ( amt , time_now ) : \n                self . _raise_request_exceeded_exception ( amt , request_token , time_now ) \n            else : \n                return self . _release_requested_amt ( amt , time_now ) "}
{"5605": "\ndef seekable ( fileobj ) : \n    if hasattr ( fileobj , 'seekable' ) : \n        return fileobj . seekable ( ) \n    else : \n        if hasattr ( fileobj , 'seek' ) and hasattr ( fileobj , 'tell' ) : \n            try : \n                fileobj . seek ( 0 , 1 ) \n                return True \n            except ( OSError , IOError ) : \n                return False \n    return False "}
{"5647": "\ndef check_type ( o , acceptable_types , may_be_none = True ) : \n    if not isinstance ( acceptable_types , tuple ) : \n        acceptable_types = ( acceptable_types , ) \n    if may_be_none and o is None : \n        pass \n    else : \n        if isinstance ( o , acceptable_types ) : \n            pass \n        else : \n            error_message = ( \"We were expecting to receive an instance of one of the following \" \"types: {types}{none}; but instead we received {o} which is a \" \"{o_type}.\" . format ( types = \", \" . join ( [ repr ( t . __name__ ) for t in acceptable_types ] ) , none = \"or 'None'\" if may_be_none else \"\" , o = o , o_type = repr ( type ( o ) . __name__ ) ) ) \n            raise TypeError ( error_message ) "}
{"5649": "\ndef check_response_code ( response , expected_response_code ) : \n    if response . status_code == expected_response_code : \n        pass \n    else : \n        if response . status_code == RATE_LIMIT_RESPONSE_CODE : \n            raise RateLimitError ( response ) \n        else : \n            raise ApiError ( response ) "}
{"5650": "\ndef json_dict ( json_data ) : \n    if isinstance ( json_data , dict ) : \n        return json_data \n    else : \n        if isinstance ( json_data , basestring ) : \n            return json . loads ( json_data , object_hook = OrderedDict ) \n        else : \n            raise TypeError ( \"'json_data' must be a dictionary or valid JSON string; \" \"received: {!r}\" . format ( json_data ) ) "}
{"5674": "\ndef create ( self , roomId = None , toPersonId = None , toPersonEmail = None , text = None , markdown = None , files = None , ** request_parameters ) : \n    check_type ( roomId , basestring ) \n    check_type ( toPersonId , basestring ) \n    check_type ( toPersonEmail , basestring ) \n    check_type ( text , basestring ) \n    check_type ( markdown , basestring ) \n    check_type ( files , list ) \n    if files : \n        if len ( files ) != 1 : \n            raise ValueError ( \"The length of the `files` list is greater \" \"than one (1). The files parameter is a \" \"list, which accepts multiple values to \" \"allow for future expansion, but currently \" \"only one file may be included with the \" \"message.\" ) \n        check_type ( files [ 0 ] , basestring ) \n    post_data = dict_from_items_with_values ( request_parameters , roomId = roomId , toPersonId = toPersonId , toPersonEmail = toPersonEmail , text = text , markdown = markdown , files = files , ) \n    if not files or is_web_url ( files [ 0 ] ) : \n        json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    else : \n        if is_local_file ( files [ 0 ] ) : \n            try : \n                post_data [ 'files' ] = open_local_file ( files [ 0 ] ) \n                multipart_data = MultipartEncoder ( post_data ) \n                headers = { 'Content-type' : multipart_data . content_type } \n                json_data = self . _session . post ( API_ENDPOINT , headers = headers , data = multipart_data ) \n            finally : \n                post_data [ 'files' ] . file_object . close ( ) \n        else : \n            raise ValueError ( \"The `files` parameter does not contain a vaild \" \"URL or path to a local file.\" ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5687": "\ndef _serialize ( cls , data ) : \n    if hasattr ( data , \"__hash__\" ) and callable ( data . __hash__ ) : \n        return data \n    else : \n        if isinstance ( data , list ) : \n            return tuple ( ( cls . _serialize ( item ) for item in data ) ) \n        else : \n            if isinstance ( data , dict ) : \n                key_value_tuples = [ ( key , cls . _serialize ( value ) ) for key , value in data . items ( ) ] \n                key_value_tuples . sort ( ) \n                return tuple ( key_value_tuples ) \n            else : \n                raise TypeError ( \"Unable to freeze {} data type.\" . format ( type ( data ) ) ) "}
{"5698": "\ndef create_dsmr_protocol ( dsmr_version , telegram_callback , loop = None ) : \n    if dsmr_version == '2.2' : \n        specification = telegram_specifications . V2_2 \n        serial_settings = SERIAL_SETTINGS_V2_2 \n    else : \n        if dsmr_version == '4' : \n            specification = telegram_specifications . V4 \n            serial_settings = SERIAL_SETTINGS_V4 \n        else : \n            if dsmr_version == '5' : \n                specification = telegram_specifications . V5 \n                serial_settings = SERIAL_SETTINGS_V5 \n            else : \n                raise NotImplementedError ( \"No telegram parser found for version: %s\" , dsmr_version ) \n    protocol = partial ( DSMRProtocol , loop , TelegramParser ( specification ) , telegram_callback = telegram_callback ) \n    return protocol , serial_settings "}
{"5707": "\ndef find_packages ( top = HERE ) : \n    packages = [ ] \n    for d , dirs , _ in os . walk ( top , followlinks = True ) : \n        if os . path . exists ( pjoin ( d , '__init__.py' ) ) : \n            packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) \n        else : \n            if d != top : \n                dirs [ : ] = [ ] \n    return packages "}
{"5720": "\ndef _translate_glob_part ( pat ) : \n    if pat == '**' : \n        return '.*' \n    i , n = 0 , len ( pat ) \n    res = [ ] \n    while i < n : \n        c = pat [ i ] \n        i = i + 1 \n        if c == '*' : \n            res . append ( '[^%s]*' % SEPARATORS ) \n        else : \n            if c == '?' : \n                res . append ( '[^%s]?' % SEPARATORS ) \n            else : \n                if c == '[' : \n                    j = i \n                    if j < n and pat [ j ] == '!' : \n                        j = j + 1 \n                    if j < n and pat [ j ] == ']' : \n                        j = j + 1 \n                    while j < n and pat [ j ] != ']' : \n                        j = j + 1 \n                    if j >= n : \n                        res . append ( '\\\\[' ) \n                    else : \n                        stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) \n                        i = j + 1 \n                        if stuff [ 0 ] == '!' : \n                            stuff = '^' + stuff [ 1 : ] \n                        else : \n                            if stuff [ 0 ] == '^' : \n                                stuff = '\\\\' + stuff \n                        res . append ( '[%s]' % stuff ) \n                else : \n                    res . append ( re . escape ( c ) ) \n    return '' . join ( res ) "}
{"5737": "\ndef get ( self , query , * parameters , ** kwparameters ) : \n    rows = self . _query ( query , parameters , kwparameters ) \n    if not rows : \n        return None \n    else : \n        if not isinstance ( rows , list ) : \n            raise MySQLError ( \"Query is not a select query\" ) \n        else : \n            if len ( rows ) > 1 : \n                raise MySQLError ( \"Multiple rows returned for Database.get() query\" ) \n            else : \n                return rows [ 0 ] "}
{"5750": "\ndef stop_step ( self , step_name ) : \n    if self . finished is not None : \n        raise AlreadyFinished ( ) \n    steps = copy . deepcopy ( self . steps ) \n    step_data = self . _get_step ( step_name , steps = steps ) \n    if step_data is None : \n        raise StepNotStarted ( ) \n    else : \n        if 'stop' in step_data : \n            raise StepAlreadyFinished ( ) \n    step_data [ 'stop' ] = datetime . utcnow ( ) \n    step_data [ 'duration' ] = util . timedelta_total_seconds ( step_data [ 'stop' ] - step_data [ 'start' ] ) \n    self . _save ( steps = steps ) "}
{"5759": "\ndef send ( self , api_key = None , secret = None , list_data = None , auth = False , ** kwargs ) : \n    if auth : \n        nonce = str ( int ( time . time ( ) * 10000000 ) ) \n        auth_string = 'AUTH' + nonce \n        auth_sig = hmac . new ( secret . encode ( ) , auth_string . encode ( ) , hashlib . sha384 ) . hexdigest ( ) \n        payload = { 'event' : 'auth' , 'apiKey' : api_key , 'authSig' : auth_sig , 'authPayload' : auth_string , 'authNonce' : nonce } \n        payload = json . dumps ( payload ) \n    else : \n        if list_data : \n            payload = json . dumps ( list_data ) \n        else : \n            payload = json . dumps ( kwargs ) \n    self . log . debug ( \"send(): Sending payload to API: %s\" , payload ) \n    try : \n        self . socket . send ( payload ) \n    except websocket . WebSocketConnectionClosedException : \n        self . log . error ( \"send(): Did not send out payload %s - client not connected. \" , kwargs ) "}
{"5761": "\ndef _system_handler ( self , data , ts ) : \n    self . log . debug ( \"_system_handler(): Received a system message: %s\" , data ) \n    event = data . pop ( 'event' ) \n    if event == 'pong' : \n        self . log . debug ( \"_system_handler(): Distributing %s to _pong_handler..\" , data ) \n        self . _pong_handler ( ) \n    else : \n        if event == 'info' : \n            self . log . debug ( \"_system_handler(): Distributing %s to _info_handler..\" , data ) \n            self . _info_handler ( data ) \n        else : \n            if event == 'error' : \n                self . log . debug ( \"_system_handler(): Distributing %s to _error_handler..\" , data ) \n                self . _error_handler ( data ) \n            else : \n                if event in ( 'subscribed' , 'unsubscribed' , 'conf' , 'auth' , 'unauth' ) : \n                    self . log . debug ( \"_system_handler(): Distributing %s to \" \"_response_handler..\" , data ) \n                    self . _response_handler ( event , data , ts ) \n                else : \n                    self . log . error ( \"Unhandled event: %s, data: %s\" , event , data ) "}
{"5795": "\ndef _onConnect ( self , mqttc , userdata , flags , rc ) : \n    if rc == 0 : \n        self . connectEvent . set ( ) \n        self . logger . info ( \"Connected successfully: %s\" % ( self . clientId ) ) \n        with self . _subLock : \n            if len ( self . _subscriptions ) > 0 : \n                for subscription in self . _subscriptions : \n                    ( result , mid ) = self . client . subscribe ( subscription , qos = self . _subscriptions [ subscription ] ) \n                    if result != paho . MQTT_ERR_SUCCESS : \n                        self . _logAndRaiseException ( ConnectionException ( \"Unable to subscribe to %s\" % subscription ) ) \n                self . logger . debug ( \"Restored %s previous subscriptions\" % len ( self . _subscriptions ) ) \n    else : \n        if rc == 1 : \n            self . _logAndRaiseException ( ConnectionException ( \"Incorrect protocol version\" ) ) \n        else : \n            if rc == 2 : \n                self . _logAndRaiseException ( ConnectionException ( \"Invalid client identifier\" ) ) \n            else : \n                if rc == 3 : \n                    self . _logAndRaiseException ( ConnectionException ( \"Server unavailable\" ) ) \n                else : \n                    if rc == 4 : \n                        self . _logAndRaiseException ( ConnectionException ( \"Bad username or password: (%s, %s)\" % ( self . username , self . password ) ) ) \n                    else : \n                        if rc == 5 : \n                            self . _logAndRaiseException ( ConnectionException ( \"Not authorized: s (%s, %s, %s)\" % ( self . clientId , self . username , self . password ) ) ) \n                        else : \n                            self . _logAndRaiseException ( ConnectionException ( \"Unexpected connection failure: %s\" % ( rc ) ) ) "}
{"5822": "\ndef crop_on_centerpoint ( self , image , width , height , ppoi = ( 0.5 , 0.5 ) ) : \n    ppoi_x_axis = int ( image . size [ 0 ] * ppoi [ 0 ] ) \n    ppoi_y_axis = int ( image . size [ 1 ] * ppoi [ 1 ] ) \n    center_pixel_coord = ( ppoi_x_axis , ppoi_y_axis ) \n    orig_aspect_ratio = float ( image . size [ 0 ] ) / float ( image . size [ 1 ] ) \n    crop_aspect_ratio = float ( width ) / float ( height ) \n    if orig_aspect_ratio >= crop_aspect_ratio : \n        orig_crop_width = int ( ( crop_aspect_ratio * float ( image . size [ 1 ] ) ) + 0.5 ) \n        orig_crop_height = image . size [ 1 ] \n        crop_boundary_top = 0 \n        crop_boundary_bottom = orig_crop_height \n        crop_boundary_left = center_pixel_coord [ 0 ] - ( orig_crop_width // 2 ) \n        crop_boundary_right = crop_boundary_left + orig_crop_width \n        if crop_boundary_left < 0 : \n            crop_boundary_left = 0 \n            crop_boundary_right = crop_boundary_left + orig_crop_width \n        else : \n            if crop_boundary_right > image . size [ 0 ] : \n                crop_boundary_right = image . size [ 0 ] \n                crop_boundary_left = image . size [ 0 ] - orig_crop_width \n    else : \n        orig_crop_width = image . size [ 0 ] \n        orig_crop_height = int ( ( float ( image . size [ 0 ] ) / crop_aspect_ratio ) + 0.5 ) \n        crop_boundary_left = 0 \n        crop_boundary_right = orig_crop_width \n        crop_boundary_top = center_pixel_coord [ 1 ] - ( orig_crop_height // 2 ) \n        crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        if crop_boundary_top < 0 : \n            crop_boundary_top = 0 \n            crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        else : \n            if crop_boundary_bottom > image . size [ 1 ] : \n                crop_boundary_bottom = image . size [ 1 ] \n                crop_boundary_top = image . size [ 1 ] - orig_crop_height \n    cropped_image = image . crop ( ( crop_boundary_left , crop_boundary_top , crop_boundary_right , crop_boundary_bottom ) ) \n    return cropped_image . resize ( ( width , height ) , Image . ANTIALIAS ) "}
{"5830": "\ndef save_form_data ( self , instance , data ) : \n    to_assign = data \n    if data and isinstance ( data , tuple ) : \n        if data [ 0 ] is None : \n            current_field = getattr ( instance , self . name ) \n            if data [ 1 ] : \n                current_field . ppoi = data [ 1 ] \n            to_assign = current_field \n        else : \n            if data [ 0 ] is False : \n                to_assign = '' \n            else : \n                to_assign = data [ 0 ] \n    super ( VersatileImageField , self ) . save_form_data ( instance , to_assign ) "}
{"5842": "\ndef preprocess ( self , image , image_format ) : \n    save_kwargs = { 'format' : image_format } \n    if hasattr ( image , '_getexif' ) : \n        exif_datadict = image . _getexif ( ) \n        if exif_datadict is not None : \n            exif = dict ( exif_datadict . items ( ) ) \n            orientation = exif . get ( EXIF_ORIENTATION_KEY , None ) \n            if orientation == 3 : \n                image = image . transpose ( Image . ROTATE_180 ) \n            else : \n                if orientation == 6 : \n                    image = image . transpose ( Image . ROTATE_270 ) \n                else : \n                    if orientation == 8 : \n                        image = image . transpose ( Image . ROTATE_90 ) \n    save_kwargs [ 'icc_profile' ] = image . info . get ( 'icc_profile' ) \n    if hasattr ( self , 'preprocess_%s' % image_format ) : \n        image , addl_save_kwargs = getattr ( self , 'preprocess_%s' % image_format ) ( image = image ) \n        save_kwargs . update ( addl_save_kwargs ) \n    return image , save_kwargs "}
{"5867": "\ndef agi_code_check ( code = None , response = None , line = None ) : \n    code = int ( code ) \n    response = response or \"\" \n    result = { 'status_code' : code , 'result' : ( '' , '' ) , 'msg' : '' } \n    if code == 100 : \n        result [ 'msg' ] = line \n    else : \n        if code == 200 : \n            for key , value , data in re_kv . findall ( response ) : \n                result [ key ] = ( value , data ) \n                if data == 'hangup' : \n                    return { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n                else : \n                    if key == 'result' and value == '-1' : \n                        return { 'error' : 'AGIAppError' , 'msg' : 'Error executing application, or hangup' } \n        else : \n            if code == 510 : \n                result [ 'error' ] = 'AGIInvalidCommand' \n            else : \n                if code == 520 : \n                    result [ 'error' ] = 'AGIUsageError' \n                    result [ 'msg' ] = line \n                else : \n                    result [ 'error' ] = 'AGIUnknownError' \n                    result [ 'msg' ] = line \n    return result "}
{"5893": "\ndef parse_interfaces ( interfaces ) : \n    parsed_interfaces = collections . defaultdict ( dict ) \n    for m , d in iteritems ( interfaces ) : \n        app , func = m . split ( '.' , 1 ) \n        method = parsed_interfaces [ app ] [ func ] = { } \n        method [ 'formats' ] = [ 'json' , 'human' ] \n        method [ 'method' ] = 'POST' \n        method [ 'optional' ] = { } \n        method [ 'required' ] = { } \n        for name , type_info in iteritems ( dict ( d [ 'params' ] ) ) : \n            optionality = 'required' \n            param_type = 'string' \n            type_info = TYPE_INFO_COMMENT_RE . sub ( '' , type_info ) \n            info_pieces = TYPE_INFO_SPLITTER_RE . findall ( type_info ) \n            for info_piece in info_pieces : \n                if info_piece in ( 'optional' , 'required' ) : \n                    optionality = info_piece \n                else : \n                    if info_piece == 'ignored' : \n                        optionality = 'optional' \n                        param_type = 'string' \n                    else : \n                        if info_piece == 'nonempty' : \n                            optionality = 'required' \n                        else : \n                            if info_piece == 'deprecated' : \n                                optionality = 'optional' \n                            else : \n                                param_type = info_piece \n            method [ optionality ] [ name ] = map_param_type ( param_type ) \n    return dict ( parsed_interfaces ) "}
{"5903": "\ndef get_contacts ( address_books , query , method = \"all\" , reverse = False , group = False , sort = \"first_name\" ) : \n    contacts = [ ] \n    for address_book in address_books : \n        contacts . extend ( address_book . search ( query , method = method ) ) \n    if group : \n        if sort == \"first_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) ) \n        else : \n            if sort == \"last_name\" : \n                return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) ) \n            else : \n                raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) \n    else : \n        if sort == \"first_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) \n        else : \n            if sort == \"last_name\" : \n                return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) \n            else : \n                raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) "}
{"5905": "\ndef load_address_books ( names , config , search_queries ) : \n    all_names = { str ( book ) for book in config . abooks } \n    if not names : \n        names = all_names \n    else : \n        if not all_names . issuperset ( names ) : \n            sys . exit ( 'Error: The entered address books \"{}\" do not exist.\\n' 'Possible values are: {}' . format ( '\", \"' . join ( set ( names ) - all_names ) , ', ' . join ( all_names ) ) ) \n    for name in names : \n        address_book = config . abook . get_abook ( name ) \n        address_book . load ( search_queries [ address_book . name ] , search_in_source_files = config . search_in_source_files ( ) ) \n        yield address_book "}
{"5909": "\ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ 0 ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                else : \n                    if len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) >= 3 : \n                        if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                            matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            print ( '\\n' . join ( matching_phone_number_list ) ) \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    else : \n        if all_phone_numbers_list : \n            if parsable : \n                print ( '\\n' . join ( all_phone_numbers_list ) ) \n            else : \n                list_phone_numbers ( all_phone_numbers_list ) \n        else : \n            if not parsable : \n                print ( \"Found no phone numbers\" ) \n            sys . exit ( 1 ) "}
{"5910": "\ndef list_subcommand ( vcard_list , parsable ) : \n    if not vcard_list : \n        if not parsable : \n            print ( \"Found no contacts\" ) \n        sys . exit ( 1 ) \n    else : \n        if parsable : \n            contact_line_list = [ ] \n            for vcard in vcard_list : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                contact_line_list . append ( '\\t' . join ( [ vcard . get_uid ( ) , name , vcard . address_book . name ] ) ) \n            print ( '\\n' . join ( contact_line_list ) ) \n        else : \n            list_contacts ( vcard_list ) "}
{"5917": "\ndef _convert_boolean_config_value ( config , name , default = True ) : \n    if name not in config : \n        config [ name ] = default \n    else : \n        if config [ name ] == \"yes\" : \n            config [ name ] = True \n        else : \n            if config [ name ] == \"no\" : \n                config [ name ] = False \n            else : \n                raise ValueError ( \"Error in config file\\nInvalid value for %s \" \"parameter\\nPossible values: yes, no\" % name ) "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = 0 \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            else : \n                if type . lower ( ) == \"pref\" : \n                    pref += 1 \n                else : \n                    if re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                        pref += int ( type . split ( \"=\" ) [ 1 ] ) \n                    else : \n                        if type . lower ( ) . startswith ( \"x-\" ) : \n                            custom_types . append ( type [ 2 : ] ) \n                            standard_types . append ( type ) \n                        else : \n                            custom_types . append ( type ) \n                            standard_types . append ( \"X-{}\" . format ( type ) ) \n    return ( standard_types , custom_types , pref ) "}
{"5931": "\ndef search ( self , query , method = \"all\" ) : \n    logging . debug ( 'address book %s, searching with %s' , self . name , query ) \n    if not self . _loaded : \n        self . load ( query ) \n    if method == \"all\" : \n        search_function = self . _search_all \n    else : \n        if method == \"name\" : \n            search_function = self . _search_names \n        else : \n            if method == \"uid\" : \n                search_function = self . _search_uid \n            else : \n                raise ValueError ( 'Only the search methods \"all\", \"name\" and \"uid\" ' 'are supported.' ) \n    return list ( search_function ( query ) ) "}
{"5932": "\ndef get_short_uid_dict ( self , query = None ) : \n    if self . _short_uids is None : \n        if not self . _loaded : \n            self . load ( query ) \n        if not self . contacts : \n            self . _short_uids = { } \n        else : \n            if len ( self . contacts ) == 1 : \n                self . _short_uids = { uid [ 0 : 1 ] : contact for uid , contact in self . contacts . items ( ) } \n            else : \n                self . _short_uids = { } \n                sorted_uids = sorted ( self . contacts ) \n                item0 , item1 = sorted_uids [ : 2 ] \n                same1 = self . _compare_uids ( item0 , item1 ) \n                self . _short_uids [ item0 [ : same1 + 1 ] ] = self . contacts [ item0 ] \n                for item_new in sorted_uids [ 2 : ] : \n                    item0 , item1 = item1 , item_new \n                    same0 , same1 = same1 , self . _compare_uids ( item0 , item1 ) \n                    same = max ( same0 , same1 ) \n                    self . _short_uids [ item0 [ : same + 1 ] ] = self . contacts [ item0 ] \n                self . _short_uids [ item1 [ : same1 + 1 ] ] = self . contacts [ item1 ] \n    return self . _short_uids "}
{"5935": "\ndef load ( self , query = None , search_in_source_files = False ) : \n    if self . _loaded : \n        return \n    logging . debug ( 'Loading Vdir %s with query %s' , self . name , query ) \n    errors = 0 \n    for filename in self . _find_vcard_files ( search = query , search_in_source_files = search_in_source_files ) : \n        try : \n            card = CarddavObject . from_file ( self , filename , self . _private_objects , self . _localize_dates ) \n        except ( IOError , vobject . base . ParseError ) as err : \n            verb = \"open\" if isinstance ( err , IOError ) else \"parse\" \n            logging . debug ( \"Error: Could not %s file %s\\n%s\" , verb , filename , err ) \n            if self . _skip : \n                errors += 1 \n            else : \n                logging . error ( \"The vcard file %s of address book %s could not be \" \"parsed\\nUse --debug for more information or \" \"--skip-unparsable to proceed\" , filename , self . name ) \n                sys . exit ( 2 ) \n        else : \n            uid = card . get_uid ( ) \n            if not uid : \n                logging . warning ( \"Card %s from address book %s has no UID \" \"and will not be availbale.\" , card , self . name ) \n            else : \n                if uid in self . contacts : \n                    logging . warning ( \"Card %s and %s from address book %s have the same \" \"UID. The former will not be availbale.\" , card , self . contacts [ uid ] , self . name ) \n                else : \n                    self . contacts [ uid ] = card \n    self . _loaded = True \n    if errors : \n        logging . warning ( \"%d of %d vCard files of address book %s could not be parsed.\" , errors , len ( self . contacts ) + errors , self ) \n    logging . debug ( 'Loded %s contacts from address book %s.' , len ( self . contacts ) , self . name ) "}
{"5943": "\ndef _guess ( kwargs ) : \n    guessed = { } \n    TYPE_AWARE_ACTIONS = 'store' , 'append' \n    value = kwargs . get ( 'default' ) \n    if value is not None : \n        if isinstance ( value , bool ) : \n            if kwargs . get ( 'action' ) is None : \n                guessed [ 'action' ] = 'store_false' if value else 'store_true' \n        else : \n            if kwargs . get ( 'type' ) is None : \n                if kwargs . get ( 'action' , 'store' ) in TYPE_AWARE_ACTIONS : \n                    guessed [ 'type' ] = type ( value ) \n    if kwargs . get ( 'choices' ) and 'type' not in list ( guessed ) + list ( kwargs ) : \n        guessed [ 'type' ] = type ( kwargs [ 'choices' ] [ 0 ] ) \n    return dict ( kwargs , ** guessed ) "}
{"5953": "\ndef _on_delete ( ent ) : \n    seen_d = set ( [ ent . _pk ] ) \n    to_delete = [ ent ] \n    seen_s = set ( ) \n    to_save = [ ] \n    def _set_default ( ent , attr , de = NULL ) : \n        pk = ent . _pk \n        if pk in seen_d : \n            return \n        col = ent . __class__ . _columns [ attr ] \n        de = de if de is not NULL else col . _default \n        if de in ( None , NULL ) : \n            setattr ( ent , attr , None ) \n        else : \n            if callable ( col . _default ) : \n                setattr ( ent , attr , col . _default ( ) ) \n            else : \n                setattr ( ent , attr , col . _default ) \n        if pk not in seen_s : \n            seen_s . add ( pk ) \n            to_save . append ( ent ) \n    for self in to_delete : \n        for tbl , attr , action in MODELS_REFERENCED . get ( self . _namespace , ( ) ) : \n            if action == 'no action' : \n                continue \n            refs = MODELS [ tbl ] . get_by ( ** { attr : self . id } ) \n            if not refs : \n                continue \n            if action == 'restrict' : \n                raise _restrict ( self , attr , refs ) \n            else : \n                if action == 'set null' : \n                    for ref in refs : \n                        _set_default ( ref , attr , None ) \n                    continue \n                else : \n                    if action == 'set default' : \n                        for ref in refs : \n                            _set_default ( ref , attr ) \n                        continue \n            for ent in ( refs if isinstance ( refs , list ) else [ refs ] ) : \n                if ent . _pk not in seen_d : \n                    seen_d . add ( ent . _pk ) \n                    to_delete . append ( ent ) \n    for self in to_delete : \n        self . delete ( skip_on_delete_i_really_mean_it = SKIP_ON_DELETE ) \n    for self in to_save : \n        if self . _pk not in seen_d : \n            self . save ( ) "}
{"5955": "\ndef estimate_work_lua ( conn , index , prefix ) : \n    if index . endswith ( ':idx' ) : \n        args = [ ] if not prefix else list ( prefix ) \n        if args : \n            args [ 0 ] = '-inf' if args [ 0 ] is None else repr ( float ( args [ 0 ] ) ) \n            args [ 1 ] = 'inf' if args [ 1 ] is None else repr ( float ( args [ 1 ] ) ) \n        return _estimate_work_lua ( conn , [ index ] , args , force_eval = True ) \n    else : \n        if index . endswith ( ':geo' ) : \n            return _estimate_work_lua ( conn , [ index ] , filter ( None , [ prefix ] ) , force_eval = True ) \n    start , end = _start_end ( prefix ) \n    return _estimate_work_lua ( conn , [ index ] , [ start , '(' + end ] , force_eval = True ) "}
{"5959": "\ndef FULL_TEXT ( val ) : \n    if isinstance ( val , float ) : \n        val = repr ( val ) \n    else : \n        if val in ( None , '' ) : \n            return None \n        else : \n            if not isinstance ( val , six . string_types ) : \n                if six . PY3 and isinstance ( val , bytes ) : \n                    val = val . decode ( 'latin-1' ) \n                else : \n                    val = str ( val ) \n    r = sorted ( set ( [ x for x in [ s . lower ( ) . strip ( string . punctuation ) for s in val . split ( ) ] if x ] ) ) \n    if not isinstance ( val , str ) : \n        return [ s . encode ( 'utf-8' ) for s in r ] \n    return r "}
{"5969": "\ndef _sem_open ( name , value = None ) : \n    if value is None : \n        handle = pthread . sem_open ( ctypes . c_char_p ( name ) , 0 ) \n    else : \n        handle = pthread . sem_open ( ctypes . c_char_p ( name ) , SEM_OFLAG , SEM_PERM , ctypes . c_int ( value ) ) \n    if handle == SEM_FAILURE : \n        e = ctypes . get_errno ( ) \n        if e == errno . EEXIST : \n            raise FileExistsError ( \"a semaphore named %s already exists\" % name ) \n        else : \n            if e == errno . ENOENT : \n                raise FileNotFoundError ( 'cannot find semaphore named %s' % name ) \n            else : \n                if e == errno . ENOSYS : \n                    raise NotImplementedError ( 'No semaphore implementation on this ' 'system' ) \n                else : \n                    raiseFromErrno ( ) \n    return handle "}
{"5977": "\ndef DupFd ( fd ) : \n    popen_obj = get_spawning_popen ( ) \n    if popen_obj is not None : \n        return popen_obj . DupFd ( popen_obj . duplicate_for_child ( fd ) ) \n    else : \n        if HAVE_SEND_HANDLE and sys . version_info [ : 2 ] > ( 3 , 3 ) : \n            from multiprocessing import resource_sharer \n            return resource_sharer . DupFd ( fd ) \n        else : \n            raise TypeError ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' ) "}
{"5978": "\ndef get_reusable_executor ( max_workers = None , context = None , timeout = 10 , kill_workers = False , reuse = \"auto\" , job_reducers = None , result_reducers = None , initializer = None , initargs = ( ) ) : \n    with _executor_lock : \n        global _executor , _executor_kwargs \n        executor = _executor \n        if max_workers is None : \n            if reuse is True and executor is not None : \n                max_workers = executor . _max_workers \n            else : \n                max_workers = cpu_count ( ) \n        else : \n            if max_workers <= 0 : \n                raise ValueError ( \"max_workers must be greater than 0, got {}.\" . format ( max_workers ) ) \n        if isinstance ( context , STRING_TYPE ) : \n            context = get_context ( context ) \n        if context is not None and context . get_start_method ( ) == \"fork\" : \n            raise ValueError ( \"Cannot use reusable executor with the 'fork' \" \"context\" ) \n        kwargs = dict ( context = context , timeout = timeout , job_reducers = job_reducers , result_reducers = result_reducers , initializer = initializer , initargs = initargs ) \n        if executor is None : \n            mp . util . debug ( \"Create a executor with max_workers={}.\" . format ( max_workers ) ) \n            executor_id = _get_next_executor_id ( ) \n            _executor_kwargs = kwargs \n            _executor = executor = _ReusablePoolExecutor ( _executor_lock , max_workers = max_workers , executor_id = executor_id , ** kwargs ) \n        else : \n            if reuse == 'auto' : \n                reuse = kwargs == _executor_kwargs \n            if ( executor . _flags . broken or executor . _flags . shutdown or not reuse ) : \n                if executor . _flags . broken : \n                    reason = \"broken\" \n                else : \n                    if executor . _flags . shutdown : \n                        reason = \"shutdown\" \n                    else : \n                        reason = \"arguments have changed\" \n                mp . util . debug ( \"Creating a new executor with max_workers={} as the \" \"previous instance cannot be reused ({}).\" . format ( max_workers , reason ) ) \n                executor . shutdown ( wait = True , kill_workers = kill_workers ) \n                _executor = executor = _executor_kwargs = None \n                return get_reusable_executor ( max_workers = max_workers , ** kwargs ) \n            else : \n                mp . util . debug ( \"Reusing existing executor with max_workers={}.\" . format ( executor . _max_workers ) ) \n                executor . _resize ( max_workers ) \n    return executor "}
{"5980": "\ndef get_preparation_data ( name , init_main_module = True ) : \n    _check_not_importing_main ( ) \n    d = dict ( log_to_stderr = util . _log_to_stderr , authkey = bytes ( process . current_process ( ) . authkey ) , ) \n    if util . _logger is not None : \n        d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) \n        if len ( util . _logger . handlers ) > 0 : \n            h = util . _logger . handlers [ 0 ] \n            d [ 'log_fmt' ] = h . formatter . _fmt \n    sys_path = [ p for p in sys . path ] \n    try : \n        i = sys_path . index ( '' ) \n    except ValueError : \n        pass \n    else : \n        sys_path [ i ] = process . ORIGINAL_DIR \n    d . update ( name = name , sys_path = sys_path , sys_argv = sys . argv , orig_dir = process . ORIGINAL_DIR , dir = os . getcwd ( ) ) \n    if sys . platform != \"win32\" : \n        from . import semaphore_tracker \n        semaphore_tracker . ensure_running ( ) \n        d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid \n    if init_main_module : \n        main_module = sys . modules [ '__main__' ] \n        try : \n            main_mod_name = getattr ( main_module . __spec__ , \"name\" , None ) \n        except BaseException : \n            main_mod_name = None \n        if main_mod_name is not None : \n            d [ 'init_main_from_name' ] = main_mod_name \n        else : \n            if sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : \n                main_path = getattr ( main_module , '__file__' , None ) \n                if main_path is not None : \n                    if ( not os . path . isabs ( main_path ) and process . ORIGINAL_DIR is not None ) : \n                        main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) \n                    d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) \n                    d [ 'main_path' ] = d [ 'init_main_from_path' ] \n    return d "}
{"5981": "\ndef prepare ( data ) : \n    if 'name' in data : \n        process . current_process ( ) . name = data [ 'name' ] \n    if 'authkey' in data : \n        process . current_process ( ) . authkey = data [ 'authkey' ] \n    if 'log_to_stderr' in data and data [ 'log_to_stderr' ] : \n        util . log_to_stderr ( ) \n    if 'log_level' in data : \n        util . get_logger ( ) . setLevel ( data [ 'log_level' ] ) \n    if 'log_fmt' in data : \n        import logging \n        util . get_logger ( ) . handlers [ 0 ] . setFormatter ( logging . Formatter ( data [ 'log_fmt' ] ) ) \n    if 'sys_path' in data : \n        sys . path = data [ 'sys_path' ] \n    if 'sys_argv' in data : \n        sys . argv = data [ 'sys_argv' ] \n    if 'dir' in data : \n        os . chdir ( data [ 'dir' ] ) \n    if 'orig_dir' in data : \n        process . ORIGINAL_DIR = data [ 'orig_dir' ] \n    if 'tracker_pid' in data : \n        from . import semaphore_tracker \n        semaphore_tracker . _semaphore_tracker . _pid = data [ \"tracker_pid\" ] \n    if 'init_main_from_name' in data : \n        _fixup_main_from_name ( data [ 'init_main_from_name' ] ) \n    else : \n        if 'init_main_from_path' in data : \n            _fixup_main_from_path ( data [ 'init_main_from_path' ] ) "}
{"5984": "\ndef _recursive_terminate ( pid ) : \n    if sys . platform == \"win32\" : \n        try : \n            subprocess . check_output ( [ \"taskkill\" , \"/F\" , \"/T\" , \"/PID\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode not in [ 1 , 128 , 255 ] : \n                raise \n            else : \n                if e . returncode == 1 : \n                    try : \n                        os . kill ( pid , signal . SIGTERM ) \n                    except OSError as e : \n                        if e . errno != errno . ESRCH : \n                            raise \n    else : \n        try : \n            children_pids = subprocess . check_output ( [ \"pgrep\" , \"-P\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode == 1 : \n                children_pids = b'' \n            else : \n                raise \n        children_pids = children_pids . decode ( ) . split ( '\\n' ) [ : - 1 ] \n        for cpid in children_pids : \n            cpid = int ( cpid ) \n            _recursive_terminate ( cpid ) \n        try : \n            os . kill ( pid , signal . SIGTERM ) \n        except OSError as e : \n            if e . errno != errno . ESRCH : \n                raise "}
{"5987": "\ndef main ( fd , verbose = 0 ) : \n    signal . signal ( signal . SIGINT , signal . SIG_IGN ) \n    signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n    if _HAVE_SIGMASK : \n        signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n    for f in ( sys . stdin , sys . stdout ) : \n        try : \n            f . close ( ) \n        except Exception : \n            pass \n    if verbose : \n        sys . stderr . write ( \"Main semaphore tracker is running\\n\" ) \n        sys . stderr . flush ( ) \n    cache = set ( ) \n    try : \n        with os . fdopen ( fd , 'rb' ) as f : \n            for line in f : \n                try : \n                    cmd , name = line . strip ( ) . split ( b':' ) \n                    if cmd == b'REGISTER' : \n                        name = name . decode ( 'ascii' ) \n                        cache . add ( name ) \n                        if verbose : \n                            sys . stderr . write ( \"[SemaphoreTracker] register {}\\n\" . format ( name ) ) \n                            sys . stderr . flush ( ) \n                    else : \n                        if cmd == b'UNREGISTER' : \n                            name = name . decode ( 'ascii' ) \n                            cache . remove ( name ) \n                            if verbose : \n                                sys . stderr . write ( \"[SemaphoreTracker] unregister {}\" \": cache({})\\n\" . format ( name , len ( cache ) ) ) \n                                sys . stderr . flush ( ) \n                        else : \n                            if cmd == b'PROBE' : \n                                pass \n                            else : \n                                raise RuntimeError ( 'unrecognized command %r' % cmd ) \n                except BaseException : \n                    try : \n                        sys . excepthook ( * sys . exc_info ( ) ) \n                    except BaseException : \n                        pass \n    finally : \n        if cache : \n            try : \n                warnings . warn ( 'semaphore_tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) \n            except Exception : \n                pass \n        for name in cache : \n            try : \n                try : \n                    sem_unlink ( name ) \n                    if verbose : \n                        sys . stderr . write ( \"[SemaphoreTracker] unlink {}\\n\" . format ( name ) ) \n                        sys . stderr . flush ( ) \n                except Exception as e : \n                    warnings . warn ( 'semaphore_tracker: %s: %r' % ( name , e ) ) \n            finally : \n                pass \n    if verbose : \n        sys . stderr . write ( \"semaphore tracker shut down\\n\" ) \n        sys . stderr . flush ( ) "}
{"5997": "\ndef get_int ( self , arg , min_value = 0 , default = 1 , cmdname = None , at_most = None ) : \n    if arg is None : \n        return default \n    default = self . get_int_noerr ( arg ) \n    if default is None : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer; \" + \"got: %s.\" ) % ( cmdname , str ( arg ) ) ) \n        else : \n            self . errmsg ( 'Expecting a positive integer, got: %s' % str ( arg ) ) \n            pass \n        return None \n        pass \n    if default < min_value : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at least\" + ' %d; got: %d.' ) % ( cmdname , min_value , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting a positive integer at least\" + ' %d; got: %d' ) % ( min_value , default ) ) \n            pass \n        return None \n    else : \n        if at_most and default > at_most : \n            if cmdname : \n                self . errmsg ( ( \"Command '%s' expects an integer at most\" + ' %d; got: %d.' ) % ( cmdname , at_most , default ) ) \n            else : \n                self . errmsg ( ( \"Expecting an integer at most %d; got: %d\" ) % ( at_most , default ) ) \n                pass \n            pass \n    return default "}
{"5999": "\ndef queue_startfile ( self , cmdfile ) : \n    expanded_cmdfile = os . path . expanduser ( cmdfile ) \n    is_readable = Mfile . readable ( expanded_cmdfile ) \n    if is_readable : \n        self . cmd_queue . append ( 'source ' + expanded_cmdfile ) \n    else : \n        if is_readable is None : \n            self . errmsg ( \"source file '%s' doesn't exist\" % expanded_cmdfile ) \n        else : \n            self . errmsg ( \"source file '%s' is not readable\" % expanded_cmdfile ) \n            pass \n    return "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - 1 , cur_line = 0 , start_line = - 1 , end_line = None , relative_pos = False , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    statement_count = 10000 \n    if end_line is None : \n        end_line = 10000 \n    else : \n        if relative_pos : \n            end_line += start_line - 1 \n            pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if start_line > cur_line : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and offset > end_offset : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( start_line > cur_line ) or start_offset and start_offset > offset ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count -= 1 \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( cur_line > end_line ) or ( end_offset and offset > end_offset ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and start_offset <= offset : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if offset == lasti : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6037": "\ndef run ( self , args ) : \n    if len ( args ) == 1 : \n        position_str = '0' \n    else : \n        if len ( args ) == 2 : \n            name_or_id = args [ 1 ] \n            frame , thread_id = self . get_from_thread_name_or_id ( name_or_id , False ) \n            if frame is None : \n                position_str = name_or_id \n            else : \n                position_str = '0' \n                self . find_and_set_debugged_frame ( frame , thread_id ) \n                pass \n        else : \n            if len ( args ) == 3 : \n                name_or_id = args [ 1 ] \n                position_str = args [ 2 ] \n                frame , thread_id = self . get_from_thread_name_or_id ( name_or_id ) \n                if frame is None : \n                    return \n                self . find_and_set_debugged_frame ( frame , thread_id ) \n                pass \n    self . one_arg_run ( position_str ) \n    return False "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return True \n    args = arg . split ( ) \n    signame = args [ 0 ] \n    signame = self . is_name_or_number ( args [ 0 ] ) \n    if not signame : \n        return \n    if len ( args ) == 1 : \n        self . info_signal ( [ signame ] ) \n        return True \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ 1 : ] : \n        if attr . startswith ( 'no' ) : \n            on = False \n            attr = attr [ 2 : ] \n        else : \n            on = True \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        else : \n            if 'print' . startswith ( attr ) and len ( attr ) >= 2 : \n                self . handle_print ( signame , on ) \n            else : \n                if 'pass' . startswith ( attr ) : \n                    self . handle_pass ( signame , on ) \n                else : \n                    if 'ignore' . startswith ( attr ) : \n                        self . handle_ignore ( signame , on ) \n                    else : \n                        if 'stack' . startswith ( attr ) : \n                            self . handle_print_stack ( signame , on ) \n                        else : \n                            self . dbgr . intf [ - 1 ] . errmsg ( 'Invalid arguments' ) \n                            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6049": "\ndef search_file ( filename , directories , cdir ) : \n    for trydir in directories : \n        if trydir == '$cwd' : \n            trydir = '.' \n        else : \n            if trydir == '$cdir' : \n                trydir = cdir \n        tryfile = osp . realpath ( osp . join ( trydir , filename ) ) \n        if osp . isfile ( tryfile ) : \n            return tryfile \n    return None "}
{"6052": "\ndef msg ( self , msg ) : \n    if hasattr ( self . output , 'writeline' ) : \n        self . output . writeline ( msg ) \n    else : \n        if hasattr ( self . output , 'writelines' ) : \n            self . output . writelines ( msg + \"\\n\" ) \n            pass \n    return "}
{"6053": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        if mainfile : \n            part1 = \"Python program '%s' is stopped\" % mainfile \n        else : \n            part1 = 'Program is stopped' \n            pass \n        if self . proc . event : \n            msg = 'via a %s event.' % self . proc . event \n        else : \n            msg = '.' \n        self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        if self . proc . curframe : \n            self . msg ( \"PC offset is %d.\" % self . proc . curframe . f_lasti ) \n        if self . proc . event == 'return' : \n            val = self . proc . event_arg \n            part1 = 'Return value is' \n            self . msg ( Mmisc . wrapped_lines ( part1 , self . proc . _saferepr ( val ) , self . settings [ 'width' ] ) ) \n            pass \n        else : \n            if self . proc . event == 'exception' : \n                exc_type , exc_value , exc_tb = self . proc . event_arg \n                self . msg ( 'Exception type: %s' % self . proc . _saferepr ( exc_type ) ) \n                if exc_value : \n                    self . msg ( 'Exception value: %s' % self . proc . _saferepr ( exc_value ) ) \n                    pass \n                pass \n        self . msg ( 'It stopped %s.' % self . core . stop_reason ) \n        if self . proc . event in [ 'signal' , 'exception' , 'c_exception' ] : \n            self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) \n            pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return False "}
{"6064": "\ndef is_stop_here ( self , frame , event , arg ) : \n    lineno = frame . f_lineno \n    filename = frame . f_code . co_filename \n    if self . different_line and event == 'line' : \n        if self . last_lineno == lineno and self . last_filename == filename : \n            return False \n        pass \n    self . last_lineno = lineno \n    self . last_filename = filename \n    if self . stop_level is not None : \n        if frame != self . last_frame : \n            self . last_level = Mstack . count_frames ( frame ) \n            self . last_frame = frame \n            pass \n        if self . last_level > self . stop_level : \n            return False \n        else : \n            if self . last_level == self . stop_level and self . stop_on_finish and event in [ 'return' , 'c_return' ] : \n                self . stop_level = None \n                self . stop_reason = \"in return for 'finish' command\" \n                return True \n        pass \n    if self . _is_step_next_stop ( event ) : \n        self . stop_reason = 'at a stepping statement' \n        return True \n    return False "}
{"6067": "\ndef run ( self , args ) : \n    if len ( args ) == 0 : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return False \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ 0 ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m += \" cached in debugger\" \n        if filename_cache [ filename ] != filename : \n            m += ' as:' \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m += '.' \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( len ( matches ) > 1 ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        else : \n            if len ( matches ) == 1 : \n                canonic_name = pyficache . unmap_file ( matches [ 0 ] ) \n                m += \" matched debugger cache file:\\n  \" + canonic_name \n                self . msg ( m ) \n            else : \n                self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if name == v ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ 1 : ] : \n        processed_arg = False \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = False , arrange_vertical = False , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = True \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6074": "\ndef open ( self , inp , opts = None ) : \n    if isinstance ( inp , io . TextIOWrapper ) : \n        self . input = inp \n    else : \n        if isinstance ( inp , 'string' . __class__ ) : \n            self . name = inp \n            self . input = open ( inp , 'r' ) \n        else : \n            raise IOError ( \"Invalid input type (%s) for %s\" % ( inp . __class__ . __name__ , inp ) ) \n    return "}
{"6076": "\ndef confirm ( self , prompt , default ) : \n    while True : \n        try : \n            self . write_confirm ( prompt , default ) \n            reply = self . readline ( '' ) . strip ( ) . lower ( ) \n        except EOFError : \n            return default \n        if reply in ( 'y' , 'yes' ) : \n            return True \n        else : \n            if reply in ( 'n' , 'no' ) : \n                return False \n            else : \n                self . msg ( \"Please answer y or n.\" ) \n                pass \n        pass \n    return default "}
{"6085": "\ndef open_json_or_csv_somehow ( filename , date_format = None ) : \n    fileformat = None \n    if filename . endswith ( '.csv' ) : \n        fileformat = 'csv' \n    else : \n        if filename . endswith ( '.jsons' ) : \n            fileformat = 'jsons' \n        else : \n            with open ( filename ) as opened : \n                line = opened . readline ( ) \n                if line [ 0 ] not in '{[' and not filename . endswith ( '.json' ) : \n                    fileformat = 'csv' \n                else : \n                    if ( line . count ( '{' ) == line . count ( '}' ) and line . count ( '[' ) == line . count ( ']' ) ) : \n                        char = ' ' \n                        while char . isspace ( ) : \n                            char = opened . read ( ) \n                            if char == '' : \n                                fileformat = 'json' \n                                break \n                        if fileformat is None : \n                            fileformat = 'jsons' \n                    else : \n                        fileformat = 'json' \n    if fileformat == 'json' : \n        stream = json . load ( open ( filename ) , encoding = 'utf-8' ) \n    else : \n        if fileformat == 'csv' : \n            stream = open_csv_somehow ( filename ) \n        else : \n            stream = stream_json_lines ( filename ) \n    return _normalize_data ( stream , date_format = date_format ) "}
{"6096": "\ndef _request ( self , req_type , url , ** kwargs ) : \n    logger . debug ( '%s %s' % ( req_type , url ) ) \n    result = self . session . request ( req_type , url , ** kwargs ) \n    try : \n        result . raise_for_status ( ) \n    except requests . HTTPError : \n        error = result . text \n        try : \n            error = json . loads ( error ) \n        except ValueError : \n            pass \n        if result . status_code in ( 401 , 403 ) : \n            error_class = LuminosoAuthError \n        else : \n            if result . status_code in ( 400 , 404 , 405 ) : \n                error_class = LuminosoClientError \n            else : \n                if result . status_code >= 500 : \n                    error_class = LuminosoServerError \n                else : \n                    error_class = LuminosoError \n        raise error_class ( error ) \n    return result "}
{"6116": "\ndef main ( ) : \n    import argparse \n    parser = argparse . ArgumentParser ( ) \n    parser . add_argument ( 'filename' ) \n    parser . add_argument ( 'account' ) \n    parser . add_argument ( 'project_name' ) \n    parser . add_argument ( '--append' , help = ( \"If append flag is used, upload documents to existing project, \" \"rather than creating a new project.\" ) , action = \"store_true\" ) \n    parser . add_argument ( '-s' , '--stage' , help = \"If stage flag is used, just upload docs, don't recalculate.\" , action = \"store_true\" ) \n    parser . add_argument ( '-a' , '--api-url' , help = \"Specify an alternate API url\" , default = URL_BASE ) \n    parser . add_argument ( '-l' , '--language' , help = ( \"Two-letter language code to use when recalculating (e.g. 'en' \" \"or 'ja')\" ) ) \n    parser . add_argument ( '-u' , '--username' , default = None , help = \"username (defaults to your username on your computer)\" ) \n    parser . add_argument ( '-p' , '--password' , default = None , help = \"password (you can leave this out and type it in later)\" ) \n    parser . add_argument ( '-d' , '--date-format' , default = 'iso' , help = ( \"format string for parsing dates, following \" \"http://strftime.org/.  Default is 'iso', which is \" \"'%%Y-%%m-%%dT%%H:%%M:%%S+00:00'.  Other shortcuts are 'epoch' \" \"for epoch time or 'us-standard' for '%%m/%%d/%%y'\" ) ) \n    args = parser . parse_args ( ) \n    date_format_lower = args . date_format . lower ( ) \n    if date_format_lower == 'iso' : \n        date_format = '%Y-%m-%dT%H:%M:%S+00:00' \n    else : \n        if date_format_lower in [ 'unix' , 'epoch' ] : \n            date_format = 'epoch' \n        else : \n            if date_format_lower == 'us-standard' : \n                date_format = '%m/%d/%y' \n            else : \n                date_format = args . date_format \n    upload_file ( args . filename , args . api_url , args . account , args . project_name , language = args . language , username = args . username , password = args . password , append = args . append , stage = args . stage , date_format = date_format ) "}
{"6128": "\ndef on_message ( self , message ) : \n    change = json . loads ( message ) \n    log . debug ( f'Update from js: {change}' ) \n    ref = change . get ( 'ref' ) \n    if not ref : \n        return \n    nodes = self . viewer . xpath ( '//*[@ref=$ref]' , ref = ref ) \n    if not nodes : \n        return \n    node = nodes [ 0 ] \n    if change . get ( 'type' ) and change . get ( 'name' ) : \n        if change [ 'type' ] == 'event' : \n            trigger = getattr ( node , change [ 'name' ] ) \n            trigger ( ) \n        else : \n            if change [ 'type' ] == 'update' : \n                setattr ( node , change [ 'name' ] , change [ 'value' ] ) \n    else : \n        log . warning ( f\"Unhandled event {self} {node}: {change}\" ) "}
{"6131": "\ndef init_widget ( self ) : \n    widget = self . widget \n    d = self . declaration \n    ref = d . ref \n    CACHE [ ref ] = atomref ( self ) \n    widget . set ( 'ref' , ref ) \n    if d . text : \n        self . set_text ( d . text ) \n    if d . tail : \n        self . set_tail ( d . tail ) \n    if d . style : \n        self . set_style ( d . style ) \n    if d . cls : \n        self . set_cls ( d . cls ) \n    if d . attrs : \n        self . set_attrs ( d . attrs ) \n    if d . id : \n        widget . set ( 'id' , d . id ) \n    if d . draggable : \n        self . set_draggable ( d . draggable ) \n    for name , member in d . members ( ) . items ( ) : \n        if not member . metadata : \n            continue \n        meta = member . metadata \n        if not ( meta . get ( 'd_member' ) and meta . get ( 'd_final' ) ) : \n            continue \n        else : \n            if not meta . get ( 'attr' , True ) : \n                continue \n            else : \n                if isinstance ( member , Event ) : \n                    continue \n        value = getattr ( d , name ) \n        if value : \n            self . set_attribute ( name , value ) "}
{"6136": "\ndef set_attribute ( self , name , value ) : \n    if value is True : \n        self . widget . set ( name , name ) \n    else : \n        if value is False : \n            del self . widget . attrib [ name ] \n        else : \n            self . widget . set ( name , str ( value ) ) "}
{"6155": "\ndef add_item_to_basket ( self , item , variant = VARIANT . MEDIUM , quantity = 1 ) : \n    item_type = item . type \n    if item_type == 'Pizza' : \n        return self . add_pizza_to_basket ( item , variant , quantity ) \n    else : \n        if item_type == 'Side' : \n            return self . add_side_to_basket ( item , quantity ) \n    return None "}
{"6167": "\ndef process_user_input ( self ) : \n    user_input = self . get_input ( ) \n    go_to_max = ord ( \"9\" ) if len ( self . items ) >= 9 else ord ( str ( len ( self . items ) ) ) \n    if ord ( '1' ) <= user_input <= go_to_max : \n        self . go_to ( user_input - ord ( '0' ) - 1 ) \n    else : \n        if user_input == curses . KEY_DOWN : \n            self . go_down ( ) \n        else : \n            if user_input == curses . KEY_UP : \n                self . go_up ( ) \n            else : \n                if user_input == ord ( \"\\n\" ) : \n                    self . select ( ) \n    return user_input "}
{"6169": "\ndef parse_old_menu ( menu_data ) : \n    menu_title = menu_data [ 'title' ] \n    menu = CursesMenu ( menu_title ) \n    for item in menu_data [ \"options\" ] : \n        item_type = item [ \"type\" ] \n        item_title = item [ \"title\" ] \n        if item_type == menuItem . COMMAND : \n            item_command = item [ \"command\" ] \n            menu . append_item ( CommandItem ( item_title , item_command , menu ) ) \n        else : \n            if item_type == menuItem . FUNCTION : \n                item_function = item [ \"function\" ] \n                menu . append_item ( FunctionItem ( item_title , item_function , menu ) ) \n            else : \n                if item_type == menuItem . EXITMENU : \n                    menu . append_item ( ExitItem ( item_title , menu ) ) \n                else : \n                    if item_type == menuItem . NUMBER : \n                        menu . append_item ( SelectionItem ( item_title , menu ) ) \n                    else : \n                        if item_type == menuItem . MENU : \n                            new_menu = parse_old_menu ( item ) \n                            menu . append_item ( SubmenuItem ( item_title , menu , new_menu ) ) \n    return menu "}
{"6185": "\ndef add_missing_row ( df : pd . DataFrame , id_cols : List [ str ] , reference_col : str , complete_index : Union [ Dict [ str , str ] , List [ str ] ] = None , method : str = None , cols_to_keep : List [ str ] = None ) -> pd . DataFrame : \n    if cols_to_keep is None : \n        cols_for_index = [ reference_col ] \n    else : \n        cols_for_index = [ reference_col ] + cols_to_keep \n    check_params_columns_duplicate ( id_cols + cols_for_index ) \n    if method == 'between' or method == 'between_and_after' : \n        df [ 'start' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( min ) \n        id_cols += [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        df [ 'end' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( max ) \n        id_cols += [ 'end' ] \n    names = id_cols + cols_for_index \n    new_df = df . set_index ( names ) \n    index_values = df . groupby ( id_cols ) . sum ( ) . index . values \n    if complete_index is None : \n        complete_index = df . groupby ( cols_for_index ) . sum ( ) . index . values \n    else : \n        if isinstance ( complete_index , dict ) : \n            if complete_index [ 'type' ] == 'date' : \n                freq = complete_index [ 'freq' ] \n                date_format = complete_index [ 'format' ] \n                start = complete_index [ 'start' ] \n                end = complete_index [ 'end' ] \n                if isinstance ( freq , dict ) : \n                    freq = pd . DateOffset ( ** { k : int ( v ) for k , v in freq . items ( ) } ) \n                complete_index = pd . date_range ( start = start , end = end , freq = freq ) \n                complete_index = complete_index . strftime ( date_format ) \n            else : \n                raise ParamsValueError ( f'Unknown complete index type: ' f'{complete_index[\"type\"]}' ) \n    if not isinstance ( index_values [ 0 ] , tuple ) : \n        index_values = [ ( x , ) for x in index_values ] \n    if not isinstance ( complete_index [ 0 ] , tuple ) : \n        complete_index = [ ( x , ) for x in complete_index ] \n    new_tuples_index = [ x + y for x in index_values for y in complete_index ] \n    new_index = pd . MultiIndex . from_tuples ( new_tuples_index , names = names ) \n    new_df = new_df . reindex ( new_index ) . reset_index ( ) \n    if method == 'between' or method == 'between_and_after' : \n        new_df = new_df [ new_df [ reference_col ] >= new_df [ 'start' ] ] \n        del new_df [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        new_df = new_df [ new_df [ reference_col ] <= new_df [ 'end' ] ] \n        del new_df [ 'end' ] \n    return new_df "}
{"6203": "\ndef filter_by_date ( df , date_col : str , date_format : str = '%Y-%m-%d' , start : str = None , stop : str = None , atdate : str = None ) : \n    mask = None \n    if start is None and stop is None and atdate is None : \n        raise TypeError ( 'either \"start\", \"stop\" or \"atdate\" must be specified' ) \n    if start is not None and atdate is not None : \n        raise TypeError ( '\"start\" and \"atdate\" are mutually exclusive' ) \n    if stop is not None and atdate is not None : \n        raise TypeError ( '\"stop\" and \"atdate\" are mutually exclusive' ) \n    filtercol = str ( uuid4 ( ) ) \n    df [ filtercol ] = pd . to_datetime ( df [ date_col ] , format = date_format ) \n    if atdate is not None : \n        mask = df [ filtercol ] == parse_date ( atdate , date_format ) \n    else : \n        if start is not None and stop is not None : \n            mask = ( ( df [ filtercol ] >= parse_date ( start , date_format ) ) & ( df [ filtercol ] < parse_date ( stop , date_format ) ) ) \n        else : \n            if stop is None : \n                mask = df [ filtercol ] >= parse_date ( start , date_format ) \n            else : \n                if start is None : \n                    mask = df [ filtercol ] < parse_date ( stop , date_format ) \n    return df [ mask ] . drop ( filtercol , axis = 1 ) "}
{"6205": "\ndef ada_family_core ( params , gparams , learning_rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum_regularization = 0.0001 ) : \n    _ , _ , _ , args = inspect . getargvalues ( inspect . currentframe ( ) ) \n    logging . info ( \"ada_family_core: %s\" % str ( args . items ( ) ) ) \n    free_parameters = [ ] \n    if method == \"FINETUNING_ADAGRAD\" : \n        method = \"ADAGRAD\" \n        gsum_regularization = 0 \n    oneMinusBeta = 1 - beta \n    gsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"gsum_%s\" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] \n    xsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"xsum_%s\" % param . name ) if method == 'ADADELTA' else None for param in params ] \n    if method == 'ADAGRAD' : \n        for gsum in gsums : \n            gsum . set_value ( gsum . get_value ( ) ** 0 ) \n    updates = OrderedDict ( ) \n    for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : \n        if method == 'ADADELTA' : \n            updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) \n            dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam \n            updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) \n            updates [ param ] = param * oneMinusBeta + dparam \n        else : \n            if method == 'ADAGRAD' : \n                updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum_regularization * gsum \n                updates [ param ] = param * oneMinusBeta - learning_rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) \n            else : \n                updates [ param ] = param * oneMinusBeta - gparam * learning_rate \n    if method == 'ADADELTA' : \n        free_parameters . extend ( gsums + xsums ) \n    else : \n        if method == 'ADAGRAD' : \n            free_parameters . extend ( gsums ) \n    for k in updates : \n        if updates [ k ] . dtype != FLOATX : \n            updates [ k ] = updates [ k ] . astype ( FLOATX ) \n    return updates . items ( ) , free_parameters "}
{"6235": "\ndef get_data ( self , data_split = \"train\" ) : \n    if data_split == 'train' : \n        return self . _current_train_set \n    else : \n        if data_split == 'valid' : \n            return self . _current_valid_set \n        else : \n            if data_split == 'test' : \n                return self . _current_test_set \n            else : \n                return None "}
{"6239": "\ndef create_vars_from_data ( self , dataset , split = \"train\" ) : \n    from deepy . core . neural_var import NeuralVariable \n    vars = [ ] \n    if split == \"valid\" : \n        data_split = dataset . valid_set ( ) \n    else : \n        if split == \"test\" : \n            data_split = dataset . test_set ( ) \n        else : \n            data_split = dataset . train_set ( ) \n    first_data_piece = list ( data_split ) [ 0 ] \n    for i , numpy_tensor in enumerate ( first_data_piece ) : \n        if numpy_tensor . dtype == \"int64\" : \n            numpy_tensor = numpy_tensor . astype ( \"int32\" ) \n        if numpy_tensor . dtype == \"float64\" : \n            numpy_tensor = numpy_tensor . astype ( env . FLOATX ) \n        type_map = { 0 : \"scalar\" , 1 : \"vector\" , 2 : \"matrix\" , 3 : \"tensor3\" , 4 : \"tensor4\" , 5 : \"tensor5\" , } \n        tensor_type = type_map [ numpy_tensor . ndim ] if numpy_tensor . ndim in type_map else type_map [ 0 ] \n        if numpy_tensor . dtype . kind == \"i\" : \n            tensor_type = \"i\" + tensor_type \n        theano_tensor = getattr ( TT , tensor_type ) ( \"input_{}_{}\" . format ( i + 1 , tensor_type ) ) \n        last_dim = numpy_tensor . shape [ - 1 ] \n        var = NeuralVariable ( theano_tensor , dim = last_dim ) \n        var . set_test_value ( numpy_tensor ) \n        vars . append ( var ) \n    return vars "}
{"6240": "\ndef shared ( self , value , name = None ) : \n    if type ( value ) == int : \n        final_value = np . array ( value , dtype = \"int32\" ) \n    else : \n        if type ( value ) == float : \n            final_value = np . array ( value , dtype = env . FLOATX ) \n        else : \n            final_value = value \n    return theano . shared ( final_value , name = name ) "}
{"6252": "\ndef load_params ( self , path , exclude_free_params = False ) : \n    if not os . path . exists ( path ) : \n        return ; \n    logging . info ( \"loading parameters from %s\" % path ) \n    if exclude_free_params : \n        params_to_load = self . parameters \n    else : \n        params_to_load = self . all_parameters \n    if path . endswith ( \".gz\" ) : \n        opener = gzip . open if path . lower ( ) . endswith ( '.gz' ) else open \n        handle = opener ( path , 'rb' ) \n        saved_params = pickle . load ( handle ) \n        handle . close ( ) \n        for target , source in zip ( params_to_load , saved_params ) : \n            logging . info ( '%s: setting value %s' , target . name , source . shape ) \n            target . set_value ( source ) \n    else : \n        if path . endswith ( \".npz\" ) : \n            arrs = np . load ( path ) \n            for target , idx in zip ( params_to_load , range ( len ( arrs . keys ( ) ) ) ) : \n                source = arrs [ 'arr_%d' % idx ] \n                logging . info ( '%s: setting value %s' , target . name , source . shape ) \n                target . set_value ( source ) \n        else : \n            raise Exception ( \"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path ) \n    self . train_logger . load ( path ) "}
{"6262": "\ndef create_request_elements ( cls , request_type , credentials , url , method = 'GET' , params = None , headers = None , body = '' , secret = None , redirect_uri = '' , scope = '' , csrf = '' , user_state = '' ) : \n    headers = headers or { } \n    params = params or { } \n    consumer_key = credentials . consumer_key or '' \n    consumer_secret = credentials . consumer_secret or '' \n    token = credentials . token or '' \n    refresh_token = credentials . refresh_token or credentials . token or '' \n    url , base_params = cls . _split_url ( url ) \n    params . update ( dict ( base_params ) ) \n    if request_type == cls . USER_AUTHORIZATION_REQUEST_TYPE : \n        if consumer_key and redirect_uri and ( csrf or not cls . supports_csrf_protection ) : \n            params [ 'client_id' ] = consumer_key \n            params [ 'redirect_uri' ] = redirect_uri \n            params [ 'scope' ] = scope \n            if cls . supports_user_state : \n                params [ 'state' ] = base64 . urlsafe_b64encode ( json . dumps ( { \"csrf\" : csrf , \"user_state\" : user_state } ) . encode ( 'utf-8' ) ) \n            else : \n                params [ 'state' ] = csrf \n            params [ 'response_type' ] = 'code' \n            headers . update ( cls . _authorization_header ( credentials ) ) \n        else : \n            raise OAuth2Error ( 'Credentials with valid consumer_key and arguments ' 'redirect_uri, scope and state are required to create ' 'OAuth 2.0 user authorization request elements!' ) \n    else : \n        if request_type == cls . ACCESS_TOKEN_REQUEST_TYPE : \n            if consumer_key and consumer_secret : \n                params [ 'code' ] = token \n                params [ 'client_id' ] = consumer_key \n                params [ 'client_secret' ] = consumer_secret \n                params [ 'redirect_uri' ] = redirect_uri \n                params [ 'grant_type' ] = 'authorization_code' \n                headers . update ( cls . _authorization_header ( credentials ) ) \n            else : \n                raise OAuth2Error ( 'Credentials with valid token, consumer_key, ' 'consumer_secret and argument redirect_uri are required ' 'to create OAuth 2.0 access token request elements!' ) \n        else : \n            if request_type == cls . REFRESH_TOKEN_REQUEST_TYPE : \n                if refresh_token and consumer_key and consumer_secret : \n                    params [ 'refresh_token' ] = refresh_token \n                    params [ 'client_id' ] = consumer_key \n                    params [ 'client_secret' ] = consumer_secret \n                    params [ 'grant_type' ] = 'refresh_token' \n                else : \n                    raise OAuth2Error ( 'Credentials with valid refresh_token, consumer_key, ' 'consumer_secret are required to create OAuth 2.0 ' 'refresh token request elements!' ) \n            else : \n                if request_type == cls . PROTECTED_RESOURCE_REQUEST_TYPE : \n                    if credentials . token_type == cls . BEARER : \n                        headers . update ( { 'Authorization' : 'Bearer {0}' . format ( credentials . token ) } ) \n                    else : \n                        if token : \n                            params [ 'access_token' ] = token \n                        else : \n                            raise OAuth2Error ( 'Credentials with valid token are required to create ' 'OAuth 2.0 protected resources request elements!' ) \n    request_elements = core . RequestElements ( url , method , params , headers , body ) \n    return cls . _x_request_elements_filter ( request_type , request_elements , credentials ) "}
{"6282": "\ndef create_request_elements ( cls , request_type , credentials , url , params = None , headers = None , body = '' , method = 'GET' , verifier = '' , callback = '' ) : \n    params = params or { } \n    headers = headers or { } \n    consumer_key = credentials . consumer_key or '' \n    consumer_secret = credentials . consumer_secret or '' \n    token = credentials . token or '' \n    token_secret = credentials . token_secret or '' \n    url , base_params = cls . _split_url ( url ) \n    params . update ( dict ( base_params ) ) \n    if request_type == cls . USER_AUTHORIZATION_REQUEST_TYPE : \n        if token : \n            params [ 'oauth_token' ] = token \n        else : \n            raise OAuth1Error ( 'Credentials with valid token are required to create ' 'User Authorization URL!' ) \n    else : \n        if request_type == cls . REQUEST_TOKEN_REQUEST_TYPE : \n            if consumer_key and consumer_secret and callback : \n                params [ 'oauth_consumer_key' ] = consumer_key \n                params [ 'oauth_callback' ] = callback \n            else : \n                raise OAuth1Error ( 'Credentials with valid consumer_key, consumer_secret ' 'and callback are required to create Request Token ' 'URL!' ) \n        else : \n            if request_type == cls . ACCESS_TOKEN_REQUEST_TYPE : \n                if consumer_key and consumer_secret and token and verifier : \n                    params [ 'oauth_token' ] = token \n                    params [ 'oauth_consumer_key' ] = consumer_key \n                    params [ 'oauth_verifier' ] = verifier \n                else : \n                    raise OAuth1Error ( 'Credentials with valid consumer_key, ' 'consumer_secret, token and argument verifier' ' are required to create Access Token URL!' ) \n            else : \n                if request_type == cls . PROTECTED_RESOURCE_REQUEST_TYPE : \n                    if consumer_key and consumer_secret and token and token_secret : \n                        params [ 'oauth_token' ] = token \n                        params [ 'oauth_consumer_key' ] = consumer_key \n                    else : \n                        raise OAuth1Error ( 'Credentials with valid consumer_key, ' + 'consumer_secret, token and token_secret are required ' 'to create Protected Resources URL!' ) \n        params [ 'oauth_signature_method' ] = cls . _signature_generator . method \n        params [ 'oauth_timestamp' ] = str ( int ( time . time ( ) ) ) \n        params [ 'oauth_nonce' ] = cls . csrf_generator ( str ( uuid . uuid4 ( ) ) ) \n        params [ 'oauth_version' ] = '1.0' \n        params [ 'oauth_signature' ] = cls . _signature_generator . create_signature ( method , url , params , consumer_secret , token_secret ) \n    request_elements = core . RequestElements ( url , method , params , headers , body ) \n    return cls . _x_request_elements_filter ( request_type , request_elements , credentials ) "}
{"6295": "\ndef flexible_str ( obj ) : \n    if obj is None : \n        return None \n    else : \n        if ( not isinstance ( obj , str ) and isinstance ( obj , collections . abc . Iterable ) ) : \n            return ', ' . join ( str ( item ) for item in sorted ( obj ) ) \n        else : \n            return str ( obj ) "}
{"6296": "\ndef ensure_iterable ( inst ) : \n    if isinstance ( inst , str ) : \n        return [ inst ] \n    else : \n        if not isinstance ( inst , collections . abc . Iterable ) : \n            return [ inst ] \n        else : \n            return inst "}
{"6301": "\ndef from_numpy_array ( nparr , framerate ) : \n    if nparr . dtype . itemsize not in ( 1 , 2 , 4 ) : \n        raise ValueError ( \"Numpy Array must contain 8, 16, or 32 bit values.\" ) \n    if len ( nparr . shape ) == 1 : \n        arrays = [ nparr ] \n    else : \n        if len ( nparr . shape ) == 2 : \n            arrays = [ nparr [ i , : ] for i in range ( nparr . shape [ 0 ] ) ] \n        else : \n            raise ValueError ( \"Numpy Array must be one or two dimensional. Shape must be: (num_samples, num_channels).\" ) \n    interleaved = np . vstack ( arrays ) . reshape ( ( - 1 , ) , order = 'F' ) \n    dubseg = pydub . AudioSegment ( interleaved . tobytes ( ) , frame_rate = framerate , sample_width = interleaved . dtype . itemsize , channels = len ( interleaved . shape ) ) \n    return AudioSegment ( dubseg , \"\" ) "}
{"6304": "\ndef fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = False ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample can be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples can be specified.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    if duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    end_sample = start_sample + num_samples \n    if end_sample > len ( self . get_array_of_samples ( ) ) and not zero_pad : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    else : \n        if end_sample > len ( self . get_array_of_samples ( ) ) and zero_pad : \n            arr = np . array ( self . get_array_of_samples ( ) ) \n            zeros = np . zeros ( end_sample - len ( arr ) ) \n            arr = np . append ( arr , zeros ) \n        else : \n            arr = np . array ( self . get_array_of_samples ( ) ) \n    audioslice = np . array ( arr [ start_sample : end_sample ] ) \n    fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2 ) ) + 1 ) ] \n    step_size = self . frame_rate / num_samples \n    bins = np . arange ( 0 , int ( round ( num_samples / 2 ) ) + 1 , 1.0 ) * step_size \n    return bins , fft_result "}
{"6310": "\ndef spectrogram ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , window_length_s = None , window_length_samples = None , overlap = 0.5 , window = ( 'tukey' , 0.25 ) ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample may be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples may be specified.\" ) \n    if window_length_s is not None and window_length_samples is not None : \n        raise ValueError ( \"Only one of window_length_s and window_length_samples may be specified.\" ) \n    if window_length_s is None and window_length_samples is None : \n        raise ValueError ( \"You must specify a window length, either in window_length_s or in window_length_samples.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    else : \n        if start_s is not None : \n            start_sample = int ( round ( start_s * self . frame_rate ) ) \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    else : \n        if duration_s is not None : \n            num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if window_length_s is not None : \n        window_length_samples = int ( round ( window_length_s * self . frame_rate ) ) \n    if start_sample + num_samples > len ( self . get_array_of_samples ( ) ) : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    arr = self . to_numpy_array ( ) [ start_sample : start_sample + num_samples ] \n    fs , ts , sxx = signal . spectrogram ( arr , self . frame_rate , scaling = 'spectrum' , nperseg = window_length_samples , noverlap = int ( round ( overlap * window_length_samples ) ) , mode = 'magnitude' , window = window ) \n    return fs , ts , sxx "}
{"6325": "\ndef _downsample_one_or_the_other ( mask , mask_indexes , stft , stft_indexes ) : \n    assert len ( mask . shape ) == 2 , \"Expected a two-dimensional `mask`, but got one of {} dimensions.\" . format ( len ( mask . shape ) ) \n    assert len ( stft . shape ) == 2 , \"Expected a two-dimensional `stft`, but got one of {} dimensions.\" . format ( len ( stft . shape ) ) \n    if mask . shape [ 1 ] > stft . shape [ 1 ] : \n        downsample_factor = mask . shape [ 1 ] / stft . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( mask , downsample_factor ) \n        mask = mask [ : , indexes ] \n        mask_indexes = np . array ( indexes ) \n    else : \n        if mask . shape [ 1 ] < stft . shape [ 1 ] : \n            downsample_factor = stft . shape [ 1 ] / mask . shape [ 1 ] \n            indexes = _get_downsampled_indexes ( stft , downsample_factor ) \n            stft = stft [ : , indexes ] \n            stft_indexes = np . array ( indexes ) \n    return mask , mask_indexes , stft , stft_indexes "}
{"6330": "\ndef expand_and_standardize_dataset ( response_index , response_header , data_set , col_vals , headers , standardizers , feats_to_ignore , columns_to_expand , outcome_trans_dict ) : \n    modified_set = [ ] \n    for row_index , row in enumerate ( data_set ) : \n        new_row = [ ] \n        for col_index , val in enumerate ( row ) : \n            header = headers [ col_index ] \n            if col_index == response_index : \n                new_outcome = outcome_trans_dict [ val ] \n                new_row . append ( new_outcome ) \n            else : \n                if header in feats_to_ignore : \n                    pass \n                else : \n                    if header in columns_to_expand : \n                        for poss_val in col_vals [ header ] : \n                            if val == poss_val : \n                                new_cat_val = 1.0 \n                            else : \n                                new_cat_val = - 1.0 \n                            new_row . append ( new_cat_val ) \n                    else : \n                        new_cont_val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std_dev' ] ) \n                        new_row . append ( new_cont_val ) \n        modified_set . append ( new_row ) \n    expanded_headers = [ ] \n    for header in headers : \n        if header in feats_to_ignore : \n            pass \n        else : \n            if ( header in columns_to_expand ) and ( header is not response_header ) : \n                for poss_val in col_vals [ header ] : \n                    new_header = '{}_{}' . format ( header , poss_val ) \n                    expanded_headers . append ( new_header ) \n            else : \n                expanded_headers . append ( header ) \n    return modified_set , expanded_headers "}
{"6340": "\ndef handle_error ( errcode ) : \n    if type ( errcode ) is c_int : \n        errcode = errcode . value \n    if errcode == 0 : \n        pass \n    else : \n        if errcode == - 1 : \n            raise TimeoutError ( \"the operation failed due to a timeout.\" ) \n        else : \n            if errcode == - 2 : \n                raise LostError ( \"the stream has been lost.\" ) \n            else : \n                if errcode == - 3 : \n                    raise InvalidArgumentError ( \"an argument was incorrectly specified.\" ) \n                else : \n                    if errcode == - 4 : \n                        raise InternalError ( \"an internal error has occurred.\" ) \n                    else : \n                        if errcode < 0 : \n                            raise RuntimeError ( \"an unknown error has occurred.\" ) "}
{"6368": "\ndef do_DBKEY ( self , key ) : \n    type_ = DB . type ( key ) . decode ( ) \n    if type_ == 'set' : \n        out = DB . smembers ( key ) \n    else : \n        if type_ == 'string' : \n            out = DB . get ( key ) \n        else : \n            out = 'Unsupported type {}' . format ( type_ ) \n    print ( 'type:' , magenta ( type_ ) ) \n    print ( 'value:' , white ( out ) ) "}
{"6375": "\ndef map ( requests , stream = True , pool = None , size = 1 , exception_handler = None ) : \n    pool = pool if pool else Pool ( size ) \n    requests = list ( requests ) \n    requests = pool . map ( send , requests ) \n    ret = [ ] \n    for request in requests : \n        if request . response is not None : \n            ret . append ( request . response ) \n        else : \n            if exception_handler and hasattr ( request , 'exception' ) : \n                ret . append ( exception_handler ( request , request . exception ) ) \n            else : \n                ret . append ( None ) \n    if not pool : \n        pool . close ( ) \n    return ret "}
{"6381": "\ndef StaticForEach ( parentUnit , items , bodyFn , name = \"\" ) : \n    items = list ( items ) \n    itemsCnt = len ( items ) \n    if itemsCnt == 0 : \n        return [ ] \n    else : \n        if itemsCnt == 1 : \n            return bodyFn ( items [ 0 ] , 0 ) \n        else : \n            index = parentUnit . _reg ( name + \"for_index\" , Bits ( log2ceil ( itemsCnt + 1 ) , signed = False ) , defVal = 0 ) \n            ackSig = parentUnit . _sig ( name + \"for_ack\" ) \n            statementLists = [ ] \n            for i , ( statementList , ack ) in [ ( i , bodyFn ( item , i ) ) for i , item in enumerate ( items ) ] : \n                statementLists . append ( statementList + [ ( ackSig ( ack ) ) , ] ) \n            If ( ackSig , If ( index . _eq ( itemsCnt - 1 ) , index ( 0 ) ) . Else ( index ( index + 1 ) ) ) \n            return Switch ( index ) . addCases ( enumerate ( statementLists ) ) . Default ( bodyFn ( items [ 0 ] , 0 ) [ 0 ] , ackSig ( True ) ) "}
{"6391": "\ndef autoAddAgents ( unit ) : \n    proc = [ ] \n    for intf in unit . _interfaces : \n        if not intf . _isExtern : \n            continue \n        intf . _initSimAgent ( ) \n        assert intf . _ag is not None , intf \n        agents = [ intf . _ag , ] \n        if intf . _direction == INTF_DIRECTION . MASTER : \n            agProcs = list ( map ( lambda a : a . getMonitors ( ) , agents ) ) \n        else : \n            if intf . _direction == INTF_DIRECTION . SLAVE : \n                agProcs = list ( map ( lambda a : a . getDrivers ( ) , agents ) ) \n            else : \n                raise NotImplementedError ( \"intf._direction %r for %r\" % ( intf . _direction , intf ) ) \n        for p in agProcs : \n            proc . extend ( p ) \n    return proc "}
{"6402": "\ndef toRtl ( unitOrCls : Unit , name : str = None , serializer : GenericSerializer = VhdlSerializer , targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : \n    if not isinstance ( unitOrCls , Unit ) : \n        u = unitOrCls ( ) \n    else : \n        u = unitOrCls \n    u . _loadDeclarations ( ) \n    if name is not None : \n        assert isinstance ( name , str ) \n        u . _name = name \n    globScope = serializer . getBaseNameScope ( ) \n    mouduleScopes = { } \n    serializedClasses = { } \n    serializedConfiguredUnits = { } \n    doSerialize = True \n    createFiles = saveTo is not None \n    if createFiles : \n        os . makedirs ( saveTo , exist_ok = True ) \n        files = UniqList ( ) \n    else : \n        codeBuff = [ ] \n    for obj in u . _toRtl ( targetPlatform ) : \n        doSerialize = serializer . serializationDecision ( obj , serializedClasses , serializedConfiguredUnits ) \n        if doSerialize : \n            if isinstance ( obj , Entity ) : \n                s = globScope . fork ( 1 ) \n                s . setLevel ( 2 ) \n                ctx = serializer . getBaseContext ( ) \n                ctx . scope = s \n                mouduleScopes [ obj ] = ctx \n                ctx . currentUnit = obj . origin \n                sc = serializer . Entity ( obj , ctx ) \n                if createFiles : \n                    fName = obj . name + serializer . fileExtension \n                    fileMode = 'w' \n            else : \n                if isinstance ( obj , Architecture ) : \n                    try : \n                        ctx = mouduleScopes [ obj . entity ] \n                    except KeyError : \n                        raise SerializerException ( \"Entity should be serialized\" \" before architecture of %s\" % ( obj . getEntityName ( ) ) ) \n                    sc = serializer . Architecture ( obj , ctx ) \n                    if createFiles : \n                        fName = obj . getEntityName ( ) + serializer . fileExtension \n                        fileMode = 'a' \n                else : \n                    if hasattr ( obj , \"_hdlSources\" ) : \n                        for fn in obj . _hdlSources : \n                            if isinstance ( fn , str ) : \n                                shutil . copy2 ( fn , saveTo ) \n                                files . append ( fn ) \n                                continue \n                    else : \n                        sc = serializer . asHdl ( obj ) \n            if sc : \n                if createFiles : \n                    fp = os . path . join ( saveTo , fName ) \n                    files . append ( fp ) \n                    with open ( fp , fileMode ) as f : \n                        if fileMode == 'a' : \n                            f . write ( \"\\n\" ) \n                        f . write ( serializer . formatter ( sc ) ) \n                else : \n                    codeBuff . append ( sc ) \n        else : \n            if not createFiles : \n                try : \n                    name = '\"%s\"' % obj . name \n                except AttributeError : \n                    name = \"\" \n                codeBuff . append ( serializer . comment ( \"Object of class %s, %s was not serialized as specified\" % ( obj . __class__ . __name__ , name ) ) ) \n    if createFiles : \n        return files \n    else : \n        return serializer . formatter ( \"\\n\" . join ( codeBuff ) ) "}
{"6405": "\ndef sig ( self , name , dtype = BIT , clk = None , syncRst = None , defVal = None ) : \n    if isinstance ( defVal , RtlSignal ) : \n        assert defVal . _const , \"Initial value of register has to be constant\" \n        _defVal = defVal . _auto_cast ( dtype ) \n    else : \n        if isinstance ( defVal , Value ) : \n            _defVal = defVal . _auto_cast ( dtype ) \n        else : \n            if isinstance ( defVal , InterfaceBase ) : \n                _defVal = defVal . _sig \n            else : \n                _defVal = dtype . fromPy ( defVal ) \n    if clk is not None : \n        s = RtlSyncSignal ( self , name , dtype , _defVal ) \n        if syncRst is not None and defVal is None : \n            raise SigLvlConfErr ( \"Probably forgotten default value on sync signal %s\" , name ) \n        if syncRst is not None : \n            r = If ( syncRst . _isOn ( ) , RtlSignal . __call__ ( s , _defVal ) ) . Else ( RtlSignal . __call__ ( s , s . next ) ) \n        else : \n            r = [ RtlSignal . __call__ ( s , s . next ) ] \n        If ( clk . _onRisingEdge ( ) , r ) \n    else : \n        if syncRst : \n            raise SigLvlConfErr ( \"Signal %s has reset but has no clk\" % name ) \n        s = RtlSignal ( self , name , dtype , defVal = _defVal ) \n    self . signals . add ( s ) \n    return s "}
{"6407": "\ndef getMaxStmIdForStm ( stm ) : \n    maxId = 0 \n    if isinstance ( stm , Assignment ) : \n        return stm . _instId \n    else : \n        if isinstance ( stm , WaitStm ) : \n            return maxId \n        else : \n            for _stm in stm . _iter_stms ( ) : \n                maxId = max ( maxId , getMaxStmIdForStm ( _stm ) ) \n            return maxId "}
{"6415": "\ndef sensitivityByOp ( op ) : \n    if op == AllOps . RISING_EDGE : \n        return SENSITIVITY . RISING \n    else : \n        if op == AllOps . FALLING_EDGE : \n            return SENSITIVITY . FALLING \n        else : \n            raise TypeError ( ) "}
{"6416": "\ndef eval ( self , operator , simulator = None ) : \n    def getVal ( v ) : \n        while not isinstance ( v , Value ) : \n            v = v . _val \n        return v \n    operands = list ( map ( getVal , operator . operands ) ) \n    if isEventDependentOp ( operator . operator ) : \n        operands . append ( simulator . now ) \n    else : \n        if operator . operator == AllOps . IntToBits : \n            operands . append ( operator . result . _dtype ) \n    return self . _evalFn ( * operands ) "}
{"6417": "\ndef convertBits ( self , sigOrVal , toType ) : \n    if isinstance ( sigOrVal , Value ) : \n        return convertBits__val ( self , sigOrVal , toType ) \n    else : \n        if isinstance ( toType , HBool ) : \n            if self . bit_length ( ) == 1 : \n                v = 0 if sigOrVal . _dtype . negated else 1 \n                return sigOrVal . _eq ( self . getValueCls ( ) . fromPy ( v , self ) ) \n        else : \n            if isinstance ( toType , Bits ) : \n                if self . bit_length ( ) == toType . bit_length ( ) : \n                    return sigOrVal . _convSign ( toType . signed ) \n            else : \n                if toType == INT : \n                    return Operator . withRes ( AllOps . BitsToInt , [ sigOrVal ] , toType ) \n    return default_auto_cast_fn ( self , sigOrVal , toType ) "}
{"6420": "\ndef groupByWordIndex ( self , transaction : 'TransTmpl' , offset : int ) : \n    actualW = None \n    partsInWord = [ ] \n    wordWidth = self . wordWidth \n    for item in self . splitOnWords ( transaction , offset ) : \n        _actualW = item . startOfPart // wordWidth \n        if actualW is None : \n            actualW = _actualW \n            partsInWord . append ( item ) \n        else : \n            if _actualW > actualW : \n                yield ( actualW , partsInWord ) \n                actualW = _actualW \n                partsInWord = [ item , ] \n            else : \n                partsInWord . append ( item ) \n    if partsInWord : \n        yield ( actualW , partsInWord ) "}
{"6431": "\ndef _is_mergable_statement_list ( cls , stmsA , stmsB ) : \n    if stmsA is None and stmsB is None : \n        return True \n    else : \n        if stmsA is None or stmsB is None : \n            return False \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = _get_stm_with_branches ( a_it ) \n    b = _get_stm_with_branches ( b_it ) \n    while a is not None or b is not None : \n        if a is None or b is None or not a . _is_mergable ( b ) : \n            return False \n        a = _get_stm_with_branches ( a_it ) \n        b = _get_stm_with_branches ( b_it ) \n    return True "}
{"6433": "\ndef _merge_statement_lists ( stmsA : List [ \"HdlStatement\" ] , stmsB : List [ \"HdlStatement\" ] ) -> List [ \"HdlStatement\" ] : \n    if stmsA is None and stmsB is None : \n        return None \n    tmp = [ ] \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = None \n    b = None \n    a_empty = False \n    b_empty = False \n    while not a_empty and not b_empty : \n        while not a_empty : \n            a = next ( a_it , None ) \n            if a is None : \n                a_empty = True \n                break \n            else : \n                if a . rank == 0 : \n                    tmp . append ( a ) \n                    a = None \n                else : \n                    break \n        while not b_empty : \n            b = next ( b_it , None ) \n            if b is None : \n                b_empty = True \n                break \n            else : \n                if b . rank == 0 : \n                    tmp . append ( b ) \n                    b = None \n                else : \n                    break \n        if a is not None or b is not None : \n            a . _merge_with_other_stm ( b ) \n            tmp . append ( a ) \n            a = None \n            b = None \n    return tmp "}
{"6442": "\ndef walkFlattenFields ( sigOrVal , skipPadding = True ) : \n    t = sigOrVal . _dtype \n    if isinstance ( t , Bits ) : \n        yield sigOrVal \n    else : \n        if isinstance ( t , HUnion ) : \n            yield from walkFlattenFields ( sigOrVal . _val , skipPadding = skipPadding ) \n        else : \n            if isinstance ( t , HStruct ) : \n                for f in t . fields : \n                    isPadding = f . name is None \n                    if not isPadding or not skipPadding : \n                        if isPadding : \n                            v = f . dtype . fromPy ( None ) \n                        else : \n                            v = getattr ( sigOrVal , f . name ) \n                        yield from walkFlattenFields ( v ) \n            else : \n                if isinstance ( t , HArray ) : \n                    for item in sigOrVal : \n                        yield from walkFlattenFields ( item ) \n                else : \n                    raise NotImplementedError ( t ) "}
{"6444": "\ndef _convSign ( self , signed ) : \n    if isinstance ( self , Value ) : \n        return self . _convSign__val ( signed ) \n    else : \n        if self . _dtype . signed == signed : \n            return self \n        t = copy ( self . _dtype ) \n        t . signed = signed \n        if signed is None : \n            cnv = AllOps . BitsAsVec \n        else : \n            if signed : \n                cnv = AllOps . BitsAsSigned \n            else : \n                cnv = AllOps . BitsAsUnsigned \n        return Operator . withRes ( cnv , [ self ] , t ) "}
{"6445": "\ndef sensitivity ( proc : HWProcess , * sensitiveTo ) : \n    for s in sensitiveTo : \n        if isinstance ( s , tuple ) : \n            sen , s = s \n            if sen == SENSITIVITY . ANY : \n                s . simSensProcs . add ( proc ) \n            else : \n                if sen == SENSITIVITY . RISING : \n                    s . simRisingSensProcs . add ( proc ) \n                else : \n                    if sen == SENSITIVITY . FALLING : \n                        s . simFallingSensProcs . add ( proc ) \n                    else : \n                        raise AssertionError ( sen ) \n        else : \n            s . simSensProcs . add ( proc ) "}
{"6451": "\ndef HWProcess ( cls , proc : HWProcess , ctx : ResourceContext ) -> None : \n    seen = ctx . seen \n    for stm in proc . statements : \n        encl = stm . _enclosed_for \n        full_ev_dep = stm . _is_completly_event_dependent \n        now_ev_dep = stm . _now_is_event_dependent \n        ev_dep = full_ev_dep or now_ev_dep \n        out_mux_dim = count_mux_inputs_for_outputs ( stm ) \n        for o in stm . _outputs : \n            if o in seen : \n                continue \n            i = out_mux_dim [ o ] \n            if isinstance ( o . _dtype , HArray ) : \n                assert i == 1 , ( o , i , \" only one ram port per HWProcess\" ) \n                for a in walk_assignments ( stm , o ) : \n                    assert len ( a . indexes ) == 1 , \"one address per RAM port\" \n                    addr = a . indexes [ 0 ] \n                ctx . registerRAM_write_port ( o , addr , ev_dep ) \n            else : \n                if ev_dep : \n                    ctx . registerFF ( o ) \n                    if i > 1 : \n                        ctx . registerMUX ( stm , o , i ) \n                else : \n                    if o not in encl : \n                        ctx . registerLatch ( o ) \n                        if i > 1 : \n                            ctx . registerMUX ( stm , o , i ) \n                    else : \n                        if i > 1 : \n                            ctx . registerMUX ( stm , o , i ) \n                        else : \n                            continue \n        if isinstance ( stm , SwitchContainer ) : \n            caseEqs = set ( [ stm . switchOn . _eq ( c [ 0 ] ) for c in stm . cases ] ) \n            inputs = chain ( [ sig for sig in stm . _inputs if sig not in caseEqs ] , [ stm . switchOn ] ) \n        else : \n            inputs = stm . _inputs \n        for i in inputs : \n            if not i . hidden or i in seen : \n                continue \n            cls . HWProcess_operators ( i , ctx , ev_dep ) "}
{"6466": "\ndef tryReduceAnd ( sig , val ) : \n    m = sig . _dtype . all_mask ( ) \n    if val . _isFullVld ( ) : \n        v = val . val \n        if v == m : \n            return sig \n        else : \n            if v == 0 : \n                return val "}
{"6467": "\ndef tryReduceXor ( sig , val ) : \n    m = sig . _dtype . all_mask ( ) \n    if not val . vldMask : \n        return val \n    if val . _isFullVld ( ) : \n        v = val . val \n        if v == m : \n            return ~ sig \n        else : \n            if v == 0 : \n                return sig "}
{"6469": "\ndef serializationDecision ( cls , obj , serializedClasses , serializedConfiguredUnits ) : \n    isDeclaration = isinstance ( obj , Entity ) \n    isDefinition = isinstance ( obj , Architecture ) \n    if isDeclaration : \n        unit = obj . origin \n    else : \n        if isDefinition : \n            unit = obj . entity . origin \n        else : \n            return True \n    assert isinstance ( unit , Unit ) \n    sd = unit . _serializeDecision \n    if sd is None : \n        return True \n    else : \n        prevPriv = serializedClasses . get ( unit . __class__ , None ) \n        seriazlize , nextPriv = sd ( unit , obj , isDeclaration , prevPriv ) \n        serializedClasses [ unit . __class__ ] = nextPriv \n        return seriazlize "}
{"6470": "\ndef HdlType ( cls , typ : HdlType , ctx : SerializerCtx , declaration = False ) : \n    if isinstance ( typ , Bits ) : \n        sFn = cls . HdlType_bits \n    else : \n        if isinstance ( typ , HEnum ) : \n            sFn = cls . HdlType_enum \n        else : \n            if isinstance ( typ , HArray ) : \n                sFn = cls . HdlType_array \n            else : \n                if isinstance ( typ , Integer ) : \n                    sFn = cls . HdlType_int \n                else : \n                    if isinstance ( typ , HBool ) : \n                        sFn = cls . HdlType_bool \n                    else : \n                        raise NotImplementedError ( \"type declaration is not implemented\" \" for type %s\" % ( typ . name ) ) \n    return sFn ( typ , ctx , declaration = declaration ) "}
{"6478": "\ndef _loadFromHType ( self , dtype : HdlType , bitAddr : int ) -> None : \n    self . bitAddr = bitAddr \n    childrenAreChoice = False \n    if isinstance ( dtype , Bits ) : \n        ld = self . _loadFromBits \n    else : \n        if isinstance ( dtype , HStruct ) : \n            ld = self . _loadFromHStruct \n        else : \n            if isinstance ( dtype , HArray ) : \n                ld = self . _loadFromArray \n            else : \n                if isinstance ( dtype , HStream ) : \n                    ld = self . _loadFromHStream \n                else : \n                    if isinstance ( dtype , HUnion ) : \n                        ld = self . _loadFromUnion \n                        childrenAreChoice = True \n                    else : \n                        raise TypeError ( \"expected instance of HdlType\" , dtype ) \n    self . bitAddrEnd = ld ( dtype , bitAddr ) \n    self . childrenAreChoice = childrenAreChoice "}
{"6480": "\ndef walkFlatten ( self , offset : int = 0 , shouldEnterFn = _default_shouldEnterFn , otherObjItCtx : ObjIteratorCtx = _DummyIteratorCtx ( ) ) -> Generator [ Union [ Tuple [ Tuple [ int , int ] , 'TransTmpl' ] , 'OneOfTransaction' ] , None , None ] : \n    t = self . dtype \n    base = self . bitAddr + offset \n    end = self . bitAddrEnd + offset \n    shouldEnter , shouldYield = shouldEnterFn ( self ) \n    if shouldYield : \n        yield ( ( base , end ) , self ) \n    if shouldEnter : \n        if isinstance ( t , Bits ) : \n            pass \n        else : \n            if isinstance ( t , HStruct ) : \n                for ch in self . children : \n                    with otherObjItCtx ( ch . origin . name ) : \n                        yield from ch . walkFlatten ( offset , shouldEnterFn , otherObjItCtx ) \n            else : \n                if isinstance ( t , HArray ) : \n                    itemSize = ( self . bitAddrEnd - self . bitAddr ) // self . itemCnt \n                    for i in range ( self . itemCnt ) : \n                        with otherObjItCtx ( i ) : \n                            yield from self . children . walkFlatten ( base + i * itemSize , shouldEnterFn , otherObjItCtx ) \n                else : \n                    if isinstance ( t , HUnion ) : \n                        yield OneOfTransaction ( self , offset , shouldEnterFn , self . children ) \n                    else : \n                        if isinstance ( t , HStream ) : \n                            assert len ( self . children ) == 1 \n                            yield StreamTransaction ( self , offset , shouldEnterFn , self . children [ 0 ] ) \n                        else : \n                            raise TypeError ( t ) "}
{"6490": "\ndef singleDriver ( self ) : \n    drv_cnt = len ( self . drivers ) \n    if not drv_cnt : \n        raise NoDriverErr ( self ) \n    else : \n        if drv_cnt != 1 : \n            raise MultipleDriversErr ( self ) \n    return self . drivers [ 0 ] "}
{"6502": "\ndef _serializeOnce_eval ( parentUnit , obj , isDeclaration , priv ) : \n    clsName = parentUnit . __class__ . __name__ \n    if isDeclaration : \n        obj . name = clsName \n    if priv is None : \n        priv = parentUnit \n    else : \n        if isDeclaration : \n            prepareEntity ( obj , clsName , parentUnit ) \n    serialize = priv is parentUnit \n    return serialize , priv "}
{"6511": "\ndef connectSig ( self , signal ) : \n    if self . direction == DIRECTION . IN : \n        if self . src is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %r\" % ( self . name , self . src ) ) \n        self . src = signal \n        signal . endpoints . append ( self ) \n    else : \n        if self . direction == DIRECTION . OUT : \n            if self . dst is not None : \n                raise HwtSyntaxError ( \"Port %s is already associated with %r\" % ( self . name , self . dst ) ) \n            self . dst = signal \n            signal . drivers . append ( self ) \n        else : \n            raise NotImplementedError ( self ) \n    signal . hidden = False \n    signal . ctx . subUnits . add ( self . unit ) "}
{"6512": "\ndef registerInternSig ( self , signal ) : \n    if self . direction == DIRECTION . OUT : \n        if self . src is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %s\" % ( self . name , str ( self . src ) ) ) \n        self . src = signal \n    else : \n        if self . direction == DIRECTION . IN : \n            if self . dst is not None : \n                raise HwtSyntaxError ( \"Port %s is already associated with %s\" % ( self . name , str ( self . dst ) ) ) \n            self . dst = signal \n        else : \n            raise NotImplementedError ( self . direction ) "}
{"6513": "\ndef connectInternSig ( self ) : \n    d = self . direction \n    if d == DIRECTION . OUT : \n        self . src . endpoints . append ( self ) \n    else : \n        if d == DIRECTION . IN or d == DIRECTION . INOUT : \n            self . dst . drivers . append ( self ) \n        else : \n            raise NotImplementedError ( d ) "}
{"6514": "\ndef getInternSig ( self ) : \n    d = self . direction \n    if d == DIRECTION . IN : \n        return self . dst \n    else : \n        if d == DIRECTION . OUT : \n            return self . src \n        else : \n            raise NotImplementedError ( d ) "}
{"6525": "\ndef write ( self , val , sig : SimSignal ) -> None : \n    try : \n        simSensProcs = sig . simSensProcs \n    except AttributeError : \n        sig = sig . _sigInside \n        simSensProcs = sig . simSensProcs \n    t = sig . _dtype \n    if isinstance ( val , Value ) : \n        v = val . clone ( ) \n        v = v . _auto_cast ( t ) \n    else : \n        v = t . fromPy ( val ) \n    sig . simUpdateVal ( self , lambda curentV : ( valueHasChanged ( curentV , v ) , v ) ) \n    if not self . _applyValPlaned : \n        if not ( simSensProcs or sig . simRisingSensProcs or sig . simFallingSensProcs ) : \n            self . _scheduleApplyValues ( ) \n        else : \n            if ( sig . _writeCallbacks or sig . _writeCallbacksToEn ) : \n                self . _scheduleApplyValues ( ) "}
{"6534": "\ndef setup_platform ( hass , config , add_entities , discovery_info = None ) : \n    host = config . get ( CONF_HOST ) \n    token = config . get ( CONF_ACCESS_TOKEN ) \n    name = config . get ( CONF_NAME ) \n    volume_step = config . get ( CONF_VOLUME_STEP ) \n    device_type = config . get ( CONF_DEVICE_CLASS ) \n    device = VizioDevice ( host , token , name , volume_step , device_type ) \n    if device . validate_setup ( ) is False : \n        _LOGGER . error ( \"Failed to set up Vizio platform, \" \"please check if host and API key are correct\" ) \n        return \n    else : \n        if ( token is None or token == \"\" ) and device_type == \"tv\" : \n            _LOGGER . error ( \"Failed to set up Vizio platform, \" \"if device_class is 'tv' then an auth_token needs \" \"to be provided, otherwise if device_class is \" \"'soundbar' then add the right device_class to config\" ) \n            return \n    if config . get ( CONF_SUPPRESS_WARNING ) : \n        from requests . packages import urllib3 \n        _LOGGER . warning ( \"InsecureRequestWarning is disabled \" \"because of Vizio platform configuration\" ) \n        urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) \n    add_entities ( [ device ] , True ) "}
{"6539": "\ndef set_volume_level ( self , volume ) : \n    if self . _volume_level is not None : \n        if volume > self . _volume_level : \n            num = int ( self . _max_volume * ( volume - self . _volume_level ) ) \n            self . _volume_level = volume \n            self . _device . vol_up ( num = num ) \n        else : \n            if volume < self . _volume_level : \n                num = int ( self . _max_volume * ( self . _volume_level - volume ) ) \n                self . _volume_level = volume \n                self . _device . vol_down ( num = num ) "}
{"6556": "\ndef from_usi ( cls , usi ) : \n    if usi == '0000' : \n        return cls . null ( ) \n    else : \n        if len ( usi ) == 4 : \n            if usi [ 1 ] == '*' : \n                piece = Piece . from_symbol ( usi [ 0 ] ) \n                return cls ( None , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , False , piece . piece_type ) \n            else : \n                return cls ( SQUARE_NAMES . index ( usi [ 0 : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) ) \n        else : \n            if len ( usi ) == 5 and usi [ 4 ] == '+' : \n                return cls ( SQUARE_NAMES . index ( usi [ 0 : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , True ) \n            else : \n                raise ValueError ( 'expected usi string to be of length 4 or 5' ) "}
{"6571": "\ndef _get_rate ( self , currency , date ) : \n    if currency == self . ref_currency : \n        return 1.0 \n    if date not in self . _rates [ currency ] : \n        first_date , last_date = self . bounds [ currency ] \n        if not self . fallback_on_wrong_date : \n            raise RateNotFoundError ( '{0} not in {1} bounds {2}/{3}' . format ( date , currency , first_date , last_date ) ) \n        if date < first_date : \n            fallback_date = first_date \n        else : \n            if date > last_date : \n                fallback_date = last_date \n            else : \n                raise AssertionError ( 'Should never happen, bug in the code!' ) \n        if self . verbose : \n            print ( r'/!\\ {0} not in {1} bounds {2}/{3}, falling back to {4}' . format ( date , currency , first_date , last_date , fallback_date ) ) \n        date = fallback_date \n    rate = self . _rates [ currency ] [ date ] \n    if rate is None : \n        raise RateNotFoundError ( '{0} has no rate for {1}' . format ( currency , date ) ) \n    return rate "}
{"6601": "\ndef configure ( module = None , prefix = 'MONGODB_' , ** kwargs ) : \n    if module is not None and isinstance ( module , types . ModuleType ) : \n        attrs = ( ( attr . replace ( prefix , '' ) . lower ( ) , value ) for attr , value in vars ( module ) . items ( ) if attr . startswith ( prefix ) ) \n        _Options . _configure ( ** dict ( attrs ) ) \n    else : \n        if kwargs : \n            _Options . _configure ( ** kwargs ) "}
{"6647": "\ndef has_equal_ast ( state , incorrect_msg = None , code = None , exact = True , append = None ) : \n    if utils . v2_only ( ) : \n        state . assert_is_not ( [ \"object_assignments\" ] , \"has_equal_ast\" , [ \"check_object\" ] ) \n        state . assert_is_not ( [ \"function_calls\" ] , \"has_equal_ast\" , [ \"check_function\" ] ) \n    if code and incorrect_msg is None : \n        raise InstructorError ( \"If you manually specify the code to match inside has_equal_ast(), \" \"you have to explicitly set the `incorrect_msg` argument.\" ) \n    if ( append is None ) : \n        append = incorrect_msg is None \n    if incorrect_msg is None : \n        incorrect_msg = \"Expected `{{sol_str}}`, but got `{{stu_str}}`.\" \n    def parse_tree ( tree ) : \n        crnt = ( tree . body [ 0 ] if isinstance ( tree , ast . Module ) and len ( tree . body ) == 1 else tree ) \n        return ast . dump ( crnt . value if isinstance ( crnt , ast . Expr ) else crnt ) \n    stu_rep = parse_tree ( state . student_ast ) \n    sol_rep = parse_tree ( state . solution_ast if not code else ast . parse ( code ) ) \n    fmt_kwargs = { \"sol_str\" : state . solution_code if not code else code , \"stu_str\" : state . student_code , } \n    _msg = state . build_message ( incorrect_msg , fmt_kwargs , append = append ) \n    if exact and not code : \n        state . do_test ( EqualTest ( stu_rep , sol_rep , Feedback ( _msg , state ) ) ) \n    else : \n        if not sol_rep in stu_rep : \n            state . report ( Feedback ( _msg , state ) ) \n    return state "}
{"6673": "\ndef authenticate ( self , username , password ) : \n    if self . config . get ( 'LDAP_BIND_DIRECT_CREDENTIALS' ) : \n        result = self . authenticate_direct_credentials ( username , password ) \n    else : \n        if not self . config . get ( 'LDAP_ALWAYS_SEARCH_BIND' ) and self . config . get ( 'LDAP_USER_RDN_ATTR' ) == self . config . get ( 'LDAP_USER_LOGIN_ATTR' ) : \n            result = self . authenticate_direct_bind ( username , password ) \n        else : \n            result = self . authenticate_search_bind ( username , password ) \n    return result "}
{"6685": "\ndef label_search ( self , key = None , value = None ) : \n    if key is not None : \n        key = key . lower ( ) \n    if value is not None : \n        value = value . lower ( ) \n    show_details = True \n    if key is None and value is None : \n        url = '%s/labels/search' % ( self . base ) \n        show_details = False \n    else : \n        if key is not None and value is not None : \n            url = '%s/labels/search/%s/key/%s/value' % ( self . base , key , value ) \n        else : \n            if key is None : \n                url = '%s/labels/search/%s/value' % ( self . base , value ) \n            else : \n                url = '%s/labels/search/%s/key' % ( self . base , key ) \n    result = self . _get ( url ) \n    if len ( result ) == 0 : \n        bot . info ( \"No labels found.\" ) \n        sys . exit ( 0 ) \n    bot . info ( \"Labels\\n\" ) \n    rows = [ ] \n    for l in result : \n        if show_details is True : \n            entry = [ \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) , \"\\n%s\\n\\n\" % \"\\n\" . join ( l [ 'containers' ] ) ] \n        else : \n            entry = [ \"N=%s\" % len ( l [ 'containers' ] ) , \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) ] \n        rows . append ( entry ) \n    bot . table ( rows ) \n    return rows "}
{"6692": "\ndef require_secrets ( self , params = None ) : \n    name = self . client_name \n    has_secrets = True \n    if not hasattr ( self , 'secrets' ) : \n        has_secrets = False \n    else : \n        if hasattr ( self , 'secrets' ) : \n            if self . secrets is None : \n                has_secrets = False \n        else : \n            if self . client_name not in self . secrets : \n                has_secrets = False \n    if has_secrets is False : \n        message = '%s requires client secrets.' % name \n        bot . error ( message ) \n        sys . exit ( 1 ) \n    if params is not None : \n        if not isinstance ( params , list ) : \n            params = [ params ] \n        for param in params : \n            if param not in self . secrets [ name ] : \n                has_secrets = False \n            else : \n                if self . secrets [ name ] [ param ] in [ None , '' ] : \n                    has_secrets = False \n        if has_secrets is False : \n            message = 'Missing %s in client secrets.' % param \n            bot . error ( message ) \n            sys . exit ( 1 ) "}
{"6694": "\ndef stream ( url , headers , stream_to = None , retry = True ) : \n    bot . debug ( \"GET %s\" % url ) \n    if DISABLE_SSL_CHECK is True : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    response = requests . get ( url , headers = headers , verify = not DISABLE_SSL_CHECK , stream = True ) \n    if response . status_code in [ 401 , 403 ] : \n        headers = update_token ( headers ) \n        return stream ( url , headers , stream_to , retry = False ) \n    else : \n        if response . status_code == 200 : \n            content_size = None \n            if 'Content-Length' in response . headers : \n                progress = 0 \n                content_size = int ( response . headers [ 'Content-Length' ] ) \n                bot . show_progress ( progress , content_size , length = 35 ) \n            chunk_size = 1 << 20 \n            with open ( stream_to , 'wb' ) as filey : \n                for chunk in response . iter_content ( chunk_size = chunk_size ) : \n                    filey . write ( chunk ) \n                    if content_size is not None : \n                        progress += chunk_size \n                        bot . show_progress ( iteration = progress , total = content_size , length = 35 , carriage_return = False ) \n            sys . stdout . write ( '\\n' ) \n            return stream_to \n    bot . error ( \"Problem with stream, response %s\" % ( response . status_code ) ) \n    sys . exit ( 1 ) "}
{"6722": "\ndef get_reqs ( lookup = None , key = 'INSTALL_REQUIRES' ) : \n    if lookup == None : \n        lookup = get_lookup ( ) \n    install_requires = [ ] \n    for module in lookup [ key ] : \n        module_name = module [ 0 ] \n        module_meta = module [ 1 ] \n        if \"exact_version\" in module_meta : \n            dependency = \"%s==%s\" % ( module_name , module_meta [ 'exact_version' ] ) \n        else : \n            if \"min_version\" in module_meta : \n                if module_meta [ 'min_version' ] == None : \n                    dependency = module_name \n                else : \n                    dependency = \"%s>=%s\" % ( module_name , module_meta [ 'min_version' ] ) \n        install_requires . append ( dependency ) \n    return install_requires "}
{"6734": "\ndef _update_secrets ( self ) : \n    self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] = self . _required_get_and_update ( 'SREGISTRY_SWIFT_AUTHTYPE' ) \n    if self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] == 'preauth' : \n        for envar in [ 'SREGISTRY_SWIFT_OS_AUTH_TOKEN' , 'SREGISTRY_SWIFT_OS_STORAGE_URL' ] : \n            self . config [ envar ] = self . _required_get_and_update ( envar ) \n        self . conn = swiftclient . Connection ( preauthurl = self . config [ 'SREGISTRY_SWIFT_OS_STORAGE_URL' ] , preauthtoken = self . config [ 'SREGISTRY_SWIFT_OS_AUTH_TOKEN' ] ) \n    else : \n        if self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] == 'keystonev3' : \n            for envar in [ 'SREGISTRY_SWIFT_USER' , 'SREGISTRY_SWIFT_TOKEN' , 'SREGISTRY_SWIFT_URL' ] : \n                self . config [ envar ] = self . _required_get_and_update ( envar ) \n            auth_url = '%s/v3' % self . config [ 'SREGISTRY_SWIFT_URL' ] \n            _os_options = { 'user_domain_name' : 'Default' , 'project_domain_name' : 'Default' , 'project_name' : 'Default' } \n            self . conn = swiftclient . Connection ( user = self . config [ 'SREGISTRY_SWIFT_USER' ] , key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , os_options = _os_options , authurl = auth_url , auth_version = '3' ) \n        else : \n            if self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] == 'keystonev2' : \n                for envar in [ 'SREGISTRY_SWIFT_USER' , 'SREGISTRY_SWIFT_TOKEN' , 'SREGISTRY_SWIFT_TENANT' , 'SREGISTRY_SWIFT_REGION' , 'SREGISTRY_SWIFT_URL' ] : \n                    self . config [ envar ] = self . _required_get_and_update ( envar ) \n                auth_url = '%s/v2.0/' % self . config [ 'SREGISTRY_SWIFT_URL' ] \n                _os_options = { 'tenant_name' : self . config [ 'SREGISTRY_SWIFT_TENANT' ] , 'region_name' : self . config [ 'SREGISTRY_SWIFT_REGION' ] } \n                self . conn = swiftclient . Connection ( user = self . config [ 'SREGISTRY_SWIFT_USER' ] , key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , os_options = _os_options , authurl = auth_url , auth_version = '2' ) \n            else : \n                for envar in [ 'SREGISTRY_SWIFT_USER' , 'SREGISTRY_SWIFT_TOKEN' , 'SREGISTRY_SWIFT_URL' ] : \n                    self . config [ envar ] = self . _required_get_and_update ( envar ) \n                auth_url = '%s/auth/' % self . config [ 'SREGISTRY_SWIFT_URL' ] \n                self . conn = swiftclient . Connection ( user = self . config [ 'SREGISTRY_SWIFT_USER' ] , key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , authurl = auth_url , ) "}
{"6736": "\ndef get_client ( image = None , quiet = False , ** kwargs ) : \n    from sregistry . defaults import SREGISTRY_CLIENT \n    if not check_install ( ) : \n        bot . warning ( 'Singularity is not installed, function might be limited.' ) \n    client_name = get_uri ( image ) \n    if client_name is not None : \n        SREGISTRY_CLIENT = client_name \n    if SREGISTRY_CLIENT == 'aws' : \n        from . aws import Client \n    else : \n        if SREGISTRY_CLIENT == 'docker' : \n            from . docker import Client \n        else : \n            if SREGISTRY_CLIENT == 'dropbox' : \n                from . dropbox import Client \n            else : \n                if SREGISTRY_CLIENT == 'gitlab' : \n                    from . gitlab import Client \n                else : \n                    if SREGISTRY_CLIENT == 'globus' : \n                        from . globus import Client \n                    else : \n                        if SREGISTRY_CLIENT == 'nvidia' : \n                            from . nvidia import Client \n                        else : \n                            if SREGISTRY_CLIENT == 'hub' : \n                                from . hub import Client \n                            else : \n                                if SREGISTRY_CLIENT == 'google-drive' : \n                                    from . google_drive import Client \n                                else : \n                                    if SREGISTRY_CLIENT == 'google-compute' : \n                                        from . google_storage import Client \n                                    else : \n                                        if SREGISTRY_CLIENT == 'google-storage' : \n                                            from . google_storage import Client \n                                        else : \n                                            if SREGISTRY_CLIENT == 'google-build' : \n                                                from . google_build import Client \n                                            else : \n                                                if SREGISTRY_CLIENT == 'registry' : \n                                                    from . registry import Client \n                                                else : \n                                                    if SREGISTRY_CLIENT == 's3' : \n                                                        from . s3 import Client \n                                                    else : \n                                                        if SREGISTRY_CLIENT == 'swift' : \n                                                            from . swift import Client \n                                                        else : \n                                                            from . hub import Client \n    Client . client_name = SREGISTRY_CLIENT \n    Client . quiet = quiet \n    Client . _credential_cache = get_credential_cache ( ) \n    if SREGISTRY_DATABASE is not None : \n        from sregistry . database import ( init_db , add , cp , get , mv , rm , rmi , images , inspect , rename , get_container , get_collection , get_or_create_collection ) \n        Client . _init_db = init_db \n        Client . add = add \n        Client . cp = cp \n        Client . get = get \n        Client . inspect = inspect \n        Client . mv = mv \n        Client . rename = rename \n        Client . rm = rm \n        Client . rmi = rmi \n        Client . images = images \n        Client . get_or_create_collection = get_or_create_collection \n        Client . get_container = get_container \n        Client . get_collection = get_collection \n    else : \n        from sregistry . database import ( add , init_db ) \n        Client . add = add \n        Client . _init_db = init_db \n    cli = Client ( ) \n    if hasattr ( Client , '_init_db' ) : \n        cli . _init_db ( SREGISTRY_DATABASE ) \n    return cli "}
{"6804": "\ndef s3errors ( path ) : \n    try : \n        yield \n    except ClientError as error : \n        _error = error . response . get ( \"Error\" , { } ) \n        error_code = _error . get ( \"Code\" , None ) \n        response_meta = error . response . get ( \"ResponseMetadata\" , { } ) \n        http_status = response_meta . get ( \"HTTPStatusCode\" , 200 ) \n        error_msg = _error . get ( \"Message\" , None ) \n        if error_code == \"NoSuchBucket\" : \n            raise errors . ResourceError ( path , exc = error , msg = error_msg ) \n        if http_status == 404 : \n            raise errors . ResourceNotFound ( path ) \n        else : \n            if http_status == 403 : \n                raise errors . PermissionDenied ( path = path , msg = error_msg ) \n            else : \n                raise errors . OperationFailed ( path = path , exc = error ) \n    except SSLError as error : \n        raise errors . OperationFailed ( path , exc = error ) \n    except EndpointConnectionError as error : \n        raise errors . RemoteConnectionError ( path , exc = error , msg = \"{}\" . format ( error ) ) "}
{"6855": "\ndef from_gitlab ( klass , repository , labor_hours = True ) : \n    if not isinstance ( repository , gitlab . v4 . objects . Project ) : \n        raise TypeError ( 'Repository must be a gitlab Repository object' ) \n    project = klass ( ) \n    logger . debug ( 'GitLab: repository_id=%d path_with_namespace=%s' , repository . id , repository . path_with_namespace , ) \n    project [ 'name' ] = repository . name \n    project [ 'repositoryURL' ] = repository . http_url_to_repo \n    project [ 'description' ] = repository . description \n    project [ 'permissions' ] [ 'licenses' ] = None \n    web_url = repository . web_url \n    public_server = web_url . startswith ( 'https://gitlab.com' ) \n    if repository . visibility in ( 'public' ) and public_server : \n        project [ 'permissions' ] [ 'usageType' ] = 'openSource' \n    else : \n        if date_parse ( repository . created_at ) < POLICY_START_DATE : \n            project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' \n    if labor_hours : \n        project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) \n    else : \n        project [ 'laborHours' ] = 0 \n    project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list \n    project [ 'contact' ] = { 'email' : '' , 'URL' : web_url , } \n    project [ 'organization' ] = repository . namespace [ 'name' ] \n    project [ 'status' ] = 'Development' \n    project [ 'vcs' ] = 'git' \n    project [ 'homepageURL' ] = repository . web_url \n    api_url = repository . manager . gitlab . _url \n    archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) \n    project [ 'downloadURL' ] = api_url + archive_suffix \n    project [ 'date' ] = { 'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 'metadataLastUpdated' : '' , } \n    _prune_dict_null_str ( project ) \n    return project "}
{"6856": "\ndef from_doecode ( klass , record ) : \n    if not isinstance ( record , dict ) : \n        raise TypeError ( '`record` must be a dict' ) \n    project = klass ( ) \n    project [ 'name' ] = record [ 'software_title' ] \n    logger . debug ( 'DOE CODE: software_title=\"%s\"' , record [ 'software_title' ] ) \n    link = record . get ( 'repository_link' , '' ) \n    if not link : \n        link = record . get ( 'landing_page' ) \n        logger . warning ( 'DOE CODE: No repositoryURL, using landing_page: %s' , link ) \n    project [ 'repositoryURL' ] = link \n    project [ 'description' ] = record [ 'description' ] \n    licenses = set ( record [ 'licenses' ] ) \n    licenses . discard ( None ) \n    logger . debug ( 'DOE CODE: licenses=%s' , licenses ) \n    license_objects = [ ] \n    if 'Other' in licenses : \n        licenses . remove ( 'Other' ) \n        license_objects = [ { 'name' : 'Other' , 'URL' : record [ 'proprietary_url' ] } ] \n    if licenses : \n        license_objects . extend ( [ _license_obj ( license ) for license in licenses ] ) \n    project [ 'permissions' ] [ 'licenses' ] = license_objects \n    if record [ 'open_source' ] : \n        usage_type = 'openSource' \n    else : \n        usage_type = 'exemptByLaw' \n        project [ 'permissions' ] [ 'exemptionText' ] = 'This source code is restricted by patent and / or intellectual property law.' \n    project [ 'permissions' ] [ 'usageType' ] = usage_type \n    project [ 'laborHours' ] = 0 \n    project [ 'tags' ] = [ 'DOE CODE' ] \n    lab_name = record . get ( 'lab_display_name' ) \n    if lab_name is not None : \n        project [ 'tags' ] . append ( lab_name ) \n    project [ 'contact' ] [ 'email' ] = record [ 'owner' ] \n    if 'version_number' in record and record [ 'version_number' ] : \n        project [ 'version' ] = record [ 'version_number' ] \n    if lab_name is not None : \n        project [ 'organization' ] = lab_name \n    status = record . get ( 'ever_announced' ) \n    if status is None : \n        raise ValueError ( 'DOE CODE: Unable to determine \"ever_announced\" value!' ) \n    else : \n        if status : \n            status = 'Production' \n        else : \n            status = 'Development' \n    project [ 'status' ] = status \n    vcs = None \n    link = project [ 'repositoryURL' ] \n    if 'github.com' in link : \n        vcs = 'git' \n    if vcs is None : \n        logger . debug ( 'DOE CODE: Unable to determine vcs for: name=\"%s\", repositoryURL=%s' , project [ 'name' ] , link ) \n        vcs = '' \n    if vcs : \n        project [ 'vcs' ] = vcs \n    url = record . get ( 'landing_page' , '' ) \n    if url : \n        project [ 'homepageURL' ] = url \n    if 'programming_languages' in record : \n        project [ 'languages' ] = record [ 'programming_languages' ] \n    if 'date_record_added' in record and 'date_record_updated' in record : \n        project [ 'date' ] = { 'created' : record [ 'date_record_added' ] , 'metadataLastUpdated' : record [ 'date_record_updated' ] } \n    return project "}
{"6857": "\ndef _license_obj ( license ) : \n    obj = None \n    if license in ( 'MIT' , 'MIT License' ) : \n        obj = { 'URL' : 'https://api.github.com/licenses/mit' , 'name' : 'MIT' } \n    else : \n        if license in ( 'BSD 2-clause \"Simplified\" License' ) : \n            obj = { 'URL' : 'https://api.github.com/licenses/bsd-2-clause' , 'name' : 'BSD-2-Clause' } \n        else : \n            if license in ( 'BSD 3-clause \"New\" or \"Revised\" License' ) : \n                obj = { 'URL' : 'https://api.github.com/licenses/bsd-3-clause' , 'name' : 'BSD-3-Clause' } \n            else : \n                if license in ( 'Apache License 2.0' ) : \n                    obj = { 'URL' : 'https://api.github.com/licenses/apache-2.0' , 'name' : 'Apache-2.0' } \n                else : \n                    if license in ( 'GNU General Public License v2.1' ) : \n                        obj = { 'URL' : 'https://api.github.com/licenses/gpl-2.1' , 'name' : 'GPL-2.1' } \n                    else : \n                        if license in ( 'GNU General Public License v2.0' ) : \n                            obj = { 'URL' : 'https://api.github.com/licenses/gpl-2.0' , 'name' : 'GPL-2.0' } \n                        else : \n                            if license in ( 'GNU Lesser General Public License v2.1' ) : \n                                obj = { 'URL' : 'https://api.github.com/licenses/lgpl-2.1' , 'name' : 'LGPL-2.1' } \n                            else : \n                                if license in ( 'GNU General Public License v3.0' ) : \n                                    obj = { 'URL' : 'https://api.github.com/licenses/gpl-3.0' , 'name' : 'GPL-3.0' } \n                                else : \n                                    if license in ( 'GNU Lesser General Public License v3.0' ) : \n                                        obj = { 'URL' : 'https://api.github.com/licenses/lgpl-3.0' , 'name' : 'LGPL-3.0' } \n                                    else : \n                                        if license in ( 'Eclipse Public License 1.0' ) : \n                                            obj = { 'URL' : 'https://api.github.com/licenses/epl-1.0' , 'name' : 'EPL-1.0' , } \n                                        else : \n                                            if license in ( 'Mozilla Public License 2.0' ) : \n                                                obj = { 'URL' : 'https://api.github.com/licenses/mpl-2.0' , 'name' : 'MPL-2.0' , } \n                                            else : \n                                                if license in ( 'The Unlicense' ) : \n                                                    obj = { 'URL' : 'https://api.github.com/licenses/unlicense' , 'name' : 'Unlicense' , } \n                                                else : \n                                                    if license in ( 'GNU Affero General Public License v3.0' ) : \n                                                        obj = { 'URL' : 'https://api.github.com/licenses/agpl-3.0' , 'name' : 'AGPL-3.0' , } \n                                                    else : \n                                                        if license in ( 'Eclipse Public License 2.0' ) : \n                                                            obj = { 'URL' : 'https://api.github.com/licenses/epl-2.0' , 'name' : 'EPL-2.0' , } \n    if obj is None : \n        logger . warn ( 'I dont understand the license: %s' , license ) \n        raise ValueError ( 'Aborting!' ) \n    return obj "}
{"6861": "\ndef get_data ( self , url = '' , headers = { } , date = str ( datetime . date . today ( ) ) , dict_to_store = { } , type = '' , repo_name = '' ) : \n    url = ( url + '/traffic/' + type ) \n    r3 = requests . get ( url , headers = headers ) \n    json = r3 . json ( ) \n    if type == 'views' : \n        self . views_json [ repo_name ] = json \n    else : \n        if type == 'clones' : \n            self . clones_json [ repo_name ] = json \n    for day in json [ type ] : \n        timestamp_seconds = day [ 'timestamp' ] / 1000 \n        try : \n            date_timestamp = datetime . datetime . utcfromtimestamp ( timestamp_seconds ) . strftime ( '%Y-%m-%d' ) \n            if date_timestamp != date : \n                tuple_in = ( day [ 'count' ] , day [ 'uniques' ] ) \n                tuple = ( dict_to_store [ timestamp_seconds ] [ 0 ] + tuple_in [ 0 ] , dict_to_store [ timestamp_seconds ] [ 1 ] + tuple_in [ 1 ] ) \n                dict_to_store [ timestamp_seconds ] = tuple \n        except KeyError : \n            tuple = dict_to_store [ timestamp_seconds ] = ( day [ 'count' ] , day [ 'uniques' ] ) "}
{"6869": "\ndef process ( filename = None , url = None , key = None ) : \n    if filename is not None : \n        yield from process_json ( filename ) \n    else : \n        if url and key : \n            yield from process_url ( url , key ) "}
{"6971": "\ndef update_attributes ( self , attr_dict ) : \n    valid_directives = [ \"pid\" , \"ignore_type\" , \"ignore_pid\" , \"extra_input\" , \"group\" , \"input_type\" ] \n    for attribute , val in attr_dict . items ( ) : \n        if attribute in valid_directives and hasattr ( self , attribute ) : \n            setattr ( self , attribute , val ) \n        else : \n            if attribute == \"params\" : \n                for name , value in val . items ( ) : \n                    if name in self . params : \n                        self . params [ name ] [ \"default\" ] = value \n                    else : \n                        raise eh . ProcessError ( \"The parameter name '{}' does not exist for \" \"component '{}'\" . format ( name , self . template ) ) \n            else : \n                for p in self . directives : \n                    self . directives [ p ] [ attribute ] = val "}
{"6972": "\ndef set_compiler_channels ( self , channel_list , operator = \"mix\" ) : \n    if not channel_list : \n        raise eh . ProcessError ( \"At least one status channel must be \" \"provided to include this process in the \" \"pipeline\" ) \n    if len ( channel_list ) == 1 : \n        logger . debug ( \"Setting only one status channel: {}\" . format ( channel_list [ 0 ] ) ) \n        self . _context = { \"compile_channels\" : channel_list [ 0 ] } \n    else : \n        first_status = channel_list [ 0 ] \n        if operator == \"mix\" : \n            lst = \",\" . join ( channel_list [ 1 : ] ) \n            s = \"{}.mix({})\" . format ( first_status , lst ) \n        else : \n            if operator == \"join\" : \n                s = first_status \n                for ch in channel_list [ 1 : ] : \n                    s += \".join({})\" . format ( ch ) \n                s += \".map{ ot -> [ ot[0], ot[1..-1] ] }\" \n        logger . debug ( \"Status channel string: {}\" . format ( s ) ) \n        self . _context = { \"compile_channels\" : s } "}
{"6984": "\ndef inner_fork_insanity_checks ( pipeline_string ) : \n    list_of_forks = [ ] \n    left_indexes = [ ] \n    for pos , char in enumerate ( pipeline_string ) : \n        if char == FORK_TOKEN : \n            left_indexes . append ( pos ) \n        else : \n            if char == CLOSE_TOKEN and len ( left_indexes ) > 0 : \n                list_of_forks . append ( pipeline_string [ left_indexes [ - 1 ] + 1 : pos ] ) \n                left_indexes = left_indexes [ : - 1 ] \n    list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = True ) \n    for fork in list_of_forks : \n        for subfork in list_of_forks : \n            if subfork in list_of_forks and subfork != fork : \n                fork_simplified = fork . replace ( \"({})\" . format ( subfork ) , \"\" ) \n            else : \n                fork_simplified = fork \n        if not len ( fork_simplified . split ( LANE_TOKEN ) ) > 1 : \n            raise SanityError ( \"One of the forks doesn't have '|' \" \"separator between the processes to fork. This is\" \" the prime suspect: '({})'\" . format ( fork ) ) "}
{"6995": "\ndef _hms ( s ) : \n    if s == \"-\" : \n        return 0 \n    if s . endswith ( \"ms\" ) : \n        return float ( s . rstrip ( \"ms\" ) ) / 1000 \n    fields = list ( map ( float , re . split ( \"[dhms]\" , s ) [ : - 1 ] ) ) \n    if len ( fields ) == 4 : \n        return fields [ 0 ] * 24 * 3600 + fields [ 1 ] * 3600 + fields [ 2 ] * 60 + fields [ 3 ] \n    if len ( fields ) == 3 : \n        return fields [ 0 ] * 3600 + fields [ 1 ] * 60 + fields [ 2 ] \n    else : \n        if len ( fields ) == 2 : \n            return fields [ 0 ] * 60 + fields [ 1 ] \n        else : \n            return fields [ 0 ] "}
{"6996": "\ndef _size_coverter ( s ) : \n    if s . upper ( ) . endswith ( \"KB\" ) : \n        return float ( s . rstrip ( \"KB\" ) ) / 1024 \n    else : \n        if s . upper ( ) . endswith ( \" B\" ) : \n            return float ( s . rstrip ( \"B\" ) ) / 1024 / 1024 \n        else : \n            if s . upper ( ) . endswith ( \"MB\" ) : \n                return float ( s . rstrip ( \"MB\" ) ) \n            else : \n                if s . upper ( ) . endswith ( \"GB\" ) : \n                    return float ( s . rstrip ( \"GB\" ) ) * 1024 \n                else : \n                    if s . upper ( ) . endswith ( \"TB\" ) : \n                        return float ( s . rstrip ( \"TB\" ) ) * 1024 * 1024 \n                    else : \n                        return float ( s ) "}
{"7003": "\ndef log_parser ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . log_retry = 0 \n    if size_stamp and size_stamp == self . log_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    r = \".* (.*) \\[.*\\].*\\[(.*)\\].*process > (.*) \\((.*)\\).*\" \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if \"Submitted process >\" in line or \"Re-submitted process >\" in line or \"Cached process >\" in line : \n                m = re . match ( r , line ) \n                if not m : \n                    continue \n                time_start = m . group ( 1 ) \n                workdir = m . group ( 2 ) \n                process = m . group ( 3 ) \n                tag = m . group ( 4 ) \n                if time_start + tag not in self . stored_log_ids : \n                    self . stored_log_ids . append ( time_start + tag ) \n                else : \n                    continue \n                if process not in self . processes : \n                    continue \n                p = self . processes [ process ] \n                if tag in list ( p [ \"finished\" ] ) + list ( p [ \"retry\" ] ) : \n                    continue \n                if tag in list ( p [ \"failed\" ] ) and \"Re-submitted process >\" in line : \n                    p [ \"retry\" ] . add ( tag ) \n                    self . send = True \n                    continue \n                p [ \"barrier\" ] = \"R\" \n                if tag not in p [ \"submitted\" ] : \n                    p [ \"submitted\" ] . add ( tag ) \n                    if tag not in self . process_tags [ process ] : \n                        self . process_tags [ process ] [ tag ] = { \"workdir\" : self . _expand_path ( workdir ) , \"start\" : time_start } \n                        self . send = True \n                    else : \n                        if not self . process_tags [ process ] [ tag ] [ \"start\" ] : \n                            self . process_tags [ process ] [ tag ] [ \"start\" ] = time_start \n                            self . send = True \n    self . _update_pipeline_status ( ) "}
{"7006": "\ndef _updown ( self , direction ) : \n    if direction == \"up\" and self . top_line != 0 : \n        self . top_line -= 1 \n    else : \n        if direction == \"down\" and self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3 : \n            self . top_line += 1 "}
{"7021": "\ndef define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : \n    tasks_array = tasks . split ( ) \n    for task_unsplit in tasks_array : \n        task = task_unsplit . split ( \"=\" ) [ 0 ] \n        if task not in process_descriptions . keys ( ) : \n            logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) \n            sys . exit ( ) \n        else : \n            process_split = task_unsplit . split ( \"=\" ) \n            if len ( process_split ) > 1 : \n                self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] \n        if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            task_pipeline = [ ] \n            if task in process_descriptions : \n                if check_upstream : \n                    task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n                task_pipeline . append ( task ) \n                if check_downstream : \n                    task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n            forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) \n        else : \n            if bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n                for fork in forks : \n                    if task not in fork : \n                        try : \n                            dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) \n                            fork . insert ( dependent_index , task ) \n                        except ValueError : \n                            continue \n    for i in range ( 0 , len ( forks ) ) : \n        for j in range ( 0 , len ( forks [ i ] ) ) : \n            try : \n                if len ( forks [ i ] [ j ] . split ( \"|\" ) ) > 1 : \n                    forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) \n                    tmp_fork = [ ] \n                    for s in forks [ i ] [ j ] : \n                        if s in total_tasks : \n                            tmp_fork . append ( s ) \n                    forks [ i ] [ j ] = tmp_fork \n            except AttributeError as e : \n                continue \n    return forks "}
{"7030": "\ndef iter_filter ( self , filters , databases = None , fields = None , filter_behavior = \"and\" ) : \n    if filter_behavior not in [ \"and\" , \"or\" ] : \n        raise ValueError ( \"Filter behavior must be either 'and' or 'or'\" ) \n    for dic in self . storage . values ( ) : \n        _pass = False \n        flag = [ ] \n        if databases : \n            if dic [ \"database\" ] not in databases : \n                continue \n        for f in filters : \n            val = dic [ f [ 0 ] ] \n            if not self . _test_truth ( val , f [ 1 ] , f [ 2 ] ) : \n                flag . append ( False ) \n            else : \n                flag . append ( True ) \n        if filter_behavior == \"and\" : \n            if all ( flag ) : \n                _pass = True \n        else : \n            if filter_behavior == \"or\" : \n                if any ( flag ) : \n                    _pass = True \n        if _pass : \n            if fields : \n                yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) \n            else : \n                yield dic "}
{"7043": "\ndef trim_range ( data_file ) : \n    logger . debug ( \"Starting trim range assessment\" ) \n    target_nuc_bias = \">>Per base sequence content\" \n    logger . debug ( \"Target string to start nucleotide bias assessment set to \" \"{}\" . format ( target_nuc_bias ) ) \n    gather = False \n    biased = [ ] \n    with open ( data_file ) as fh : \n        for line in fh : \n            if line . startswith ( target_nuc_bias ) : \n                logger . debug ( \"Found target string at line: {}\" . format ( line ) ) \n                next ( fh ) \n                gather = True \n            else : \n                if line . startswith ( \">>END_MODULE\" ) and gather : \n                    logger . debug ( \"Stopping parsing at line: {}\" . format ( line ) ) \n                    break \n                else : \n                    if gather : \n                        g , a , t , c = [ float ( x ) for x in line . strip ( ) . split ( ) [ 1 : ] ] \n                        gc = ( g + 0.1 ) / ( c + 0.1 ) \n                        at = ( a + 0.1 ) / ( t + 0.1 ) \n                        if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2 : \n                            biased . append ( False ) \n                        else : \n                            biased . append ( True ) \n    logger . debug ( \"Finished bias assessment with result: {}\" . format ( biased ) ) \n    biased_5end , biased_3end = biased [ : int ( len ( biased ) / 2 ) ] , biased [ int ( len ( biased ) / 2 ) : ] [ : : - 1 ] \n    logger . debug ( \"Getting optimal trim range from biased list\" ) \n    trim_nt = [ 0 , 0 ] \n    trim_nt [ 0 ] = get_trim_index ( biased_5end ) \n    logger . debug ( \"Optimal trim range at 5' end set to: {}\" . format ( trim_nt [ 0 ] ) ) \n    trim_nt [ 1 ] = len ( biased ) - get_trim_index ( biased_3end ) \n    logger . debug ( \"Optimal trim range at 3' end set to: {}\" . format ( trim_nt [ 1 ] ) ) \n    return trim_nt "}
{"7070": "\ndef set_kmers ( kmer_opt , max_read_len ) : \n    logger . debug ( \"Kmer option set to: {}\" . format ( kmer_opt ) ) \n    if kmer_opt == \"auto\" : \n        if max_read_len >= 175 : \n            kmers = [ 55 , 77 , 99 , 113 , 127 ] \n        else : \n            kmers = [ 21 , 33 , 55 , 67 , 77 ] \n        logger . debug ( \"Kmer range automatically selected based on max read\" \"length of {}: {}\" . format ( max_read_len , kmers ) ) \n    else : \n        if len ( kmer_opt . split ( ) ) > 1 : \n            kmers = kmer_opt . split ( ) \n            logger . debug ( \"Kmer range manually set to: {}\" . format ( kmers ) ) \n        else : \n            kmers = [ ] \n            logger . debug ( \"Kmer range set to empty (will be automatically \" \"determined by SPAdes\" ) \n    return kmers "}
{"7085": "\ndef procs_dict_parser ( procs_dict ) : \n    logger . info ( colored_print ( \"\\n===== L I S T   O F   P R O C E S S E S =====\\n\" , \"green_bold\" ) ) \n    procs_dict_ordered = { k : procs_dict [ k ] for k in sorted ( procs_dict ) } \n    for template , dict_proc_info in procs_dict_ordered . items ( ) : \n        template_str = \"=> {}\" . format ( template ) \n        logger . info ( colored_print ( template_str , \"blue_bold\" ) ) \n        for info in dict_proc_info : \n            info_str = \"{}:\" . format ( info ) \n            if isinstance ( dict_proc_info [ info ] , list ) : \n                if not dict_proc_info [ info ] : \n                    arg_msg = \"None\" \n                else : \n                    arg_msg = \", \" . join ( dict_proc_info [ info ] ) \n            else : \n                if info == \"directives\" : \n                    if not dict_proc_info [ info ] : \n                        arg_msg = \"None\" \n                    else : \n                        list_msg = [ \"\\n      {}: {}\" . format ( templt , \" , \" . join ( [ \"{}: {}\" . format ( dr , val ) for dr , val in drs . items ( ) ] ) ) for templt , drs in dict_proc_info [ info ] . items ( ) ] \n                        arg_msg = \"\" . join ( list_msg ) \n                else : \n                    arg_msg = dict_proc_info [ info ] \n            logger . info ( \"   {} {}\" . format ( colored_print ( info_str , \"white_underline\" ) , arg_msg ) ) "}
{"7091": "\ndef filter_assembly ( assembly_file , minimum_coverage , coverage_info , output_file ) : \n    write_flag = False \n    with open ( assembly_file ) as fh , open ( output_file , \"w\" ) as out_fh : \n        for line in fh : \n            if line . startswith ( \">\" ) : \n                write_flag = False \n                header = line . strip ( ) [ 1 : ] \n                contig_cov = coverage_info [ header ] [ \"cov\" ] \n                if contig_cov >= minimum_coverage : \n                    write_flag = True \n                    out_fh . write ( line ) \n            else : \n                if write_flag : \n                    out_fh . write ( line ) "}
{"7105": "\ndef gaussian_filter ( X , M = 8 , axis = 0 ) : \n    for i in range ( X . shape [ axis ] ) : \n        if axis == 1 : \n            X [ : , i ] = filters . gaussian_filter ( X [ : , i ] , sigma = M / 2. ) \n        else : \n            if axis == 0 : \n                X [ i , : ] = filters . gaussian_filter ( X [ i , : ] , sigma = M / 2. ) \n    return X "}
{"7137": "\ndef frame_times ( self ) : \n    frame_times = None \n    self . features \n    if self . feat_type is FeatureTypes . framesync : \n        self . _compute_framesync_times ( ) \n        frame_times = self . _framesync_times \n    else : \n        if self . feat_type is FeatureTypes . est_beatsync : \n            frame_times = self . _est_beatsync_times \n        else : \n            if self . feat_type is FeatureTypes . ann_beatsync : \n                frame_times = self . _ann_beatsync_times \n    return frame_times "}
{"7138": "\ndef features ( self ) : \n    if self . _features is None : \n        try : \n            self . read_features ( ) \n        except ( NoFeaturesFileError , FeaturesNotFound , WrongFeaturesFormatError , FeatureParamsError ) as e : \n            try : \n                self . _compute_all_features ( ) \n                self . write_features ( ) \n            except IOError : \n                if isinstance ( e , FeaturesNotFound ) or isinstance ( e , FeatureParamsError ) : \n                    msg = \"Computation of the features is needed for \" \"current parameters but no audio file was found.\" \"Please, change your parameters or add the audio\" \" file in %s\" \n                else : \n                    msg = \"Couldn't find audio file in %s\" \n                raise NoAudioFileError ( msg % self . file_struct . audio_file ) \n    if self . feat_type is FeatureTypes . framesync : \n        self . _features = self . _framesync_features \n    else : \n        if self . feat_type is FeatureTypes . est_beatsync : \n            self . _features = self . _est_beatsync_features \n        else : \n            if self . feat_type is FeatureTypes . ann_beatsync : \n                if self . _ann_beatsync_features is None : \n                    raise FeatureTypeNotFound ( \"Feature type %s is not valid because no annotated beats \" \"were found\" % self . feat_type ) \n                self . _features = self . _ann_beatsync_features \n            else : \n                raise FeatureTypeNotFound ( \"Feature type %s is not valid.\" % self . feat_type ) \n    return self . _features "}
{"7139": "\ndef select_features ( cls , features_id , file_struct , annot_beats , framesync ) : \n    if not annot_beats and framesync : \n        feat_type = FeatureTypes . framesync \n    else : \n        if annot_beats and not framesync : \n            feat_type = FeatureTypes . ann_beatsync \n        else : \n            if not annot_beats and not framesync : \n                feat_type = FeatureTypes . est_beatsync \n            else : \n                raise FeatureTypeNotFound ( \"Type of features not valid.\" ) \n    if features_id not in features_registry . keys ( ) : \n        raise FeaturesNotFound ( \"The features '%s' are invalid (valid features are %s)\" % ( features_id , features_registry . keys ( ) ) ) \n    return features_registry [ features_id ] ( file_struct , feat_type ) "}
{"7199": "\ndef plot ( self , data , bbox = None , plot_type = 'scatter' , fig_kwargs = None , bmap_kwargs = None , plot_kwargs = None , cbar_kwargs = None ) : \n    from mpl_toolkits . basemap import Basemap \n    fig_kwargs = fig_kwargs or { } \n    bmap_kwargs = bmap_kwargs or { } \n    plot_kwargs = plot_kwargs or { } \n    cbar_kwargs = cbar_kwargs or { } \n    if not bbox : \n        bbox = ( self . nodes_df . y . min ( ) , self . nodes_df . x . min ( ) , self . nodes_df . y . max ( ) , self . nodes_df . x . max ( ) ) \n    fig , ax = plt . subplots ( ** fig_kwargs ) \n    bmap = Basemap ( bbox [ 1 ] , bbox [ 0 ] , bbox [ 3 ] , bbox [ 2 ] , ax = ax , ** bmap_kwargs ) \n    bmap . drawcoastlines ( ) \n    bmap . drawmapboundary ( ) \n    x , y = bmap ( self . nodes_df . x . values , self . nodes_df . y . values ) \n    if plot_type == 'scatter' : \n        plot = bmap . scatter ( x , y , c = data . values , ** plot_kwargs ) \n    else : \n        if plot_type == 'hexbin' : \n            plot = bmap . hexbin ( x , y , C = data . values , ** plot_kwargs ) \n    bmap . colorbar ( plot , ** cbar_kwargs ) \n    return bmap , fig , ax "}
{"7259": "\ndef propagate_averages ( self , n , tv , bv , var , outgroup = False ) : \n    if n . is_terminal ( ) and outgroup == False : \n        if tv is None or np . isinf ( tv ) or np . isnan ( tv ) : \n            res = np . array ( [ 0 , 0 , 0 , 0 , 0 , 0 ] ) \n        else : \n            if var == 0 : \n                res = np . array ( [ np . inf , np . inf , np . inf , np . inf , np . inf , np . inf ] ) \n            else : \n                res = np . array ( [ tv / var , bv / var , tv ** 2 / var , bv * tv / var , bv ** 2 / var , 1.0 / var ] , dtype = float ) \n    else : \n        tmpQ = n . O if outgroup else n . Q \n        denom = 1.0 / ( 1 + var * tmpQ [ sii ] ) \n        res = np . array ( [ tmpQ [ tavgii ] * denom , ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ tsqii ] - var * tmpQ [ tavgii ] ** 2 * denom , tmpQ [ dtavgii ] + tmpQ [ tavgii ] * bv - var * tmpQ [ tavgii ] * ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ dsqii ] + 2 * bv * tmpQ [ davgii ] + bv ** 2 * tmpQ [ sii ] - var * ( tmpQ [ davgii ] ** 2 + 2 * bv * tmpQ [ davgii ] * tmpQ [ sii ] + bv ** 2 * tmpQ [ sii ] ** 2 ) * denom , tmpQ [ sii ] * denom ] ) \n    return res "}
{"7271": "\ndef set_gtr ( self , in_gtr , ** kwargs ) : \n    if isinstance ( in_gtr , str ) : \n        self . _gtr = GTR . standard ( model = in_gtr , ** kwargs ) \n        self . _gtr . logger = self . logger \n    else : \n        if isinstance ( in_gtr , GTR ) or isinstance ( in_gtr , GTR_site_specific ) : \n            self . _gtr = in_gtr \n            self . _gtr . logger = self . logger \n        else : \n            self . logger ( \"TreeAnc.gtr_setter: can't interpret GTR model\" , 1 , warn = True ) \n            raise TypeError ( \"Cannot set GTR model in TreeAnc class: GTR or \" \"string expected\" ) \n    if self . _gtr . ambiguous is None : \n        self . fill_overhangs = False "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = 0 \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        else : \n            if l . is_terminal ( ) : \n                self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , 0 , warn = True ) \n                failed_leaves += 1 \n                l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n                if failed_leaves > self . tree . count_terminals ( ) / 3 : \n                    self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , 0 , warn = True ) \n                    self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2 , warn = True ) \n                    break \n            else : \n                pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , 0 , warn = True ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7286": "\ndef optimize_branch_length ( self , mode = 'joint' , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\" % mode , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    store_old_dist = False \n    if 'store_old' in kwargs : \n        store_old_dist = kwargs [ 'store_old' ] \n    if mode == 'marginal' : \n        if not hasattr ( self . tree . root , \"marginal_profile\" ) : \n            self . infer_ancestral_sequences ( marginal = True ) \n    max_bl = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            continue \n        if store_old_dist : \n            node . _old_length = node . branch_length \n        if mode == 'marginal' : \n            new_len = self . optimal_marginal_branch_length ( node ) \n        else : \n            if mode == 'joint' : \n                new_len = self . optimal_branch_length ( node ) \n            else : \n                self . logger ( \"treeanc.optimize_branch_length: unsupported optimization mode\" , 4 , warn = True ) \n                new_len = node . branch_length \n        if new_len < 0 : \n            continue \n        self . logger ( \"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\" \" Updating branch length...\" % ( node . branch_length , new_len , len ( node . mutations ) * self . one_mutation ) , 5 ) \n        node . branch_length = new_len \n        node . mutation_length = new_len \n        max_bl = max ( max_bl , new_len ) \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    if max_bl > 0.15 and mode == 'joint' : \n        self . logger ( \"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\" \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\" \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\" \" \\n\\t ****branch_length_mode='input'\" , 0 , warn = True ) \n    self . _prepare_nodes ( ) \n    return ttconf . SUCCESS "}
{"7293": "\ndef standard ( model , ** kwargs ) : \n    from . nuc_models import JC69 , K80 , F81 , HKY85 , T92 , TN93 \n    from . aa_models import JTT92 \n    if model . lower ( ) in [ 'jc' , 'jc69' , 'jukes-cantor' , 'jukes-cantor69' , 'jukescantor' , 'jukescantor69' ] : \n        return JC69 ( ** kwargs ) \n    else : \n        if model . lower ( ) in [ 'k80' , 'kimura80' , 'kimura1980' ] : \n            return K80 ( ** kwargs ) \n        else : \n            if model . lower ( ) in [ 'f81' , 'felsenstein81' , 'felsenstein1981' ] : \n                return F81 ( ** kwargs ) \n            else : \n                if model . lower ( ) in [ 'hky' , 'hky85' , 'hky1985' ] : \n                    return HKY85 ( ** kwargs ) \n                else : \n                    if model . lower ( ) in [ 't92' , 'tamura92' , 'tamura1992' ] : \n                        return T92 ( ** kwargs ) \n                    else : \n                        if model . lower ( ) in [ 'tn93' , 'tamura_nei_93' , 'tamuranei93' ] : \n                            return TN93 ( ** kwargs ) \n                        else : \n                            if model . lower ( ) in [ 'jtt' , 'jtt92' ] : \n                                return JTT92 ( ** kwargs ) \n                            else : \n                                raise KeyError ( \"The GTR model '{}' is not in the list of available models.\" \"\" . format ( model ) ) "}
{"7301": "\ndef _set_branch_length_mode ( self , branch_length_mode ) : \n    if branch_length_mode in [ 'joint' , 'marginal' , 'input' ] : \n        self . branch_length_mode = branch_length_mode \n    else : \n        if self . aln : \n            bl_dis = [ n . branch_length for n in self . tree . find_clades ( ) if n . up ] \n            max_bl = np . max ( bl_dis ) \n            if max_bl > 0.1 : \n                bl_mode = 'input' \n            else : \n                bl_mode = 'joint' \n            self . logger ( \"TreeTime._set_branch_length_mode: maximum branch length is %1.3e, using branch length mode %s\" % ( max_bl , bl_mode ) , 1 ) \n            self . branch_length_mode = bl_mode \n        else : \n            self . branch_length_mode = 'input' "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2 : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    else : \n                        if keyval [ 0 ] not in [ 'alphabet' ] : \n                            keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = False \n        except : \n            print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = False \n    return gtr "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    else : \n        if isinstance ( distribution , Distribution ) : \n            xvals = distribution . _func . x \n            log_prob = distribution . _func . y \n        else : \n            raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( log_prob < 0.693147 ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if L < 2 : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if n_delta > 1 : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    else : \n        if n_delta == 1 : \n            delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n            delta_dist = dists [ delta_dist_ii ] \n            new_xpos = delta_dist . peak_pos \n            new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n            res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n        else : \n            new_xmin = np . max ( [ k . xmin for k in dists ] ) \n            new_xmax = np . min ( [ k . xmax for k in dists ] ) \n            x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n            x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] \n            y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n            peak = y_vals . min ( ) \n            ind = ( y_vals - peak ) < BIG_NUMBER / 1000 \n            n_points = ind . sum ( ) \n            if n_points == 0 : \n                print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n                x_vals = [ 0 , 1 ] \n                y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n                res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n            else : \n                if n_points == 1 : \n                    res = Distribution . delta_function ( x_vals [ 0 ] ) \n                else : \n                    res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7321": "\ndef get_max_posterior_region ( self , node , fraction = 0.9 ) : \n    if node . marginal_inverse_cdf == \"delta\" : \n        return np . array ( [ node . numdate , node . numdate ] ) \n    min_max = ( node . marginal_pos_LH . xmin , node . marginal_pos_LH . xmax ) \n    min_date , max_date = [ self . date2dist . to_numdate ( x ) for x in min_max ] [ : : - 1 ] \n    if node . marginal_pos_LH . peak_pos == min_max [ 0 ] : \n        return self . get_confidence_interval ( node , ( 0 , fraction ) ) \n    else : \n        if node . marginal_pos_LH . peak_pos == min_max [ 1 ] : \n            return self . get_confidence_interval ( node , ( 1.0 - fraction , 1.0 ) ) \n        else : \n            rate_contribution = self . date_uncertainty_due_to_rate ( node , ( ( 1 - fraction ) * 0.5 , 1.0 - ( 1.0 - fraction ) * 0.5 ) ) \n            from scipy . interpolate import interp1d \n            from scipy . optimize import minimize_scalar as minimize \n            pidx = np . argmin ( node . marginal_pos_LH . y ) \n            pval = np . min ( node . marginal_pos_LH . y ) \n            left = interp1d ( node . marginal_pos_LH . y [ : ( pidx + 1 ) ] - pval , node . marginal_pos_LH . x [ : ( pidx + 1 ) ] , kind = 'linear' , fill_value = min_max [ 0 ] , bounds_error = False ) \n            right = interp1d ( node . marginal_pos_LH . y [ pidx : ] - pval , node . marginal_pos_LH . x [ pidx : ] , kind = 'linear' , fill_value = min_max [ 1 ] , bounds_error = False ) \n            def func ( x , thres ) : \n                interval = np . array ( [ left ( x ) , right ( x ) ] ) . squeeze ( ) \n                return ( thres - np . diff ( node . marginal_cdf ( np . array ( interval ) ) ) ) ** 2 \n            sol = minimize ( func , bracket = [ 0 , 10 ] , args = ( fraction , ) ) \n            if sol [ 'success' ] : \n                mutation_contribution = self . date2dist . to_numdate ( np . array ( [ right ( sol [ 'x' ] ) , left ( sol [ 'x' ] ) ] ) . squeeze ( ) ) \n            else : \n                mutation_contribution = None \n            return self . combine_confidence ( node . numdate , ( min_date , max_date ) , c1 = rate_contribution , c2 = mutation_contribution ) "}
{"7349": "\ndef _process_filters ( cls , filters ) : \n    data = [ ] \n    for f in filters : \n        if isinstance ( f , Filter ) : \n            if f . filters : \n                data . extend ( cls . _process_filters ( f . filters ) ) \n        else : \n            if isinstance ( f , dict ) : \n                key = list ( f . keys ( ) ) [ 0 ] \n                val = f [ key ] \n                if isinstance ( val , dict ) : \n                    filter_filters = cls . _process_filters ( [ val ] ) \n                    if len ( filter_filters ) == 1 : \n                        filter_filters = filter_filters [ 0 ] \n                    data . append ( { key : filter_filters } ) \n                else : \n                    data . append ( { key : cls . _process_filters ( val ) } ) \n            else : \n                data . extend ( ( f , ) ) \n    return data "}
{"7364": "\ndef _format ( val , valtype , floatfmt , missingval = \"\" ) : \n    if val is None : \n        return missingval \n    if valtype in [ int , _binary_type , _text_type ] : \n        return \"{0}\" . format ( val ) \n    else : \n        if valtype is float : \n            return format ( float ( val ) , floatfmt ) \n        else : \n            return \"{0}\" . format ( val ) "}
{"7365": "\ndef _normalize_tabular_data ( tabular_data , headers , sort = True ) : \n    if hasattr ( tabular_data , \"keys\" ) and hasattr ( tabular_data , \"values\" ) : \n        if hasattr ( tabular_data . values , \"__call__\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            rows = list ( izip_longest ( * list ( tabular_data . values ( ) ) ) ) \n        else : \n            if hasattr ( tabular_data , \"index\" ) : \n                keys = list ( tabular_data . keys ( ) ) \n                vals = tabular_data . values \n                names = tabular_data . index \n                rows = [ [ v ] + list ( row ) for v , row in zip ( names , vals ) ] \n            else : \n                raise ValueError ( \"tabular data doesn't appear to be a dict \" \"or a DataFrame\" ) \n        if headers == \"keys\" : \n            headers = list ( map ( _text_type , keys ) ) \n    else : \n        rows = list ( tabular_data ) \n        if headers == \"keys\" and len ( rows ) > 0 : \n            headers = list ( map ( _text_type , list ( range ( len ( rows [ 0 ] ) ) ) ) ) \n    if headers == \"firstrow\" and len ( rows ) > 0 : \n        headers = list ( map ( _text_type , rows [ 0 ] ) ) \n        rows = rows [ 1 : ] \n    headers = list ( headers ) \n    rows = list ( map ( list , rows ) ) \n    if sort and len ( rows ) > 1 : \n        rows = sorted ( rows , key = lambda x : x [ 0 ] ) \n    if headers and len ( rows ) > 0 : \n        nhs = len ( headers ) \n        ncols = len ( rows [ 0 ] ) \n        if nhs < ncols : \n            headers = [ \"\" ] * ( ncols - nhs ) + headers \n    return rows , headers "}
{"7374": "\ndef validate_api_host_url ( url ) : \n    if not url : \n        raise SolveError ( 'No SolveBio API host is set' ) \n    parsed = urlparse ( url ) \n    if parsed . scheme not in [ 'http' , 'https' ] : \n        raise SolveError ( 'Invalid API host: %s. ' 'Missing url scheme (HTTP or HTTPS).' % url ) \n    else : \n        if not parsed . netloc : \n            raise SolveError ( 'Invalid API host: %s.' % url ) \n    return True "}
{"7375": "\ndef add ( self , * args ) : \n    def _is_url ( path ) : \n        p = urlparse ( path ) \n        return bool ( p . scheme ) \n    for path in args : \n        path = os . path . expanduser ( path ) \n        if _is_url ( path ) : \n            self . add_url ( path ) \n        else : \n            if os . path . isfile ( path ) : \n                self . add_file ( path ) \n            else : \n                if os . path . isdir ( path ) : \n                    for f in os . listdir ( path ) : \n                        self . add_file ( f ) \n                else : \n                    if glob . glob ( path ) : \n                        for f in glob . glob ( path ) : \n                            self . add_file ( f ) \n                    else : \n                        raise ValueError ( 'Path: \"{0}\" is not a valid format or does not exist. ' 'Manifest paths must be files, directories, or URLs.' . format ( path ) ) "}
{"7384": "\ndef read_default_config ( self ) : \n    if self . validate : \n        self . default_config = ConfigObj ( configspec = self . default_file , list_values = False , _inspec = True , encoding = 'utf8' ) \n        valid = self . default_config . validate ( Validator ( ) , copy = True , preserve_errors = True ) \n        if valid is not True : \n            for name , section in valid . items ( ) : \n                if section is True : \n                    continue \n                for key , value in section . items ( ) : \n                    if isinstance ( value , ValidateError ) : \n                        raise DefaultConfigValidationError ( 'section [{}], key \"{}\": {}' . format ( name , key , value ) ) \n    else : \n        if self . default_file : \n            self . default_config , _ = self . read_config_file ( self . default_file ) \n    self . update ( self . default_config ) "}
{"7400": "\ndef format_numbers ( data , headers , column_types = ( ) , integer_format = None , float_format = None , ** _ ) : \n    if ( integer_format is None and float_format is None ) or not column_types : \n        return iter ( data ) , headers \n    def _format_number ( field , column_type ) : \n        if integer_format and column_type is int and type ( field ) in int_types : \n            return format ( field , integer_format ) \n        else : \n            if float_format and column_type is float and type ( field ) in float_types : \n                return format ( field , float_format ) \n        return field \n    data = ( [ _format_number ( v , column_types [ i ] ) for i , v in enumerate ( row ) ] for row in data ) \n    return data , headers "}
{"7436": "\ndef matchmaker_request ( url , token , method , content_type = None , accept = None , data = None ) : \n    headers = Headers ( ) \n    headers = { 'X-Auth-Token' : token } \n    if content_type : \n        headers [ 'Content-Type' ] = content_type \n    if accept : \n        headers [ 'Accept' ] = accept \n    req_data = data or { 'timestamp' : datetime . datetime . now ( ) . timestamp ( ) } \n    json_response = None \n    try : \n        LOG . info ( 'Sending {} request to MME url {}. Data sent: {}' . format ( method , url , req_data ) ) \n        resp = requests . request ( method = method , url = url , headers = headers , data = json . dumps ( req_data ) ) \n        json_response = resp . json ( ) \n        LOG . info ( 'MME server response was:{}' . format ( json_response ) ) \n        if isinstance ( json_response , str ) : \n            json_response = { 'message' : json_response , } \n        else : \n            if isinstance ( json_response , list ) : \n                return json_response \n        json_response [ 'status_code' ] = resp . status_code \n    except Exception as err : \n        LOG . info ( 'An error occurred while sending HTTP request to server ({})' . format ( err ) ) \n        json_response = { 'message' : str ( err ) } \n    return json_response "}
{"7439": "\ndef get_sub_category ( alt_len , ref_len , category , svtype = None ) : \n    subcategory = '' \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        if ref_len == alt_len : \n            subcategory = 'snv' \n        else : \n            subcategory = 'indel' \n    else : \n        if category == 'sv' : \n            subcategory = svtype \n    return subcategory "}
{"7440": "\ndef get_length ( alt_len , ref_len , category , pos , end , svtype = None , svlen = None ) : \n    length = - 1 \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        if ref_len == alt_len : \n            length = alt_len \n        else : \n            length = abs ( ref_len - alt_len ) \n    else : \n        if category == 'sv' : \n            if svtype == 'bnd' : \n                length = int ( 10e10 ) \n            else : \n                if svlen : \n                    length = abs ( int ( svlen ) ) \n                else : \n                    if end : \n                        if end != pos : \n                            length = end - pos \n    return length "}
{"7441": "\ndef get_end ( pos , alt , category , snvend = None , svend = None , svlen = None ) : \n    end = pos \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        end = snvend \n    else : \n        if category == 'sv' : \n            end = svend \n            if svend == pos : \n                if svlen : \n                    end = pos + svlen \n            if ':' in alt : \n                match = BND_ALT_PATTERN . match ( alt ) \n                if match : \n                    end = int ( match . group ( 2 ) ) \n    return end "}
{"7454": "\ndef index ( ) : \n    accessible_institutes = current_user . institutes \n    if not 'admin' in current_user . roles : \n        accessible_institutes = current_user . institutes \n        if not accessible_institutes : \n            flash ( 'Not allowed to see information - please visit the dashboard later!' ) \n            return redirect ( url_for ( 'cases.dahboard_general.html' ) ) \n    LOG . debug ( 'User accessible institutes: {}' . format ( accessible_institutes ) ) \n    institutes = [ inst for inst in store . institutes ( accessible_institutes ) ] \n    institutes . insert ( 0 , { '_id' : None , 'display_name' : 'All institutes' } ) \n    institute_id = None \n    slice_query = None \n    panel = 1 \n    if request . method == 'POST' : \n        institute_id = request . form . get ( 'institute' ) \n        slice_query = request . form . get ( 'query' ) \n        panel = request . form . get ( 'pane_id' ) \n    else : \n        if request . method == 'GET' : \n            institute_id = request . args . get ( 'institute' ) \n            slice_query = request . args . get ( 'query' ) \n    if not institute_id : \n        institute_id = accessible_institutes [ 0 ] \n    else : \n        if ( not current_user . is_admin ) and ( slice_query and institute_id == 'None' ) : \n            institute_id = accessible_institutes [ 0 ] \n        else : \n            if ( not institute_id in accessible_institutes ) and not ( institute_id == 'None' ) : \n                institute_id = accessible_institutes [ 0 ] \n    LOG . info ( \"Fetch all cases with institute: %s\" , institute_id ) \n    data = get_dashboard_info ( store , institute_id , slice_query ) \n    data [ 'institutes' ] = institutes \n    data [ 'choice' ] = institute_id \n    total_cases = data [ 'total_cases' ] \n    LOG . info ( \"Found %s cases\" , total_cases ) \n    if total_cases == 0 : \n        flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute_id ) , 'info' ) \n    return render_template ( 'dashboard/dashboard_general.html' , institute = institute_id , query = slice_query , panel = panel , ** data ) "}
{"7465": "\ndef variant_case ( store , case_obj , variant_obj ) : \n    case_obj [ 'bam_files' ] = [ ] \n    case_obj [ 'mt_bams' ] = [ ] \n    case_obj [ 'bai_files' ] = [ ] \n    case_obj [ 'mt_bais' ] = [ ] \n    case_obj [ 'sample_names' ] = [ ] \n    for individual in case_obj [ 'individuals' ] : \n        bam_path = individual . get ( 'bam_file' ) \n        mt_bam = individual . get ( 'mt_bam' ) \n        case_obj [ 'sample_names' ] . append ( individual . get ( 'display_name' ) ) \n        if bam_path and os . path . exists ( bam_path ) : \n            case_obj [ 'bam_files' ] . append ( individual [ 'bam_file' ] ) \n            case_obj [ 'bai_files' ] . append ( find_bai_file ( individual [ 'bam_file' ] ) ) \n        if mt_bam and os . path . exists ( mt_bam ) : \n            case_obj [ 'mt_bams' ] . append ( individual [ 'mt_bam' ] ) \n            case_obj [ 'mt_bais' ] . append ( find_bai_file ( individual [ 'mt_bam' ] ) ) \n        else : \n            LOG . debug ( \"%s: no bam file found\" , individual [ 'individual_id' ] ) \n    try : \n        genes = variant_obj . get ( 'genes' , [ ] ) \n        if len ( genes ) == 1 : \n            hgnc_gene_obj = store . hgnc_gene ( variant_obj [ 'genes' ] [ 0 ] [ 'hgnc_id' ] ) \n            if hgnc_gene_obj : \n                vcf_path = store . get_region_vcf ( case_obj , gene_obj = hgnc_gene_obj ) \n                case_obj [ 'region_vcf_file' ] = vcf_path \n            else : \n                case_obj [ 'region_vcf_file' ] = None \n        else : \n            if len ( genes ) > 1 : \n                chrom = variant_obj [ 'genes' ] [ 0 ] [ 'common' ] [ 'chromosome' ] \n                start = min ( gene [ 'common' ] [ 'start' ] for gene in variant_obj [ 'genes' ] ) \n                end = max ( gene [ 'common' ] [ 'end' ] for gene in variant_obj [ 'genes' ] ) \n                vcf_path = store . get_region_vcf ( case_obj , chrom = chrom , start = start , end = end ) \n                case_obj [ 'region_vcf_file' ] = vcf_path \n    except ( SyntaxError , Exception ) : \n        LOG . warning ( \"skip VCF region for alignment view\" ) "}
{"7469": "\ndef transcript_str ( transcript_obj , gene_name = None ) : \n    if transcript_obj . get ( 'exon' ) : \n        gene_part , part_count_raw = 'exon' , transcript_obj [ 'exon' ] \n    else : \n        if transcript_obj . get ( 'intron' ) : \n            gene_part , part_count_raw = 'intron' , transcript_obj [ 'intron' ] \n        else : \n            gene_part , part_count_raw = 'intergenic' , '0' \n    part_count = part_count_raw . rpartition ( '/' ) [ 0 ] \n    change_str = \"{}:{}{}:{}:{}\" . format ( transcript_obj . get ( 'refseq_id' , '' ) , gene_part , part_count , transcript_obj . get ( 'coding_sequence_name' , 'NA' ) , transcript_obj . get ( 'protein_sequence_name' , 'NA' ) , ) \n    if gene_name : \n        change_str = \"{}:\" . format ( gene_name ) + change_str \n    return change_str "}
{"7471": "\ndef frequency ( variant_obj ) : \n    most_common_frequency = max ( variant_obj . get ( 'thousand_genomes_frequency' ) or 0 , variant_obj . get ( 'exac_frequency' ) or 0 ) \n    if most_common_frequency > .05 : \n        return 'common' \n    else : \n        if most_common_frequency > .01 : \n            return 'uncommon' \n        else : \n            return 'rare' "}
{"7477": "\ndef spidex_human ( variant_obj ) : \n    if variant_obj . get ( 'spidex' ) is None : \n        return 'not_reported' \n    else : \n        if abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : \n            return 'low' \n        else : \n            if abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : \n                return 'medium' \n            else : \n                return 'high' "}
{"7489": "\ndef parse_clnsig ( acc , sig , revstat , transcripts ) : \n    clnsig_accsessions = [ ] \n    if acc : \n        try : \n            acc = int ( acc ) \n        except ValueError : \n            pass \n        if isinstance ( acc , int ) : \n            revstat_groups = [ ] \n            if revstat : \n                revstat_groups = [ rev . lstrip ( '_' ) for rev in revstat . split ( ',' ) ] \n            sig_groups = [ ] \n            if sig : \n                for significance in sig . split ( '/' ) : \n                    splitted_word = significance . split ( '_' ) \n                    sig_groups . append ( ' ' . join ( splitted_word [ : 2 ] ) ) \n            for sign_term in sig_groups : \n                clnsig_accsessions . append ( { 'value' : sign_term , 'accession' : int ( acc ) , 'revstat' : ', ' . join ( revstat_groups ) , } ) \n        else : \n            acc_groups = acc . split ( '|' ) \n            sig_groups = sig . split ( '|' ) \n            revstat_groups = revstat . split ( '|' ) \n            for acc_group , sig_group , revstat_group in zip ( acc_groups , sig_groups , revstat_groups ) : \n                accessions = acc_group . split ( ',' ) \n                significances = sig_group . split ( ',' ) \n                revstats = revstat_group . split ( ',' ) \n                for accession , significance , revstat in zip ( accessions , significances , revstats ) : \n                    clnsig_accsessions . append ( { 'value' : int ( significance ) , 'accession' : accession , 'revstat' : revstat , } ) \n    else : \n        if transcripts : \n            clnsig = set ( ) \n            for transcript in transcripts : \n                for annotation in transcript . get ( 'clinsig' , [ ] ) : \n                    clnsig . add ( annotation ) \n            for annotation in clnsig : \n                clnsig_accsessions . append ( { 'value' : annotation } ) \n    return clnsig_accsessions "}
{"7500": "\ndef get_next_and_prev ( net ) : \n    if net == 0 : \n        nxt = prev = 1 \n    else : \n        if net > 0 : \n            nxt = net + 1 \n            prev = - ( net - 1 ) \n        else : \n            nxt = net + 1 \n            prev = abs ( net ) + 1 \n    return nxt , prev "}
{"7511": "\ndef is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if ps_terms : \n            return True \n        if pm_terms : \n            if pp_terms : \n                return True \n            if len ( pm_terms ) >= 2 : \n                return True \n        if len ( pp_terms ) >= 2 : \n            return True \n    if ps_terms : \n        if len ( ps_terms ) >= 2 : \n            return True \n        if pm_terms : \n            if len ( pm_terms ) >= 3 : \n                return True \n            else : \n                if len ( pm_terms ) >= 2 : \n                    if len ( pp_terms ) >= 2 : \n                        return True \n                else : \n                    if len ( pp_terms ) >= 4 : \n                        return True \n    return False "}
{"7512": "\ndef is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if pm_terms : \n            return True \n    if ps_terms : \n        if pm_terms : \n            return True \n        if len ( pp_terms ) >= 2 : \n            return True \n    if pm_terms : \n        if len ( pm_terms ) >= 3 : \n            return True \n        else : \n            if len ( pm_terms ) >= 2 : \n                if len ( pp_terms ) >= 2 : \n                    return True \n            else : \n                if len ( pp_terms ) >= 4 : \n                    return True \n    return False "}
{"7514": "\ndef get_acmg ( acmg_terms ) : \n    prediction = 'uncertain_significance' \n    pvs = False \n    ps_terms = [ ] \n    pm_terms = [ ] \n    pp_terms = [ ] \n    ba = False \n    bs_terms = [ ] \n    bp_terms = [ ] \n    for term in acmg_terms : \n        if term . startswith ( 'PVS' ) : \n            pvs = True \n        else : \n            if term . startswith ( 'PS' ) : \n                ps_terms . append ( term ) \n            else : \n                if term . startswith ( 'PM' ) : \n                    pm_terms . append ( term ) \n                else : \n                    if term . startswith ( 'PP' ) : \n                        pp_terms . append ( term ) \n                    else : \n                        if term . startswith ( 'BA' ) : \n                            ba = True \n                        else : \n                            if term . startswith ( 'BS' ) : \n                                bs_terms . append ( term ) \n                            else : \n                                if term . startswith ( 'BP' ) : \n                                    bp_terms . append ( term ) \n    pathogenic = is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) \n    likely_pathogenic = is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) \n    benign = is_benign ( ba , bs_terms ) \n    likely_benign = is_likely_benign ( bs_terms , bp_terms ) \n    if ( pathogenic or likely_pathogenic ) : \n        if ( benign or likely_benign ) : \n            prediction = 'uncertain_significance' \n        else : \n            if pathogenic : \n                prediction = 'pathogenic' \n            else : \n                prediction = 'likely_pathogenic' \n    else : \n        if benign : \n            prediction = 'benign' \n        if likely_benign : \n            prediction = 'likely_benign' \n    return prediction "}
{"7516": "\ndef variants ( self , case_id , query = None , variant_ids = None , category = 'snv' , nr_of_variants = 10 , skip = 0 , sort_key = 'variant_rank' ) : \n    LOG . debug ( \"Fetching variants from {0}\" . format ( case_id ) ) \n    if variant_ids : \n        nr_of_variants = len ( variant_ids ) \n    else : \n        if nr_of_variants == - 1 : \n            nr_of_variants = 0 \n        else : \n            nr_of_variants = skip + nr_of_variants \n    mongo_query = self . build_query ( case_id , query = query , variant_ids = variant_ids , category = category ) \n    sorting = [ ] \n    if sort_key == 'variant_rank' : \n        sorting = [ ( 'variant_rank' , pymongo . ASCENDING ) ] \n    if sort_key == 'rank_score' : \n        sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if sort_key == 'position' : \n        sorting = [ ( 'position' , pymongo . ASCENDING ) ] \n    result = self . variant_collection . find ( mongo_query , skip = skip , limit = nr_of_variants ) . sort ( sorting ) \n    return result "}
{"7521": "\ndef get_causatives ( self , institute_id , case_id = None ) : \n    causatives = [ ] \n    if case_id : \n        case_obj = self . case_collection . find_one ( { \"_id\" : case_id } ) \n        causatives = [ causative for causative in case_obj [ 'causatives' ] ] \n    else : \n        if institute_id : \n            query = self . case_collection . aggregate ( [ { '$match' : { 'collaborators' : institute_id , 'causatives' : { '$exists' : True } } } , { '$unwind' : '$causatives' } , { '$group' : { '_id' : '$causatives' } } ] ) \n            causatives = [ item [ '_id' ] for item in query ] \n    return causatives "}
{"7527": "\ndef get_region_vcf ( self , case_obj , chrom = None , start = None , end = None , gene_obj = None , variant_type = 'clinical' , category = 'snv' , rank_threshold = None ) : \n    rank_threshold = rank_threshold or - 100 \n    variant_file = None \n    if variant_type == 'clinical' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv' ) \n        else : \n            if category == 'sv' : \n                variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) \n            else : \n                if category == 'str' : \n                    variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) \n    else : \n        if variant_type == 'research' : \n            if category == 'snv' : \n                variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) \n            else : \n                if category == 'sv' : \n                    variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) \n    if not variant_file : \n        raise SyntaxError ( \"Vcf file does not seem to exist\" ) \n    vcf_obj = VCF ( variant_file ) \n    region = \"\" \n    if gene_obj : \n        chrom = gene_obj [ 'chromosome' ] \n        start = gene_obj [ 'start' ] \n        end = gene_obj [ 'end' ] \n    if chrom : \n        if ( start and end ) : \n            region = \"{0}:{1}-{2}\" . format ( chrom , start , end ) \n        else : \n            region = \"{0}\" . format ( chrom ) \n    else : \n        rank_threshold = rank_threshold or 5 \n    with tempfile . NamedTemporaryFile ( mode = 'w' , delete = False ) as temp : \n        file_name = str ( pathlib . Path ( temp . name ) ) \n        for header_line in vcf_obj . raw_header . split ( '\\n' ) : \n            if len ( header_line ) > 3 : \n                temp . write ( header_line + '\\n' ) \n        for variant in vcf_obj ( region ) : \n            temp . write ( str ( variant ) ) \n    return file_name "}
{"7552": "\ndef add_phenotype ( self , institute , case , user , link , hpo_term = None , omim_term = None , is_group = False ) : \n    hpo_results = [ ] \n    try : \n        if hpo_term : \n            hpo_results = [ hpo_term ] \n        else : \n            if omim_term : \n                LOG . debug ( \"Fetching info for mim term {0}\" . format ( omim_term ) ) \n                disease_obj = self . disease_term ( omim_term ) \n                if disease_obj : \n                    for hpo_term in disease_obj . get ( 'hpo_terms' , [ ] ) : \n                        hpo_results . append ( hpo_term ) \n            else : \n                raise ValueError ( 'Must supply either hpo or omim term' ) \n    except ValueError as e : \n        raise e \n    existing_terms = set ( term [ 'phenotype_id' ] for term in case . get ( 'phenotype_terms' , [ ] ) ) \n    updated_case = case \n    phenotype_terms = [ ] \n    for hpo_term in hpo_results : \n        LOG . debug ( \"Fetching info for hpo term {0}\" . format ( hpo_term ) ) \n        hpo_obj = self . hpo_term ( hpo_term ) \n        if hpo_obj is None : \n            raise ValueError ( \"Hpo term: %s does not exist in database\" % hpo_term ) \n        phenotype_id = hpo_obj [ '_id' ] \n        description = hpo_obj [ 'description' ] \n        if phenotype_id not in existing_terms : \n            phenotype_term = dict ( phenotype_id = phenotype_id , feature = description ) \n            phenotype_terms . append ( phenotype_term ) \n            LOG . info ( \"Creating event for adding phenotype term for case\" \" {0}\" . format ( case [ 'display_name' ] ) ) \n            self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'add_phenotype' , subject = case [ 'display_name' ] , content = phenotype_id ) \n        if is_group : \n            updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$addToSet' : { 'phenotype_terms' : { '$each' : phenotype_terms } , 'phenotype_groups' : { '$each' : phenotype_terms } , } , } , return_document = pymongo . ReturnDocument . AFTER ) \n        else : \n            updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$addToSet' : { 'phenotype_terms' : { '$each' : phenotype_terms } , } , } , return_document = pymongo . ReturnDocument . AFTER ) \n    LOG . debug ( \"Case updated\" ) \n    return updated_case "}
{"7573": "\ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while len ( term ) < 7 : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    else : \n        if description : \n            sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n            for term in sorted_terms : \n                term . pop ( 'genes' ) \n                print ( \"name: {} | {} | {}\" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) \n            context . abort ( ) \n        else : \n            hpo_terms = adapter . hpo_terms ( ) \n    if hpo_terms . count ( ) == 0 : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7598": "\ndef get_date ( date , date_format = None ) : \n    date_obj = datetime . datetime . now ( ) \n    if date : \n        if date_format : \n            date_obj = datetime . datetime . strptime ( date , date_format ) \n        else : \n            if match_date ( date ) : \n                if len ( date . split ( '-' ) ) == 3 : \n                    date = date . split ( '-' ) \n                else : \n                    if len ( date . split ( ' ' ) ) == 3 : \n                        date = date . split ( ' ' ) \n                    else : \n                        if len ( date . split ( '.' ) ) == 3 : \n                            date = date . split ( '.' ) \n                        else : \n                            date = date . split ( '/' ) \n                date_obj = datetime . datetime ( * ( int ( number ) for number in date ) ) \n            else : \n                raise ValueError ( \"Date %s is invalid\" % date ) \n    return date_obj "}
{"7612": "\ndef parse_hgnc_genes ( lines ) : \n    header = [ ] \n    logger . info ( \"Parsing hgnc genes...\" ) \n    for index , line in enumerate ( lines ) : \n        if index == 0 : \n            header = line . split ( '\\t' ) \n        else : \n            if len ( line ) > 1 : \n                hgnc_gene = parse_hgnc_line ( line = line , header = header ) \n                if hgnc_gene : \n                    yield hgnc_gene "}
{"7621": "\ndef parse_hpo_obo ( hpo_lines ) : \n    term = { } \n    for line in hpo_lines : \n        if len ( line ) == 0 : \n            continue \n        line = line . rstrip ( ) \n        if line == '[Term]' : \n            if term : \n                yield term \n            term = { } \n        else : \n            if line . startswith ( 'id' ) : \n                term [ 'hpo_id' ] = line [ 4 : ] \n            else : \n                if line . startswith ( 'name' ) : \n                    term [ 'description' ] = line [ 6 : ] \n                else : \n                    if line . startswith ( 'alt_id' ) : \n                        if 'aliases' not in term : \n                            term [ 'aliases' ] = [ ] \n                        term [ 'aliases' ] . append ( line [ 8 : ] ) \n                    else : \n                        if line . startswith ( 'is_a' ) : \n                            if 'ancestors' not in term : \n                                term [ 'ancestors' ] = [ ] \n                            term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) \n    if term : \n        yield term "}
{"7628": "\ndef templated ( template = None ) : \n    def decorator ( f ) : \n        \n        @ wraps ( f ) \n        def decorated_function ( * args , ** kwargs ) : \n            template_name = template \n            if template_name is None : \n                template_name = request . endpoint . replace ( '.' , '/' ) + '.html' \n            context = f ( * args , ** kwargs ) \n            if context is None : \n                context = { } \n            else : \n                if not isinstance ( context , dict ) : \n                    return context \n            return render_template ( template_name , ** context ) \n        return decorated_function \n    return decorator "}
{"7652": "\ndef apply_pending ( self , panel_obj , version ) : \n    updates = { } \n    new_panel = deepcopy ( panel_obj ) \n    new_panel [ 'pending' ] = [ ] \n    new_panel [ 'date' ] = dt . datetime . now ( ) \n    info_fields = [ 'disease_associated_transcripts' , 'inheritance_models' , 'reduced_penetrance' , 'mosaicism' , 'database_entry_version' , 'comment' ] \n    new_genes = [ ] \n    for update in panel_obj . get ( 'pending' , [ ] ) : \n        hgnc_id = update [ 'hgnc_id' ] \n        if update [ 'action' ] != 'add' : \n            updates [ hgnc_id ] = update \n            continue \n        info = update . get ( 'info' , { } ) \n        gene_obj = { 'hgnc_id' : hgnc_id , 'symbol' : update [ 'symbol' ] } \n        for field in info_fields : \n            if field in info : \n                gene_obj [ field ] = info [ field ] \n        new_genes . append ( gene_obj ) \n    for gene in panel_obj [ 'genes' ] : \n        hgnc_id = gene [ 'hgnc_id' ] \n        if hgnc_id not in updates : \n            new_genes . append ( gene ) \n            continue \n        current_update = updates [ hgnc_id ] \n        action = current_update [ 'action' ] \n        info = current_update [ 'info' ] \n        if action == 'delete' : \n            continue \n        else : \n            if action == 'edit' : \n                for field in info_fields : \n                    if field in info : \n                        gene [ field ] = info [ field ] \n                new_genes . append ( gene ) \n    new_panel [ 'genes' ] = new_genes \n    new_panel [ 'version' ] = float ( version ) \n    inserted_id = None \n    if new_panel [ 'version' ] == panel_obj [ 'version' ] : \n        result = self . panel_collection . find_one_and_replace ( { '_id' : panel_obj [ '_id' ] } , new_panel , return_document = pymongo . ReturnDocument . AFTER ) \n        inserted_id = result [ '_id' ] \n    else : \n        new_panel . pop ( '_id' ) \n        panel_obj [ 'is_archived' ] = True \n        self . update_panel ( panel_obj = panel_obj , date_obj = panel_obj [ 'date' ] ) \n        inserted_id = self . panel_collection . insert_one ( new_panel ) . inserted_id \n    return inserted_id "}
{"7660": "\ndef build_query ( self , case_id , query = None , variant_ids = None , category = 'snv' ) : \n    query = query or { } \n    mongo_query = { } \n    gene_query = None \n    for criterion in FUNDAMENTAL_CRITERIA : \n        if criterion == 'case_id' : \n            LOG . debug ( \"Building a mongo query for %s\" % case_id ) \n            mongo_query [ 'case_id' ] = case_id \n        else : \n            if criterion == 'variant_ids' and variant_ids : \n                LOG . debug ( \"Adding variant_ids %s to query\" % ', ' . join ( variant_ids ) ) \n                mongo_query [ 'variant_id' ] = { '$in' : variant_ids } \n            else : \n                if criterion == 'category' : \n                    LOG . debug ( \"Querying category %s\" % category ) \n                    mongo_query [ 'category' ] = category \n                else : \n                    if criterion == 'variant_type' : \n                        mongo_query [ 'variant_type' ] = query . get ( 'variant_type' , 'clinical' ) \n                        LOG . debug ( \"Set variant type to %s\" , mongo_query [ 'variant_type' ] ) \n                    else : \n                        if criterion in [ 'hgnc_symbols' , 'gene_panels' ] and gene_query is None : \n                            gene_query = self . gene_filter ( query , mongo_query ) \n                        else : \n                            if criterion == 'chrom' and query . get ( 'chrom' ) : \n                                self . coordinate_filter ( query , mongo_query ) \n                            else : \n                                if criterion == 'variant_ids' and variant_ids : \n                                    LOG . debug ( \"Adding variant_ids %s to query\" % ', ' . join ( variant_ids ) ) \n                                    mongo_query [ 'variant_id' ] = { '$in' : variant_ids } \n    primary_terms = False \n    secondary_terms = False \n    for term in PRIMARY_CRITERIA : \n        if query . get ( term ) : \n            primary_terms = True \n    for term in SECONDARY_CRITERIA : \n        if query . get ( term ) : \n            secondary_terms = True \n    if primary_terms is True : \n        clinsign_filter = self . clinsig_query ( query , mongo_query ) \n    if secondary_terms is True : \n        secondary_filter = self . secondary_query ( query , mongo_query ) \n        if primary_terms is False : \n            if gene_query : \n                mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] \n            else : \n                mongo_query [ '$and' ] = secondary_filter \n        if primary_terms is True : \n            if query . get ( 'clinsig_confident_always_returned' ) == True : \n                if gene_query : \n                    mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$or' : [ { '$and' : secondary_filter } , clinsign_filter ] } ] \n                else : \n                    mongo_query [ '$or' ] = [ { '$and' : secondary_filter } , clinsign_filter ] \n            else : \n                secondary_filter . append ( clinsign_filter ) \n                if gene_query : \n                    mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] \n                else : \n                    mongo_query [ '$and' ] = secondary_filter \n    else : \n        if primary_terms is True : \n            mongo_query [ 'clnsig' ] = clinsign_filter [ 'clnsig' ] \n            if gene_query : \n                mongo_query [ '$and' ] = [ { '$or' : gene_query } ] \n        else : \n            if gene_query : \n                mongo_query [ '$and' ] = [ { '$or' : gene_query } ] \n    LOG . info ( \"mongo query: %s\" , mongo_query ) \n    return mongo_query "}
{"7689": "\ndef matchmaker_matches ( institute_id , case_name ) : \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    if not mme_base_url or not mme_token : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    data = controllers . mme_matches ( case_obj , institute_obj , mme_base_url , mme_token ) \n    if data and data . get ( 'server_errors' ) : \n        flash ( 'MatchMaker server returned error:{}' . format ( data [ 'server_errors' ] ) , 'danger' ) \n        return redirect ( request . referrer ) \n    else : \n        if not data : \n            data = { 'institute' : institute_obj , 'case' : case_obj } \n    return data "}
{"7696": "\ndef phenotypes_actions ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    action = request . form [ 'action' ] \n    hpo_ids = request . form . getlist ( 'hpo_id' ) \n    user_obj = store . user ( current_user . email ) \n    if action == 'DELETE' : \n        for hpo_id in hpo_ids : \n            store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_id ) \n    else : \n        if action == 'PHENOMIZER' : \n            if len ( hpo_ids ) == 0 : \n                hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n            username = current_app . config [ 'PHENOMIZER_USERNAME' ] \n            password = current_app . config [ 'PHENOMIZER_PASSWORD' ] \n            diseases = controllers . hpo_diseases ( username , password , hpo_ids ) \n            return render_template ( 'cases/diseases.html' , diseases = diseases , institute = institute_obj , case = case_obj ) \n        else : \n            if action == 'GENES' : \n                hgnc_symbols = set ( ) \n                for raw_symbols in request . form . getlist ( 'genes' ) : \n                    if raw_symbols : \n                        hgnc_symbols . update ( raw_symbol . split ( ' ' , 1 ) [ 0 ] for raw_symbol in raw_symbols . split ( '|' ) ) \n                store . update_dynamic_gene_list ( case_obj , hgnc_symbols = hgnc_symbols ) \n            else : \n                if action == 'GENERATE' : \n                    if len ( hpo_ids ) == 0 : \n                        hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n                    results = store . generate_hpo_gene_list ( * hpo_ids ) \n                    hpo_count = int ( request . form . get ( 'min_match' ) or 1 ) \n                    hgnc_ids = [ result [ 0 ] for result in results if result [ 1 ] >= hpo_count ] \n                    store . update_dynamic_gene_list ( case_obj , hgnc_ids = hgnc_ids , phenotype_ids = hpo_ids ) \n    return redirect ( case_url ) "}
{"7702": "\ndef mark_causative ( institute_id , case_name , variant_id ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    variant_obj = store . variant ( variant_id ) \n    user_obj = store . user ( current_user . email ) \n    link = url_for ( 'variants.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) \n    if request . form [ 'action' ] == 'ADD' : \n        store . mark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) \n    else : \n        if request . form [ 'action' ] == 'DELETE' : \n            store . unmark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    return redirect ( case_url ) "}
{"7724": "\ndef parse_callers ( variant , category = 'snv' ) : \n    relevant_callers = CALLERS [ category ] \n    callers = { caller [ 'id' ] : None for caller in relevant_callers } \n    raw_info = variant . INFO . get ( 'set' ) \n    if raw_info : \n        info = raw_info . split ( '-' ) \n        for call in info : \n            if call == 'FilteredInAll' : \n                for caller in callers : \n                    callers [ caller ] = 'Filtered' \n            else : \n                if call == 'Intersection' : \n                    for caller in callers : \n                        callers [ caller ] = 'Pass' \n                else : \n                    if 'filterIn' in call : \n                        for caller in callers : \n                            if caller in call : \n                                callers [ caller ] = 'Filtered' \n                    else : \n                        if call in set ( callers . keys ( ) ) : \n                            callers [ call ] = 'Pass' \n    other_info = variant . INFO . get ( 'FOUND_IN' ) \n    if other_info : \n        for info in other_info . split ( ',' ) : \n            called_by = info . split ( '|' ) [ 0 ] \n            callers [ called_by ] = 'Pass' \n    return callers "}
{"7740": "\ndef diagnose ( self , institute , case , user , link , level , omim_id , remove = False ) : \n    if level == 'phenotype' : \n        case_key = 'diagnosis_phenotypes' \n    else : \n        if level == 'gene' : \n            case_key = 'diagnosis_genes' \n        else : \n            raise TypeError ( 'wrong level' ) \n    diagnosis_list = case . get ( case_key , [ ] ) \n    omim_number = int ( omim_id . split ( ':' ) [ - 1 ] ) \n    updated_case = None \n    if remove and omim_number in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$pull' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    else : \n        if omim_number not in diagnosis_list : \n            updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$push' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    if updated_case : \n        self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'update_diagnosis' , subject = case [ 'display_name' ] , content = omim_id ) \n    return updated_case "}
{"7754": "\ndef update_dynamic_gene_list ( self , case , hgnc_symbols = None , hgnc_ids = None , phenotype_ids = None , build = '37' ) : \n    dynamic_gene_list = [ ] \n    res = [ ] \n    if hgnc_ids : \n        LOG . info ( \"Fetching genes by hgnc id\" ) \n        res = self . hgnc_collection . find ( { 'hgnc_id' : { '$in' : hgnc_ids } , 'build' : build } ) \n    else : \n        if hgnc_symbols : \n            LOG . info ( \"Fetching genes by hgnc symbols\" ) \n            res = [ ] \n            for symbol in hgnc_symbols : \n                for gene_obj in self . gene_by_alias ( symbol = symbol , build = build ) : \n                    res . append ( gene_obj ) \n    for gene_obj in res : \n        dynamic_gene_list . append ( { 'hgnc_symbol' : gene_obj [ 'hgnc_symbol' ] , 'hgnc_id' : gene_obj [ 'hgnc_id' ] , 'description' : gene_obj [ 'description' ] , } ) \n    LOG . info ( \"Update dynamic gene panel for: %s\" , case [ 'display_name' ] ) \n    updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$set' : { 'dynamic_gene_list' : dynamic_gene_list , 'dynamic_panel_phenotypes' : phenotype_ids or [ ] } } , return_document = pymongo . ReturnDocument . AFTER ) \n    LOG . debug ( \"Case updated\" ) \n    return updated_case "}
{"7765": "\ndef parse_ensembl_line ( line , header ) : \n    line = line . rstrip ( ) . split ( '\\t' ) \n    header = [ head . lower ( ) for head in header ] \n    raw_info = dict ( zip ( header , line ) ) \n    ensembl_info = { } \n    for word in raw_info : \n        value = raw_info [ word ] \n        if not value : \n            continue \n        if 'chromosome' in word : \n            ensembl_info [ 'chrom' ] = value \n        if 'gene' in word : \n            if 'id' in word : \n                ensembl_info [ 'ensembl_gene_id' ] = value \n            else : \n                if 'start' in word : \n                    ensembl_info [ 'gene_start' ] = int ( value ) \n                else : \n                    if 'end' in word : \n                        ensembl_info [ 'gene_end' ] = int ( value ) \n        if 'hgnc symbol' in word : \n            ensembl_info [ 'hgnc_symbol' ] = value \n        if \"gene name\" in word : \n            ensembl_info [ 'hgnc_symbol' ] = value \n        if 'hgnc id' in word : \n            ensembl_info [ 'hgnc_id' ] = int ( value . split ( ':' ) [ - 1 ] ) \n        if 'transcript' in word : \n            if 'id' in word : \n                ensembl_info [ 'ensembl_transcript_id' ] = value \n            else : \n                if 'start' in word : \n                    ensembl_info [ 'transcript_start' ] = int ( value ) \n                else : \n                    if 'end' in word : \n                        ensembl_info [ 'transcript_end' ] = int ( value ) \n        if 'exon' in word : \n            if 'start' in word : \n                ensembl_info [ 'exon_start' ] = int ( value ) \n            else : \n                if 'end' in word : \n                    ensembl_info [ 'exon_end' ] = int ( value ) \n                else : \n                    if 'rank' in word : \n                        ensembl_info [ 'exon_rank' ] = int ( value ) \n        if 'utr' in word : \n            if 'start' in word : \n                if '5' in word : \n                    ensembl_info [ 'utr_5_start' ] = int ( value ) \n                else : \n                    if '3' in word : \n                        ensembl_info [ 'utr_3_start' ] = int ( value ) \n            else : \n                if 'end' in word : \n                    if '5' in word : \n                        ensembl_info [ 'utr_5_end' ] = int ( value ) \n                    else : \n                        if '3' in word : \n                            ensembl_info [ 'utr_3_end' ] = int ( value ) \n        if 'strand' in word : \n            ensembl_info [ 'strand' ] = int ( value ) \n        if 'refseq' in word : \n            if 'mrna' in word : \n                if 'predicted' in word : \n                    ensembl_info [ 'refseq_mrna_predicted' ] = value \n                else : \n                    ensembl_info [ 'refseq_mrna' ] = value \n            if 'ncrna' in word : \n                ensembl_info [ 'refseq_ncrna' ] = value \n    return ensembl_info "}
{"7767": "\ndef parse_ensembl_exons ( lines ) : \n    header = [ ] \n    LOG . debug ( \"Parsing ensembl exons...\" ) \n    for index , line in enumerate ( lines ) : \n        if index == 0 : \n            header = line . rstrip ( ) . split ( '\\t' ) \n            continue \n        exon_info = parse_ensembl_line ( line , header ) \n        chrom = exon_info [ 'chrom' ] \n        start = exon_info [ 'exon_start' ] \n        end = exon_info [ 'exon_end' ] \n        transcript = exon_info [ 'ensembl_transcript_id' ] \n        gene = exon_info [ 'ensembl_gene_id' ] \n        rank = exon_info [ 'exon_rank' ] \n        strand = exon_info [ 'strand' ] \n        if strand == 1 : \n            start = max ( start , exon_info . get ( 'utr_5_end' ) or - 1 ) \n            end = min ( end , exon_info . get ( 'utr_3_start' ) or float ( 'inf' ) ) \n        else : \n            if strand == - 1 : \n                start = max ( start , exon_info . get ( 'utr_3_end' ) or - 1 ) \n                end = min ( end , exon_info . get ( 'utr_5_start' ) or float ( 'inf' ) ) \n        exon_id = \"-\" . join ( [ chrom , str ( start ) , str ( end ) ] ) \n        if start > end : \n            raise ValueError ( \"ERROR: %s\" % exon_id ) \n        data = { \"exon_id\" : exon_id , \"chrom\" : chrom , \"start\" : start , \"end\" : end , \"transcript\" : transcript , \"gene\" : gene , \"rank\" : rank , } \n        yield data "}
{"7768": "\ndef parse_ensembl_exon_request ( result ) : \n    keys = [ 'chrom' , 'gene' , 'transcript' , 'exon_id' , 'exon_chrom_start' , 'exon_chrom_end' , '5_utr_start' , '5_utr_end' , '3_utr_start' , '3_utr_end' , 'strand' , 'rank' ] \n    for res in zip ( result [ 'Chromosome/scaffold name' ] , result [ 'Gene stable ID' ] , result [ 'Transcript stable ID' ] , result [ 'Exon stable ID' ] , result [ 'Exon region start (bp)' ] , result [ 'Exon region end (bp)' ] , result [ \"5' UTR start\" ] , result [ \"5' UTR end\" ] , result [ \"3' UTR start\" ] , result [ \"3' UTR end\" ] , result [ \"Strand\" ] , result [ \"Exon rank in transcript\" ] ) : \n        ensembl_info = dict ( zip ( keys , res ) ) \n        if ensembl_info [ 'strand' ] == 1 : \n            start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '5_utr_end' ] or - 1 ) \n            end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '3_utr_start' ] or float ( 'inf' ) ) \n        else : \n            if ensembl_info [ 'strand' ] == - 1 : \n                start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '3_utr_end' ] or - 1 ) \n                end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '5_utr_start' ] or float ( 'inf' ) ) \n        ensembl_info [ 'start' ] = start \n        ensembl_info [ 'end' ] = end \n        yield ensembl_info "}
{"7774": "\ndef convert_number ( string ) : \n    res = None \n    if isint ( string ) : \n        res = int ( string ) \n    else : \n        if isfloat ( string ) : \n            res = float ( string ) \n    return res "}
{"7780": "\ndef parse_gene ( gene_info ) : \n    gene = { } \n    identifier = None \n    hgnc_id = None \n    try : \n        if 'hgnc_id' in gene_info : \n            hgnc_id = int ( gene_info [ 'hgnc_id' ] ) \n        else : \n            if 'hgnc_idnumber' in gene_info : \n                hgnc_id = int ( gene_info [ 'hgnc_idnumber' ] ) \n            else : \n                if 'hgncid' in gene_info : \n                    hgnc_id = int ( gene_info [ 'hgncid' ] ) \n    except ValueError as e : \n        raise SyntaxError ( \"Invalid hgnc id: {0}\" . format ( hgnc_id ) ) \n    gene [ 'hgnc_id' ] = hgnc_id \n    identifier = hgnc_id \n    hgnc_symbol = None \n    if 'hgnc_symbol' in gene_info : \n        hgnc_symbol = gene_info [ 'hgnc_symbol' ] \n    else : \n        if 'hgncsymbol' in gene_info : \n            hgnc_symbol = gene_info [ 'hgncsymbol' ] \n        else : \n            if 'symbol' in gene_info : \n                hgnc_symbol = gene_info [ 'symbol' ] \n    gene [ 'hgnc_symbol' ] = hgnc_symbol \n    if not identifier : \n        if hgnc_symbol : \n            identifier = hgnc_symbol \n        else : \n            raise SyntaxError ( \"No gene identifier could be found\" ) \n    gene [ 'identifier' ] = identifier \n    transcripts = \"\" \n    if 'disease_associated_transcripts' in gene_info : \n        transcripts = gene_info [ 'disease_associated_transcripts' ] \n    else : \n        if 'disease_associated_transcript' in gene_info : \n            transcripts = gene_info [ 'disease_associated_transcript' ] \n        else : \n            if 'transcripts' in gene_info : \n                transcripts = gene_info [ 'transcripts' ] \n    gene [ 'transcripts' ] = [ transcript . strip ( ) for transcript in transcripts . split ( ',' ) if transcript ] \n    models = \"\" \n    if 'genetic_disease_models' in gene_info : \n        models = gene_info [ 'genetic_disease_models' ] \n    else : \n        if 'genetic_disease_model' in gene_info : \n            models = gene_info [ 'genetic_disease_model' ] \n        else : \n            if 'inheritance_models' in gene_info : \n                models = gene_info [ 'inheritance_models' ] \n            else : \n                if 'genetic_inheritance_models' in gene_info : \n                    models = gene_info [ 'genetic_inheritance_models' ] \n    gene [ 'inheritance_models' ] = [ model . strip ( ) for model in models . split ( ',' ) if model . strip ( ) in VALID_MODELS ] \n    gene [ 'mosaicism' ] = True if gene_info . get ( 'mosaicism' ) else False \n    gene [ 'reduced_penetrance' ] = True if gene_info . get ( 'reduced_penetrance' ) else False \n    gene [ 'database_entry_version' ] = gene_info . get ( 'database_entry_version' ) \n    return gene "}
{"7789": "\ndef get_case_groups ( adapter , total_cases , institute_id = None , slice_query = None ) : \n    cases = [ { 'status' : 'all' , 'count' : total_cases , 'percent' : 1 } ] \n    pipeline = [ ] \n    group = { '$group' : { '_id' : '$status' , 'count' : { '$sum' : 1 } } } \n    subquery = { } \n    if institute_id and slice_query : \n        subquery = adapter . cases ( owner = institute_id , name_query = slice_query , yield_query = True ) \n    else : \n        if institute_id : \n            subquery = adapter . cases ( owner = institute_id , yield_query = True ) \n        else : \n            if slice_query : \n                subquery = adapter . cases ( name_query = slice_query , yield_query = True ) \n    query = { '$match' : subquery } if subquery else { } \n    if query : \n        pipeline . append ( query ) \n    pipeline . append ( group ) \n    res = adapter . case_collection . aggregate ( pipeline ) \n    for status_group in res : \n        cases . append ( { 'status' : status_group [ '_id' ] , 'count' : status_group [ 'count' ] , 'percent' : status_group [ 'count' ] / total_cases } ) \n    return cases "}
{"7794": "\ndef hpo_terms ( self , query = None , hpo_term = None , text = None , limit = None ) : \n    query_dict = { } \n    search_term = None \n    if query : \n        query_dict = { '$or' : [ { 'hpo_id' : { '$regex' : query , '$options' : 'i' } } , { 'description' : { '$regex' : query , '$options' : 'i' } } , ] } \n        search_term = query \n    else : \n        if text : \n            new_string = '' \n            for i , word in enumerate ( text . split ( ' ' ) ) : \n                if i == 0 : \n                    new_string += word \n                else : \n                    new_string += ' \\\"{0}\\\"' . format ( word ) \n            LOG . info ( \"Search HPO terms with %s\" , new_string ) \n            query_dict [ '$text' ] = { '$search' : new_string } \n            search_term = text \n        else : \n            if hpo_term : \n                query_dict [ 'hpo_id' ] = hpo_term \n                search_term = hpo_term \n    limit = limit or int ( 10e10 ) \n    res = self . hpo_term_collection . find ( query_dict ) . limit ( limit ) . sort ( 'hpo_number' , ASCENDING ) \n    LOG . info ( \"Found {0} terms with search word {1}\" . format ( res . count ( ) , search_term ) ) \n    return res "}
{"7799": "\ndef read_hdf5 ( self , filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    print ( \"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\" ) \n    self . header = { } \n    self . filename = filename \n    self . h5 = h5py . File ( filename ) \n    for key , val in self . h5 [ b'data' ] . attrs . items ( ) : \n        if six . PY3 : \n            key = bytes ( key , 'ascii' ) \n        if key == b'src_raj' : \n            self . header [ key ] = Angle ( val , unit = 'hr' ) \n        else : \n            if key == b'src_dej' : \n                self . header [ key ] = Angle ( val , unit = 'deg' ) \n            else : \n                self . header [ key ] = val \n    self . n_ints_in_file = self . h5 [ b\"data\" ] . shape [ 0 ] \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    if load_data : \n        self . data = self . h5 [ b\"data\" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] \n        self . file_size_bytes = os . path . getsize ( self . filename ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] ) \n        self . n_ints_in_file = 0 \n        self . file_size_bytes = os . path . getsize ( self . filename ) "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    else : \n        if n_bytes == 4 : \n            dd_type = b'float32' \n        else : \n            if n_bytes == 2 : \n                dd_type = b'uint16' \n            else : \n                if n_bytes == 1 : \n                    dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7803": "\ndef compute_lst ( self ) : \n    if self . header [ b'telescope_id' ] == 6 : \n        self . coords = gbt_coords \n    else : \n        if self . header [ b'telescope_id' ] == 4 : \n            self . coords = parkes_coords \n        else : \n            raise RuntimeError ( \"Currently only Parkes and GBT supported\" ) \n    if HAS_SLALIB : \n        dut1 = 0.0 \n        mjd = self . header [ b'tstart' ] \n        tellong = np . deg2rad ( self . coords [ 1 ] ) \n        last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 \n        if last < 0.0 : \n            last = last + 2.0 * np . pi \n        return last \n    else : \n        raise RuntimeError ( \"This method requires pySLALIB\" ) "}
{"7809": "\ndef write_to_filterbank ( self , filename_out ) : \n    print ( \"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\" ) \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    with open ( filename_out , \"wb\" ) as fileh : \n        fileh . write ( generate_sigproc_header ( self ) ) \n        j = self . data \n        if n_bytes == 4 : \n            np . float32 ( j . ravel ( ) ) . tofile ( fileh ) \n        else : \n            if n_bytes == 2 : \n                np . int16 ( j . ravel ( ) ) . tofile ( fileh ) \n            else : \n                if n_bytes == 1 : \n                    np . int8 ( j . ravel ( ) ) . tofile ( fileh ) "}
{"7817": "\ndef rebin ( d , n_x , n_y = None ) : \n    if d . ndim == 2 : \n        if n_y is None : \n            n_y = 1 \n        if n_x is None : \n            n_x = 1 \n        d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x , : int ( d . shape [ 1 ] // n_y ) * n_y ] \n        d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x , d . shape [ 1 ] // n_y , n_y ) ) \n        d = d . mean ( axis = 3 ) \n        d = d . mean ( axis = 1 ) \n    else : \n        if d . ndim == 1 : \n            d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x ] \n            d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x ) ) \n            d = d . mean ( axis = 1 ) \n        else : \n            raise RuntimeError ( \"Only NDIM <= 2 supported\" ) \n    return d "}
{"7818": "\ndef unpack ( data , nbit ) : \n    if nbit > 8 : \n        raise ValueError ( \"unpack: nbit must be <= 8\" ) \n    if 8 % nbit != 0 : \n        raise ValueError ( \"unpack: nbit must divide into 8\" ) \n    if data . dtype not in ( np . uint8 , np . int8 ) : \n        raise TypeError ( \"unpack: dtype must be 8-bit\" ) \n    if nbit == 8 : \n        return data \n    else : \n        if nbit == 4 : \n            data = unpack_4to8 ( data ) \n            return data \n        else : \n            if nbit == 2 : \n                data = unpack_2to8 ( data ) \n                return data \n            else : \n                if nbit == 1 : \n                    data = unpack_1to8 ( data ) \n                    return data "}
{"7823": "\ndef open_file ( filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True , max_load = 1. ) : \n    if not os . path . isfile ( filename ) : \n        type ( filename ) \n        print ( filename ) \n        raise IOError ( \"No such file or directory: \" + filename ) \n    filename = os . path . expandvars ( os . path . expanduser ( filename ) ) \n    ext = filename . split ( \".\" ) [ - 1 ] . strip ( ) . lower ( ) \n    if six . PY3 : \n        ext = bytes ( ext , 'ascii' ) \n    if h5py . is_hdf5 ( filename ) : \n        return H5Reader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    else : \n        if sigproc . is_filterbank ( filename ) : \n            return FilReader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n        else : \n            raise NotImplementedError ( 'Cannot open this type of file with Waterfall' ) "}
{"7831": "\ndef calc_n_coarse_chan ( self , chan_bw = None ) : \n    nchans = int ( self . header [ b'nchans' ] ) \n    if chan_bw is not None : \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / chan_bw ) \n        return n_coarse_chan \n    else : \n        if nchans >= 2 ** 20 : \n            if nchans % 2 ** 20 == 0 : \n                n_coarse_chan = nchans // 2 ** 20 \n                return n_coarse_chan \n            else : \n                if self . header [ b'telescope_id' ] == 6 : \n                    coarse_chan_bw = 2.9296875 \n                    bandwidth = abs ( self . f_stop - self . f_start ) \n                    n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n                    return n_coarse_chan \n                else : \n                    logger . warning ( \"Couldn't figure out n_coarse_chan\" ) \n        else : \n            if self . header [ b'telescope_id' ] == 6 and nchans < 2 ** 20 : \n                coarse_chan_bw = 2.9296875 \n                bandwidth = abs ( self . f_stop - self . f_start ) \n                n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n                return n_coarse_chan \n            else : \n                logger . warning ( \"This function currently only works for hires BL Parkes or GBT data.\" ) "}
{"7844": "\ndef __get_chunk_dimensions ( self ) : \n    if np . abs ( self . header [ b'foff' ] ) < 1e-5 : \n        logger . info ( 'Detecting high frequency resolution data.' ) \n        chunk_dim = ( 1 , 1 , 1048576 ) \n        return chunk_dim \n    else : \n        if np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : \n            logger . info ( 'Detecting high time resolution data.' ) \n            chunk_dim = ( 2048 , 1 , 512 ) \n            return chunk_dim \n        else : \n            if np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : \n                logger . info ( 'Detecting intermediate frequency and time resolution data.' ) \n                chunk_dim = ( 10 , 1 , 65536 ) \n                return chunk_dim \n            else : \n                logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) \n                chunk_dim = ( 1 , 1 , 512 ) \n                return chunk_dim "}
{"7851": "\ndef generate_filterbank_header ( self , nchans = 1 , ) : \n    gp_head = self . read_first_header ( ) \n    fb_head = { } \n    telescope_str = gp_head . get ( \"TELESCOP\" , \"unknown\" ) \n    if telescope_str in ( 'GBT' , 'GREENBANK' ) : \n        fb_head [ \"telescope_id\" ] = 6 \n    else : \n        if telescope_str in ( 'PKS' , 'PARKES' ) : \n            fb_head [ \"telescop_id\" ] = 7 \n        else : \n            fb_head [ \"telescop_id\" ] = 0 \n    fb_head [ \"source_name\" ] = gp_head . get ( \"SRC_NAME\" , \"unknown\" ) \n    fb_head [ \"az_start\" ] = gp_head . get ( \"AZ\" , 0 ) \n    fb_head [ \"za_start\" ] = gp_head . get ( \"ZA\" , 0 ) \n    fb_head [ \"src_raj\" ] = Angle ( str ( gp_head . get ( \"RA\" , 0.0 ) ) + \"hr\" ) \n    fb_head [ \"src_dej\" ] = Angle ( str ( gp_head . get ( \"DEC\" , 0.0 ) ) + \"deg\" ) \n    fb_head [ \"rawdatafile\" ] = self . filename \n    fb_head [ \"machine_id\" ] = 20 \n    fb_head [ \"data_type\" ] = 1 \n    fb_head [ \"barycentric\" ] = 0 \n    fb_head [ \"pulsarcentric\" ] = 0 \n    fb_head [ \"nbits\" ] = 32 \n    fb_head [ \"tstart\" ] = 0.0 \n    fb_head [ \"tsamp\" ] = 1.0 \n    fb_head [ \"fch1\" ] = 0.0 \n    fb_head [ \"foff\" ] = 187.5 / nchans \n    fb_head [ \"nchans\" ] = nchans \n    fb_head [ \"nifs\" ] = 1 \n    fb_head [ \"nbeams\" ] = 1 \n    return fb_head "}
{"7866": "\ndef generate_sigproc_header ( f ) : \n    header_string = b'' \n    header_string += to_sigproc_keyword ( b'HEADER_START' ) \n    for keyword in f . header . keys ( ) : \n        if keyword == b'src_raj' : \n            header_string += to_sigproc_keyword ( b'src_raj' ) + to_sigproc_angle ( f . header [ b'src_raj' ] ) \n        else : \n            if keyword == b'src_dej' : \n                header_string += to_sigproc_keyword ( b'src_dej' ) + to_sigproc_angle ( f . header [ b'src_dej' ] ) \n            else : \n                if keyword == b'az_start' or keyword == b'za_start' : \n                    header_string += to_sigproc_keyword ( keyword ) + np . float64 ( f . header [ keyword ] ) . tostring ( ) \n                else : \n                    if keyword not in header_keyword_types . keys ( ) : \n                        pass \n                    else : \n                        header_string += to_sigproc_keyword ( keyword , f . header [ keyword ] ) \n    header_string += to_sigproc_keyword ( b'HEADER_END' ) \n    return header_string "}
{"7874": "\ndef parse_line ( parser , record_token , parsed_records ) : \n    global SUPPORTED_RECORDS \n    line = \" \" . join ( record_token ) \n    if len ( record_token ) >= 2 and record_token [ 1 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 1 ] ] + record_token \n    else : \n        if len ( record_token ) >= 3 and record_token [ 2 ] in SUPPORTED_RECORDS : \n            record_token = [ record_token [ 2 ] ] + record_token \n            if record_token [ 0 ] == \"TXT\" : \n                record_token = record_token [ : 2 ] + [ \"--ttl\" ] + record_token [ 2 : ] \n    try : \n        rr , unmatched = parser . parse_known_args ( record_token ) \n        assert len ( unmatched ) == 0 , \"Unmatched fields: %s\" % unmatched \n    except ( SystemExit , AssertionError , InvalidLineException ) : \n        raise InvalidLineException ( line ) \n    record_dict = rr . __dict__ \n    if record_token [ 0 ] == \"TXT\" and len ( record_dict [ 'txt' ] ) == 1 : \n        record_dict [ 'txt' ] = record_dict [ 'txt' ] [ 0 ] \n    record_type = None \n    for key in record_dict . keys ( ) : \n        if key in SUPPORTED_RECORDS and ( key . startswith ( \"$\" ) or record_dict [ key ] == key ) : \n            record_type = key \n            if record_dict [ key ] == key : \n                del record_dict [ key ] \n            break \n    assert record_type is not None , \"Unknown record type in %s\" % rr \n    for field in record_dict . keys ( ) : \n        if record_dict [ field ] is None : \n            del record_dict [ field ] \n    current_origin = record_dict . get ( '$ORIGIN' , parsed_records . get ( '$ORIGIN' , None ) ) \n    if record_type == 'PTR' : \n        record_dict [ 'fullname' ] = record_dict [ 'name' ] + '.' + current_origin \n    if len ( record_dict ) > 0 : \n        if record_type . startswith ( \"$\" ) : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] = record_dict [ record_type ] \n        else : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] . append ( record_dict ) \n    return parsed_records "}
{"7883": "\ndef load_json_dct ( dct , record_store = None , schema = None , loader = from_json_compatible ) : \n    if schema is None : \n        if record_store is None : \n            record_store = auto_store \n        try : \n            schema_name = dct . pop ( SCHEMA_FIELD_NAME ) \n        except KeyError : \n            raise ParseError ( ( \"Serialized record missing '{0}' \" \"record identifier and no schema supplied\" ) . format ( SCHEMA_FIELD_NAME ) ) \n        try : \n            schema = record_store . get ( schema_name ) \n        except KeyError : \n            raise ParseError ( \"Can't recognize record type %r\" % ( schema_name , ) , schema_name ) \n    else : \n        if SCHEMA_FIELD_NAME in dct : \n            dct . pop ( SCHEMA_FIELD_NAME ) \n    record = loader ( schema , dct ) \n    return record "}
{"7900": "\ndef escape_filter ( o ) : \n    if o is None : \n        return u'NULL' \n    if isinstance ( o , int ) : \n        return str ( o ) \n    if not isinstance ( o , six . string_types ) : \n        raise ValueError ( 'Filters take only None, int or a string type' ) \n    if not o : \n        return u\"''\" \n    o = unicode_process ( o ) \n    if u'\"' not in o : \n        return u'\"' + o + u'\"' \n    else : \n        if u\"'\" not in o : \n            return u\"'\" + o + u\"'\" \n        else : \n            first_char = o [ 0 ] \n            last_char = o [ - 1 ] \n            if first_char in QUOTES and last_char in QUOTES : \n                if first_char == last_char : \n                    quote = give_another_quote ( first_char ) \n                    return quote + o + quote \n                else : \n                    return u\"'\" + o + u\"'\" \n            else : \n                if first_char not in QUOTES and last_char not in QUOTES : \n                    return u\"'\" + o + u\"'\" \n                else : \n                    if first_char in QUOTES : \n                        quote = give_another_quote ( first_char ) \n                    else : \n                        quote = give_another_quote ( last_char ) \n                    return quote + o + quote "}
{"7901": "\ndef elementaryRotationMatrix ( axis , rotationAngle ) : \n    if ( axis == \"x\" or axis == \"X\" ) : \n        return array ( [ [ 1.0 , 0.0 , 0.0 ] , [ 0.0 , cos ( rotationAngle ) , sin ( rotationAngle ) ] , [ 0.0 , - sin ( rotationAngle ) , cos ( rotationAngle ) ] ] ) \n    else : \n        if ( axis == \"y\" or axis == \"Y\" ) : \n            return array ( [ [ cos ( rotationAngle ) , 0.0 , - sin ( rotationAngle ) ] , [ 0.0 , 1.0 , 0.0 ] , [ sin ( rotationAngle ) , 0.0 , cos ( rotationAngle ) ] ] ) \n        else : \n            if ( axis == \"z\" or axis == \"Z\" ) : \n                return array ( [ [ cos ( rotationAngle ) , sin ( rotationAngle ) , 0.0 ] , [ - sin ( rotationAngle ) , cos ( rotationAngle ) , 0.0 ] , [ 0.0 , 0.0 , 1.0 ] ] ) \n            else : \n                raise Exception ( \"Unknown rotation axis \" + axis + \"!\" ) "}
{"7907": "\ndef makePlot ( args ) : \n    gmag = np . linspace ( 3.0 , 20.0 , 171 ) \n    vmini = args [ 'vmini' ] \n    vmag = gmag - gminvFromVmini ( vmini ) \n    if args [ 'eom' ] : \n        sigmaG = gMagnitudeErrorEoM ( gmag ) \n        sigmaGBp = bpMagnitudeErrorEoM ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeErrorEoM ( gmag , vmini ) \n        yminmax = ( 1.0 - 4 , 0.1 ) \n    else : \n        sigmaG = gMagnitudeError ( gmag ) \n        sigmaGBp = bpMagnitudeError ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeError ( gmag , vmini ) \n        yminmax = ( 1.0 - 4 , 1 ) \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    if ( args [ 'vmagAbscissa' ] ) : \n        plt . semilogy ( vmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( vmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( vmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6 , 20 ) ) \n        plt . legend ( loc = 0 ) \n        plt . xlabel ( '$V$ [mag]' ) \n    else : \n        ax = fig . add_subplot ( 111 ) \n        plt . semilogy ( gmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( gmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( gmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6 , 20 ) ) \n        plt . legend ( loc = 0 ) \n        plt . xlabel ( '$G$ [mag]' ) \n    plt . xticks ( np . arange ( 6 , 20 , 2 ) ) \n    ax = plt . gca ( ) . yaxis \n    plt . grid ( which = 'both' ) \n    plt . ylabel ( 'Photometric error [mag]' ) \n    if args [ 'eom' ] : \n        plt . title ( 'End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14 ) \n    else : \n        plt . title ( 'Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14 ) \n    basename = 'PhotometricErrors' \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( basename + '.pdf' ) \n    else : \n        if ( args [ 'pngOutput' ] ) : \n            plt . savefig ( basename + '.png' ) \n        else : \n            plt . show ( ) "}
{"7914": "\ndef makePlot ( pdf = False , png = False ) : \n    logdistancekpc = np . linspace ( - 1 , np . log10 ( 20.0 ) , 100 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    lines = { } \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ 0 ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( vmag > 14 ) & ( vmag < 16 ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ 1 ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ 1 ] ) \n        relparerrors = parerrors * 10 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10 ** logdistancekpc , relparerrors , '--k' , lw = 1 ) \n        plt . loglog ( 10 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    else : \n        if ( args [ 'pngOutput' ] ) : \n            plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n        else : \n            plt . show ( ) "}
{"7915": "\ndef makePlot ( args ) : \n    gRvs = np . linspace ( 5.7 , 16.1 , 101 ) \n    spts = [ 'B0V' , 'B5V' , 'A0V' , 'A5V' , 'F0V' , 'G0V' , 'G5V' , 'K0V' , 'K1IIIMP' , 'K4V' , 'K1III' ] \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    deltaHue = 240.0 / ( len ( spts ) - 1 ) \n    hsv = np . zeros ( ( 1 , 1 , 3 ) ) \n    hsv [ 0 , 0 , 1 ] = 1.0 \n    hsv [ 0 , 0 , 2 ] = 0.9 \n    count = 0 \n    for spt in spts : \n        hsv [ 0 , 0 , 0 ] = ( 240 - count * deltaHue ) / 360.0 \n        vmag = vminGrvsFromVmini ( vminiFromSpt ( spt ) ) + gRvs \n        vradErrors = vradErrorSkyAvg ( vmag , spt ) \n        plt . plot ( vmag , vradErrors , '-' , label = spt , color = hsv_to_rgb ( hsv ) [ 0 , 0 , : ] ) \n        count += 1 \n    plt . grid ( which = 'both' ) \n    plt . xlim ( 9 , 17.5 ) \n    plt . ylim ( 0 , 20 ) \n    plt . xticks ( np . arange ( 9 , 18 , 1 ) ) \n    plt . yticks ( np . arange ( 0 , 20.5 , 5 ) ) \n    plt . xlabel ( '$V$ [mag]' ) \n    plt . ylabel ( 'End-of-mission radial velocity error [km s$^{-1}$]' ) \n    leg = plt . legend ( loc = 0 , handlelength = 2.0 , labelspacing = 0.10 ) \n    for t in leg . get_texts ( ) : \n        t . set_fontsize ( 12 ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.pdf' ) \n    else : \n        if ( args [ 'pngOutput' ] ) : \n            plt . savefig ( 'RadialVelocityErrors.png' ) \n        else : \n            plt . show ( ) "}
{"7929": "\ndef eventstr ( event_tuple = None , event = None , register = None , parameters = None ) : \n    if len ( event_tuple ) == 3 : \n        event , register , parameters = event_tuple \n    else : \n        if len ( event_tuple ) == 2 : \n            event , register = event_tuple \n    event_dscr = [ event , register ] \n    if parameters : \n        for k , v in sorted ( event_tuple [ 2 ] . items ( ) ) : \n            if type ( v ) is int : \n                k += \"={}\" . format ( hex ( v ) ) \n            event_dscr . append ( k ) \n    return \":\" . join ( event_dscr ) "}
{"7933": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and self . _args . verbose > 2 : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        print ( \"{}D layer condition:\" . format ( dimension ) , file = output_file ) \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            print ( cache + \": \" , end = '' , file = output_file ) \n            if lc_solution [ 'lt' ] is sympy . true : \n                print ( \"unconditionally fulfilled\" , file = output_file ) \n            else : \n                if lc_solution [ 'eq' ] is None : \n                    print ( \"{}\" . format ( lc_solution [ 'lt' ] ) , file = output_file ) \n                else : \n                    if type ( lc_solution [ 'eq' ] ) is not list : \n                        print ( \"{}\" . format ( lc_solution [ 'eq' ] ) , file = output_file ) \n                    else : \n                        for solu in lc_solution [ 'eq' ] : \n                            for s , v in solu . items ( ) : \n                                print ( \"{} <= {}\" . format ( s , v ) , file = output_file ) "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while idx < len ( code ) - 1 : \n            if comment_start is None and code [ idx : idx + 2 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            else : \n                if comment_start is None and code [ idx : idx + 2 ] == '/*' : \n                    comment_start = idx \n                else : \n                    if comment_start is not None and code [ idx : idx + 2 ] == '*/' : \n                        code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                        idx -= idx - comment_start \n                        comment_start = None \n            idx += 1 \n    return code "}
{"7938": "\ndef calculate_cycles ( self ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    elements_per_cacheline = float ( self . machine [ 'cacheline size' ] ) // element_size \n    iterations_per_cacheline = ( sympy . Integer ( self . machine [ 'cacheline size' ] ) / sympy . Integer ( self . kernel . bytes_per_iteration ) ) \n    self . results [ 'iterations per cacheline' ] = iterations_per_cacheline \n    cacheline_size = float ( self . machine [ 'cacheline size' ] ) \n    loads , stores = ( self . predictor . get_loads ( ) , self . predictor . get_stores ( ) ) \n    for cache_level , cache_info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) [ 1 : ] : \n        throughput , duplexness = cache_info [ 'non-overlap upstream throughput' ] \n        if type ( throughput ) is str and throughput == 'full socket memory bandwidth' : \n            read_streams = loads [ cache_level ] \n            write_streams = stores [ cache_level ] \n            threads_per_core = 1 \n            bw , measurement_kernel = self . machine . get_bandwidth ( cache_level , read_streams , write_streams , threads_per_core ) \n            if duplexness == 'half-duplex' : \n                cycles = float ( loads [ cache_level ] + stores [ cache_level ] ) * float ( elements_per_cacheline ) * float ( element_size ) * float ( self . machine [ 'clock' ] ) / float ( bw ) \n            else : \n                raise NotImplementedError ( \"full-duplex mode is not (yet) supported for memory transfers.\" ) \n            if 'penalty cycles per read stream' in cache_info : \n                cycles += stores [ cache_level ] * cache_info [ 'penalty cycles per read stream' ] \n            self . results . update ( { 'memory bandwidth kernel' : measurement_kernel , 'memory bandwidth' : bw } ) \n        else : \n            throughput = float ( throughput ) / cacheline_size \n            if duplexness == 'half-duplex' : \n                cycles = ( loads [ cache_level ] + stores [ cache_level ] ) / float ( throughput ) \n            else : \n                if duplexness == 'full-duplex' : \n                    cycles = max ( loads [ cache_level ] / float ( throughput ) , stores [ cache_level ] / float ( throughput ) ) \n                else : \n                    raise ValueError ( \"Duplexness of cache throughput may only be 'half-duplex'\" \"or 'full-duplex', found {} in {}.\" . format ( duplexness , cache_info [ 'name' ] ) ) \n        self . results [ 'cycles' ] . append ( ( cache_info [ 'level' ] , cycles ) ) \n        self . results [ cache_info [ 'level' ] ] = cycles \n    return self . results "}
{"7947": "\ndef iaca_instrumentation ( input_file , output_file , block_selection = 'auto' , pointer_increment = 'auto_with_manual_fallback' , debug = False ) : \n    assembly_orig = input_file . readlines ( ) \n    if input_file is output_file : \n        output_file . seek ( 0 ) \n        output_file . truncate ( ) \n    if debug : \n        block_selection = 'manual' \n    assembly = strip_and_uncomment ( copy ( assembly_orig ) ) \n    assembly = strip_unreferenced_labels ( assembly ) \n    blocks = find_asm_blocks ( assembly ) \n    if block_selection == 'auto' : \n        block_idx = select_best_block ( blocks ) \n    else : \n        if block_selection == 'manual' : \n            block_idx = userselect_block ( blocks , default = select_best_block ( blocks ) , debug = debug ) \n        else : \n            if isinstance ( block_selection , int ) : \n                block_idx = block_selection \n            else : \n                raise ValueError ( \"block_selection has to be an integer, 'auto' or 'manual' \" ) \n    block = blocks [ block_idx ] [ 1 ] \n    if pointer_increment == 'auto' : \n        if block [ 'pointer_increment' ] is None : \n            raise RuntimeError ( \"pointer_increment could not be detected automatically. Use \" \"--pointer-increment to set manually to byte offset of store \" \"pointer address between consecutive assembly block iterations.\" ) \n    else : \n        if pointer_increment == 'auto_with_manual_fallback' : \n            if block [ 'pointer_increment' ] is None : \n                block [ 'pointer_increment' ] = userselect_increment ( block ) \n        else : \n            if pointer_increment == 'manual' : \n                block [ 'pointer_increment' ] = userselect_increment ( block ) \n            else : \n                if isinstance ( pointer_increment , int ) : \n                    block [ 'pointer_increment' ] = pointer_increment \n                else : \n                    raise ValueError ( \"pointer_increment has to be an integer, 'auto', 'manual' or  \" \"'auto_with_manual_fallback' \" ) \n    instrumented_asm = insert_markers ( assembly_orig , block [ 'first_line' ] , block [ 'last_line' ] ) \n    output_file . writelines ( instrumented_asm ) \n    return block "}
{"7958": "\ndef find_node_type ( ast , node_type ) : \n    if type ( ast ) is node_type : \n        return [ ast ] \n    else : \n        if type ( ast ) is list : \n            return reduce ( operator . add , list ( map ( lambda a : find_node_type ( a , node_type ) , ast ) ) , [ ] ) \n        else : \n            if ast is None : \n                return [ ] \n            else : \n                return reduce ( operator . add , [ find_node_type ( o [ 1 ] , node_type ) for o in ast . children ( ) ] , [ ] ) "}
{"7979": "\ndef conv_ast_to_sym ( self , math_ast ) : \n    if type ( math_ast ) is c_ast . ID : \n        return symbol_pos_int ( math_ast . name ) \n    else : \n        if type ( math_ast ) is c_ast . Constant : \n            return sympy . Integer ( math_ast . value ) \n        else : \n            op = { '*' : operator . mul , '+' : operator . add , '-' : operator . sub } \n            return op [ math_ast . op ] ( self . conv_ast_to_sym ( math_ast . left ) , self . conv_ast_to_sym ( math_ast . right ) ) "}
{"7981": "\ndef _get_basename ( cls , aref ) : \n    if isinstance ( aref . name , c_ast . ArrayRef ) : \n        return cls . _get_basename ( aref . name ) \n    else : \n        if isinstance ( aref . name , str ) : \n            return aref . name \n        else : \n            return aref . name . name "}
{"7991": "\ndef _build_scalar_declarations ( self , with_init = True ) : \n    scalar_declarations = [ deepcopy ( d ) for d in self . kernel_ast . block_items if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . TypeDecl ] \n    if with_init : \n        random . seed ( 2342 ) \n        for d in scalar_declarations : \n            if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : \n                d . init = c_ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) \n            else : \n                if d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : \n                    d . init = c_ast . Constant ( 'int' , 2 ) \n    return scalar_declarations "}
{"7997": "\ndef string_to_sympy ( cls , s ) : \n    if isinstance ( s , int ) : \n        return sympy . Integer ( s ) \n    else : \n        if isinstance ( s , list ) : \n            return tuple ( [ cls . string_to_sympy ( e ) for e in s ] ) \n        else : \n            if s is None : \n                return None \n            else : \n                local_dict = { c : symbol_pos_int ( c ) for c in s if c in string . ascii_letters } \n                preliminary_expr = parse_expr ( s , local_dict = local_dict ) \n                local_dict . update ( { s . name : symbol_pos_int ( s . name ) for s in preliminary_expr . free_symbols } ) \n                return parse_expr ( s , local_dict = local_dict ) "}
{"8006": "\ndef _align_iteration_with_cl_boundary ( self , iteration , subtract = True ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    cacheline_size = self . machine [ 'cacheline size' ] \n    elements_per_cacheline = int ( cacheline_size // element_size ) \n    inner_loop = list ( self . kernel . get_loop_stack ( subs_consts = True ) ) [ - 1 ] \n    inner_increment = inner_loop [ 'increment' ] \n    o = self . kernel . compile_global_offsets ( iteration = iteration ) [ 0 ] \n    if len ( o [ 1 ] ) : \n        first_offset = min ( o [ 1 ] ) \n    else : \n        first_offset = min ( o [ 0 ] ) \n    diff = first_offset - ( int ( first_offset ) >> self . csim . first_level . cl_bits << self . csim . first_level . cl_bits ) \n    if diff == 0 : \n        return iteration \n    else : \n        if subtract : \n            return iteration - ( diff // element_size ) // inner_increment \n        else : \n            return iteration + ( elements_per_cacheline - diff // element_size ) // inner_increment "}
{"8013": "\ndef fix_env_variable ( name , value ) : \n    orig = os . environ . get ( name , None ) \n    if value is not None : \n        os . environ [ name ] = value \n    else : \n        if name in os . environ : \n            del os . environ [ name ] \n    try : \n        yield \n    finally : \n        if orig is not None : \n            os . environ [ name ] = orig \n        else : \n            if name in os . environ : \n                del os . environ [ name ] "}
{"8016": "\ndef parse_description ( ) : \n    from os . path import dirname , join , exists \n    readme_fpath = join ( dirname ( __file__ ) , 'README.md' ) \n    if exists ( readme_fpath ) : \n        textlines = [ ] \n        with open ( readme_fpath , 'r' ) as f : \n            capture = False \n            for line in f . readlines ( ) : \n                if '# Purpose' in line : \n                    capture = True \n                else : \n                    if line . startswith ( '##' ) : \n                        break \n                    else : \n                        if capture : \n                            textlines += [ line ] \n        text = '' . join ( textlines ) . strip ( ) \n        text = text . replace ( '\\n\\n' , '_NLHACK_' ) \n        text = text . replace ( '\\n' , ' ' ) \n        text = text . replace ( '_NLHACK_' , '\\n\\n' ) \n        return text \n    return '' "}
{"8022": "\ndef _update_unenrolled_list ( sailthru_client , email , course_url , unenroll ) : \n    try : \n        sailthru_response = sailthru_client . api_get ( \"user\" , { \"id\" : email , \"fields\" : { \"vars\" : 1 } } ) \n        if not sailthru_response . is_ok ( ) : \n            error = sailthru_response . get_error ( ) \n            logger . error ( \"Error attempting to read user record from Sailthru: %s\" , error . get_message ( ) ) \n            return not can_retry_sailthru_request ( error ) \n        response_json = sailthru_response . json \n        unenroll_list = [ ] \n        if response_json and \"vars\" in response_json and response_json [ \"vars\" ] and \"unenrolled\" in response_json [ \"vars\" ] : \n            unenroll_list = response_json [ \"vars\" ] [ \"unenrolled\" ] \n        changed = False \n        if unenroll : \n            if course_url not in unenroll_list : \n                unenroll_list . append ( course_url ) \n                changed = True \n        else : \n            if course_url in unenroll_list : \n                unenroll_list . remove ( course_url ) \n                changed = True \n        if changed : \n            sailthru_response = sailthru_client . api_post ( 'user' , { 'id' : email , 'key' : 'email' , 'vars' : { 'unenrolled' : unenroll_list } } ) \n            if not sailthru_response . is_ok ( ) : \n                error = sailthru_response . get_error ( ) \n                logger . error ( \"Error attempting to update user record in Sailthru: %s\" , error . get_message ( ) ) \n                return not can_retry_sailthru_request ( error ) \n        return True \n    except SailthruClientError as exc : \n        logger . exception ( \"Exception attempting to update user record for %s in Sailthru - %s\" , email , text_type ( exc ) ) \n        return False "}
{"8062": "\ndef update_agent_state ( ) : \n    configure_service ( 'capture.admin' ) \n    status = 'idle' \n    if get_service_status ( db . Service . SCHEDULE ) == db . ServiceStatus . STOPPED : \n        status = 'offline' \n    else : \n        if get_service_status ( db . Service . CAPTURE ) == db . ServiceStatus . BUSY : \n            status = 'capturing' \n        else : \n            if get_service_status ( db . Service . INGEST ) == db . ServiceStatus . BUSY : \n                status = 'uploading' \n    register_ca ( status = status ) "}
{"8081": "\ndef get_config_params ( properties ) : \n    param = [ ] \n    wdef = '' \n    for prop in properties . split ( '\\n' ) : \n        if prop . startswith ( 'org.opencastproject.workflow.config' ) : \n            key , val = prop . split ( '=' , 1 ) \n            key = key . split ( '.' ) [ - 1 ] \n            param . append ( ( key , val ) ) \n        else : \n            if prop . startswith ( 'org.opencastproject.workflow.definition' ) : \n                wdef = prop . split ( '=' , 1 ) [ - 1 ] \n    return wdef , param "}
{"8082": "\ndef ingest ( event ) : \n    set_service_status ( Service . INGEST , ServiceStatus . BUSY ) \n    notify . notify ( 'STATUS=Uploading' ) \n    recording_state ( event . uid , 'uploading' ) \n    update_event_status ( event , Status . UPLOADING ) \n    service = config ( 'service-ingest' ) \n    service = service [ randrange ( 0 , len ( service ) ) ] \n    logger . info ( 'Selecting ingest service to use: ' + service ) \n    logger . info ( 'Creating new mediapackage' ) \n    mediapackage = http_request ( service + '/createMediaPackage' ) \n    prop = 'org.opencastproject.capture.agent.properties' \n    dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' \n    for attachment in event . get_data ( ) . get ( 'attach' ) : \n        data = attachment . get ( 'data' ) \n        if attachment . get ( 'x-apple-filename' ) == prop : \n            workflow_def , workflow_config = get_config_params ( data ) \n        else : \n            if attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : \n                name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] \n                logger . info ( 'Adding %s DC catalog' % name ) \n                fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublinCore' , data . encode ( 'utf-8' ) ) ] \n                mediapackage = http_request ( service + '/addDCCatalog' , fields ) \n    for ( flavor , track ) in event . get_tracks ( ) : \n        logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) \n        track = track . encode ( 'ascii' , 'ignore' ) \n        fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM_FILE , track ) ) ] \n        mediapackage = http_request ( service + '/addTrack' , fields ) \n    logger . info ( 'Ingest recording' ) \n    fields = [ ( 'mediaPackage' , mediapackage ) ] \n    if workflow_def : \n        fields . append ( ( 'workflowDefinitionId' , workflow_def ) ) \n    if event . uid : \n        fields . append ( ( 'workflowInstanceId' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) \n    fields += workflow_config \n    mediapackage = http_request ( service + '/ingest' , fields ) \n    recording_state ( event . uid , 'upload_finished' ) \n    update_event_status ( event , Status . FINISHED_UPLOADING ) \n    notify . notify ( 'STATUS=Running' ) \n    set_service_status_immediate ( Service . INGEST , ServiceStatus . IDLE ) \n    logger . info ( 'Finished ingest' ) "}
{"8093": "\ndef resource_to_html ( resource ) : \n    if resource . mimetype == \"text/css\" : \n        if resource . kind == \"text\" : \n            return u\"<style type='text/css'>\\n%s\\n</style>\" % resource . data \n        else : \n            if resource . kind == \"url\" : \n                return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource . data \n            else : \n                raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) \n    else : \n        if resource . mimetype == \"application/javascript\" : \n            if resource . kind == \"text\" : \n                return u\"<script>\\n%s\\n</script>\" % resource . data \n            else : \n                if resource . kind == \"url\" : \n                    return u\"<script src='%s' type='application/javascript'></script>\" % resource . data \n                else : \n                    raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) \n        else : \n            if resource . mimetype == \"text/html\" : \n                assert resource . kind == \"text\" \n                return resource . data \n            else : \n                raise Exception ( \"Unrecognized mimetype %r\" % resource . mimetype ) "}
{"8098": "\ndef to_one_dim_array ( values , as_type = None ) : \n    if isinstance ( values , ( list , tuple ) ) : \n        values = np . array ( values , dtype = np . float32 ) \n    else : \n        if isinstance ( values , pd . Series ) : \n            values = values . values \n    values = values . flatten ( ) \n    assert values . ndim == 1 , \"values has wrong dimension\" \n    if as_type is not None : \n        return values . astype ( as_type ) \n    return values "}
{"8121": "\ndef create_dataset ( self , name , shape = None , dtype = None , data = None , sparse_format = None , indptr_dtype = np . int64 , indices_dtype = np . int32 , ** kwargs ) : \n    if isinstance ( data , Dataset ) : \n        assert sparse_format is None \n        group = self . create_group ( name ) \n        group . attrs [ 'h5sparse_format' ] = data . attrs [ 'h5sparse_format' ] \n        group . attrs [ 'h5sparse_shape' ] = data . attrs [ 'h5sparse_shape' ] \n        group . create_dataset ( 'data' , data = data . h5py_group [ 'data' ] , dtype = dtype , ** kwargs ) \n        group . create_dataset ( 'indices' , data = data . h5py_group [ 'indices' ] , dtype = indices_dtype , ** kwargs ) \n        group . create_dataset ( 'indptr' , data = data . h5py_group [ 'indptr' ] , dtype = indptr_dtype , ** kwargs ) \n    else : \n        if ss . issparse ( data ) : \n            if sparse_format is not None : \n                format_class = get_format_class ( sparse_format ) \n                data = format_class ( data ) \n            group = self . create_group ( name ) \n            group . attrs [ 'h5sparse_format' ] = get_format_str ( data ) \n            group . attrs [ 'h5sparse_shape' ] = data . shape \n            group . create_dataset ( 'data' , data = data . data , dtype = dtype , ** kwargs ) \n            group . create_dataset ( 'indices' , data = data . indices , dtype = indices_dtype , ** kwargs ) \n            group . create_dataset ( 'indptr' , data = data . indptr , dtype = indptr_dtype , ** kwargs ) \n        else : \n            if data is None and sparse_format is not None : \n                format_class = get_format_class ( sparse_format ) \n                if dtype is None : \n                    dtype = np . float64 \n                if shape is None : \n                    shape = ( 0 , 0 ) \n                data = format_class ( shape , dtype = dtype ) \n                group = self . create_group ( name ) \n                group . attrs [ 'h5sparse_format' ] = get_format_str ( data ) \n                group . attrs [ 'h5sparse_shape' ] = data . shape \n                group . create_dataset ( 'data' , data = data . data , dtype = dtype , ** kwargs ) \n                group . create_dataset ( 'indices' , data = data . indices , dtype = indices_dtype , ** kwargs ) \n                group . create_dataset ( 'indptr' , data = data . indptr , dtype = indptr_dtype , ** kwargs ) \n            else : \n                assert sparse_format is None \n                return super ( Group , self ) . create_dataset ( name , data = data , shape = shape , dtype = dtype , ** kwargs ) \n    return Dataset ( group ) "}
{"8133": "\ndef _stdout_filed ( func ) : \n    def wrapper ( self , file = None ) : \n        if file : \n            return func ( self , file = file ) \n        else : \n            if self . io_manager : \n                with self . io_manager . with_stdout ( ) as stdout : \n                    return func ( self , file = stdout ) \n            else : \n                return func ( self , file = sys . stdout ) \n    wrapper . __doc__ = func . __doc__ \n    return wrapper "}
{"8134": "\ndef _stderr_filed ( func ) : \n    def wrapper ( self , msg , file = None ) : \n        if file : \n            return func ( self , msg , file = file ) \n        else : \n            if self . io_manager : \n                with self . io_manager . with_stderr ( ) as stderr : \n                    return func ( self , msg , file = stderr ) \n            else : \n                return func ( self , msg , file = sys . stderr ) \n    wrapper . __doc__ = func . __doc__ \n    return wrapper "}
{"8152": "\ndef quote ( value , safe = '/:' ) : \n    if isinstance ( value , six . text_type ) : \n        value = value . encode ( 'utf8' ) \n    else : \n        if not isinstance ( value , six . string_types ) : \n            value = str ( value ) \n    return parse . quote ( value , safe ) "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = Crypto . Random . new ( ) . read ( 16 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = True \n    left = None \n    if content_length is not None and content_length >= 0 : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and size > left : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and left > 0 : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16 ) \n            break \n        if left is not None : \n            left -= len ( chunk ) \n            if left <= 0 : \n                reading = False \n        block = chunk \n        trailing = len ( block ) % 16 \n        while trailing : \n            size = 16 - trailing \n            if left is not None and size > left : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and left > 0 : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = False \n                chunk = chr ( trailing ) * ( 16 - trailing ) \n            else : \n                if left is not None : \n                    left -= len ( chunk ) \n                    if left <= 0 : \n                        reading = False \n            block += chunk \n            trailing = len ( block ) % 16 \n        yield encryptor . encrypt ( block ) "}
{"8164": "\ndef cli_help ( context , command_name , general_parser , command_parsers ) : \n    if command_name == 'for' : \n        command_name = 'fordo' \n    with context . io_manager . with_stdout ( ) as stdout : \n        if not command_name : \n            general_parser . print_help ( stdout ) \n        else : \n            if command_name in command_parsers : \n                command_parsers [ command_name ] . option_parser . print_help ( stdout ) \n            else : \n                raise ReturnCode ( 'unknown command %r' % command_name ) "}
{"8186": "\ndef create_index ( idx_url , clean = False ) : \n    try : \n        r = requests . get ( idx_url ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s)\" % idx_url \n        raise ElasticSearchError ( cause = cause ) \n    if r . status_code != 200 : \n        r = requests . put ( idx_url ) \n        if r . status_code != 200 : \n            logger . info ( \"Can't create index %s (%s)\" , idx_url , r . status_code ) \n            cause = \"Error creating Elastic Search index %s\" % idx_url \n            raise ElasticSearchError ( cause = cause ) \n        logger . info ( \"Index %s created\" , idx_url ) \n        return True \n    else : \n        if r . status_code == 200 and clean : \n            requests . delete ( idx_url ) \n            requests . put ( idx_url ) \n            logger . info ( \"Index deleted and created (index: %s)\" , idx_url ) \n            return True \n    return False "}
{"8199": "\ndef listen ( self ) : \n    pubsub = self . conn . pubsub ( ) \n    pubsub . subscribe ( self . pubsub_channel ) \n    logger . debug ( \"Listening on channel %s\" , self . pubsub_channel ) \n    for msg in pubsub . listen ( ) : \n        logger . debug ( \"New message received of type %s\" , str ( msg [ 'type' ] ) ) \n        if msg [ 'type' ] != 'message' : \n            logger . debug ( \"Ignoring job message\" ) \n            continue \n        data = pickle . loads ( msg [ 'data' ] ) \n        job_id = data [ 'job_id' ] \n        job = rq . job . Job . fetch ( job_id , connection = self . conn ) \n        if data [ 'status' ] == 'finished' : \n            logging . debug ( \"Job #%s completed\" , job_id ) \n            handler = self . result_handler \n        else : \n            if data [ 'status' ] == 'failed' : \n                logging . debug ( \"Job #%s failed\" , job_id ) \n                handler = self . result_handler_err \n            else : \n                continue \n        if handler : \n            logging . debug ( \"Calling handler for job #%s\" , job_id ) \n            handler ( job ) "}
{"8221": "\ndef parse_bool ( value ) : \n    boolean = parse_str ( value ) . capitalize ( ) \n    if boolean in ( \"True\" , \"Yes\" , \"On\" , \"1\" ) : \n        return True \n    else : \n        if boolean in ( \"False\" , \"No\" , \"Off\" , \"0\" ) : \n            return False \n        else : \n            raise ValueError ( 'Unable to parse boolean value \"{}\"' . format ( value ) ) "}
{"8225": "\ndef from_model ( cls , model , * fields , ** named_fields ) : \n    d = ModelDict ( ) \n    if not ( fields or named_fields ) : \n        fields = [ f . attname for f in model . _meta . concrete_fields ] \n    not_found = object ( ) \n    for name , field in chain ( zip ( fields , fields ) , named_fields . items ( ) ) : \n        _fields = field . split ( \"__\" ) \n        value = model \n        for i , _field in enumerate ( _fields , start = 1 ) : \n            previous_value = value \n            value = getattr ( previous_value , _field , not_found ) \n            if value is not_found : \n                if _field in dir ( previous_value ) : \n                    raise ValueError ( \"{!r}.{} had an AttributeError exception\" . format ( previous_value , _field ) ) \n                else : \n                    raise AttributeError ( \"{!r} does not have {!r} attribute\" . format ( previous_value , _field ) ) \n            else : \n                if value is None : \n                    if name not in named_fields : \n                        name = \"__\" . join ( _fields [ : i ] ) \n                    break \n        d [ name ] = value \n    return d "}
{"8243": "\ndef check_log_config ( self ) : \n    if self . report_progress : \n        if self . report_progress is True : \n            self . report_progress = ( 5 , 'pypet' , logging . INFO ) \n        else : \n            if isinstance ( self . report_progress , ( int , float ) ) : \n                self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) \n            else : \n                if isinstance ( self . report_progress , str ) : \n                    self . report_progress = ( 5 , self . report_progress , logging . INFO ) \n                else : \n                    if len ( self . report_progress ) == 2 : \n                        self . report_progress = ( self . report_progress [ 0 ] , self . report_progress [ 1 ] , logging . INFO ) \n    if self . log_config : \n        if self . log_config == pypetconstants . DEFAULT_LOGGING : \n            pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n            init_path = os . path . join ( pypet_path , 'logging' ) \n            self . log_config = os . path . join ( init_path , 'default.ini' ) \n        if isinstance ( self . log_config , str ) : \n            if not os . path . isfile ( self . log_config ) : \n                raise ValueError ( 'Could not find the logger init file ' '`%s`.' % self . log_config ) \n            parser = NoInterpolationParser ( ) \n            parser . read ( self . log_config ) \n        else : \n            if isinstance ( self . log_config , cp . RawConfigParser ) : \n                parser = self . log_config \n            else : \n                parser = None \n        if parser is not None : \n            self . _sp_config = self . _parser_to_string_io ( parser ) \n            self . _mp_config = self . _find_multiproc_options ( parser ) \n            if self . _mp_config is not None : \n                self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) \n        else : \n            if isinstance ( self . log_config , dict ) : \n                self . _sp_config = self . log_config \n                self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) \n    if self . log_stdout : \n        if self . log_stdout is True : \n            self . log_stdout = ( 'STDOUT' , logging . INFO ) \n        if isinstance ( self . log_stdout , str ) : \n            self . log_stdout = ( self . log_stdout , logging . INFO ) \n        if isinstance ( self . log_stdout , int ) : \n            self . log_stdout = ( 'STDOUT' , self . log_stdout ) "}
{"8245": "\ndef _handle_dict_config ( self , log_config ) : \n    new_dict = dict ( ) \n    for key in log_config . keys ( ) : \n        if key == 'filename' : \n            filename = log_config [ key ] \n            filename = rename_log_file ( filename , env_name = self . env_name , traj_name = self . traj_name , set_name = self . set_name , run_name = self . run_name ) \n            new_dict [ key ] = filename \n            try_make_dirs ( filename ) \n        else : \n            if isinstance ( log_config [ key ] , dict ) : \n                inner_dict = self . _handle_dict_config ( log_config [ key ] ) \n                new_dict [ key ] = inner_dict \n            else : \n                new_dict [ key ] = log_config [ key ] \n    return new_dict "}
{"8256": "\ndef retry ( n , errors , wait = 0.0 , logger_name = None ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( * args , ** kwargs ) : \n            retries = 0 \n            while True : \n                try : \n                    result = func ( * args , ** kwargs ) \n                    if retries and logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'Retry of `%s` successful' % func . __name__ ) \n                    return result \n                except errors : \n                    if retries >= n : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                        raise \n                    else : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                    retries += 1 \n                    if wait : \n                        time . sleep ( wait ) \n        return new_func \n    return wrapper "}
{"8264": "\ndef storage_factory ( storage_service , trajectory = None , ** kwargs ) : \n    if 'filename' in kwargs and storage_service is None : \n        filename = kwargs [ 'filename' ] \n        _ , ext = os . path . splitext ( filename ) \n        if ext in ( '.hdf' , '.h4' , '.hdf4' , '.he2' , '.h5' , '.hdf5' , '.he5' ) : \n            storage_service = HDF5StorageService \n        else : \n            raise ValueError ( 'Extension `%s` of filename `%s` not understood.' % ( ext , filename ) ) \n    else : \n        if isinstance ( storage_service , str ) : \n            class_name = storage_service . split ( '.' ) [ - 1 ] \n            storage_service = create_class ( class_name , [ storage_service , HDF5StorageService ] ) \n    if inspect . isclass ( storage_service ) : \n        return _create_storage ( storage_service , trajectory , ** kwargs ) \n    else : \n        return storage_service , set ( kwargs . keys ( ) ) "}
{"8265": "\ndef add_parameters ( traj ) : \n    traj . f_add_parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) \n    traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) \n    traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) \n    traj . f_add_parameter ( 'diff_name' , 'diff_lorenz' , comment = 'Name of our differential equation' ) \n    if traj . diff_name == 'diff_lorenz' : \n        traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) \n        traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) \n        traj . f_add_parameter ( 'func_params.rho' , 28.0 ) \n    else : \n        if traj . diff_name == 'diff_roessler' : \n            traj . f_add_parameter ( 'func_params.a' , 0.1 ) \n            traj . f_add_parameter ( 'func_params.c' , 14.0 ) \n        else : \n            raise ValueError ( 'I don\\'t know what %s is.' % traj . diff_name ) "}
{"8291": "\ndef _node_to_msg ( store_load , node ) : \n    if node . v_is_leaf : \n        if store_load == STORE : \n            return pypetconstants . LEAF \n        else : \n            if store_load == LOAD : \n                return pypetconstants . LEAF \n            else : \n                if store_load == REMOVE : \n                    return pypetconstants . DELETE \n    else : \n        if store_load == STORE : \n            return pypetconstants . GROUP \n        else : \n            if store_load == LOAD : \n                return pypetconstants . GROUP \n            else : \n                if store_load == REMOVE : \n                    return pypetconstants . DELETE "}
{"8292": "\ndef _remove_subtree ( self , start_node , name , predicate = None ) : \n    def _delete_from_children ( node , child_name ) : \n        del node . _children [ child_name ] \n        if child_name in node . _groups : \n            del node . _groups [ child_name ] \n        else : \n            if child_name in node . _leaves : \n                del node . _leaves [ child_name ] \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n    def _remove_subtree_inner ( node , predicate ) : \n        if not predicate ( node ) : \n            return False \n        else : \n            if node . v_is_group : \n                for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , list ( node . _groups . keys ( ) ) ) : \n                    child_ = node . _children [ name_ ] \n                    child_deleted = _remove_subtree_inner ( child_ , predicate ) \n                    if child_deleted : \n                        _delete_from_children ( node , name_ ) \n                        del child_ \n                for link_ in list ( node . _links . keys ( ) ) : \n                    node . f_remove_link ( link_ ) \n                if len ( node . _children ) == 0 : \n                    self . _delete_node ( node ) \n                    return True \n                else : \n                    return False \n            else : \n                self . _delete_node ( node ) \n                return True \n    if name in start_node . _links : \n        start_node . f_remove_link ( name ) \n    else : \n        child = start_node . _children [ name ] \n        if predicate is None : \n            predicate = lambda x : True \n        if _remove_subtree_inner ( child , predicate ) : \n            _delete_from_children ( start_node , name ) \n            del child \n            return True \n        else : \n            return False "}
{"8293": "\ndef _delete_node ( self , node ) : \n    full_name = node . v_full_name \n    root = self . _root_instance \n    if full_name == '' : \n        return \n    if node . v_is_leaf : \n        if full_name in root . _parameters : \n            del root . _parameters [ full_name ] \n        else : \n            if full_name in root . _config : \n                del root . _config [ full_name ] \n            else : \n                if full_name in root . _derived_parameters : \n                    del root . _derived_parameters [ full_name ] \n                else : \n                    if full_name in root . _results : \n                        del root . _results [ full_name ] \n                    else : \n                        if full_name in root . _other_leaves : \n                            del root . _other_leaves [ full_name ] \n        if full_name in root . _explored_parameters : \n            if root . _stored : \n                root . _explored_parameters [ full_name ] = None \n            else : \n                del root . _explored_parameters [ full_name ] \n            if len ( root . _explored_parameters ) == 0 : \n                root . f_shrink ( ) \n        del self . _flat_leaf_storage_dict [ full_name ] \n    else : \n        del root . _all_groups [ full_name ] \n        if full_name in root . _run_parent_groups : \n            del root . _run_parent_groups [ full_name ] \n    if full_name in root . _linked_by : \n        linking = root . _linked_by [ full_name ] \n        for linking_name in list ( linking . keys ( ) ) : \n            linking_group , link_set = linking [ linking_name ] \n            for link in list ( link_set ) : \n                linking_group . f_remove_link ( link ) \n    if ( node . v_location , node . v_name ) in self . _root_instance . _new_nodes : \n        del self . _root_instance . _new_nodes [ ( node . v_location , node . v_name ) ] \n    self . _remove_from_nodes_and_leaves ( node ) \n    node . _vars = None \n    node . _func = None "}
{"8295": "\ndef _remove_along_branch ( self , actual_node , split_name , recursive = False ) : \n    if len ( split_name ) == 0 : \n        if actual_node . v_is_group and actual_node . f_has_children ( ) : \n            if recursive : \n                for child in list ( actual_node . _children . keys ( ) ) : \n                    actual_node . f_remove_child ( child , recursive = True ) \n            else : \n                raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) \n        self . _delete_node ( actual_node ) \n        return True \n    name = split_name . popleft ( ) \n    if name in actual_node . _links : \n        if len ( split_name ) > 0 : \n            raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) \n        actual_node . f_remove_link ( name ) \n    else : \n        child = actual_node . _children [ name ] \n        if self . _remove_along_branch ( child , split_name , recursive = recursive ) : \n            del actual_node . _children [ name ] \n            if name in actual_node . _groups : \n                del actual_node . _groups [ name ] \n            else : \n                if name in actual_node . _leaves : \n                    del actual_node . _leaves [ name ] \n                else : \n                    raise RuntimeError ( 'You shall not pass!' ) \n            del child \n            return False "}
{"8296": "\ndef _translate_shortcut ( self , name ) : \n    if isinstance ( name , int ) : \n        return True , self . _root_instance . f_wildcard ( '$' , name ) \n    if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2 : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$' , int ( index ) ) \n            else : \n                if index == 'A' : \n                    return True , self . _root_instance . f_wildcard ( '$' , - 1 ) \n    if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2 : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) \n            else : \n                if index == 'A' : \n                    return True , self . _root_instance . f_wildcard ( '$set' , - 1 ) \n    if name in SHORTCUT_SET : \n        if name == 'par' : \n            return True , 'parameters' \n        else : \n            if name == 'dpar' : \n                return True , 'derived_parameters' \n            else : \n                if name == 'res' : \n                    return True , 'results' \n                else : \n                    if name == 'conf' : \n                        return True , 'config' \n                    else : \n                        raise RuntimeError ( 'You shall not pass!' ) \n    return False , name "}
{"8297": "\ndef _add_prefix ( self , split_names , start_node , group_type_name ) : \n    root = self . _root_instance \n    prepend = [ ] \n    if start_node . v_depth < 3 and not group_type_name == GROUP : \n        if start_node . v_depth == 0 : \n            if group_type_name == DERIVED_PARAMETER_GROUP : \n                if split_names [ 0 ] == 'derived_parameters' : \n                    return split_names \n                else : \n                    prepend += [ 'derived_parameters' ] \n            else : \n                if group_type_name == RESULT_GROUP : \n                    if split_names [ 0 ] == 'results' : \n                        return split_names \n                    else : \n                        prepend += [ 'results' ] \n                else : \n                    if group_type_name == CONFIG_GROUP : \n                        if split_names [ 0 ] == 'config' : \n                            return split_names \n                        else : \n                            prepend += [ 'config' ] \n                    else : \n                        if group_type_name == PARAMETER_GROUP : \n                            if split_names [ 0 ] == 'parameters' : \n                                return split_names [ 0 ] \n                            else : \n                                prepend += [ 'parameters' ] \n                        else : \n                            raise RuntimeError ( 'Why are you here?' ) \n        if root . _is_run and root . _auto_run_prepend : \n            dummy = root . f_wildcard ( '$' , - 1 ) \n            crun = root . f_wildcard ( '$' ) \n            if any ( name in root . _run_information for name in split_names ) : \n                pass \n            else : \n                if any ( name == dummy for name in split_names ) : \n                    pass \n                else : \n                    if ( group_type_name == RESULT_GROUP or group_type_name == DERIVED_PARAMETER_GROUP ) : \n                        if start_node . v_depth == 0 : \n                            prepend += [ 'runs' , crun ] \n                        else : \n                            if start_node . v_depth == 1 : \n                                if len ( split_names ) == 1 and split_names [ 0 ] == 'runs' : \n                                    return split_names \n                                else : \n                                    prepend += [ 'runs' , crun ] \n                            else : \n                                if start_node . v_depth == 2 and start_node . v_name == 'runs' : \n                                    prepend += [ crun ] \n    if prepend : \n        split_names = prepend + split_names \n    return split_names "}
{"8299": "\ndef _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , add_prefix = True , check_naming = True ) : \n    args = list ( args ) \n    create_new = True \n    name = '' \n    instance = None \n    constructor = None \n    add_link = type_name == LINK \n    if add_link : \n        name = args [ 0 ] \n        instance = args [ 1 ] \n        create_new = False \n    else : \n        if len ( args ) == 1 and len ( kwargs ) == 0 : \n            item = args [ 0 ] \n            try : \n                name = item . v_full_name \n                instance = item \n                create_new = False \n            except AttributeError : \n                pass \n    if create_new : \n        if len ( args ) > 0 and inspect . isclass ( args [ 0 ] ) : \n            constructor = args . pop ( 0 ) \n        if len ( args ) > 0 and isinstance ( args [ 0 ] , str ) : \n            name = args . pop ( 0 ) \n        else : \n            if 'name' in kwargs : \n                name = kwargs . pop ( 'name' ) \n            else : \n                if 'full_name' in kwargs : \n                    name = kwargs . pop ( 'full_name' ) \n                else : \n                    raise ValueError ( 'Could not determine a name of the new item you want to add. ' 'Either pass the name as positional argument or as a keyword ' 'argument `name`.' ) \n    split_names = name . split ( '.' ) \n    if check_naming : \n        for idx , name in enumerate ( split_names ) : \n            translated_shortcut , name = self . _translate_shortcut ( name ) \n            replaced , name = self . _replace_wildcards ( name ) \n            if translated_shortcut or replaced : \n                split_names [ idx ] = name \n        faulty_names = self . _check_names ( split_names , start_node ) \n        if faulty_names : \n            full_name = '.' . join ( split_names ) \n            raise ValueError ( 'Your Parameter/Result/Node `%s` contains the following not admissible names: ' '%s please choose other names.' % ( full_name , faulty_names ) ) \n        if add_link : \n            if instance is None : \n                raise ValueError ( 'You must provide an instance to link to!' ) \n            if instance . v_is_root : \n                raise ValueError ( 'You cannot create a link to the root node' ) \n            if start_node . v_is_root and name in SUBTREE_MAPPING : \n                raise ValueError ( '`%s` is a reserved name for a group under root.' % name ) \n            if not self . _root_instance . f_contains ( instance , with_links = False , shortcuts = False ) : \n                raise ValueError ( 'You can only link to items within the trajectory tree!' ) \n    if add_prefix : \n        split_names = self . _add_prefix ( split_names , start_node , group_type_name ) \n    if group_type_name == GROUP : \n        add_leaf = type_name != group_type_name and not add_link \n        group_type_name , type_name = self . _determine_types ( start_node , split_names [ 0 ] , add_leaf , add_link ) \n    if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : \n        raise TypeError ( 'You are not allowed to add config or parameter data or groups ' 'during a single run.' ) \n    return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) "}
{"8300": "\ndef _add_to_tree ( self , start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) : \n    try : \n        act_node = start_node \n        last_idx = len ( split_names ) - 1 \n        add_link = type_name == LINK \n        link_added = False \n        for idx , name in enumerate ( split_names ) : \n            if name not in act_node . _children : \n                if idx == last_idx : \n                    if add_link : \n                        new_node = self . _create_link ( act_node , name , instance ) \n                        link_added = True \n                    else : \n                        if group_type_name != type_name : \n                            new_node = self . _create_any_param_or_result ( act_node , name , type_name , instance , constructor , args , kwargs ) \n                            self . _flat_leaf_storage_dict [ new_node . v_full_name ] = new_node \n                        else : \n                            new_node = self . _create_any_group ( act_node , name , group_type_name , instance , constructor , args , kwargs ) \n                else : \n                    new_node = self . _create_any_group ( act_node , name , group_type_name ) \n                if name in self . _root_instance . _run_information : \n                    self . _root_instance . _run_parent_groups [ act_node . v_full_name ] = act_node \n                if self . _root_instance . _is_run : \n                    if link_added : \n                        self . _root_instance . _new_links [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n                    else : \n                        self . _root_instance . _new_nodes [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n            else : \n                if name in act_node . _links : \n                    raise AttributeError ( 'You cannot hop over links when adding ' 'data to the tree. ' 'There is a link called `%s` under `%s`.' % ( name , act_node . v_full_name ) ) \n                if idx == last_idx : \n                    if self . _root_instance . _no_clobber : \n                        self . _logger . warning ( 'You already have a group/instance/link `%s` ' 'under `%s`. ' 'However, you set `v_no_clobber=True`, ' 'so I will ignore your addition of ' 'data.' % ( name , act_node . v_full_name ) ) \n                    else : \n                        raise AttributeError ( 'You already have a group/instance/link `%s` ' 'under `%s`' % ( name , act_node . v_full_name ) ) \n            act_node = act_node . _children [ name ] \n        return act_node \n    except : \n        self . _logger . error ( 'Failed adding `%s` under `%s`.' % ( name , start_node . v_full_name ) ) \n        raise "}
{"8302": "\ndef _check_names ( self , split_names , parent_node = None ) : \n    faulty_names = '' \n    if parent_node is not None and parent_node . v_is_root and split_names [ 0 ] == 'overview' : \n        faulty_names = '%s `overview` cannot be added directly under the root node ' 'this is a reserved keyword,' % ( faulty_names ) \n    for split_name in split_names : \n        if len ( split_name ) == 0 : \n            faulty_names = '%s `%s` contains no characters, please use at least 1,' % ( faulty_names , split_name ) \n        else : \n            if split_name . startswith ( '_' ) : \n                faulty_names = '%s `%s` starts with a leading underscore,' % ( faulty_names , split_name ) \n            else : \n                if re . match ( CHECK_REGEXP , split_name ) is None : \n                    faulty_names = '%s `%s` contains non-admissible characters ' '(use only [A-Za-z0-9_-]),' % ( faulty_names , split_name ) \n                else : \n                    if '$' in split_name : \n                        if split_name not in self . _root_instance . _wildcard_keys : \n                            faulty_names = '%s `%s` contains `$` but has no associated ' 'wildcard function,' % ( faulty_names , split_name ) \n                    else : \n                        if split_name in self . _not_admissible_names : \n                            warnings . warn ( '`%s` is a method/attribute of the ' 'trajectory/treenode/naminginterface, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n                        else : \n                            if split_name in self . _python_keywords : \n                                warnings . warn ( '`%s` is a python keyword, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n    name = split_names [ - 1 ] \n    if len ( name ) >= pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH : \n        faulty_names = '%s `%s` is too long the name can only have %d characters but it has ' '%d,' % ( faulty_names , name , len ( name ) , pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) \n    return faulty_names "}
{"8303": "\ndef _create_any_group ( self , parent_node , name , type_name , instance = None , constructor = None , args = None , kwargs = None ) : \n    if args is None : \n        args = [ ] \n    if kwargs is None : \n        kwargs = { } \n    full_name = self . _make_full_name ( parent_node . v_full_name , name ) \n    if instance is None : \n        if constructor is None : \n            if type_name == RESULT_GROUP : \n                constructor = ResultGroup \n            else : \n                if type_name == PARAMETER_GROUP : \n                    constructor = ParameterGroup \n                else : \n                    if type_name == CONFIG_GROUP : \n                        constructor = ConfigGroup \n                    else : \n                        if type_name == DERIVED_PARAMETER_GROUP : \n                            constructor = DerivedParameterGroup \n                        else : \n                            if type_name == GROUP : \n                                constructor = NNGroupNode \n                            else : \n                                raise RuntimeError ( 'You shall not pass!' ) \n        instance = self . _root_instance . _construct_instance ( constructor , full_name , * args , ** kwargs ) \n    else : \n        instance . _rename ( full_name ) \n        if type_name == RESULT_GROUP : \n            if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : \n                raise TypeError ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) \n        else : \n            if type_name == PARAMETER_GROUP : \n                if type ( instance ) in ( NNGroupNode , ResultGroup , ConfigGroup , DerivedParameterGroup ) : \n                    raise TypeError ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) \n            else : \n                if type_name == CONFIG_GROUP : \n                    if type ( instance ) in ( NNGroupNode , ParameterGroup , ResultGroup , DerivedParameterGroup ) : \n                        raise TypeError ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) \n                else : \n                    if type_name == DERIVED_PARAMETER_GROUP : \n                        if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , ResultGroup ) : \n                            raise TypeError ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) \n                    else : \n                        if type_name == GROUP : \n                            if type ( instance ) in ( ResultGroup , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : \n                                raise TypeError ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) \n                        else : \n                            raise RuntimeError ( 'You shall not pass!' ) \n    self . _set_details_tree_node ( parent_node , name , instance ) \n    instance . _nn_interface = self \n    self . _root_instance . _all_groups [ instance . v_full_name ] = instance \n    self . _add_to_nodes_and_leaves ( instance ) \n    parent_node . _children [ name ] = instance \n    parent_node . _groups [ name ] = instance \n    return instance "}
{"8304": "\ndef _create_any_param_or_result ( self , parent_node , name , type_name , instance , constructor , args , kwargs ) : \n    root = self . _root_instance \n    full_name = self . _make_full_name ( parent_node . v_full_name , name ) \n    if instance is None : \n        if constructor is None : \n            if type_name == RESULT : \n                constructor = root . _standard_result \n            else : \n                if type_name in [ PARAMETER , CONFIG , DERIVED_PARAMETER ] : \n                    constructor = root . _standard_parameter \n                else : \n                    constructor = root . _standard_leaf \n        instance = root . _construct_instance ( constructor , full_name , * args , ** kwargs ) \n    else : \n        instance . _rename ( full_name ) \n    self . _set_details_tree_node ( parent_node , name , instance ) \n    where_dict = self . _map_type_to_dict ( type_name ) \n    full_name = instance . _full_name \n    if full_name in where_dict : \n        raise AttributeError ( full_name + ' is already part of trajectory,' ) \n    if type_name != RESULT and full_name in root . _changed_default_parameters : \n        self . _logger . info ( 'You have marked parameter %s for change before, so here you go!' % full_name ) \n        change_args , change_kwargs = root . _changed_default_parameters . pop ( full_name ) \n        instance . f_set ( * change_args , ** change_kwargs ) \n    where_dict [ full_name ] = instance \n    self . _add_to_nodes_and_leaves ( instance ) \n    parent_node . _children [ name ] = instance \n    parent_node . _leaves [ name ] = instance \n    if full_name in self . _root_instance . _explored_parameters : \n        instance . _explored = True \n        self . _root_instance . _explored_parameters [ full_name ] = instance \n    self . _logger . debug ( 'Added `%s` to trajectory.' % full_name ) \n    return instance "}
{"8306": "\ndef _iter_nodes ( self , node , recursive = False , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    def _run_predicate ( x , run_name_set ) : \n        branch = x . v_run_branch \n        return branch == 'trajectory' or branch in run_name_set \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if predicate is None : \n        predicate = lambda x : True \n    else : \n        if isinstance ( predicate , ( tuple , list ) ) : \n            run_list = predicate \n            run_name_set = set ( ) \n            for item in run_list : \n                if item == - 1 : \n                    run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - 1 ) ) \n                else : \n                    if isinstance ( item , int ) : \n                        run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) \n                    else : \n                        run_name_set . add ( item ) \n            predicate = lambda x : _run_predicate ( x , run_name_set ) \n    if recursive : \n        return NaturalNamingInterface . _recursive_traversal_bfs ( node , self . _root_instance . _linked_by , max_depth , with_links , in_search , predicate ) \n    else : \n        iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if predicate ( x [ 2 ] ) ) \n        if in_search : \n            return iterator \n        else : \n            return ( x [ 2 ] for x in iterator ) "}
{"8308": "\ndef _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    if predicate is None : \n        predicate = lambda x : True \n    iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) \n    start = True \n    visited_linked_nodes = set ( [ ] ) \n    while True : \n        try : \n            depth , name , item = next ( iterator_queue ) \n            full_name = item . _full_name \n            if start or predicate ( item ) : \n                if full_name in visited_linked_nodes : \n                    if in_search : \n                        yield depth , name , item \n                else : \n                    if depth <= max_depth : \n                        if start : \n                            start = False \n                        else : \n                            if in_search : \n                                yield depth , name , item \n                            else : \n                                yield item \n                        if full_name in linked_by : \n                            visited_linked_nodes . add ( full_name ) \n                        if not item . _is_leaf and depth < max_depth : \n                            child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) \n                            iterator_queue . add ( child_iterator ) \n        except StopIteration : \n            break "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - 1 ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if key != candidate . v_name or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if parent_full_name != '' : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if len ( candidate_split_name ) > max_depth : \n                break \n            if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            else : \n                if shortcuts : \n                    candidate_set = set ( candidate_split_name ) \n                    climbing = True \n                    for name in split_name : \n                        if name not in candidate_set : \n                            climbing = False \n                            break \n                    if climbing : \n                        count = 0 \n                        candidate_length = len ( candidate_split_name ) \n                        for idx in range ( candidate_length ) : \n                            if idx + split_length - count > candidate_length : \n                                break \n                            if split_name [ count ] == candidate_split_name [ idx ] : \n                                count += 1 \n                                if count == len ( split_name ) : \n                                    result_list . append ( candidate ) \n                                    full_name_set . add ( candidate . v_full_name ) \n                                    break \n    return result_list "}
{"8317": "\ndef f_get_parent ( self ) : \n    if self . v_is_root : \n        raise TypeError ( 'Root does not have a parent' ) \n    else : \n        if self . v_location == '' : \n            return self . v_root \n        else : \n            return self . v_root . f_get ( self . v_location , fast_access = False , shortcuts = False ) "}
{"8348": "\ndef _get_argspec ( func ) : \n    if inspect . isclass ( func ) : \n        func = func . __init__ \n    if not inspect . isfunction ( func ) : \n        return [ ] , False \n    parameters = inspect . signature ( func ) . parameters \n    args = [ ] \n    uses_starstar = False \n    for par in parameters . values ( ) : \n        if ( par . kind == inspect . Parameter . POSITIONAL_OR_KEYWORD or par . kind == inspect . Parameter . KEYWORD_ONLY ) : \n            args . append ( par . name ) \n        else : \n            if par . kind == inspect . Parameter . VAR_KEYWORD : \n                uses_starstar = True \n    return args , uses_starstar "}
{"8359": "\ndef make_shared_result ( result , key , trajectory , new_class = None ) : \n    data = result . f_get ( key ) \n    if new_class is None : \n        if isinstance ( data , ObjectTable ) : \n            new_class = SharedTable \n        else : \n            if isinstance ( data , pd . DataFrame ) : \n                new_class = SharedPandasFrame \n            else : \n                if isinstance ( data , ( tuple , list ) ) : \n                    new_class = SharedArray \n                else : \n                    if isinstance ( data , ( np . ndarray , np . matrix ) ) : \n                        new_class = SharedCArray \n                    else : \n                        raise RuntimeError ( 'Your data `%s` is not understood.' % key ) \n    shared_data = new_class ( result . f_translate_key ( key ) , result , trajectory = trajectory ) \n    result [ key ] = shared_data \n    shared_data . _request_data ( 'make_shared' ) \n    return result "}
{"8371": "\ndef acquire ( self ) : \n    self . start ( test_connection = False ) \n    while True : \n        str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) \n        response = str_response . split ( LockerServer . DELIMITER ) \n        if response [ 0 ] == LockerServer . GO : \n            return True \n        else : \n            if response [ 0 ] == LockerServer . LOCK_ERROR and retries > 0 : \n                self . _logger . error ( str_response + '; Probably due to retry' ) \n                return True \n            else : \n                if response [ 0 ] == LockerServer . WAIT : \n                    time . sleep ( self . SLEEP ) \n                else : \n                    raise RuntimeError ( 'Response `%s` not understood' % response ) "}
{"8372": "\ndef listen ( self ) : \n    count = 0 \n    self . _start ( ) \n    while True : \n        result = self . _socket . recv_pyobj ( ) \n        if isinstance ( result , tuple ) : \n            request , data = result \n        else : \n            request = result \n            data = None \n        if request == self . SPACE : \n            if self . queue . qsize ( ) + count < self . queue_maxsize : \n                self . _socket . send_string ( self . SPACE_AVAILABLE ) \n                count += 1 \n            else : \n                self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) \n        else : \n            if request == self . PING : \n                self . _socket . send_string ( self . PONG ) \n            else : \n                if request == self . DATA : \n                    self . _socket . send_string ( self . STORING ) \n                    self . queue . put ( data ) \n                    count -= 1 \n                else : \n                    if request == self . DONE : \n                        self . _socket . send_string ( ZMQServer . CLOSED ) \n                        self . queue . put ( ( 'DONE' , [ ] , { } ) ) \n                        self . _close ( ) \n                        break \n                    else : \n                        raise RuntimeError ( 'I did not understand your request %s' % request ) "}
{"8375": "\ndef _handle_data ( self , msg , args , kwargs ) : \n    stop = False \n    try : \n        if msg == 'DONE' : \n            stop = True \n        else : \n            if msg == 'STORE' : \n                if 'msg' in kwargs : \n                    store_msg = kwargs . pop ( 'msg' ) \n                else : \n                    store_msg = args [ 0 ] \n                    args = args [ 1 : ] \n                if 'stuff_to_store' in kwargs : \n                    stuff_to_store = kwargs . pop ( 'stuff_to_store' ) \n                else : \n                    stuff_to_store = args [ 0 ] \n                    args = args [ 1 : ] \n                trajectory_name = kwargs [ 'trajectory_name' ] \n                if self . _trajectory_name != trajectory_name : \n                    if self . _storage_service . is_open : \n                        self . _close_file ( ) \n                    self . _trajectory_name = trajectory_name \n                    self . _open_file ( ) \n                self . _storage_service . store ( store_msg , stuff_to_store , * args , ** kwargs ) \n                self . _storage_service . store ( pypetconstants . FLUSH , None ) \n                self . _check_and_collect_garbage ( ) \n            else : \n                raise RuntimeError ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) \n    except Exception : \n        self . _logger . exception ( 'ERROR occurred during storing!' ) \n        time . sleep ( 0.01 ) \n        pass \n    return stop "}
{"8388": "\ndef make_initial_state ( name , ncells , seed = 42 ) : \n    if name == 'single' : \n        just_one_cell = np . zeros ( ncells ) \n        just_one_cell [ int ( ncells / 2 ) ] = 1.0 \n        return just_one_cell \n    else : \n        if name == 'random' : \n            np . random . seed ( seed ) \n            random_init = np . random . randint ( 2 , size = ncells ) \n            return random_init \n        else : \n            raise ValueError ( 'I cannot handel your initial state `%s`.' % name ) "}
{"8394": "\ndef load ( self , msg , stuff_to_load , * args , ** kwargs ) : \n    opened = True \n    try : \n        opened = self . _srvc_opening_routine ( 'r' , kwargs = kwargs ) \n        if msg == pypetconstants . TRAJECTORY : \n            self . _trj_load_trajectory ( stuff_to_load , * args , ** kwargs ) \n        else : \n            if msg == pypetconstants . LEAF : \n                self . _prm_load_parameter_or_result ( stuff_to_load , * args , ** kwargs ) \n            else : \n                if msg == pypetconstants . GROUP : \n                    self . _grp_load_group ( stuff_to_load , * args , ** kwargs ) \n                else : \n                    if msg == pypetconstants . TREE : \n                        self . _tree_load_sub_branch ( stuff_to_load , * args , ** kwargs ) \n                    else : \n                        if msg == pypetconstants . LIST : \n                            self . _srvc_load_several_items ( stuff_to_load , * args , ** kwargs ) \n                        else : \n                            raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) \n    except pt . NoSuchNodeError as exc : \n        self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) \n        raise pex . DataNotInStorageError ( repr ( exc ) ) \n    except : \n        self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) \n        raise \n    finally : \n        self . _srvc_closing_routine ( opened ) "}
{"8395": "\ndef store ( self , msg , stuff_to_store , * args , ** kwargs ) : \n    opened = True \n    try : \n        opened = self . _srvc_opening_routine ( 'a' , msg , kwargs ) \n        if msg == pypetconstants . MERGE : \n            self . _trj_merge_trajectories ( * args , ** kwargs ) \n        else : \n            if msg == pypetconstants . BACKUP : \n                self . _trj_backup_trajectory ( stuff_to_store , * args , ** kwargs ) \n            else : \n                if msg == pypetconstants . PREPARE_MERGE : \n                    self . _trj_prepare_merge ( stuff_to_store , * args , ** kwargs ) \n                else : \n                    if msg == pypetconstants . TRAJECTORY : \n                        self . _trj_store_trajectory ( stuff_to_store , * args , ** kwargs ) \n                    else : \n                        if msg == pypetconstants . SINGLE_RUN : \n                            self . _srn_store_single_run ( stuff_to_store , * args , ** kwargs ) \n                        else : \n                            if msg in pypetconstants . LEAF : \n                                self . _prm_store_parameter_or_result ( stuff_to_store , * args , ** kwargs ) \n                            else : \n                                if msg == pypetconstants . DELETE : \n                                    self . _all_delete_parameter_or_result_or_group ( stuff_to_store , * args , ** kwargs ) \n                                else : \n                                    if msg == pypetconstants . GROUP : \n                                        self . _grp_store_group ( stuff_to_store , * args , ** kwargs ) \n                                    else : \n                                        if msg == pypetconstants . TREE : \n                                            self . _tree_store_sub_branch ( stuff_to_store , * args , ** kwargs ) \n                                        else : \n                                            if msg == pypetconstants . DELETE_LINK : \n                                                self . _lnk_delete_link ( stuff_to_store , * args , ** kwargs ) \n                                            else : \n                                                if msg == pypetconstants . LIST : \n                                                    self . _srvc_store_several_items ( stuff_to_store , * args , ** kwargs ) \n                                                else : \n                                                    if msg == pypetconstants . ACCESS_DATA : \n                                                        return self . _hdf5_interact_with_data ( stuff_to_store , * args , ** kwargs ) \n                                                    else : \n                                                        if msg == pypetconstants . OPEN_FILE : \n                                                            opened = False \n                                                            self . _keep_open = True \n                                                            self . _node_processing_timer . active = False \n                                                        else : \n                                                            if msg == pypetconstants . CLOSE_FILE : \n                                                                opened = True \n                                                                self . _keep_open = False \n                                                            else : \n                                                                if msg == pypetconstants . FLUSH : \n                                                                    self . _hdf5file . flush ( ) \n                                                                else : \n                                                                    raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) \n    except : \n        self . _logger . error ( 'Failed storing `%s`' % str ( stuff_to_store ) ) \n        raise \n    finally : \n        self . _srvc_closing_routine ( opened ) "}
{"8406": "\ndef _trj_check_version ( self , version , python , force ) : \n    curr_python = pypetconstants . python_version_string \n    if ( version != VERSION or curr_python != python ) and not force : \n        raise pex . VersionMismatchError ( 'Current pypet version is %s used under python %s ' '  but your trajectory' ' was created with version %s and python %s.' ' Use >>force=True<< to perform your load regardless' ' of version mismatch.' % ( VERSION , curr_python , version , python ) ) \n    else : \n        if version != VERSION or curr_python != python : \n            self . _logger . warning ( 'Current pypet version is %s with python %s but your trajectory' ' was created with version %s under python %s.' ' Yet, you enforced the load, so I will' ' handle the trajectory despite the' ' version mismatch.' % ( VERSION , curr_python , version , python ) ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = 0 \n        maximum_display_other = 10 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if counter < maximum_display_other : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                else : \n                    if counter == maximum_display_other : \n                        self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter += 1 \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = True "}
{"8419": "\ndef _all_set_attributes_to_recall_natives ( data , ptitem , prefix ) : \n    if type ( data ) is tuple : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_TUPLE ) \n    else : \n        if type ( data ) is list : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_LIST ) \n        else : \n            if type ( data ) is np . ndarray : \n                HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_NDARRAY ) \n            else : \n                if type ( data ) is np . matrix : \n                    HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_MATRIX ) \n                else : \n                    if type ( data ) in pypetconstants . PARAMETER_SUPPORTED_DATA : \n                        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_SCALAR ) \n                        strtype = type ( data ) . __name__ \n                        if not strtype in pypetconstants . PARAMETERTYPEDICT : \n                            raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n                        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n                    else : \n                        if type ( data ) is dict : \n                            if len ( data ) > 0 : \n                                HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_DICT ) \n                            else : \n                                HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_EMPTY_DICT ) \n                        else : \n                            raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n    if type ( data ) in ( list , tuple ) : \n        if len ( data ) > 0 : \n            strtype = type ( data [ 0 ] ) . __name__ \n            if not strtype in pypetconstants . PARAMETERTYPEDICT : \n                raise TypeError ( 'I do not know how to handle `%s` its type is ' '`%s`.' % ( str ( data ) , strtype ) ) \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    else : \n        if ( type ( data ) in ( np . ndarray , np . matrix ) and np . issubdtype ( data . dtype , str ) ) : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , str . __name__ ) "}
{"8420": "\ndef _all_recall_native_type ( self , data , ptitem , prefix ) : \n    typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) \n    colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) \n    type_changed = False \n    if colltype == HDF5StorageService . COLL_SCALAR : \n        if isinstance ( data , np . ndarray ) : \n            data = np . array ( [ data ] ) [ 0 ] \n            type_changed = True \n        if not typestr is None : \n            if typestr != type ( data ) . __name__ : \n                if typestr == str . __name__ : \n                    data = data . decode ( self . _encoding ) \n                else : \n                    try : \n                        data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) \n                    except KeyError : \n                        data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) \n                type_changed = True \n    else : \n        if ( colltype == HDF5StorageService . COLL_TUPLE or colltype == HDF5StorageService . COLL_LIST ) : \n            if type ( data ) is not list and type is not tuple : \n                type_changed = True \n                data = list ( data ) \n            if len ( data ) > 0 : \n                first_item = data [ 0 ] \n                if not typestr == type ( first_item ) . __name__ : \n                    if not isinstance ( data , list ) : \n                        data = list ( data ) \n                    for idx , item in enumerate ( data ) : \n                        if typestr == str . __name__ : \n                            data [ idx ] = data [ idx ] . decode ( self . _encoding ) \n                        else : \n                            try : \n                                data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) \n                            except KeyError : \n                                data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) \n                        type_changed = True \n            if colltype == HDF5StorageService . COLL_TUPLE : \n                if type ( data ) is not tuple : \n                    data = tuple ( data ) \n                    type_changed = True \n        else : \n            if colltype == HDF5StorageService . COLL_EMPTY_DICT : \n                data = { } \n                type_changed = True \n            else : \n                if isinstance ( data , np . ndarray ) : \n                    if typestr == str . __name__ : \n                        data = np . core . defchararray . decode ( data , self . _encoding ) \n                        type_changed = True \n                    if colltype == HDF5StorageService . COLL_MATRIX : \n                        data = np . matrix ( data ) \n                        type_changed = True \n    return data , type_changed "}
{"8421": "\ndef _all_add_or_modify_row ( self , item_name , insert_dict , table , index = None , condition = None , condvars = None , flags = ( ADD_ROW , MODIFY_ROW , ) ) : \n    if len ( flags ) == 0 : \n        return \n    if index is not None and condition is not None : \n        raise ValueError ( 'Please give either a condition or an index or none!' ) \n    else : \n        if condition is not None : \n            row_iterator = table . where ( condition , condvars = condvars ) \n        else : \n            if index is not None : \n                row_iterator = table . iterrows ( index , index + 1 ) \n            else : \n                row_iterator = None \n    try : \n        row = next ( row_iterator ) \n    except TypeError : \n        row = None \n    except StopIteration : \n        row = None \n    if ( ( HDF5StorageService . MODIFY_ROW in flags or HDF5StorageService . ADD_ROW in flags ) and HDF5StorageService . REMOVE_ROW in flags ) : \n        raise ValueError ( 'You cannot add or modify and remove a row at the same time.' ) \n    if row is None and HDF5StorageService . ADD_ROW in flags : \n        row = table . row \n        self . _all_insert_into_row ( row , insert_dict ) \n        row . append ( ) \n    else : \n        if row is not None and HDF5StorageService . MODIFY_ROW in flags : \n            self . _all_insert_into_row ( row , insert_dict ) \n            row . update ( ) \n        else : \n            if HDF5StorageService . REMOVE_ROW in flags : \n                if row is not None : \n                    row_number = row . nrow \n                    try : \n                        table . remove_rows ( start = row_number , stop = row_number + 1 ) \n                    except NotImplementedError : \n                        pass \n            else : \n                raise ValueError ( 'Something is wrong, you might not have found ' 'a row, or your flags are not set appropriately' ) \n    self . _all_kill_iterator ( row_iterator ) \n    table . flush ( ) \n    if HDF5StorageService . REMOVE_ROW not in flags and row is None : \n        raise RuntimeError ( 'Could not add or modify entries of `%s` in ' 'table %s' % ( item_name , table . _v_name ) ) "}
{"8429": "\ndef _grp_store_group ( self , traj_group , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , _hdf5_group = None , _newly_created = False ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    else : \n        if store_data == pypetconstants . STORE_DATA_SKIPPING and traj_group . _stored : \n            self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % traj_group . v_full_name ) \n        else : \n            if not recursive : \n                if _hdf5_group is None : \n                    _hdf5_group , _newly_created = self . _all_create_or_get_groups ( traj_group . v_full_name ) \n                overwrite = store_data == pypetconstants . OVERWRITE_DATA \n                if ( traj_group . v_comment != '' and ( HDF5StorageService . COMMENT not in _hdf5_group . _v_attrs or overwrite ) ) : \n                    setattr ( _hdf5_group . _v_attrs , HDF5StorageService . COMMENT , traj_group . v_comment ) \n                if ( ( _newly_created or overwrite ) and type ( traj_group ) not in ( nn . NNGroupNode , nn . ConfigGroup , nn . ParameterGroup , nn . DerivedParameterGroup , nn . ResultGroup ) ) : \n                    setattr ( _hdf5_group . _v_attrs , HDF5StorageService . CLASS_NAME , traj_group . f_get_class_name ( ) ) \n                self . _ann_store_annotations ( traj_group , _hdf5_group , overwrite = overwrite ) \n                self . _hdf5file . flush ( ) \n                traj_group . _stored = True \n                self . _node_processing_timer . signal_update ( ) \n    if recursive : \n        parent_traj_group = traj_group . f_get_parent ( ) \n        parent_hdf5_group = self . _all_create_or_get_groups ( parent_traj_group . v_full_name ) [ 0 ] \n        self . _tree_store_nodes_dfs ( parent_traj_group , traj_group . v_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , parent_hdf5_group = parent_hdf5_group ) "}
{"8430": "\ndef _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _traj = None , _as_new = False , _hdf5_group = None ) : \n    if _hdf5_group is None : \n        _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) \n        _traj = traj_group . v_root \n    if recursive : \n        parent_traj_node = traj_group . f_get_parent ( ) \n        self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , trajectory = _traj , as_new = _as_new , hdf5_group = _hdf5_group ) \n    else : \n        if load_data == pypetconstants . LOAD_NOTHING : \n            return \n        else : \n            if load_data == pypetconstants . OVERWRITE_DATA : \n                traj_group . v_annotations . f_empty ( ) \n                traj_group . v_comment = '' \n        self . _all_load_skeleton ( traj_group , _hdf5_group ) \n        traj_group . _stored = not _as_new \n        self . _node_processing_timer . signal_update ( ) "}
{"8435": "\ndef _prm_store_from_dict ( self , fullname , store_dict , hdf5_group , store_flags , kwargs ) : \n    for key , data_to_store in store_dict . items ( ) : \n        original_hdf5_group = None \n        flag = store_flags [ key ] \n        if '.' in key : \n            original_hdf5_group = hdf5_group \n            split_key = key . split ( '.' ) \n            key = split_key . pop ( ) \n            for inner_key in split_key : \n                hdf5_group , newly_created = self . _all_create_or_get_group ( inner_key , hdf5_group ) \n                if newly_created : \n                    setattr ( hdf5_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . NESTED_GROUP ) \n                else : \n                    store_type = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . STORAGE_TYPE ) \n                    if store_type != HDF5StorageService . NESTED_GROUP : \n                        raise ValueError ( 'You want to nested results but `%s` is already ' 'of type `%s`!' % ( hdf5_group . _v_name , store_type ) ) \n        if key in hdf5_group : \n            self . _logger . debug ( 'Found %s already in hdf5 node of %s, so I will ignore it.' % ( key , fullname ) ) \n            continue \n        if flag == HDF5StorageService . TABLE : \n            self . _prm_write_into_pytable ( key , data_to_store , hdf5_group , fullname , ** kwargs ) \n        else : \n            if flag == HDF5StorageService . DICT : \n                self . _prm_write_dict_as_table ( key , data_to_store , hdf5_group , fullname , ** kwargs ) \n            else : \n                if flag == HDF5StorageService . ARRAY : \n                    self . _prm_write_into_array ( key , data_to_store , hdf5_group , fullname , ** kwargs ) \n                else : \n                    if flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : \n                        self . _prm_write_into_other_array ( key , data_to_store , hdf5_group , fullname , flag = flag , ** kwargs ) \n                    else : \n                        if flag in ( HDF5StorageService . SERIES , HDF5StorageService . FRAME , ) : \n                            self . _prm_write_pandas_data ( key , data_to_store , hdf5_group , fullname , flag , ** kwargs ) \n                        else : \n                            if flag == HDF5StorageService . SHARED_DATA : \n                                pass \n                            else : \n                                raise RuntimeError ( 'You shall not pass!' ) \n        if original_hdf5_group is not None : \n            hdf5_group = original_hdf5_group "}
{"8436": "\ndef _prm_store_parameter_or_result ( self , instance , store_data = pypetconstants . STORE_DATA , store_flags = None , overwrite = None , with_links = False , recursive = False , _hdf5_group = None , _newly_created = False , ** kwargs ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    else : \n        if store_data == pypetconstants . STORE_DATA_SKIPPING and instance . _stored : \n            self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % instance . v_full_name ) \n            return \n        else : \n            if store_data == pypetconstants . OVERWRITE_DATA : \n                if not overwrite : \n                    overwrite = True \n    fullname = instance . v_full_name \n    self . _logger . debug ( 'Storing `%s`.' % fullname ) \n    if _hdf5_group is None : \n        _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) \n    store_dict = { } \n    if store_flags is None : \n        store_flags = { } \n    try : \n        if not instance . f_is_empty ( ) : \n            store_dict = instance . _store ( ) \n        try : \n            instance_flags = instance . _store_flags ( ) . copy ( ) \n        except AttributeError : \n            instance_flags = { } \n        instance_flags . update ( store_flags ) \n        store_flags = instance_flags \n        self . _prm_extract_missing_flags ( store_dict , store_flags ) \n        if overwrite : \n            if isinstance ( overwrite , str ) : \n                overwrite = [ overwrite ] \n            if overwrite is True : \n                to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] \n                self . _all_delete_parameter_or_result_or_group ( instance , delete_only = to_delete , _hdf5_group = _hdf5_group ) \n            else : \n                if isinstance ( overwrite , ( list , tuple ) ) : \n                    overwrite_set = set ( overwrite ) \n                    key_set = set ( store_dict . keys ( ) ) \n                    stuff_not_to_be_overwritten = overwrite_set - key_set \n                    if overwrite != 'v_annotations' and len ( stuff_not_to_be_overwritten ) > 0 : \n                        self . _logger . warning ( 'Cannot overwrite `%s`, these items are not supposed to ' 'be stored by the leaf node.' % str ( stuff_not_to_be_overwritten ) ) \n                    stuff_to_overwrite = overwrite_set & key_set \n                    if len ( stuff_to_overwrite ) > 0 : \n                        self . _all_delete_parameter_or_result_or_group ( instance , delete_only = list ( stuff_to_overwrite ) ) \n                else : \n                    raise ValueError ( 'Your value of overwrite `%s` is not understood. ' 'Please pass `True` of a list of strings to fine grain ' 'overwriting.' % str ( overwrite ) ) \n        self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) \n        self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) \n        if _newly_created or overwrite is True : \n            self . _prm_add_meta_info ( instance , _hdf5_group , overwrite = not _newly_created ) \n        instance . _stored = True \n        self . _node_processing_timer . signal_update ( ) \n    except : \n        self . _logger . error ( 'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname ) \n        for key in store_dict . keys ( ) : \n            if key in _hdf5_group : \n                hdf5_child = _hdf5_group . _f_get_child ( key ) \n                hdf5_child . _f_remove ( recursive = True ) \n        if _hdf5_group . _v_nchildren == 0 : \n            _hdf5_group . _f_remove ( recursive = True ) \n        raise "}
{"8437": "\ndef _prm_write_shared_array ( self , key , data , hdf5_group , full_name , flag , ** kwargs ) : \n    if flag == HDF5StorageService . ARRAY : \n        self . _prm_write_into_array ( key , data , hdf5_group , full_name , ** kwargs ) \n    else : \n        if flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : \n            self . _prm_write_into_other_array ( key , data , hdf5_group , full_name , flag = flag , ** kwargs ) \n        else : \n            raise RuntimeError ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full_name ) ) \n    self . _hdf5file . flush ( ) "}
{"8441": "\ndef _prm_write_into_other_array ( self , key , data , group , fullname , flag , ** kwargs ) : \n    try : \n        if flag == HDF5StorageService . CARRAY : \n            factory = self . _hdf5file . create_carray \n        else : \n            if flag == HDF5StorageService . EARRAY : \n                factory = self . _hdf5file . create_earray \n            else : \n                if flag == HDF5StorageService . VLARRAY : \n                    factory = self . _hdf5file . create_vlarray \n                else : \n                    raise RuntimeError ( 'You shall not pass!' ) \n        if key in group : \n            raise ValueError ( 'CArray `%s` already exists in `%s`. Appending is not supported (yet).' ) \n        if 'filters' in kwargs : \n            filters = kwargs . pop ( 'filters' ) \n        else : \n            filters = self . _all_get_filters ( kwargs ) \n        try : \n            other_array = factory ( where = group , name = key , obj = data , filters = filters , ** kwargs ) \n        except ( ValueError , TypeError ) as exc : \n            try : \n                conv_data = data [ : ] \n                conv_data = np . core . defchararray . encode ( conv_data , self . encoding ) \n                other_array = factory ( where = group , name = key , obj = conv_data , filters = filters , ** kwargs ) \n            except Exception : \n                raise exc \n        if data is not None : \n            self . _all_set_attributes_to_recall_natives ( data , other_array , HDF5StorageService . DATA_PREFIX ) \n        setattr ( other_array . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing %s `%s` of `%s`.' % ( flag , key , fullname ) ) \n        raise "}
{"8442": "\ndef _prm_write_into_array ( self , key , data , group , fullname , ** kwargs ) : \n    try : \n        if key in group : \n            raise ValueError ( 'Array `%s` already exists in `%s`. Appending is not supported (yet).' ) \n        try : \n            array = self . _hdf5file . create_array ( where = group , name = key , obj = data , ** kwargs ) \n        except ( TypeError , ValueError ) as exc : \n            try : \n                if type ( data ) is dict and len ( data ) == 0 : \n                    conv_data = ( ) \n                else : \n                    if isinstance ( data , str ) : \n                        conv_data = data . encode ( self . _encoding ) \n                    else : \n                        if isinstance ( data , int ) : \n                            conv_data = np . int64 ( data ) \n                        else : \n                            conv_data = [ ] \n                            for string in data : \n                                conv_data . append ( string . encode ( self . _encoding ) ) \n                array = self . _hdf5file . create_array ( where = group , name = key , obj = conv_data , ** kwargs ) \n            except Exception : \n                raise exc \n        if data is not None : \n            self . _all_set_attributes_to_recall_natives ( data , array , HDF5StorageService . DATA_PREFIX ) \n        setattr ( array . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . ARRAY ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing array `%s` of `%s`.' % ( key , fullname ) ) \n        raise "}
{"8449": "\ndef _prm_load_into_dict ( self , full_name , load_dict , hdf5_group , instance , load_only , load_except , load_flags , _prefix = '' ) : \n    for node in hdf5_group : \n        load_type = self . _all_get_from_attrs ( node , HDF5StorageService . STORAGE_TYPE ) \n        if _prefix : \n            load_name = '%s.%s' % ( _prefix , node . _v_name ) \n        else : \n            load_name = node . _v_name \n        if load_type == HDF5StorageService . NESTED_GROUP : \n            self . _prm_load_into_dict ( full_name = full_name , load_dict = load_dict , hdf5_group = node , instance = instance , load_only = load_only , load_except = load_except , load_flags = load_flags , _prefix = load_name ) \n            continue \n        if load_only is not None : \n            if load_name not in load_only : \n                continue \n            else : \n                load_only . remove ( load_name ) \n        else : \n            if load_except is not None : \n                if load_name in load_except : \n                    load_except . remove ( load_name ) \n                    continue \n        if load_name in load_flags : \n            load_type = load_flags [ load_name ] \n        if load_type == HDF5StorageService . DICT : \n            to_load = self . _prm_read_dictionary ( node , full_name ) \n        else : \n            if load_type == HDF5StorageService . TABLE : \n                to_load = self . _prm_read_table ( node , full_name ) \n            else : \n                if load_type in ( HDF5StorageService . ARRAY , HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : \n                    to_load = self . _prm_read_array ( node , full_name ) \n                else : \n                    if load_type in ( HDF5StorageService . FRAME , HDF5StorageService . SERIES , ) : \n                        to_load = self . _prm_read_pandas ( node , full_name ) \n                    else : \n                        if load_type . startswith ( HDF5StorageService . SHARED_DATA ) : \n                            to_load = self . _prm_read_shared_data ( node , instance ) \n                        else : \n                            raise pex . NoSuchServiceError ( 'Cannot load %s, do not understand the hdf5 file ' 'structure of %s [%s].' % ( full_name , str ( node ) , str ( load_type ) ) ) \n        if to_load is None : \n            raise RuntimeError ( 'You shall not pass!' ) \n        load_dict [ load_name ] = to_load "}
{"8454": "\ndef load_trajectory ( name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = True , load_data = None , max_depth = None , force = False , dynamic_imports = None , new_name = 'my_trajectory' , add_time = True , wildcard_functions = None , with_run_information = True , storage_service = storage . HDF5StorageService , ** kwargs ) : \n    if name is None and index is None : \n        raise ValueError ( 'Please specify either a name or an index' ) \n    else : \n        if name is not None and index is not None : \n            raise ValueError ( 'Please specify either a name or an index' ) \n    traj = Trajectory ( name = new_name , add_time = add_time , dynamic_imports = dynamic_imports , wildcard_functions = wildcard_functions ) \n    traj . f_load ( name = name , index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , load_data = load_data , max_depth = max_depth , force = force , with_run_information = with_run_information , storage_service = storage_service , ** kwargs ) \n    return traj "}
{"8459": "\ndef f_iter_runs ( self , start = 0 , stop = None , step = 1 , yields = 'name' ) : \n    if stop is None : \n        stop = len ( self ) \n    else : \n        if stop > len ( self ) : \n            raise ValueError ( 'Stop cannot be larger than the trajectory lenght.' ) \n    yields = yields . lower ( ) \n    if yields == 'name' : \n        yield_func = lambda x : self . f_idx_to_run ( x ) \n    else : \n        if yields == 'idx' : \n            yield_func = lambda x : x \n        else : \n            if yields == 'self' : \n                yield_func = lambda x : self \n            else : \n                if yields == 'copy' : \n                    yield_func = lambda x : self . __copy__ ( ) \n                else : \n                    raise ValueError ( 'Please choose yields among: `name`, `idx`, or `self`.' ) \n    for idx in range ( start , stop , step ) : \n        self . f_set_crun ( idx ) \n        yield yield_func ( idx ) \n    self . f_set_crun ( None ) "}
{"8467": "\ndef _copy_from ( self , node , copy_leaves = True , overwrite = False , with_links = True ) : \n    def _copy_skeleton ( node_in , node_out ) : \n        new_annotations = node_out . v_annotations \n        node_in . _annotations = new_annotations \n        node_in . v_comment = node_out . v_comment \n    def _add_leaf ( leaf ) : \n        leaf_full_name = leaf . v_full_name \n        try : \n            found_leaf = self . f_get ( leaf_full_name , with_links = False , shortcuts = False , auto_load = False ) \n            if overwrite : \n                found_leaf . __setstate__ ( leaf . __getstate__ ( ) ) \n            return found_leaf \n        except AttributeError : \n            pass \n        if copy_leaves is True or ( copy_leaves == 'explored' and leaf . v_is_parameter and leaf . v_explored ) : \n            new_leaf = self . f_add_leaf ( cp . copy ( leaf ) ) \n        else : \n            new_leaf = self . f_add_leaf ( leaf ) \n        if new_leaf . v_is_parameter and new_leaf . v_explored : \n            self . _explored_parameters [ new_leaf . v_full_name ] = new_leaf \n        return new_leaf \n    def _add_group ( group ) : \n        group_full_name = group . v_full_name \n        try : \n            found_group = self . f_get ( group_full_name , with_links = False , shortcuts = False , auto_load = False ) \n            if overwrite : \n                _copy_skeleton ( found_group , group ) \n            return found_group \n        except AttributeError : \n            pass \n        new_group = self . f_add_group ( group_full_name ) \n        _copy_skeleton ( new_group , group ) \n        return new_group \n    is_run = self . _is_run \n    self . _is_run = False \n    try : \n        if node . v_is_leaf : \n            return _add_leaf ( node ) \n        else : \n            if node . v_is_group : \n                other_root = node . v_root \n                if other_root is self : \n                    raise RuntimeError ( 'You cannot copy a given tree to itself!' ) \n                result = _add_group ( node ) \n                nodes_iterator = node . f_iter_nodes ( recursive = True , with_links = with_links ) \n                has_links = [ ] \n                if node . _links : \n                    has_links . append ( node ) \n                for child in nodes_iterator : \n                    if child . v_is_leaf : \n                        _add_leaf ( child ) \n                    else : \n                        _add_group ( child ) \n                        if child . _links : \n                            has_links . append ( child ) \n                if with_links : \n                    for current in has_links : \n                        mine = self . f_get ( current . v_full_name , with_links = False , shortcuts = False , auto_load = False ) \n                        my_link_set = set ( mine . _links . keys ( ) ) \n                        other_link_set = set ( current . _links . keys ( ) ) \n                        new_links = other_link_set - my_link_set \n                        for link in new_links : \n                            where_full_name = current . _links [ link ] . v_full_name \n                            mine . f_add_link ( link , where_full_name ) \n                return result \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n    except Exception : \n        self . _is_run = is_run "}
{"8468": "\ndef f_explore ( self , build_dict ) : \n    for run_idx in range ( len ( self ) ) : \n        if self . f_is_completed ( run_idx ) : \n            raise TypeError ( 'You cannot explore a trajectory which has been explored before, ' 'please use `f_expand` instead.' ) \n    added_explored_parameters = [ ] \n    try : \n        length = len ( self ) \n        for key , builditerable in build_dict . items ( ) : \n            act_param = self . f_get ( key ) \n            if not act_param . v_is_leaf or not act_param . v_is_parameter : \n                raise ValueError ( '%s is not an appropriate search string for a parameter.' % key ) \n            act_param . f_unlock ( ) \n            act_param . _explore ( builditerable ) \n            added_explored_parameters . append ( act_param ) \n            full_name = act_param . v_full_name \n            self . _explored_parameters [ full_name ] = act_param \n            act_param . _explored = True \n            if len ( self . _explored_parameters ) == 1 : \n                length = act_param . f_get_range_length ( ) \n            else : \n                if not length == act_param . f_get_range_length ( ) : \n                    raise ValueError ( 'The parameters to explore have not the same size!' ) \n        for irun in range ( length ) : \n            self . _add_run_info ( irun ) \n        self . _test_run_addition ( length ) \n    except Exception : \n        for param in added_explored_parameters : \n            param . f_unlock ( ) \n            param . _shrink ( ) \n            param . _explored = False \n            full_name = param . v_full_name \n            del self . _explored_parameters [ full_name ] \n        if len ( self . _explored_parameters ) == 0 : \n            self . f_shrink ( force = True ) \n        raise "}
{"8480": "\ndef _rename_full_name ( self , full_name , other_trajectory , used_runs = None , new_run_idx = None ) : \n    split_name = full_name . split ( '.' ) \n    for idx , name in enumerate ( split_name ) : \n        if name in other_trajectory . _reversed_wildcards : \n            run_indices , wildcards = other_trajectory . _reversed_wildcards [ name ] \n            if new_run_idx is None : \n                run_idx = None \n                for run_jdx in run_indices : \n                    if run_jdx in used_runs : \n                        run_idx = used_runs [ run_jdx ] \n                        break \n                    else : \n                        if run_jdx == - 1 : \n                            run_idx = - 1 \n                            break \n                if run_idx is None : \n                    raise RuntimeError ( 'You shall not pass!' ) \n            else : \n                run_idx = new_run_idx \n            new_name = self . f_wildcard ( wildcards [ 0 ] , run_idx ) \n            split_name [ idx ] = new_name \n    full_name = '.' . join ( split_name ) \n    return full_name "}
{"8523": "\ndef f_get_range_length ( self ) : \n    if not self . f_has_range ( ) : \n        raise TypeError ( 'Not applicable, parameter does not have a range' ) \n    else : \n        if hasattr ( self , '__len__' ) : \n            return len ( self ) \n        else : \n            raise NotImplementedError ( \"Should have implemented this.\" ) "}
{"8526": "\ndef f_get_range ( self , copy = True ) : \n    if not self . f_has_range ( ) : \n        raise TypeError ( 'Your parameter `%s` is not array, so cannot return array.' % self . v_full_name ) \n    else : \n        if copy : \n            return self . _explored_range [ : ] \n        else : \n            return self . _explored_range "}
{"8535": "\ndef _serialize_matrix ( matrix ) : \n    if ( spsp . isspmatrix_csc ( matrix ) or spsp . isspmatrix_csr ( matrix ) or spsp . isspmatrix_bsr ( matrix ) ) : \n        if matrix . size > 0 : \n            return_list = [ matrix . data , matrix . indices , matrix . indptr , matrix . shape ] \n        else : \n            return_list = [ '__empty__' , ( ) , ( ) , matrix . shape ] \n        return_names = SparseParameter . OTHER_NAME_LIST \n        if spsp . isspmatrix_csc ( matrix ) : \n            return_list = [ 'csc' ] + return_list \n        else : \n            if spsp . isspmatrix_csr ( matrix ) : \n                return_list = [ 'csr' ] + return_list \n            else : \n                if spsp . isspmatrix_bsr ( matrix ) : \n                    return_list = [ 'bsr' ] + return_list \n                else : \n                    raise RuntimeError ( 'You shall not pass!' ) \n    else : \n        if spsp . isspmatrix_dia ( matrix ) : \n            if matrix . size > 0 : \n                return_list = [ 'dia' , matrix . data , matrix . offsets , matrix . shape ] \n            else : \n                return_list = [ 'dia' , '__empty__' , ( ) , matrix . shape ] \n            return_names = SparseParameter . DIA_NAME_LIST \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    hash_list = [ ] \n    for item in return_list : \n        if type ( item ) is np . ndarray : \n            hash_list . append ( HashArray ( item ) ) \n        else : \n            hash_list . append ( item ) \n    return return_list , return_names , tuple ( hash_list ) "}
{"8537": "\ndef _reconstruct_matrix ( data_list ) : \n    matrix_format = data_list [ 0 ] \n    data = data_list [ 1 ] \n    is_empty = isinstance ( data , str ) and data == '__empty__' \n    if matrix_format == 'csc' : \n        if is_empty : \n            return spsp . csc_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . csc_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) \n    else : \n        if matrix_format == 'csr' : \n            if is_empty : \n                return spsp . csr_matrix ( data_list [ 4 ] ) \n            else : \n                return spsp . csr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) \n        else : \n            if matrix_format == 'bsr' : \n                if is_empty : \n                    return spsp . bsr_matrix ( data_list [ 4 ] ) \n                else : \n                    return spsp . bsr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) \n            else : \n                if matrix_format == 'dia' : \n                    if is_empty : \n                        return spsp . dia_matrix ( data_list [ 3 ] ) \n                    else : \n                        return spsp . dia_matrix ( tuple ( data_list [ 1 : 3 ] ) , shape = data_list [ 3 ] ) \n                else : \n                    raise RuntimeError ( 'You shall not pass!' ) "}
{"8545": "\ndef f_get ( self , * args ) : \n    if len ( args ) == 0 : \n        if len ( self . _data ) == 1 : \n            return list ( self . _data . values ( ) ) [ 0 ] \n        else : \n            if len ( self . _data ) > 1 : \n                raise ValueError ( 'Your result `%s` contains more than one entry: ' '`%s` Please use >>f_get<< with one of these.' % ( self . v_full_name , str ( list ( self . _data . keys ( ) ) ) ) ) \n            else : \n                raise AttributeError ( 'Your result `%s` is empty, cannot access data.' % self . v_full_name ) \n    result_list = [ ] \n    for name in args : \n        name = self . f_translate_key ( name ) \n        if not name in self . _data : \n            if name == 'data' and len ( self . _data ) == 1 : \n                return self . _data [ list ( self . _data . keys ( ) ) [ 0 ] ] \n            else : \n                raise AttributeError ( '`%s` is not part of your result `%s`.' % ( name , self . v_full_name ) ) \n        result_list . append ( self . _data [ name ] ) \n    if len ( args ) == 1 : \n        return result_list [ 0 ] \n    else : \n        return result_list "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    print ( 'Starting Euler Integration' ) \n    for step in range ( 1 , steps ) : \n        if V_array [ step - 1 ] >= 1 : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        else : \n            if spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : \n                V_array [ step ] = 0 \n            else : \n                dV = - 1 / tau_V * V_array [ step - 1 ] + I \n                V_array [ step ] = V_array [ step - 1 ] + dV * dt \n    print ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8600": "\ndef send_request ( self ) : \n    assert self . client \n    if self . current_state == STATE_BOUND : \n        pkt = self . client . gen_request_unicast ( ) \n    else : \n        pkt = self . client . gen_request ( ) \n    sendp ( pkt ) \n    logger . debug ( 'Modifying FSM obj, setting time_sent_request.' ) \n    self . time_sent_request = nowutc ( ) \n    logger . info ( 'DHCPREQUEST of %s on %s to %s port %s' , self . client . iface , self . client . client_ip , self . client . server_ip , self . client . server_port ) \n    if self . request_attempts < MAX_ATTEMPTS_REQUEST : \n        self . request_attempts *= 2 \n        logger . debug ( 'Increased request attempts to %s' , self . request_attempts ) \n    if self . current_state == STATE_RENEWING : \n        timeout_renewing = gen_timeout_request_renew ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_renewing , timeout_renewing ) \n    else : \n        if self . current_state == STATE_REBINDING : \n            timeout_rebinding = gen_timeout_request_rebind ( self . client . lease ) \n            self . set_timeout ( self . current_state , self . timeout_request_rebinding , timeout_rebinding ) \n        else : \n            timeout_requesting = gen_timeout_resend ( self . request_attempts ) \n            self . set_timeout ( self . current_state , self . timeout_requesting , timeout_requesting ) "}
{"8643": "\ndef copy ( self , dest ) : \n    if isinstance ( dest , File ) : \n        dest_dir = dest . get_directory ( ) \n        dest_dir . create ( ) \n        dest = dest . filename \n    else : \n        if isinstance ( dest , Directory ) : \n            dest = dest . dirname \n    shutil . copy2 ( self . filename , dest ) "}
{"8645": "\ndef backup_file ( self , file , dest_dir , copy_empty = False ) : \n    if file . exists ( ) : \n        if not copy_empty and file . is_empty ( ) : \n            return None \n        dest_dir . create ( ) \n        file . copy ( dest_dir ) \n        return dest_dir + file . get_basefile ( ) \n    else : \n        if copy_empty : \n            dest_dir = dest_dir + file . get_directory ( ) \n            dest_dir . create ( ) \n            dest_file = dest_dir + file . get_basefile ( ) \n            dest_file . touch ( ) \n            return dest_file \n        else : \n            return None "}
{"8690": "\ndef build_shape ( relation , nodes , ways ) : \n    sequence_index = 0 \n    for member_type , member_id , member_role in relation . member_info : \n        if member_id in nodes : \n            yield Shape ( relation . id , nodes [ member_id ] . lat , nodes [ member_id ] . lon , sequence_index ) \n            sequence_index += 1 \n        else : \n            if member_id in ways : \n                continue \n            else : \n                pass "}
{"8692": "\ndef send_apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : \n    if data is None : \n        data = b'' \n    else : \n        if isinstance ( data , int ) : \n            data = int2byte ( data ) \n    size = len ( data ) \n    l0 = size >> 16 & 0xff \n    l1 = size >> 8 & 0xff \n    l2 = size & 0xff \n    apdu_data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) \n    try : \n        resp = self . _do_send_apdu ( apdu_data ) \n    except Exception as e : \n        raise exc . DeviceError ( e ) \n    status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ 0 ] \n    data = resp [ : - 2 ] \n    if status != APDU_OK : \n        raise exc . APDUError ( status ) \n    return data "}
{"8697": "\ndef u2str ( data ) : \n    if isinstance ( data , dict ) : \n        return { u2str ( k ) : u2str ( v ) for k , v in data . items ( ) } \n    else : \n        if isinstance ( data , list ) : \n            return [ u2str ( x ) for x in data ] \n        else : \n            if isinstance ( data , text_type ) : \n                return data . encode ( 'utf-8' ) \n            else : \n                return data "}
{"8768": "\ndef _get_resource_url ( self , url , auto_page , data_key ) : \n    headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    response = DAO . getURL ( url , headers ) \n    if response . status != 200 : \n        raise DataFailureException ( url , response . status , response . data ) \n    data = json . loads ( response . data ) \n    self . next_page_url = self . _next_page ( response ) \n    if auto_page and self . next_page_url : \n        if isinstance ( data , list ) : \n            data . extend ( self . _get_resource_url ( self . next_page_url , True , data_key ) ) \n        else : \n            if isinstance ( data , dict ) and data_key is not None : \n                data [ data_key ] . extend ( self . _get_resource_url ( self . next_page_url , True , data_key ) [ data_key ] ) \n    return data "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if len ( unique_labels ) != 2 : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] += 1 \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( class_labels == unique_labels [ 0 ] ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ 0 ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if fraction > overall_class_fraction : \n            self . feature_map [ feature_instance ] = unique_labels [ 0 ] \n        else : \n            if fraction == overall_class_fraction : \n                self . feature_map [ feature_instance ] = self . tie_break \n            else : \n                self . feature_map [ feature_instance ] = unique_labels [ 1 ] \n    return self "}
{"8847": "\ndef fit ( self , features , targets ) : \n    self . feature_map = defaultdict ( lambda : self . default_label ) \n    self . overall_mean_trait_value = np . mean ( targets ) \n    self . mdr_matrix_values = defaultdict ( list ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . mdr_matrix_values [ feature_instance ] . append ( targets [ row_i ] ) \n    for feature_instance in self . mdr_matrix_values : \n        grid_mean_trait_value = np . mean ( self . mdr_matrix_values [ feature_instance ] ) \n        if grid_mean_trait_value > self . overall_mean_trait_value : \n            self . feature_map [ feature_instance ] = 1 \n        else : \n            if grid_mean_trait_value == self . overall_mean_trait_value : \n                self . feature_map [ feature_instance ] = self . tie_break \n            else : \n                self . feature_map [ feature_instance ] = 0 \n    self . feature_map = dict ( self . feature_map ) \n    self . mdr_matrix_values = dict ( self . mdr_matrix_values ) \n    return self "}
{"8861": "\ndef run ( file_or_code , code , in_ns , use_var_indirection , warn_on_shadowed_name , warn_on_shadowed_var , warn_on_var_indirection , ) : \n    basilisp . init ( ) \n    ctx = compiler . CompilerContext ( filename = CLI_INPUT_FILE_PATH if code else ( STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code ) , opts = { compiler . WARN_ON_SHADOWED_NAME : warn_on_shadowed_name , compiler . WARN_ON_SHADOWED_VAR : warn_on_shadowed_var , compiler . USE_VAR_INDIRECTION : use_var_indirection , compiler . WARN_ON_VAR_INDIRECTION : warn_on_var_indirection , } , ) \n    eof = object ( ) \n    with runtime . ns_bindings ( in_ns ) as ns : \n        if code : \n            print ( runtime . lrepr ( eval_str ( file_or_code , ctx , ns . module , eof ) ) ) \n        else : \n            if file_or_code == STDIN_FILE_NAME : \n                print ( runtime . lrepr ( eval_stream ( click . get_text_stream ( \"stdin\" ) , ctx , ns . module ) ) ) \n            else : \n                print ( runtime . lrepr ( eval_file ( file_or_code , ctx , ns . module ) ) ) "}
{"8871": "\ndef _assert_no_recur ( node : Node ) -> None : \n    if node . op == NodeOp . RECUR : \n        raise ParserException ( \"recur must appear in tail position\" , form = node . form , lisp_ast = node ) \n    else : \n        if node . op in { NodeOp . FN , NodeOp . LOOP } : \n            pass \n        else : \n            node . visit ( _assert_no_recur ) "}
{"8872": "\ndef _assert_recur_is_tail ( node : Node ) -> None : \n    if node . op == NodeOp . DO : \n        assert isinstance ( node , Do ) \n        for child in node . statements : \n            _assert_no_recur ( child ) \n        _assert_recur_is_tail ( node . ret ) \n    else : \n        if node . op in { NodeOp . FN , NodeOp . FN_METHOD , NodeOp . METHOD } : \n            assert isinstance ( node , ( Fn , FnMethod , Method ) ) \n            node . visit ( _assert_recur_is_tail ) \n        else : \n            if node . op == NodeOp . IF : \n                assert isinstance ( node , If ) \n                _assert_no_recur ( node . test ) \n                _assert_recur_is_tail ( node . then ) \n                _assert_recur_is_tail ( node . else_ ) \n            else : \n                if node . op in { NodeOp . LET , NodeOp . LETFN } : \n                    assert isinstance ( node , ( Let , LetFn ) ) \n                    for binding in node . bindings : \n                        assert binding . init is not None \n                        _assert_no_recur ( binding . init ) \n                    _assert_recur_is_tail ( node . body ) \n                else : \n                    if node . op == NodeOp . LOOP : \n                        assert isinstance ( node , Loop ) \n                        for binding in node . bindings : \n                            assert binding . init is not None \n                            _assert_no_recur ( binding . init ) \n                    else : \n                        if node . op == NodeOp . RECUR : \n                            pass \n                        else : \n                            if node . op == NodeOp . TRY : \n                                assert isinstance ( node , Try ) \n                                _assert_recur_is_tail ( node . body ) \n                                for catch in node . catches : \n                                    _assert_recur_is_tail ( catch ) \n                                if node . finally_ : \n                                    _assert_no_recur ( node . finally_ ) \n                            else : \n                                node . visit ( _assert_no_recur ) "}
{"8881": "\ndef _lrepr_fallback ( o : Any , human_readable : bool = False , print_dup : bool = PRINT_DUP , print_length : PrintCountSetting = PRINT_LENGTH , print_level : PrintCountSetting = PRINT_LEVEL , print_meta : bool = PRINT_META , print_readably : bool = PRINT_READABLY , ) -> str : \n    kwargs = { \"human_readable\" : human_readable , \"print_dup\" : print_dup , \"print_length\" : print_length , \"print_level\" : print_level , \"print_meta\" : print_meta , \"print_readably\" : print_readably , } \n    if isinstance ( o , bool ) : \n        return _lrepr_bool ( o ) \n    else : \n        if o is None : \n            return _lrepr_nil ( o ) \n        else : \n            if isinstance ( o , str ) : \n                return _lrepr_str ( o , human_readable = human_readable , print_readably = print_readably ) \n            else : \n                if isinstance ( o , dict ) : \n                    return _lrepr_py_dict ( o , ** kwargs ) \n                else : \n                    if isinstance ( o , list ) : \n                        return _lrepr_py_list ( o , ** kwargs ) \n                    else : \n                        if isinstance ( o , set ) : \n                            return _lrepr_py_set ( o , ** kwargs ) \n                        else : \n                            if isinstance ( o , tuple ) : \n                                return _lrepr_py_tuple ( o , ** kwargs ) \n                            else : \n                                if isinstance ( o , complex ) : \n                                    return _lrepr_complex ( o ) \n                                else : \n                                    if isinstance ( o , datetime . datetime ) : \n                                        return _lrepr_datetime ( o ) \n                                    else : \n                                        if isinstance ( o , Decimal ) : \n                                            return _lrepr_decimal ( o , print_dup = print_dup ) \n                                        else : \n                                            if isinstance ( o , Fraction ) : \n                                                return _lrepr_fraction ( o ) \n                                            else : \n                                                if isinstance ( o , Pattern ) : \n                                                    return _lrepr_pattern ( o ) \n                                                else : \n                                                    if isinstance ( o , uuid . UUID ) : \n                                                        return _lrepr_uuid ( o ) \n                                                    else : \n                                                        return repr ( o ) "}
{"8895": "\ndef _read_namespaced ( ctx : ReaderContext , allowed_suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : \n    ns : List [ str ] = [ ] \n    name : List [ str ] = [ ] \n    reader = ctx . reader \n    has_ns = False \n    while True : \n        token = reader . peek ( ) \n        if token == \"/\" : \n            reader . next_token ( ) \n            if has_ns : \n                raise SyntaxError ( \"Found '/'; expected word character\" ) \n            else : \n                if len ( name ) == 0 : \n                    name . append ( \"/\" ) \n                else : \n                    if \"/\" in name : \n                        raise SyntaxError ( \"Found '/' after '/'\" ) \n                    has_ns = True \n                    ns = name \n                    name = [ ] \n        else : \n            if ns_name_chars . match ( token ) : \n                reader . next_token ( ) \n                name . append ( token ) \n            else : \n                if allowed_suffix is not None and token == allowed_suffix : \n                    reader . next_token ( ) \n                    name . append ( token ) \n                else : \n                    break \n    ns_str = None if not has_ns else \"\" . join ( ns ) \n    name_str = \"\" . join ( name ) \n    if ns_str is None : \n        if \"/\" in name_str and name_str != \"/\" : \n            raise SyntaxError ( \"'/' character disallowed in names\" ) \n    assert ns_str is None or len ( ns_str ) > 0 \n    return ns_str , name_str "}
{"8902": "\ndef _read_sym ( ctx : ReaderContext ) -> MaybeSymbol : \n    ns , name = _read_namespaced ( ctx , allowed_suffix = \"#\" ) \n    if not ctx . is_syntax_quoted and name . endswith ( \"#\" ) : \n        raise SyntaxError ( \"Gensym may not appear outside syntax quote\" ) \n    if ns is not None : \n        if any ( map ( lambda s : len ( s ) == 0 , ns . split ( \".\" ) ) ) : \n            raise SyntaxError ( \"All '.' separated segments of a namespace \" \"must contain at least one character.\" ) \n    if name . startswith ( \".\" ) and ns is not None : \n        raise SyntaxError ( \"Symbols starting with '.' may not have a namespace\" ) \n    if ns is None : \n        if name == \"nil\" : \n            return None \n        else : \n            if name == \"true\" : \n                return True \n            else : \n                if name == \"false\" : \n                    return False \n    if ctx . is_syntax_quoted and not name . endswith ( \"#\" ) : \n        return ctx . resolve ( symbol . symbol ( name , ns ) ) \n    return symbol . symbol ( name , ns = ns ) "}
{"8904": "\ndef _read_meta ( ctx : ReaderContext ) -> IMeta : \n    start = ctx . reader . advance ( ) \n    assert start == \"^\" \n    meta = _read_next_consuming_comment ( ctx ) \n    meta_map : Optional [ lmap . Map [ LispForm , LispForm ] ] = None \n    if isinstance ( meta , symbol . Symbol ) : \n        meta_map = lmap . map ( { keyword . keyword ( \"tag\" ) : meta } ) \n    else : \n        if isinstance ( meta , keyword . Keyword ) : \n            meta_map = lmap . map ( { meta : True } ) \n        else : \n            if isinstance ( meta , lmap . Map ) : \n                meta_map = meta \n            else : \n                raise SyntaxError ( f\"Expected symbol, keyword, or map for metadata, not {type(meta)}\" ) \n    obj_with_meta = _read_next_consuming_comment ( ctx ) \n    try : \n        return obj_with_meta . with_meta ( meta_map ) \n    except AttributeError : \n        raise SyntaxError ( f\"Can not attach metadata to object of type {type(obj_with_meta)}\" ) "}
{"8905": "\ndef _read_function ( ctx : ReaderContext ) -> llist . List : \n    if ctx . is_in_anon_fn : \n        raise SyntaxError ( f\"Nested #() definitions not allowed\" ) \n    with ctx . in_anon_fn ( ) : \n        form = _read_list ( ctx ) \n    arg_set = set ( ) \n    def arg_suffix ( arg_num ) : \n        if arg_num is None : \n            return \"1\" \n        else : \n            if arg_num == \"&\" : \n                return \"rest\" \n            else : \n                return arg_num \n    def sym_replacement ( arg_num ) : \n        suffix = arg_suffix ( arg_num ) \n        return symbol . symbol ( f\"arg-{suffix}\" ) \n    def identify_and_replace ( f ) : \n        if isinstance ( f , symbol . Symbol ) : \n            if f . ns is None : \n                match = fn_macro_args . match ( f . name ) \n                if match is not None : \n                    arg_num = match . group ( 2 ) \n                    suffix = arg_suffix ( arg_num ) \n                    arg_set . add ( suffix ) \n                    return sym_replacement ( arg_num ) \n        return f \n    body = walk . postwalk ( identify_and_replace , form ) if len ( form ) > 0 else None \n    arg_list : List [ symbol . Symbol ] = [ ] \n    numbered_args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg_set ) ) ) \n    if len ( numbered_args ) > 0 : \n        max_arg = max ( numbered_args ) \n        arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] \n        if \"rest\" in arg_set : \n            arg_list . append ( _AMPERSAND ) \n            arg_list . append ( sym_replacement ( \"rest\" ) ) \n    return llist . l ( _FN , vector . vector ( arg_list ) , body ) "}
{"8907": "\ndef _expand_syntax_quote ( ctx : ReaderContext , form : IterableLispForm ) -> Iterable [ LispForm ] : \n    expanded = [ ] \n    for elem in form : \n        if _is_unquote ( elem ) : \n            expanded . append ( llist . l ( _LIST , elem [ 1 ] ) ) \n        else : \n            if _is_unquote_splicing ( elem ) : \n                expanded . append ( elem [ 1 ] ) \n            else : \n                expanded . append ( llist . l ( _LIST , _process_syntax_quoted_form ( ctx , elem ) ) ) \n    return expanded "}
{"8908": "\ndef _process_syntax_quoted_form ( ctx : ReaderContext , form : ReaderForm ) -> ReaderForm : \n    lconcat = lambda v : llist . list ( v ) . cons ( _CONCAT ) \n    if _is_unquote ( form ) : \n        return form [ 1 ] \n    else : \n        if _is_unquote_splicing ( form ) : \n            raise SyntaxError ( \"Cannot splice outside collection\" ) \n        else : \n            if isinstance ( form , llist . List ) : \n                return llist . l ( _SEQ , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) \n            else : \n                if isinstance ( form , vector . Vector ) : \n                    return llist . l ( _APPLY , _VECTOR , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) \n                else : \n                    if isinstance ( form , lset . Set ) : \n                        return llist . l ( _APPLY , _HASH_SET , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) \n                    else : \n                        if isinstance ( form , lmap . Map ) : \n                            flat_kvs = seq ( form . items ( ) ) . flatten ( ) . to_list ( ) \n                            return llist . l ( _APPLY , _HASH_MAP , lconcat ( _expand_syntax_quote ( ctx , flat_kvs ) ) ) \n                        else : \n                            if isinstance ( form , symbol . Symbol ) : \n                                if form . ns is None and form . name . endswith ( \"#\" ) : \n                                    try : \n                                        return llist . l ( _QUOTE , ctx . gensym_env [ form . name ] ) \n                                    except KeyError : \n                                        genned = symbol . symbol ( langutil . genname ( form . name [ : - 1 ] ) ) . with_meta ( form . meta ) \n                                        ctx . gensym_env [ form . name ] = genned \n                                        return llist . l ( _QUOTE , genned ) \n                                return llist . l ( _QUOTE , form ) \n                            else : \n                                return form "}
{"8914": "\ndef _read_reader_macro ( ctx : ReaderContext ) -> LispReaderForm : \n    start = ctx . reader . advance ( ) \n    assert start == \"#\" \n    token = ctx . reader . peek ( ) \n    if token == \"{\" : \n        return _read_set ( ctx ) \n    else : \n        if token == \"(\" : \n            return _read_function ( ctx ) \n        else : \n            if token == \"'\" : \n                ctx . reader . advance ( ) \n                s = _read_sym ( ctx ) \n                return llist . l ( _VAR , s ) \n            else : \n                if token == '\"' : \n                    return _read_regex ( ctx ) \n                else : \n                    if token == \"_\" : \n                        ctx . reader . advance ( ) \n                        _read_next ( ctx ) \n                        return COMMENT \n                    else : \n                        if ns_name_chars . match ( token ) : \n                            s = _read_sym ( ctx ) \n                            assert isinstance ( s , symbol . Symbol ) \n                            v = _read_next_consuming_comment ( ctx ) \n                            if s in ctx . data_readers : \n                                f = ctx . data_readers [ s ] \n                                return f ( v ) \n                            else : \n                                raise SyntaxError ( f\"No data reader found for tag #{s}\" ) \n    raise SyntaxError ( f\"Unexpected token '{token}' in reader macro\" ) "}
{"8916": "\ndef _read_next ( ctx : ReaderContext ) -> LispReaderForm : \n    reader = ctx . reader \n    token = reader . peek ( ) \n    if token == \"(\" : \n        return _read_list ( ctx ) \n    else : \n        if token == \"[\" : \n            return _read_vector ( ctx ) \n        else : \n            if token == \"{\" : \n                return _read_map ( ctx ) \n            else : \n                if begin_num_chars . match ( token ) : \n                    return _read_num ( ctx ) \n                else : \n                    if whitespace_chars . match ( token ) : \n                        reader . next_token ( ) \n                        return _read_next ( ctx ) \n                    else : \n                        if token == \":\" : \n                            return _read_kw ( ctx ) \n                        else : \n                            if token == '\"' : \n                                return _read_str ( ctx ) \n                            else : \n                                if token == \"'\" : \n                                    return _read_quoted ( ctx ) \n                                else : \n                                    if token == \"\\\\\" : \n                                        return _read_character ( ctx ) \n                                    else : \n                                        if ns_name_chars . match ( token ) : \n                                            return _read_sym ( ctx ) \n                                        else : \n                                            if token == \"#\" : \n                                                return _read_reader_macro ( ctx ) \n                                            else : \n                                                if token == \"^\" : \n                                                    return _read_meta ( ctx ) \n                                                else : \n                                                    if token == \";\" : \n                                                        return _read_comment ( ctx ) \n                                                    else : \n                                                        if token == \"`\" : \n                                                            return _read_syntax_quoted ( ctx ) \n                                                        else : \n                                                            if token == \"~\" : \n                                                                return _read_unquote ( ctx ) \n                                                            else : \n                                                                if token == \"@\" : \n                                                                    return _read_deref ( ctx ) \n                                                                else : \n                                                                    if token == \"\" : \n                                                                        return ctx . eof \n                                                                    else : \n                                                                        raise SyntaxError ( \"Unexpected token '{token}'\" . format ( token = token ) ) "}
{"8924": "\ndef _get_basilisp_bytecode ( fullname : str , mtime : int , source_size : int , cache_data : bytes ) -> List [ types . CodeType ] : \n    exc_details = { \"name\" : fullname } \n    magic = cache_data [ : 4 ] \n    raw_timestamp = cache_data [ 4 : 8 ] \n    raw_size = cache_data [ 8 : 12 ] \n    if magic != MAGIC_NUMBER : \n        message = ( f\"Incorrect magic number ({magic}) in {fullname}; expected {MAGIC_NUMBER}\" ) \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    else : \n        if len ( raw_timestamp ) != 4 : \n            message = f\"Reached EOF while reading timestamp in {fullname}\" \n            logger . debug ( message ) \n            raise EOFError ( message ) \n        else : \n            if _r_long ( raw_timestamp ) != mtime : \n                message = f\"Non-matching timestamp ({_r_long(raw_timestamp)}) in {fullname} bytecode cache; expected {mtime}\" \n                logger . debug ( message ) \n                raise ImportError ( message , ** exc_details ) \n            else : \n                if len ( raw_size ) != 4 : \n                    message = f\"Reached EOF while reading size of source in {fullname}\" \n                    logger . debug ( message ) \n                    raise EOFError ( message ) \n                else : \n                    if _r_long ( raw_size ) != source_size : \n                        message = f\"Non-matching filesize ({_r_long(raw_size)}) in {fullname} bytecode cache; expected {source_size}\" \n                        logger . debug ( message ) \n                        raise ImportError ( message , ** exc_details ) \n    return marshal . loads ( cache_data [ 12 : ] ) "}
{"8946": "\ndef __should_warn_on_redef ( ctx : GeneratorContext , defsym : sym . Symbol , safe_name : str , def_meta : lmap . Map ) -> bool : \n    no_warn_on_redef = def_meta . entry ( SYM_NO_WARN_ON_REDEF_META_KEY , False ) \n    if no_warn_on_redef : \n        return False \n    else : \n        if safe_name in ctx . current_ns . module . __dict__ : \n            return True \n        else : \n            if defsym in ctx . current_ns . interns : \n                var = ctx . current_ns . find ( defsym ) \n                assert var is not None , f\"Var {defsym} cannot be none here\" \n                if var . meta is not None and var . meta . entry ( SYM_REDEF_META_KEY ) : \n                    return False \n                else : \n                    if var . is_bound : \n                        return True \n                    else : \n                        return False \n            else : \n                return False "}
{"8953": "\ndef __if_body_to_py_ast ( ctx : GeneratorContext , node : Node , result_name : str ) -> GeneratedPyAST : \n    if node . op == NodeOp . RECUR and ctx . recur_point . type == RecurType . LOOP : \n        assert isinstance ( node , Recur ) \n        return _recur_to_py_ast ( ctx , node ) \n    else : \n        if node . op == NodeOp . DO : \n            assert isinstance ( node , Do ) \n            if_body = _synthetic_do_to_py_ast ( ctx , node . assoc ( is_body = True ) ) \n            return GeneratedPyAST ( node = ast . Assign ( targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = if_body . node ) , dependencies = list ( map ( statementize , if_body . dependencies ) ) , ) \n        else : \n            py_ast = gen_py_ast ( ctx , node ) \n            return GeneratedPyAST ( node = ast . Assign ( targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = py_ast . node ) , dependencies = py_ast . dependencies , ) "}
{"8959": "\ndef _set_bang_to_py_ast ( ctx : GeneratorContext , node : SetBang ) -> GeneratedPyAST : \n    assert node . op == NodeOp . SET_BANG \n    val_temp_name = genname ( \"set_bang_val\" ) \n    val_ast = gen_py_ast ( ctx , node . val ) \n    target = node . target \n    assert isinstance ( target , ( HostField , Local , VarRef ) ) , f\"invalid set! target type {type(target)}\" \n    if isinstance ( target , HostField ) : \n        target_ast = _interop_prop_to_py_ast ( ctx , target , is_assigning = True ) \n    else : \n        if isinstance ( target , VarRef ) : \n            target_ast = _var_sym_to_py_ast ( ctx , target , is_assigning = True ) \n        else : \n            if isinstance ( target , Local ) : \n                target_ast = _local_sym_to_py_ast ( ctx , target , is_assigning = True ) \n            else : \n                raise GeneratorException ( f\"invalid set! target type {type(target)}\" , lisp_ast = target ) \n    return GeneratedPyAST ( node = ast . Name ( id = val_temp_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val_ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val_temp_name , ctx = ast . Store ( ) ) ] , value = val_ast . node , ) ] , target_ast . dependencies , [ ast . Assign ( targets = [ target_ast . node ] , value = val_ast . node ) ] , ) ) , ) "}
{"8992": "\ndef deref ( o , timeout_s = None , timeout_val = None ) : \n    if isinstance ( o , IDeref ) : \n        return o . deref ( ) \n    else : \n        if isinstance ( o , IBlockingDeref ) : \n            return o . deref ( timeout_s , timeout_val ) \n    raise TypeError ( f\"Object of type {type(o)} cannot be dereferenced\" ) "}
{"8999": "\ndef to_py ( o , keyword_fn : Callable [ [ kw . Keyword ] , Any ] = _kw_name ) : \n    if isinstance ( o , ISeq ) : \n        return _to_py_list ( o , keyword_fn = keyword_fn ) \n    else : \n        if not isinstance ( o , ( IPersistentList , IPersistentMap , IPersistentSet , IPersistentVector ) ) : \n            return o \n        else : \n            return _to_py_backup ( o , keyword_fn = keyword_fn ) "}
{"9045": "\ndef parse_str_to_expression ( fiql_str ) : \n    nesting_lvl = 0 \n    last_element = None \n    expression = Expression ( ) \n    for ( preamble , selector , comparison , argument ) in iter_parse ( fiql_str ) : \n        if preamble : \n            for char in preamble : \n                if char == '(' : \n                    if isinstance ( last_element , BaseExpression ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Expression ) ) \n                    expression = expression . create_nested_expression ( ) \n                    nesting_lvl += 1 \n                else : \n                    if char == ')' : \n                        expression = expression . get_parent ( ) \n                        last_element = expression \n                        nesting_lvl -= 1 \n                    else : \n                        if not expression . has_constraint ( ) : \n                            raise FiqlFormatException ( \"%s proceeding initial %s\" % ( Operator , Constraint ) ) \n                        if isinstance ( last_element , Operator ) : \n                            raise FiqlFormatException ( \"%s can not be followed by %s\" % ( Operator , Operator ) ) \n                        last_element = Operator ( char ) \n                        expression = expression . add_operator ( last_element ) \n        if selector : \n            if isinstance ( last_element , BaseExpression ) : \n                raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Constraint ) ) \n            last_element = Constraint ( selector , comparison , argument ) \n            expression . add_element ( last_element ) \n    if nesting_lvl != 0 : \n        raise FiqlFormatException ( \"At least one nested expression was not correctly closed\" ) \n    if not expression . has_constraint ( ) : \n        raise FiqlFormatException ( \"Parsed string '%s' contained no constraint\" % fiql_str ) \n    return expression "}
{"9057": "\ndef add_operator ( self , operator ) : \n    if not isinstance ( operator , Operator ) : \n        raise FiqlObjectException ( \"%s is not a valid element type\" % ( operator . __class__ ) ) \n    if not self . _working_fragment . operator : \n        self . _working_fragment . operator = operator \n    else : \n        if operator > self . _working_fragment . operator : \n            last_constraint = self . _working_fragment . elements . pop ( ) \n            self . _working_fragment = self . _working_fragment . create_nested_expression ( ) \n            self . _working_fragment . add_element ( last_constraint ) \n            self . _working_fragment . add_operator ( operator ) \n        else : \n            if operator < self . _working_fragment . operator : \n                if self . _working_fragment . parent : \n                    return self . _working_fragment . parent . add_operator ( operator ) \n                else : \n                    return Expression ( ) . add_element ( self . _working_fragment ) . add_operator ( operator ) \n    return self "}
{"9074": "\ndef _handle_api_error_with_json ( http_exc , jsondata , response ) : \n    if 'code' in jsondata and 'message' in jsondata : \n        code = jsondata [ 'code' ] \n        message = jsondata [ 'message' ] \n        if code == 'error:noloop' : \n            raise YOURLSNoLoopError ( message , response = response ) \n        else : \n            if code == 'error:nourl' : \n                raise YOURLSNoURLError ( message , response = response ) \n    else : \n        if 'message' in jsondata : \n            message = jsondata [ 'message' ] \n            raise YOURLSHTTPError ( message , response = response ) \n    http_error_message = http_exc . args [ 0 ] \n    raise YOURLSHTTPError ( http_error_message , response = response ) "}
{"9075": "\ndef _validate_yourls_response ( response , data ) : \n    try : \n        response . raise_for_status ( ) \n    except HTTPError as http_exc : \n        http_error_info = sys . exc_info ( ) \n        reraise = False \n        try : \n            jsondata = response . json ( ) \n        except ValueError : \n            reraise = True \n        else : \n            logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) \n            _handle_api_error_with_json ( http_exc , jsondata , response ) \n        if reraise : \n            six . reraise ( * http_error_info ) \n    else : \n        jsondata = response . json ( ) \n        logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) \n        if { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) : \n            status = jsondata [ 'status' ] \n            code = jsondata [ 'code' ] \n            message = jsondata [ 'message' ] \n            if status == 'fail' : \n                if code == 'error:keyword' : \n                    raise YOURLSKeywordExistsError ( message , keyword = data [ 'keyword' ] ) \n                else : \n                    if code == 'error:url' : \n                        url = _json_to_shortened_url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) \n                        raise YOURLSURLExistsError ( message , url = url ) \n                    else : \n                        raise YOURLSAPIError ( message ) \n            else : \n                return jsondata \n        else : \n            return jsondata "}
{"9077": "\ndef _interp_dep_vector ( wave , indep_vector ) : \n    dep_vector_is_int = wave . dep_vector . dtype . name . startswith ( \"int\" ) \n    dep_vector_is_complex = wave . dep_vector . dtype . name . startswith ( \"complex\" ) \n    if ( wave . interp , wave . indep_scale ) == ( \"CONTINUOUS\" , \"LOG\" ) : \n        wave_interp_func = scipy . interpolate . interp1d ( np . log10 ( wave . indep_vector ) , wave . dep_vector ) \n        ret = wave_interp_func ( np . log10 ( indep_vector ) ) \n    else : \n        if ( wave . interp , wave . indep_scale ) == ( \"CONTINUOUS\" , \"LINEAR\" ) : \n            dep_vector = ( wave . dep_vector . astype ( np . float64 ) if not dep_vector_is_complex else wave . dep_vector ) \n            wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , dep_vector ) \n            ret = wave_interp_func ( indep_vector ) \n        else : \n            wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , wave . dep_vector , kind = \"zero\" ) \n            ret = wave_interp_func ( indep_vector ) \n            eq_comp = np . all ( np . isclose ( wave . indep_vector [ - 1 ] , indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) \n            if eq_comp : \n                ret [ - 1 ] = wave . dep_vector [ - 1 ] \n    round_ret = np . round ( ret , 0 ) \n    return ( round_ret . astype ( \"int\" ) if ( dep_vector_is_int and np . all ( np . isclose ( round_ret , ret , FP_RTOL , FP_ATOL ) ) ) else ret ) "}
{"9106": "\ndef _chunk_pars ( freq_vector , data_matrix , pformat ) : \n    pformat = pformat . upper ( ) \n    length = 4 \n    for freq , data in zip ( freq_vector , data_matrix ) : \n        data = data . flatten ( ) \n        for index in range ( 0 , data . size , length ) : \n            fpoint = [ freq ] if not index else [ None ] \n            cdata = data [ index : index + length ] \n            if pformat == \"MA\" : \n                vector1 = np . abs ( cdata ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            else : \n                if pformat == \"RI\" : \n                    vector1 = np . real ( cdata ) \n                    vector2 = np . imag ( cdata ) \n                else : \n                    vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) \n                    vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            sep_data = np . array ( [ ] ) \n            for item1 , item2 in zip ( vector1 , vector2 ) : \n                sep_data = np . concatenate ( ( sep_data , np . array ( [ item1 , item2 ] ) ) ) \n            ret = np . concatenate ( ( np . array ( fpoint ) , sep_data ) ) \n            yield ret "}
{"9145": "\ndef get_short_desc ( long_desc ) : \n    found = False \n    olines = [ ] \n    for line in [ item . rstrip ( ) for item in long_desc . split ( \"\\n\" ) ] : \n        if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : \n            olines . append ( line ) \n        else : \n            if found and olines and ( not line ) : \n                return ( \" \" . join ( olines ) . split ( \".\" ) [ 0 ] ) . strip ( ) \n        found = line == \".. [[[end]]]\" if not found else found \n    return \"\" "}
{"9170": "\ndef parse_docstring ( doc ) : \n    doc = inspect . cleandoc ( doc ) \n    lines = doc . split ( '\\n' ) \n    section = None \n    section_indent = None \n    params = { } \n    returns = None \n    for line in lines : \n        line = line . rstrip ( ) \n        if len ( line ) == 0 : \n            continue \n        else : \n            if str ( line ) == 'Args:' : \n                section = 'args' \n                section_indent = None \n                continue \n            else : \n                if str ( line ) == 'Returns:' : \n                    section = 'return' \n                    section_indent = None \n                    continue \n        if section is not None : \n            stripped = line . lstrip ( ) \n            margin = len ( line ) - len ( stripped ) \n            if section_indent is None : \n                section_indent = margin \n            if margin != section_indent : \n                continue \n            if section == 'args' : \n                param_name , type_info = parse_param ( stripped ) \n                params [ param_name ] = type_info \n            else : \n                if section == 'return' : \n                    returns = parse_return ( stripped ) \n    return params , returns "}
{"9176": "\ndef find_function ( self , context , funname ) : \n    if funname in self . builtins : \n        return self . builtins [ funname ] \n    func = None \n    if isinstance ( context , dict ) : \n        if funname in context : \n            func = context [ funname ] \n            if isinstance ( func , str ) : \n                func = self . _deferred_add ( func ) \n                context [ funname ] = func \n    else : \n        if hasattr ( context , funname ) : \n            func = getattr ( context , funname ) \n    if func is None : \n        raise NotFoundError ( \"Function not found\" , function = funname ) \n    return func "}
{"9179": "\ndef process_arguments ( self , func , args ) : \n    pos_args = [ ] \n    kw_args = { } \n    while len ( args ) > 0 : \n        if func . metadata . spec_filled ( pos_args , kw_args ) and not self . _is_flag ( args [ 0 ] ) : \n            break \n        arg = args . pop ( 0 ) \n        if arg == '--' : \n            break \n        else : \n            if self . _is_flag ( arg ) : \n                arg_value = None \n                arg_name = None \n                if len ( arg ) == 2 : \n                    arg_name = func . metadata . match_shortname ( arg [ 1 : ] , filled_args = pos_args ) \n                else : \n                    if not arg . startswith ( '--' ) : \n                        raise ArgumentError ( \"Invalid method of specifying keyword argument that did not start with --\" , argument = arg ) \n                    arg = arg [ 2 : ] \n                    if '=' in arg : \n                        arg , arg_value = arg . split ( '=' , 1 ) \n                    arg_name = func . metadata . match_shortname ( arg , filled_args = pos_args ) \n                arg_type = func . metadata . param_type ( arg_name ) \n                if arg_type is None : \n                    raise ArgumentError ( \"Attempting to set a parameter from command line that does not have type information\" , argument = arg_name ) \n                if arg_value is None : \n                    arg_value = self . _extract_arg_value ( arg_name , arg_type , args ) \n                kw_args [ arg_name ] = arg_value \n            else : \n                pos_args . append ( arg ) \n    if len ( args ) > 0 and args [ 0 ] == '--' : \n        args . pop ( 0 ) \n    return pos_args , kw_args , args "}
{"9181": "\ndef invoke_one ( self , line ) : \n    funname = line . pop ( 0 ) \n    context = self . contexts [ - 1 ] \n    func = self . find_function ( context , funname ) \n    if isinstance ( func , dict ) : \n        self . contexts . append ( func ) \n        self . _check_initialize_context ( ) \n        return None , line , False \n    if func . takes_cmdline is True : \n        val = func ( line ) \n        line = [ ] \n    else : \n        posargs , kwargs , line = self . process_arguments ( func , line ) \n        if inspect . isclass ( func ) and not func . metadata . spec_filled ( posargs , kwargs ) : \n            raise ValidationError ( \"Not enough parameters specified to call function\" , function = func . metadata . name , signature = func . metadata . signature ( ) ) \n        val = func ( * posargs , ** kwargs ) \n    finished = True \n    if func . finalizer is True : \n        self . contexts . pop ( ) \n    else : \n        if val is not None : \n            if func . metadata . returns_data ( ) : \n                val = func . metadata . format_returnvalue ( val ) \n            else : \n                self . contexts . append ( val ) \n                self . _check_initialize_context ( ) \n                finished = False \n                val = None \n    return val , line , finished "}
{"9188": "\ndef _join_paragraphs ( cls , lines , use_indent = False , leading_blanks = False , trailing_blanks = False ) : \n    curr_para = [ ] \n    paragraphs = [ ] \n    for line in lines : \n        if use_indent : \n            if line . startswith ( ' ' ) : \n                curr_para . append ( line . lstrip ( ) ) \n                continue \n            else : \n                if line == '' : \n                    continue \n                else : \n                    if len ( curr_para ) > 0 : \n                        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                    curr_para = [ line . lstrip ( ) ] \n        else : \n            if len ( line ) != 0 : \n                curr_para . append ( line ) \n            else : \n                paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ ] \n    if len ( curr_para ) > 0 : \n        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n    return paragraphs "}
{"9189": "\ndef wrap_and_format ( self , width = None , include_params = False , include_return = False , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        else : \n            if isinstance ( line , BlankLine ) : \n                out . write ( '\\n' ) \n            else : \n                if isinstance ( line , ListItem ) : \n                    out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ 0 ] , subsequent_indent = \"   \" , width = width ) ) \n                    out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if len ( included_params ) > 0 : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        print ( \"Returns:\" ) \n        print ( \"    \" + self . return_info . type_name ) \n    return out . getvalue ( ) "}
{"9198": "\ndef get_type ( self , type_name ) : \n    type_name = self . _canonicalize_type ( type_name ) \n    if str ( type_name ) == 'int' : \n        type_name = 'integer' \n    else : \n        if str ( type_name ) == 'str' : \n            type_name = 'string' \n        else : \n            if str ( type_name ) == 'dict' : \n                type_name = 'basic_dict' \n    if self . is_known_type ( type_name ) : \n        return self . known_types [ type_name ] \n    base_type , is_complex , subtypes = self . split_type ( type_name ) \n    if is_complex and base_type in self . type_factories : \n        self . instantiate_type ( type_name , base_type , subtypes ) \n        return self . known_types [ type_name ] \n    i = 0 \n    for i , ( source , name ) in enumerate ( self . _lazy_type_sources ) : \n        if isinstance ( source , str ) : \n            import pkg_resources \n            for entry in pkg_resources . iter_entry_points ( source ) : \n                try : \n                    mod = entry . load ( ) \n                    type_system . load_type_module ( mod ) \n                except : \n                    fail_info = ( \"Entry point group: %s, name: %s\" % ( source , entry . name ) , sys . exc_info ) \n                    logging . exception ( \"Error loading external type source from entry point, group: %s, name: %s\" , source , entry . name ) \n                    self . failed_sources . append ( fail_info ) \n        else : \n            try : \n                source ( self ) \n            except : \n                fail_info = ( \"source: %s\" % name , sys . exc_info ) \n                logging . exception ( \"Error loading external type source, source: %s\" , source ) \n                self . failed_sources . append ( fail_info ) \n        if self . is_known_type ( type_name ) or ( is_complex and base_type in self . type_factories ) : \n            break \n    self . _lazy_type_sources = self . _lazy_type_sources [ i : ] \n    if not ( self . is_known_type ( type_name ) or ( is_complex and base_type in self . type_factories ) ) : \n        raise ArgumentError ( \"get_type called on unknown type\" , type = type_name , failed_external_sources = [ x [ 0 ] for x in self . failed_sources ] ) \n    return self . get_type ( type_name ) "}
{"9206": "\ndef match_shortname ( self , name , filled_args = None ) : \n    filled_count = 0 \n    if filled_args is not None : \n        filled_count = len ( filled_args ) \n    possible = [ x for x in self . arg_names [ filled_count : ] if x . startswith ( name ) ] \n    if len ( possible ) == 0 : \n        raise ArgumentError ( \"Could not convert short-name full parameter name, none could be found\" , short_name = name , parameters = self . arg_names ) \n    else : \n        if len ( possible ) > 1 : \n            raise ArgumentError ( \"Short-name is ambiguous, could match multiple keyword parameters\" , short_name = name , possible_matches = possible ) \n    return possible [ 0 ] "}
{"9216": "\ndef _parse_validators ( valids ) : \n    outvals = [ ] \n    for val in valids : \n        if isinstance ( val , str ) : \n            args = [ ] \n        else : \n            if len ( val ) > 1 : \n                args = val [ 1 : ] \n                val = val [ 0 ] \n            else : \n                raise ValidationError ( \"You must pass either an n-tuple or a string to define a validator\" , validator = val ) \n        name = \"validate_%s\" % str ( val ) \n        outvals . append ( ( name , args ) ) \n    return outvals "}
{"9217": "\ndef find_all ( container ) : \n    if isinstance ( container , dict ) : \n        names = container . keys ( ) \n    else : \n        names = dir ( container ) \n    built_context = BasicContext ( ) \n    for name in names : \n        if name . startswith ( '_' ) : \n            continue \n        if isinstance ( container , dict ) : \n            obj = container [ name ] \n        else : \n            obj = getattr ( container , name ) \n        if isinstance ( container , dict ) and isinstance ( obj , str ) : \n            built_context [ name ] = obj \n        else : \n            if hasattr ( obj , 'metadata' ) and isinstance ( getattr ( obj , 'metadata' ) , AnnotatedMetadata ) : \n                built_context [ name ] = obj \n    return built_context "}
{"9255": "\ndef handle_input ( self , input ) : \n    dirs = { 'h' : ( - 1 , 0 ) , 'j' : ( 0 , 1 ) , 'k' : ( 0 , - 1 ) , 'l' : ( 1 , 0 ) , 'y' : ( - 1 , - 1 ) , 'u' : ( 1 , - 1 ) , 'n' : ( 1 , 1 ) , 'b' : ( - 1 , 1 ) , } \n    if input in dirs : \n        new_self = ( lens . player + dirs [ input ] ) ( self ) \n        if not new_self . player . inside ( ) : \n            return self , False \n        return new_self , True \n    else : \n        if input == '.' : \n            return self , True \n        else : \n            if input == 'q' : \n                return self . end_game ( ) , False \n            else : \n                if input == 't' : \n                    self = lens . player . set ( Vector . random ( ) ) ( self ) \n                    return self , True \n                else : \n                    return self , False "}
{"9261": "\ndef winner ( self ) : \n    for potential_win in self . _potential_wins ( ) : \n        if potential_win == tuple ( 'XXX' ) : \n            return Outcome . win_for_crosses \n        else : \n            if potential_win == tuple ( 'OOO' ) : \n                return Outcome . win_for_naughts \n    if self . _count ( ' ' ) == 0 : \n        return Outcome . draw \n    return Outcome . ongoing "}
{"9281": "\ndef _call ( self , method , params = None , request_id = None ) : \n    params = params or [ ] \n    rid = request_id or self . _id_counter \n    if request_id is None : \n        self . _id_counter += 1 \n    payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } \n    headers = { 'Content-Type' : 'application/json' } \n    scheme = 'https' if self . tls else 'http' \n    url = '{}://{}:{}' . format ( scheme , self . host , self . port ) \n    try : \n        response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) \n        response . raise_for_status ( ) \n    except HTTPError : \n        raise TransportError ( 'Got unsuccessful response from server (status code: {})' . format ( response . status_code ) , response = response ) \n    try : \n        response_data = response . json ( ) \n    except ValueError as e : \n        raise ProtocolError ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) \n    if response_data . get ( 'error' ) : \n        code = response_data [ 'error' ] . get ( 'code' , '' ) \n        message = response_data [ 'error' ] . get ( 'message' , '' ) \n        raise ProtocolError ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response_data ) \n    else : \n        if 'result' not in response_data : \n            raise ProtocolError ( 'Response is empty (result field is missing)' , response = response , data = response_data ) \n    return response_data [ 'result' ] "}
{"9284": "\ndef encode_invocation_params ( params ) : \n    final_params = [ ] \n    for p in params : \n        if isinstance ( p , bool ) : \n            final_params . append ( { 'type' : ContractParameterTypes . BOOLEAN . value , 'value' : p } ) \n        else : \n            if isinstance ( p , int ) : \n                final_params . append ( { 'type' : ContractParameterTypes . INTEGER . value , 'value' : p } ) \n            else : \n                if is_hash256 ( p ) : \n                    final_params . append ( { 'type' : ContractParameterTypes . HASH256 . value , 'value' : p } ) \n                else : \n                    if is_hash160 ( p ) : \n                        final_params . append ( { 'type' : ContractParameterTypes . HASH160 . value , 'value' : p } ) \n                    else : \n                        if isinstance ( p , bytearray ) : \n                            final_params . append ( { 'type' : ContractParameterTypes . BYTE_ARRAY . value , 'value' : p } ) \n                        else : \n                            if isinstance ( p , str ) : \n                                final_params . append ( { 'type' : ContractParameterTypes . STRING . value , 'value' : p } ) \n                            else : \n                                if isinstance ( p , list ) : \n                                    innerp = encode_invocation_params ( p ) \n                                    final_params . append ( { 'type' : ContractParameterTypes . ARRAY . value , 'value' : innerp } ) \n    return final_params "}
{"9316": "\ndef decode_obj ( obj , force = False ) : \n    if isinstance ( obj , unicode ) : \n        return obj \n    else : \n        if isinstance ( obj , bytes ) : \n            if force_encoding is not None : \n                return obj . decode ( force_encoding ) \n            if chardet : \n                enc_guess = chardet . detect ( obj ) \n                if enc_guess [ 'confidence' ] > 0.7 : \n                    return obj . decode ( enc_guess [ 'encoding' ] ) \n            return obj . decode ( 'utf-8' ) \n        else : \n            return obj if not force else repr ( obj ) "}
{"9355": "\ndef ResetView ( self , grid ) : \n    grid . BeginBatch ( ) \n    for current , new , delmsg , addmsg in [ ( self . _rows , self . GetNumberRows ( ) , gridlib . GRIDTABLE_NOTIFY_ROWS_DELETED , gridlib . GRIDTABLE_NOTIFY_ROWS_APPENDED ) , ( self . _cols , self . GetNumberCols ( ) , gridlib . GRIDTABLE_NOTIFY_COLS_DELETED , gridlib . GRIDTABLE_NOTIFY_COLS_APPENDED ) , ] : \n        if new < current : \n            msg = gridlib . GridTableMessage ( self , delmsg , new , current - new ) \n            grid . ProcessTableMessage ( msg ) \n        else : \n            if new > current : \n                msg = gridlib . GridTableMessage ( self , addmsg , new - current ) \n                grid . ProcessTableMessage ( msg ) \n                self . UpdateValues ( grid ) \n    grid . EndBatch ( ) \n    self . _rows = self . GetNumberRows ( ) \n    self . _cols = self . GetNumberCols ( ) \n    self . _updateColAttrs ( grid ) \n    grid . AdjustScrollbars ( ) \n    grid . ForceRefresh ( ) "}
{"9364": "\ndef StartingKey ( self , evt ) : \n    key = evt . GetKeyCode ( ) \n    ch = None \n    if key in [ wx . WXK_NUMPAD0 , wx . WXK_NUMPAD1 , wx . WXK_NUMPAD2 , wx . WXK_NUMPAD3 , wx . WXK_NUMPAD4 , wx . WXK_NUMPAD5 , wx . WXK_NUMPAD6 , wx . WXK_NUMPAD7 , wx . WXK_NUMPAD8 , wx . WXK_NUMPAD9 ] : \n        ch = ch = chr ( ord ( '0' ) + key - wx . WXK_NUMPAD0 ) \n    else : \n        if key < 256 and key >= 0 and chr ( key ) in string . printable : \n            ch = chr ( key ) \n            if not evt . ShiftDown ( ) : \n                ch = ch . lower ( ) \n    if ch is not None : \n        self . _tc . SetStringSelection ( ch ) \n    else : \n        evt . Skip ( ) "}
{"9375": "\ndef mangle_signature ( sig , max_chars = 30 ) : \n    s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) \n    s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) \n    s = re . sub ( r\"\\\\'\" , \"\" , s ) \n    s = re . sub ( r\"'[^']*'\" , \"\" , s ) \n    args = [ ] \n    opts = [ ] \n    opt_re = re . compile ( r\"^(.*, |)([a-zA-Z0-9_*]+)=\" ) \n    while s : \n        m = opt_re . search ( s ) \n        if not m : \n            args = s . split ( ', ' ) \n            break \n        opts . insert ( 0 , m . group ( 2 ) ) \n        s = m . group ( 1 ) [ : - 2 ] \n    sig = limited_join ( \", \" , args , max_chars = max_chars - 2 ) \n    if opts : \n        if not sig : \n            sig = \"[%s]\" % limited_join ( \", \" , opts , max_chars = max_chars - 4 ) \n        else : \n            if len ( sig ) < max_chars - 4 - 2 - 3 : \n                sig += \"[, %s]\" % limited_join ( \", \" , opts , max_chars = max_chars - len ( sig ) - 4 - 2 ) \n    return u\"(%s)\" % sig "}
{"9379": "\ndef prompt ( message = \"\" , title = \"\" , default = \"\" , multiline = False , password = None , parent = None ) : \n    if password : \n        style = wx . TE_PASSWORD | wx . OK | wx . CANCEL \n        result = dialogs . textEntryDialog ( parent , message , title , default , style ) \n    else : \n        if multiline : \n            style = wx . TE_MULTILINE | wx . OK | wx . CANCEL \n            result = dialogs . textEntryDialog ( parent , message , title , default , style ) \n            result . text = '\\n' . join ( result . text . splitlines ( ) ) \n        else : \n            result = dialogs . textEntryDialog ( parent , message , title , default ) \n    if result . accepted : \n        return result . text "}
{"9390": "\ndef build_component ( res , parent = None ) : \n    kwargs = dict ( res . items ( ) ) \n    comtype = kwargs . pop ( 'type' ) \n    if 'components' in res : \n        components = kwargs . pop ( 'components' ) \n    else : \n        if comtype == 'Menu' and 'items' in res : \n            components = kwargs . pop ( 'items' ) \n        else : \n            components = [ ] \n    from gui import registry \n    if comtype in registry . CONTROLS : \n        comclass = registry . CONTROLS [ comtype ] \n    else : \n        if comtype in registry . MENU : \n            comclass = registry . MENU [ comtype ] \n        else : \n            if comtype in registry . MISC : \n                comclass = registry . MISC [ comtype ] \n            else : \n                raise RuntimeError ( \"%s not in registry\" % comtype ) \n    com = comclass ( parent = parent , ** kwargs ) \n    for comp in components : \n        build_component ( comp , parent = com ) \n    return com "}
{"9393": "\ndef set_data ( data ) : \n    try : \n        if wx . TheClipboard . Open ( ) : \n            if isinstance ( data , ( str , unicode ) ) : \n                do = wx . TextDataObject ( ) \n                do . SetText ( data ) \n                wx . TheClipboard . SetData ( do ) \n            else : \n                if isinstance ( data , wx . Bitmap ) : \n                    do = wx . BitmapDataObject ( ) \n                    do . SetBitmap ( data ) \n                    wx . TheClipboard . SetData ( do ) \n            wx . TheClipboard . Close ( ) \n    except : \n        pass "}
{"9411": "\ndef matches_filters ( self , node ) : \n    visible = self . visible \n    if self . options [ \"text\" ] : \n        if isregex ( self . options [ \"text\" ] ) : \n            regex = self . options [ \"text\" ] \n        else : \n            if self . exact_text is True : \n                regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . options [ \"text\" ] ) ) ) \n            else : \n                regex = toregex ( self . options [ \"text\" ] ) \n        text = normalize_text ( node . all_text if visible == \"all\" else node . visible_text ) \n        if not regex . search ( text ) : \n            return False \n    if isinstance ( self . exact_text , ( bytes_ , str_ ) ) : \n        regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . exact_text ) ) ) \n        text = normalize_text ( node . all_text if visible == \"all\" else node . visible_text ) \n        if not regex . search ( text ) : \n            return False \n    if visible == \"visible\" : \n        if not node . visible : \n            return False \n    else : \n        if visible == \"hidden\" : \n            if node . visible : \n                return False \n    for name , node_filter in iter ( self . _node_filters . items ( ) ) : \n        if name in self . filter_options : \n            if not node_filter . matches ( node , self . filter_options [ name ] ) : \n                return False \n        else : \n            if node_filter . has_default : \n                if not node_filter . matches ( node , node_filter . default ) : \n                    return False \n    if self . options [ \"filter\" ] and not self . options [ \"filter\" ] ( node ) : \n        return False \n    return True "}
{"9412": "\ndef switch_to_frame ( self , frame ) : \n    if isinstance ( frame , Element ) : \n        self . driver . switch_to_frame ( frame ) \n        self . _scopes . append ( \"frame\" ) \n    else : \n        if frame == \"parent\" : \n            if self . _scopes [ - 1 ] != \"frame\" : \n                raise ScopeError ( \"`switch_to_frame(\\\"parent\\\")` cannot be called \" \"from inside a descendant frame's `scope` context.\" ) \n            self . _scopes . pop ( ) \n            self . driver . switch_to_frame ( \"parent\" ) \n        else : \n            if frame == \"top\" : \n                if \"frame\" in self . _scopes : \n                    idx = self . _scopes . index ( \"frame\" ) \n                    if any ( [ scope not in [ \"frame\" , None ] for scope in self . _scopes [ idx : ] ] ) : \n                        raise ScopeError ( \"`switch_to_frame(\\\"top\\\")` cannot be called \" \"from inside a descendant frame's `scope` context.\" ) \n                    self . _scopes = self . _scopes [ : idx ] \n                    self . driver . switch_to_frame ( \"top\" ) \n            else : \n                raise ValueError ( \"You must provide a frame element, \\\"parent\\\", or \\\"top\\\" \" \"when calling switch_to_frame\" ) "}
{"9443": "\ndef failure_message ( description , options ) : \n    message = \"expected to find {}\" . format ( description ) \n    if options [ \"count\" ] is not None : \n        message += \" {count} {times}\" . format ( count = options [ \"count\" ] , times = declension ( \"time\" , \"times\" , options [ \"count\" ] ) ) \n    else : \n        if options [ \"between\" ] is not None : \n            between = options [ \"between\" ] \n            if between : \n                first , last = between [ 0 ] , between [ - 1 ] \n            else : \n                first , last = None , None \n            message += \" between {first} and {last} times\" . format ( first = first , last = last ) \n        else : \n            if options [ \"maximum\" ] is not None : \n                message += \" at most {maximum} {times}\" . format ( maximum = options [ \"maximum\" ] , times = declension ( \"time\" , \"times\" , options [ \"maximum\" ] ) ) \n            else : \n                if options [ \"minimum\" ] is not None : \n                    message += \" at least {minimum} {times}\" . format ( minimum = options [ \"minimum\" ] , times = declension ( \"time\" , \"times\" , options [ \"minimum\" ] ) ) \n    return message "}
{"9456": "\ndef _get_logger_for_instance ( self , instance : typing . Any ) -> logging . Logger : \n    if self . logger is not None : \n        return self . logger \n    else : \n        if hasattr ( instance , \"logger\" ) and isinstance ( instance . logger , logging . Logger ) : \n            return instance . logger \n        else : \n            if hasattr ( instance , \"log\" ) and isinstance ( instance . log , logging . Logger ) : \n                return instance . log \n    return _LOGGER "}
{"9471": "\ndef v2_playbook_on_stats ( self , stats ) : \n    print ( ) \n    self . printed_last_task = False \n    self . _print_task ( \"STATS\" ) \n    hosts = sorted ( stats . processed . keys ( ) ) \n    for host in hosts : \n        s = stats . summarize ( host ) \n        if s [ \"failures\" ] or s [ \"unreachable\" ] : \n            color = \"failed\" \n        else : \n            if s [ \"changed\" ] : \n                color = \"changed\" \n            else : \n                color = \"ok\" \n        msg = \"{}    : ok={}\\tchanged={}\\tfailed={}\\tunreachable={}\" . format ( host , s [ \"ok\" ] , s [ \"changed\" ] , s [ \"failures\" ] , s [ \"unreachable\" ] ) \n        print ( colorize ( msg , color ) ) "}
{"9477": "\ndef load_dict ( self , data , overwrite = False , auto_load_model = True ) : \n    for k , v in data . items ( ) : \n        if k not in self . _elements . keys ( ) and not auto_load_model : \n            raise AttributeError ( \"Model {} is not loaded\" . format ( k ) ) \n        else : \n            if k not in self . _elements . keys ( ) and auto_load_model : \n                self . _load_model ( k ) \n        attr = getattr ( self , k ) \n        _load_dict ( attr , v ) "}
{"9484": "\ndef model_to_dict ( model , mode = \"\" , show_defaults = False ) : \n    def is_mode ( obj , mode ) : \n        if mode == \"\" : \n            return True \n        else : \n            if mode == \"config\" : \n                return obj . _yang_name == \"config\" or obj . _is_config \n            else : \n                if mode == \"state\" : \n                    return obj . _yang_name == \"state\" or not obj . _is_config \n                else : \n                    raise ValueError ( \"mode can only be config, state or ''. Passed: {}\" . format ( mode ) ) \n    def get_key ( key , model , parent_defining_module , show_defaults ) : \n        if not show_defaults : \n            key = \"{} {}\" . format ( key , \"[rw]\" if model . _is_config else \"[ro]\" ) \n        if parent_defining_module != model . _defining_module : \n            key = \"{}:{}\" . format ( model . _defining_module , key ) \n        return key \n    if model . _yang_type in ( \"container\" , \"list\" ) : \n        cls = model if model . _yang_type in ( \"container\" , ) else model . _contained_class ( ) \n        result = { } \n        for k , v in cls : \n            r = model_to_dict ( v , mode = mode , show_defaults = show_defaults ) \n            if r : \n                result [ get_key ( k , v , model . _defining_module , show_defaults ) ] = r \n        return result \n    else : \n        if show_defaults : \n            if model . _default is False : \n                if model . _yang_type != \"boolean\" : \n                    return None \n            return model . _default \n        return model . _yang_type if is_mode ( model , mode ) else None "}
{"9485": "\ndef diff ( f , s ) : \n    if isinstance ( f , base . Root ) or f . _yang_type in ( \"container\" , None ) : \n        result = _diff_root ( f , s ) \n    else : \n        if f . _yang_type in ( \"list\" , ) : \n            result = _diff_list ( f , s ) \n        else : \n            result = { } \n            first = \"{}\" . format ( f ) \n            second = \"{}\" . format ( s ) \n            if first != second : \n                result = { \"first\" : first , \"second\" : second } \n    return result "}
{"9514": "\ndef upload ( cls , file_obj , store = None ) : \n    if store is None : \n        store = 'auto' \n    else : \n        if store : \n            store = '1' \n        else : \n            store = '0' \n    data = { 'UPLOADCARE_STORE' : store , } \n    files = uploading_request ( 'POST' , 'base/' , data = data , files = { 'file' : file_obj } ) \n    file_ = cls ( files [ 'file' ] ) \n    return file_ "}
{"9515": "\ndef upload_from_url ( cls , url , store = None , filename = None ) : \n    if store is None : \n        store = 'auto' \n    else : \n        if store : \n            store = '1' \n        else : \n            store = '0' \n    data = { 'source_url' : url , 'store' : store , } \n    if filename : \n        data [ 'filename' ] = filename \n    result = uploading_request ( 'POST' , 'from_url/' , data = data ) \n    if 'token' not in result : \n        raise APIError ( 'could not find token in result: {0}' . format ( result ) ) \n    file_from_url = cls . FileFromUrl ( result [ 'token' ] ) \n    return file_from_url "}
{"9521": "\ndef uuids ( self ) : \n    for f in self . _seq : \n        if isinstance ( f , File ) : \n            yield f . uuid \n        else : \n            if isinstance ( f , six . string_types ) : \n                yield f \n            else : \n                raise ValueError ( 'Invalid type for sequence item: {0}' . format ( type ( f ) ) ) "}
{"9546": "\ndef _get_document_data ( f , image_handler = None ) : \n    if image_handler is None : \n        def image_handler ( image_id , relationship_dict ) : \n            return relationship_dict . get ( image_id ) \n    document_xml = None \n    numbering_xml = None \n    relationship_xml = None \n    styles_xml = None \n    parser = etree . XMLParser ( strip_cdata = False ) \n    path , _ = os . path . split ( f . filename ) \n    media = { } \n    image_sizes = { } \n    for item in f . infolist ( ) : \n        if item . filename == 'word/document.xml' : \n            xml = f . read ( item . filename ) \n            document_xml = etree . fromstring ( xml , parser ) \n        else : \n            if item . filename == 'word/numbering.xml' : \n                xml = f . read ( item . filename ) \n                numbering_xml = etree . fromstring ( xml , parser ) \n            else : \n                if item . filename == 'word/styles.xml' : \n                    xml = f . read ( item . filename ) \n                    styles_xml = etree . fromstring ( xml , parser ) \n                else : \n                    if item . filename == 'word/_rels/document.xml.rels' : \n                        xml = f . read ( item . filename ) \n                        try : \n                            relationship_xml = etree . fromstring ( xml , parser ) \n                        except XMLSyntaxError : \n                            relationship_xml = etree . fromstring ( '<xml></xml>' , parser ) \n        if item . filename . startswith ( 'word/media/' ) : \n            media [ item . filename [ len ( 'word/' ) : ] ] = f . extract ( item . filename , path , ) \n    f . close ( ) \n    numbering_dict = get_numbering_info ( numbering_xml ) \n    image_sizes = get_image_sizes ( document_xml ) \n    relationship_dict = get_relationship_info ( relationship_xml , media , image_sizes ) \n    styles_dict = get_style_dict ( styles_xml ) \n    font_sizes_dict = defaultdict ( int ) \n    if DETECT_FONT_SIZE : \n        font_sizes_dict = get_font_sizes_dict ( document_xml , styles_dict ) \n    meta_data = MetaData ( numbering_dict = numbering_dict , relationship_dict = relationship_dict , styles_dict = styles_dict , font_sizes_dict = font_sizes_dict , image_handler = image_handler , image_sizes = image_sizes , ) \n    return document_xml , meta_data "}
{"9548": "\ndef build_list ( li_nodes , meta_data ) : \n    ol_dict = { } \n    current_ilvl = - 1 \n    current_numId = - 1 \n    current_ol = None \n    root_ol = None \n    visited_nodes = [ ] \n    list_contents = [ ] \n    def _build_li ( list_contents ) : \n        data = '<br />' . join ( t for t in list_contents if t is not None ) \n        return etree . XML ( '<li>%s</li>' % data ) \n    def _build_non_li_content ( el , meta_data ) : \n        w_namespace = get_namespace ( el , 'w' ) \n        if el . tag == '%stbl' % w_namespace : \n            new_el , visited_nodes = build_table ( el , meta_data ) \n            return etree . tostring ( new_el ) , visited_nodes \n        else : \n            if el . tag == '%sp' % w_namespace : \n                return get_element_content ( el , meta_data ) , [ el ] \n        if has_text ( el ) : \n            raise UnintendedTag ( 'Did not expect %s' % el . tag ) \n    def _merge_lists ( ilvl , current_ilvl , ol_dict , current_ol ) : \n        for i in reversed ( range ( ilvl , current_ilvl ) ) : \n            if i not in ol_dict : \n                continue \n            if ol_dict [ i ] is not current_ol : \n                if ol_dict [ i ] is current_ol : \n                    continue \n                ol_dict [ i ] [ - 1 ] . append ( current_ol ) \n                current_ol = ol_dict [ i ] \n        for key in list ( ol_dict ) : \n            if key > ilvl : \n                del ol_dict [ key ] \n        return current_ol \n    for li_node in li_nodes : \n        w_namespace = get_namespace ( li_node , 'w' ) \n        if not is_li ( li_node , meta_data ) : \n            new_el , el_visited_nodes = _build_non_li_content ( li_node , meta_data , ) \n            list_contents . append ( new_el ) \n            visited_nodes . extend ( el_visited_nodes ) \n            continue \n        if list_contents : \n            li_el = _build_li ( list_contents ) \n            list_contents = [ ] \n            current_ol . append ( li_el ) \n        list_contents . append ( get_element_content ( li_node , meta_data , ) ) \n        ilvl = get_ilvl ( li_node , w_namespace ) \n        numId = get_numId ( li_node , w_namespace ) \n        list_type = get_ordered_list_type ( meta_data , numId , ilvl ) \n        if ( ilvl > current_ilvl ) or ( numId != current_numId ) : \n            ol_dict [ ilvl ] = create_list ( list_type ) \n            current_ol = ol_dict [ ilvl ] \n            current_ilvl = ilvl \n            current_numId = numId \n        else : \n            current_ol = _merge_lists ( ilvl = ilvl , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n        if root_ol is None : \n            root_ol = current_ol \n        if ilvl in ol_dict : \n            current_ol = ol_dict [ ilvl ] \n        else : \n            if current_ol is not root_ol : \n                root_ol [ - 1 ] . append ( current_ol ) \n                current_ol = create_list ( list_type ) \n        visited_nodes . extend ( list ( li_node . iter ( ) ) ) \n    if list_contents : \n        li_el = _build_li ( list_contents ) \n        list_contents = [ ] \n        current_ol . append ( li_el ) \n    current_ol = _merge_lists ( ilvl = 0 , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n    return root_ol , visited_nodes "}
{"9549": "\ndef build_tr ( tr , meta_data , row_spans ) : \n    tr_el = etree . Element ( 'tr' ) \n    w_namespace = get_namespace ( tr , 'w' ) \n    visited_nodes = [ ] \n    for el in tr : \n        if el in visited_nodes : \n            continue \n        visited_nodes . append ( el ) \n        if el . tag == '%stc' % w_namespace : \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) != 'restart' ) : \n                continue \n            texts = [ ] \n            for td_content in el : \n                if td_content in visited_nodes : \n                    continue \n                if is_li ( td_content , meta_data ) : \n                    li_nodes = get_single_list_nodes_data ( td_content , meta_data , ) \n                    list_el , list_visited_nodes = build_list ( li_nodes , meta_data , ) \n                    visited_nodes . extend ( list_visited_nodes ) \n                    texts . append ( etree . tostring ( list_el ) ) \n                else : \n                    if td_content . tag == '%stbl' % w_namespace : \n                        table_el , table_visited_nodes = build_table ( td_content , meta_data , ) \n                        visited_nodes . extend ( table_visited_nodes ) \n                        texts . append ( etree . tostring ( table_el ) ) \n                    else : \n                        if td_content . tag == '%stcPr' % w_namespace : \n                            visited_nodes . append ( td_content ) \n                            continue \n                        else : \n                            text = get_element_content ( td_content , meta_data , is_td = True , ) \n                            texts . append ( text ) \n            data = '<br />' . join ( t for t in texts if t is not None ) \n            td_el = etree . XML ( '<td>%s</td>' % data ) \n            colspan = get_grid_span ( el ) \n            if colspan > 1 : \n                td_el . set ( 'colspan' , '%d' % colspan ) \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) == 'restart' ) : \n                rowspan = next ( row_spans ) \n                td_el . set ( 'rowspan' , '%d' % rowspan ) \n            tr_el . append ( td_el ) \n    return tr_el "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = 0 , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , 0 ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ 0 ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if algo == 'sample' : \n            algo = trainer . SampleTrainer ( self ) \n        else : \n            if algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n                algo = trainer . SupervisedPretrainer ( subalgo , self ) \n            else : \n                if algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n                    algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n                else : \n                    algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return False \n        if isinstance ( save_every , float ) : \n            return elapsed > 60 * save_every \n        if isinstance ( save_every , int ) : \n            return iteration % save_every == 0 \n        return False \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9604": "\ndef resolve_outputs ( self ) : \n    input_shape = None \n    for i , shape in enumerate ( self . _input_shapes . values ( ) ) : \n        if i == 0 : \n            input_shape = shape \n        if len ( input_shape ) != len ( shape ) or any ( a is not None and b is not None and a != b for a , b in zip ( input_shape [ : - 1 ] , shape [ : - 1 ] ) ) : \n            raise util . ConfigurationError ( 'layer \"{}\" incompatible input shapes {}' . format ( self . name , self . _input_shapes ) ) \n    size = self . kwargs . get ( 'size' ) \n    shape = self . kwargs . get ( 'shape' ) \n    if shape is not None : \n        pass \n    else : \n        if size is not None : \n            shape = tuple ( input_shape [ : - 1 ] ) + ( size , ) \n        else : \n            raise util . ConfigurationError ( 'layer \"{}\" does not specify a size' . format ( self . name ) ) \n    self . _output_shapes [ 'out' ] = shape "}
{"9614": "\ndef add_tier ( self , name , tier_type = 'IntervalTier' , number = None ) : \n    if number is None : \n        number = 1 if not self . tiers else len ( self . tiers ) + 1 \n    else : \n        if number < 1 or number > len ( self . tiers ) : \n            raise ValueError ( 'Number not in [1..{}]' . format ( len ( self . tiers ) ) ) \n        else : \n            if tier_type not in Tier . P_TIERS : \n                raise ValueError ( 'tier_type has to be in {}' . format ( self . P_TIERS ) ) \n    self . tiers . insert ( number - 1 , Tier ( self . xmin , self . xmax , name , tier_type ) ) \n    return self . tiers [ number - 1 ] "}
{"9645": "\ndef merge_tiers ( self , tiers , tiernew = None , gapt = 0 , sep = '_' , safe = False ) : \n    if tiernew is None : \n        tiernew = u'{}_merged' . format ( '_' . join ( tiers ) ) \n    self . add_tier ( tiernew ) \n    aa = [ ( sys . maxsize , sys . maxsize , None ) ] + sorted ( ( a for t in tiers for a in self . get_annotation_data_for_tier ( t ) ) , reverse = True ) \n    l = None \n    while aa : \n        begin , end , value = aa . pop ( ) \n        if l is None : \n            l = [ begin , end , [ value ] ] \n        else : \n            if begin - l [ 1 ] >= gapt : \n                if not safe or l [ 1 ] > l [ 0 ] : \n                    self . add_annotation ( tiernew , l [ 0 ] , l [ 1 ] , sep . join ( l [ 2 ] ) ) \n                l = [ begin , end , [ value ] ] \n            else : \n                if end > l [ 1 ] : \n                    l [ 1 ] = end \n                l [ 2 ] . append ( value ) \n    return tiernew "}
{"9656": "\ndef shift_annotations ( self , time ) : \n    total_re = [ ] \n    total_sq = [ ] \n    for name , tier in self . tiers . items ( ) : \n        squashed = [ ] \n        for aid , ( begin , end , value , _ ) in tier [ 0 ] . items ( ) : \n            if self . timeslots [ end ] + time <= 0 : \n                squashed . append ( ( name , aid ) ) \n            else : \n                if self . timeslots [ begin ] + time < 0 : \n                    total_sq . append ( ( name , self . timeslots [ begin ] , self . timeslots [ end ] , value ) ) \n                    self . timeslots [ begin ] = 0 \n                else : \n                    self . timeslots [ begin ] += time \n                    self . timeslots [ end ] += time \n        for name , aid in squashed : \n            start , end , value , _ = self . tiers [ name ] [ 0 ] [ aid ] \n            del ( self . tiers [ name ] [ 0 ] [ aid ] ) \n            del ( self . annotations [ aid ] ) \n            total_re . append ( ( name , self . timeslots [ start ] , self . timeslots [ end ] , value ) ) \n    return total_sq , total_re "}
{"9659": "\ndef get_base_level ( text , upper_is_rtl = False ) : \n    base_level = None \n    prev_surrogate = False \n    for _ch in text : \n        if _IS_UCS2 and ( _SURROGATE_MIN <= ord ( _ch ) <= _SURROGATE_MAX ) : \n            prev_surrogate = _ch \n            continue \n        else : \n            if prev_surrogate : \n                _ch = prev_surrogate + _ch \n                prev_surrogate = False \n        if upper_is_rtl and _ch . isupper ( ) : \n            base_level = 1 \n            break \n        bidi_type = bidirectional ( _ch ) \n        if bidi_type in ( 'AL' , 'R' ) : \n            base_level = 1 \n            break \n        else : \n            if bidi_type == 'L' : \n                base_level = 0 \n                break \n    if base_level is None : \n        base_level = 0 \n    return base_level "}
{"9660": "\ndef get_embedding_levels ( text , storage , upper_is_rtl = False , debug = False ) : \n    prev_surrogate = False \n    base_level = storage [ 'base_level' ] \n    for _ch in text : \n        if _IS_UCS2 and ( _SURROGATE_MIN <= ord ( _ch ) <= _SURROGATE_MAX ) : \n            prev_surrogate = _ch \n            continue \n        else : \n            if prev_surrogate : \n                _ch = prev_surrogate + _ch \n                prev_surrogate = False \n        if upper_is_rtl and _ch . isupper ( ) : \n            bidi_type = 'R' \n        else : \n            bidi_type = bidirectional ( _ch ) \n        storage [ 'chars' ] . append ( { 'ch' : _ch , 'level' : base_level , 'type' : bidi_type , 'orig' : bidi_type } ) \n    if debug : \n        debug_storage ( storage , base_info = True ) "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != 0 : \n                overflow_counter += 1 \n                continue \n            new_level = level_func ( embedding_level ) \n            if new_level < EXPLICIT_LEVEL_LIMIT : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            else : \n                if embedding_level == EXPLICIT_LEVEL_LIMIT - 2 : \n                    almost_overflow_counter += 1 \n                else : \n                    overflow_counter += 1 \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if directional_override != 'N' : \n                    _ch [ 'type' ] = directional_override \n            else : \n                if bidi_type == 'PDF' : \n                    if overflow_counter : \n                        overflow_counter -= 1 \n                    else : \n                        if almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - 1 : \n                            almost_overflow_counter -= 1 \n                        else : \n                            if levels : \n                                embedding_level , directional_override = levels . pop ( ) \n                else : \n                    if bidi_type == 'B' : \n                        levels . clear ( ) \n                        overflow_counter = almost_overflow_counter = 0 \n                        embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                        directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        else : \n            if should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n                _ch [ 'level' ] = storage [ 'base_level' ] \n            else : \n                should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if char_level > highest_level : \n            highest_level = char_level \n        if char_level % 2 and char_level < lowest_odd_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end -= 1 \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9708": "\ndef respond ( self , message , channel = None , nick = None ) : \n    if channel : \n        if not channel . startswith ( '#' ) : \n            channel = '#%s' % channel \n        self . send ( 'PRIVMSG %s :%s' % ( channel , message ) ) \n    else : \n        if nick : \n            self . send ( 'PRIVMSG %s :%s' % ( nick , message ) ) "}
{"9740": "\ndef delete ( self , request , * args , ** kwargs ) : \n    auth = get_authorization_header ( request ) . split ( ) \n    if not auth or auth [ 0 ] . lower ( ) != b'token' : \n        return response . Response ( status = status . HTTP_400_BAD_REQUEST ) \n    if len ( auth ) == 1 : \n        msg = 'Invalid token header. No credentials provided.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    else : \n        if len ( auth ) > 2 : \n            msg = 'Invalid token header. Token string should not contain spaces.' \n            return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    try : \n        token = self . model . objects . get ( key = auth [ 1 ] ) \n    except self . model . DoesNotExist : \n        pass \n    else : \n        token . delete ( ) \n        signals . user_logged_out . send ( type ( self ) , user = token . user , request = request , ) \n    return response . Response ( status = status . HTTP_204_NO_CONTENT ) "}
{"9758": "\ndef _assign_auth_values ( self , http_auth ) : \n    if not http_auth : \n        pass \n    else : \n        if isinstance ( http_auth , ( tuple , list ) ) : \n            self . _auth_user , self . _auth_password = http_auth \n        else : \n            if isinstance ( http_auth , str ) : \n                self . _auth_user , self . _auth_password = http_auth . split ( ':' ) \n            else : \n                raise ValueError ( 'HTTP Auth Credentials should be str or ' 'tuple, not %s' % type ( http_auth ) ) "}
{"9762": "\ndef bytes_to_readable ( num ) : \n    if num < 512 : \n        return \"0 Kb\" \n    else : \n        if num < 1024 : \n            return \"1 Kb\" \n    for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : \n        if abs ( num ) < 1024.0 : \n            return \"%3.1f%s\" % ( num , unit ) \n        num /= 1024.0 \n    return \"%.1f%s\" % ( num , 'Yb' ) "}
{"9779": "\ndef for_request ( request , body = None ) : \n    tenant , jwt_data = Tenant . objects . for_request ( request , body ) \n    webhook_sender_id = jwt_data . get ( 'sub' ) \n    sender_data = None \n    if body and 'item' in body : \n        if 'sender' in body [ 'item' ] : \n            sender_data = body [ 'item' ] [ 'sender' ] \n        else : \n            if 'message' in body [ 'item' ] and 'from' in body [ 'item' ] [ 'message' ] : \n                sender_data = body [ 'item' ] [ 'message' ] [ 'from' ] \n    if sender_data is None : \n        if webhook_sender_id is None : \n            raise BadTenantError ( 'Cannot identify sender in tenant' ) \n        sender_data = { 'id' : webhook_sender_id } \n    return Context ( tenant = tenant , sender = HipchatUser ( id = sender_data . get ( 'id' ) , name = sender_data . get ( 'name' ) , mention_name = sender_data . get ( 'mention_name' ) , ) , signed_request = request . GET . get ( 'signed_request' ) , context = jwt_data . get ( 'context' ) or { } , ) "}
{"9785": "\ndef do_GET ( self ) : \n    parsed_url = urlparse ( self . path ) \n    if parsed_url [ 2 ] == \"/\" + SERVER_REDIRECT_PATH : \n        parsed_query = parse_qs ( parsed_url [ 4 ] ) \n        if \"code\" not in parsed_query : \n            self . send_response ( 200 ) \n            self . send_header ( \"Content-Type\" , \"text/plain\" ) \n            self . end_headers ( ) \n            self . wfile . write ( \"No code found, try again!\" . encode ( \"utf-8\" ) ) \n            return \n        self . server . response_code = parsed_query [ \"code\" ] [ 0 ] \n        self . send_response ( 200 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"Thank you for using OAuth2Util. The authorization was successful, \" \"you can now close this window.\" . encode ( \"utf-8\" ) ) \n    else : \n        if parsed_url [ 2 ] == \"/\" + SERVER_LINK_PATH : \n            self . send_response ( 200 ) \n            self . send_header ( \"Content-Type\" , \"text/html\" ) \n            self . end_headers ( ) \n            self . wfile . write ( \"<html><body>Hey there!<br/>Click <a href=\\\"{0}\\\">here</a> to claim your prize.</body></html>\" . format ( self . server . authorize_url ) . encode ( \"utf-8\" ) ) \n        else : \n            self . send_response ( 404 ) \n            self . send_header ( \"Content-Type\" , \"text/plain\" ) \n            self . end_headers ( ) \n            self . wfile . write ( \"404 not found\" . encode ( \"utf-8\" ) ) "}
{"9796": "\ndef split_full_path ( path ) : \n    if path . startswith ( 's3://' ) : \n        path = path [ 5 : ] \n    else : \n        if path . startswith ( 's3n://' ) : \n            path = path [ 6 : ] \n        else : \n            if path . startswith ( 's3a://' ) : \n                path = path [ 6 : ] \n            else : \n                raise ValueError ( \"S3 path should start with s3://, s3n:// or \" \"s3a:// prefix\" ) \n    parts = path . split ( '/' ) \n    bucket = parts [ 0 ] \n    path = '/' . join ( parts [ 1 : ] ) \n    return bucket , normalize_prefix ( path ) "}
{"9808": "\ndef get_used_template ( response ) : \n    if not hasattr ( response , 'template_name' ) : \n        return None , None \n    template = response . template_name \n    if template is None : \n        return None , None \n    if isinstance ( template , ( list , tuple ) ) : \n        if len ( template ) == 1 : \n            return template [ 0 ] , None \n        else : \n            used_name = _get_used_template_name ( template ) \n            return used_name , template \n    else : \n        if isinstance ( template , six . string_types ) : \n            return template , None \n        else : \n            filename = _get_template_filename ( template ) \n            template_name = '<template object from {0}>' . format ( filename ) if filename else '<template object>' \n            return template_name , None "}
{"9812": "\ndef pformat_django_context_html ( object ) : \n    if isinstance ( object , QuerySet ) : \n        text = '' \n        lineno = 0 \n        for item in object . all ( ) [ : 21 ] : \n            lineno += 1 \n            if lineno >= 21 : \n                text += u'   (remaining items truncated...)' \n                break \n            text += u'   {0}\\n' . format ( escape ( repr ( item ) ) ) \n        return text \n    else : \n        if isinstance ( object , Manager ) : \n            return mark_safe ( u'    (use <kbd>.all</kbd> to read it)' ) \n        else : \n            if isinstance ( object , six . string_types ) : \n                return escape ( repr ( object ) ) \n            else : \n                if isinstance ( object , Promise ) : \n                    return escape ( _format_lazy ( object ) ) \n                else : \n                    if isinstance ( object , dict ) : \n                        return _format_dict ( object ) \n                    else : \n                        if isinstance ( object , list ) : \n                            return _format_list ( object ) \n                        else : \n                            if hasattr ( object , '__dict__' ) : \n                                return _format_object ( object ) \n                            else : \n                                text = DebugPrettyPrinter ( width = 200 ) . pformat ( object ) \n                                return _style_text ( text ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and ord ( ch ) < 127 ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and lch [ 0 : 1 ] == '\\\\' else lch ) \n            else : \n                if ( ( ord ( ch ) >= 32 and ord ( ch ) <= 127 ) or ( ch in \"\\n\\r\\t\" ) ) : \n                    result += ch \n                else : \n                    msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                    if fail_bad_chars : \n                        raise ValueError ( msg ) \n                    log . warning ( msg ) \n                    if substitute_bad_chars : \n                        result += r'{\\bfseries ?}' \n                    else : \n                        result += ch \n    return result "}
{"9920": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    if self . qos : \n        header [ 0 ] = 0x30 | self . retain | ( self . qos << 1 ) | ( self . dup << 3 ) \n        varHeader . extend ( encodeString ( self . topic ) ) \n        varHeader . extend ( encode16Int ( self . msgId ) ) \n    else : \n        header [ 0 ] = 0x30 | self . retain \n        varHeader . extend ( encodeString ( self . topic ) ) \n    if isinstance ( self . payload , bytearray ) : \n        payload . extend ( self . payload ) \n    else : \n        if isinstance ( self . payload , str ) : \n            payload . extend ( bytearray ( self . payload , encoding = 'utf-8' ) ) \n        else : \n            raise PayloadTypeError ( type ( self . payload ) ) \n    totalLen = len ( varHeader ) + len ( payload ) \n    if totalLen > 268435455 : \n        raise PayloadValueError ( totalLen ) \n    header . extend ( encodeLength ( totalLen ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9937": "\ndef unitpicker ( a , llim = 0.1 , denominator = None , focus_stage = None ) : \n    if not isinstance ( a , ( int , float ) ) : \n        a = nominal_values ( a ) \n        a = np . percentile ( a [ ~ np . isnan ( a ) ] , 25 ) \n    if denominator is not None : \n        pd = pretty_element ( denominator ) \n    else : \n        pd = '' \n    if focus_stage == 'calibrated' : \n        udict = { 0 : 'mol/mol ' + pd , 1 : 'mmol/mol ' + pd , 2 : '$\\mu$mol/mol ' + pd , 3 : 'nmol/mol ' + pd , 4 : 'pmol/mol ' + pd , 5 : 'fmol/mol ' + pd } \n    else : \n        if focus_stage == 'ratios' : \n            udict = { 0 : 'counts/count ' + pd , 1 : '$10^{-3}$ counts/count ' + pd , 2 : '$10^{-6}$ counts/count ' + pd , 3 : '$10^{-9}$ counts/count ' + pd , 4 : '$10^{-12}$ counts/count ' + pd , 5 : '$10^{-15}$ counts/count ' + pd } \n        else : \n            if focus_stage in ( 'rawdata' , 'despiked' , 'bkgsub' ) : \n                udict = udict = { 0 : 'counts' , 1 : '$10^{-3}$ counts' , 2 : '$10^{-6}$ counts' , 3 : '$10^{-9}$ counts' , 4 : '$10^{-12}$ counts' , 5 : '$10^{-15}$ counts' } \n            else : \n                udict = { 0 : '' , 1 : '' , 2 : '' , 3 : '' , 4 : '' , 5 : '' } \n    a = abs ( a ) \n    n = 0 \n    if a < llim : \n        while a < llim : \n            a *= 1000 \n            n += 1 \n    return float ( 1000 ** n ) , udict [ n ] "}
{"9953": "\ndef print_all ( ) : \n    _ , conf = read_latoolscfg ( ) \n    default = conf [ 'DEFAULT' ] [ 'config' ] \n    pstr = '\\nCurrently defined LAtools configurations:\\n\\n' \n    for s in conf . sections ( ) : \n        if s == default : \n            pstr += s + ' [DEFAULT]\\n' \n        else : \n            if s == 'REPRODUCE' : \n                pstr += s + ' [DO NOT ALTER]\\n' \n            else : \n                pstr += s + '\\n' \n        for k , v in conf [ s ] . items ( ) : \n            if k != 'config' : \n                if v [ : 9 ] == 'resources' : \n                    v = pkgrs . resource_filename ( 'latools' , v ) \n                pstr += '   ' + k + ': ' + v + '\\n' \n        pstr += '\\n' \n    print ( pstr ) \n    return "}
{"9960": "\ndef autorange_plot ( self , analyte = 'total_counts' , gwin = 7 , swin = None , win = 20 , on_mult = [ 1.5 , 1. ] , off_mult = [ 1. , 1.5 ] , transform = 'log' ) : \n    if analyte is None : \n        sig = self . data [ 'total_counts' ] \n    else : \n        if analyte == 'total_counts' : \n            sig = self . data [ 'total_counts' ] \n        else : \n            if analyte in self . analytes : \n                sig = self . focus [ analyte ] \n            else : \n                raise ValueError ( 'Invalid analyte.' ) \n    if transform == 'log' : \n        sig = np . log10 ( sig ) \n    fig , axs = plot . autorange_plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on_mult = on_mult , off_mult = off_mult ) \n    return fig , axs "}
{"9964": "\ndef sample_stats ( self , analytes = None , filt = True , stat_fns = { } , eachtrace = True ) : \n    if analytes is None : \n        analytes = self . analytes \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats [ 'analytes' ] = analytes \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) \n        for n , f in stat_fns . items ( ) : \n            self . stats [ n ] = [ ] \n            for a in analytes : \n                ind = self . filt . grab_filt ( filt , a ) \n                dat = nominal_values ( self . focus [ a ] ) \n                if eachtrace : \n                    sts = [ ] \n                    for t in np . arange ( self . n ) + 1 : \n                        sts . append ( f ( dat [ ind & ( self . ns == t ) ] ) ) \n                    self . stats [ n ] . append ( sts ) \n                else : \n                    self . stats [ n ] . append ( f ( dat [ ind ] ) ) \n            self . stats [ n ] = np . array ( self . stats [ n ] ) \n    return "}
{"9986": "\ndef bkg_calc_weightedmean ( self , analytes = None , weight_fwhm = None , n_min = 20 , n_max = None , cstep = None , bkg_filter = False , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    if weight_fwhm is None : \n        weight_fwhm = 600 \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    if 'calc' not in self . bkg . keys ( ) : \n        if cstep is None : \n            cstep = weight_fwhm / 20 \n        else : \n            if cstep > weight_fwhm : \n                warnings . warn ( \"\\ncstep should be less than weight_fwhm. Your backgrounds\\n\" + \"might not behave as expected.\\n\" ) \n        bkg_t = np . linspace ( 0 , self . max_time , self . max_time // cstep ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    mean , std , stderr = gauss_weighted_stats ( self . bkg [ 'raw' ] . uTime , self . bkg [ 'raw' ] . loc [ : , analytes ] . values , self . bkg [ 'calc' ] [ 'uTime' ] , fwhm = weight_fwhm ) \n    for i , a in enumerate ( analytes ) : \n        self . bkg [ 'calc' ] [ a ] = { 'mean' : mean [ i ] , 'std' : std [ i ] , 'stderr' : stderr [ i ] } "}
{"9987": "\ndef bkg_calc_interp1d ( self , analytes = None , kind = 1 , n_min = 10 , n_max = None , cstep = None , bkg_filter = False , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    def pad ( a , lo = None , hi = None ) : \n        if lo is None : \n            lo = [ a [ 0 ] ] \n        if hi is None : \n            hi = [ a [ - 1 ] ] \n        return np . concatenate ( ( lo , a , hi ) ) \n    if 'calc' not in self . bkg . keys ( ) : \n        bkg_t = pad ( self . bkg [ 'summary' ] . loc [ : , ( 'uTime' , 'mean' ) ] , [ 0 ] , [ self . max_time ] ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    d = self . bkg [ 'summary' ] \n    with self . pbar . set ( total = len ( analytes ) , desc = 'Calculating Analyte Backgrounds' ) as prog : \n        for a in analytes : \n            self . bkg [ 'calc' ] [ a ] = { 'mean' : pad ( d . loc [ : , ( a , 'mean' ) ] . values ) , 'std' : pad ( d . loc [ : , ( a , 'std' ) ] . values ) , 'stderr' : pad ( d . loc [ : , ( a , 'stderr' ) ] . values ) } \n            prog . update ( ) \n    self . bkg [ 'calc' ] \n    return "}
{"9988": "\ndef bkg_subtract ( self , analytes = None , errtype = 'stderr' , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    if focus_stage == 'despiked' : \n        if 'despiked' not in self . stages_complete : \n            focus_stage = 'rawdata' \n    bkg_interps = { } \n    for a in analytes : \n        bkg_interps [ a ] = un_interp1d ( x = self . bkg [ 'calc' ] [ 'uTime' ] , y = un . uarray ( self . bkg [ 'calc' ] [ a ] [ 'mean' ] , self . bkg [ 'calc' ] [ a ] [ errtype ] ) ) \n    self . bkg_interps = bkg_interps \n    with self . pbar . set ( total = len ( self . data ) , desc = 'Background Subtraction' ) as prog : \n        for d in self . data . values ( ) : \n            [ d . bkg_subtract ( a , bkg_interps [ a ] . new ( d . uTime ) , ~ d . sig , focus_stage = focus_stage ) for a in analytes ] \n            d . setfocus ( 'bkgsub' ) \n            prog . update ( ) \n    self . stages_complete . update ( [ 'bkgsub' ] ) \n    self . focus_stage = 'bkgsub' \n    return "}
{"9991": "\ndef filter_gradient_threshold_percentile ( self , analyte , percentiles , level = 'population' , win = 15 , filt = False , samples = None , subset = None ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ analyte ] ) \n    self . get_gradients ( analytes = [ analyte ] , win = win , filt = filt , subset = subset ) \n    grad = self . gradients [ analyte ] [ ~ np . isnan ( self . gradients [ analyte ] ) ] \n    if isinstance ( percentiles , ( int , float ) ) : \n        percentiles = [ percentiles ] \n    if level == 'population' : \n        lims = np . percentile ( grad , percentiles ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Percentile Threshold Filter' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            setn = d . filt . maxset + 1 \n            g = calc_grads ( d . Time , d . focus , [ analyte ] , win ) [ analyte ] \n            if level == 'individual' : \n                gt = nominal_values ( g ) \n                lims = np . percentile ( gt [ ~ np . isnan ( gt ) ] , percentiles ) \n            if len ( lims ) == 1 : \n                above = g >= lims [ 0 ] \n                below = g < lims [ 0 ] \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_below' . format ( percentiles [ 0 ] ) , below , 'Gradients below {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_above' . format ( percentiles [ 0 ] ) , above , 'Gradients above {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n            else : \n                if len ( lims ) == 2 : \n                    inside = ( g >= min ( lims ) ) & ( g <= max ( lims ) ) \n                    outside = ( g < min ( lims ) ) | ( g > max ( lims ) ) \n                    lpc = '-' . join ( [ '{:.1f}' . format ( p ) for p in percentiles ] ) \n                    d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_inside' , inside , 'Gradients between ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n                    d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_outside' , outside , 'Gradients outside ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n            prog . update ( ) \n    return "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                else : \n                    if int ( n ) in self . subsets : \n                        n = int ( n ) \n                        pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n            if len ( self . subsets [ 'not_in_set' ] ) > 0 : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    else : \n        if sample is not None : \n            s += 'Sample: ' + sample + '\\n' \n            s += self . data [ sample ] . filt . __repr__ ( ) \n            print ( s ) \n            return \n        else : \n            if subset is not None : \n                if isinstance ( subset , ( str , int , float ) ) : \n                    subset = [ subset ] \n                for n in subset : \n                    s += 'Subset: ' + str ( n ) + '\\n' \n                    s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                    s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n                print ( s ) \n                return "}
{"10003": "\ndef trace_plots ( self , analytes = None , samples = None , ranges = False , focus = None , outdir = None , filt = None , scale = 'log' , figsize = [ 10 , 4 ] , stats = False , stat = 'nanmean' , err = 'nanstd' , subset = 'All_Analyses' ) : \n    if focus is None : \n        focus = self . focus_stage \n    if outdir is None : \n        outdir = self . report_dir + '/' + focus \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if subset is not None : \n        samples = self . _get_samples ( subset ) \n    else : \n        if samples is None : \n            samples = self . subsets [ 'All_Analyses' ] \n        else : \n            if isinstance ( samples , str ) : \n                samples = [ samples ] \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            f , a = self . data [ s ] . tplot ( analytes = analytes , figsize = figsize , scale = scale , filt = filt , ranges = ranges , stats = stats , stat = stat , err = err , focus_stage = focus ) \n            f . savefig ( outdir + '/' + s + '_traces.pdf' ) \n            plt . close ( f ) \n            prog . update ( ) \n    return "}
{"10004": "\ndef gradient_plots ( self , analytes = None , win = 15 , samples = None , ranges = False , focus = None , outdir = None , figsize = [ 10 , 4 ] , subset = 'All_Analyses' ) : \n    if focus is None : \n        focus = self . focus_stage \n    if outdir is None : \n        outdir = self . report_dir + '/' + focus + '_gradient' \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if subset is not None : \n        samples = self . _get_samples ( subset ) \n    else : \n        if samples is None : \n            samples = self . subsets [ 'All_Analyses' ] \n        else : \n            if isinstance ( samples , str ) : \n                samples = [ samples ] \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            f , a = self . data [ s ] . gplot ( analytes = analytes , win = win , figsize = figsize , ranges = ranges , focus_stage = focus ) \n            f . savefig ( outdir + '/' + s + '_gradients.pdf' ) \n            plt . close ( f ) \n            prog . update ( ) \n    return "}
{"10006": "\ndef sample_stats ( self , analytes = None , filt = True , stats = [ 'mean' , 'std' ] , eachtrace = True , csf_dict = { } ) : \n    if analytes is None : \n        analytes = self . analytes \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats_calced = [ ] \n    stat_fns = Bunch ( ) \n    stat_dict = { 'mean' : np . nanmean , 'std' : np . nanstd , 'nanmean' : np . nanmean , 'nanstd' : np . nanstd , 'se' : stderr , 'H15_mean' : H15_mean , 'H15_std' : H15_std , 'H15_se' : H15_se } \n    for s in stats : \n        if isinstance ( s , str ) : \n            if s in stat_dict . keys ( ) : \n                self . stats_calced . append ( s ) \n                stat_fns [ s ] = stat_dict [ s ] \n            if s in csf_dict . keys ( ) : \n                self . stats_calced . append ( s ) \n                exec ( csf_dict [ s ] ) \n                stat_fns [ s ] = eval ( s ) \n        else : \n            if callable ( s ) : \n                self . stats_calced . append ( s . __name__ ) \n                stat_fns [ s . __name__ ] = s \n                if not hasattr ( self , 'custom_stat_functions' ) : \n                    self . custom_stat_functions = '' \n                self . custom_stat_functions += inspect . getsource ( s ) + '\\n\\n\\n\\n' \n    with self . pbar . set ( total = len ( self . samples ) , desc = 'Calculating Stats' ) as prog : \n        for s in self . samples : \n            if self . srm_identifier not in s : \n                self . data [ s ] . sample_stats ( analytes , filt = filt , stat_fns = stat_fns , eachtrace = eachtrace ) \n                self . stats [ s ] = self . data [ s ] . stats \n            prog . update ( ) "}
{"10008": "\ndef _minimal_export_traces ( self , outdir = None , analytes = None , samples = None , subset = 'All_Analyses' ) : \n    if analytes is None : \n        analytes = self . analytes \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    focus_stage = 'rawdata' \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    for s in samples : \n        d = self . data [ s ] . data [ focus_stage ] \n        out = Bunch ( ) \n        for a in analytes : \n            out [ a ] = d [ a ] \n        out = pd . DataFrame ( out , index = self . data [ s ] . Time ) \n        out . index . name = 'Time' \n        d = dateutil . parser . parse ( self . data [ s ] . meta [ 'date' ] ) \n        header = [ '# Minimal Reproduction Dataset Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , \"# Analysis described in '../analysis.lalog'\" , '# Run latools.reproduce to import analysis.' , '#' , '# Sample: %s' % ( s ) , '# Analysis Time: ' + d . strftime ( '%Y-%m-%d %H:%M:%S' ) ] \n        header = '\\n' . join ( header ) + '\\n' \n        csv = out . to_csv ( ) \n        with open ( '%s/%s.csv' % ( outdir , s ) , 'w' ) as f : \n            f . write ( header ) \n            f . write ( csv ) \n    return "}
{"10009": "\ndef export_traces ( self , outdir = None , focus_stage = None , analytes = None , samples = None , subset = 'All_Analyses' , filt = False , zip_archive = False ) : \n    if analytes is None : \n        analytes = self . analytes \n    else : \n        if isinstance ( analytes , str ) : \n            analytes = [ analytes ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    if focus_stage is None : \n        focus_stage = self . focus_stage \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        analytes = [ a for a in analytes if a != self . internal_standard ] \n    if outdir is None : \n        outdir = os . path . join ( self . export_dir , 'trace_export' ) \n    ud = { 'rawdata' : 'counts' , 'despiked' : 'counts' , 'bkgsub' : 'background corrected counts' , 'ratios' : 'counts/count {:s}' , 'calibrated' : 'mol/mol {:s}' } \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        ud [ focus_stage ] = ud [ focus_stage ] . format ( self . internal_standard ) \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    for s in samples : \n        d = self . data [ s ] . data [ focus_stage ] \n        ind = self . data [ s ] . filt . grab_filt ( filt ) \n        out = Bunch ( ) \n        for a in analytes : \n            out [ a ] = nominal_values ( d [ a ] [ ind ] ) \n            if focus_stage not in [ 'rawdata' , 'despiked' ] : \n                out [ a + '_std' ] = std_devs ( d [ a ] [ ind ] ) \n                out [ a + '_std' ] [ out [ a + '_std' ] == 0 ] = np . nan \n        out = pd . DataFrame ( out , index = self . data [ s ] . Time [ ind ] ) \n        out . index . name = 'Time' \n        header = [ '# Sample: %s' % ( s ) , '# Data Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , '# Processed using %s configuration' % ( self . config [ 'config' ] ) , '# Analysis Stage: %s' % ( focus_stage ) , '# Unit: %s' % ud [ focus_stage ] ] \n        header = '\\n' . join ( header ) + '\\n' \n        csv = out . to_csv ( ) \n        with open ( '%s/%s_%s.csv' % ( outdir , s , focus_stage ) , 'w' ) as f : \n            f . write ( header ) \n            f . write ( csv ) \n    if zip_archive : \n        utils . zipdir ( outdir , delete = True ) \n    return "}
{"10020": "\ndef remove ( self , name = None , setn = None ) : \n    if isinstance ( name , int ) : \n        name = self . index [ name ] \n    if setn is not None : \n        name = self . sets [ setn ] \n        del self . sets [ setn ] \n    else : \n        if isinstance ( name , ( int , str ) ) : \n            name = [ name ] \n    if setn is True : \n        for n in name : \n            for k , v in self . sets . items ( ) : \n                if n in v : \n                    name . append ( [ m for m in v if m != n ] ) \n    for n in name : \n        for k , v in self . sets . items ( ) : \n            if n in v : \n                self . sets [ k ] = [ m for m in v if n != m ] \n        del self . components [ n ] \n        del self . info [ n ] \n        del self . params [ n ] \n        del self . keys [ n ] \n        for a in self . analytes : \n            del self . switches [ a ] [ n ] \n        return "}
{"10025": "\ndef grab_filt ( self , filt , analyte = None ) : \n    if isinstance ( filt , str ) : \n        if filt in self . components : \n            if analyte is None : \n                return self . components [ filt ] \n            else : \n                if self . switches [ analyte ] [ filt ] : \n                    return self . components [ filt ] \n        else : \n            try : \n                ind = self . make_fromkey ( filt ) \n            except KeyError : \n                print ( ( \"\\n\\n***Filter key invalid. Please consult \" \"manual and try again.\" ) ) \n    else : \n        if isinstance ( filt , dict ) : \n            try : \n                ind = self . make_fromkey ( filt [ analyte ] ) \n            except ValueError : \n                print ( ( \"\\n\\n***Filter key invalid. Please consult manual \" \"and try again.\\nOR\\nAnalyte missing from filter \" \"key dict.\" ) ) \n        else : \n            if filt : \n                ind = self . make ( analyte ) \n            else : \n                ind = ~ np . zeros ( self . size , dtype = bool ) \n    return ind "}
{"10033": "\ndef parse ( file_or_string ) : \n    from mysqlparse . grammar . sql_file import sql_file_syntax \n    if hasattr ( file_or_string , 'read' ) and hasattr ( file_or_string . read , '__call__' ) : \n        return sql_file_syntax . parseString ( file_or_string . read ( ) ) \n    else : \n        if isinstance ( file_or_string , six . string_types ) : \n            return sql_file_syntax . parseString ( file_or_string ) \n        else : \n            raise TypeError ( \"Expected file-like or string object, but got '{type_name}' instead.\" . format ( type_name = type ( file_or_string ) . __name__ , ) ) "}
{"10047": "\ndef copy_thumbnail_figure ( self ) : \n    ret = None \n    if self . _thumbnail_figure is not None : \n        if not isstring ( self . _thumbnail_figure ) : \n            ret = self . _thumbnail_figure \n        else : \n            ret = osp . join ( osp . dirname ( self . outfile ) , osp . basename ( self . _thumbnail_figure ) ) \n            copyfile ( self . _thumbnail_figure , ret ) \n            return ret \n    else : \n        if hasattr ( self . nb . metadata , 'thumbnail_figure' ) : \n            if not isstring ( self . nb . metadata . thumbnail_figure ) : \n                ret = self . nb . metadata . thumbnail_figure \n            else : \n                ret = osp . join ( osp . dirname ( self . outfile ) , 'images' , osp . basename ( self . nb . metadata . thumbnail_figure ) ) \n                copyfile ( osp . join ( osp . dirname ( self . infile ) , self . nb . metadata . thumbnail_figure ) , ret ) \n    return ret "}
{"10048": "\ndef get_url ( self , nbfile ) : \n    urls = self . urls \n    if isinstance ( urls , dict ) : \n        return urls . get ( nbfile ) \n    else : \n        if isstring ( urls ) : \n            if not urls . endswith ( '/' ) : \n                urls += '/' \n            return urls + nbfile "}
{"10050": "\ndef default_value ( field ) : \n    def default_value_func ( self ) : \n        attname = lambda x : get_real_fieldname ( field , x ) \n        if getattr ( self , attname ( get_language ( ) ) , None ) : \n            result = getattr ( self , attname ( get_language ( ) ) ) \n        else : \n            if getattr ( self , attname ( get_language ( ) [ : 2 ] ) , None ) : \n                result = getattr ( self , attname ( get_language ( ) [ : 2 ] ) ) \n            else : \n                default_language = fallback_language ( ) \n                if getattr ( self , attname ( default_language ) , None ) : \n                    result = getattr ( self , attname ( default_language ) , None ) \n                else : \n                    result = getattr ( self , attname ( settings . LANGUAGE_CODE ) , None ) \n        return result \n    return default_value_func "}
{"10077": "\ndef as_string ( self , default_from = None ) : \n    encoding = self . charset or 'utf-8' \n    attachments = self . attachments or [ ] \n    if len ( attachments ) == 0 and not self . html : \n        msg = self . _mimetext ( self . body ) \n    else : \n        if len ( attachments ) > 0 and not self . html : \n            msg = MIMEMultipart ( ) \n            msg . attach ( self . _mimetext ( self . body ) ) \n        else : \n            msg = MIMEMultipart ( ) \n            alternative = MIMEMultipart ( 'alternative' ) \n            alternative . attach ( self . _mimetext ( self . body , 'plain' ) ) \n            alternative . attach ( self . _mimetext ( self . html , 'html' ) ) \n            msg . attach ( alternative ) \n    if self . charset : \n        msg [ 'Subject' ] = Header ( self . subject , encoding ) \n    else : \n        msg [ 'Subject' ] = self . subject \n    sender = self . sender or default_from \n    if sender is not None : \n        msg [ 'From' ] = sanitize_address ( sender , encoding ) \n    msg [ 'To' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . recipients , encoding ) ) ) ) \n    msg [ 'Date' ] = formatdate ( self . date , localtime = True ) \n    msg [ 'Message-ID' ] = self . msgId \n    if self . cc : \n        msg [ 'Cc' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . cc , encoding ) ) ) ) \n    if self . reply_to : \n        msg [ 'Reply-To' ] = sanitize_address ( self . reply_to , encoding ) \n    if self . extra_headers : \n        for k , v in self . extra_headers . items ( ) : \n            msg [ k ] = v \n    for attachment in attachments : \n        f = MIMEBase ( * attachment . content_type . split ( '/' ) ) \n        f . set_payload ( attachment . data ) \n        encode_base64 ( f ) \n        try : \n            attachment . filename and attachment . filename . encode ( 'ascii' ) \n        except UnicodeEncodeError : \n            filename = attachment . filename \n            if not PY3 : \n                filename = filename . encode ( 'utf8' ) \n            f . add_header ( 'Content-Disposition' , attachment . disposition , filename = ( 'UTF8' , '' , filename ) ) \n        else : \n            f . add_header ( 'Content-Disposition' , '%s;filename=%s' % ( attachment . disposition , attachment . filename ) ) \n        for key , value in attachment . headers : \n            f . add_header ( key , value ) \n        msg . attach ( f ) \n    return msg . as_string ( ) "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    else : \n        if params and 'max_results' not in params : \n            params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if self . processes == 1 : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if end > number_pages : \n                end = number_pages \n            begin += 1 \n            end += 1 \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10099": "\ndef _get_new_column_header ( self , vcf_reader ) : \n    mutect_dict = self . _build_mutect_dict ( vcf_reader . metaheaders ) \n    new_header_list = [ ] \n    required_keys = set ( [ self . _NORMAL_SAMPLE_KEY , self . _TUMOR_SAMPLE_KEY ] ) \n    mutect_keys = set ( mutect_dict . keys ( ) ) \n    if not required_keys . issubset ( mutect_keys ) : \n        raise utils . JQException ( \"Unable to determine normal \" \"and tumor sample ordering \" \"based on MuTect metaheader.\" ) \n    for field_name in vcf_reader . column_header . split ( \"\\t\" ) : \n        if field_name == mutect_dict [ self . _NORMAL_SAMPLE_KEY ] : \n            field_name = \"NORMAL\" \n        else : \n            if field_name == mutect_dict [ self . _TUMOR_SAMPLE_KEY ] : \n                field_name = \"TUMOR\" \n        new_header_list . append ( field_name ) \n    return \"\\t\" . join ( new_header_list ) "}
{"10101": "\ndef _init_population_stats ( self , vcf_reader , dependent_tag_id ) : \n    n = 0 \n    mean = 0 \n    M2 = 0 \n    try : \n        vcf_reader . open ( ) \n        for vcf_record in vcf_reader . vcf_records ( ) : \n            for tag_values in vcf_record . sample_tag_values . values ( ) : \n                value = self . _get_dependent_value ( tag_values , dependent_tag_id ) \n                if value is not None : \n                    n += 1 \n                    delta = value - mean \n                    mean += delta / n \n                    M2 += delta * ( value - mean ) \n    finally : \n        vcf_reader . close ( ) \n    mean = round ( mean , self . _MAX_PRECISION ) \n    stdev = 0 \n    if n == 0 : \n        mean = None \n        stdev = None \n    else : \n        if n >= 2 : \n            variance = M2 / n \n            stdev = round ( math . sqrt ( variance ) , self . _MAX_PRECISION ) \n    return mean , stdev "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        if offset == where : \n            break \n        read_size = self . read_size if self . read_size <= where else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ 0 ] == b'\\n' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            if terminator_where > data_len + 1 : \n                self . file . seek ( where - offset - data_len - 1 , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( 1 ) \n                if terminator_data [ 0 ] == b'\\r' [ 0 ] : \n                    data_len += 1 \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while data_where > 0 : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and offset == 0 and data_where == data_len : \n                data_where -= len ( terminator ) \n            else : \n                if terminator : \n                    self . file . seek ( where - offset - ( data_len - data_where ) ) \n                    return self . file . tell ( ) \n                else : \n                    data_where -= 1 \n        offset += data_len \n    if where == 0 : \n        return - 1 \n    else : \n        self . file . seek ( 0 ) \n        return 0 "}
{"10120": "\ndef add_or_replace_filter ( self , new_filter ) : \n    if self . filter . lower ( ) in self . _FILTERS_TO_REPLACE : \n        self . filter = new_filter \n    else : \n        if new_filter not in self . filter . split ( \";\" ) : \n            self . filter = \";\" . join ( [ self . filter , new_filter ] ) "}
{"10130": "\ndef parse_osm_file ( f , parse_timestamps = True ) : \n    nodes = [ ] \n    ways = [ ] \n    relations = [ ] \n    for p in iter_osm_file ( f , parse_timestamps ) : \n        if type ( p ) == model . Node : \n            nodes . append ( p ) \n        else : \n            if type ( p ) == model . Way : \n                ways . append ( p ) \n            else : \n                if type ( p ) == model . Relation : \n                    relations . append ( p ) \n    return ( nodes , ways , relations ) "}
{"10131": "\ndef iter_osm_notes ( feed_limit = 25 , interval = 60 , parse_timestamps = True ) : \n    last_seen_guid = None \n    while True : \n        u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit ) \n        tree = etree . parse ( u ) \n        new_notes = [ ] \n        for note_item in tree . xpath ( '/rss/channel/item' ) : \n            title = note_item . xpath ( 'title' ) [ 0 ] . text \n            if title . startswith ( 'new note (' ) : \n                action = 'create' \n            else : \n                if title . startswith ( 'new comment (' ) : \n                    action = 'comment' \n                else : \n                    if title . startswith ( 'closed note (' ) : \n                        action = 'close' \n            guid = note_item . xpath ( 'link' ) [ 0 ] . text \n            if last_seen_guid == guid : \n                break \n            else : \n                if last_seen_guid == None : \n                    last_seen_guid = guid \n                else : \n                    note_id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) \n                    new_notes . append ( ( action , get_note ( note_id , parse_timestamps ) ) ) \n        for note in reversed ( new_notes ) : \n            yield note \n        yield model . Finished ( None , None ) \n        time . sleep ( interval ) "}
{"10147": "\ndef _add_discount ( self , product , quantity , discounts ) : \n    def matches ( discount ) : \n        if isinstance ( discount . clause , conditions . DiscountForCategory ) : \n            return discount . clause . category == product . category \n        else : \n            return discount . clause . product == product \n    def value ( discount ) : \n        if discount . clause . percentage is not None : \n            return discount . clause . percentage * product . price \n        else : \n            return discount . clause . price \n    discounts = [ i for i in discounts if matches ( i ) ] \n    discounts . sort ( key = value ) \n    for candidate in reversed ( discounts ) : \n        if quantity == 0 : \n            break \n        else : \n            if candidate . quantity == 0 : \n                continue \n        discount_item = commerce . DiscountItem . objects . create ( product = product , cart = self . cart , discount = candidate . discount , quantity = quantity , ) \n        ours = discount_item . quantity \n        allowed = candidate . quantity \n        if ours > allowed : \n            discount_item . quantity = allowed \n            discount_item . save ( ) \n            quantity = ours - allowed \n        else : \n            quantity = 0 \n        candidate . quantity -= discount_item . quantity "}
{"10159": "\ndef product_line_items ( request , form ) : \n    products = form . cleaned_data [ \"product\" ] \n    categories = form . cleaned_data [ \"category\" ] \n    invoices = commerce . Invoice . objects . filter ( ( Q ( lineitem__product__in = products ) | Q ( lineitem__product__category__in = categories ) ) , status = commerce . Invoice . STATUS_PAID , ) . select_related ( \"cart\" , \"user\" , \"user__attendee\" , \"user__attendee__attendeeprofilebase\" ) . order_by ( \"issue_time\" ) \n    headings = [ 'Invoice' , 'Invoice Date' , 'Attendee' , 'Qty' , 'Product' , 'Status' ] \n    data = [ ] \n    for invoice in invoices : \n        for item in invoice . cart . productitem_set . all ( ) : \n            if item . product in products or item . product . category in categories : \n                output = [ ] \n                output . append ( invoice . id ) \n                output . append ( invoice . issue_time . strftime ( '%Y-%m-%d %H:%M:%S' ) ) \n                output . append ( invoice . user . attendee . attendeeprofilebase . attendee_name ( ) ) \n                output . append ( item . quantity ) \n                output . append ( item . product ) \n                cart = invoice . cart \n                if cart . status == commerce . Cart . STATUS_PAID : \n                    output . append ( 'PAID' ) \n                else : \n                    if cart . status == commerce . Cart . STATUS_ACTIVE : \n                        output . append ( 'UNPAID' ) \n                    else : \n                        if cart . status == commerce . Cart . STATUS_RELEASED : \n                            output . append ( 'REFUNDED' ) \n                data . append ( output ) \n    return ListReport ( \"Line Items\" , headings , data ) "}
{"10165": "\ndef manifest ( request , form ) : \n    products = form . cleaned_data [ \"product\" ] \n    categories = form . cleaned_data [ \"category\" ] \n    line_items = ( Q ( lineitem__product__in = products ) | Q ( lineitem__product__category__in = categories ) ) \n    invoices = commerce . Invoice . objects . filter ( line_items , status = commerce . Invoice . STATUS_PAID , ) . select_related ( \"cart\" , \"user\" , \"user__attendee\" , \"user__attendee__attendeeprofilebase\" ) \n    users = set ( i . user for i in invoices ) \n    carts = commerce . Cart . objects . filter ( user__in = users ) \n    items = commerce . ProductItem . objects . filter ( cart__in = carts ) . select_related ( \"product\" , \"product__category\" , \"cart\" , \"cart__user\" , \"cart__user__attendee\" , \"cart__user__attendee__attendeeprofilebase\" ) . order_by ( \"product__category__order\" , \"product__order\" ) \n    users = { } \n    for item in items : \n        cart = item . cart \n        if cart . user not in users : \n            users [ cart . user ] = { \"unpaid\" : [ ] , \"paid\" : [ ] , \"refunded\" : [ ] } \n        items = users [ cart . user ] \n        if cart . status == commerce . Cart . STATUS_ACTIVE : \n            items [ \"unpaid\" ] . append ( item ) \n        else : \n            if cart . status == commerce . Cart . STATUS_PAID : \n                items [ \"paid\" ] . append ( item ) \n            else : \n                if cart . status == commerce . Cart . STATUS_RELEASED : \n                    items [ \"refunded\" ] . append ( item ) \n    users_by_name = list ( users . keys ( ) ) \n    users_by_name . sort ( key = ( lambda i : i . attendee . attendeeprofilebase . attendee_name ( ) . lower ( ) ) ) \n    headings = [ \"User ID\" , \"Name\" , \"Paid\" , \"Unpaid\" , \"Refunded\" ] \n    def format_items ( item_list ) : \n        strings = [ '%d x %s' % ( item . quantity , str ( item . product ) ) for item in item_list ] \n        return \", \\n\" . join ( strings ) \n    output = [ ] \n    for user in users_by_name : \n        items = users [ user ] \n        output . append ( [ user . id , user . attendee . attendeeprofilebase . attendee_name ( ) , format_items ( items [ \"paid\" ] ) , format_items ( items [ \"unpaid\" ] ) , format_items ( items [ \"refunded\" ] ) , ] ) \n    return ListReport ( \"Manifest\" , headings , output ) "}
{"10169": "\ndef guided_registration ( request , page_number = None ) : \n    PAGE_PROFILE = 1 \n    PAGE_TICKET = 2 \n    PAGE_PRODUCTS = 3 \n    PAGE_PRODUCTS_MAX = 4 \n    TOTAL_PAGES = 4 \n    ticket_category = inventory . Category . objects . get ( id = settings . TICKET_PRODUCT_CATEGORY ) \n    cart = CartController . for_user ( request . user ) \n    attendee = people . Attendee . get_instance ( request . user ) \n    if attendee . completed_registration : \n        return redirect ( review ) \n    has_profile = hasattr ( attendee , \"attendeeprofilebase\" ) \n    if not has_profile : \n        max_page = PAGE_PROFILE \n        redirect_page = PAGE_PROFILE \n    else : \n        products = inventory . Product . objects . filter ( productitem__cart = cart . cart ) \n        products = products . filter ( category = ticket_category ) \n        if products . count ( ) == 0 : \n            max_page = PAGE_TICKET \n            redirect_page = PAGE_TICKET \n        else : \n            max_page = PAGE_PRODUCTS_MAX \n            redirect_page = PAGE_PRODUCTS \n    if page_number is None or int ( page_number ) > max_page : \n        return redirect ( \"guided_registration\" , redirect_page ) \n    page_number = int ( page_number ) \n    next_step = redirect ( \"guided_registration\" , page_number + 1 ) \n    with BatchController . batch ( request . user ) : \n        available = ProductController . available_products ( request . user , category = ticket_category ) \n        if not available : \n            messages . error ( request , \"There are no more tickets available.\" ) \n            return redirect ( \"dashboard\" ) \n        sections = [ ] \n        if page_number == PAGE_PROFILE : \n            title = \"Attendee information\" \n            sections = _guided_registration_profile_and_voucher ( request ) \n        else : \n            if page_number == PAGE_TICKET : \n                title = \"Select ticket type\" \n                sections = _guided_registration_products ( request , GUIDED_MODE_TICKETS_ONLY ) \n            else : \n                if page_number == PAGE_PRODUCTS : \n                    title = \"Additional items\" \n                    sections = _guided_registration_products ( request , GUIDED_MODE_ALL_ADDITIONAL ) \n                else : \n                    if page_number == PAGE_PRODUCTS_MAX : \n                        title = \"More additional items\" \n                        sections = _guided_registration_products ( request , GUIDED_MODE_EXCLUDE_COMPLETE ) \n        if not sections : \n            attendee . completed_registration = True \n            attendee . save ( ) \n            return redirect ( \"review\" ) \n        if sections and request . method == \"POST\" : \n            for section in sections : \n                if section . form . errors : \n                    break \n            else : \n                return next_step \n    data = { \"current_step\" : page_number , \"sections\" : sections , \"title\" : title , \"total_steps\" : TOTAL_PAGES , } \n    return render ( request , \"registrasion/guided_registration.html\" , data ) "}
{"10176": "\ndef invoice_access ( request , access_code ) : \n    invoices = commerce . Invoice . objects . filter ( user__attendee__access_code = access_code , ) . order_by ( \"-issue_time\" ) \n    if not invoices : \n        raise Http404 ( ) \n    unpaid = invoices . filter ( status = commerce . Invoice . STATUS_UNPAID ) \n    paid = invoices . filter ( status = commerce . Invoice . STATUS_PAID ) \n    if unpaid : \n        invoice = unpaid [ 0 ] \n    else : \n        if paid : \n            invoice = paid [ 0 ] \n        else : \n            invoice = invoices [ 0 ] \n    return redirect ( \"invoice\" , invoice . id , access_code ) "}
{"10180": "\ndef credit_note ( request , note_id , access_code = None ) : \n    note_id = int ( note_id ) \n    current_note = CreditNoteController . for_id_or_404 ( note_id ) \n    apply_form = forms . ApplyCreditNoteForm ( current_note . credit_note . invoice . user , request . POST or None , prefix = \"apply_note\" ) \n    refund_form = forms . ManualCreditNoteRefundForm ( request . POST or None , prefix = \"refund_note\" ) \n    cancellation_fee_form = forms . CancellationFeeForm ( request . POST or None , prefix = \"cancellation_fee\" ) \n    if request . POST and apply_form . is_valid ( ) : \n        inv_id = apply_form . cleaned_data [ \"invoice\" ] \n        invoice = commerce . Invoice . objects . get ( pk = inv_id ) \n        current_note . apply_to_invoice ( invoice ) \n        messages . success ( request , \"Applied credit note %d to invoice.\" % note_id , ) \n        return redirect ( \"invoice\" , invoice . id ) \n    else : \n        if request . POST and refund_form . is_valid ( ) : \n            refund_form . instance . entered_by = request . user \n            refund_form . instance . parent = current_note . credit_note \n            refund_form . save ( ) \n            messages . success ( request , \"Applied manual refund to credit note.\" ) \n            refund_form = forms . ManualCreditNoteRefundForm ( prefix = \"refund_note\" , ) \n        else : \n            if request . POST and cancellation_fee_form . is_valid ( ) : \n                percentage = cancellation_fee_form . cleaned_data [ \"percentage\" ] \n                invoice = current_note . cancellation_fee ( percentage ) \n                messages . success ( request , \"Generated cancellation fee for credit note %d.\" % note_id , ) \n                return redirect ( \"invoice\" , invoice . invoice . id ) \n    data = { \"credit_note\" : current_note . credit_note , \"apply_form\" : apply_form , \"refund_form\" : refund_form , \"cancellation_fee_form\" : cancellation_fee_form , } \n    return render ( request , \"registrasion/credit_note.html\" , data ) "}
{"10186": "\ndef available_discounts ( cls , user , categories , products ) : \n    filtered_clauses = cls . _filtered_clauses ( user ) \n    categories = set ( categories ) \n    products = set ( products ) \n    product_categories = set ( product . category for product in products ) \n    all_categories = categories | product_categories \n    filtered_clauses = ( clause for clause in filtered_clauses if hasattr ( clause , 'product' ) and clause . product in products or hasattr ( clause , 'category' ) and clause . category in all_categories ) \n    discounts = [ ] \n    accepted_discounts = set ( ) \n    failed_discounts = set ( ) \n    for clause in filtered_clauses : \n        discount = clause . discount \n        cond = ConditionController . for_condition ( discount ) \n        past_use_count = clause . past_use_count \n        if past_use_count >= clause . quantity : \n            pass \n        else : \n            if discount not in failed_discounts : \n                is_accepted = discount in accepted_discounts \n                if is_accepted or cond . is_met ( user , filtered = True ) : \n                    discounts . append ( DiscountAndQuantity ( discount = discount , clause = clause , quantity = clause . quantity - past_use_count , ) ) \n                    accepted_discounts . add ( discount ) \n                else : \n                    failed_discounts . add ( discount ) \n    return discounts "}
{"10187": "\ndef _annotate_with_past_uses ( cls , queryset , user ) : \n    if queryset . model == conditions . DiscountForCategory : \n        matches = ( Q ( category = F ( 'discount__discountitem__product__category' ) ) ) \n    else : \n        if queryset . model == conditions . DiscountForProduct : \n            matches = ( Q ( product = F ( 'discount__discountitem__product' ) ) ) \n    in_carts = ( Q ( discount__discountitem__cart__user = user ) & Q ( discount__discountitem__cart__status = commerce . Cart . STATUS_PAID ) ) \n    past_use_quantity = When ( in_carts & matches , then = \"discount__discountitem__quantity\" , ) \n    past_use_quantity_or_zero = Case ( past_use_quantity , default = Value ( 0 ) , ) \n    queryset = queryset . annotate ( past_use_count = Sum ( past_use_quantity_or_zero ) ) \n    return queryset "}
{"10201": "\ndef update_status ( self ) : \n    old_status = self . invoice . status \n    total_paid = self . invoice . total_payments ( ) \n    num_payments = commerce . PaymentBase . objects . filter ( invoice = self . invoice , ) . count ( ) \n    remainder = self . invoice . value - total_paid \n    if old_status == commerce . Invoice . STATUS_UNPAID : \n        if remainder <= 0 : \n            self . _mark_paid ( ) \n        else : \n            if total_paid == 0 and num_payments > 0 : \n                self . _mark_void ( ) \n    else : \n        if old_status == commerce . Invoice . STATUS_PAID : \n            if remainder > 0 : \n                self . _mark_refunded ( ) \n        else : \n            if old_status == commerce . Invoice . STATUS_REFUNDED : \n                pass \n            else : \n                if old_status == commerce . Invoice . STATUS_VOID : \n                    pass \n    residual = 0 \n    if self . invoice . is_paid : \n        if remainder < 0 : \n            residual = 0 - remainder \n    else : \n        if self . invoice . is_void or self . invoice . is_refunded : \n            residual = total_paid \n    if residual != 0 : \n        CreditNoteController . generate_from_invoice ( self . invoice , residual ) \n    self . email_on_invoice_change ( self . invoice , old_status , self . invoice . status , ) "}
{"10205": "\ndef void ( self ) : \n    if self . invoice . total_payments ( ) > 0 : \n        raise ValidationError ( \"Invoices with payments must be refunded.\" ) \n    else : \n        if self . invoice . is_refunded : \n            raise ValidationError ( \"Refunded invoices may not be voided.\" ) \n    if self . invoice . is_paid : \n        self . _release_cart ( ) \n    self . _mark_void ( ) "}
{"10223": "\ndef search_variants_by_coordinates ( coordinate_query , search_mode = 'any' ) : \n    get_all_variants ( ) \n    ct = COORDINATE_TABLE \n    start_idx = COORDINATE_TABLE_START \n    stop_idx = COORDINATE_TABLE_STOP \n    chr_idx = COORDINATE_TABLE_CHR \n    start = int ( coordinate_query . start ) \n    stop = int ( coordinate_query . stop ) \n    chromosome = str ( coordinate_query . chr ) \n    left_idx = chr_idx . searchsorted ( chromosome ) \n    right_idx = chr_idx . searchsorted ( chromosome , side = 'right' ) \n    chr_ct_idx = chr_idx [ left_idx : right_idx ] . index \n    right_idx = start_idx . searchsorted ( stop , side = 'right' ) \n    start_ct_idx = start_idx [ : right_idx ] . index \n    left_idx = stop_idx . searchsorted ( start ) \n    stop_ct_idx = stop_idx [ left_idx : ] . index \n    match_idx = chr_ct_idx & start_ct_idx & stop_ct_idx \n    m_df = ct . loc [ match_idx , ] \n    if search_mode == 'any' : \n        var_digests = m_df . v_hash . to_list ( ) \n        return [ CACHE [ v ] for v in var_digests ] \n    else : \n        if search_mode == 'include_smaller' : \n            match_idx = ( start <= m_df . start ) & ( stop >= m_df . stop ) \n        else : \n            if search_mode == 'include_larger' : \n                match_idx = ( start >= m_df . start ) & ( stop <= m_df . stop ) \n            else : \n                if search_mode == 'exact' : \n                    match_idx = ( start == m_df . stop ) & ( stop == m_df . start ) \n                    if coordinate_query . alt : \n                        match_idx = match_idx & ( coordinate_query . alt == m_df . alt ) \n                else : \n                    raise ValueError ( \"unexpected search mode\" ) \n    var_digests = m_df . loc [ match_idx , ] . v_hash . to_list ( ) \n    return [ CACHE [ v ] for v in var_digests ] "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if prev_q [ 'chr' ] < current_q [ 'chr' ] : \n            return True \n        if prev_q [ 'chr' ] > current_q [ 'chr' ] : \n            return False \n        if prev_q [ 'start' ] < current_q [ 'start' ] : \n            return True \n        if prev_q [ 'start' ] > current_q [ 'start' ] : \n            return False \n        if prev_q [ 'stop' ] < current_q [ 'stop' ] : \n            return True \n        if prev_q [ 'stop' ] > current_q [ 'stop' ] : \n            return False \n        return True \n    ct_pointer = 0 \n    query_pointer = 0 \n    last_query_pointer = - 1 \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while query_pointer < len ( sorted_queries ) and ct_pointer < len ( ct ) : \n        if last_query_pointer != query_pointer : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if q_chr < c_chr : \n            query_pointer += 1 \n            continue \n        if q_chr > c_chr : \n            ct_pointer += 1 \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if q_start > c_stop : \n            ct_pointer += 1 \n            continue \n        if q_stop < c_start : \n            query_pointer += 1 \n            continue \n        if search_mode == 'any' : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        else : \n            if search_mode == 'exact' and q_start == c_start and q_stop == c_stop : \n                q_alt = q . alt \n                c_alt = c . alt \n                if not ( q_alt and c_alt and q_alt != c_alt ) : \n                    matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n            else : \n                if search_mode == 'include_smaller' : \n                    raise NotImplementedError \n                else : \n                    if search_mode == 'include_larger' : \n                        raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer += 1 \n    return dict ( matches ) "}
{"10229": "\ndef get_saved_issue_data ( self , issue , namespace = 'open' ) : \n    if isinstance ( issue , int ) : \n        issue_number = str ( issue ) \n    else : \n        if isinstance ( issue , basestring ) : \n            issue_number = issue \n        else : \n            issue_number = issue . number \n    issue_data_key = self . _issue_data_key ( namespace ) \n    issue_data = self . data . get ( issue_data_key , { } ) \n    _data = issue_data . get ( str ( issue_number ) , { } ) \n    issue_data [ str ( issue_number ) ] = _data \n    return _data "}
{"10230": "\ndef move_saved_issue_data ( self , issue , ns , other_ns ) : \n    if isinstance ( issue , int ) : \n        issue_number = str ( issue ) \n    else : \n        if isinstance ( issue , basestring ) : \n            issue_number = issue \n        else : \n            issue_number = issue . number \n    issue_data_key = self . _issue_data_key ( ns ) \n    other_issue_data_key = self . _issue_data_key ( other_ns ) \n    issue_data = self . data . get ( issue_data_key , { } ) \n    other_issue_data = self . data . get ( other_issue_data_key , { } ) \n    _id = issue_data . pop ( issue_number , None ) \n    if _id : \n        other_issue_data [ issue_number ] = _id \n    self . data [ other_issue_data_key ] = other_issue_data \n    self . data [ issue_data_key ] = issue_data "}
{"10231": "\ndef get_saved_task_data ( self , task ) : \n    if isinstance ( task , int ) : \n        task_number = str ( task ) \n    else : \n        if isinstance ( task , basestring ) : \n            task_number = task \n        else : \n            task_number = task [ 'id' ] \n    task_data_key = self . _task_data_key ( ) \n    task_data = self . data . get ( task_data_key , { } ) \n    _data = task_data . get ( str ( task_number ) , { } ) \n    task_data [ str ( task_number ) ] = _data \n    return _data "}
{"10234": "\ndef apply ( self , key , value , prompt = None , on_load = lambda a : a , on_save = lambda a : a ) : \n    if value == '' : \n        value = None \n        if key and self . data . has_key ( key ) : \n            del self . data [ key ] \n    if value is not None : \n        value = on_load ( value ) \n        if key : \n            self . data [ key ] = on_save ( value ) \n        return value \n    else : \n        if not key or not self . has_key ( key ) : \n            if callable ( prompt ) : \n                value = prompt ( ) \n            else : \n                if prompt is not None : \n                    value = raw_input ( prompt + \": \" ) \n            if value is None : \n                if self . data . has_key ( key ) : \n                    del self . data [ key ] \n                return None \n            self . data [ key ] = on_save ( value ) \n            return value \n    return on_load ( self . data [ key ] ) "}
{"10247": "\ndef calcPF ( pf ) : \n    pf_y = pf [ : 1 ] \n    pf_x = pf [ 1 : ] \n    result = 100 \n    if pf_y == CosTheta . CapacitiveLead : \n        result = 200 - int ( pf_x ) \n    else : \n        if pf_y == CosTheta . InductiveLag : \n            result = int ( pf_x ) \n    return result "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = 0 \n    if kwh_scale == ScaleKWH . EmptyScale : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count += 1 \n            continue \n        if len ( contents ) == 0 : \n            count += 1 \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if fld_type == FieldType . Float : \n                float_data = float ( str ( raw_data ) ) \n                divisor = 1 \n                if fld_scale == ScaleType . KWH : \n                    divisor = 1 \n                    if kwh_scale == ScaleKWH . Scale10 : \n                        divisor = 10 \n                    else : \n                        if kwh_scale == ScaleKWH . Scale100 : \n                            divisor = 100 \n                        else : \n                            if ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : \n                                ekm_log ( \"Unrecognized kwh scale.\" ) \n                else : \n                    if fld_scale == ScaleType . Div10 : \n                        divisor = 10 \n                    else : \n                        if fld_scale == ScaleType . Div100 : \n                            divisor = 100 \n                        else : \n                            if fld_scale != ScaleType . No : \n                                ekm_log ( \"Unrecognized float scale.\" ) \n                float_data /= divisor \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            else : \n                if fld_type == FieldType . Hex : \n                    hex_data = raw_data . encode ( 'hex' ) \n                    def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                    def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n                else : \n                    if fld_type == FieldType . Int : \n                        integer_data = int ( raw_data ) \n                        integer_data_str = str ( integer_data ) \n                        if len ( integer_data_str ) == 0 : \n                            integer_data_str = str ( 0 ) \n                        def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                        def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n                    else : \n                        if fld_type == FieldType . String : \n                            string_data = str ( raw_data ) \n                            def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                            def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n                        else : \n                            if fld_type == FieldType . PowerFactor : \n                                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n                            else : \n                                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count += 1 \n    return True "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if tableset == ReadSchedules . Schedules_1_To_4 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return True \n        else : \n            if tableset == ReadSchedules . Schedules_5_To_6 : \n                unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n                self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n                if str ( return_crc ) == str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                    ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                    self . setContext ( \"\" ) \n                    return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10324": "\ndef merge ( self , status : 'Status[Input, Output]' ) -> 'Status[Input, Output]' : \n    if status is None or status . farthest is None : \n        pass \n    else : \n        if self . farthest is None : \n            self . farthest = status . farthest \n            self . expected = status . expected \n        else : \n            if status . farthest . position < self . farthest . position : \n                pass \n            else : \n                if status . farthest . position > self . farthest . position : \n                    self . farthest = status . farthest \n                    self . expected = status . expected \n                else : \n                    self . expected = status . expected + self . expected \n    return self "}
{"10352": "\ndef validate_args ( cls , tag_name , * args , ** kwargs ) : \n    if cls . min_args is not None and len ( args ) < cls . min_args : \n        if cls . min_args == 1 : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} argument\" . format ( tag_name , cls . min_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} arguments\" . format ( tag_name , cls . min_args ) ) \n    if cls . max_args is not None and len ( args ) > cls . max_args : \n        if cls . max_args == 0 : \n            if cls . allowed_kwargs : \n                raise TemplateSyntaxError ( \"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\" . format ( tag_name , cls . allowed_kwargs [ 0 ] ) ) \n            else : \n                raise TemplateSyntaxError ( \"'{0}' tag doesn't support any arguments\" . format ( tag_name ) ) \n        else : \n            if cls . max_args == 1 : \n                raise TemplateSyntaxError ( \"'{0}' tag only allows {1} argument.\" . format ( tag_name , cls . max_args ) ) \n            else : \n                raise TemplateSyntaxError ( \"'{0}' tag only allows {1} arguments.\" . format ( tag_name , cls . max_args ) ) "}
{"10378": "\ndef object_iter ( obj , parent = None , parent_key = None , idx = None , siblings = None ) : \n    obj_node = Node ( value = obj , parent = parent , parent_key = parent_key , siblings = siblings , idx = idx ) \n    if isinstance ( obj , list ) : \n        _siblings = len ( obj ) \n        for i , elem in enumerate ( obj ) : \n            for node in object_iter ( elem , obj_node , None , i + 1 , _siblings ) : \n                yield node \n    else : \n        if isinstance ( obj , collections . Mapping ) : \n            for key in obj : \n                for node in object_iter ( obj [ key ] , obj_node , key ) : \n                    yield node \n    yield obj_node "}
{"10380": "\ndef parse ( self , selector ) : \n    log . debug ( self . obj ) \n    tokens = lex ( selector ) \n    if self . peek ( tokens , 'operator' ) == '*' : \n        self . match ( tokens , 'operator' ) \n        results = list ( object_iter ( self . obj ) ) \n    else : \n        results = self . selector_production ( tokens ) \n    results = [ node . value for node in results ] \n    if len ( results ) == 1 : \n        return results [ 0 ] \n    else : \n        if not len ( results ) : \n            return None \n    return results "}
{"10381": "\ndef selector_production ( self , tokens ) : \n    validators = [ ] \n    if self . peek ( tokens , 'type' ) : \n        type_ = self . match ( tokens , 'type' ) \n        validators . append ( self . type_production ( type_ ) ) \n    if self . peek ( tokens , 'identifier' ) : \n        key = self . match ( tokens , 'identifier' ) \n        validators . append ( self . key_production ( key ) ) \n    if self . peek ( tokens , 'pclass' ) : \n        pclass = self . match ( tokens , 'pclass' ) \n        validators . append ( self . pclass_production ( pclass ) ) \n    if self . peek ( tokens , 'nth_func' ) : \n        nth_func = self . match ( tokens , 'nth_func' ) \n        validators . append ( self . nth_child_production ( nth_func , tokens ) ) \n    if self . peek ( tokens , 'pclass_func' ) : \n        pclass_func = self . match ( tokens , 'pclass_func' ) \n        validators . append ( self . pclass_func_production ( pclass_func , tokens ) ) \n    if not len ( validators ) : \n        raise SelectorSyntaxError ( 'no selector recognized.' ) \n    results = self . _match_nodes ( validators , self . obj ) \n    if self . peek ( tokens , 'operator' ) : \n        operator = self . match ( tokens , 'operator' ) \n        rvals = self . selector_production ( tokens ) \n        if operator == ',' : \n            results . extend ( rvals ) \n        else : \n            if operator == '>' : \n                results = self . parents ( results , rvals ) \n            else : \n                if operator == '~' : \n                    results = self . siblings ( results , rvals ) \n                else : \n                    if operator == ' ' : \n                        results = self . ancestors ( results , rvals ) \n                    else : \n                        raise SelectorSyntaxError ( \"unrecognized operator '%s'\" % operator ) \n    else : \n        if len ( tokens ) : \n            rvals = self . selector_production ( tokens ) \n            results = self . ancestors ( results , rvals ) \n    return results "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = 1 if pat . group ( 5 ) == 'odd' else 0 \n    else : \n        if pat . group ( 6 ) : \n            a = 0 \n            b = int ( pat . group ( 6 ) ) \n        else : \n            sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n            coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n            a = eval ( sign + coef ) \n            b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 \n    reverse = False \n    if lexeme == 'nth-last-child' : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return False \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += 1 \n        if a == 0 : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and ( idx * a + b ) >= 0 \n        return m \n    return validate "}
{"10410": "\ndef _parse_format ( self , format ) : \n    format = format . strip ( ) \n    format = re . sub ( '[ \\t]+' , ' ' , format ) \n    subpatterns = [ ] \n    findquotes = re . compile ( r'^\\\\\"' ) \n    findreferreragent = re . compile ( 'Referer|User-Agent' ) \n    findpercent = re . compile ( '^%.*t$' ) \n    lstripquotes = re . compile ( r'^\\\\\"' ) \n    rstripquotes = re . compile ( r'\\\\\"$' ) \n    header = re . compile ( r'.*%\\{([^\\}]+)\\}i' ) \n    for element in format . split ( ' ' ) : \n        hasquotes = 0 \n        if findquotes . search ( element ) : \n            hasquotes = 1 \n        if hasquotes : \n            element = lstripquotes . sub ( '' , element ) \n            element = rstripquotes . sub ( '' , element ) \n        head = header . match ( element ) \n        if head : \n            self . _names . append ( head . groups ( ) [ 0 ] . lower ( ) ) \n            self . _types . append ( str ) \n        else : \n            self . _names . append ( self . alias ( element ) ) \n            self . _types . append ( self . types . get ( element , [ None , str ] ) [ 1 ] ) \n        subpattern = '(\\S*)' \n        if hasquotes : \n            if element == '%r' or findreferreragent . search ( element ) : \n                subpattern = r'\\\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\\\"' \n            else : \n                subpattern = r'\\\"([^\\\"]*)\\\"' \n        else : \n            if findpercent . search ( element ) : \n                subpattern = r'(\\[[^\\]]+\\])' \n            else : \n                if element == '%U' : \n                    subpattern = '(.+?)' \n        subpatterns . append ( subpattern ) \n    self . _pattern = '^' + ' ' . join ( subpatterns ) + '$' \n    try : \n        self . _regex = re . compile ( self . _pattern ) \n    except Exception as e : \n        raise ApacheLogParserError ( e ) "}
{"10434": "\ndef verify ( cls , timestamp : int , message_hash : SHA512Hash , signature : bytes , ) -> bool : \n    if timestamp < 1496176860 : \n        verifier = cls . _VERIFIER_20130905 \n    else : \n        if timestamp < 1502202360 : \n            verifier = None \n        else : \n            verifier = cls . _VERIFIER_20170808 \n    if verifier : \n        result = verifier . verify ( message_hash , signature , ) \n    else : \n        result = False \n    if isinstance ( result , int ) : \n        result = True if result == 1 else False \n    return result "}
{"10441": "\ndef items ( self ) : \n    if self . asc is not None : \n        if self . _selected and self . asc : \n            return self . query . order_by ( self . _selected ) \n        else : \n            if self . _selected and not self . asc : \n                return self . query . order_by ( desc ( self . _selected ) ) \n    return self . query "}
{"10465": "\ndef _saslprep_do_mapping ( chars ) : \n    i = 0 \n    while i < len ( chars ) : \n        c = chars [ i ] \n        if stringprep . in_table_c12 ( c ) : \n            chars [ i ] = \"\\u0020\" \n        else : \n            if stringprep . in_table_b1 ( c ) : \n                del chars [ i ] \n                continue \n        i += 1 "}
{"10511": "\ndef delete ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = False ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] in self . __BROWSABLE_TYPES : \n        contents = self . api_client . list_folder_content ( entity [ 'uuid' ] ) \n        if contents [ 'count' ] > 0 : \n            raise StorageArgumentException ( 'This method cannot delete non-empty folder. Please empty the folder first.' ) \n        self . api_client . delete_folder ( entity [ 'uuid' ] ) \n    else : \n        if entity [ 'entity_type' ] == 'file' : \n            self . api_client . delete_file ( entity [ 'uuid' ] ) "}
{"10563": "\nasync def search_vndb ( self , stype , term ) : \n    fstype = \"\" \n    if stype not in [ 'v' , 'r' , 'p' , 's' , 'c' , 'g' , 'i' , 'u' ] : \n        raise VNDBBadStype ( stype ) \n    else : \n        if stype in [ 'v' , 'p' , 's' , 'c' , 'u' ] : \n            fstype = '/{}/all' . format ( stype ) \n        else : \n            if stype in [ 'g' , 'i' ] : \n                fstype = '/{}/list' . format ( stype ) \n            else : \n                if stype == 'r' : \n                    fstype = '/r' \n    async with self . session . get ( self . base_url + \"{}\" . format ( fstype ) , params = { \"q\" : term } , headers = self . headers ) as response : \n        if response . status == 404 : \n            raise aiohttp . HttpBadRequest ( \"VN Not Found\" ) \n        else : \n            if 'q=' not in response . url : \n                raise VNDBOneResult ( term , response . url . rsplit ( '/' , 1 ) [ 1 ] ) \n        text = await response . text ( ) \n        if 'No Results' in text : \n            raise VNDBNoResults ( term ) \n        soup = BeautifulSoup ( text , 'lxml' ) \n        resp = await self . parse_search ( stype , soup ) \n        if resp == [ ] : \n            raise VNDBNoResults ( term ) \n        return resp "}
{"10564": "\nasync def parse_search ( self , stype , soup ) : \n    if stype == 'v' : \n        return await parse_vn_results ( soup ) \n    else : \n        if stype == 'r' : \n            return await parse_release_results ( soup ) \n        else : \n            if stype == 'p' : \n                return await parse_prod_staff_results ( soup ) \n            else : \n                if stype == 's' : \n                    return await parse_prod_staff_results ( soup ) \n                else : \n                    if stype == 'c' : \n                        return await parse_character_results ( soup ) \n                    else : \n                        if stype == 'g' : \n                            return await parse_tag_results ( soup ) \n                        else : \n                            if stype == 'i' : \n                                return await parse_tag_results ( soup ) \n                            else : \n                                if stype == 'u' : \n                                    return await parse_user_results ( soup ) "}
{"10565": "\ndef addStream ( self , stream , interpolator = \"closest\" , t1 = None , t2 = None , dt = None , limit = None , i1 = None , i2 = None , transform = None , colname = None ) : \n    streamquery = query_maker ( t1 , t2 , limit , i1 , i2 , transform ) \n    param_stream ( self . cdb , streamquery , stream ) \n    streamquery [ \"interpolator\" ] = interpolator \n    if colname is None : \n        if isinstance ( stream , six . string_types ) : \n            colname = stream \n        else : \n            if isinstance ( stream , Stream ) : \n                colname = stream . path \n            else : \n                raise Exception ( \"Could not find a name for the column! use the 'colname' parameter.\" ) \n    if colname in self . query [ \"dataset\" ] or colname is \"x\" : \n        raise Exception ( \"The column name either exists, or is labeled 'x'. Use the colname parameter to change the column name.\" ) \n    self . query [ \"dataset\" ] [ colname ] = streamquery "}
{"10596": "\ndef handleresult ( self , r ) : \n    if r . status_code >= 400 and r . status_code < 500 : \n        msg = r . json ( ) \n        raise AuthenticationError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n    else : \n        if r . status_code > 300 : \n            err = None \n            try : \n                msg = r . json ( ) \n                err = ServerError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n            except : \n                raise ServerError ( \"Server returned error, but did not give a valid error message\" ) \n            raise err \n    return r "}
{"10621": "\ndef __on_close ( self , ws ) : \n    if self . status == \"disconnected\" : \n        return \n    logging . debug ( \"ConnectorDB:WS: Websocket closed\" ) \n    if self . pingtimer is not None : \n        self . pingtimer . cancel ( ) \n    self . disconnected_time = time . time ( ) \n    if self . status == \"disconnecting\" : \n        self . status = \"disconnected\" \n    else : \n        if self . status == \"connected\" : \n            self . __reconnect ( ) "}
{"10627": "\ndef gatk_variant_recalibrator ( job , mode , vcf , ref_fasta , ref_fai , ref_dict , annotations , hapmap = None , omni = None , phase = None , dbsnp = None , mills = None , max_gaussians = 4 , unsafe_mode = False ) : \n    mode = mode . upper ( ) \n    inputs = { 'genome.fa' : ref_fasta , 'genome.fa.fai' : ref_fai , 'genome.dict' : ref_dict , 'input.vcf' : vcf } \n    command = [ '-T' , 'VariantRecalibrator' , '-R' , 'genome.fa' , '-input' , 'input.vcf' , '-tranche' , '100.0' , '-tranche' , '99.9' , '-tranche' , '99.0' , '-tranche' , '90.0' , '--maxGaussians' , str ( max_gaussians ) , '-recalFile' , 'output.recal' , '-tranchesFile' , 'output.tranches' , '-rscriptFile' , 'output.plots.R' ] \n    if mode == 'SNP' : \n        command . extend ( [ '-resource:hapmap,known=false,training=true,truth=true,prior=15.0' , 'hapmap.vcf' , '-resource:omni,known=false,training=true,truth=true,prior=12.0' , 'omni.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-resource:1000G,known=false,training=true,truth=false,prior=10.0' , '1000G.vcf' , '-mode' , 'SNP' ] ) \n        inputs [ 'hapmap.vcf' ] = hapmap \n        inputs [ 'omni.vcf' ] = omni \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n        inputs [ '1000G.vcf' ] = phase \n    else : \n        if mode == 'INDEL' : \n            command . extend ( [ '-resource:mills,known=false,training=true,truth=true,prior=12.0' , 'mills.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-mode' , 'INDEL' ] ) \n            inputs [ 'mills.vcf' ] = mills \n            inputs [ 'dbsnp.vcf' ] = dbsnp \n        else : \n            raise ValueError ( 'Variant filter modes can be SNP or INDEL, got %s' % mode ) \n    for annotation in annotations : \n        command . extend ( [ '-an' , annotation ] ) \n    if unsafe_mode : \n        command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    job . fileStore . logToMaster ( 'Running GATK VariantRecalibrator on {mode}s using the following annotations:\\n' '{annotations}' . format ( mode = mode , annotations = '\\n' . join ( annotations ) ) ) \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , dockerParameters = docker_parameters ) \n    recal_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.recal' ) ) \n    tranches_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.tranches' ) ) \n    plots_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.plots.R' ) ) \n    return recal_id , tranches_id , plots_id "}
{"10631": "\ndef load_handlers ( handler_mapping ) : \n    handlers = { } \n    for packet_type , handler in handler_mapping . items ( ) : \n        if packet_type == '*' : \n            Packet = packet_type \n        else : \n            if isinstance ( packet_type , str ) : \n                Packet = importer ( packet_type ) \n            else : \n                Packet = packet_type \n        if isinstance ( handler , str ) : \n            Handler = importer ( handler ) \n        else : \n            Handler = handler \n        if Packet in handlers : \n            raise HandlerConfigError ( \"Handler already provided for packet %s\" % Packet ) \n        handlers [ Packet ] = Handler \n    return handlers "}
{"10666": "\ndef _transform_result ( typ , result ) : \n    if issubclass ( typ , bytes ) : \n        return tostring ( result , encoding = 'utf-8' ) \n    else : \n        if issubclass ( typ , unicode ) : \n            return tostring ( result , encoding = 'unicode' ) \n        else : \n            return result "}
{"10672": "\ndef drop_tag ( self ) : \n    parent = self . getparent ( ) \n    assert parent is not None \n    previous = self . getprevious ( ) \n    if self . text and isinstance ( self . tag , basestring ) : \n        if previous is None : \n            parent . text = ( parent . text or '' ) + self . text \n        else : \n            previous . tail = ( previous . tail or '' ) + self . text \n    if self . tail : \n        if len ( self ) : \n            last = self [ - 1 ] \n            last . tail = ( last . tail or '' ) + self . tail \n        else : \n            if previous is None : \n                parent . text = ( parent . text or '' ) + self . tail \n            else : \n                previous . tail = ( previous . tail or '' ) + self . tail \n    index = parent . index ( self ) \n    parent [ index : index + 1 ] = self [ : ] "}
{"10678": "\ndef is_single_module ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if counts [ \"modules\" ] == 1 : \n        ret = True \n    else : \n        if counts [ \"modules\" ] < 1 : \n            ret = is_single_class ( ) \n    return ret "}
{"10686": "\ndef amount_converter ( obj ) : \n    if isinstance ( obj , Decimal ) : \n        return obj \n    else : \n        if isinstance ( obj , ( str , int , float ) ) : \n            return Decimal ( str ( obj ) ) \n        else : \n            raise ValueError ( 'do not know how to convert: {}' . format ( type ( obj ) ) ) "}
{"10703": "\ndef verifiable ( self ) : \n    trusted = self . trusted or getattr ( self . comes_from , \"trusted\" , None ) \n    if trusted is not None and trusted : \n        try : \n            api_version = getattr ( self . comes_from , \"api_version\" , None ) \n            api_version = int ( api_version ) \n        except ( ValueError , TypeError ) : \n            api_version = None \n        if api_version is None or api_version <= 1 : \n            return \n        if self . hash : \n            return True \n        else : \n            return False \n    else : \n        if trusted is not None : \n            return False "}
{"10726": "\ndef url_for ( endpoint , ** values ) : \n    appctx = _app_ctx_stack . top \n    reqctx = _request_ctx_stack . top \n    if appctx is None : \n        raise RuntimeError ( 'Attempted to generate a URL without the ' 'application context being pushed. This has to be ' 'executed when application context is available.' ) \n    if reqctx is not None : \n        url_adapter = reqctx . url_adapter \n        blueprint_name = request . blueprint \n        if not reqctx . request . _is_old_module : \n            if endpoint [ : 1 ] == '.' : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + endpoint \n                else : \n                    endpoint = endpoint [ 1 : ] \n        else : \n            if '.' not in endpoint : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + '.' + endpoint \n            else : \n                if endpoint . startswith ( '.' ) : \n                    endpoint = endpoint [ 1 : ] \n        external = values . pop ( '_external' , False ) \n    else : \n        url_adapter = appctx . url_adapter \n        if url_adapter is None : \n            raise RuntimeError ( 'Application was not able to create a URL ' 'adapter for request independent URL generation. ' 'You might be able to fix this by setting ' 'the SERVER_NAME config variable.' ) \n        external = values . pop ( '_external' , True ) \n    anchor = values . pop ( '_anchor' , None ) \n    method = values . pop ( '_method' , None ) \n    scheme = values . pop ( '_scheme' , None ) \n    appctx . app . inject_url_defaults ( endpoint , values ) \n    if scheme is not None : \n        if not external : \n            raise ValueError ( 'When specifying _scheme, _external must be True' ) \n        url_adapter . url_scheme = scheme \n    try : \n        rv = url_adapter . build ( endpoint , values , method = method , force_external = external ) \n    except BuildError as error : \n        values [ '_external' ] = external \n        values [ '_anchor' ] = anchor \n        values [ '_method' ] = method \n        return appctx . app . handle_url_build_error ( error , endpoint , values ) \n    if anchor is not None : \n        rv += '#' + url_quote ( anchor ) \n    return rv "}
{"10735": "\ndef check_compatibility ( version , name ) : \n    if not version : \n        raise UnsupportedWheel ( \"%s is in an unsupported or invalid wheel\" % name ) \n    if version [ 0 ] > VERSION_COMPATIBLE [ 0 ] : \n        raise UnsupportedWheel ( \"%s's Wheel-Version (%s) is not compatible with this version \" \"of pip\" % ( name , '.' . join ( map ( str , version ) ) ) ) \n    else : \n        if version > VERSION_COMPATIBLE : \n            logger . warning ( 'Installing from a newer Wheel-Version (%s)' , '.' . join ( map ( str , version ) ) , ) "}
{"10737": "\ndef iter_symbols ( code ) : \n    for name in code . co_names : \n        yield name \n    for const in code . co_consts : \n        if isinstance ( const , basestring ) : \n            yield const \n        else : \n            if isinstance ( const , CodeType ) : \n                for name in iter_symbols ( const ) : \n                    yield name "}
{"10741": "\ndef running_under_virtualenv ( ) : \n    if hasattr ( sys , 'real_prefix' ) : \n        return True \n    else : \n        if sys . prefix != getattr ( sys , \"base_prefix\" , sys . prefix ) : \n            return True \n    return False "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and cc [ 'max-age' ] == 0 : \n        no_cache = True \n    if no_cache : \n        return False \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return False \n    if resp . status == 301 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return False \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    else : \n        if 'expires' in headers : \n            expires = parsedate_tz ( headers [ 'expires' ] ) \n            if expires is not None : \n                expire_time = calendar . timegm ( expires ) - date \n                freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age += min_fresh \n    fresh = ( freshness_lifetime > current_age ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return False "}
{"10746": "\ndef cache_response ( self , request , response , body = None ) : \n    if response . status not in [ 200 , 203 , 300 , 301 ] : \n        return \n    response_headers = CaseInsensitiveDict ( response . headers ) \n    cc_req = self . parse_cache_control ( request . headers ) \n    cc = self . parse_cache_control ( response_headers ) \n    cache_url = self . cache_url ( request . url ) \n    no_store = cc . get ( 'no-store' ) or cc_req . get ( 'no-store' ) \n    if no_store and self . cache . get ( cache_url ) : \n        self . cache . delete ( cache_url ) \n    if self . cache_etags and 'etag' in response_headers : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n    else : \n        if response . status == 301 : \n            self . cache . set ( cache_url , self . serializer . dumps ( request , response ) ) \n        else : \n            if 'date' in response_headers : \n                if cc and cc . get ( 'max-age' ) : \n                    if int ( cc [ 'max-age' ] ) > 0 : \n                        self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n                else : \n                    if 'expires' in response_headers : \n                        if response_headers [ 'expires' ] : \n                            self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) "}
{"10750": "\ndef save ( self ) : \n    if not self . dirty : \n        return \n    data = '\\n' . join ( map ( self . make_relative , self . paths ) ) \n    if data : \n        log . debug ( \"Saving %s\" , self . filename ) \n        data = ( \"import sys; sys.__plen = len(sys.path)\\n\" \"%s\\n\" \"import sys; new=sys.path[sys.__plen:];\" \" del sys.path[sys.__plen:];\" \" p=getattr(sys,'__egginsert',0); sys.path[p:p]=new;\" \" sys.__egginsert = p+len(new)\\n\" ) % data \n        if os . path . islink ( self . filename ) : \n            os . unlink ( self . filename ) \n        f = open ( self . filename , 'wt' ) \n        f . write ( data ) \n        f . close ( ) \n    else : \n        if os . path . exists ( self . filename ) : \n            log . debug ( \"Deleting empty %s\" , self . filename ) \n            os . unlink ( self . filename ) \n    self . dirty = False "}
{"10751": "\ndef convert ( self , value ) : \n    if not isinstance ( value , ConvertingDict ) and isinstance ( value , dict ) : \n        value = ConvertingDict ( value ) \n        value . configurator = self \n    else : \n        if not isinstance ( value , ConvertingList ) and isinstance ( value , list ) : \n            value = ConvertingList ( value ) \n            value . configurator = self \n        else : \n            if not isinstance ( value , ConvertingTuple ) and isinstance ( value , tuple ) : \n                value = ConvertingTuple ( value ) \n                value . configurator = self \n            else : \n                if isinstance ( value , six . string_types ) : \n                    m = self . CONVERT_PATTERN . match ( value ) \n                    if m : \n                        d = m . groupdict ( ) \n                        prefix = d [ 'prefix' ] \n                        converter = self . value_converters . get ( prefix , None ) \n                        if converter : \n                            suffix = d [ 'suffix' ] \n                            converter = getattr ( self , converter ) \n                            value = converter ( suffix ) \n    return value "}
{"10753": "\ndef configure_handler ( self , config ) : \n    formatter = config . pop ( 'formatter' , None ) \n    if formatter : \n        try : \n            formatter = self . config [ 'formatters' ] [ formatter ] \n        except StandardError as e : \n            raise ValueError ( 'Unable to set formatter ' '%r: %s' % ( formatter , e ) ) \n    level = config . pop ( 'level' , None ) \n    filters = config . pop ( 'filters' , None ) \n    if '()' in config : \n        c = config . pop ( '()' ) \n        if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and type ( c ) != types . ClassType : \n            c = self . resolve ( c ) \n        factory = c \n    else : \n        klass = self . resolve ( config . pop ( 'class' ) ) \n        if issubclass ( klass , logging . handlers . MemoryHandler ) and 'target' in config : \n            try : \n                config [ 'target' ] = self . config [ 'handlers' ] [ config [ 'target' ] ] \n            except StandardError as e : \n                raise ValueError ( 'Unable to set target handler ' '%r: %s' % ( config [ 'target' ] , e ) ) \n        else : \n            if issubclass ( klass , logging . handlers . SMTPHandler ) and 'mailhost' in config : \n                config [ 'mailhost' ] = self . as_tuple ( config [ 'mailhost' ] ) \n            else : \n                if issubclass ( klass , logging . handlers . SysLogHandler ) and 'address' in config : \n                    config [ 'address' ] = self . as_tuple ( config [ 'address' ] ) \n        factory = klass \n    kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) \n    try : \n        result = factory ( ** kwargs ) \n    except TypeError as te : \n        if \"'stream'\" not in str ( te ) : \n            raise \n        kwargs [ 'strm' ] = kwargs . pop ( 'stream' ) \n        result = factory ( ** kwargs ) \n    if formatter : \n        result . setFormatter ( formatter ) \n    if level is not None : \n        result . setLevel ( _checkLevel ( level ) ) \n    if filters : \n        self . add_filters ( result , filters ) \n    return result "}
{"10764": "\ndef find_on_path ( importer , path_item , only = False ) : \n    path_item = _normalize_cached ( path_item ) \n    if os . path . isdir ( path_item ) and os . access ( path_item , os . R_OK ) : \n        if path_item . lower ( ) . endswith ( '.egg' ) : \n            yield Distribution . from_filename ( path_item , metadata = PathMetadata ( path_item , os . path . join ( path_item , 'EGG-INFO' ) ) ) \n        else : \n            for entry in os . listdir ( path_item ) : \n                lower = entry . lower ( ) \n                if lower . endswith ( '.egg-info' ) or lower . endswith ( '.dist-info' ) : \n                    fullpath = os . path . join ( path_item , entry ) \n                    if os . path . isdir ( fullpath ) : \n                        metadata = PathMetadata ( path_item , fullpath ) \n                    else : \n                        metadata = FileMetadata ( fullpath ) \n                    yield Distribution . from_location ( path_item , entry , metadata , precedence = DEVELOP_DIST ) \n                else : \n                    if not only and lower . endswith ( '.egg' ) : \n                        dists = find_distributions ( os . path . join ( path_item , entry ) ) \n                        for dist in dists : \n                            yield dist \n                    else : \n                        if not only and lower . endswith ( '.egg-link' ) : \n                            with open ( os . path . join ( path_item , entry ) ) as entry_file : \n                                entry_lines = entry_file . readlines ( ) \n                            for line in entry_lines : \n                                if not line . strip ( ) : \n                                    continue \n                                path = os . path . join ( path_item , line . rstrip ( ) ) \n                                dists = find_distributions ( path ) \n                                for item in dists : \n                                    yield item \n                                break "}
{"10769": "\ndef iter_entry_points ( self , group , name = None ) : \n    for dist in self : \n        entries = dist . get_entry_map ( group ) \n        if name is None : \n            for ep in entries . values ( ) : \n                yield ep \n        else : \n            if name in entries : \n                yield entries [ name ] "}
{"10776": "\ndef parse_pattern ( pattern ) : \n    if isinstance ( pattern , NumberPattern ) : \n        return pattern \n    def _match_number ( pattern ) : \n        rv = number_re . search ( pattern ) \n        if rv is None : \n            raise ValueError ( 'Invalid number pattern %r' % pattern ) \n        return rv . groups ( ) \n    pos_pattern = pattern \n    if ';' in pattern : \n        pos_pattern , neg_pattern = pattern . split ( ';' , 1 ) \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix , _ , neg_suffix = _match_number ( neg_pattern ) \n    else : \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix = '-' + pos_prefix \n        neg_suffix = pos_suffix \n    if 'E' in number : \n        number , exp = number . split ( 'E' , 1 ) \n    else : \n        exp = None \n    if '@' in number : \n        if '.' in number and '0' in number : \n            raise ValueError ( 'Significant digit patterns can not contain ' '\"@\" or \"0\"' ) \n    if '.' in number : \n        integer , fraction = number . rsplit ( '.' , 1 ) \n    else : \n        integer = number \n        fraction = '' \n    def parse_precision ( p ) : \n        min = max = 0 \n        for c in p : \n            if c in '@0' : \n                min += 1 \n                max += 1 \n            else : \n                if c == '#' : \n                    max += 1 \n                else : \n                    if c == ',' : \n                        continue \n                    else : \n                        break \n        return min , max \n    int_prec = parse_precision ( integer ) \n    frac_prec = parse_precision ( fraction ) \n    if exp : \n        exp_plus = exp . startswith ( '+' ) \n        exp = exp . lstrip ( '+' ) \n        exp_prec = parse_precision ( exp ) \n    else : \n        exp_plus = None \n        exp_prec = None \n    grouping = babel . numbers . parse_grouping ( integer ) \n    return NumberPattern ( pattern , ( pos_prefix , neg_prefix ) , ( pos_suffix , neg_suffix ) , grouping , int_prec , frac_prec , exp_prec , exp_plus ) "}
{"10779": "\ndef scientific_notation_elements ( self , value , locale ) : \n    exp = value . adjusted ( ) \n    value = value * get_decimal_quantum ( exp ) \n    assert value . adjusted ( ) == 0 \n    lead_shift = max ( [ 1 , min ( self . int_prec ) ] ) - 1 \n    exp = exp - lead_shift \n    value = value * get_decimal_quantum ( - lead_shift ) \n    exp_sign = '' \n    if exp < 0 : \n        exp_sign = babel . numbers . get_minus_sign_symbol ( locale ) \n    else : \n        if self . exp_plus : \n            exp_sign = babel . numbers . get_plus_sign_symbol ( locale ) \n    exp = abs ( exp ) \n    return value , exp , exp_sign "}
{"10781": "\ndef parse_requirements ( strs ) : \n    lines = iter ( yield_lines ( strs ) ) \n    def scan_list ( ITEM , TERMINATOR , line , p , groups , item_name ) : \n        items = [ ] \n        while not TERMINATOR ( line , p ) : \n            if CONTINUE ( line , p ) : \n                try : \n                    line = next ( lines ) \n                    p = 0 \n                except StopIteration : \n                    msg = \"\\\\ must not appear on the last nonblank line\" \n                    raise RequirementParseError ( msg ) \n            match = ITEM ( line , p ) \n            if not match : \n                msg = \"Expected \" + item_name + \" in\" \n                raise RequirementParseError ( msg , line , \"at\" , line [ p : ] ) \n            items . append ( match . group ( * groups ) ) \n            p = match . end ( ) \n            match = COMMA ( line , p ) \n            if match : \n                p = match . end ( ) \n            else : \n                if not TERMINATOR ( line , p ) : \n                    msg = \"Expected ',' or end-of-list in\" \n                    raise RequirementParseError ( msg , line , \"at\" , line [ p : ] ) \n        match = TERMINATOR ( line , p ) \n        if match : \n            p = match . end ( ) \n        return line , p , items \n    for line in lines : \n        match = DISTRO ( line ) \n        if not match : \n            raise RequirementParseError ( \"Missing distribution spec\" , line ) \n        project_name = match . group ( 1 ) \n        p = match . end ( ) \n        extras = [ ] \n        match = OBRACKET ( line , p ) \n        if match : \n            p = match . end ( ) \n            line , p , extras = scan_list ( DISTRO , CBRACKET , line , p , ( 1 , ) , \"'extra' name\" ) \n        line , p , specs = scan_list ( VERSION , LINE_END , line , p , ( 1 , 2 ) , \"version spec\" ) \n        specs = [ ( op , val ) for op , val in specs ] \n        yield Requirement ( project_name , specs , extras ) "}
{"10794": "\ndef make_abstract_dist ( req_to_install ) : \n    if req_to_install . editable : \n        return IsSDist ( req_to_install ) \n    else : \n        if req_to_install . link and req_to_install . link . is_wheel : \n            return IsWheel ( req_to_install ) \n        else : \n            return IsSDist ( req_to_install ) "}
{"10800": "\ndef default ( self , obj ) : \n    if isinstance ( obj , models . Model ) : \n        return self . encode ( model_to_dict ( obj ) ) \n    else : \n        if isinstance ( obj , models . query . QuerySet ) : \n            return serializers . serialize ( 'json' , obj ) \n        else : \n            return super ( JsonResponseEncoder , self ) . default ( obj ) "}
{"10808": "\ndef fixup_chunks ( chunks ) : \n    tag_accum = [ ] \n    cur_word = None \n    result = [ ] \n    for chunk in chunks : \n        if isinstance ( chunk , tuple ) : \n            if chunk [ 0 ] == 'img' : \n                src = chunk [ 1 ] \n                tag , trailing_whitespace = split_trailing_whitespace ( chunk [ 2 ] ) \n                cur_word = tag_token ( 'img' , src , html_repr = tag , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            else : \n                if chunk [ 0 ] == 'href' : \n                    href = chunk [ 1 ] \n                    cur_word = href_token ( href , pre_tags = tag_accum , trailing_whitespace = \" \" ) \n                    tag_accum = [ ] \n                    result . append ( cur_word ) \n            continue \n        if is_word ( chunk ) : \n            chunk , trailing_whitespace = split_trailing_whitespace ( chunk ) \n            cur_word = token ( chunk , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n            tag_accum = [ ] \n            result . append ( cur_word ) \n        else : \n            if is_start_tag ( chunk ) : \n                tag_accum . append ( chunk ) \n            else : \n                if is_end_tag ( chunk ) : \n                    if tag_accum : \n                        tag_accum . append ( chunk ) \n                    else : \n                        assert cur_word , ( \"Weird state, cur_word=%r, result=%r, chunks=%r of %r\" % ( cur_word , result , chunk , chunks ) ) \n                        cur_word . post_tags . append ( chunk ) \n                else : \n                    assert ( 0 ) \n    if not result : \n        return [ token ( '' , pre_tags = tag_accum ) ] \n    else : \n        result [ - 1 ] . post_tags . extend ( tag_accum ) \n    return result "}
{"10815": "\ndef extract_constant ( code , symbol , default = - 1 ) : \n    if symbol not in code . co_names : \n        return None \n    name_idx = list ( code . co_names ) . index ( symbol ) \n    STORE_NAME = 90 \n    STORE_GLOBAL = 97 \n    LOAD_CONST = 100 \n    const = default \n    for op , arg in _iter_code ( code ) : \n        if op == LOAD_CONST : \n            const = code . co_consts [ arg ] \n        else : \n            if arg == name_idx and ( op == STORE_NAME or op == STORE_GLOBAL ) : \n                return const \n            else : \n                const = default "}
{"10821": "\ndef getTreeWalker ( treeType , implementation = None , ** kwargs ) : \n    treeType = treeType . lower ( ) \n    if treeType not in treeWalkerCache : \n        if treeType in ( \"dom\" , \"pulldom\" ) : \n            name = \"%s.%s\" % ( __name__ , treeType ) \n            __import__ ( name ) \n            mod = sys . modules [ name ] \n            treeWalkerCache [ treeType ] = mod . TreeWalker \n        else : \n            if treeType == \"genshi\" : \n                from . import genshistream \n                treeWalkerCache [ treeType ] = genshistream . TreeWalker \n            else : \n                if treeType == \"lxml\" : \n                    from . import lxmletree \n                    treeWalkerCache [ treeType ] = lxmletree . TreeWalker \n                else : \n                    if treeType == \"etree\" : \n                        from . import etree \n                        if implementation is None : \n                            implementation = default_etree \n                        return etree . getETreeModule ( implementation , ** kwargs ) . TreeWalker \n    return treeWalkerCache . get ( treeType ) "}
{"10823": "\ndef get_revision ( self , location ) : \n    revision = 0 \n    for base , dirs , files in os . walk ( location ) : \n        if self . dirname not in dirs : \n            dirs [ : ] = [ ] \n            continue \n        dirs . remove ( self . dirname ) \n        entries_fn = os . path . join ( base , self . dirname , 'entries' ) \n        if not os . path . exists ( entries_fn ) : \n            continue \n        dirurl , localrev = self . _get_svn_url_rev ( base ) \n        if base == location : \n            base_url = dirurl + '/' \n        else : \n            if not dirurl or not dirurl . startswith ( base_url ) : \n                dirs [ : ] = [ ] \n                continue \n        revision = max ( revision , localrev ) \n    return revision "}
{"10859": "\ndef get_wsgi_headers ( self , environ ) : \n    headers = Headers ( self . headers ) \n    location = None \n    content_location = None \n    content_length = None \n    status = self . status_code \n    for key , value in headers : \n        ikey = key . lower ( ) \n        if ikey == u'location' : \n            location = value \n        else : \n            if ikey == u'content-location' : \n                content_location = value \n            else : \n                if ikey == u'content-length' : \n                    content_length = value \n    if location is not None : \n        old_location = location \n        if isinstance ( location , text_type ) : \n            location = iri_to_uri ( location , safe_conversion = True ) \n        if self . autocorrect_location_header : \n            current_url = get_current_url ( environ , root_only = True ) \n            if isinstance ( current_url , text_type ) : \n                current_url = iri_to_uri ( current_url ) \n            location = url_join ( current_url , location ) \n        if location != old_location : \n            headers [ 'Location' ] = location \n    if content_location is not None and isinstance ( content_location , text_type ) : \n        headers [ 'Content-Location' ] = iri_to_uri ( content_location ) \n    if 100 <= status < 200 or status == 204 : \n        headers [ 'Content-Length' ] = content_length = u'0' \n    else : \n        if status == 304 : \n            remove_entity_headers ( headers ) \n    if self . automatically_set_content_length and self . is_sequence and content_length is None and status != 304 : \n        try : \n            content_length = sum ( len ( to_bytes ( x , 'ascii' ) ) for x in self . response ) \n        except UnicodeError : \n            pass \n        else : \n            headers [ 'Content-Length' ] = str ( content_length ) \n    return headers "}
{"10861": "\ndef user_cache_dir ( appname ) : \n    if WINDOWS : \n        path = os . path . normpath ( _get_win_folder ( \"CSIDL_LOCAL_APPDATA\" ) ) \n        path = os . path . join ( path , appname , \"Cache\" ) \n    else : \n        if sys . platform == \"darwin\" : \n            path = os . path . expanduser ( \"~/Library/Caches\" ) \n            path = os . path . join ( path , appname ) \n        else : \n            path = os . getenv ( \"XDG_CACHE_HOME\" , os . path . expanduser ( \"~/.cache\" ) ) \n            path = os . path . join ( path , appname ) \n    return path "}
{"10862": "\ndef user_data_dir ( appname , roaming = False ) : \n    if WINDOWS : \n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\" \n        path = os . path . join ( os . path . normpath ( _get_win_folder ( const ) ) , appname ) \n    else : \n        if sys . platform == \"darwin\" : \n            path = os . path . join ( os . path . expanduser ( '~/Library/Application Support/' ) , appname , ) \n        else : \n            path = os . path . join ( os . getenv ( 'XDG_DATA_HOME' , os . path . expanduser ( \"~/.local/share\" ) ) , appname , ) \n    return path "}
{"10863": "\ndef user_log_dir ( appname ) : \n    if WINDOWS : \n        path = os . path . join ( user_data_dir ( appname ) , \"Logs\" ) \n    else : \n        if sys . platform == \"darwin\" : \n            path = os . path . join ( os . path . expanduser ( '~/Library/Logs' ) , appname ) \n        else : \n            path = os . path . join ( user_cache_dir ( appname ) , \"log\" ) \n    return path "}
{"10864": "\ndef user_config_dir ( appname , roaming = True ) : \n    if WINDOWS : \n        path = user_data_dir ( appname , roaming = roaming ) \n    else : \n        if sys . platform == \"darwin\" : \n            path = user_data_dir ( appname ) \n        else : \n            path = os . getenv ( 'XDG_CONFIG_HOME' , os . path . expanduser ( \"~/.config\" ) ) \n            path = os . path . join ( path , appname ) \n    return path "}
{"10865": "\ndef site_config_dirs ( appname ) : \n    if WINDOWS : \n        path = os . path . normpath ( _get_win_folder ( \"CSIDL_COMMON_APPDATA\" ) ) \n        pathlist = [ os . path . join ( path , appname ) ] \n    else : \n        if sys . platform == 'darwin' : \n            pathlist = [ os . path . join ( '/Library/Application Support' , appname ) ] \n        else : \n            xdg_config_dirs = os . getenv ( 'XDG_CONFIG_DIRS' , '/etc/xdg' ) \n            if xdg_config_dirs : \n                pathlist = [ os . sep . join ( [ os . path . expanduser ( x ) , appname ] ) for x in xdg_config_dirs . split ( os . pathsep ) ] \n            else : \n                pathlist = [ ] \n            pathlist . append ( '/etc' ) \n    return pathlist "}
{"10868": "\ndef to_text ( s , blank_if_none = True ) : \n    if s is None : \n        if blank_if_none : \n            return \"\" \n        else : \n            return None \n    else : \n        if isinstance ( s , text_type ) : \n            return s \n        else : \n            return text_type ( s ) "}
{"10883": "\ndef local_open ( url ) : \n    scheme , server , path , param , query , frag = urlparse ( url ) \n    filename = url2pathname ( path ) \n    if os . path . isfile ( filename ) : \n        return urllib2 . urlopen ( url ) \n    else : \n        if path . endswith ( '/' ) and os . path . isdir ( filename ) : \n            files = [ ] \n            for f in os . listdir ( filename ) : \n                if f == 'index.html' : \n                    with open ( os . path . join ( filename , f ) , 'r' ) as fp : \n                        body = fp . read ( ) \n                    break \n                else : \n                    if os . path . isdir ( os . path . join ( filename , f ) ) : \n                        f += '/' \n                files . append ( \"<a href=%r>%s</a>\" % ( f , f ) ) \n            else : \n                body = ( \"<html><head><title>%s</title>\" % url ) + \"</head><body>%s</body></html>\" % '\\n' . join ( files ) \n            status , message = 200 , \"OK\" \n        else : \n            status , message , body = 404 , \"Path not found\" , \"Not found\" \n    headers = { 'content-type' : 'text/html' } \n    return HTTPError ( url , status , message , headers , StringIO ( body ) ) "}
{"10891": "\ndef setquit ( ) : \n    if os . sep == ':' : \n        eof = 'Cmd-Q' \n    else : \n        if os . sep == '\\\\' : \n            eof = 'Ctrl-Z plus Return' \n        else : \n            eof = 'Ctrl-D (i.e. EOF)' \n    class Quitter ( object ) : \n        def __init__ ( self , name ) : \n            self . name = name \n        def __repr__ ( self ) : \n            return 'Use %s() or %s to exit' % ( self . name , eof ) \n        def __call__ ( self , code = None ) : \n            try : \n                sys . stdin . close ( ) \n            except : \n                pass \n            raise SystemExit ( code ) \n    builtins . quit = Quitter ( 'quit' ) \n    builtins . exit = Quitter ( 'exit' ) "}
{"10901": "\ndef user_agent ( ) : \n    data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . __version__ } , \"python\" : platform . python_version ( ) , \"implementation\" : { \"name\" : platform . python_implementation ( ) , } , } \n    if data [ \"implementation\" ] [ \"name\" ] == 'CPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    else : \n        if data [ \"implementation\" ] [ \"name\" ] == 'PyPy' : \n            if sys . pypy_version_info . releaselevel == 'final' : \n                pypy_version_info = sys . pypy_version_info [ : 3 ] \n            else : \n                pypy_version_info = sys . pypy_version_info \n            data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy_version_info ] ) \n        else : \n            if data [ \"implementation\" ] [ \"name\" ] == 'Jython' : \n                data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n            else : \n                if data [ \"implementation\" ] [ \"name\" ] == 'IronPython' : \n                    data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    if sys . platform . startswith ( \"linux\" ) : \n        distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux_distribution ( ) ) , ) ) \n        libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"lib\" , \"version\" ] , platform . libc_ver ( ) ) , ) ) \n        if libc : \n            distro [ \"libc\" ] = libc \n        if distro : \n            data [ \"distro\" ] = distro \n    if sys . platform . startswith ( \"darwin\" ) and platform . mac_ver ( ) [ 0 ] : \n        data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac_ver ( ) [ 0 ] } \n    if platform . system ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) \n    if platform . release ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) \n    if platform . machine ( ) : \n        data [ \"cpu\" ] = platform . machine ( ) \n    return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort_keys = True ) , ) "}
{"10915": "\ndef visit_Name ( self , node ) : \n    if node . ctx == 'store' : \n        self . identifiers . declared_locally . add ( node . name ) \n    else : \n        if node . ctx == 'param' : \n            self . identifiers . declared_parameter . add ( node . name ) \n        else : \n            if node . ctx == 'load' and not self . identifiers . is_declared ( node . name ) : \n                self . identifiers . undeclared . add ( node . name ) "}
{"10916": "\ndef visit_Include ( self , node , frame ) : \n    if node . with_context : \n        self . unoptimize_scope ( frame ) \n    if node . ignore_missing : \n        self . writeline ( 'try:' ) \n        self . indent ( ) \n    func_name = 'get_or_select_template' \n    if isinstance ( node . template , nodes . Const ) : \n        if isinstance ( node . template . value , string_types ) : \n            func_name = 'get_template' \n        else : \n            if isinstance ( node . template . value , ( tuple , list ) ) : \n                func_name = 'select_template' \n    else : \n        if isinstance ( node . template , ( nodes . Tuple , nodes . List ) ) : \n            func_name = 'select_template' \n    self . writeline ( 'template = environment.%s(' % func_name , node ) \n    self . visit ( node . template , frame ) \n    self . write ( ', %r)' % self . name ) \n    if node . ignore_missing : \n        self . outdent ( ) \n        self . writeline ( 'except TemplateNotFound:' ) \n        self . indent ( ) \n        self . writeline ( 'pass' ) \n        self . outdent ( ) \n        self . writeline ( 'else:' ) \n        self . indent ( ) \n    if node . with_context : \n        self . writeline ( 'for event in template.root_render_func(' 'template.new_context(context.parent, True, ' 'locals())):' ) \n    else : \n        self . writeline ( 'for event in template.module._body_stream:' ) \n    self . indent ( ) \n    self . simple_write ( 'event' , frame ) \n    self . outdent ( ) \n    if node . ignore_missing : \n        self . outdent ( ) "}
{"10935": "\ndef build_response ( self , request , response , from_cache = False ) : \n    if not from_cache and request . method == 'GET' : \n        if response . status == 304 : \n            cached_response = self . controller . update_cached_response ( request , response ) \n            if cached_response is not response : \n                from_cache = True \n            response . read ( decode_content = False ) \n            response . release_conn ( ) \n            response = cached_response \n        else : \n            if response . status == 301 : \n                self . controller . cache_response ( request , response ) \n            else : \n                if self . heuristic : \n                    response = self . heuristic . apply ( response ) \n                response . _fp = CallbackFileWrapper ( response . _fp , functools . partial ( self . controller . cache_response , request , response , ) ) \n    resp = super ( CacheControlAdapter , self ) . build_response ( request , response ) \n    if request . method in self . invalidating_methods and resp . ok : \n        cache_url = self . controller . cache_url ( request . url ) \n        self . cache . delete ( cache_url ) \n    resp . from_cache = from_cache \n    return resp "}
{"10944": "\ndef derive_key ( self ) : \n    salt = want_bytes ( self . salt ) \n    if self . key_derivation == 'concat' : \n        return self . digest_method ( salt + self . secret_key ) . digest ( ) \n    else : \n        if self . key_derivation == 'django-concat' : \n            return self . digest_method ( salt + b'signer' + self . secret_key ) . digest ( ) \n        else : \n            if self . key_derivation == 'hmac' : \n                mac = hmac . new ( self . secret_key , digestmod = self . digest_method ) \n                mac . update ( salt ) \n                return mac . digest ( ) \n            else : \n                if self . key_derivation == 'none' : \n                    return self . secret_key \n                else : \n                    raise TypeError ( 'Unknown key derivation method' ) "}
{"10964": "\ndef _on_edges ( self , object , name , old , new ) : \n    if name == \"edges_items\" : \n        edges = new . added \n    else : \n        if name == \"edges\" : \n            edges = new \n        else : \n            edges = [ ] \n    all_nodes = [ n for g in self . all_graphs for n in g . nodes ] \n    for each_edge in edges : \n        if each_edge . tail_node not in all_nodes : \n            object . nodes . append ( each_edge . tail_node ) \n        if each_edge . head_node not in all_nodes : \n            object . nodes . append ( each_edge . head_node ) \n        each_edge . _nodes = all_nodes "}
{"10998": "\ndef add_edge ( self , info ) : \n    if not info . initialized : \n        return \n    graph = self . _request_graph ( info . ui . control ) \n    if graph is None : \n        return \n    n_nodes = len ( graph . nodes ) \n    IDs = [ v . ID for v in graph . nodes ] \n    if n_nodes == 0 : \n        tail_node = Node ( ID = make_unique_name ( \"node\" , IDs ) ) \n        head_name = make_unique_name ( \"node\" , IDs + [ tail_node . ID ] ) \n        head_node = Node ( ID = head_name ) \n    else : \n        if n_nodes == 1 : \n            tail_node = graph . nodes [ 0 ] \n            head_node = Node ( ID = make_unique_name ( \"node\" , IDs ) ) \n        else : \n            tail_node = graph . nodes [ 0 ] \n            head_node = graph . nodes [ 1 ] \n    edge = Edge ( tail_node , head_node , _nodes = graph . nodes ) \n    retval = edge . edit_traits ( parent = info . ui . control , kind = \"livemodal\" ) \n    if retval . result : \n        graph . edges . append ( edge ) "}
{"11005": "\ndef move_to_origin ( components ) : \n    for component in components : \n        if isinstance ( component , Ellipse ) : \n            component . x_origin = component . e_width \n            component . y_origin = component . e_height \n        else : \n            if isinstance ( component , ( Polygon , BSpline ) ) : \n                min_x = min ( [ t [ 0 ] for t in component . points ] ) \n                min_y = min ( [ t [ 1 ] for t in component . points ] ) \n                component . points = [ ( p [ 0 ] - min_x , p [ 1 ] - min_y ) for p in component . points ] \n            else : \n                if isinstance ( component , Text ) : \n                    font = str_to_font ( str ( component . pen . font ) ) \n                    component . text_x = 0 \n                    component . text_y = 0 "}
{"11017": "\ndef create ( self , prog = None , format = None ) : \n    prog = self . program if prog is None else prog \n    format = self . format if format is None else format \n    tmp_fd , tmp_name = tempfile . mkstemp ( ) \n    os . close ( tmp_fd ) \n    dot_fd = file ( tmp_name , \"w+b\" ) \n    self . save_dot ( dot_fd ) \n    dot_fd . close ( ) \n    tmp_dir = os . path . dirname ( tmp_name ) \n    p = subprocess . Popen ( ( self . programs [ prog ] , '-T' + format , tmp_name ) , cwd = tmp_dir , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    stderr = p . stderr \n    stdout = p . stdout \n    stdout_output = list ( ) \n    while True : \n        data = stdout . read ( ) \n        if not data : \n            break \n        stdout_output . append ( data ) \n    stdout . close ( ) \n    if stdout_output : \n        stdout_output = '' . join ( stdout_output ) \n    if not stderr . closed : \n        stderr_output = list ( ) \n        while True : \n            data = stderr . read ( ) \n            if not data : \n                break \n            stderr_output . append ( data ) \n        stderr . close ( ) \n        if stderr_output : \n            stderr_output = '' . join ( stderr_output ) \n    status = p . wait ( ) \n    if status != 0 : \n        logger . error ( \"Program terminated with status: %d. stderr \" \"follows: %s\" % ( status , stderr_output ) ) \n    else : \n        if stderr_output : \n            logger . error ( \"%s\" , stderr_output ) \n    os . unlink ( tmp_name ) \n    return stdout_output "}
{"11023": "\ndef add_subgraph ( self , subgraph_or_ID ) : \n    if not isinstance ( subgraph_or_ID , ( godot . subgraph . Subgraph , godot . cluster . Cluster ) ) : \n        subgraphID = str ( subgraph_or_ID ) \n        if subgraph_or_ID . startswith ( \"cluster\" ) : \n            subgraph = godot . cluster . Cluster ( ID = subgraphID ) \n        else : \n            subgraph = godot . subgraph . Subgraph ( ID = subgraphID ) \n    else : \n        subgraph = subgraph_or_ID \n    subgraph . default_node = self . default_node \n    subgraph . default_edge = self . default_edge \n    if isinstance ( subgraph , godot . subgraph . Subgraph ) : \n        self . subgraphs . append ( subgraph ) \n    else : \n        if isinstance ( subgraph , godot . cluster . Cluster ) : \n            self . clusters . append ( subgraph ) \n        else : \n            raise \n    return subgraph "}
{"11029": "\ndef build_graph ( self , graph , tokens ) : \n    subgraph = None \n    for element in tokens : \n        cmd = element [ 0 ] \n        if cmd == ADD_NODE : \n            cmd , nodename , opts = element \n            graph . add_node ( nodename , ** opts ) \n        else : \n            if cmd == ADD_EDGE : \n                cmd , src , dest , opts = element \n                srcport = destport = \"\" \n                if isinstance ( src , tuple ) : \n                    srcport = src [ 1 ] \n                    src = src [ 0 ] \n                if isinstance ( dest , tuple ) : \n                    destport = dest [ 1 ] \n                    dest = dest [ 0 ] \n                graph . add_edge ( src , dest , tailport = srcport , headport = destport , ** opts ) \n            else : \n                if cmd in [ ADD_GRAPH_TO_NODE_EDGE , ADD_GRAPH_TO_GRAPH_EDGE , ADD_NODE_TO_GRAPH_EDGE ] : \n                    cmd , src , dest , opts = element \n                    srcport = destport = \"\" \n                    if isinstance ( src , tuple ) : \n                        srcport = src [ 1 ] \n                    if isinstance ( dest , tuple ) : \n                        destport = dest [ 1 ] \n                    if not ( cmd == ADD_NODE_TO_GRAPH_EDGE ) : \n                        if cmd == ADD_GRAPH_TO_NODE_EDGE : \n                            src = subgraph \n                        else : \n                            src = prev_subgraph \n                            dest = subgraph \n                    else : \n                        dest = subgraph \n                    src_is_graph = isinstance ( src , ( Subgraph , Cluster ) ) \n                    dst_is_graph = isinstance ( dst , ( Subgraph , Cluster ) ) \n                    if src_is_graph : \n                        src_nodes = src . nodes \n                    else : \n                        src_nodes = [ src ] \n                    if dst_is_graph : \n                        dst_nodes = dst . nodes \n                    else : \n                        dst_nodes = [ dst ] \n                    for src_node in src_nodes : \n                        for dst_node in dst_nodes : \n                            graph . add_edge ( from_node = src_node , to_node = dst_node , tailport = srcport , headport = destport , ** kwds ) \n                else : \n                    if cmd == SET_GRAPH_ATTR : \n                        graph . set ( ** element [ 1 ] ) \n                    else : \n                        if cmd == SET_DEF_NODE_ATTR : \n                            graph . default_node . set ( ** element [ 1 ] ) \n                        else : \n                            if cmd == SET_DEF_EDGE_ATTR : \n                                graph . default_edge . set ( ** element [ 1 ] ) \n                            else : \n                                if cmd == SET_DEF_GRAPH_ATTR : \n                                    graph . default_graph . set ( ** element [ 1 ] ) \n                                else : \n                                    if cmd == ADD_SUBGRAPH : \n                                        cmd , name , elements = element \n                                        if subgraph : \n                                            prev_subgraph = subgraph \n                                        if name . startswith ( \"cluster\" ) : \n                                            cluster = Cluster ( ID = name ) \n                                            cluster = self . build_graph ( cluster , elements ) \n                                            graph . add_cluster ( cluster ) \n                                        else : \n                                            subgraph = Subgraph ( ID = name ) \n                                            subgraph = self . build_graph ( subgraph , elements ) \n                                            graph . add_subgraph ( subgraph ) \n    return graph "}
{"11038": "\ndef append_child ( self , object , child ) : \n    if isinstance ( child , Subgraph ) : \n        object . subgraphs . append ( child ) \n    else : \n        if isinstance ( child , Cluster ) : \n            object . clusters . append ( child ) \n        else : \n            if isinstance ( child , Node ) : \n                object . nodes . append ( child ) \n            else : \n                if isinstance ( child , Edge ) : \n                    object . edges . append ( child ) \n                else : \n                    pass "}
{"11039": "\ndef insert_child ( self , object , index , child ) : \n    if isinstance ( child , Subgraph ) : \n        object . subgraphs . insert ( index , child ) \n    else : \n        if isinstance ( child , Cluster ) : \n            object . clusters . insert ( index , child ) \n        else : \n            if isinstance ( child , Node ) : \n                object . nodes . insert ( index , child ) \n            else : \n                if isinstance ( child , Edge ) : \n                    object . edges . insert ( index , child ) \n                else : \n                    pass "}
{"11040": "\ndef delete_child ( self , object , index ) : \n    if isinstance ( child , Subgraph ) : \n        object . subgraphs . pop ( index ) \n    else : \n        if isinstance ( child , Cluster ) : \n            object . clusters . pop ( index ) \n        else : \n            if isinstance ( child , Node ) : \n                object . nodes . pop ( index ) \n            else : \n                if isinstance ( child , Edge ) : \n                    object . edges . pop ( index ) \n                else : \n                    pass "}
{"11058": "\ndef edge_factory ( ** row_factory_kw ) : \n    if \"__table_editor__\" in row_factory_kw : \n        table_editor = row_factory_kw [ \"__table_editor__\" ] \n        graph = table_editor . object \n        ID = make_unique_name ( \"node\" , [ node . ID for node in graph . nodes ] ) \n        n_nodes = len ( graph . nodes ) \n        IDs = [ v . ID for v in graph . nodes ] \n        if n_nodes == 0 : \n            tail_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n            head_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n        else : \n            if n_nodes == 1 : \n                tail_node = graph . nodes [ 0 ] \n                head_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n            else : \n                tail_node = graph . nodes [ 0 ] \n                head_node = graph . nodes [ 1 ] \n        return godot . edge . Edge ( tail_node , head_node , _nodes = graph . nodes ) \n    else : \n        return None "}
{"11076": "\ndef select_content_type ( requested , available ) : \n    class Match ( object ) : \n        WILDCARD , PARTIAL , FULL_TYPE , = 2 , 1 , 0 \n        def __init__ ( self , candidate , pattern ) : \n            self . candidate = candidate \n            self . pattern = pattern \n            if pattern . content_type == pattern . content_subtype == '*' : \n                self . match_type = self . WILDCARD \n            else : \n                if pattern . content_subtype == '*' : \n                    self . match_type = self . PARTIAL \n                else : \n                    self . match_type = self . FULL_TYPE \n            self . parameter_distance = len ( self . candidate . parameters ) \n            for key , value in candidate . parameters . items ( ) : \n                if key in pattern . parameters : \n                    if pattern . parameters [ key ] == value : \n                        self . parameter_distance -= 1 \n                    else : \n                        self . parameter_distance += 1 \n    def extract_quality ( obj ) : \n        return getattr ( obj , 'quality' , 1.0 ) \n    matches = [ ] \n    for pattern in sorted ( requested , key = extract_quality , reverse = True ) : \n        for candidate in sorted ( available ) : \n            if _content_type_matches ( candidate , pattern ) : \n                if candidate == pattern : \n                    if extract_quality ( pattern ) == 0.0 : \n                        raise errors . NoMatch \n                    return candidate , pattern \n                matches . append ( Match ( candidate , pattern ) ) \n    if not matches : \n        raise errors . NoMatch \n    matches = sorted ( matches , key = attrgetter ( 'match_type' , 'parameter_distance' ) ) \n    return matches [ 0 ] . candidate , matches [ 0 ] . pattern "}
{"11077": "\ndef rewrite_url ( input_url , ** kwargs ) : \n    scheme , netloc , path , query , fragment = parse . urlsplit ( input_url ) \n    if 'scheme' in kwargs : \n        scheme = kwargs [ 'scheme' ] \n    ident , host_n_port = parse . splituser ( netloc ) \n    user , password = parse . splitpasswd ( ident ) if ident else ( None , None ) \n    if 'user' in kwargs : \n        user = kwargs [ 'user' ] \n    else : \n        if user is not None : \n            user = parse . unquote_to_bytes ( user ) . decode ( 'utf-8' ) \n    if 'password' in kwargs : \n        password = kwargs [ 'password' ] \n    else : \n        if password is not None : \n            password = parse . unquote_to_bytes ( password ) . decode ( 'utf-8' ) \n    ident = _create_url_identifier ( user , password ) \n    host , port = parse . splitnport ( host_n_port , defport = None ) \n    if 'host' in kwargs : \n        host = kwargs [ 'host' ] \n        if host is not None : \n            host = _normalize_host ( host , enable_long_host = kwargs . get ( 'enable_long_host' , False ) , encode_with_idna = kwargs . get ( 'encode_with_idna' , None ) , scheme = scheme , ) \n    if 'port' in kwargs : \n        port = kwargs [ 'port' ] \n        if port is not None : \n            port = int ( kwargs [ 'port' ] ) \n            if port < 0 : \n                raise ValueError ( 'port is required to be non-negative' ) \n    if host is None or host == '' : \n        host_n_port = None \n    else : \n        if port is None : \n            host_n_port = host \n        else : \n            host_n_port = '{0}:{1}' . format ( host , port ) \n    if 'path' in kwargs : \n        path = kwargs [ 'path' ] \n        if path is None : \n            path = '/' \n        else : \n            path = parse . quote ( path . encode ( 'utf-8' ) , safe = PATH_SAFE_CHARS ) \n    netloc = '{0}@{1}' . format ( ident , host_n_port ) if ident else host_n_port \n    if 'query' in kwargs : \n        new_query = kwargs [ 'query' ] \n        if new_query is None : \n            query = None \n        else : \n            params = [ ] \n            try : \n                for param in sorted ( new_query . keys ( ) ) : \n                    params . append ( ( param , new_query [ param ] ) ) \n            except AttributeError : \n                pass \n            if not params : \n                try : \n                    params = [ ( param , value ) for param , value in new_query ] \n                except ValueError : \n                    pass \n            if params : \n                query = parse . urlencode ( params ) \n            else : \n                query = new_query \n    if 'fragment' in kwargs : \n        fragment = kwargs [ 'fragment' ] \n        if fragment is not None : \n            fragment = parse . quote ( fragment . encode ( 'utf-8' ) , safe = FRAGMENT_SAFE_CHARS ) \n    if scheme is None : \n        scheme = '' \n    return parse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) "}
{"11100": "\ndef add_details ( self , message ) : \n    msg = message \n    try : \n        from flask import request \n        url = request . url \n        method = request . method \n        endpoint = request . endpoint \n        form_dict = dict ( request . form ) \n        for key in form_dict : \n            if key . lower ( ) in _error_reporting_obscured_fields : \n                form_dict [ key ] = '******' \n            else : \n                if len ( form_dict [ key ] ) == 1 : \n                    form_dict [ key ] = form_dict [ key ] [ 0 ] \n        form = pprint . pformat ( form_dict ) . replace ( '\\n' , '\\n          ' ) \n        msg = '%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % ( msg , url , method , endpoint , form ) \n    except Exception : \n        traceback . print_exc ( ) \n    try : \n        from flask import session \n        from flask . json import JSONEncoder \n        session_str = json . dumps ( dict ( ** session ) , indent = 2 , cls = JSONEncoder ) \n        msg = '%s\\nSession:\\n\\n%s\\n' % ( msg , session_str ) \n    except Exception : \n        traceback . print_exc ( ) \n    return msg "}
{"11104": "\ndef add_to_queue ( self , url ) : \n    if self . connection_handler . current_music is None : \n        log . error ( 'Music service is not initialized. URL was not added to queue.' ) \n    else : \n        if self . connection_handler . current_storage is None : \n            log . error ( 'Drive service is not initialized. URL was not added to queue.' ) \n        else : \n            self . queues [ 'download' ] . put ( url ) "}
{"11115": "\ndef use_music_service ( self , service_name , api_key ) : \n    try : \n        self . current_music = self . music_services [ service_name ] \n    except KeyError : \n        if service_name == 'youtube' : \n            self . music_services [ 'youtube' ] = Youtube ( ) \n            self . current_music = self . music_services [ 'youtube' ] \n        else : \n            if service_name == 'soundcloud' : \n                self . music_services [ 'soundcloud' ] = Soundcloud ( api_key = api_key ) \n                self . current_music = self . music_services [ 'soundcloud' ] \n            else : \n                log . error ( 'Music service name is not recognized.' ) "}
{"11116": "\ndef use_storage_service ( self , service_name , custom_path ) : \n    try : \n        self . current_storage = self . storage_services [ service_name ] \n    except KeyError : \n        if service_name == 'google drive' : \n            self . storage_services [ 'google drive' ] = GoogleDrive ( ) \n            self . current_storage = self . storage_services [ 'google drive' ] \n            self . current_storage . connect ( ) \n        else : \n            if service_name == 'dropbox' : \n                log . error ( 'Dropbox is not supported yet.' ) \n            else : \n                if service_name == 'local' : \n                    self . storage_services [ 'local' ] = LocalStorage ( custom_path = custom_path ) \n                    self . current_storage = self . storage_services [ 'local' ] \n                    self . current_storage . connect ( ) \n                else : \n                    log . error ( 'Storage service name is not recognized.' ) "}
{"11133": "\ndef handle_ssl_redirect ( ) : \n    if request . endpoint and request . endpoint not in [ 'static' , 'filemanager.static' ] : \n        needs_ssl = False \n        ssl_enabled = False \n        view_function = current_app . view_functions [ request . endpoint ] \n        if request . endpoint . startswith ( 'admin.' ) or ( hasattr ( view_function , 'ssl_required' ) and view_function . ssl_required ) : \n            needs_ssl = True \n            ssl_enabled = True \n        if hasattr ( view_function , 'ssl_allowed' ) and view_function . ssl_allowed : \n            ssl_enabled = True \n        if ( hasattr ( view_function , 'ssl_disabled' ) and view_function . ssl_disabled ) : \n            needs_ssl = False \n            ssl_enabled = False \n        if current_app . config [ 'SSL_ENABLED' ] : \n            if needs_ssl and not request . is_secure : \n                log . debug ( 'Redirecting to https: %s' % request . endpoint ) \n                return redirect ( request . url . replace ( \"http://\" , \"https://\" ) ) \n            else : \n                if not ssl_enabled and request . is_secure : \n                    log . debug ( 'Redirecting to http: %s' % request . endpoint ) \n                    return redirect ( request . url . replace ( \"https://\" , \"http://\" ) ) \n        else : \n            if needs_ssl : \n                log . info ( 'Not redirecting to HTTPS for endpoint %s as SSL_ENABLED is set to False' % request . endpoint ) "}
{"11136": "\ndef parse_accept ( header_value ) : \n    next_explicit_q = decimal . ExtendedContext . next_plus ( decimal . Decimal ( '5.0' ) ) \n    headers = [ parse_content_type ( header ) for header in parse_list ( header_value ) ] \n    for header in headers : \n        q = header . parameters . pop ( 'q' , None ) \n        if q is None : \n            q = '1.0' \n        else : \n            if float ( q ) == 1.0 : \n                q = float ( next_explicit_q ) \n                next_explicit_q = next_explicit_q . next_minus ( ) \n        header . quality = float ( q ) \n    def ordering ( left , right ) : \n        if left . quality != right . quality : \n            return right . quality - left . quality \n        if left == right : \n            return 0 \n        if left > right : \n            return - 1 \n        return 1 \n    return sorted ( headers , key = functools . cmp_to_key ( ordering ) ) "}
{"11137": "\ndef parse_cache_control ( header_value ) : \n    directives = { } \n    for segment in parse_list ( header_value ) : \n        name , sep , value = segment . partition ( '=' ) \n        if sep != '=' : \n            directives [ name ] = None \n        else : \n            if sep and value : \n                value = _dequote ( value . strip ( ) ) \n                try : \n                    directives [ name ] = int ( value ) \n                except ValueError : \n                    directives [ name ] = value \n    for name in _CACHE_CONTROL_BOOL_DIRECTIVES : \n        if directives . get ( name , '' ) is None : \n            directives [ name ] = True \n    return directives "}
{"11143": "\ndef add_value ( self , name , value ) : \n    try : \n        if self . _rfc_values [ name ] is None : \n            self . _rfc_values [ name ] = value \n        else : \n            if self . strict : \n                if name in ( 'media' , 'type' ) : \n                    raise errors . MalformedLinkValue ( 'More than one {} parameter present' . format ( name ) ) \n                return \n    except KeyError : \n        pass \n    if self . strict and name in ( 'title' , 'title*' ) : \n        return \n    self . _values . append ( ( name , value ) ) "}
{"11163": "\ndef read_pr_report ( self , filename ) : \n    done = False \n    f = open ( filename ) \n    while f : \n        line = f . readline ( ) \n        if not line : \n            done = True \n            break \n        if \"# Quad solid angle mean point theta table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_theta' ] = tmp \n        else : \n            if '#' not in line or not line . strip ( ) : \n                element = line . split ( ',' ) \n                self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"# Quad solid angle mean point phi table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_phi' ] = tmp \n        else : \n            if '#' not in line or not line . strip ( ) : \n                element = line . split ( ',' ) \n                self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"L_w band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_w_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n        if \"L_it band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_it_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n    return self . data_dictionary "}
{"11166": "\ndef default_handler ( self , signum , frame ) : \n    self . log . debug ( \"Signal handler called with signal: {0}\" . format ( signum ) ) \n    if signum in self . restart_signals : \n        self . set_handler ( self . handled_signals , self . pseudo_handler ) \n        self . _cleanup ( ) \n        os . execl ( 'python' , 'python' , * sys . argv ) \n    else : \n        if signum in self . abort_signals : \n            self . abort ( signum ) \n        else : \n            if signum in self . pause_signals : \n                self . pause ( signum ) \n            else : \n                if signum in self . resume_signals : \n                    self . resume ( signum ) \n                else : \n                    if signum in self . status_signals : \n                        self . status ( signum ) \n                    else : \n                        if signum in self . error_signals : \n                            self . log . error ( 'Signal handler received error signal from an external process, aborting' ) \n                            self . abort ( signum ) \n                        else : \n                            self . log . error ( \"Unhandled signal received: {0}\" . format ( signum ) ) \n                            raise "}
{"11178": "\ndef __new_argv ( self , * new_pargs , ** new_kargs ) : \n    new_argv = self . argv . copy ( ) \n    new_extra_argv = list ( self . extra_argv ) \n    for v in new_pargs : \n        arg_name = None \n        for name in self . pargl : \n            if not name in new_argv : \n                arg_name = name \n                break \n        if arg_name : \n            new_argv [ arg_name ] = v \n        else : \n            if self . var_pargs : \n                new_extra_argv . append ( v ) \n            else : \n                num_prev_pargs = len ( [ name for name in self . pargl if name in self . argv ] ) \n                raise TypeError ( \"%s() takes exactly %d positional arguments (%d given)\" % ( self . __name__ , len ( self . pargl ) , num_prev_pargs + len ( new_pargs ) ) ) \n    for k , v in new_kargs . items ( ) : \n        if not ( self . var_kargs or ( k in self . pargl ) or ( k in self . kargl ) ) : \n            raise TypeError ( \"%s() got an unexpected keyword argument '%s'\" % ( self . __name__ , k ) ) \n        new_argv [ k ] = v \n    return ( new_argv , new_extra_argv ) "}
{"11180": "\ndef file_key ( filename ) : \n    prio = 4 \n    if filename == 'install.rdf' : \n        prio = 1 \n    else : \n        if filename in [ \"chrome.manifest\" , \"icon.png\" , \"icon64.png\" ] : \n            prio = 2 \n        else : \n            if filename in [ \"MPL\" , \"GPL\" , \"LGPL\" , \"COPYING\" , \"LICENSE\" , \"license.txt\" ] : \n                prio = 5 \n    return ( prio , os . path . split ( filename . lower ( ) ) ) "}
{"11189": "\ndef data_processing ( self ) : \n    the_file_name = str ( self . result_file ) \n    the_file = open ( the_file_name , 'r' ) \n    lines = the_file . readlines ( ) \n    lines_array = [ ] \n    for line in lines : \n        line = line . split ( ',' ) \n        lines_array . append ( line ) \n    labels_line = lines_array [ 0 ] \n    cell_labels_line = 0 \n    flag = True \n    try : \n        while flag : \n            if \"wave length (nm)\" in labels_line [ cell_labels_line ] : \n                index = labels_line . index ( labels_line [ cell_labels_line ] ) \n                flag = False \n            else : \n                cell_labels_line += 1 \n    except IndexError : \n        raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) \n    self . information = [ ] \n    data_wavelength = [ ] \n    self . num_line = 0 \n    for line in lines_array : \n        cell_line = 0 \n        self . information . append ( [ ] ) \n        data_wavelength . append ( [ ] ) \n        while cell_line < len ( line ) : \n            if cell_line < index : \n                self . information [ self . num_line ] . append ( line [ cell_line ] ) \n            else : \n                if cell_line > index : \n                    data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) \n            cell_line += 1 \n        self . num_line += 1 \n    line_wavelength = 0 \n    for row_data_wavelength in data_wavelength : \n        row_data_wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row_data_wavelength ] \n        data_wavelength [ line_wavelength ] = row_data_wavelength \n        line_wavelength += 1 \n    self . wavelength = data_wavelength [ 0 ] \n    self . data_wanted = data_wavelength [ 1 : ] \n    the_file . close ( ) "}
{"11194": "\ndef run ( self ) : \n    print ( 'Executing planarrad' ) \n    if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n        self . data ( ) \n        self . check_values ( ) \n        if self . without_error == False : \n            self . display_error_message ( ) \n        else : \n            if self . without_error == True : \n                self . is_running = True \n                self . hide_error_message ( ) \n                self . write_to_file ( ) \n                os . chdir ( './' ) \n                self . progress_bar ( ) \n                this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) \n                batch_file = os . path . join ( this_dir , \"inputs/batch_files/\" + str ( self . batch_name_value ) + \"_batch.txt\" ) \n                print ( batch_file ) \n                self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch_file ] , shell = True ) \n                if self . ui . progressBar . value ( ) == 100 : \n                    self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) "}
{"11234": "\ndef find_teradata_home ( ) : \n    if platform . system ( ) == 'Windows' : \n        if is_64bit ( ) : \n            return latest_teradata_version ( \"C:/Program Files/Teradata/Client\" ) \n        else : \n            return latest_teradata_version ( \"C:/Program Files (x86)/Teradata/Client\" ) \n    else : \n        if platform . system ( ) == 'Linux' : \n            return latest_teradata_version ( \"/opt/teradata/client\" ) \n        else : \n            if platform . system ( ) == 'Darwin' : \n                return latest_teradata_version ( \"/Library/Application Support/teradata/client\" ) \n            else : \n                return latest_teradata_version ( \"/opt/teradata/client\" ) "}
{"11237": "\ndef do_table ( self , line ) : \n    if len ( line ) > 0 : \n        if line . strip ( ) . lower ( ) == \"on\" : \n            log . write ( \"Table ON\" ) \n            self . table_output = True \n            return \n        else : \n            if line . strip ( ) . lower ( ) == \"off\" : \n                log . write ( \"Table OFF\" ) \n                self . table_output = False \n                return \n    log . write ( \"Table output: {}\" . format ( \"ON\" if self . table_output else \"OFF\" ) ) "}
{"11251": "\ndef wait_for_result ( self , psd_state ) : \n    if len ( psd_state [ 'futures' ] ) > 1 : \n        concurrent . futures . wait ( psd_state [ 'futures' ] ) \n    else : \n        if psd_state [ 'futures' ] : \n            psd_state [ 'futures' ] [ 0 ] . result ( ) \n    return self . result ( psd_state ) "}
{"11290": "\ndef _check_status ( cls , response_json ) : \n    status = response_json [ 'status' ] \n    msg = response_json [ 'msg' ] \n    if status == 400 : \n        raise BadRequestException ( msg ) \n    else : \n        if status == 403 : \n            raise PermissionDeniedException ( msg ) \n        else : \n            if status == 404 : \n                raise FileNotFoundException ( msg ) \n            else : \n                if status == 451 : \n                    raise UnavailableForLegalReasonsException ( msg ) \n                else : \n                    if status == 509 : \n                        raise BandwidthUsageExceeded ( msg ) \n                    else : \n                        if status >= 500 : \n                            raise ServerErrorException ( msg ) "}
{"11330": "\ndef run ( query , params = None , config = None , conn = None , ** kwargs ) : \n    if params is None : \n        params = { } \n    if conn is None : \n        conn = Connection . get ( DEFAULT_CONFIGURABLE [ \"uri\" ] ) \n    else : \n        if isinstance ( conn , string_types ) : \n            conn = Connection . get ( conn ) \n    if config is None : \n        default_config = DEFAULT_CONFIGURABLE . copy ( ) \n        kwargs . update ( default_config ) \n        config = DefaultConfigurable ( ** kwargs ) \n    if query . strip ( ) : \n        params = extract_params_from_query ( query , params ) \n        result = conn . session . query ( query , params , data_contents = config . data_contents ) \n        if config . feedback : \n            print ( interpret_stats ( result ) ) \n        resultset = ResultSet ( result , query , config ) \n        if config . auto_pandas : \n            return resultset . get_dataframe ( ) \n        else : \n            if config . auto_networkx : \n                graph = resultset . get_graph ( ) \n                resultset . draw ( ) \n                return graph \n            else : \n                return resultset \n    else : \n        return 'Connected: %s' % conn . name "}
{"11349": "\ndef concatenate ( arrays , axis = 0 ) : \n    if not isinstance ( arrays , tuple ) : \n        raise ValueError ( \"data type not understood\" ) \n    if not len ( arrays ) == 2 : \n        raise NotImplementedError ( \"spark concatenation only supports two arrays\" ) \n    first , second = arrays \n    if isinstance ( first , BoltArraySpark ) : \n        return first . concatenate ( second , axis ) \n    else : \n        if isinstance ( second , BoltArraySpark ) : \n            first = ConstructSpark . array ( first , second . _rdd . context ) \n            return first . concatenate ( second , axis ) \n        else : \n            raise ValueError ( \"at least one array must be a spark bolt array\" ) "}
{"11351": "\ndef _format_axes ( axes , shape ) : \n    if isinstance ( axes , int ) : \n        axes = ( axes , ) \n    else : \n        if isinstance ( axes , list ) or hasattr ( axes , '__iter__' ) : \n            axes = tuple ( axes ) \n    if not isinstance ( axes , tuple ) : \n        raise ValueError ( \"axes argument %s in the constructor not specified correctly\" % str ( axes ) ) \n    if min ( axes ) < 0 or max ( axes ) > len ( shape ) - 1 : \n        raise ValueError ( \"invalid key axes %s given shape %s\" % ( str ( axes ) , str ( shape ) ) ) \n    return axes "}
{"11357": "\ndef map ( self , func ) : \n    vshape = self . shape [ self . split : ] \n    x = self . _rdd . values ( ) . first ( ) \n    if x . shape == vshape : \n        a , b = asarray ( [ x ] ) , asarray ( [ x , x ] ) \n    else : \n        a , b = x , concatenate ( ( x , x ) ) \n    try : \n        atest = func ( a ) \n        btest = func ( b ) \n    except Exception as e : \n        raise RuntimeError ( \"Error evaluating function on test array, got error:\\n %s\" % e ) \n    if not ( isinstance ( atest , ndarray ) and isinstance ( btest , ndarray ) ) : \n        raise ValueError ( \"Function must return ndarray\" ) \n    else : \n        if atest . shape == btest . shape : \n            if self . _rekeyed is True : \n                rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] , func ( kv [ 1 ] ) ) ) \n                shape = ( self . shape [ 0 ] , ) + atest . shape \n            else : \n                count , rdd = zip_with_index ( self . _rdd . values ( ) ) \n                rdd = rdd . map ( lambda kv : ( ( kv [ 1 ] , ) , func ( kv [ 0 ] ) ) ) \n                shape = ( count , ) + atest . shape \n            split = 1 \n            rekeyed = True \n        else : \n            if atest . shape [ 0 ] == a . shape [ 0 ] and btest . shape [ 0 ] == b . shape [ 0 ] : \n                shape = self . shape [ 0 : self . split ] + atest . shape [ 1 : ] \n                split = self . split \n                rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] , func ( kv [ 1 ] ) ) ) \n                rekeyed = self . _rekeyed \n            else : \n                raise ValueError ( \"Cannot infer effect of function on shape\" ) \n    return self . _constructor ( rdd , rekeyed = rekeyed , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11361": "\ndef getplan ( self , size = \"150\" , axes = None , padding = None ) : \n    from numpy import dtype as gettype \n    plan = self . vshape \n    if axes is None : \n        if isinstance ( size , str ) : \n            axes = arange ( len ( self . vshape ) ) \n        else : \n            axes = arange ( len ( size ) ) \n    else : \n        axes = asarray ( axes , 'int' ) \n    pad = array ( len ( self . vshape ) * [ 0 , ] ) \n    if padding is not None : \n        pad [ axes ] = padding \n    if isinstance ( size , tuple ) : \n        plan [ axes ] = size \n    else : \n        if isinstance ( size , str ) : \n            size = 1000.0 * float ( size ) \n            elsize = gettype ( self . dtype ) . itemsize \n            nelements = prod ( self . vshape ) \n            dims = self . vshape [ self . vmask ( axes ) ] \n            if size <= elsize : \n                s = ones ( len ( axes ) ) \n            else : \n                remsize = 1.0 * nelements * elsize \n                s = [ ] \n                for ( i , d ) in enumerate ( dims ) : \n                    minsize = remsize / d \n                    if minsize >= size : \n                        s . append ( 1 ) \n                        remsize = minsize \n                        continue \n                    else : \n                        s . append ( min ( d , floor ( size / minsize ) ) ) \n                        s [ i + 1 : ] = plan [ i + 1 : ] \n                        break \n            plan [ axes ] = s \n        else : \n            raise ValueError ( \"Chunk size not understood, must be tuple or int\" ) \n    return plan , pad "}
{"11383": "\ndef squeeze ( self , axis = None ) : \n    if not any ( [ d == 1 for d in self . shape ] ) : \n        return self \n    if axis is None : \n        drop = where ( asarray ( self . shape ) == 1 ) [ 0 ] \n    else : \n        if isinstance ( axis , int ) : \n            drop = asarray ( ( axis , ) ) \n        else : \n            if isinstance ( axis , tuple ) : \n                drop = asarray ( axis ) \n            else : \n                raise ValueError ( \"an integer or tuple is required for the axis\" ) \n    if any ( [ self . shape [ i ] > 1 for i in drop ] ) : \n        raise ValueError ( \"cannot select an axis to squeeze out which has size greater than one\" ) \n    if any ( asarray ( drop ) < self . split ) : \n        kmask = set ( [ d for d in drop if d < self . split ] ) \n        kfunc = lambda k : tuple ( [ kk for ii , kk in enumerate ( k ) if ii not in kmask ] ) \n    else : \n        kfunc = lambda k : k \n    if any ( asarray ( drop ) >= self . split ) : \n        vmask = tuple ( [ d - self . split for d in drop if d >= self . split ] ) \n        vfunc = lambda v : v . squeeze ( vmask ) \n    else : \n        vfunc = lambda v : v \n    rdd = self . _rdd . map ( lambda kv : ( kfunc ( kv [ 0 ] ) , vfunc ( kv [ 1 ] ) ) ) \n    shape = tuple ( [ ss for ii , ss in enumerate ( self . shape ) if ii not in drop ] ) \n    split = len ( [ d for d in range ( self . keys . ndim ) if d not in drop ] ) \n    return self . _constructor ( rdd , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11387": "\ndef tupleize ( arg ) : \n    if arg is None : \n        return None \n    if not isinstance ( arg , ( tuple , list , ndarray , Iterable ) ) : \n        return tuple ( ( arg , ) ) \n    else : \n        if isinstance ( arg , ( list , ndarray ) ) : \n            return tuple ( arg ) \n        else : \n            if isinstance ( arg , Iterable ) and not isinstance ( arg , str ) : \n                return tuple ( arg ) \n            else : \n                return arg "}
{"11388": "\ndef argpack ( args ) : \n    if isinstance ( args [ 0 ] , ( tuple , list , ndarray ) ) : \n        return tupleize ( args [ 0 ] ) \n    else : \n        if isinstance ( args [ 0 ] , Iterable ) and not isinstance ( args [ 0 ] , str ) : \n            return tupleize ( list ( args [ 0 ] ) ) \n        else : \n            return tuple ( args ) "}
{"11392": "\ndef slicify ( slc , dim ) : \n    if isinstance ( slc , slice ) : \n        start = 0 if slc . start is None else slc . start \n        stop = dim if slc . stop is None else slc . stop \n        step = 1 if slc . step is None else slc . step \n        if start < 0 : \n            start += dim \n        if stop < 0 : \n            stop += dim \n        if step > 0 : \n            if start < 0 : \n                start = 0 \n            if stop > dim : \n                stop = dim \n        else : \n            if stop < 0 : \n                stop = - 1 \n            if start > dim : \n                start = dim - 1 \n        return slice ( start , stop , step ) \n    else : \n        if isinstance ( slc , int ) : \n            if slc < 0 : \n                slc += dim \n            return slice ( slc , slc + 1 , 1 ) \n        else : \n            raise ValueError ( \"Type for slice %s not recongized\" % type ( slc ) ) "}
{"11458": "\ndef check_secure ( ) : \n    if this . request . is_secure ( ) : \n        return True \n    else : \n        if this . request . META [ 'REMOTE_ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : \n            return True \n    raise MeteorError ( 403 , 'Authentication refused without SSL.' ) "}
{"11459": "\ndef get_username ( self , user ) : \n    if isinstance ( user , basestring ) : \n        return user \n    else : \n        if isinstance ( user , dict ) and len ( user ) == 1 : \n            [ ( key , val ) ] = user . items ( ) \n            if key == 'username' or ( key == self . user_model . USERNAME_FIELD ) : \n                return val \n            else : \n                if key in ( 'email' , 'emails.address' ) : \n                    email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) \n                    if self . user_model . USERNAME_FIELD == email_field : \n                        return val \n                    return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( ** { email_field : val } ) \n                else : \n                    if key in ( 'id' , 'pk' ) : \n                        return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( pk = val , ) \n                    else : \n                        raise MeteorError ( 400 , 'Invalid user lookup: %r' % key ) \n        else : \n            raise MeteorError ( 400 , 'Invalid user expression: %r' % user ) "}
{"11463": "\ndef login ( self , params ) : \n    if 'password' in params : \n        return self . login_with_password ( params ) \n    else : \n        if 'resume' in params : \n            return self . login_with_resume_token ( params ) \n        else : \n            self . auth_failed ( ** params ) "}
{"11471": "\ndef get_meteor_id ( obj_or_model , obj_pk = None ) : \n    if obj_or_model is None : \n        return None \n    meta = obj_or_model . _meta \n    model = meta . model \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( obj_or_model , model ) : \n        if isinstance ( meta . pk , AleaIdField ) : \n            return obj_or_model . pk \n        if obj_pk is None : \n            obj_pk = str ( obj_or_model . pk ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if len ( alea_unique_fields ) == 1 : \n        aid = alea_unique_fields [ 0 ] . attname \n        if isinstance ( obj_or_model , model ) : \n            val = getattr ( obj_or_model , aid ) \n        else : \n            if obj_pk is None : \n                val = None \n            else : \n                val = model . objects . values_list ( aid , flat = True ) . get ( pk = obj_pk , ) \n        if val : \n            return val \n    if obj_pk is None : \n        return None \n    content_type = ContentType . objects . get_for_model ( model ) \n    try : \n        return ObjectMapping . objects . values_list ( 'meteor_id' , flat = True , ) . get ( content_type = content_type , object_id = obj_pk , ) \n    except ObjectDoesNotExist : \n        return ObjectMapping . objects . create ( content_type = content_type , object_id = obj_pk , meteor_id = meteor_random_id ( '/collection/%s' % meta ) , ) . meteor_id "}
{"11497": "\ndef process_ddp ( self , data ) : \n    msg_id = data . get ( 'id' , None ) \n    try : \n        msg = data . pop ( 'msg' ) \n    except KeyError : \n        self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) \n        return \n    try : \n        self . dispatch ( msg , data ) \n    except Exception as err : \n        kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } \n        if msg_id is not None : \n            kwargs [ 'id' ] = msg_id \n        if isinstance ( err , MeteorError ) : \n            error = err . as_dict ( ) \n        else : \n            error = { 'error' : 500 , 'reason' : 'Internal server error' , } \n        if kwargs [ 'msg' ] == 'error' : \n            kwargs . update ( error ) \n        else : \n            kwargs [ 'error' ] = error \n        if not isinstance ( err , MeteorError ) : \n            stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = 1 , ) \n            if stack is not None : \n                traceback . print_exc ( file = sys . stderr ) \n                sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) \n                sys . stderr . write ( stack ) \n        else : \n            if settings . DEBUG : \n                print ( 'ERROR: %s' % err ) \n                dprint ( 'msg' , msg ) \n                dprint ( 'data' , data ) \n                error . setdefault ( 'details' , traceback . format_exc ( ) ) \n                print ( error [ 'details' ] ) \n        self . reply ( ** kwargs ) \n        if msg_id and msg == 'method' : \n            self . reply ( 'updated' , methods = [ msg_id ] ) "}
{"11499": "\ndef recv_connect ( self , version = None , support = None , session = None ) : \n    del session \n    if self . connection is not None : \n        raise MeteorError ( 400 , 'Session already established.' , self . connection . connection_id , ) \n    else : \n        if None in ( version , support ) or version not in self . versions : \n            self . reply ( 'failed' , version = self . versions [ 0 ] ) \n        else : \n            if version not in support : \n                raise MeteorError ( 400 , 'Client version/support mismatch.' ) \n            else : \n                from dddp . models import Connection \n                cur = connection . cursor ( ) \n                cur . execute ( 'SELECT pg_backend_pid()' ) \n                ( backend_pid , ) = cur . fetchone ( ) \n                this . version = version \n                this . support = support \n                self . connection = Connection . objects . create ( server_addr = '%d:%s' % ( backend_pid , self . ws . handler . socket . getsockname ( ) , ) , remote_addr = self . remote_addr , version = version , ) \n                self . pgworker . connections [ self . connection . pk ] = self \n                atexit . register ( self . on_close , 'Shutting down.' ) \n                self . reply ( 'connected' , session = self . connection . connection_id ) "}
{"11511": "\ndef poll ( self , conn ) : \n    while 1 : \n        state = conn . poll ( ) \n        if state == psycopg2 . extensions . POLL_OK : \n            while conn . notifies : \n                notify = conn . notifies . pop ( ) \n                self . logger . info ( \"Got NOTIFY (pid=%d, payload=%r)\" , notify . pid , notify . payload , ) \n                hdr , chunk = notify . payload . split ( '|' , 1 ) \n                header = ejson . loads ( hdr ) \n                uuid = header [ 'uuid' ] \n                size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) \n                if header [ 'fin' ] : \n                    size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] \n                chunks [ header [ 'seq' ] ] = chunk \n                if len ( chunks ) != size : \n                    continue \n                data = '' . join ( chunk for _ , chunk in sorted ( chunks . items ( ) ) ) \n                del self . chunks [ uuid ] \n                data = ejson . loads ( data ) \n                sender = data . pop ( '_sender' , None ) \n                tx_id = data . pop ( '_tx_id' , None ) \n                for connection_id in data . pop ( '_connection_ids' ) : \n                    try : \n                        websocket = self . connections [ connection_id ] \n                    except KeyError : \n                        continue \n                    if connection_id == sender : \n                        websocket . send ( data , tx_id = tx_id ) \n                    else : \n                        websocket . send ( data ) \n            break \n        else : \n            if state == psycopg2 . extensions . POLL_WRITE : \n                gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) \n            else : \n                if state == psycopg2 . extensions . POLL_READ : \n                    gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) \n                else : \n                    self . logger . warn ( 'POLL_ERR: %s' , state ) "}
{"11527": "\ndef send_json ( self , ids = None ) : \n    items = ids or self . _registration_id \n    values = { \"registration_ids\" : items } \n    if self . _data is not None : \n        values [ \"data\" ] = self . _data \n    for key , val in self . _kwargs . items ( ) : \n        if val : \n            values [ key ] = val \n    data = json . dumps ( values , separators = ( \",\" , \":\" ) , sort_keys = True ) . encode ( self . encoding ) \n    result = json . loads ( self . _send ( data , \"application/json\" ) ) \n    if ( \"failure\" in result ) and ( result [ \"failure\" ] ) : \n        unregistered = [ ] \n        throw_error = False \n        for index , error in enumerate ( result . get ( \"results\" , [ ] ) ) : \n            error = error . get ( \"error\" , \"\" ) \n            if error in ( \"NotRegistered\" , \"InvalidRegistration\" ) : \n                unregistered . append ( items [ index ] ) \n            else : \n                if error != \"\" : \n                    throw_error = True \n        self . deactivate_unregistered_devices ( unregistered ) \n        if throw_error : \n            raise GCMPushError ( result ) \n    return result "}
{"11539": "\ndef translate ( self , term = None , phrase = None , strict = False , rating = None ) : \n    assert any ( ( term , phrase ) ) , 'You must supply a term or phrase to search' \n    if phrase : \n        phrase = phrase . replace ( ' ' , '-' ) \n    params = { 's' : ( term or phrase ) } \n    if rating : \n        params . update ( { 'rating' : rating } ) \n    resp = self . _fetch ( 'translate' , ** params ) \n    if resp [ 'data' ] : \n        return GiphyImage ( resp [ 'data' ] ) \n    else : \n        if strict or self . strict : \n            raise GiphyApiException ( \"Term/Phrase '%s' could not be translated into a GIF\" % ( term or phrase ) ) "}
{"11541": "\ndef gif ( self , gif_id , strict = False ) : \n    resp = self . _fetch ( gif_id ) \n    if resp [ 'data' ] : \n        return GiphyImage ( resp [ 'data' ] ) \n    else : \n        if strict or self . strict : \n            raise GiphyApiException ( \"GIF with ID '%s' could not be found\" % gif_id ) "}
{"11543": "\ndef _access_control ( self , access_control , my_media_group = None ) : \n    extension = None \n    if access_control is AccessControl . Private : \n        if my_media_group : \n            my_media_group . private = gdata . media . Private ( ) \n    else : \n        if access_control is AccessControl . Unlisted : \n            from gdata . media import YOUTUBE_NAMESPACE \n            from atom import ExtensionElement \n            kwargs = { \"namespace\" : YOUTUBE_NAMESPACE , \"attributes\" : { 'action' : 'list' , 'permission' : 'denied' } , } \n            extension = ( [ ExtensionElement ( 'accessControl' , ** kwargs ) ] ) \n    return extension "}
{"11576": "\ndef dispose ( json_str ) : \n    result_str = list ( json_str ) \n    escaped = False \n    normal = True \n    sl_comment = False \n    ml_comment = False \n    quoted = False \n    a_step_from_comment = False \n    a_step_from_comment_away = False \n    former_index = None \n    for index , char in enumerate ( json_str ) : \n        if escaped : \n            escaped = False \n            continue \n        if a_step_from_comment : \n            if char != '/' and char != '*' : \n                a_step_from_comment = False \n                normal = True \n                continue \n        if a_step_from_comment_away : \n            if char != '/' : \n                a_step_from_comment_away = False \n        if char == '\"' : \n            if normal and not escaped : \n                quoted = True \n                normal = False \n            else : \n                if quoted and not escaped : \n                    quoted = False \n                    normal = True \n        else : \n            if char == '\\\\' : \n                if normal or quoted : \n                    escaped = True \n            else : \n                if char == '/' : \n                    if a_step_from_comment : \n                        a_step_from_comment = False \n                        sl_comment = True \n                        normal = False \n                        former_index = index - 1 \n                    else : \n                        if a_step_from_comment_away : \n                            a_step_from_comment_away = False \n                            normal = True \n                            ml_comment = False \n                            for i in range ( former_index , index + 1 ) : \n                                result_str [ i ] = \"\" \n                        else : \n                            if normal : \n                                a_step_from_comment = True \n                                normal = False \n                else : \n                    if char == '*' : \n                        if a_step_from_comment : \n                            a_step_from_comment = False \n                            ml_comment = True \n                            normal = False \n                            former_index = index - 1 \n                        else : \n                            if ml_comment : \n                                a_step_from_comment_away = True \n                    else : \n                        if char == '\\n' : \n                            if sl_comment : \n                                sl_comment = False \n                                normal = True \n                                for i in range ( former_index , index + 1 ) : \n                                    result_str [ i ] = \"\" \n                        else : \n                            if char == ']' or char == '}' : \n                                if normal : \n                                    _remove_last_comma ( result_str , index ) \n    return ( \"\" if isinstance ( json_str , str ) else u\"\" ) . join ( result_str ) "}
{"11599": "\ndef recursive_unicode ( obj ) : \n    if isinstance ( obj , dict ) : \n        return dict ( ( recursive_unicode ( k ) , recursive_unicode ( v ) ) for ( k , v ) in obj . iteritems ( ) ) \n    else : \n        if isinstance ( obj , list ) : \n            return list ( recursive_unicode ( i ) for i in obj ) \n        else : \n            if isinstance ( obj , tuple ) : \n                return tuple ( recursive_unicode ( i ) for i in obj ) \n            else : \n                if isinstance ( obj , bytes ) : \n                    return to_unicode ( obj ) \n                else : \n                    return obj "}
{"11602": "\ndef selectPolicy ( self , origin , request_method = None ) : \n    ret_origin = None \n    policyname = None \n    if self . matchstrategy in ( \"firstmatch\" , \"verbmatch\" ) : \n        for pol in self . activepolicies : \n            policy = self . policies [ pol ] \n            ret_origin = None \n            policyname = policy . name \n            if policyname == \"deny\" : \n                break \n            if self . matchstrategy == \"verbmatch\" : \n                if policy . methods != \"*\" and not CORS . matchlist ( request_method , policy . methods , case_sensitive = True ) : \n                    continue \n            if origin and policy . match : \n                if CORS . matchlist ( origin , policy . match ) : \n                    ret_origin = origin \n            else : \n                if policy . origin == \"copy\" : \n                    ret_origin = origin \n                else : \n                    if policy . origin : \n                        ret_origin = policy . origin \n            if ret_origin : \n                break \n    return policyname , ret_origin "}
{"11606": "\ndef determine_molecule_numbers ( total , molecules , absolute , relative ) : \n    weight = sum ( relative ) \n    if not any ( absolute ) : \n        numbers = [ int ( total * i / weight ) for i in relative ] \n    else : \n        if any ( relative ) : \n            rest = total - sum ( absolute ) \n            numbers = [ int ( rest * i / weight ) if i else j for i , j in zip ( relative , absolute ) ] \n        else : \n            numbers = absolute \n    return list ( zip ( molecules , numbers ) ) "}
{"11607": "\ndef resize_pbc_for_lipids ( pbc , relL , relU , absL , absU , uparea , area , hole , proteins ) : \n    if any ( relL ) and any ( relU ) : \n        if 0 in ( pbc . x , pbc . y , pbc . z ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n    else : \n        if any ( absL ) or any ( absU ) : \n            if pbc . z == 0 : \n                raise PBCException ( 'Not enough information to set the box size.' ) \n            if 0 in ( pbc . x , pbc . y ) : \n                pbc . x = pbc . y = 1 \n            upsize = sum ( absU ) * uparea \n            losize = sum ( absL ) * area \n            holesize = np . pi * hole ** 2 \n            xysize = pbc . x * pbc . y \n            psize_up = sum ( [ p . areaxy ( 0 , 2.4 ) for p in proteins ] ) \n            psize_lo = sum ( [ p . areaxy ( - 2.4 , 0 ) for p in proteins ] ) \n            unavail_up = holesize + psize_up \n            unavail_lo = holesize + psize_lo \n            upscale = ( upsize + unavail_up ) / xysize \n            loscale = ( losize + unavail_lo ) / xysize \n            area_scale = max ( upscale , loscale ) \n            aspect_ratio = pbc . x / pbc . y \n            scale_x = np . sqrt ( area_scale / aspect_ratio ) \n            scale_y = np . sqrt ( area_scale / aspect_ratio ) \n            pbc . box [ : 2 , : ] *= math . sqrt ( area_scale ) "}
{"11628": "\ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    if encoding == \"json\" : \n        resp = requests . post ( self . url , json = self . message ) \n    else : \n        if encoding == \"url\" : \n            resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and resp . history [ 0 ] . status_code >= 300 : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        else : \n            if \"invalid_auth\" in resp . text : \n                raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        print ( timestamp ( ) , type ( self ) . __name__ , \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status_code , ) \n    print ( \"Message sent.\" ) "}
{"11634": "\ndef validate_twilio ( attr , value ) : \n    if attr in ( \"from_\" , \"to\" ) : \n        check_valid ( \"Twilio\" , attr , value , validus . isphone , \"phone number\" ) \n    else : \n        if attr in ( \"attachments\" ) : \n            check_valid ( \"Twilio\" , attr , value , validus . isurl , \"url\" ) "}
{"11635": "\ndef validate_slackpost ( attr , value ) : \n    if attr in ( \"channel\" , \"credentials\" ) : \n        if not isinstance ( value , str ) : \n            raise InvalidMessageInputError ( \"SlackPost\" , attr , value , \"string\" ) \n    else : \n        if attr in ( \"attachments\" ) : \n            check_valid ( \"SlackPost\" , attr , value , validus . isurl , \"url\" ) "}
{"11636": "\ndef validate_whatsapp ( attr , value ) : \n    if attr in ( \"from_\" , \"to\" ) : \n        if value is not None and \"whatsapp:\" in value : \n            value = value . split ( \"whatsapp:+\" ) [ - 1 ] \n        check_valid ( \"WhatsApp\" , attr , value , validus . isint , \"phone number starting with the '+' symbol\" , ) \n    else : \n        if attr in ( \"attachments\" ) : \n            check_valid ( \"WhatsApp\" , attr , value , validus . isurl , \"url\" ) "}
{"11643": "\ndef _send_content ( self , method = \"/sendMessage\" ) : \n    url = self . base_url + method \n    try : \n        resp = requests . post ( url , json = self . message ) \n        resp . raise_for_status ( ) \n    except requests . exceptions . HTTPError as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        if method == \"/sendMessage\" : \n            content_type = \"Message body\" \n        else : \n            if method == \"/sendDocument\" : \n                content_type = \"Attachment: \" + self . message [ \"document\" ] \n        print ( timestamp ( ) , content_type , \"sent.\" ) "}
{"11650": "\ndef _get_session ( self ) : \n    if self . port in ( 465 , \"465\" ) : \n        session = self . _get_ssl ( ) \n    else : \n        if self . port in ( 587 , \"587\" ) : \n            session = self . _get_tls ( ) \n    try : \n        session . login ( self . from_ , self . _auth ) \n    except SMTPResponseException as e : \n        raise MessageSendError ( e . smtp_error . decode ( \"unicode_escape\" ) ) \n    return session "}
{"11666": "\ndef parse_file ( self , filename ) : \n    self . reset ( ) \n    self . filename = filename \n    fileinput . close ( ) \n    self . format = None \n    self . lineno = 0 \n    self . lines = [ ] \n    for line in fileinput . input ( filename ) : \n        if line [ - 1 ] == '\\012' : \n            line = line [ 0 : - 1 ] \n        if self . format == None : \n            self . process_normal_line ( line ) \n        else : \n            if self . format . end . match ( line ) : \n                self . lines . append ( line ) \n                self . add_block_lines ( ) \n            else : \n                if self . format . column . match ( line ) : \n                    self . lines . append ( line ) \n                else : \n                    self . add_block_lines ( ) \n                    self . process_normal_line ( line ) \n    self . add_block_lines ( ) "}
{"11684": "\ndef send_request ( self , path , data , method ) : \n    headers = self . get_request_headers ( ) \n    if method == 'GET' : \n        return self . _send_get_request ( path , data , headers ) \n    else : \n        if method == 'POST' : \n            return self . _send_post_request ( path , data , headers ) \n        else : \n            if method == 'DELETE' : \n                return self . _send_delete_request ( path , headers ) \n            else : \n                raise Route53Error ( \"Invalid request method: %s\" % method ) "}
{"11688": "\ndef APEValue ( value , kind ) : \n    if kind in ( TEXT , EXTERNAL ) : \n        if not isinstance ( value , text_type ) : \n            if PY3 : \n                raise TypeError ( \"str only for text/external values\" ) \n        else : \n            value = value . encode ( \"utf-8\" ) \n    if kind == TEXT : \n        return APETextValue ( value , kind ) \n    else : \n        if kind == BINARY : \n            return APEBinaryValue ( value , kind ) \n        else : \n            if kind == EXTERNAL : \n                return APEExtValue ( value , kind ) \n            else : \n                raise ValueError ( \"kind must be TEXT, BINARY, or EXTERNAL\" ) "}
{"11702": "\ndef process_content ( self , content ) : \n    markup = None \n    markup_lines = [ ] \n    first = 1 \n    for line in content : \n        found = None \n        for t in re_markup_tags : \n            m = t . match ( line ) \n            if m : \n                found = string . lower ( m . group ( 1 ) ) \n                prefix = len ( m . group ( 0 ) ) \n                line = \" \" * prefix + line [ prefix : ] \n                break \n        if found : \n            first = 0 \n            self . add_markup ( ) \n            self . markup = found \n            if len ( string . strip ( line ) ) > 0 : \n                self . markup_lines . append ( line ) \n        else : \n            if first == 0 : \n                self . markup_lines . append ( line ) \n    self . add_markup ( ) \n    return self . markups "}
{"11708": "\ndef utf8 ( data ) : \n    if isinstance ( data , bytes ) : \n        return data . decode ( \"utf-8\" , \"replace\" ) . encode ( \"utf-8\" ) \n    else : \n        if isinstance ( data , text_type ) : \n            return data . encode ( \"utf-8\" ) \n        else : \n            raise TypeError ( \"only unicode/bytes types can be converted to UTF-8\" ) "}
{"11716": "\ndef MakeID3v1 ( id3 ) : \n    v1 = { } \n    for v2id , name in { \"TIT2\" : \"title\" , \"TPE1\" : \"artist\" , \"TALB\" : \"album\" } . items ( ) : \n        if v2id in id3 : \n            text = id3 [ v2id ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 30 ] \n        else : \n            text = b'' \n        v1 [ name ] = text + ( b'\\x00' * ( 30 - len ( text ) ) ) \n    if \"COMM\" in id3 : \n        cmnt = id3 [ \"COMM\" ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 28 ] \n    else : \n        cmnt = b'' \n    v1 [ 'comment' ] = cmnt + ( b'\\x00' * ( 29 - len ( cmnt ) ) ) \n    if \"TRCK\" in id3 : \n        try : \n            v1 [ \"track\" ] = chr_ ( + id3 [ \"TRCK\" ] ) \n        except ValueError : \n            v1 [ \"track\" ] = b'\\x00' \n    else : \n        v1 [ \"track\" ] = b'\\x00' \n    if \"TCON\" in id3 : \n        try : \n            genre = id3 [ \"TCON\" ] . genres [ 0 ] \n        except IndexError : \n            pass \n        else : \n            if genre in TCON . GENRES : \n                v1 [ \"genre\" ] = chr_ ( TCON . GENRES . index ( genre ) ) \n    if \"genre\" not in v1 : \n        v1 [ \"genre\" ] = b\"\\xff\" \n    if \"TDRC\" in id3 : \n        year = text_type ( id3 [ \"TDRC\" ] ) . encode ( 'latin1' , 'replace' ) \n    else : \n        if \"TYER\" in id3 : \n            year = text_type ( id3 [ \"TYER\" ] ) . encode ( 'latin1' , 'replace' ) \n        else : \n            year = b'' \n    v1 [ 'year' ] = ( year + b'\\x00\\x00\\x00\\x00' ) [ : 4 ] \n    return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] ) "}
{"11732": "\ndef parse_hosted_zone ( e_zone , connection ) : \n    kwargs = { } \n    for e_field in e_zone : \n        tag_name = e_field . tag . split ( '}' ) [ 1 ] \n        field_text = e_field . text \n        if tag_name == 'Config' : \n            e_comment = e_field . find ( './{*}Comment' ) \n            kwargs [ 'comment' ] = e_comment . text if e_comment is not None else None \n            continue \n        else : \n            if tag_name == 'Id' : \n                field_text = field_text . strip ( '/hostedzone/' ) \n        kw_name = HOSTED_ZONE_TAG_TO_KWARG_MAP [ tag_name ] \n        kwargs [ kw_name ] = field_text \n    return HostedZone ( connection , ** kwargs ) "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = False ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and header > 4 : \n            available += header - 4 \n            header = 4 \n        if len ( data ) > available : \n            padding = self . metadata_blocks [ - 1 ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if newlength > 0 : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        else : \n            if len ( data ) < available : \n                self . metadata_blocks [ - 1 ] . length += ( available - len ( data ) ) \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        if len ( data ) != available : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128 , 2 ) \n            except IOError : \n                pass \n            else : \n                if f . read ( 3 ) == b\"TAG\" : \n                    f . seek ( - 128 , 2 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11740": "\ndef parse_rrset ( e_rrset , connection , zone_id ) : \n    kwargs = { 'connection' : connection , 'zone_id' : zone_id , } \n    rrset_type = None \n    for e_field in e_rrset : \n        tag_name = e_field . tag . split ( '}' ) [ 1 ] \n        field_text = e_field . text \n        if tag_name == 'Type' : \n            rrset_type = field_text \n            continue \n        else : \n            if tag_name == 'AliasTarget' : \n                alias_hosted_zone_id , alias_dns_name = parse_rrset_alias ( e_field ) \n                kwargs [ 'alias_hosted_zone_id' ] = alias_hosted_zone_id \n                kwargs [ 'alias_dns_name' ] = alias_dns_name \n                kwargs [ 'ttl' ] = None \n                continue \n            else : \n                if tag_name == 'ResourceRecords' : \n                    kwargs [ 'records' ] = parse_rrset_record_values ( e_field ) \n                    continue \n        kw_name = RRSET_TAG_TO_KWARG_MAP [ tag_name ] \n        kwargs [ kw_name ] = field_text \n    if not rrset_type : \n        raise Route53Error ( \"No Type tag found in ListResourceRecordSetsResponse.\" ) \n    if 'records' not in kwargs : \n        kwargs [ 'records' ] = [ ] \n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP [ rrset_type ] \n    return RRSetSubclass ( ** kwargs ) "}
{"11771": "\ndef handle ( self , * args , ** kwargs ) : \n    frequency = kwargs [ 'frequency' ] \n    frequencies = settings . STATISTIC_FREQUENCY_ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) \n    if kwargs [ 'list' ] : \n        maintenance . list_statistics ( ) \n    else : \n        if kwargs [ 'calculate' ] : \n            maintenance . calculate_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'calculate' ] ) , frequencies ) \n        else : \n            if kwargs [ 'reset' ] : \n                maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset_cumulative' ] ) \n            else : \n                if kwargs [ 'recalculate' ] : \n                    maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset_cumulative' ] , True ) "}
{"11790": "\ndef winner ( self ) : \n    hmScore = self . home_score ( ) \n    awScore = self . away_score ( ) \n    if hmScore > awScore : \n        return self . home ( ) \n    else : \n        if hmScore < awScore : \n            return self . away ( ) \n        else : \n            return None "}
{"11837": "\ndef flatten_links ( td , _recurse = False ) : \n    def _flatten_node ( c ) : \n        if isinstance ( c , basestring ) : \n            return c . strip ( ) \n        else : \n            if 'href' in c . attrib : \n                c_id = rel_url_to_id ( c . attrib [ 'href' ] ) \n                return c_id if c_id else c . text_content ( ) . strip ( ) \n            else : \n                return flatten_links ( pq ( c ) , _recurse = True ) \n    if td is None or not td . text ( ) : \n        return '' if _recurse else None \n    td . remove ( 'span.note' ) \n    return '' . join ( _flatten_node ( c ) for c in td . contents ( ) ) "}
{"11839": "\ndef _kwargs_to_qs ( ** kwargs ) : \n    inpOptDef = inputs_options_defaults ( ) \n    opts = { name : dct [ 'value' ] for name , dct in inpOptDef . items ( ) } \n    for k , v in kwargs . items ( ) : \n        del kwargs [ k ] \n        if isinstance ( v , bool ) : \n            kwargs [ k ] = 'Y' if v else 'N' \n        else : \n            if k . lower ( ) in ( 'tm' , 'team' ) : \n                kwargs [ 'team_id' ] = v \n            else : \n                if k . lower ( ) in ( 'yr' , 'year' , 'yrs' , 'years' ) : \n                    if isinstance ( v , collections . Iterable ) : \n                        lst = list ( v ) \n                        kwargs [ 'year_min' ] = min ( lst ) \n                        kwargs [ 'year_max' ] = max ( lst ) \n                    else : \n                        if isinstance ( v , basestring ) : \n                            v = list ( map ( int , v . split ( ',' ) ) ) \n                            kwargs [ 'year_min' ] = min ( v ) \n                            kwargs [ 'year_max' ] = max ( v ) \n                        else : \n                            kwargs [ 'year_min' ] = v \n                            kwargs [ 'year_max' ] = v \n                else : \n                    if k . lower ( ) in ( 'pos' , 'position' , 'positions' ) : \n                        if isinstance ( v , basestring ) : \n                            v = v . split ( ',' ) \n                        else : \n                            if not isinstance ( v , collections . Iterable ) : \n                                v = [ v ] \n                        kwargs [ 'pos[]' ] = v \n                    else : \n                        if k . lower ( ) in ( 'draft_pos' , 'draftpos' , 'draftposition' , 'draftpositions' , 'draft_position' , 'draft_positions' ) : \n                            if isinstance ( v , basestring ) : \n                                v = v . split ( ',' ) \n                            else : \n                                if not isinstance ( v , collections . Iterable ) : \n                                    v = [ v ] \n                            kwargs [ 'draft_pos[]' ] = v \n                        else : \n                            kwargs [ k ] = v \n    for k , v in kwargs . items ( ) : \n        if k in opts or k in ( 'pos[]' , 'draft_pos[]' ) : \n            if isinstance ( v , basestring ) : \n                v = v . split ( ',' ) \n            else : \n                if not isinstance ( v , collections . Iterable ) : \n                    v = [ v ] \n            opts [ k ] = v \n        if 'draft' in k : \n            opts [ 'draft' ] = [ 1 ] \n    opts [ 'request' ] = [ 1 ] \n    opts [ 'offset' ] = [ kwargs . get ( 'offset' , 0 ) ] \n    qs = '&' . join ( '{}={}' . format ( urllib . parse . quote_plus ( name ) , val ) for name , vals in sorted ( opts . items ( ) ) for val in vals ) \n    return qs "}
{"11859": "\ndef until_condition ( self , condition , condition_description ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    while True : \n        try : \n            if not hasattr ( condition , '__call__' ) : \n                raise TypeError ( \"condition is not callable\" ) \n            value = condition ( ) \n            if type ( value ) is bool and value is not False : \n                return value \n            else : \n                if type ( value ) is not bool and value is not None : \n                    return value \n                else : \n                    logger . debug ( \"#\" + str ( count ) + \" - wait until \" + condition_description ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0} : {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n        time . sleep ( self . _poll ) \n        count += 1 \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"condition <\" + condition_description + \"> was not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11875": "\ndef check_if_song_name ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    a_list = soup . findAll ( 'a' , 'touch' ) \n    text = [ str ( x ) for x in a_list ] \n    text = '' . join ( text ) \n    text = text . lower ( ) \n    string1 = 'download in 48 kbps' \n    string2 = 'download in 128 kbps' \n    string3 = 'download in 320 kbps' \n    href = '' \n    if string3 in text : \n        href = a_list [ 2 ] . get ( 'href' ) \n    else : \n        if string2 in text : \n            href = a_list [ 1 ] . get ( 'href' ) \n        else : \n            if string1 in text : \n                href = a_list [ 0 ] . get ( 'href' ) \n            else : \n                return ( True , 'nothing' ) \n    return ( False , href ) "}
{"11904": "\ndef simple_error_handler ( exc , * args ) : \n    if isinstance ( exc , exceptions . APIException ) : \n        headers = { } \n        if getattr ( exc , 'auth_header' , None ) : \n            headers [ 'WWW-Authenticate' ] = exc . auth_header \n        if getattr ( exc , 'wait' , None ) : \n            headers [ 'X-Throttle-Wait-Seconds' ] = '%d' % exc . wait \n        return Response ( { 'error' : exc . detail } , status = exc . status_code , headers = headers ) \n    else : \n        if isinstance ( exc , Http404 ) : \n            return Response ( { 'error' : 'Not found' } , status = status . HTTP_404_NOT_FOUND ) \n        else : \n            if isinstance ( exc , PermissionDenied ) : \n                return Response ( { 'error' : 'Permission denied' } , status = status . HTTP_403_FORBIDDEN ) \n    return None "}
{"11924": "\ndef metadata_updated_on ( item ) : \n    if 'end' in item : \n        updated = item [ 'end' ] \n    else : \n        if 'date_joined_program' in item : \n            updated = item [ 'date_joined_program' ] \n        else : \n            if 'report_date' in item : \n                updated = item [ 'report_date' ] \n            else : \n                raise ValueError ( \"Can't find updated field for item \" + str ( item ) ) \n    return float ( str_to_datetime ( updated ) . timestamp ( ) ) "}
{"11925": "\ndef metadata_category ( item ) : \n    if 'estimated_attendance' in item : \n        category = CATEGORY_EVENT \n    else : \n        if 'activity' in item : \n            category = CATEGORY_ACTIVITY \n        else : \n            if 'first_name' in item : \n                category = CATEGORY_USER \n            else : \n                raise TypeError ( \"Could not define the category of item \" + str ( item ) ) \n    return category "}
{"11926": "\ndef get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : \n    more = True \n    next_uri = None \n    page = ReMoClient . FIRST_PAGE \n    page += int ( offset / ReMoClient . ITEMS_PER_PAGE ) \n    if category == CATEGORY_EVENT : \n        api = self . api_events_url \n    else : \n        if category == CATEGORY_ACTIVITY : \n            api = self . api_activities_url \n        else : \n            if category == CATEGORY_USER : \n                api = self . api_users_url \n            else : \n                raise ValueError ( category + ' not supported in ReMo' ) \n    while more : \n        params = { \"page\" : page , \"orderby\" : \"ASC\" } \n        logger . debug ( \"ReMo client calls APIv2: %s params: %s\" , api , str ( params ) ) \n        raw_items = self . fetch ( api , payload = params ) \n        yield raw_items \n        items_data = json . loads ( raw_items ) \n        next_uri = items_data [ 'next' ] \n        if not next_uri : \n            more = False \n        else : \n            parsed_uri = urllib . parse . urlparse ( next_uri ) \n            parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) \n            page = parsed_params [ 'page' ] [ 0 ] "}
{"11943": "\ndef export ( pid , record , template = None , ** kwargs ) : \n    formats = current_app . config . get ( 'RECORDS_UI_EXPORT_FORMATS' , { } ) . get ( pid . pid_type ) \n    fmt = formats . get ( request . view_args . get ( 'format' ) ) \n    if fmt is False : \n        abort ( 410 ) \n    else : \n        if fmt is None : \n            abort ( 404 ) \n        else : \n            serializer = obj_or_import_string ( fmt [ 'serializer' ] ) \n            data = serializer . serialize ( pid , record ) \n            if isinstance ( data , six . binary_type ) : \n                data = data . decode ( 'utf8' ) \n            return render_template ( template , pid = pid , record = record , data = data , format_title = fmt [ 'title' ] , ) "}
{"11949": "\ndef timing_since ( self , name , start_time , rate = 1 ) : \n    duration = 0 \n    if isinstance ( start_time , datetime ) : \n        duration = ( datetime . now ( start_time . tzinfo ) - start_time ) . total_seconds ( ) * 1000 \n    else : \n        if is_numeric ( start_time ) : \n            assert start_time > 0 \n            duration = ( time ( ) - start_time ) * 1000 \n        else : \n            raise ValueError ( \"start time should be a timestamp or a datetime\" ) \n    self . timing ( name , duration , rate ) "}
{"11962": "\ndef interpretAsOpenMath ( x ) : \n    if hasattr ( x , \"_ishelper\" ) and x . _ishelper : \n        return x . _toOM ( ) \n    else : \n        if isinstance ( x , om . OMAny ) : \n            return x \n        else : \n            if isinstance ( x , six . integer_types ) : \n                return om . OMInteger ( x ) \n            else : \n                if isinstance ( x , float ) : \n                    return om . OMFloat ( x ) \n                else : \n                    if isinstance ( x , six . string_types ) : \n                        return om . OMString ( x ) \n                    else : \n                        if isinstance ( x , WrappedHelper ) : \n                            return x . toOM ( ) \n                        else : \n                            if inspect . isfunction ( x ) : \n                                paramMap = inspect . signature ( x ) . parameters \n                                params = [ v for k , v in six . iteritems ( paramMap ) ] \n                                posArgKinds = [ inspect . Parameter . POSITIONAL_ONLY , inspect . Parameter . POSITIONAL_OR_KEYWORD ] \n                                if not all ( [ p . kind in posArgKinds for p in params ] ) : \n                                    raise CannotInterpretAsOpenMath ( \"no sequence arguments allowed\" ) \n                                paramsOM = [ om . OMVariable ( name = p . name ) for p in params ] \n                                bodyOM = interpretAsOpenMath ( x ( * paramsOM ) ) \n                                return OMBinding ( om . OMSymbol ( name = \"lambda\" , cd = \"python\" , cdbase = \"http://python.org\" ) , paramsOM , bodyOM ) \n                            else : \n                                raise CannotInterpretAsOpenMath ( \"unknown kind of object: \" + str ( x ) ) "}
{"11964": "\ndef to_python ( self , omobj ) : \n    if omobj . __class__ in self . _omclass_to_py : \n        return self . _omclass_to_py [ omobj . __class__ ] ( omobj ) \n    else : \n        if isinstance ( omobj , om . OMSymbol ) : \n            return self . _lookup_to_python ( omobj . cdbase , omobj . cd , omobj . name ) \n        else : \n            if isinstance ( omobj , om . OMApplication ) : \n                elem = self . to_python ( omobj . elem ) \n                arguments = [ self . to_python ( x ) for x in omobj . arguments ] \n                return elem ( * arguments ) \n    raise ValueError ( 'Cannot convert object of class %s to Python.' % omobj . __class__ . __name__ ) "}
{"11994": "\ndef decode ( data ) : \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    pos = 0 \n    while pos < len ( data ) : \n        header_byte = data [ pos ] \n        if header_byte > 127 : \n            header_byte -= 256 \n        pos += 1 \n        if 0 <= header_byte <= 127 : \n            result . extend ( data [ pos : pos + header_byte + 1 ] ) \n            pos += header_byte + 1 \n        else : \n            if header_byte == - 128 : \n                pass \n            else : \n                result . extend ( [ data [ pos ] ] * ( 1 - header_byte ) ) \n                pos += 1 \n    return bytes ( result ) "}
{"11995": "\ndef encode ( data ) : \n    if len ( data ) == 0 : \n        return data \n    if len ( data ) == 1 : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = 0 \n    repeat_count = 0 \n    MAX_LENGTH = 127 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if len ( buf ) == 0 : \n            return \n        result . append ( len ( buf ) - 1 ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256 - ( repeat_count - 1 ) ) \n        result . append ( data [ pos ] ) \n    while pos < len ( data ) - 1 : \n        current_byte = data [ pos ] \n        if data [ pos ] == data [ pos + 1 ] : \n            if state == 'RAW' : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = 1 \n            else : \n                if state == 'RLE' : \n                    if repeat_count == MAX_LENGTH : \n                        finish_rle ( ) \n                        repeat_count = 0 \n                    repeat_count += 1 \n        else : \n            if state == 'RLE' : \n                repeat_count += 1 \n                finish_rle ( ) \n                state = 'RAW' \n                repeat_count = 0 \n            else : \n                if state == 'RAW' : \n                    if len ( buf ) == MAX_LENGTH : \n                        finish_raw ( ) \n                    buf . append ( current_byte ) \n        pos += 1 \n    if state == 'RAW' : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count += 1 \n        finish_rle ( ) \n    return bytes ( result ) "}
{"12011": "\ndef execute ( self , args ) : \n    if args . name is not None : \n        self . print_workspace ( args . name ) \n    else : \n        if args . all is not None : \n            self . print_all ( ) "}
{"12037": "\ndef execute ( self , args ) : \n    if args . name is not None : \n        self . show_workspace ( slashes2dash ( args . name ) ) \n    else : \n        if args . all is not None : \n            self . show_all ( ) "}
{"12043": "\ndef convert ( in_file , out_file , in_fmt = \"\" , out_fmt = \"\" ) : \n    in_file = os . path . expanduser ( in_file ) \n    out_file = os . path . expanduser ( out_file ) \n    if not os . path . exists ( in_file ) : \n        raise IOError ( \"Input file {0} does not exist, stopping...\" . format ( in_file ) ) \n    in_fmt = in_fmt . lower ( ) or _guess_format_from_extension ( in_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    out_fmt = out_fmt . lower ( ) or _guess_format_from_extension ( out_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    if not in_fmt or not out_fmt : \n        raise ValueError ( \"Cannot determine conversion formats.\" ) \n        return False \n    if in_fmt is out_fmt : \n        shutil . copyfileobj ( in_file , out_file ) \n        return out_file \n    if in_fmt == 'hdf5' : \n        from . import hdf5 \n        data = hdf5 . load ( in_file ) \n    else : \n        if in_fmt == 'tiff' : \n            from . import tiff \n            data = tiff . load ( in_file ) \n        else : \n            if in_fmt == 'png' : \n                from . import png \n                data = png . load ( in_file ) \n            else : \n                return _fail_pair_conversion ( in_fmt , out_fmt ) \n    if out_fmt == 'hdf5' : \n        from . import hdf5 \n        return hdf5 . save ( out_file , data ) \n    else : \n        if out_fmt == 'tiff' : \n            from . import tiff \n            return tiff . save ( out_file , data ) \n        else : \n            if out_fmt == 'png' : \n                from . import png \n                return png . export_png ( out_file , data ) \n            else : \n                return _fail_pair_conversion ( in_fmt , out_fmt ) \n    return _fail_pair_conversion ( in_fmt , out_fmt ) "}
{"12048": "\ndef RAMON ( typ ) : \n    if six . PY2 : \n        lookup = [ str , unicode ] \n    else : \n        if six . PY3 : \n            lookup = [ str ] \n    if type ( typ ) is int : \n        return _ramon_types [ typ ] \n    else : \n        if type ( typ ) in lookup : \n            return _ramon_types [ _types [ typ ] ] "}
{"12055": "\ndef identify_imagesize ( self , image_type , image_path = '/tmp/img.' ) : \n    dims = ( ) \n    try : \n        if ( image_type . lower ( ) == 'png' ) : \n            dims = np . shape ( ndpng . load ( '{}{}' . format ( image_path , image_type ) ) ) \n        else : \n            if ( image_type . lower ( ) == 'tif' or image_type . lower ( ) == 'tiff' ) : \n                dims = np . shape ( ndtiff . load ( '{}{}' . format ( image_path , image_type ) ) ) \n            else : \n                raise ValueError ( \"Unsupported image type.\" ) \n    except : \n        raise OSError ( 'The file was not accessible at {}{}' . format ( image_path , image_type ) ) \n    return dims [ : : - 1 ] "}
{"12078": "\ndef human_bytes ( value ) : \n    value = float ( value ) \n    if value >= 1073741824 : \n        gigabytes = value / 1073741824 \n        size = '%.2f GB' % gigabytes \n    else : \n        if value >= 1048576 : \n            megabytes = value / 1048576 \n            size = '%.2f MB' % megabytes \n        else : \n            if value >= 1024 : \n                kilobytes = value / 1024 \n                size = '%.2f KB' % kilobytes \n            else : \n                size = '%.2f B' % value \n    return size "}
{"12079": "\ndef find_config ( self , children ) : \n    named_config = None \n    found_config = None \n    if 'config' in children : \n        if type ( children [ 'config' ] ) == str : \n            children [ 'config' ] = ConfigFile ( children [ 'config' ] ) \n        else : \n            if isinstance ( children [ 'config' ] , Config ) : \n                children [ 'config' ] = children [ 'config' ] \n            else : \n                if type ( children [ 'config' ] ) == dict : \n                    children [ 'config' ] = Config ( data = children [ 'config' ] ) \n                else : \n                    raise TypeError ( \"Don't know how to turn {} into a Config\" . format ( type ( children [ 'config' ] ) ) ) \n        named_config = children [ 'config' ] \n    for k in children : \n        if isinstance ( children [ k ] , Config ) : \n            found_config = children [ k ] \n    for k in children : \n        if isinstance ( children [ k ] , Directory ) : \n            for j in children [ k ] . _children : \n                if j == 'config' and not named_config : \n                    named_config = children [ k ] . _children [ j ] \n                if isinstance ( children [ k ] . _children [ j ] , Config ) : \n                    found_config = children [ k ] . _children [ j ] \n    if named_config : \n        return named_config \n    else : \n        return found_config "}
{"12085": "\ndef configure ( self ) : \n    handler = logging . FileHandler ( self . path , delay = True ) \n    if self . _format : \n        handler . setFormatter ( logging . Formatter ( self . _format ) ) \n    if type ( self . _formatter ) == str : \n        if self . _env and self . _env . config . logging . dict_config . formatters [ self . _formatter ] : \n            d = self . _env . config . logging . dict_config . formatters [ self . _formatter ] . to_dict ( ) \n            handler . setFormatter ( logging . Formatter ( ** d ) ) \n    else : \n        if type ( self . _formatter ) == dict : \n            handler . setFormatter ( logging . Formatter ( ** self . _formatter ) ) \n    if len ( self . _loggers ) : \n        for name in self . _loggers : \n            logging . getLogger ( name ) . addHandler ( handler ) \n    else : \n        logging . getLogger ( ) . addHandler ( handler ) "}
{"12096": "\ndef add ( self , * args , ** kwargs ) : \n    for key in kwargs : \n        if isinstance ( kwargs [ key ] , str ) : \n            self . _children [ key ] = File ( kwargs [ key ] ) \n        else : \n            self . _children [ key ] = kwargs [ key ] \n        self . _children [ key ] . _parent = self \n        self . _children [ key ] . _env = self . _env \n    added = [ ] \n    for arg in args : \n        if isinstance ( arg , File ) : \n            self . _children [ arg . name ] = arg \n            self . _children [ arg . name ] . _parent = self \n            self . _children [ arg . name ] . _env = self . _env \n        else : \n            if isinstance ( arg , str ) : \n                f = File ( arg ) \n                added . append ( f ) \n                self . _children [ arg ] = f \n                self . _children [ arg ] . _parent = self \n                self . _children [ arg ] . _env = self . _env \n            else : \n                raise TypeError ( type ( arg ) ) \n    if len ( added ) == 1 : \n        return added [ 0 ] \n    if len ( args ) == 1 : \n        return args [ 0 ] "}
{"12103": "\ndef _resolve_path ( self , create = False ) : \n    if type ( self . _path ) == str : \n        key_path = self . _path . split ( '.' ) \n    else : \n        key_path = [ self . _path ] \n    node = self . _root . _data \n    nodes = [ self . _root . _data ] \n    while len ( key_path ) : \n        key = key_path . pop ( 0 ) \n        try : \n            key = int ( key ) \n        except : \n            pass \n        if create : \n            if type ( node ) == dict and key not in node : \n                node [ key ] = { } \n            else : \n                if type ( node ) == list and type ( key ) == int and len ( node ) < key : \n                    node . append ( [ None for i in range ( key - len ( node ) ) ] ) \n        nodes . append ( node ) \n        try : \n            node = node [ key ] \n        except TypeError : \n            if type ( key ) == int : \n                raise IndexError ( key ) \n            else : \n                raise KeyError ( key ) \n    return ( nodes [ - 1 ] , key ) "}
{"12108": "\ndef build_callback_url ( request , urlname , message ) : \n    location = reverse ( urlname , kwargs = { \"pk\" : message . pk } ) \n    callback_domain = getattr ( settings , \"TWILIO_CALLBACK_DOMAIN\" , None ) \n    if callback_domain : \n        url = \"{}://{}{}\" . format ( \"https\" if getattr ( settings , \"TWILIO_CALLBACK_USE_HTTPS\" , False ) else \"http\" , callback_domain , location ) \n    else : \n        if request is not None : \n            url = request . build_absolute_uri ( location ) \n        else : \n            raise ValueError ( \"Unable to build callback url. Configure TWILIO_CALLBACK_DOMAIN \" \"or pass request object to function call\" ) \n    return url "}
{"12113": "\ndef write_socket_output ( connection , socket_obj ) : \n    count = connection . has_output \n    if count <= 0 : \n        return count \n    data = connection . output_data ( ) \n    if not data : \n        return Connection . EOS \n    while True : \n        try : \n            count = socket_obj . send ( data ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if count > 0 : \n        connection . output_written ( count ) \n    else : \n        if data : \n            LOG . debug ( \"Socket closed\" ) \n            count = Connection . EOS \n            connection . close_output ( ) \n            connection . close_input ( ) \n    return count "}
{"12115": "\ndef _get_remote_settle_modes ( pn_link ) : \n    modes = { } \n    snd = pn_link . remote_snd_settle_mode \n    if snd == proton . Link . SND_UNSETTLED : \n        modes [ 'snd-settle-mode' ] = 'unsettled' \n    else : \n        if snd == proton . Link . SND_SETTLED : \n            modes [ 'snd-settle-mode' ] = 'settled' \n    if pn_link . remote_rcv_settle_mode == proton . Link . RCV_SECOND : \n        modes [ 'rcv-settle-mode' ] = 'second' \n    return modes "}
{"12116": "\ndef configure ( self , target_address , source_address , handler , properties ) : \n    self . _handler = handler \n    self . _properties = properties \n    dynamic_props = None \n    if properties : \n        dynamic_props = properties . get ( \"dynamic-node-properties\" ) \n        mode = _dist_modes . get ( properties . get ( \"distribution-mode\" ) ) \n        if mode is not None : \n            self . _pn_link . source . distribution_mode = mode \n        mode = _snd_settle_modes . get ( properties . get ( \"snd-settle-mode\" ) ) \n        if mode is not None : \n            self . _pn_link . snd_settle_mode = mode \n        mode = _rcv_settle_modes . get ( properties . get ( \"rcv-settle-mode\" ) ) \n        if mode is not None : \n            self . _pn_link . rcv_settle_mode = mode \n    if target_address is None : \n        if not self . _pn_link . is_sender : \n            raise Exception ( \"Dynamic target not allowed\" ) \n        self . _pn_link . target . dynamic = True \n        if dynamic_props : \n            self . _pn_link . target . properties . clear ( ) \n            self . _pn_link . target . properties . put_dict ( dynamic_props ) \n    else : \n        if target_address : \n            self . _pn_link . target . address = target_address \n    if source_address is None : \n        if not self . _pn_link . is_receiver : \n            raise Exception ( \"Dynamic source not allowed\" ) \n        self . _pn_link . source . dynamic = True \n        if dynamic_props : \n            self . _pn_link . source . properties . clear ( ) \n            self . _pn_link . source . properties . put_dict ( dynamic_props ) \n    else : \n        if source_address : \n            self . _pn_link . source . address = source_address "}
{"12119": "\ndef _session_closed ( self ) : \n    if self . _endpoint_state & proton . Endpoint . REMOTE_ACTIVE : \n        self . _process_remote_state ( ) \n    else : \n        if self . _endpoint_state & proton . Endpoint . REMOTE_UNINIT : \n            self . _failed = True \n            self . _link_failed ( \"Parent session closed.\" ) "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return 0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return 0 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( _PROTON_VERSION < ( 0 , 10 ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = True \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        else : \n            if self . _handle_proton_event ( pn_event ) : \n                pass \n            else : \n                if _SessionProxy . _handle_proton_event ( pn_event , self ) : \n                    pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    else : \n        if ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12144": "\ndef twilio_view ( f ) : \n    \n    @ csrf_exempt \n    @ wraps ( f ) \n    def decorator ( request , * args , ** kwargs ) : \n        if request . method != \"POST\" : \n            logger . error ( \"Twilio: Expected POST request\" , extra = { \"request\" : request } ) \n            return HttpResponseNotAllowed ( request . method ) \n        if not getattr ( settings , \"TWILIO_SKIP_SIGNATURE_VALIDATION\" ) : \n            try : \n                validator = RequestValidator ( settings . TWILIO_AUTH_TOKEN ) \n                url = request . build_absolute_uri ( ) \n                if \"HTTP_X_FORWARDED_SERVER\" in request . META : \n                    protocol = \"https\" if request . META [ \"HTTP_X_TWILIO_SSL\" ] == \"Enabled\" else \"http\" \n                    url = \"{0}://{1}{2}\" . format ( protocol , request . META [ \"HTTP_X_FORWARDED_SERVER\" ] , request . META [ \"REQUEST_URI\" ] ) \n                signature = request . META [ \"HTTP_X_TWILIO_SIGNATURE\" ] \n            except ( AttributeError , KeyError ) as e : \n                logger . exception ( \"Twilio: Missing META param\" , extra = { \"request\" : request } ) \n                return HttpResponseForbidden ( \"Missing META param: %s\" % e ) \n            if not validator . validate ( url , request . POST , signature ) : \n                logger . error ( \"Twilio: Invalid url signature %s - %s - %s\" , url , request . POST , signature , extra = { \"request\" : request } ) \n                return HttpResponseForbidden ( \"Invalid signature\" ) \n        response = f ( request , * args , ** kwargs ) \n        if isinstance ( response , six . text_type ) : \n            return HttpResponse ( response , mimetype = \"application/xml\" ) \n        else : \n            if isinstance ( response , Verb ) : \n                return HttpResponse ( force_text ( response ) , mimetype = \"application/xml\" ) \n            else : \n                return response \n    return decorator "}
{"12145": "\ndef _get_color_string ( self ) : \n    s = '' \n    if self . color_type == 'd' : \n        if self . name is \"black\" : \n            s = '%.3f G' % 0 \n        else : \n            s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) \n    else : \n        if self . color_type == 'f' or self . color_type == 't' : \n            if self . name is \"black\" : \n                s = '%.3f g' % 0 \n            else : \n                s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) \n    return s "}
{"12146": "\ndef get_ttf ( self ) : \n    font_dict = { } \n    families = [ ] \n    rootdirlist = string . split ( self . search_path , os . pathsep ) \n    for dirName , subdirList , filelist in itertools . chain . from_iterable ( os . walk ( path ) for path in rootdirlist ) : \n        for item in filelist : \n            root , ext = os . path . splitext ( item ) \n            if ext == '.ttf' : \n                if root [ 0 ] . lower ( ) in english : \n                    source = os . path . join ( dirName , item ) \n                    name = root . lower ( ) . replace ( '_' , ' ' ) \n                    if ' bold' in name : \n                        name = name . replace ( ' bold' , '_bold' ) \n                        if ' italic' in name : \n                            name = name . replace ( ' italic' , '_italic' ) \n                    else : \n                        if 'bold' in name : \n                            name = name . replace ( 'bold' , '_bold' ) \n                            if 'italic' in name : \n                                name = name . replace ( 'italic' , '_italic' ) \n                        else : \n                            if ' italic' in name : \n                                name = name . replace ( ' italic' , '_italic' ) \n                            else : \n                                if 'italic' in name : \n                                    name = name . replace ( 'italic' , '_italic' ) \n                                else : \n                                    if 'oblique' in name : \n                                        name = name . replace ( 'oblique' , '_italic' ) \n                                    else : \n                                        families . append ( name ) \n                    font_dict [ name ] = source \n                else : \n                    source = os . path . join ( dirName , item ) \n                    name = root . lower ( ) . replace ( '_' , ' ' ) \n                    font_dict [ name ] = source \n                    families . append ( name ) \n    self . font_dict = font_dict \n    self . families = families "}
{"12166": "\ndef _set_style ( self , style = None ) : \n    if style is None : \n        self . style = '' \n        self . underline = False \n    else : \n        if self . family == ( 'symbol' or 'zapfdingbats' ) : \n            self . style = '' \n            self . underline = False \n    self . style = style . upper ( ) \n    if 'U' in self . style or self . style == 'U' : \n        self . underline = True \n    else : \n        self . underline = False "}
{"12170": "\ndef close ( self ) : \n    self . document . _set_page_numbers ( ) \n    self . _put_header ( ) \n    self . _put_pages ( ) \n    self . _put_resources ( ) \n    self . _put_information ( ) \n    self . _put_catalog ( ) \n    self . _put_trailer ( ) \n    if hasattr ( self . destination , \"write\" ) : \n        output = self . _output_to_io ( ) \n    else : \n        if self . destination == 'string' : \n            output = self . _output_to_string ( ) \n        else : \n            self . _output_to_file ( ) \n            output = None \n    return output "}
{"12175": "\ndef _put_catalog ( self ) : \n    self . session . _add_object ( ) \n    self . session . _out ( '<<' ) \n    self . session . _out ( '/Type /Catalog' ) \n    self . session . _out ( '/Pages 1 0 R' ) \n    if self . zoom_mode == 'fullpage' : \n        self . session . _out ( '/OpenAction [3 0 R /Fit]' ) \n    else : \n        if self . zoom_mode == 'fullwidth' : \n            self . session . _out ( '/OpenAction [3 0 R /FitH null]' ) \n        else : \n            if self . zoom_mode == 'real' : \n                self . session . _out ( '/OpenAction [3 0 R /XYZ null null 1]' ) \n            else : \n                if not isinstance ( self . zoom_mode , basestring ) : \n                    self . session . _out ( '/OpenAction [3 0 R /XYZ null null ' + ( self . zoom_mode / 100 ) + ']' ) \n    if self . layout_mode == 'single' : \n        self . session . _out ( '/PageLayout /SinglePage' ) \n    else : \n        if self . layout_mode == 'continuous' : \n            self . session . _out ( '/PageLayout /OneColumn' ) \n        else : \n            if self . layout_mode == 'two' : \n                self . session . _out ( '/PageLayout /TwoColumnLeft' ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12210": "\ndef update ( self , ** kwargs ) : \n    if kwargs . get ( 'verify_kwargs' , True ) : \n        valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . _fields_ ] \n        valid += [ 'b' , 'times' ] \n        for k in kwargs . keys ( ) : \n            if k not in valid : \n                raise Exception ( \"Invalid kwarg '%s'.\" % k ) \n    if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : KIPPING } ) \n    else : \n        if ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : \n            kwargs . update ( { 'ldmodel' : NONLINEAR } ) \n    self . limbdark . update ( ** kwargs ) \n    self . transit . update ( ** kwargs ) \n    self . settings . update ( ** kwargs ) "}
{"12248": "\ndef post ( self , headers = { } , body = \"\" ) : \n    code , message = self . command ( \"POST\" ) \n    if code != 340 : \n        raise NNTPReplyError ( code , message ) \n    hdrs = utils . unparse_headers ( headers ) \n    self . socket . sendall ( hdrs ) \n    if isinstance ( body , basestring ) : \n        body = cStringIO . StringIO ( body ) \n    illegal = False \n    for line in body : \n        if line . startswith ( \".\" ) : \n            line = \".\" + line \n        if line . endswith ( \"\\r\\n\" ) : \n            line = line [ : - 2 ] \n        else : \n            if line . endswith ( \"\\n\" ) : \n                line = line [ : - 1 ] \n        if any ( c in line for c in \"\\0\\r\" ) : \n            illegal = True \n            break \n        self . socket . sendall ( line + \"\\r\\n\" ) \n    self . socket . sendall ( \".\\r\\n\" ) \n    code , message = self . status ( ) \n    if illegal : \n        raise NNTPDataError ( \"Illegal characters found\" ) \n    if code != 240 : \n        raise NNTPReplyError ( code , message ) \n    message_id = message . split ( None , 1 ) [ 0 ] \n    if message_id . startswith ( \"<\" ) and message_id . endswith ( \">\" ) : \n        return message_id \n    return True "}
{"12273": "\ndef check_type ( self , value ) : \n    if self . __dict__ [ 'dtype' ] is None : \n        return \n    else : \n        if value is None : \n            return \n        else : \n            if isinstance ( value , self . __dict__ [ 'dtype' ] ) : \n                return \n    msg = \"Value of type %s, when %s was expected.\" % ( type ( value ) , self . __dict__ [ 'dtype' ] ) \n    raise TypeError ( msg ) "}
{"12286": "\ndef _init_properties ( self ) : \n    self . _missing = { } \n    for k , p in self . params . items ( ) : \n        if p . required : \n            self . _missing [ k ] = p \n        if isinstance ( p , Derived ) : \n            if p . loader is None : \n                p . loader = self . __getattribute__ ( \"_%s\" % k ) \n            else : \n                if isinstance ( p . loader , str ) : \n                    p . loader = self . __getattribute__ ( p . loader ) "}
{"12300": "\ndef verify_type_product ( self , satellite ) : \n    if satellite == 'L5' : \n        id_satellite = '3119' \n        stations = [ 'GLC' , 'ASA' , 'KIR' , 'MOR' , 'KHC' , 'PAC' , 'KIS' , 'CHM' , 'LGS' , 'MGR' , 'COA' , 'MPS' ] \n    else : \n        if satellite == 'L7' : \n            id_satellite = '3373' \n            stations = [ 'EDC' , 'SGS' , 'AGS' , 'ASN' , 'SG1' ] \n        else : \n            if satellite == 'L8' : \n                id_satellite = '4923' \n                stations = [ 'LGN' ] \n            else : \n                raise ProductInvalidError ( 'Type product invalid. the permitted types are: L5, L7, L8. ' ) \n    typ_product = dict ( id_satelite = id_satellite , stations = stations ) \n    return typ_product "}
{"12302": "\ndef download ( self , bands = None , download_dir = None , metadata = False ) : \n    if not download_dir : \n        download_dir = DOWNLOAD_DIR \n    if bands is None : \n        bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] \n    else : \n        self . validate_bands ( bands ) \n    pattern = re . compile ( '^[^\\s]+_(.+)\\.tiff?' , re . I ) \n    band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] \n    image_list = [ ] \n    self . connect_earthexplorer ( ) \n    tgzname = self . sceneInfo . name + '.tgz' \n    dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) \n    downloaded = self . download_file ( self . url , dest_dir , tgzname ) \n    logger . debug ( 'Status downloaded %s' % downloaded ) \n    print ( '\\n Status downloaded %s' % downloaded ) \n    if downloaded [ 'sucess' ] : \n        print ( '\\n Downloaded sucess' ) \n        logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) \n        try : \n            tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) \n            folder_path = join ( download_dir , self . sceneInfo . name ) \n            tar . extractall ( folder_path ) \n            remove ( downloaded [ 'file_path' ] ) \n            images_path = listdir ( folder_path ) \n            for image_path in images_path : \n                matched = pattern . match ( image_path ) \n                file_path = join ( folder_path , image_path ) \n                if matched and matched . group ( 1 ) in band_list : \n                    image_list . append ( [ file_path , getsize ( file_path ) ] ) \n                else : \n                    if matched : \n                        remove ( file_path ) \n        except tarfile . ReadError as error : \n            print ( '\\nError when extracting files. %s' % error ) \n            logger . error ( 'Error when extracting files. %s' % error ) \n        return image_list \n    else : \n        logger . debug ( 'Info downloaded: %s' % downloaded ) \n        print ( '\\n Info downloaded: %s' % downloaded ) \n        return downloaded "}
{"12330": "\ndef _get_rule_transform ( self , rule ) : \n    rd = self . _find_directive ( lambda d : d . name == \"rule\" and d . args . get ( \"name\" ) == rule . name ) \n    if rd : \n        args = rd . args \n    else : \n        args = { } \n    transform = args . get ( \"transform\" , \"retype\" ) \n    if transform == \"retype\" : \n        new_name = args . get ( \"to_type\" , \"TokenType.{0}\" . format ( rule . name ) ) \n        return \".retyped({0})\" . format ( new_name ) \n    else : \n        if transform == \"compress\" : \n            new_name = args . get ( \"to_type\" , \"TokenType.{0}\" . format ( rule . name ) ) \n            if new_name == \"identity\" : \n                return \".compressed()\" \n            else : \n                return \".compressed({0})\" . format ( new_name ) \n        else : \n            if transform == \"identity\" : \n                return \"\" "}
{"12332": "\ndef _node_to_asn ( self , node ) : \n    if node . is_type ( TokenType . identifier ) : \n        return Identifier ( node . svalue ) \n    else : \n        if node . is_type ( TokenType . terminal ) : \n            return Terminal ( node . svalue ) \n        else : \n            if node . is_type ( TokenType . option_group ) : \n                expr = node . children [ 0 ] \n                return OptionGroup ( self . _expression_to_asn ( expr ) ) \n            else : \n                if node . is_type ( TokenType . repetition_group ) : \n                    expr = node . children [ 0 ] \n                    return RepetitionGroup ( self . _expression_to_asn ( expr ) ) \n                else : \n                    if node . is_type ( TokenType . grouping_group ) : \n                        expr = node . children [ 0 ] \n                        return GroupingGroup ( self . _expression_to_asn ( expr ) ) \n                    else : \n                        if node . is_type ( TokenType . special_handling ) : \n                            ident = node . children [ 0 ] \n                            return SpecialHandling ( ident ) \n                        else : \n                            if node . is_type ( TokenType . number ) : \n                                return Number ( node . svalue ) \n                            else : \n                                if node . is_type ( ( TokenType . operator , TokenType . op_mult , TokenType . op_add ) ) : \n                                    return OperatorNode ( OPERATOR_INDEX [ node . svalue ] , node . position ) \n                                else : \n                                    raise Exception ( \"Unhandled parse tree node: {0}\" . format ( node ) ) "}
{"12334": "\ndef _remove_grouping_groups ( self , optree ) : \n    new_operands = [ ] \n    for operand in optree . operands : \n        if isinstance ( operand , OptreeNode ) : \n            new_operands . append ( self . _remove_grouping_groups ( operand ) ) \n        else : \n            if isinstance ( operand , GroupingGroup ) : \n                new_operands . append ( operand . expression ) \n            else : \n                new_operands . append ( operand ) \n    return OptreeNode ( optree . opnode , new_operands ) "}
{"12335": "\ndef _ast_to_code ( self , node , ** kwargs ) : \n    if isinstance ( node , OptreeNode ) : \n        return self . _ast_optree_node_to_code ( node , ** kwargs ) \n    else : \n        if isinstance ( node , Identifier ) : \n            return self . _ast_identifier_to_code ( node , ** kwargs ) \n        else : \n            if isinstance ( node , Terminal ) : \n                return self . _ast_terminal_to_code ( node , ** kwargs ) \n            else : \n                if isinstance ( node , OptionGroup ) : \n                    return self . _ast_option_group_to_code ( node , ** kwargs ) \n                else : \n                    if isinstance ( node , RepetitionGroup ) : \n                        return self . _ast_repetition_group_to_code ( node , ** kwargs ) \n                    else : \n                        if isinstance ( node , SpecialHandling ) : \n                            return self . _ast_special_handling_to_code ( node , ** kwargs ) \n                        else : \n                            if isinstance ( node , Number ) : \n                                return self . _ast_number_to_code ( node , ** kwargs ) \n                            else : \n                                raise Exception ( \"Unhandled ast node: {0}\" . format ( node ) ) "}
{"12336": "\ndef _ast_optree_node_to_code ( self , node , ** kwargs ) : \n    opnode = node . opnode \n    if opnode is None : \n        return self . _ast_to_code ( node . operands [ 0 ] ) \n    else : \n        operator = opnode . operator \n        if operator is OP_ALTERNATE : \n            return self . _ast_op_alternate_to_code ( node , ** kwargs ) \n        else : \n            if operator is OP_WS_CONCAT : \n                kwargs [ \"ignore_whitespace\" ] = False \n                return self . _ast_op_concat_to_code ( node , ** kwargs ) \n            else : \n                if operator is OP_CONCAT : \n                    kwargs [ \"ignore_whitespace\" ] = True \n                    return self . _ast_op_concat_to_code ( node , ** kwargs ) \n                else : \n                    if operator is OP_EXCLUDE : \n                        return self . _ast_op_exclude_to_code ( node , ** kwargs ) \n                    else : \n                        if operator is OP_MULTIPLY : \n                            return self . _ast_op_multiply_to_code ( node , ** kwargs ) \n                        else : \n                            if operator is OP_REPEAT : \n                                return self . _ast_op_repeat_to_code ( node , ** kwargs ) \n                            else : \n                                raise Exception ( \"Unhandled optree node: {0}\" . format ( node ) ) "}
{"12376": "\ndef pprint ( root , depth = 0 , space_unit = \"    \" , * , source_len = 0 , file = None ) : \n    spacing = space_unit * depth \n    if isinstance ( root , str ) : \n        print ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) \n    else : \n        if root . position is None : \n            position = - 1 \n        else : \n            if root . position < 0 : \n                position = source_len + root . position \n            else : \n                position = root . position \n        if root . is_value : \n            print ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node_type , position , root . consumed , root . svalue ) , file = file ) \n        else : \n            print ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node_type , position , root . consumed ) , file = file ) \n            for child in root . children : \n                pprint ( child , depth + 1 , source_len = source_len , file = file ) "}
{"12425": "\ndef normalize ( ast : Node ) -> Node : \n    res = ast \n    typemap = { DictNode , ListNode , TupleNode } \n    if type ( ast ) is dict : \n        res = DictNode ( ast ) \n    else : \n        if type ( ast ) is list : \n            res = ListNode ( ast ) \n        else : \n            if type ( ast ) is tuple : \n                res = TupleNode ( ast ) \n    if hasattr ( res , 'items' ) : \n        for k , v in res . items ( ) : \n            res [ k ] = normalize ( v ) \n    else : \n        if hasattr ( res , '__getitem__' ) : \n            for idx , v in zip ( range ( len ( res ) ) , res ) : \n                res [ idx ] = normalize ( v ) \n    if type ( res ) not in typemap and hasattr ( res , '__dict__' ) : \n        subattr = vars ( res ) \n        for k , v in subattr . items ( ) : \n            setattr ( res , k , normalize ( v ) ) \n    return res "}
{"12441": "\ndef checktypes ( func ) : \n    sig = inspect . signature ( func ) \n    types = { } \n    for param in sig . parameters . values ( ) : \n        param_type = param . annotation \n        if param_type is param . empty or not inspect . isclass ( param_type ) : \n            continue \n        types [ param . name ] = param_type \n        if ( param . default is not param . empty and not isinstance ( param . default , param_type ) ) : \n            raise ValueError ( \"{func}: wrong type of a default value for {arg!r}\" . format ( func = func . __qualname__ , arg = param . name ) ) \n    def check_type ( sig , arg_name , arg_type , arg_value ) : \n        if not isinstance ( arg_value , arg_type ) : \n            raise ValueError ( \"{func}: wrong type of {arg!r} argument, \" \"{exp!r} expected, got {got!r}\" . format ( func = func . __qualname__ , arg = arg_name , exp = arg_type . __name__ , got = type ( arg_value ) . __name__ ) ) \n    \n    @ functools . wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        ba = sig . bind ( * args , ** kwargs ) \n        for arg_name , arg in ba . arguments . items ( ) : \n            try : \n                type_ = types [ arg_name ] \n            except KeyError : \n                continue \n            else : \n                param = sig . parameters [ arg_name ] \n                if param . kind == param . VAR_POSITIONAL : \n                    for value in arg : \n                        check_type ( sig , arg_name , type_ , value ) \n                else : \n                    if param . kind == param . VAR_KEYWORD : \n                        for subname , value in arg . items ( ) : \n                            check_type ( sig , arg_name + ':' + subname , type_ , value ) \n                    else : \n                        check_type ( sig , arg_name , type_ , arg ) \n        result = func ( * ba . args , ** ba . kwargs ) \n        return_type = sig . return_annotation \n        if ( return_type is not sig . empty and isinstance ( return_type , type ) and not isinstance ( result , return_type ) ) : \n            raise ValueError ( '{func}: wrong return type, {exp} expected, got {got}' . format ( func = func . __qualname__ , exp = return_type . __name__ , got = type ( result ) . __name__ ) ) \n        return result \n    return wrapper "}
{"12492": "\ndef nextstate ( self , newstate , treenode = None , user_data = None ) : \n    if newstate is None : \n        return self \n    if isinstance ( newstate , State ) and id ( newstate ) != id ( self ) : \n        return newstate \n    else : \n        if isinstance ( newstate , StateEvent ) : \n            self . state_register . named_events [ newstate . name ] = True \n            return newstate . st \n        else : \n            if isinstance ( newstate , StatePrecond ) : \n                return newstate . st \n            else : \n                if isinstance ( newstate , StateHook ) : \n                    newstate . call ( treenode , user_data ) \n                    return newstate . st \n    return self "}
{"12493": "\ndef resetLivingState ( self ) : \n    must_delete = [ ] \n    l = len ( self . ls ) \n    for idx , ls in zip ( range ( l ) , self . ls ) : \n        ids = id ( ls [ 1 ] . thestate ( ) ) \n        if ids == id ( ls [ 0 ] ) and ( ls [ 1 ] . have_finish or not ls [ 1 ] . alive ) : \n            must_delete . append ( idx ) \n        else : \n            if ls [ 1 ] . alive : \n                ls [ 1 ] . alive = False \n    for delete in reversed ( must_delete ) : \n        self . ls . pop ( delete ) \n    self . init_all ( ) "}
{"12515": "\ndef list_to_str ( lst : list , content : str , indent : int = 1 ) : \n    for i in lst : \n        if isinstance ( i , indentable ) : \n            content = i . to_str ( content , indent ) \n        else : \n            if isinstance ( i , list ) : \n                content = list_to_str ( i , content , indent ) \n            else : \n                if isinstance ( i , str ) : \n                    content = catend ( content , i , indent ) \n    return content "}
{"12517": "\ndef populate_from_sequence ( seq : list , r : ref ( Edge ) , sr : state . StateRegister ) : \n    base_state = r \n    idxlast = len ( seq ) - 1 \n    idx = 0 \n    for m in seq : \n        if isinstance ( m , list ) : \n            for item in m : \n                populate_from_sequence ( item , r , sr ) \n        else : \n            if isinstance ( m , MatchExpr ) : \n                eX = r ( ) . get_next_edge ( m ) \n                if eX is None : \n                    sX = None \n                    if idx != idxlast : \n                        sX = state . State ( sr ) \n                        sX . matchDefault ( base_state ( ) . s ) \n                    else : \n                        sX = base_state ( ) . s \n                    eX = Edge ( sX ) \n                    r ( ) . next_edge [ id ( sX ) ] = eX \n                    m . attach ( r ( ) . s , sX , sr ) \n                r = ref ( eX ) \n        idx += 1 "}
{"12567": "\ndef _loadrecord ( record_dump , source_type , eager = False ) : \n    if eager : \n        import_record . s ( record_dump , source_type = source_type ) . apply ( throw = True ) \n    else : \n        if current_migrator . records_post_task : \n            chain ( import_record . s ( record_dump , source_type = source_type ) , current_migrator . records_post_task . s ( ) ) ( ) \n        else : \n            import_record . delay ( record_dump , source_type = source_type ) "}
{"12581": "\ndef load_user ( data ) : \n    from invenio_accounts . models import User \n    from invenio_userprofiles . api import UserProfile \n    email = data [ 'email' ] . strip ( ) \n    if User . query . filter_by ( email = email ) . count ( ) > 0 : \n        raise UserEmailExistsError ( \"User email '{email}' already exists.\" . format ( email = email ) ) \n    last_login = None \n    if data [ 'last_login' ] : \n        last_login = arrow . get ( data [ 'last_login' ] ) . datetime \n    confirmed_at = None \n    if data [ 'note' ] == '1' : \n        confirmed_at = datetime . utcnow ( ) \n    salt = data [ 'password_salt' ] \n    checksum = data [ 'password' ] \n    if not checksum : \n        new_password = None \n    else : \n        if checksum . startswith ( '$' ) : \n            new_password = checksum \n        else : \n            new_password = str . join ( '$' , [ '' , u'invenio-aes' , salt , checksum ] ) \n    with db . session . begin_nested ( ) : \n        obj = User ( id = data [ 'id' ] , password = new_password , email = email , confirmed_at = confirmed_at , last_login_at = last_login , active = ( data [ 'note' ] != '0' ) , ) \n        db . session . add ( obj ) \n    nickname = data [ 'nickname' ] . strip ( ) \n    overwritten_username = ( 'username' in data and 'displayname' in data ) \n    if nickname or overwritten_username : \n        p = UserProfile ( user = obj ) \n        p . full_name = data . get ( 'full_name' , '' ) . strip ( ) \n        if overwritten_username : \n            p . _username = data [ 'username' ] . lower ( ) \n            p . _displayname = data [ 'displayname' ] \n        else : \n            if nickname : \n                if UserProfile . query . filter ( UserProfile . _username == nickname . lower ( ) ) . count ( ) > 0 : \n                    raise UserUsernameExistsError ( \"Username '{username}' already exists.\" . format ( username = nickname ) ) \n                try : \n                    p . username = nickname \n                except ValueError : \n                    current_app . logger . warn ( u'Invalid username {0} for user_id {1}' . format ( nickname , data [ 'id' ] ) ) \n                    p . _username = nickname . lower ( ) \n                    p . _displayname = nickname \n        db . session . add ( p ) \n    db . session . commit ( ) "}
{"12602": "\ndef delete ( self ) : \n    del self . bg . widget \n    del self . bg \n    del self . _pos \n    del self . _size \n    self . actions = { } \n    for e_type , e_handlers in self . peng . eventHandlers . items ( ) : \n        if True or e_type in eh : \n            to_del = [ ] \n            for e_handler in e_handlers : \n                if isinstance ( e_handler , weakref . ref ) : \n                    if super ( weakref . WeakMethod , e_handler ) . __call__ ( ) is self : \n                        to_del . append ( e_handler ) \n                else : \n                    if e_handler is self : \n                        to_del . append ( e_handler ) \n            for d in to_del : \n                try : \n                    del e_handlers [ e_handlers . index ( d ) ] \n                except Exception : \n                    import traceback ; \n                    traceback . print_exc ( ) "}
{"12624": "\ndef draw ( self ) : \n    self . window . set2d ( ) \n    if isinstance ( self . bg , Layer ) : \n        self . bg . _draw ( ) \n    else : \n        if hasattr ( self . bg , \"draw\" ) and callable ( self . bg . draw ) : \n            self . bg . draw ( ) \n        else : \n            if isinstance ( self . bg , list ) or isinstance ( self . bg , tuple ) : \n                self . bg_vlist . draw ( GL_QUADS ) \n            else : \n                if callable ( self . bg ) : \n                    self . bg ( ) \n                else : \n                    if isinstance ( self . bg , Background ) : \n                        if not self . bg . initialized : \n                            self . bg . init_bg ( ) \n                            self . bg . redraw_bg ( ) \n                            self . bg . initialized = True \n                    else : \n                        if self . bg == \"blank\" : \n                            pass \n                        else : \n                            raise TypeError ( \"Unknown background type\" ) \n    self . window . set2d ( ) \n    for widget in self . widgets . values ( ) : \n        if widget . do_redraw : \n            widget . on_redraw ( ) \n            widget . do_redraw = False \n    self . batch2d . draw ( ) \n    for widget in self . widgets . values ( ) : \n        widget . draw ( ) "}
{"12653": "\ndef _draw ( self , mode , vertex_list = None ) : \n    glPushClientAttrib ( GL_CLIENT_VERTEX_ARRAY_BIT ) \n    for buffer , attributes in self . buffer_attributes : \n        buffer . bind ( ) \n        for attribute in attributes : \n            attribute . enable ( ) \n            attribute . set_pointer ( attribute . buffer . ptr ) \n    if vertexbuffer . _workaround_vbo_finish : \n        glFinish ( ) \n    if vertex_list is not None : \n        glDrawArrays ( mode , vertex_list . start , vertex_list . count ) \n    else : \n        starts , sizes = self . allocator . get_allocated_regions ( ) \n        primcount = len ( starts ) \n        if primcount == 0 : \n            pass \n        else : \n            if primcount == 1 : \n                glDrawArrays ( mode , starts [ 0 ] , int ( sizes [ 0 ] ) ) \n            else : \n                if gl_info . have_version ( 1 , 4 ) : \n                    starts = ( GLint * primcount ) ( * starts ) \n                    sizes = ( GLsizei * primcount ) ( * sizes ) \n                    glMultiDrawArrays ( mode , starts , sizes , primcount ) \n                else : \n                    for start , size in zip ( starts , sizes ) : \n                        glDrawArrays ( mode , start , size ) \n    for buffer , _ in self . buffer_attributes : \n        buffer . unbind ( ) \n    glPopClientAttrib ( ) "}
{"12669": "\ndef centroids_and_volumes ( self , sort_index = True ) : \n    elements = self . elements \n    out = [ ] \n    for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : \n        etype_info = ELEMENTS [ etype ] \n        simplices_info = etype_info . simplices \n        index = group . index \n        simplices_data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) \n        simplices = simplices_data . values . reshape ( index . size , simplices_info . shape [ 0 ] , simplices_info . shape [ 1 ] , 3 ) \n        edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] \n        simplices_centroids = simplices . mean ( axis = 2 ) \n        if etype_info . space == 2 : \n            simplices_volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. \n        else : \n            if etype_info . space == 3 : \n                simplices_volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. \n        elements_volumes = simplices_volumes . sum ( axis = 1 ) \n        elements_centroids = ( ( simplices_volumes . reshape ( * simplices_volumes . shape , 1 ) * simplices_centroids ) . sum ( axis = 1 ) / elements_volumes . reshape ( * elements_volumes . shape , 1 ) ) \n        volumes_df = pd . DataFrame ( index = index , data = elements_volumes , columns = pd . MultiIndex . from_product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) \n        centroids_df = pd . DataFrame ( index = index , data = elements_centroids , columns = pd . MultiIndex . from_product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) \n        out . append ( pd . concat ( [ volumes_df , centroids_df ] , axis = 1 ) ) \n    out = pd . concat ( out ) \n    if sort_index : \n        out . sort_index ( inplace = True ) \n    return out . sort_index ( axis = 1 ) "}
{"12686": "\ndef parse_response ( self , response ) : \n    payload = None \n    try : \n        if isinstance ( response . json , collections . Callable ) : \n            payload = response . json ( ) \n        else : \n            payload = response . json \n    except ValueError : \n        payload = response . content \n    if not self . _raise_errors : \n        return payload \n    else : \n        if response . status_code == 401 : \n            raise AuthenticationError ( payload [ 'message' ] ) \n        else : \n            if response . status_code == 500 : \n                raise ServerError ( payload [ 'message' ] ) \n            else : \n                if isinstance ( payload , dict ) and not payload [ 'success' ] : \n                    raise APIError ( payload [ 'message' ] ) \n                else : \n                    return payload "}
{"12689": "\ndef write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - 1 , frame = - 1 , sortItem = 'Node Label' ) : \n    stepKeys = get_steps ( odb ) \n    step = xrange ( len ( stepKeys ) ) [ step ] \n    frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] \n    nf = NumberFormat ( numDigits = 9 , precision = 0 , format = SCIENTIFIC ) \n    session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) \n    leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) \n    session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) \n    session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) \n    lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] \n    isdata = - 1 \n    data = [ ] \n    for line in lines : \n        if isdata == 1 : \n            if len ( line ) == 0 : \n                isdata -= 1 \n            else : \n                data . append ( line ) \n        else : \n            if isdata < 1 : \n                if line . startswith ( \"--\" ) : \n                    isdata += 1 \n    data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) \n    header = str ( output_position ) . lower ( ) + \",\" \n    header += \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + \"\\n\" \n    metadata = ( ( \"label\" , label ) , ( \"argiope_class\" , argiope_class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output_position ) , ( \"step_num\" , step ) , ( \"step_label\" , stepKeys [ step ] ) , ( \"frame\" , frame ) , ( \"frame_value\" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) \n    out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) \n    open ( path , \"w\" ) . write ( out ) "}
{"12721": "\ndef strip_prefix ( s , prefix , strict = False ) : \n    if s . startswith ( prefix ) : \n        return s [ len ( prefix ) : ] \n    else : \n        if strict : \n            raise WimpyError ( \"string doesn't start with prefix\" ) \n    return s "}
{"12722": "\ndef strip_suffix ( s , suffix , strict = False ) : \n    if s . endswith ( suffix ) : \n        return s [ : len ( s ) - len ( suffix ) ] \n    else : \n        if strict : \n            raise WimpyError ( \"string doesn't end with suffix\" ) \n    return s "}
{"12729": "\ndef static ( self , root , path , media_type = None , charset = 'UTF-8' ) : \n    root = os . path . abspath ( os . path . join ( root , '' ) ) \n    path = os . path . abspath ( os . path . join ( root , path . lstrip ( '/\\\\' ) ) ) \n    self . response . state [ 'filename' ] = os . path . basename ( path ) \n    if not path . startswith ( root ) : \n        return 403 \n    else : \n        if not os . path . isfile ( path ) : \n            return 404 \n    if media_type is not None : \n        self . response . media_type = media_type \n    else : \n        self . response . media_type = mimetypes . guess_type ( path ) [ 0 ] \n    self . response . charset = charset \n    with open ( path , 'rb' ) as f : \n        return f . read ( ) "}
{"12730": "\ndef _get_error_page_callback ( self ) : \n    if self . response . status in self . _error_handlers : \n        return self . _error_handlers [ self . response . status ] \n    else : \n        if None in self . _error_handlers : \n            return self . _error_handlers [ None ] \n        else : \n            self . response . media_type = 'text/plain' \n            return lambda : self . response . status_line "}
{"12731": "\ndef add ( self , method , pattern , callback ) : \n    pat_type , pat = self . _normalize_pattern ( pattern ) \n    if pat_type == 'literal' : \n        self . _literal [ method ] [ pat ] = callback \n    else : \n        if pat_type == 'wildcard' : \n            self . _wildcard [ method ] . append ( WildcardRoute ( pat , callback ) ) \n        else : \n            self . _regex [ method ] . append ( RegexRoute ( pat , callback ) ) "}
{"12734": "\ndef _normalize_pattern ( pattern ) : \n    if pattern . startswith ( 'regex:' ) : \n        pattern_type = 'regex' \n        pattern = pattern [ len ( 'regex:' ) : ] \n    else : \n        if pattern . startswith ( 'wildcard:' ) : \n            pattern_type = 'wildcard' \n            pattern = pattern [ len ( 'wildcard:' ) : ] \n        else : \n            if pattern . startswith ( 'literal:' ) : \n                pattern_type = 'literal' \n                pattern = pattern [ len ( 'literal:' ) : ] \n            else : \n                if RegexRoute . like ( pattern ) : \n                    pattern_type = 'regex' \n                else : \n                    if WildcardRoute . like ( pattern ) : \n                        pattern_type = 'wildcard' \n                    else : \n                        pattern_type = 'literal' \n    return pattern_type , pattern "}
{"12735": "\ndef response ( self ) : \n    if isinstance ( self . body , bytes ) : \n        out = self . body \n    else : \n        if isinstance ( self . body , str ) : \n            out = self . body . encode ( self . charset ) \n        else : \n            out = b'' \n    self . add_header ( 'Content-Type' , self . content_type ) \n    self . add_header ( 'Content-Length' , str ( len ( out ) ) ) \n    self . start ( self . status_line , self . _headers ) \n    return [ out ] "}
{"12747": "\ndef _random_adjspecies_pair ( ) : \n    describer , desc_position = random_describer ( ) \n    if desc_position == 'prefix' : \n        return ( describer , random_species ( ) ) \n    else : \n        if desc_position == 'suffix' : \n            return ( random_species ( ) , describer ) "}
{"12759": "\ndef run ( self , i_str , start_count = 0 , start_chunk_time = None ) : \n    try : \n        if not os . path . exists ( self . tmp_dir_path ) : \n            os . makedirs ( self . tmp_dir_path ) \n        if start_chunk_time is None : \n            start_chunk_time = time . time ( ) \n        i_chunk = self . reader ( i_str ) \n        t_path = None \n        len_clean_visible = 0 \n        sources = set ( ) \n        next_idx = 0 \n        input_item_count = 0 \n        for si in i_chunk : \n            next_idx += 1 \n            if gevent : \n                gevent . sleep ( 0 ) \n            if next_idx <= start_count : \n                continue \n            if next_idx % self . rate_log_interval == 0 : \n                elapsed = time . time ( ) - start_chunk_time \n                if elapsed > 0 : \n                    rate = float ( next_idx ) / elapsed \n                    logger . info ( '%d in %.1f --> %.1f per sec on ' '(pre-partial_commit) %s' , next_idx - start_count , elapsed , rate , i_str ) \n            if not self . t_chunk : \n                t_path = os . path . join ( self . tmp_dir_path , 't_chunk-%s' % uuid . uuid4 ( ) . hex ) \n                self . t_chunk = streamcorpus . Chunk ( path = t_path , mode = 'wb' ) \n                assert self . t_chunk . message == streamcorpus . StreamItem_v0_3_0 , self . t_chunk . message \n            si = self . _run_incremental_transforms ( si , self . incremental_transforms ) \n            if si : \n                sources . add ( si . source ) \n                if self . assert_single_source and len ( sources ) != 1 : \n                    raise InvalidStreamItem ( 'stream item %r had source %r, not %r ' '(set assert_single_source: false to suppress)' % ( si . stream_id , si . source , sources ) ) \n            if si and si . body and si . body . clean_visible : \n                len_clean_visible += len ( si . body . clean_visible ) \n            if ( ( self . output_chunk_max_count is not None and len ( self . t_chunk ) == self . output_chunk_max_count ) ) : \n                logger . info ( 'reached output_chunk_max_count (%d) at: %d' , len ( self . t_chunk ) , next_idx ) \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            else : \n                if ( self . output_max_clean_visible_bytes is not None and len_clean_visible >= self . output_chunk_max_clean_visible_bytes ) : \n                    logger . info ( 'reached output_chunk_max_clean_visible_bytes ' '(%d) at: %d' , self . output_chunk_max_clean_visible_bytes , len_clean_visible ) \n                    len_clean_visible = 0 \n                    self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                    start_count = next_idx \n            input_item_count += 1 \n            if ( ( ( self . input_item_limit is not None ) and ( input_item_count > self . input_item_limit ) ) ) : \n                break \n        if self . t_chunk is not None : \n            self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n        return next_idx \n    finally : \n        if self . t_chunk is not None : \n            self . t_chunk . close ( ) \n        for transform in self . batch_transforms : \n            transform . shutdown ( ) \n        if self . cleanup_tmp_files : \n            rmtree ( self . tmp_dir_path ) "}
{"12777": "\ndef html_entities_to_unicode ( text , space_padding = False , safe_only = False ) : \n    def convert_entities ( match ) : \n        x = match . group ( 1 ) \n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD : \n            return u'&%s;' % x \n        if x in name2codepoint : \n            return unichr ( name2codepoint [ x ] ) \n        else : \n            if x in XML_ENTITIES_TO_SPECIAL_CHARS : \n                return XML_ENTITIES_TO_SPECIAL_CHARS [ x ] \n            else : \n                if len ( x ) > 0 and x [ 0 ] == '#' : \n                    if len ( x ) > 1 and x [ 1 ] == 'x' : \n                        return unichr ( int ( x [ 2 : ] , 16 ) ) \n                    else : \n                        return unichr ( int ( x [ 1 : ] ) ) \n                else : \n                    return u'&%s;' % x \n    def convert_to_padded_entitites ( match ) : \n        converted_string = convert_entities ( match ) \n        num_spaces_needed = len ( match . group ( 0 ) ) - len ( converted_string ) \n        assert num_spaces_needed >= 0 , 'len(%r) !<= len(%r)' % ( converted_string , match . group ( 0 ) ) \n        num_left = int ( num_spaces_needed / 2 ) \n        num_right = num_spaces_needed - num_left \n        return ( ' ' * num_left ) + converted_string + ( ' ' * num_right ) \n    if space_padding : \n        return tags . sub ( convert_to_padded_entitites , text ) \n    else : \n        return tags . sub ( convert_entities , text ) "}
{"12782": "\ndef make_absolute_paths ( config ) : \n    if not 'streamcorpus_pipeline' in config : \n        logger . critical ( 'bad config: %r' , config ) \n        raise ConfigurationError ( 'missing \"streamcorpus_pipeline\" from config' ) \n    root_path = config [ 'streamcorpus_pipeline' ] . pop ( 'root_path' , None ) \n    if not root_path : \n        root_path = os . getcwd ( ) \n    if not root_path . startswith ( '/' ) : \n        root_path = os . path . join ( os . getcwd ( ) , root_path ) \n    def recursive_abs_path ( sub_config , root_path ) : \n        for key , val in sub_config . items ( ) : \n            if isinstance ( val , basestring ) : \n                if key . endswith ( 'path' ) : \n                    if re . match ( '^http.?://' , val ) : \n                        continue \n                    if not val . startswith ( '/' ) : \n                        sub_config [ key ] = os . path . join ( root_path , val ) \n            else : \n                if isinstance ( val , dict ) : \n                    recursive_abs_path ( val , root_path ) \n    recursive_abs_path ( config , root_path ) \n    config [ 'root_path' ] = root_path "}
{"12786": "\ndef make_clean_visible ( _html , tag_replacement_char = ' ' ) : \n    def non_tag_chars ( html ) : \n        n = 0 \n        while n < len ( html ) : \n            angle = html . find ( '<' , n ) \n            if angle == - 1 : \n                yield html [ n : ] \n                n = len ( html ) \n                break \n            yield html [ n : angle ] \n            n = angle \n            while n < len ( html ) : \n                nl = html . find ( '\\n' , n ) \n                angle = html . find ( '>' , n ) \n                if angle == - 1 : \n                    yield ' ' * ( len ( html ) - n ) \n                    n = len ( html ) \n                    break \n                else : \n                    if nl == - 1 or angle < nl : \n                        yield ' ' * ( angle + 1 - n ) \n                        n = angle + 1 \n                        break \n                    else : \n                        yield ' ' * ( nl - n ) + '\\n' \n                        n = nl + 1 \n    if not isinstance ( _html , unicode ) : \n        _html = unicode ( _html , 'utf-8' ) \n    _html = fix_emails ( _html ) \n    non_tag = '' . join ( non_tag_chars ( _html ) ) \n    return non_tag . encode ( 'utf-8' ) "}
{"12796": "\ndef make_labels ( self , clean_html , clean_visible = None ) : \n    if self . offset_type == OffsetType . BYTES : \n        parser = self . byte_href_anchors \n    else : \n        if self . offset_type == OffsetType . CHARS : \n            parser = self . char_href_anchors \n        else : \n            if self . offset_type == OffsetType . LINES : \n                parser = self . line_href_anchors \n    labels = [ ] \n    self . clean_html = clean_html \n    for href , first , length , value in parser ( ) : \n        if self . href_filter ( href ) : \n            label = Label ( annotator = Annotator ( annotator_id = 'author' ) , target = Target ( target_id = href ) , ) \n            label . offsets [ self . offset_type ] = Offset ( first = first , length = length , value = value , content_form = 'clean_html' ) \n            labels . append ( label ) \n    return labels "}
{"12805": "\ndef _decode ( self , data ) : \n    informat = self . config [ 'input_format' ] . lower ( ) \n    if informat == 'spinn3r' : \n        return _generate_stream_items ( data ) \n    else : \n        if informat == 'streamitem' : \n            ver = self . config [ 'streamcorpus_version' ] \n            if ver not in _message_versions : \n                raise ConfigurationError ( 'Not a valid streamcorpus version: %s ' '(choose from: %s)' % ( ver , ', ' . join ( _message_versions . keys ( ) ) ) ) \n            message = _message_versions [ ver ] \n            return streamcorpus . Chunk ( data = data , message = message ) \n        else : \n            if informat == 'featurecollection' and FCChunk is not None : \n                return FCChunk ( data = data ) \n            else : \n                raise ConfigurationError ( 'from_s3_chunks unknown input_format = %r' % informat ) "}
{"12836": "\ndef render ( self , file_path , ** kwargs ) : \n    temp = get_tempfile ( suffix = '.svg' ) \n    self . save_content ( temp . name ) \n    file_type = kwargs . get ( 'file_type' , 'pdf' ) \n    dpi = kwargs . get ( 'dpi' , 150 ) \n    support_unicode = kwargs . get ( 'support_unicode' , False ) \n    try : \n        if file_type == 'svg' : \n            shutil . copyfile ( temp . name , file_path ) \n        else : \n            if file_type == 'png' : \n                svg2png ( temp . name , file_path , dpi = dpi ) \n            else : \n                if file_type == 'pdf' : \n                    svg2pdf ( temp . name , file_path , dpi = dpi , support_unicode = support_unicode ) \n    except : \n        log . exception ( 'Error exporting file {} to {}' . format ( file_path , file_type ) ) \n        raise "}
{"12844": "\ndef execute ( option ) : \n    namelist_option = [ ] \n    makefile_option = [ ] \n    flags = \"\" \n    for entry in option : \n        key = entry . keys ( ) [ 0 ] \n        if key == \"Problem Size\" : \n            namelist_option . append ( { \"SIZE\" : entry [ key ] } ) \n        else : \n            if key == \"F90\" : \n                makefile_option . append ( entry ) \n            else : \n                flags += entry [ key ] + \" \" \n    makefile_option . append ( { \"F90FLAGS\" : flags } ) \n    namelist = create_input ( namelist_option , \"namelist\" , template_location = \"templates\" ) \n    makefile_include = create_input ( makefile_option , \"Makefile.include\" , template_location = \"templates\" ) \n    benchmark_base = \"shallow\" \n    location = benchmark_base + \"/original/namelist\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( namelist ) \n    my_file . flush ( ) \n    location = benchmark_base + \"/common/Makefile.include\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( makefile_include ) \n    my_file . flush ( ) \n    base_path = benchmark_base + \"/original\" \n    import subprocess \n    make_process = subprocess . Popen ( [ \"make\" , \"clean\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"make\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"./shallow_base\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    stdout = make_process . stdout . read ( ) \n    for line in stdout . split ( \"\\n\" ) : \n        if \"Time-stepping\" in line : \n            total_time = line . split ( ) [ 2 ] \n    return True , total_time "}
{"12845": "\ndef strval ( node , outermost = True ) : \n    if not isinstance ( node , element ) : \n        return node . xml_value if outermost else [ node . xml_value ] \n    accumulator = [ ] \n    for child in node . xml_children : \n        if isinstance ( child , text ) : \n            accumulator . append ( child . xml_value ) \n        else : \n            if isinstance ( child , element ) : \n                accumulator . extend ( strval ( child , outermost = False ) ) \n    if outermost : \n        accumulator = '' . join ( accumulator ) \n    return accumulator "}
{"12873": "\ndef string_length ( ctx , s = None ) : \n    if s is None : \n        s = ctx . node \n    else : \n        if callable ( s ) : \n            s = next ( s . compute ( ctx ) , '' ) \n    yield len ( s ) "}
{"12888": "\ndef to_string ( obj ) : \n    if isinstance ( obj , LiteralWrapper ) : \n        val = obj . obj \n    else : \n        if isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : \n            val = next ( obj , None ) \n        else : \n            val = obj \n    if val is None : \n        yield '' \n    else : \n        if isinstance ( val , str ) : \n            yield val \n        else : \n            if isinstance ( val , node ) : \n                yield strval ( val ) \n            else : \n                if isinstance ( val , int ) or isinstance ( val , float ) : \n                    yield str ( val ) \n                else : \n                    if isinstance ( item , bool ) : \n                        yield 'true' if item else 'false' \n                    else : \n                        raise RuntimeError ( 'Unknown type for string conversion: {}' . format ( val ) ) "}
{"12889": "\ndef to_number ( obj ) : \n    if isinstance ( obj , LiteralWrapper ) : \n        val = obj . obj \n    else : \n        if isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : \n            val = next ( obj , None ) \n        else : \n            val = obj \n    if val is None : \n        yield 0 \n    else : \n        if isinstance ( val , str ) : \n            yield float ( val ) \n        else : \n            if isinstance ( val , node ) : \n                yield float ( strval ( val ) ) \n            else : \n                if isinstance ( val , int ) or isinstance ( val , float ) : \n                    yield val \n                else : \n                    raise RuntimeError ( 'Unknown type for number conversion: {}' . format ( val ) ) "}
{"12890": "\ndef to_boolean ( obj ) : \n    if isinstance ( obj , LiteralWrapper ) : \n        val = obj . obj \n    else : \n        if isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : \n            val = next ( obj , None ) \n        else : \n            val = obj \n    if val is None : \n        yield False \n    else : \n        if isinstance ( val , bool ) : \n            yield val \n        else : \n            if isinstance ( val , str ) : \n                yield bool ( str ) \n            else : \n                if isinstance ( val , node ) : \n                    yield True \n                else : \n                    if isinstance ( val , float ) or isinstance ( val , int ) : \n                        yield bool ( val ) \n                    else : \n                        raise RuntimeError ( 'Unknown type for boolean conversion: {}' . format ( val ) ) "}
{"12891": "\ndef _serialize ( xp_ast ) : \n    if hasattr ( xp_ast , '_serialize' ) : \n        for tok in xp_ast . _serialize ( ) : \n            yield ( tok ) \n    else : \n        if isinstance ( xp_ast , str ) : \n            yield ( repr ( xp_ast ) ) "}
{"12899": "\ndef Geometry ( * args , ** kwargs ) : \n    arg = kwargs . pop ( 'geojson' , None ) or len ( args ) and args [ 0 ] \n    try : \n        srs = kwargs . pop ( 'srs' , None ) or arg . srs . wkt \n    except AttributeError : \n        srs = SpatialReference ( 4326 ) \n    if hasattr ( arg , 'keys' ) : \n        geom = ogr . CreateGeometryFromJson ( json . dumps ( arg ) ) \n    else : \n        if hasattr ( arg , 'startswith' ) : \n            char = arg [ 0 ] if arg else ' ' \n            i = char if isinstance ( char , int ) else ord ( char ) \n            if i in ( 0 , 1 ) : \n                geom = ogr . CreateGeometryFromWkb ( arg ) \n            else : \n                if arg . startswith ( '{' ) : \n                    geom = ogr . CreateGeometryFromJson ( arg ) \n                else : \n                    if arg . startswith ( '<gml' ) : \n                        geom = ogr . CreateGeometryFromGML ( arg ) \n                    else : \n                        raise ValueError ( 'Invalid geometry value: %s' % arg ) \n        else : \n            if hasattr ( arg , 'wkb' ) : \n                geom = ogr . CreateGeometryFromWkb ( bytes ( arg . wkb ) ) \n            else : \n                geom = ogr . Geometry ( * args , ** kwargs ) \n    if geom : \n        if not isinstance ( srs , SpatialReference ) : \n            srs = SpatialReference ( srs ) \n        geom . AssignSpatialReference ( srs ) \n    return geom "}
{"12951": "\ndef save ( self , to , driver = None ) : \n    path = getattr ( to , 'name' , to ) \n    if not driver and hasattr ( path , 'encode' ) : \n        driver = driver_for_path ( path , self . driver . filter_copyable ( ) ) \n    else : \n        if hasattr ( driver , 'encode' ) : \n            driver = ImageDriver ( driver ) \n    if driver is None or not driver . copyable : \n        raise ValueError ( 'Copy supporting driver not found for %s' % path ) \n    driver . copy ( self , path ) . close ( ) "}
{"12963": "\ndef to_dict ( self ) : \n    result = { } \n    for attr , _ in iteritems ( self . swagger_types ) : \n        value = getattr ( self , attr ) \n        if isinstance ( value , list ) : \n            result [ attr ] = list ( map ( lambda x : x . to_dict ( ) if hasattr ( x , \"to_dict\" ) else x , value ) ) \n        else : \n            if hasattr ( value , \"to_dict\" ) : \n                result [ attr ] = value . to_dict ( ) \n            else : \n                result [ attr ] = value \n    return result "}
{"12977": "\ndef create_adapter ( cmph , ffi , obj ) : \n    if is_file_location ( obj ) : \n        fd = open ( obj ) \n        adapter = cmph . cmph_io_nlfile_adapter ( fd ) \n        def dtor ( ) : \n            cmph . cmph_io_nlfile_adapter_destroy ( adapter ) \n            fd . close ( ) \n        return _AdapterCxt ( adapter , dtor ) \n    else : \n        if is_file ( obj ) : \n            adapter = cmph . cmph_io_nlfile_adapter ( obj ) \n            dtor = lambda : cmph . cmph_io_nlfile_adapter_destroy ( adapter ) \n            return _AdapterCxt ( adapter , dtor ) \n        else : \n            if isinstance ( obj , Sequence ) : \n                if len ( obj ) == 0 : \n                    raise ValueError ( \"An empty sequence is already a perfect hash!\" ) \n                return _create_pyobj_adapter ( cmph , ffi , obj ) \n            else : \n                raise ValueError ( \"data cannot have a cmph wrapper generated\" ) "}
{"12987": "\ndef read_configs_ ( self ) : \n    if not self . config_files_ : \n        return { } , [ ] , [ ] \n    content = { section : { } for section in self } \n    empty_files = [ ] \n    faulty_files = [ ] \n    for cfile in self . config_files_ : \n        conf_dict = self . read_config_ ( cfile ) \n        if conf_dict is None : \n            faulty_files . append ( cfile ) \n            continue \n        else : \n            if not conf_dict : \n                empty_files . append ( cfile ) \n                continue \n        for section , secdict in conf_dict . items ( ) : \n            content [ section ] . update ( secdict ) \n    return content , empty_files , faulty_files "}
{"12991": "\ndef _add_options_to_parser ( self , opts_dict , parser ) : \n    store_bool = ( 'store_true' , 'store_false' ) \n    for opt , sct in opts_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        kwargs = copy . deepcopy ( meta . cmd_kwargs ) \n        action = kwargs . get ( 'action' ) \n        if action is internal . Switch : \n            kwargs . update ( nargs = 0 ) \n        else : \n            if meta . default is not None and action not in store_bool : \n                kwargs . setdefault ( 'type' , type ( meta . default ) ) \n        kwargs . update ( help = meta . help ) \n        kwargs . setdefault ( 'default' , self . _conf [ sct ] [ opt ] ) \n        parser . add_argument ( * _names ( self . _conf [ sct ] , opt ) , ** kwargs ) "}
{"12994": "\ndef _zsh_comp_command ( self , zcf , cmd , grouping , add_help = True ) : \n    if add_help : \n        if grouping : \n            print ( \"+ '(help)'\" , end = BLK , file = zcf ) \n        print ( \"'--help[show help message]'\" , end = BLK , file = zcf ) \n        print ( \"'-h[show help message]'\" , end = BLK , file = zcf ) \n    no_comp = ( 'store_true' , 'store_false' ) \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        if meta . cmd_kwargs . get ( 'action' ) == 'append' : \n            grpfmt , optfmt = \"+ '{}'\" , \"'*{}[{}]{}'\" \n            if meta . comprule is None : \n                meta . comprule = '' \n        else : \n            grpfmt , optfmt = \"+ '({})'\" , \"'{}[{}]{}'\" \n        if meta . cmd_kwargs . get ( 'action' ) in no_comp or meta . cmd_kwargs . get ( 'nargs' ) == 0 : \n            meta . comprule = None \n        if meta . comprule is None : \n            compstr = '' \n        else : \n            if meta . comprule == '' : \n                optfmt = optfmt . split ( '[' ) \n                optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n                compstr = ': :( )' \n            else : \n                optfmt = optfmt . split ( '[' ) \n                optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n                compstr = ': :{}' . format ( meta . comprule ) \n        if grouping : \n            print ( grpfmt . format ( opt ) , end = BLK , file = zcf ) \n        for name in _names ( self . _conf [ sct ] , opt ) : \n            print ( optfmt . format ( name , meta . help . replace ( \"'\" , \"'\\\"'\\\"'\" ) , compstr ) , end = BLK , file = zcf ) "}
{"13023": "\ndef _match_regex ( regex , obj ) : \n    if isinstance ( obj , six . string_types ) : \n        return len ( regex . findall ( obj ) ) > 0 \n    else : \n        if isinstance ( obj , dict ) : \n            return _match_regex ( regex , obj . values ( ) ) \n        else : \n            if hasattr ( obj , '__iter__' ) : \n                return any ( _match_regex ( regex , s ) for s in obj if isinstance ( s , six . string_types ) ) \n            else : \n                return False "}
{"13029": "\ndef _get_attrib ( self , attr , convert_to_str = False ) : \n    if attr . startswith ( 'tags.' ) : \n        tag = attr [ len ( 'tags.' ) : ] \n        if tag in self . tags and self . tags [ tag ] != '' : \n            return self . tags [ tag ] \n        else : \n            if convert_to_str is True : \n                return '<not set>' \n            else : \n                return self . tags . get ( tag ) \n    else : \n        if not hasattr ( self , attr ) : \n            raise AttributeError ( 'Invalid attribute: {0}. Perhaps you meant ' '{1}?' . format ( red ( attr ) , green ( 'tags.' + attr ) ) ) \n        else : \n            result = getattr ( self , attr ) \n            if convert_to_str is True and not result : \n                return '<none>' \n            else : \n                if convert_to_str is True and isinstance ( result , list ) : \n                    return ', ' . join ( result ) \n                else : \n                    if convert_to_str is True : \n                        return str ( result ) \n                    else : \n                        return result "}
{"13033": "\ndef matches ( self , _filter ) : \n    within_attrib = re . match ( r'^([a-z_.]+):(.*)' , _filter ) \n    having_attrib = re . match ( r'^([a-z_.]+)\\?$' , _filter ) \n    if within_attrib is not None : \n        val = self . _get_attrib ( within_attrib . group ( 1 ) ) \n        sub_regex = within_attrib . group ( 2 ) \n        if len ( sub_regex ) > 0 : \n            sub_regex = re . compile ( sub_regex , re . IGNORECASE ) \n            return _match_regex ( sub_regex , val ) \n        else : \n            return val == '' or val is None or val == [ ] \n    else : \n        if having_attrib is not None : \n            val = self . _get_attrib ( having_attrib . group ( 1 ) ) \n            return val != '' and val is not None and val != [ ] \n        else : \n            regex = re . compile ( _filter , re . IGNORECASE ) \n            return _match_regex ( regex , vars ( self ) ) "}
{"13040": "\ndef get ( self , worker_id ) : \n    code = 200 \n    if worker_id == 'all' : \n        report = { 'workers' : [ { 'id' : job , 'report' : self . _inspect_worker ( job ) } for job in self . jobs ] } \n    else : \n        if worker_id in self . jobs : \n            report = { 'id' : worker_id , 'report' : self . _inspect_worker ( worker_id ) } \n        else : \n            report = { 'error' : 'job {} unknown' . format ( worker_id ) } \n            code = 404 \n    return flask . jsonify ( report ) , code "}
{"13061": "\ndef request ( self , method , url , query_params = None , headers = None , post_params = None , body = None ) : \n    if method == \"GET\" : \n        return self . rest_client . GET ( url , query_params = query_params , headers = headers ) \n    else : \n        if method == \"HEAD\" : \n            return self . rest_client . HEAD ( url , query_params = query_params , headers = headers ) \n        else : \n            if method == \"OPTIONS\" : \n                return self . rest_client . OPTIONS ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n            else : \n                if method == \"POST\" : \n                    return self . rest_client . POST ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n                else : \n                    if method == \"PUT\" : \n                        return self . rest_client . PUT ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n                    else : \n                        if method == \"PATCH\" : \n                            return self . rest_client . PATCH ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n                        else : \n                            if method == \"DELETE\" : \n                                return self . rest_client . DELETE ( url , query_params = query_params , headers = headers ) \n                            else : \n                                raise ValueError ( \"http method must be `GET`, `HEAD`,\" \" `POST`, `PATCH`, `PUT` or `DELETE`.\" ) "}
{"13063": "\ndef serve ( self , app_docopt = DEFAULT_DOC , description = '' ) : \n    exit_status = 0 \n    if isinstance ( app_docopt , str ) : \n        args = docopt ( app_docopt , version = description ) \n    else : \n        if isinstance ( app_docopt , dict ) : \n            args = app_docopt \n        else : \n            raise ValueError ( 'unknown configuration object ({})' . format ( type ( app_docopt ) ) ) \n    log_level = args . get ( '--log' , 'debug' ) \n    is_debug = args . get ( '--debug' , False ) \n    log_output = 'stdout' if is_debug else 'apy.log' \n    safe_bind = args . get ( '--bind' , '127.0.0.1' ) \n    safe_port = int ( args . get ( '--port' , 5000 ) ) \n    log_setup = dna . logging . setup ( level = log_level , output = log_output ) \n    with log_setup . applicationbound ( ) : \n        try : \n            log . info ( 'server ready' , version = description , log = log_level , debug = is_debug , bind = '{}:{}' . format ( safe_bind , safe_port ) ) \n            self . app . run ( host = safe_bind , port = safe_port , debug = is_debug ) \n        except Exception as error : \n            if is_debug : \n                raise \n            log . error ( '{}: {}' . format ( type ( error ) . __name__ , str ( error ) ) ) \n            exit_status = 1 \n        finally : \n            log . info ( 'session ended with status {}' . format ( exit_status ) ) \n    return exit_status "}
{"13065": "\ndef stream_command ( command , formatter = None , write_stdin = None , ignore_empty = False ) : \n    command_list = shlex . split ( command ) \n    try : \n        proc = subprocess . Popen ( command_list , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , stdin = subprocess . PIPE ) \n    except Exception as e : \n        raise IOError ( 'Encountered error: {0} when running command {1}' . format ( e . message , ' ' . join ( command_list ) ) ) \n    if write_stdin is not None : \n        proc . stdin . write ( write_stdin ) \n        proc . stdin . flush ( ) \n    while proc . poll ( ) is None : \n        try : \n            line = proc . stdout . readline ( ) \n        except KeyboardInterrupt : \n            sys . exit ( 'Keyboard interrupt while running {}' . format ( command ) ) \n        if len ( line . strip ( ) ) == 0 and ignore_empty is True : \n            continue \n        else : \n            if 'killed by signal 1' in decode ( line ) . lower ( ) : \n                continue \n            else : \n                if 'to the list of known hosts' in decode ( line ) . lower ( ) : \n                    continue \n        if formatter is not None : \n            line = formatter ( line ) \n        sys . stdout . write ( line ) \n    result = proc . poll ( ) \n    return result "}
{"13075": "\ndef _connect_ssh ( entry , username , idfile , tunnel = None ) : \n    if entry . hostname != \"\" and entry . hostname is not None : \n        _host = entry . hostname \n    else : \n        if entry . public_ip != \"\" and entry . public_ip is not None : \n            _host = entry . public_ip \n        else : \n            if entry . private_ip != \"\" and entry . private_ip is not None : \n                if tunnel is None : \n                    raise ValueError ( \"Entry does not have a hostname or public IP. \" \"You can connect via private IP if you use a \" \"tunnel.\" ) \n                _host = entry . private_ip \n            else : \n                raise ValueError ( \"No hostname, public IP or private IP information \" \"found on host entry. I don't know how to connect.\" ) \n    command = _build_ssh_command ( _host , username , idfile , None , tunnel ) \n    print ( 'Connecting to %s...' % cyan ( entry . display ( ) ) ) \n    print ( 'SSH command: %s' % green ( command ) ) \n    proc = subprocess . Popen ( command , shell = True ) \n    return proc . wait ( ) "}
{"13076": "\ndef load ( cls , profile_name = None ) : \n    lsi_location = os . path . expanduser ( '~/.lsi' ) \n    if not os . path . exists ( lsi_location ) : \n        return LsiProfile ( ) \n    cfg_parser = ConfigParser ( ) \n    cfg_parser . read ( lsi_location ) \n    if profile_name is None : \n        if cfg_parser . has_section ( 'default' ) : \n            profile_name = 'default' \n        else : \n            return cls ( ) \n    else : \n        if not cfg_parser . has_section ( profile_name ) : \n            raise cls . LoadError ( 'No such profile {}' . format ( profile_name ) ) \n    def _get ( option , alt = None ) : \n        if cfg_parser . has_option ( profile_name , option ) : \n            return cfg_parser . get ( profile_name , option ) \n        else : \n            return alt \n    if cfg_parser . has_option ( profile_name , 'inherit' ) : \n        profile = cls . load ( cfg_parser . get ( profile_name , 'inherit' ) ) \n    else : \n        profile = cls ( ) \n    profile . override ( 'username' , _get ( 'username' ) ) \n    profile . override ( 'identity_file' , _get ( 'identity file' ) ) \n    profile . override ( 'command' , _get ( 'command' ) ) \n    filters = [ s for s in _get ( 'filters' , '' ) . split ( ',' ) if len ( s ) > 0 ] \n    exclude = [ s for s in _get ( 'exclude' , '' ) . split ( ',' ) if len ( s ) > 0 ] \n    profile . filters . extend ( filters ) \n    profile . exclude . extend ( exclude ) \n    return profile "}
{"13087": "\ndef assign_force_field ( ampal_obj , ff ) : \n    if hasattr ( ampal_obj , 'ligands' ) : \n        atoms = ampal_obj . get_atoms ( ligands = True , inc_alt_states = True ) \n    else : \n        atoms = ampal_obj . get_atoms ( inc_alt_states = True ) \n    for atom in atoms : \n        w_str = None \n        a_ff_id = None \n        if atom . element == 'H' : \n            continue \n        else : \n            if atom . parent . mol_code . upper ( ) in ff : \n                if atom . res_label . upper ( ) in ff [ atom . parent . mol_code ] : \n                    a_ff_id = ( atom . parent . mol_code . upper ( ) , atom . res_label . upper ( ) ) \n                else : \n                    if atom . res_label . upper ( ) in ff [ 'WLD' ] : \n                        a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) \n                    else : \n                        w_str = ( '{} atom is not parameterised in the selected ' 'force field for {} residues, this will be ' 'ignored.' ) . format ( atom . res_label , atom . parent . mol_code ) \n            else : \n                if atom . res_label . upper ( ) in ff [ 'WLD' ] : \n                    a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) \n                else : \n                    w_str = ( '{} ({}) atom is not parameterised in the selected' ' residue force field.' ) . format ( atom . res_label , atom . parent . mol_code ) \n        if w_str : \n            warnings . warn ( w_str , NotParameterisedWarning ) \n        atom . tags [ '_buff_ff_id' ] = a_ff_id \n    return "}
{"13113": "\ndef _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n    travis_event = os . getenv ( 'TRAVIS_EVENT_TYPE' ) \n    if travis_event is None : \n        raise click . UsageError ( 'Using --travis but the TRAVIS_EVENT_TYPE ' 'environment variable is not detected.' ) \n    if travis_event == 'push' and on_travis_push is False : \n        click . echo ( 'Skipping upload on Travis push event.' ) \n        return True \n    else : \n        if travis_event == 'pull_request' and on_travis_pr is False : \n            click . echo ( 'Skipping upload on Travis pull request event.' ) \n            return True \n        else : \n            if travis_event == 'api' and on_travis_api is False : \n                click . echo ( 'Skipping upload on Travis pull request event.' ) \n                return True \n            else : \n                if travis_event == 'cron' and on_travis_cron is False : \n                    click . echo ( 'Skipping upload on Travis cron event.' ) \n                    return True \n                else : \n                    return False "}
{"13117": "\ndef deep_update ( d , u ) : \n    for k , v in u . items ( ) : \n        if isinstance ( v , Mapping ) : \n            d [ k ] = deep_update ( d . get ( k , { } ) , v ) \n        else : \n            if isinstance ( v , list ) : \n                existing_elements = d . get ( k , [ ] ) \n                d [ k ] = existing_elements + [ ele for ele in v if ele not in existing_elements ] \n            else : \n                d [ k ] = v \n    return d "}
{"13127": "\ndef serialize ( self , dataobj , xfield , yfield , time_unit = None , chart_type = \"line\" , width = 800 , height = 300 , color = None , size = None , scale = Scale ( zero = False ) , shape = None , options = { } ) : \n    dataset = dataobj \n    if self . _is_dict ( dataobj ) is True : \n        dataset = self . _dict_to_df ( dataobj , xfield , yfield ) \n    else : \n        if isinstance ( dataobj , list ) : \n            dataset = Data ( values = dataobj ) \n    xencode , yencode = self . _encode_fields ( xfield , yfield , time_unit ) \n    opts = dict ( x = xencode , y = yencode ) \n    if color is not None : \n        opts [ \"color\" ] = color \n    if size is not None : \n        opts [ \"size\" ] = size \n    if shape is not None : \n        opts [ \"shape\" ] = shape \n    chart = self . _chart_class ( dataset , chart_type , ** options ) . encode ( ** opts ) . configure_cell ( width = width , height = height , ) \n    return chart "}
{"13132": "\ndef _chart_class ( self , df , chart_type , ** kwargs ) : \n    if chart_type == \"bar\" : \n        return Chart ( df ) . mark_bar ( ** kwargs ) \n    else : \n        if chart_type == \"circle\" : \n            return Chart ( df ) . mark_circle ( ** kwargs ) \n        else : \n            if chart_type == \"line\" : \n                return Chart ( df ) . mark_line ( ** kwargs ) \n            else : \n                if chart_type == \"point\" : \n                    return Chart ( df ) . mark_point ( ** kwargs ) \n                else : \n                    if chart_type == \"area\" : \n                        return Chart ( df ) . mark_area ( ** kwargs ) \n                    else : \n                        if chart_type == \"tick\" : \n                            return Chart ( df ) . mark_tick ( ** kwargs ) \n                        else : \n                            if chart_type == \"text\" : \n                                return Chart ( df ) . mark_text ( ** kwargs ) \n                            else : \n                                if chart_type == \"square\" : \n                                    return Chart ( df ) . mark_square ( ** kwargs ) \n                                else : \n                                    if chart_type == \"rule\" : \n                                        return Chart ( df ) . mark_rule ( ** kwargs ) \n    return None "}
{"13145": "\ndef check_build_status ( self , build_id ) : \n    data = self . api_request ( 'GET' , '/app-setups/%s' % build_id ) \n    status = data . get ( 'status' ) \n    if status == 'pending' : \n        return False \n    else : \n        if status == 'succeeded' : \n            return True \n        else : \n            raise BuildError ( str ( data ) ) "}
{"13151": "\ndef get_description ( self ) : \n    if self . description : \n        return self . description \n    else : \n        if self . __doc__ and self . __doc__ . strip ( ) : \n            return self . __doc__ . strip ( ) . split ( '.' ) [ 0 ] + '.' \n        else : \n            return '' "}
{"13152": "\ndef get_help ( self ) : \n    if self . help : \n        return self . help \n    else : \n        if self . __doc__ and self . __doc__ . strip ( ) : \n            return self . __doc__ . strip ( ) \n        else : \n            return '' "}
{"13154": "\ndef encode ( self , * args , ** kwargs ) : \n    if isinstance ( args [ 0 ] , str ) : \n        return self . encode ( [ args [ 0 ] ] , ** kwargs ) \n    else : \n        if isinstance ( args [ 0 ] , int ) or isinstance ( args [ 0 ] , float ) : \n            return self . encode ( [ [ args [ 0 ] ] ] , ** kwargs ) \n    if len ( args ) > 1 : \n        dataset = args \n    else : \n        dataset = args [ 0 ] \n    typemap = list ( map ( type , dataset ) ) \n    code = self . encoding [ 0 ] \n    if type ( '' ) in typemap : \n        data = ',' . join ( map ( str , dataset ) ) \n    else : \n        if type ( [ ] ) in typemap or type ( ( ) ) in typemap : \n            data = self . codeset [ 'char' ] . join ( map ( self . encodedata , dataset ) ) \n        else : \n            if len ( dataset ) == 1 and hasattr ( dataset [ 0 ] , '__iter__' ) : \n                data = self . encodedata ( dataset [ 0 ] ) \n            else : \n                try : \n                    data = self . encodedata ( dataset ) \n                except ValueError : \n                    data = self . encodedata ( ',' . join ( map ( unicode , dataset ) ) ) \n    if not '.' in data and code == 't' : \n        code = 'e' \n    return '%s%s:%s' % ( code , self . series , data ) "}
{"13161": "\ndef _get_request ( self , endpoint ) : \n    try : \n        response = requests . get ( endpoint ) \n    except requests . exceptions . RequestException : \n        raise GoldenCheetahNotAvailable ( endpoint ) \n    if response . text . startswith ( 'unknown athlete' ) : \n        match = re . match ( pattern = 'unknown athlete (?P<athlete>.+)' , string = response . text ) \n        raise AthleteDoesNotExist ( athlete = match . groupdict ( ) [ 'athlete' ] ) \n    else : \n        if response . text == 'file not found' : \n            match = re . match ( pattern = '.+/activity/(?P<filename>.+)' , string = endpoint ) \n            raise ActivityDoesNotExist ( filename = match . groupdict ( ) [ 'filename' ] ) \n    return response "}
{"13163": "\ndef url_with_auth ( regex , view , kwargs = None , name = None , prefix = '' ) : \n    from djapiauth . auth import api_auth \n    if isinstance ( view , six . string_types ) : \n        return url ( regex , api_auth ( import_by_path ( prefix + \".\" + view if prefix else view ) ) ) \n    else : \n        if isinstance ( view , ( list , tuple ) ) : \n            return url ( regex , view , name , prefix , ** kwargs ) \n        else : \n            return url ( regex , api_auth ( view ) ) "}
{"13169": "\ndef render ( self ) : \n    self . update ( self . axes . render ( ) ) \n    encoder = Encoder ( self . _encoding , None , self . _series ) \n    if not 'chs' in self : \n        self [ 'chs' ] = '300x150' \n    else : \n        size = self [ 'chs' ] . split ( 'x' ) \n        assert len ( size ) == 2 , 'Invalid size, must be in the format WxH' \n        self . check_size ( * map ( int , size ) ) \n    assert 'cht' in self , 'No chart type defined, use type method' \n    self [ 'cht' ] = self . check_type ( self [ 'cht' ] ) \n    if ( 'any' in dir ( self . _dataset ) and self . _dataset . any ( ) ) or self . _dataset : \n        self [ 'chd' ] = encoder . encode ( self . _dataset ) \n    else : \n        if not 'choe' in self : \n            assert 'chd' in self , 'You must have a dataset, or use chd' \n    if self . _scale : \n        assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' \n        self [ 'chds' ] = ',' . join ( self . _scale ) \n    if self . _geo and self . _ld : \n        self [ 'chtm' ] = self . _geo \n        self [ 'chld' ] = self . _ld \n    if self . lines : \n        self [ 'chls' ] = '|' . join ( self . lines ) \n    if self . markers : \n        self [ 'chm' ] = '|' . join ( self . markers ) \n    if self . fills : \n        self [ 'chf' ] = '|' . join ( self . fills ) "}
{"13200": "\ndef make_signing_service ( config , entity_id ) : \n    _args = dict ( [ ( k , v ) for k , v in config . items ( ) if k in KJ_SPECS ] ) \n    _kj = init_key_jar ( ** _args ) \n    if config [ 'type' ] == 'internal' : \n        signer = InternalSigningService ( entity_id , _kj ) \n    else : \n        if config [ 'type' ] == 'web' : \n            _kj . issuer_keys [ config [ 'iss' ] ] = _kj . issuer_keys [ '' ] \n            del _kj . issuer_keys [ '' ] \n            signer = WebSigningServiceClient ( config [ 'iss' ] , config [ 'url' ] , entity_id , _kj ) \n        else : \n            raise ValueError ( 'Unknown signer type: {}' . format ( config [ 'type' ] ) ) \n    return signer "}
{"13256": "\ndef assert_unit_convertability ( name , value , target_unit , unit_framework ) : \n    if unit_framework == ASTROPY : \n        from astropy . units import Quantity \n        if not isinstance ( value , Quantity ) : \n            raise TraitError ( \"{0} should be given as an Astropy Quantity instance\" . format ( name ) ) \n        if not target_unit . is_equivalent ( value . unit ) : \n            raise TraitError ( \"{0} should be in units convertible to {1}\" . format ( name , target_unit ) ) \n    else : \n        if unit_framework == PINT : \n            from pint . unit import UnitsContainer \n            if not ( hasattr ( value , 'dimensionality' ) and isinstance ( value . dimensionality , UnitsContainer ) ) : \n                raise TraitError ( \"{0} should be given as a Pint Quantity instance\" . format ( name ) ) \n            if value . dimensionality != target_unit . dimensionality : \n                raise TraitError ( \"{0} should be in units convertible to {1}\" . format ( name , target_unit ) ) \n        else : \n            if unit_framework == QUANTITIES : \n                from quantities import Quantity \n                if not isinstance ( value , Quantity ) : \n                    raise TraitError ( \"{0} should be given as a quantities Quantity instance\" . format ( name ) ) \n                if value . dimensionality . simplified != target_unit . dimensionality . simplified : \n                    raise TraitError ( \"{0} should be in units convertible to {1}\" . format ( name , target_unit . dimensionality . string ) ) "}
{"13257": "\ndef pad ( data_to_pad , block_size , style = 'pkcs7' ) : \n    padding_len = block_size - len ( data_to_pad ) % block_size \n    if style == 'pkcs7' : \n        padding = bchr ( padding_len ) * padding_len \n    else : \n        if style == 'x923' : \n            padding = bchr ( 0 ) * ( padding_len - 1 ) + bchr ( padding_len ) \n        else : \n            if style == 'iso7816' : \n                padding = bchr ( 128 ) + bchr ( 0 ) * ( padding_len - 1 ) \n            else : \n                raise ValueError ( \"Unknown padding style\" ) \n    return data_to_pad + padding "}
{"13258": "\ndef unpad ( padded_data , block_size , style = 'pkcs7' ) : \n    pdata_len = len ( padded_data ) \n    if pdata_len % block_size : \n        raise ValueError ( \"Input data is not padded\" ) \n    if style in ( 'pkcs7' , 'x923' ) : \n        padding_len = bord ( padded_data [ - 1 ] ) \n        if padding_len < 1 or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if style == 'pkcs7' : \n            if padded_data [ - padding_len : ] != bchr ( padding_len ) * padding_len : \n                raise ValueError ( \"PKCS#7 padding is incorrect.\" ) \n        else : \n            if padded_data [ - padding_len : - 1 ] != bchr ( 0 ) * ( padding_len - 1 ) : \n                raise ValueError ( \"ANSI X.923 padding is incorrect.\" ) \n    else : \n        if style == 'iso7816' : \n            padding_len = pdata_len - padded_data . rfind ( bchr ( 128 ) ) \n            if padding_len < 1 or padding_len > min ( block_size , pdata_len ) : \n                raise ValueError ( \"Padding is incorrect.\" ) \n            if padding_len > 1 and padded_data [ 1 - padding_len : ] != bchr ( 0 ) * ( padding_len - 1 ) : \n                raise ValueError ( \"ISO 7816-4 padding is incorrect.\" ) \n        else : \n            raise ValueError ( \"Unknown padding style\" ) \n    return padded_data [ : - padding_len ] "}
{"13272": "\ndef _parse_remote_response ( self , response ) : \n    try : \n        if response . headers [ \"Content-Type\" ] == 'application/json' : \n            logger . debug ( \"Loaded JWKS: %s from %s\" % ( response . text , self . source ) ) \n            try : \n                return json . loads ( response . text ) \n            except ValueError : \n                return None \n        else : \n            if response . headers [ \"Content-Type\" ] == 'application/jwt' : \n                logger . debug ( \"Signed JWKS: %s from %s\" % ( response . text , self . source ) ) \n                _jws = factory ( response . text ) \n                _resp = _jws . verify_compact ( response . text , keys = self . verify_keys . get_signing_key ( ) ) \n                return _resp \n            else : \n                logger . error ( 'Wrong content type: {}' . format ( response . headers [ 'Content-Type' ] ) ) \n                raise ValueError ( 'Content-type mismatch' ) \n    except KeyError : \n        pass "}
{"13289": "\ndef word_list ( sowpods = False , start = \"\" , end = \"\" ) : \n    location = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \"wordlists\" , ) \n    if sowpods : \n        filename = \"sowpods.txt\" \n    else : \n        filename = \"twl.txt\" \n    filepath = os . path . join ( location , filename ) \n    with open ( filepath ) as wordfile : \n        for word in wordfile . readlines ( ) : \n            word = word . strip ( ) \n            if start and end and word . startswith ( start ) and word . endswith ( end ) : \n                yield word \n            else : \n                if start and word . startswith ( start ) and not end : \n                    yield word \n                else : \n                    if end and word . endswith ( end ) and not start : \n                        yield word \n                    else : \n                        if not start and not end : \n                            yield word "}
{"13330": "\ndef call_function ( self , c , i ) : \n    callable_ = self . __stack [ - 1 - i . arg ] \n    args = tuple ( self . __stack [ len ( self . __stack ) - i . arg : ] ) \n    self . _print ( 'call function' ) \n    self . _print ( '\\tfunction ' , callable_ ) \n    self . _print ( '\\ti.arg    ' , i . arg ) \n    self . _print ( '\\targs     ' , args ) \n    self . call_callbacks ( 'CALL_FUNCTION' , callable_ , * args ) \n    if isinstance ( callable_ , FunctionType ) : \n        ret = callable_ ( * args ) \n    else : \n        if callable_ is builtins . __build_class__ : \n            ret = self . build_class ( callable_ , args ) \n        else : \n            if callable_ is builtins . globals : \n                ret = self . builtins_globals ( ) \n            else : \n                ret = callable_ ( * args ) \n    self . pop ( 1 + i . arg ) \n    self . __stack . append ( ret ) "}
{"13336": "\ndef copytree ( src , dst , symlinks = True ) : \n    from shutil import copy2 , Error , copystat \n    names = os . listdir ( src ) \n    if not Path ( dst ) . exists ( ) : \n        os . makedirs ( dst ) \n    errors = [ ] \n    for name in names : \n        srcname = os . path . join ( src , name ) \n        dstname = os . path . join ( dst , name ) \n        try : \n            if symlinks and os . path . islink ( srcname ) : \n                linkto = os . readlink ( srcname ) \n                os . symlink ( linkto , dstname ) \n            else : \n                if os . path . isdir ( srcname ) : \n                    copytree ( srcname , dstname , symlinks ) \n                else : \n                    copy2 ( srcname , dstname ) \n        except OSError as why : \n            errors . append ( ( srcname , dstname , str ( why ) ) ) \n        except Error as err : \n            errors . extend ( err . args [ 0 ] ) \n    try : \n        copystat ( src , dst ) \n    except OSError as why : \n        if why . winerror is None : \n            errors . extend ( ( src , dst , str ( why ) ) ) \n    if errors : \n        raise Error ( errors ) "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ 1 : ] \n    if takes_context : \n        if 'context' in params [ : 1 ] : \n            params = params [ 1 : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = False \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if len ( bits ) > 1 and bits [ - 2 ] == 'as' : \n        output_var = bits [ - 1 ] \n        if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > 0 : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                if kw in handled_params : \n                    raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n                else : \n                    kwargs [ str ( kw ) ] = var \n                    kwarg_found = True \n                    handled_params . append ( kw ) \n        else : \n            if kwarg_found : \n                raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n            else : \n                args . append ( bit ) \n                try : \n                    handled_params . append ( unhandled_params . pop ( 0 ) ) \n                except IndexError : \n                    if not takes_var_args : \n                        raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if len ( unhandled_params ) == 1 : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ 0 ] ) ) \n    else : \n        if len ( unhandled_params ) > 1 : \n            raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13349": "\ndef find_symbol ( self , name = None , kind = None ) : \n    for s in reversed ( self . stack ) : \n        for symbol_name , handle in s . symbols . items ( ) : \n            symbol_kind = handle . __class__ . __name__ \n            if name == symbol_name and kind == symbol_kind : \n                return handle \n            else : \n                if name is None and kind == handle . __class__ . __name__ : \n                    return handle \n                else : \n                    if name == symbol_name and kind is None : \n                        return handle \n        if name is None and kind == s . handle . __class__ . __name__ : \n            return s . handle "}
{"13350": "\ndef is_contained_in ( pe_pe , root ) : \n    if not pe_pe : \n        return False \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    ep_pkg = one ( pe_pe ) . EP_PKG [ 8000 ] ( ) \n    c_c = one ( pe_pe ) . C_C [ 8003 ] ( ) \n    if root in [ ep_pkg , c_c ] : \n        return True \n    else : \n        if is_contained_in ( ep_pkg , root ) : \n            return True \n        else : \n            if is_contained_in ( c_c , root ) : \n                return True \n            else : \n                return False "}
{"13361": "\ndef mk_class ( m , o_obj , derived_attributes = False ) : \n    first_filter = lambda selected : not one ( selected ) . O_ATTR [ 103 , 'succeeds' ] ( ) \n    o_attr = one ( o_obj ) . O_ATTR [ 102 ] ( first_filter ) \n    attributes = list ( ) \n    while o_attr : \n        s_dt = get_attribute_type ( o_attr ) \n        ty = _get_data_type_name ( s_dt ) \n        if not derived_attributes and one ( o_attr ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n            pass \n        else : \n            if not ty : \n                logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o_obj . Key_Lett , o_attr . Name ) ) \n            else : \n                attributes . append ( ( o_attr . Name , ty ) ) \n        o_attr = one ( o_attr ) . O_ATTR [ 103 , 'precedes' ] ( ) \n    metaclass = m . define_class ( o_obj . Key_Lett , list ( attributes ) , o_obj . Descrip ) \n    for o_id in many ( o_obj ) . O_ID [ 104 ] ( ) : \n        o_oida = many ( o_id ) . O_OIDA [ 105 ] ( ) \n        o_attrs = many ( o_oida ) . O_ATTR [ 105 ] ( ) \n        if not derived_attributes and one ( o_attrs ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n            logger . warning ( 'Omitting unique identifier %s.I%d' % ( o_obj . Key_Lett , o_id . Oid_ID + 1 ) ) \n            continue \n        names = [ o_attr . Name for o_attr in o_attrs ] \n        m . define_unique_identifier ( o_obj . Key_Lett , o_id . Oid_ID + 1 , * names ) \n    for o_tfr in many ( o_obj ) . O_TFR [ 115 ] ( ) : \n        fn = mk_operation ( metaclass , o_tfr ) \n        setattr ( metaclass . clazz , o_tfr . Name , fn ) \n    for o_dbattr in many ( o_obj ) . O_ATTR [ 102 ] . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n        o_attr = one ( o_dbattr ) . O_BATTR [ 107 ] . O_ATTR [ 106 ] ( ) \n        fn = mk_derived_attribute ( metaclass , o_dbattr ) \n        setattr ( metaclass . clazz , o_attr . Name , fn ) \n    return metaclass "}
{"13372": "\ndef dispatch_reply ( self , reply , value ) : \n    method = reply . method \n    call_id = reply . call_id \n    task_id = reply . task_id \n    if method & ACK : \n        try : \n            result_queue = self . result_queues [ call_id ] \n        except KeyError : \n            raise KeyError ( 'already established or unprepared call' ) \n        if method == ACCEPT : \n            worker_info = value \n            result = RemoteResult ( self , call_id , task_id , worker_info ) \n            self . results [ call_id ] [ task_id ] = result \n            result_queue . put_nowait ( result ) \n        else : \n            if method == REJECT : \n                result_queue . put_nowait ( None ) \n    else : \n        result = self . results [ call_id ] [ task_id ] \n        result . set_reply ( reply . method , value ) "}
{"13373": "\ndef guess_type_name ( value ) : \n    value = str ( value ) \n    if value . upper ( ) in [ 'TRUE' , 'FALSE' ] : \n        return 'BOOLEAN' \n    else : \n        if re . match ( r'(-)?(\\d+)(\\.\\d+)' , value ) : \n            return 'REAL' \n        else : \n            if re . match ( r'(-)?(\\d+)' , value ) : \n                return 'INTEGER' \n            else : \n                if re . match ( r'\\'((\\'\\')|[^\\'])*\\'' , value ) : \n                    return 'STRING' \n                else : \n                    if re . match ( r'\\\"([^\\\\\\n]|(\\\\.))*?\\\"' , value ) : \n                        return 'UNIQUE_ID' "}
{"13374": "\ndef deserialize_value ( ty , value ) : \n    uty = ty . upper ( ) \n    if uty == 'BOOLEAN' : \n        if value . isdigit ( ) : \n            return bool ( int ( value ) ) \n        else : \n            if value . upper ( ) == 'FALSE' : \n                return False \n            else : \n                if value . upper ( ) == 'TRUE' : \n                    return True \n                else : \n                    return None \n    else : \n        if uty == 'INTEGER' : \n            if '\"' in value : \n                return uuid . UUID ( value [ 1 : - 1 ] ) . int \n            else : \n                return int ( value ) \n        else : \n            if uty == 'REAL' : \n                return float ( value ) \n            else : \n                if uty == 'STRING' : \n                    return value [ 1 : - 1 ] . replace ( \"''\" , \"'\" ) \n                else : \n                    if uty == 'UNIQUE_ID' : \n                        if '\"' in value : \n                            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n                        else : \n                            return int ( value ) "}
{"13392": "\ndef _source ( self , feature_names ) : \n    if feature_names is None : \n        return True \n    else : \n        if isinstance ( feature_names , bool ) : \n            return feature_names \n        else : \n            return map ( lambda n : 'fc.' + n , feature_names ) "}
{"13393": "\ndef _range_filters ( self , * key_ranges ) : \n    filters = [ ] \n    for s , e in key_ranges : \n        if isinstance ( s , basestring ) : \n            s = eid ( s ) \n        if isinstance ( e , basestring ) : \n            e += u'\\U0010FFFF' \n            e = eid ( e ) \n        if s == ( ) and e == ( ) : \n            filters . append ( { 'match_all' : { } } ) \n        else : \n            if e == ( ) : \n                filters . append ( { 'range' : { '_id' : { 'gte' : s } } } ) \n            else : \n                if s == ( ) : \n                    filters . append ( { 'range' : { '_id' : { 'lte' : e } } } ) \n                else : \n                    filters . append ( { 'range' : { '_id' : { 'gte' : s , 'lte' : e } } } ) \n    if len ( filters ) == 0 : \n        return [ { 'match_all' : { } } ] \n    else : \n        return filters "}
{"13424": "\ndef check_pypi_name ( pypi_package_name , pypi_registry_host = None ) : \n    if pypi_registry_host is None : \n        pypi_registry_host = 'pypi.python.org' \n    receive_buffer = bytearray ( b'------------' ) \n    context = ssl . create_default_context ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD /simple/\" , pypi_package_name . encode ( 'ascii' ) , b\"/ HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return True \n    else : \n        if b'HTTP/1.1 404' in receive_buffer : \n            ssl_http_socket . shutdown ( 1 ) \n            ssl_http_socket . close ( ) \n            return False \n    remaining_bytes = ssl_http_socket . recv ( 2048 ) \n    redirect_path_location_start = remaining_bytes . find ( b'Location:' ) + 10 \n    redirect_path_location_end = remaining_bytes . find ( b'\\r\\n' , redirect_path_location_start ) \n    redirect_path = remaining_bytes [ redirect_path_location_start : redirect_path_location_end ] + b'/' \n    ssl_http_socket . shutdown ( 1 ) \n    ssl_http_socket . close ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD \" , redirect_path , b\" HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        return True \n    else : \n        if b'HTTP/1.1 404' in receive_buffer : \n            return False \n        else : \n            NotImplementedError ( 'A definitive answer was not found by primary or secondary lookups.' ) "}
{"13425": "\ndef add_direction ( value , arg = u\"rtl_only\" ) : \n    if arg == u'rtl_only' : \n        directions = ( u'' , u'_rtl' ) \n    else : \n        if arg == u'both' : \n            directions = ( u'_ltr' , u'_rtl' ) \n        else : \n            if arg == u'ltr_only' : \n                directions = ( u'_ltr' , u'' ) \n            else : \n                raise template . TemplateSyntaxError ( 'add_direction can use arg with one of [\"rtl_only\", \"both\", \"ltr_only\"]' ) \n    parts = value . rsplit ( '.' , 1 ) \n    if not len ( parts ) : \n        return value \n    else : \n        if len ( parts ) == 1 : \n            return value + directions [ translation . get_language_bidi ( ) ] \n        else : \n            return '.' . join ( ( parts [ 0 ] + directions [ translation . get_language_bidi ( ) ] , parts [ 1 ] ) ) "}
{"13428": "\ndef build_core_type ( s_cdt ) : \n    s_dt = nav_one ( s_cdt ) . S_DT [ 17 ] ( ) \n    if s_dt . name == 'void' : \n        type_name = None \n    else : \n        if s_dt . name == 'boolean' : \n            type_name = 'xs:boolean' \n        else : \n            if s_dt . name == 'integer' : \n                type_name = 'xs:integer' \n            else : \n                if s_dt . name == 'real' : \n                    type_name = 'xs:decimal' \n                else : \n                    if s_dt . name == 'string' : \n                        type_name = 'xs:string' \n                    else : \n                        if s_dt . name == 'unique_id' : \n                            type_name = 'xs:integer' \n                        else : \n                            type_name = None \n    if type_name : \n        mapped_type = ET . Element ( 'xs:simpleType' , name = s_dt . name ) \n        ET . SubElement ( mapped_type , 'xs:restriction' , base = type_name ) \n        return mapped_type "}
{"13468": "\ndef run ( locations , random , bikes , crime , nearby , json , update_bikes , api_server , cross_origin , host , port , db_path , verbose ) : \n    log_levels = [ logging . WARNING , logging . INFO , logging . DEBUG ] \n    logging . basicConfig ( level = log_levels [ min ( verbose , 2 ) ] ) \n    initialize_database ( db_path ) \n    loop = get_event_loop ( ) \n    if update_bikes : \n        logger . info ( \"Force updating bikes.\" ) \n        loop . run_until_complete ( util . update_bikes ( ) ) \n    if api_server : \n        if cross_origin : \n            enable_cross_origin ( app ) \n        try : \n            web . run_app ( app , host = host , port = port ) \n        except CancelledError as e : \n            if e . __context__ is not None : \n                click . echo ( Fore . RED + ( f\"Could not bind to address {host}:{port}\" if e . __context__ . errno == 48 else e . __context__ ) ) \n                exit ( 1 ) \n            else : \n                click . echo ( \"Goodbye!\" ) \n    else : \n        if len ( locations ) > 0 or random > 0 : \n            exit ( loop . run_until_complete ( cli ( locations , random , bikes = bikes , crime = crime , nearby = nearby , as_json = json ) ) ) \n        else : \n            click . echo ( Fore . RED + \"Either include a post code, or the --api-server flag.\" ) "}
{"13484": "\nasync def normalize_postcode_middleware ( request , handler ) : \n    postcode : Optional [ str ] = request . match_info . get ( 'postcode' , None ) \n    if postcode is None or postcode == \"random\" : \n        return await handler ( request ) \n    else : \n        if not is_uk_postcode ( postcode ) : \n            raise web . HTTPNotFound ( text = \"Invalid Postcode\" ) \n    postcode_processed = postcode . upper ( ) . replace ( \" \" , \"\" ) \n    if postcode_processed == postcode : \n        return await handler ( request ) \n    else : \n        url_name = request . match_info . route . name \n        url = request . app . router [ url_name ] \n        params = dict ( request . match_info ) \n        params [ 'postcode' ] = postcode_processed \n        raise web . HTTPMovedPermanently ( str ( url . url_for ( ** params ) ) ) "}
{"13491": "\ndef _find_match ( self , position ) : \n    document = self . _text_edit . document ( ) \n    start_char = document . characterAt ( position ) \n    search_char = self . _opening_map . get ( start_char ) \n    if search_char : \n        increment = 1 \n    else : \n        search_char = self . _closing_map . get ( start_char ) \n        if search_char : \n            increment = - 1 \n        else : \n            return - 1 \n    char = start_char \n    depth = 0 \n    while position >= 0 and position < document . characterCount ( ) : \n        if char == start_char : \n            depth += 1 \n        else : \n            if char == search_char : \n                depth -= 1 \n        if depth == 0 : \n            break \n        position += increment \n        char = document . characterAt ( position ) \n    else : \n        position = - 1 \n    return position "}
{"13501": "\ndef call ( self , url , method = None , args = None ) : \n    if not args : \n        args = { } \n    if sys . version_info . major == 3 : \n        data = urllib . parse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urllib . parse . parse_qs ( data . query , keep_blank_values = True ) ) \n    else : \n        if sys . version_info . major == 2 : \n            data = urlparse . urlparse ( url ) \n            path = data . path . rstrip ( '/' ) + '/' \n            _args = dict ( urlparse . parse_qs ( data . query , keep_blank_values = True ) ) \n    for elem in self . _data_store : \n        pattern = elem [ 'pattern' ] \n        function = elem [ 'function' ] \n        _method = elem [ 'method' ] \n        type_cast = elem [ 'type_cast' ] \n        result = re . match ( pattern , path ) \n        if result and _method == method : \n            _args = dict ( _args , ** result . groupdict ( ) ) \n            for key , val in _args . items ( ) : \n                if isinstance ( _args [ key ] , list ) and len ( _args [ key ] ) == 1 : \n                    _args [ key ] = _args [ key ] [ 0 ] \n            for key , val in type_cast . items ( ) : \n                if key not in _args : \n                    continue \n                if not _args [ key ] : \n                    continue \n                if isinstance ( _args [ key ] , list ) : \n                    for i , _val in enumerate ( _args [ key ] ) : \n                        _args [ key ] [ i ] = self . _cast ( _val , val ) \n                else : \n                    _args [ key ] = self . _cast ( _args [ key ] , val ) \n            requiered_args = self . _get_function_args ( function ) \n            for key , val in args . items ( ) : \n                if key in requiered_args : \n                    _args [ key ] = val \n            return function ( ** _args ) \n    return None "}
{"13509": "\ndef _get_edited_history ( self , index ) : \n    if index in self . _history_edits : \n        return self . _history_edits [ index ] \n    else : \n        if index == len ( self . _history ) : \n            return unicode ( ) \n    return self . _history [ index ] "}
{"13521": "\ndef convert_to_this_nbformat ( nb , orig_version = 1 ) : \n    if orig_version == 1 : \n        newnb = new_notebook ( ) \n        ws = new_worksheet ( ) \n        for cell in nb . cells : \n            if cell . cell_type == u'code' : \n                newcell = new_code_cell ( input = cell . get ( 'code' ) , prompt_number = cell . get ( 'prompt_number' ) ) \n            else : \n                if cell . cell_type == u'text' : \n                    newcell = new_text_cell ( u'markdown' , source = cell . get ( 'text' ) ) \n            ws . cells . append ( newcell ) \n        newnb . worksheets . append ( ws ) \n        return newnb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v2' % orig_version ) "}
{"13533": "\ndef parse_filename ( fname ) : \n    if fname . endswith ( u'.ipynb' ) : \n        format = u'json' \n    else : \n        if fname . endswith ( u'.json' ) : \n            format = u'json' \n        else : \n            if fname . endswith ( u'.py' ) : \n                format = u'py' \n            else : \n                fname = fname + u'.ipynb' \n                format = u'json' \n    name = fname . split ( '.' ) [ 0 ] \n    return fname , name , format "}
{"13543": "\ndef _stdin_raw_nonblock ( self ) : \n    handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) \n    result = WaitForSingleObject ( handle , 100 ) \n    if result == WAIT_FAILED : \n        raise ctypes . WinError ( ) \n    else : \n        if result == WAIT_TIMEOUT : \n            print ( \".\" , end = '' ) \n            return None \n        else : \n            data = ctypes . create_string_buffer ( 256 ) \n            bytesRead = DWORD ( 0 ) \n            print ( '?' , end = '' ) \n            if not ReadFile ( handle , data , 256 , ctypes . byref ( bytesRead ) , None ) : \n                raise ctypes . WinError ( ) \n            FlushConsoleInputBuffer ( handle ) \n            data = data . value \n            data = data . replace ( '\\r\\n' , '\\n' ) \n            data = data . replace ( '\\r' , '\\n' ) \n            print ( repr ( data ) + \" \" , end = '' ) \n            return data "}
{"13569": "\ndef handle_iopub ( self ) : \n    while self . km . sub_channel . msg_ready ( ) : \n        sub_msg = self . km . sub_channel . get_msg ( ) \n        msg_type = sub_msg [ 'header' ] [ 'msg_type' ] \n        parent = sub_msg [ \"parent_header\" ] \n        if ( not parent ) or self . session_id == parent [ 'session' ] : \n            if msg_type == 'status' : \n                if sub_msg [ \"content\" ] [ \"execution_state\" ] == \"busy\" : \n                    pass \n            else : \n                if msg_type == 'stream' : \n                    if sub_msg [ \"content\" ] [ \"name\" ] == \"stdout\" : \n                        print ( sub_msg [ \"content\" ] [ \"data\" ] , file = io . stdout , end = \"\" ) \n                        io . stdout . flush ( ) \n                    else : \n                        if sub_msg [ \"content\" ] [ \"name\" ] == \"stderr\" : \n                            print ( sub_msg [ \"content\" ] [ \"data\" ] , file = io . stderr , end = \"\" ) \n                            io . stderr . flush ( ) \n                else : \n                    if msg_type == 'pyout' : \n                        self . execution_count = int ( sub_msg [ \"content\" ] [ \"execution_count\" ] ) \n                        format_dict = sub_msg [ \"content\" ] [ \"data\" ] \n                        hook = self . displayhook \n                        hook . start_displayhook ( ) \n                        hook . write_output_prompt ( ) \n                        hook . write_format_data ( format_dict ) \n                        hook . log_output ( format_dict ) \n                        hook . finish_displayhook ( ) "}
{"13575": "\ndef _get_format_from_style ( self , token , style ) : \n    result = QtGui . QTextCharFormat ( ) \n    for key , value in style . style_for_token ( token ) . items ( ) : \n        if value : \n            if key == 'color' : \n                result . setForeground ( self . _get_brush ( value ) ) \n            else : \n                if key == 'bgcolor' : \n                    result . setBackground ( self . _get_brush ( value ) ) \n                else : \n                    if key == 'bold' : \n                        result . setFontWeight ( QtGui . QFont . Bold ) \n                    else : \n                        if key == 'italic' : \n                            result . setFontItalic ( True ) \n                        else : \n                            if key == 'underline' : \n                                result . setUnderlineStyle ( QtGui . QTextCharFormat . SingleUnderline ) \n                            else : \n                                if key == 'sans' : \n                                    result . setFontStyleHint ( QtGui . QFont . SansSerif ) \n                                else : \n                                    if key == 'roman' : \n                                        result . setFontStyleHint ( QtGui . QFont . Times ) \n                                    else : \n                                        if key == 'mono' : \n                                            result . setFontStyleHint ( QtGui . QFont . TypeWriter ) \n    return result "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if inisp < indent_spaces : \n        indent_spaces = inisp \n        if indent_spaces <= 0 : \n            full_dedent = True \n    if line . rstrip ( ) [ - 1 ] == ':' : \n        indent_spaces += 4 \n    else : \n        if dedent_re . match ( line ) : \n            indent_spaces -= 4 \n            if indent_spaces <= 0 : \n                full_dedent = True \n    if indent_spaces < 0 : \n        indent_spaces = 0 \n    return indent_spaces , full_dedent "}
{"13600": "\ndef _update_status ( self ) : \n    srun , scomp , sdead = self . _s_running , self . _s_completed , self . _s_dead \n    running , completed , dead = self . _running , self . _completed , self . _dead \n    for num , job in enumerate ( running ) : \n        stat = job . stat_code \n        if stat == srun : \n            continue \n        else : \n            if stat == scomp : \n                completed . append ( job ) \n                self . _comp_report . append ( job ) \n                running [ num ] = False \n            else : \n                if stat == sdead : \n                    dead . append ( job ) \n                    self . _dead_report . append ( job ) \n                    running [ num ] = False \n    running [ : ] = filter ( None , running ) "}
{"13629": "\ndef model_verbose ( obj , capitalize = True ) : \n    if isinstance ( obj , ModelForm ) : \n        name = obj . _meta . model . _meta . verbose_name \n    else : \n        if isinstance ( obj , Model ) : \n            name = obj . _meta . verbose_name \n        else : \n            raise Exception ( 'Unhandled type: ' + type ( obj ) ) \n    return name . capitalize ( ) if capitalize else name "}
{"13642": "\ndef help ( self , error = None , topic = None , parser = None ) : \n    assert error or topic or parser \n    if error : \n        print ( error ) \n        print ( \"Use 'coverage help' for help.\" ) \n    else : \n        if parser : \n            print ( parser . format_help ( ) . strip ( ) ) \n        else : \n            help_msg = HELP_TOPICS . get ( topic , '' ) . strip ( ) \n            if help_msg : \n                print ( help_msg % self . covpkg . __dict__ ) \n            else : \n                print ( \"Don't know topic %r\" % topic ) "}
{"13646": "\ndef do_debug ( self , args ) : \n    if not args : \n        self . help_fn ( \"What information would you like: data, sys?\" ) \n        return ERR \n    for info in args : \n        if info == 'sys' : \n            print ( \"-- sys ----------------------------------------\" ) \n            for line in info_formatter ( self . coverage . sysinfo ( ) ) : \n                print ( \" %s\" % line ) \n        else : \n            if info == 'data' : \n                print ( \"-- data ---------------------------------------\" ) \n                self . coverage . load ( ) \n                print ( \"path: %s\" % self . coverage . data . filename ) \n                print ( \"has_arcs: %r\" % self . coverage . data . has_arcs ( ) ) \n                summary = self . coverage . data . summary ( fullpath = True ) \n                if summary : \n                    filenames = sorted ( summary . keys ( ) ) \n                    print ( \"\\n%d files:\" % len ( filenames ) ) \n                    for f in filenames : \n                        print ( \"%s: %d lines\" % ( f , summary [ f ] ) ) \n                else : \n                    print ( \"No data collected\" ) \n            else : \n                self . help_fn ( \"Don't know what you mean by %r\" % info ) \n                return ERR \n    return OK "}
{"13647": "\ndef unserialize_object ( bufs ) : \n    bufs = list ( bufs ) \n    sobj = pickle . loads ( bufs . pop ( 0 ) ) \n    if isinstance ( sobj , ( list , tuple ) ) : \n        for s in sobj : \n            if s . data is None : \n                s . data = bufs . pop ( 0 ) \n        return uncanSequence ( map ( unserialize , sobj ) ) , bufs \n    else : \n        if isinstance ( sobj , dict ) : \n            newobj = { } \n            for k in sorted ( sobj . iterkeys ( ) ) : \n                s = sobj [ k ] \n                if s . data is None : \n                    s . data = bufs . pop ( 0 ) \n                newobj [ k ] = uncan ( unserialize ( s ) ) \n            return newobj , bufs \n        else : \n            if sobj . data is None : \n                sobj . data = bufs . pop ( 0 ) \n            return uncan ( unserialize ( sobj ) ) , bufs "}
{"13652": "\ndef validate_url_container ( container ) : \n    if isinstance ( container , basestring ) : \n        url = container \n        return validate_url ( url ) \n    else : \n        if isinstance ( container , dict ) : \n            container = container . itervalues ( ) \n    for element in container : \n        validate_url_container ( element ) "}
{"13660": "\ndef init_logstart ( self ) : \n    if self . logappend : \n        self . magic ( 'logstart %s append' % self . logappend ) \n    else : \n        if self . logfile : \n            self . magic ( 'logstart %s' % self . logfile ) \n        else : \n            if self . logstart : \n                self . magic ( 'logstart' ) "}
{"13671": "\ndef push ( self , variables , interactive = True ) : \n    vdict = None \n    if isinstance ( variables , dict ) : \n        vdict = variables \n    else : \n        if isinstance ( variables , ( basestring , list , tuple ) ) : \n            if isinstance ( variables , basestring ) : \n                vlist = variables . split ( ) \n            else : \n                vlist = variables \n            vdict = { } \n            cf = sys . _getframe ( 1 ) \n            for name in vlist : \n                try : \n                    vdict [ name ] = eval ( name , cf . f_globals , cf . f_locals ) \n                except : \n                    print ( 'Could not get variable %s from %s' % ( name , cf . f_code . co_name ) ) \n        else : \n            raise ValueError ( 'variables must be a dict/str/list/tuple' ) \n    self . user_ns . update ( vdict ) \n    user_ns_hidden = self . user_ns_hidden \n    if interactive : \n        user_ns_hidden . difference_update ( vdict ) \n    else : \n        user_ns_hidden . update ( vdict ) "}
{"13672": "\ndef _ofind ( self , oname , namespaces = None ) : \n    oname = oname . strip ( ) \n    if not oname . startswith ( ESC_MAGIC ) and not oname . startswith ( ESC_MAGIC2 ) and not py3compat . isidentifier ( oname , dotted = True ) : \n        return dict ( found = False ) \n    alias_ns = None \n    if namespaces is None : \n        namespaces = [ ( 'Interactive' , self . user_ns ) , ( 'Interactive (global)' , self . user_global_ns ) , ( 'Python builtin' , builtin_mod . __dict__ ) , ( 'Alias' , self . alias_manager . alias_table ) , ] \n        alias_ns = self . alias_manager . alias_table \n    found = False ; \n    obj = None ; \n    ospace = None ; \n    ds = None ; \n    ismagic = False ; \n    isalias = False ; \n    parent = None \n    if ( oname == 'print' and not py3compat . PY3 and not ( self . compile . compiler_flags & __future__ . CO_FUTURE_PRINT_FUNCTION ) ) : \n        return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n    oname_parts = oname . split ( '.' ) \n    oname_head , oname_rest = oname_parts [ 0 ] , oname_parts [ 1 : ] \n    for nsname , ns in namespaces : \n        try : \n            obj = ns [ oname_head ] \n        except KeyError : \n            continue \n        else : \n            for part in oname_rest : \n                try : \n                    parent = obj \n                    obj = getattr ( obj , part ) \n                except : \n                    break \n            else : \n                found = True \n                ospace = nsname \n                if ns == alias_ns : \n                    isalias = True \n                break \n    if not found : \n        obj = None \n        if oname . startswith ( ESC_MAGIC2 ) : \n            oname = oname . lstrip ( ESC_MAGIC2 ) \n            obj = self . find_cell_magic ( oname ) \n        else : \n            if oname . startswith ( ESC_MAGIC ) : \n                oname = oname . lstrip ( ESC_MAGIC ) \n                obj = self . find_line_magic ( oname ) \n            else : \n                obj = self . find_line_magic ( oname ) \n                if obj is None : \n                    obj = self . find_cell_magic ( oname ) \n        if obj is not None : \n            found = True \n            ospace = 'IPython internal' \n            ismagic = True \n    if not found and oname_head in [ \"''\" , '\"\"' , '[]' , '{}' , '()' ] : \n        obj = eval ( oname_head ) \n        found = True \n        ospace = 'Interactive' \n    return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } "}
{"13678": "\ndef showtraceback ( self , exc_tuple = None , filename = None , tb_offset = None , exception_only = False ) : \n    try : \n        try : \n            etype , value , tb = self . _get_exc_info ( exc_tuple ) \n        except ValueError : \n            self . write_err ( 'No traceback available to show.\\n' ) \n            return \n        if etype is SyntaxError : \n            self . showsyntaxerror ( filename ) \n        else : \n            if etype is UsageError : \n                self . write_err ( \"UsageError: %s\" % value ) \n            else : \n                if exception_only : \n                    stb = [ 'An exception has occurred, use %tb to see ' 'the full traceback.\\n' ] \n                    stb . extend ( self . InteractiveTB . get_exception_only ( etype , value ) ) \n                else : \n                    try : \n                        stb = value . _render_traceback_ ( ) \n                    except Exception : \n                        stb = self . InteractiveTB . structured_traceback ( etype , value , tb , tb_offset = tb_offset ) \n                    self . _showtraceback ( etype , value , stb ) \n                    if self . call_pdb : \n                        self . debugger ( force = True ) \n                    return \n                self . _showtraceback ( etype , value , stb ) \n    except KeyboardInterrupt : \n        self . write_err ( \"\\nKeyboardInterrupt\\n\" ) "}
{"13696": "\ndef run_ast_nodes ( self , nodelist , cell_name , interactivity = 'last_expr' ) : \n    if not nodelist : \n        return \n    if interactivity == 'last_expr' : \n        if isinstance ( nodelist [ - 1 ] , ast . Expr ) : \n            interactivity = \"last\" \n        else : \n            interactivity = \"none\" \n    if interactivity == 'none' : \n        to_run_exec , to_run_interactive = nodelist , [ ] \n    else : \n        if interactivity == 'last' : \n            to_run_exec , to_run_interactive = nodelist [ : - 1 ] , nodelist [ - 1 : ] \n        else : \n            if interactivity == 'all' : \n                to_run_exec , to_run_interactive = [ ] , nodelist \n            else : \n                raise ValueError ( \"Interactivity was %r\" % interactivity ) \n    exec_count = self . execution_count \n    try : \n        for i , node in enumerate ( to_run_exec ) : \n            mod = ast . Module ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"exec\" ) \n            if self . run_code ( code ) : \n                return True \n        for i , node in enumerate ( to_run_interactive ) : \n            mod = ast . Interactive ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"single\" ) \n            if self . run_code ( code ) : \n                return True \n        if softspace ( sys . stdout , 0 ) : \n            print \n    except : \n        self . showtraceback ( ) \n    return False "}
{"13701": "\ndef find_user_code ( self , target , raw = True , py_only = False ) : \n    code = self . extract_input_lines ( target , raw = raw ) \n    if code : \n        return code \n    utarget = unquote_filename ( target ) \n    try : \n        if utarget . startswith ( ( 'http://' , 'https://' ) ) : \n            return openpy . read_py_url ( utarget , skip_encoding_cookie = True ) \n    except UnicodeDecodeError : \n        if not py_only : \n            response = urllib . urlopen ( target ) \n            return response . read ( ) . decode ( 'latin1' ) \n        raise ValueError ( ( \"'%s' seem to be unreadable.\" ) % utarget ) \n    potential_target = [ target ] \n    try : \n        potential_target . insert ( 0 , get_py_filename ( target ) ) \n    except IOError : \n        pass \n    for tgt in potential_target : \n        if os . path . isfile ( tgt ) : \n            try : \n                return openpy . read_py_file ( tgt , skip_encoding_cookie = True ) \n            except UnicodeDecodeError : \n                if not py_only : \n                    with io_open ( tgt , 'r' , encoding = 'latin1' ) as f : \n                        return f . read ( ) \n                raise ValueError ( ( \"'%s' seem to be unreadable.\" ) % target ) \n    try : \n        codeobj = eval ( target , self . user_ns ) \n    except Exception : \n        raise ValueError ( ( \"'%s' was not found in history, as a file, url, \" \"nor in the user namespace.\" ) % target ) \n    if isinstance ( codeobj , basestring ) : \n        return codeobj \n    else : \n        if isinstance ( codeobj , Macro ) : \n            return codeobj . value \n    raise TypeError ( \"%s is neither a string nor a macro.\" % target , codeobj ) "}
{"13737": "\ndef _get_pc_covered_str ( self ) : \n    pc = self . pc_covered \n    if 0 < pc < self . _near0 : \n        pc = self . _near0 \n    else : \n        if self . _near100 < pc < 100 : \n            pc = self . _near100 \n        else : \n            pc = round ( pc , self . _precision ) \n    return \"%.*f\" % ( self . _precision , pc ) "}
{"13757": "\ndef _default_pprint ( obj , p , cycle ) : \n    klass = getattr ( obj , '__class__' , None ) or type ( obj ) \n    if getattr ( klass , '__repr__' , None ) not in _baseclass_reprs : \n        p . text ( repr ( obj ) ) \n        return \n    p . begin_group ( 1 , '<' ) \n    p . pretty ( klass ) \n    p . text ( ' at 0x%x' % id ( obj ) ) \n    if cycle : \n        p . text ( ' ...' ) \n    else : \n        if p . verbose : \n            first = True \n            for key in dir ( obj ) : \n                if not key . startswith ( '_' ) : \n                    try : \n                        value = getattr ( obj , key ) \n                    except AttributeError : \n                        continue \n                    if isinstance ( value , types . MethodType ) : \n                        continue \n                    if not first : \n                        p . text ( ',' ) \n                    p . breakable ( ) \n                    p . text ( key ) \n                    p . text ( '=' ) \n                    step = len ( key ) + 1 \n                    p . indentation += step \n                    p . pretty ( value ) \n                    p . indentation -= step \n                    first = False \n    p . end_group ( 1 , '>' ) "}
{"13779": "\ndef extract_dates ( obj ) : \n    if isinstance ( obj , dict ) : \n        obj = dict ( obj ) \n        for k , v in obj . iteritems ( ) : \n            obj [ k ] = extract_dates ( v ) \n    else : \n        if isinstance ( obj , ( list , tuple ) ) : \n            obj = [ extract_dates ( o ) for o in obj ] \n        else : \n            if isinstance ( obj , basestring ) : \n                if ISO8601_PAT . match ( obj ) : \n                    obj = datetime . strptime ( obj , ISO8601 ) \n    return obj "}
{"13780": "\ndef squash_dates ( obj ) : \n    if isinstance ( obj , dict ) : \n        obj = dict ( obj ) \n        for k , v in obj . iteritems ( ) : \n            obj [ k ] = squash_dates ( v ) \n    else : \n        if isinstance ( obj , ( list , tuple ) ) : \n            obj = [ squash_dates ( o ) for o in obj ] \n        else : \n            if isinstance ( obj , datetime ) : \n                obj = obj . strftime ( ISO8601 ) \n    return obj "}
{"13783": "\ndef check_site_dir ( self ) : \n    instdir = normalize_path ( self . install_dir ) \n    pth_file = os . path . join ( instdir , 'easy-install.pth' ) \n    is_site_dir = instdir in self . all_site_dirs \n    if not is_site_dir and not self . multi_version : \n        is_site_dir = self . check_pth_processing ( ) \n    else : \n        testfile = self . pseudo_tempname ( ) + '.write-test' \n        test_exists = os . path . exists ( testfile ) \n        try : \n            if test_exists : \n                os . unlink ( testfile ) \n            open ( testfile , 'w' ) . close ( ) \n            os . unlink ( testfile ) \n        except ( OSError , IOError ) : \n            self . cant_write_to_target ( ) \n    if not is_site_dir and not self . multi_version : \n        raise DistutilsError ( self . no_default_version_msg ( ) ) \n    if is_site_dir : \n        if self . pth_file is None : \n            self . pth_file = PthDistributions ( pth_file , self . all_site_dirs ) \n    else : \n        self . pth_file = None \n    PYTHONPATH = os . environ . get ( 'PYTHONPATH' , '' ) . split ( os . pathsep ) \n    if instdir not in map ( normalize_path , [ _f for _f in PYTHONPATH if _f ] ) : \n        self . sitepy_installed = True \n    else : \n        if self . multi_version and not os . path . exists ( pth_file ) : \n            self . sitepy_installed = True \n            self . pth_file = None \n    self . install_dir = instdir "}
{"13788": "\ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    reg_url = cfg [ 'url' ] \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = 0 ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while True : \n        try : \n            idents , msg = session . recv ( sub , mode = 0 ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ 0 ] \n        if msg [ 'msg_type' ] == 'stream' : \n            print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) \n        else : \n            if msg [ 'msg_type' ] == 'pyerr' : \n                c = msg [ 'content' ] \n                print ( topic + ':' ) \n                for line in c [ 'traceback' ] : \n                    print ( '    ' + line ) "}
{"13824": "\ndef _method_magic_marker ( magic_kind ) : \n    validate_type ( magic_kind ) \n    def magic_deco ( arg ) : \n        call = lambda f , * a , ** k : f ( * a , ** k ) \n        if callable ( arg ) : \n            func = arg \n            name = func . func_name \n            retval = decorator ( call , func ) \n            record_magic ( magics , magic_kind , name , name ) \n        else : \n            if isinstance ( arg , basestring ) : \n                name = arg \n                def mark ( func , * a , ** kw ) : \n                    record_magic ( magics , magic_kind , name , func . func_name ) \n                    return decorator ( call , func ) \n                retval = mark \n            else : \n                raise TypeError ( \"Decorator can only be called with \" \"string or function\" ) \n        return retval \n    magic_deco . __doc__ = _docstring_template . format ( 'method' , magic_kind ) \n    return magic_deco "}
{"13825": "\ndef _function_magic_marker ( magic_kind ) : \n    validate_type ( magic_kind ) \n    def magic_deco ( arg ) : \n        call = lambda f , * a , ** k : f ( * a , ** k ) \n        caller = sys . _getframe ( 1 ) \n        for ns in [ 'f_locals' , 'f_globals' , 'f_builtins' ] : \n            get_ipython = getattr ( caller , ns ) . get ( 'get_ipython' ) \n            if get_ipython is not None : \n                break \n        else : \n            raise NameError ( 'Decorator can only run in context where ' '`get_ipython` exists' ) \n        ip = get_ipython ( ) \n        if callable ( arg ) : \n            func = arg \n            name = func . func_name \n            ip . register_magic_function ( func , magic_kind , name ) \n            retval = decorator ( call , func ) \n        else : \n            if isinstance ( arg , basestring ) : \n                name = arg \n                def mark ( func , * a , ** kw ) : \n                    ip . register_magic_function ( func , magic_kind , name ) \n                    return decorator ( call , func ) \n                retval = mark \n            else : \n                raise TypeError ( \"Decorator can only be called with \" \"string or function\" ) \n        return retval \n    ds = _docstring_template . format ( 'function' , magic_kind ) \n    ds += dedent ( \"\"\"    Note: this decorator can only be used in a context where IPython is already    active, so that the `get_ipython()` call succeeds.  You can therefore use    it in your startup files loaded after IPython initializes, but *not* in the    IPython configuration file itself, which is executed before IPython is    fully up and running.  Any file located in the `startup` subdirectory of    your configuration profile will be OK in this sense.    \"\"\" ) \n    magic_deco . __doc__ = ds \n    return magic_deco "}
{"13835": "\ndef func_from_info ( self ) : \n    info = self . funcinfo \n    functype = info [ 'func_type' ] \n    if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : \n        the_modelclass = get_module_member_by_dottedpath ( info [ 'class_path' ] ) \n        if functype == 'instancemethod' : \n            the_modelobject = the_modelclass . objects . get ( pk = info [ 'model_pk' ] ) \n            the_callable = get_member ( the_modelobject , info [ 'func_name' ] ) \n        else : \n            the_callable = get_member ( the_modelclass , info [ 'func_name' ] ) \n        return the_callable \n    else : \n        if functype == 'function' : \n            mod = import_module ( info [ 'module_name' ] ) \n            the_callable = get_member ( mod , info [ 'func_name' ] ) \n            return the_callable \n        else : \n            raise ValueError ( f\"Unknown functype '{functype} in task {self.pk} ({self.label})\" ) "}
{"13846": "\ndef run ( self ) : \n    try : \n        from _winapi import WAIT_OBJECT_0 , INFINITE \n    except ImportError : \n        from _subprocess import WAIT_OBJECT_0 , INFINITE \n    handles = [ ] \n    if self . interrupt_handle : \n        handles . append ( self . interrupt_handle ) \n    if self . parent_handle : \n        handles . append ( self . parent_handle ) \n    arch = platform . architecture ( ) [ 0 ] \n    c_int = ctypes . c_int64 if arch . startswith ( '64' ) else ctypes . c_int \n    while True : \n        result = ctypes . windll . kernel32 . WaitForMultipleObjects ( len ( handles ) , ( c_int * len ( handles ) ) ( * handles ) , False , INFINITE ) \n        if WAIT_OBJECT_0 <= result < len ( handles ) : \n            handle = handles [ result - WAIT_OBJECT_0 ] \n            if handle == self . interrupt_handle : \n                interrupt_main ( ) \n            else : \n                if handle == self . parent_handle : \n                    os . _exit ( 1 ) \n        else : \n            if result < 0 : \n                warn ( \"\"\"Parent poll failed.  If the frontend dies,                the kernel may be left running.  Please let us know                about your system (bitness, Python, etc.) at                ipython-dev@scipy.org\"\"\" ) \n                return "}
{"13908": "\ndef current ( self , value ) : \n    current = min ( max ( self . _min , value ) , self . _max ) \n    self . _current = current \n    if current > self . _stop : \n        self . _stop = current \n        self . _start = current - self . _width \n    else : \n        if current < self . _start : \n            self . _start = current \n            self . _stop = current + self . _width \n    if abs ( self . _start - self . _min ) <= self . _sticky_lenght : \n        self . _start = self . _min \n    if abs ( self . _stop - self . _max ) <= self . _sticky_lenght : \n        self . _stop = self . _max "}
{"13910": "\ndef _select_index ( self , row , col ) : \n    nr , nc = self . _size \n    nr = nr - 1 \n    nc = nc - 1 \n    if ( row > nr and col >= nc ) or ( row >= nr and col > nc ) : \n        self . _select_index ( 0 , 0 ) \n    else : \n        if ( row <= 0 and col < 0 ) or ( row < 0 and col <= 0 ) : \n            self . _select_index ( nr , nc ) \n        else : \n            if row > nr : \n                self . _select_index ( 0 , col + 1 ) \n            else : \n                if row < 0 : \n                    self . _select_index ( nr , col - 1 ) \n                else : \n                    if col > nc : \n                        self . _select_index ( row + 1 , 0 ) \n                    else : \n                        if col < 0 : \n                            self . _select_index ( row - 1 , nc ) \n                        else : \n                            if 0 <= row and row <= nr and 0 <= col and col <= nc : \n                                self . _index = ( row , col ) \n                            else : \n                                raise NotImplementedError ( \"you'r trying to go where no completion\\                           have gone before : %d:%d (%d:%d)\" % ( row , col , nr , nc ) ) "}
{"13927": "\ndef monitored ( total : int , name = None , message = None ) : \n    def decorator ( f ) : \n        nonlocal name \n        monitor_index = list ( inspect . signature ( f ) . parameters . keys ( ) ) . index ( 'monitor' ) \n        if name is None : \n            name = f . __name__ \n        \n        @ wraps ( f ) \n        def wrapper ( * args , ** kargs ) : \n            if len ( args ) > monitor_index : \n                monitor = args [ monitor_index ] \n            else : \n                if 'monitor' in kargs : \n                    monitor = kargs [ 'monitor' ] \n                else : \n                    monitor = kargs [ 'monitor' ] = NullMonitor ( ) \n            with monitor . task ( total , name , message ) : \n                f ( * args , ** kargs ) \n        return wrapper \n    return decorator "}
{"13941": "\ndef load_config ( self , argv = None , aliases = None , flags = None ) : \n    from IPython . config . configurable import Configurable \n    self . clear ( ) \n    if argv is None : \n        argv = self . argv \n    if aliases is None : \n        aliases = self . aliases \n    if flags is None : \n        flags = self . flags \n    uargv = self . _decode_argv ( argv ) \n    for idx , raw in enumerate ( uargv ) : \n        item = raw . lstrip ( '-' ) \n        if raw == '--' : \n            self . extra_args . extend ( uargv [ idx + 1 : ] ) \n            break \n        if kv_pattern . match ( raw ) : \n            lhs , rhs = item . split ( '=' , 1 ) \n            if lhs in aliases : \n                lhs = aliases [ lhs ] \n            if '.' not in lhs : \n                warn . warn ( \"Unrecognized alias: '%s', it will probably have no effect.\" % lhs ) \n            try : \n                self . _exec_config_str ( lhs , rhs ) \n            except Exception : \n                raise ArgumentError ( \"Invalid argument: '%s'\" % raw ) \n        else : \n            if flag_pattern . match ( raw ) : \n                if item in flags : \n                    cfg , help = flags [ item ] \n                    self . _load_flag ( cfg ) \n                else : \n                    raise ArgumentError ( \"Unrecognized flag: '%s'\" % raw ) \n            else : \n                if raw . startswith ( '-' ) : \n                    kv = '--' + item \n                    if kv_pattern . match ( kv ) : \n                        raise ArgumentError ( \"Invalid argument: '%s', did you mean '%s'?\" % ( raw , kv ) ) \n                    else : \n                        raise ArgumentError ( \"Invalid argument: '%s'\" % raw ) \n                else : \n                    self . extra_args . append ( item ) \n    return self . config "}
{"13953": "\ndef _fetch_file ( self , remote , local ) : \n    full_remote = \"%s:%s\" % ( self . location , remote ) \n    self . log . info ( \"fetching %s from %s\" , local , full_remote ) \n    for i in range ( 10 ) : \n        check = check_output ( self . ssh_cmd + self . ssh_args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) \n        check = check . strip ( ) \n        if check == 'no' : \n            time . sleep ( 1 ) \n        else : \n            if check == 'yes' : \n                break \n    check_output ( self . scp_cmd + [ full_remote , local ] ) "}
{"13987": "\ndef attr_matches ( self , text ) : \n    m = re . match ( r\"(\\S+(\\.\\w+)*)\\.(\\w*)$\" , text ) \n    if m : \n        expr , attr = m . group ( 1 , 3 ) \n    else : \n        if self . greedy : \n            m2 = re . match ( r\"(.+)\\.(\\w*)$\" , self . line_buffer ) \n            if not m2 : \n                return [ ] \n            expr , attr = m2 . group ( 1 , 2 ) \n        else : \n            return [ ] \n    try : \n        obj = eval ( expr , self . namespace ) \n    except : \n        try : \n            obj = eval ( expr , self . global_namespace ) \n        except : \n            return [ ] \n    if self . limit_to__all__ and hasattr ( obj , '__all__' ) : \n        words = get__all__entries ( obj ) \n    else : \n        words = dir2 ( obj ) \n    try : \n        words = generics . complete_object ( obj , words ) \n    except TryNext : \n        pass \n    except Exception : \n        pass \n    n = len ( attr ) \n    res = [ \"%s.%s\" % ( expr , w ) for w in words if w [ : n ] == attr ] \n    return res "}
{"13992": "\ndef _default_arguments ( self , obj ) : \n    if not ( inspect . isfunction ( obj ) or inspect . ismethod ( obj ) ) : \n        if inspect . isclass ( obj ) : \n            obj = ( getattr ( obj , '__init__' , None ) or getattr ( obj , '__new__' , None ) ) \n        else : \n            if hasattr ( obj , '__call__' ) : \n                obj = obj . __call__ \n    try : \n        args , _ , _1 , defaults = inspect . getargspec ( obj ) \n        if defaults : \n            return args [ - len ( defaults ) : ] \n    except TypeError : \n        pass \n    return [ ] "}
{"14087": "\ndef parse_step ( cls , ctxt , step_addr , step_conf ) : \n    if isinstance ( step_conf , six . string_types ) : \n        step_conf = { step_conf : None } \n    else : \n        if not isinstance ( step_conf , collections . Mapping ) : \n            raise ConfigError ( 'Unable to parse step configuration: expecting string or ' 'dictionary, not \"%s\"' % step_conf . __class__ . __name__ , step_addr , ) \n    action_item = None \n    mod_items = { } \n    kwargs = { } \n    for key , key_conf in step_conf . items ( ) : \n        if key in cls . schemas : \n            utils . schema_validate ( key_conf , cls . schemas [ key ] , ConfigError , key , step_addr = step_addr ) \n            kwargs [ key ] = key_conf \n        else : \n            if key in entry . points [ NAMESPACE_ACTION ] : \n                if action_item is not None : \n                    raise ConfigError ( 'Bad step configuration: action \"%s\" specified, ' 'but action \"%s\" already processed' % ( key , action_item . name ) , step_addr , ) \n                action_item = StepItem ( entry . points [ NAMESPACE_ACTION ] [ key ] , key , key_conf ) \n            else : \n                if key in entry . points [ NAMESPACE_MODIFIER ] : \n                    mod_class = entry . points [ NAMESPACE_MODIFIER ] [ key ] \n                    mod_items . setdefault ( mod_class . priority , [ ] ) \n                    mod_items [ mod_class . priority ] . append ( StepItem ( mod_class , key , key_conf ) ) \n                else : \n                    raise ConfigError ( 'Bad step configuration: unable to resolve action ' '\"%s\"' % key , step_addr , ) \n    if action_item is None : \n        raise ConfigError ( 'Bad step configuration: no action specified' , step_addr , ) \n    action_type = ( Modifier . STEP if action_item . cls . step_action else Modifier . NORMAL ) \n    modifiers = [ ] \n    for mod_item in utils . iter_prio_dict ( mod_items ) : \n        if mod_item . cls . restriction & action_type == 0 : \n            raise ConfigError ( 'Bad step configuration: modifier \"%s\" is ' 'incompatible with the action \"%s\"' % ( mod_item . name , action_item . name ) , step_addr , ) \n        modifier = mod_item . init ( ctxt , step_addr ) \n        modifiers . append ( modifier ) \n        action_item . conf = modifier . action_conf ( ctxt , action_item . cls , action_item . name , action_item . conf , step_addr ) \n    action = action_item . init ( ctxt , step_addr ) \n    step = cls ( step_addr , action , modifiers , ** kwargs ) \n    if action_item . cls . step_action : \n        return step ( ctxt ) \n    return [ step ] "}
{"14114": "\ndef init_colors ( self , widget ) : \n    try : \n        colors = self . config . ZMQInteractiveShell . colors \n    except AttributeError : \n        colors = None \n    try : \n        style = self . config . IPythonWidget . syntax_style \n    except AttributeError : \n        style = None \n    try : \n        sheet = self . config . IPythonWidget . style_sheet \n    except AttributeError : \n        sheet = None \n    if colors : \n        colors = colors . lower ( ) \n        if colors in ( 'lightbg' , 'light' ) : \n            colors = 'lightbg' \n        else : \n            if colors in ( 'dark' , 'linux' ) : \n                colors = 'linux' \n            else : \n                colors = 'nocolor' \n    else : \n        if style : \n            if style == 'bw' : \n                colors = 'nocolor' \n            else : \n                if styles . dark_style ( style ) : \n                    colors = 'linux' \n                else : \n                    colors = 'lightbg' \n        else : \n            colors = None \n    if style : \n        widget . style_sheet = styles . sheet_from_template ( style , colors ) \n        widget . syntax_style = style \n        widget . _syntax_style_changed ( ) \n        widget . _style_sheet_changed ( ) \n    else : \n        if colors : \n            widget . set_default_style ( colors = colors ) \n    if self . stylesheet : \n        if os . path . isfile ( self . stylesheet ) : \n            with open ( self . stylesheet ) as f : \n                sheet = f . read ( ) \n        else : \n            raise IOError ( \"Stylesheet %r not found.\" % self . stylesheet ) \n    if sheet : \n        widget . style_sheet = sheet \n        widget . _style_sheet_changed ( ) "}
{"14116": "\ndef Rconverter ( Robj , dataframe = False ) : \n    is_data_frame = ro . r ( 'is.data.frame' ) \n    colnames = ro . r ( 'colnames' ) \n    rownames = ro . r ( 'rownames' ) \n    names = ro . r ( 'names' ) \n    if dataframe : \n        as_data_frame = ro . r ( 'as.data.frame' ) \n        cols = colnames ( Robj ) \n        _names = names ( Robj ) \n        if cols != ri . NULL : \n            Robj = as_data_frame ( Robj ) \n            names = tuple ( np . array ( cols ) ) \n        else : \n            if _names != ri . NULL : \n                names = tuple ( np . array ( _names ) ) \n            else : \n                return np . asarray ( Robj ) \n        Robj = np . rec . fromarrays ( Robj , names = names ) \n    return np . asarray ( Robj ) "}
{"14128": "\ndef ensure_fromlist ( mod , fromlist , buf , recursive ) : \n    if not hasattr ( mod , '__path__' ) : \n        return \n    for item in fromlist : \n        if not hasattr ( item , 'rindex' ) : \n            raise TypeError ( \"Item in ``from list'' not a string\" ) \n        if item == '*' : \n            if recursive : \n                continue \n            try : \n                all = mod . __all__ \n            except AttributeError : \n                pass \n            else : \n                ret = ensure_fromlist ( mod , all , buf , 1 ) \n                if not ret : \n                    return 0 \n        else : \n            if not hasattr ( mod , item ) : \n                import_submodule ( mod , item , buf + '.' + item ) "}
{"14132": "\ndef expr_code ( self , expr ) : \n    if \"|\" in expr : \n        pipes = expr . split ( \"|\" ) \n        code = self . expr_code ( pipes [ 0 ] ) \n        for func in pipes [ 1 : ] : \n            self . all_vars . add ( func ) \n            code = \"c_%s(%s)\" % ( func , code ) \n    else : \n        if \".\" in expr : \n            dots = expr . split ( \".\" ) \n            code = self . expr_code ( dots [ 0 ] ) \n            args = [ repr ( d ) for d in dots [ 1 : ] ] \n            code = \"dot(%s, %s)\" % ( code , \", \" . join ( args ) ) \n        else : \n            self . all_vars . add ( expr ) \n            code = \"c_%s\" % expr \n    return code "}
{"14139": "\ndef _float_precision_changed ( self , name , old , new ) : \n    if '%' in new : \n        fmt = new \n        try : \n            fmt % 3.14159 \n        except Exception : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n    else : \n        if new : \n            try : \n                i = int ( new ) \n                assert i >= 0 \n            except ValueError : \n                raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n            except AssertionError : \n                raise ValueError ( \"int precision must be non-negative, not %r\" % i ) \n            fmt = '%%.%if' % i \n            if 'numpy' in sys . modules : \n                import numpy \n                numpy . set_printoptions ( precision = i ) \n        else : \n            fmt = '%r' \n            if 'numpy' in sys . modules : \n                import numpy \n                numpy . set_printoptions ( precision = 8 ) \n    self . float_format = fmt "}
{"14142": "\ndef configureLogging ( self ) : \n    if self . loggingConfig : \n        from logging . config import fileConfig \n        fileConfig ( self . loggingConfig ) \n        return \n    format = logging . Formatter ( '%(name)s: %(levelname)s: %(message)s' ) \n    if self . debugLog : \n        handler = logging . FileHandler ( self . debugLog ) \n    else : \n        handler = logging . StreamHandler ( self . logStream ) \n    handler . setFormatter ( format ) \n    logger = logging . getLogger ( 'nose' ) \n    logger . propagate = 0 \n    if handler not in logger . handlers : \n        logger . addHandler ( handler ) \n    lvl = logging . WARNING \n    if self . verbosity >= 5 : \n        lvl = 0 \n    else : \n        if self . verbosity >= 4 : \n            lvl = logging . DEBUG \n        else : \n            if self . verbosity >= 3 : \n                lvl = logging . INFO \n    logger . setLevel ( lvl ) \n    if self . debug : \n        debug_loggers = [ name for name in self . debug . split ( ',' ) if name ] \n        for logger_name in debug_loggers : \n            l = logging . getLogger ( logger_name ) \n            l . setLevel ( logging . DEBUG ) \n            if not l . handlers and not logger_name . startswith ( 'nose' ) : \n                l . addHandler ( handler ) "}
{"14147": "\ndef get_pager_cmd ( pager_cmd = None ) : \n    if os . name == 'posix' : \n        default_pager_cmd = 'less -r' \n    else : \n        if os . name in [ 'nt' , 'dos' ] : \n            default_pager_cmd = 'type' \n    if pager_cmd is None : \n        try : \n            pager_cmd = os . environ [ 'PAGER' ] \n        except : \n            pager_cmd = default_pager_cmd \n    return pager_cmd "}
{"14153": "\ndef can_print_latex ( o ) : \n    import sympy \n    if isinstance ( o , ( list , tuple , set , frozenset ) ) : \n        return all ( can_print_latex ( i ) for i in o ) \n    else : \n        if isinstance ( o , dict ) : \n            return all ( ( isinstance ( i , basestring ) or can_print_latex ( i ) ) and can_print_latex ( o [ i ] ) for i in o ) \n        else : \n            if isinstance ( o , ( sympy . Basic , sympy . matrices . Matrix , int , long , float ) ) : \n                return True \n    return False "}
{"14178": "\ndef is_alive ( self ) : \n    if self . has_kernel : \n        if self . kernel . poll ( ) is None : \n            return True \n        else : \n            return False \n    else : \n        if self . _hb_channel is not None : \n            return self . _hb_channel . is_beating ( ) \n        else : \n            return True "}
{"14213": "\ndef clipboard_get ( self ) : \n    from IPython . lib . clipboard import ( osx_clipboard_get , tkinter_clipboard_get , win32_clipboard_get ) \n    if sys . platform == 'win32' : \n        chain = [ win32_clipboard_get , tkinter_clipboard_get ] \n    else : \n        if sys . platform == 'darwin' : \n            chain = [ osx_clipboard_get , tkinter_clipboard_get ] \n        else : \n            chain = [ tkinter_clipboard_get ] \n    dispatcher = CommandChainDispatcher ( ) \n    for func in chain : \n        dispatcher . add ( func ) \n    text = dispatcher ( ) \n    return text "}
{"14227": "\ndef _handle_execute_reply ( self , msg ) : \n    parent = msg [ 'parent_header' ] \n    msg_id = parent [ 'msg_id' ] \n    if msg_id not in self . outstanding : \n        if msg_id in self . history : \n            print ( \"got stale result: %s\" % msg_id ) \n        else : \n            print ( \"got unknown result: %s\" % msg_id ) \n    else : \n        self . outstanding . remove ( msg_id ) \n    content = msg [ 'content' ] \n    header = msg [ 'header' ] \n    md = self . metadata [ msg_id ] \n    md . update ( self . _extract_metadata ( header , parent , content ) ) \n    self . metadata [ msg_id ] = md \n    e_outstanding = self . _outstanding_dict [ md [ 'engine_uuid' ] ] \n    if msg_id in e_outstanding : \n        e_outstanding . remove ( msg_id ) \n    if content [ 'status' ] == 'ok' : \n        self . results [ msg_id ] = ExecuteReply ( msg_id , content , md ) \n    else : \n        if content [ 'status' ] == 'aborted' : \n            self . results [ msg_id ] = error . TaskAborted ( msg_id ) \n        else : \n            if content [ 'status' ] == 'resubmitted' : \n                pass \n            else : \n                self . results [ msg_id ] = self . _unwrap_exception ( content ) "}
{"14232": "\ndef _flush_iopub ( self , sock ) : \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        if self . debug : \n            pprint ( msg ) \n        parent = msg [ 'parent_header' ] \n        if not parent : \n            continue \n        msg_id = parent [ 'msg_id' ] \n        content = msg [ 'content' ] \n        header = msg [ 'header' ] \n        msg_type = msg [ 'header' ] [ 'msg_type' ] \n        md = self . metadata [ msg_id ] \n        if msg_type == 'stream' : \n            name = content [ 'name' ] \n            s = md [ name ] or '' \n            md [ name ] = s + content [ 'data' ] \n        else : \n            if msg_type == 'pyerr' : \n                md . update ( { 'pyerr' : self . _unwrap_exception ( content ) } ) \n            else : \n                if msg_type == 'pyin' : \n                    md . update ( { 'pyin' : content [ 'code' ] } ) \n                else : \n                    if msg_type == 'display_data' : \n                        md [ 'outputs' ] . append ( content ) \n                    else : \n                        if msg_type == 'pyout' : \n                            md [ 'pyout' ] = content \n                        else : \n                            if msg_type == 'status' : \n                                if content [ 'execution_state' ] == 'idle' : \n                                    md [ 'outputs_ready' ] = True \n                            else : \n                                pass \n        self . metadata [ msg_id ] = md \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14236": "\ndef wait ( self , jobs = None , timeout = - 1 ) : \n    tic = time . time ( ) \n    if jobs is None : \n        theids = self . outstanding \n    else : \n        if isinstance ( jobs , ( int , basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        theids = set ( ) \n        for job in jobs : \n            if isinstance ( job , int ) : \n                job = self . history [ job ] \n            else : \n                if isinstance ( job , AsyncResult ) : \n                    map ( theids . add , job . msg_ids ) \n                    continue \n            theids . add ( job ) \n    if not theids . intersection ( self . outstanding ) : \n        return True \n    self . spin ( ) \n    while theids . intersection ( self . outstanding ) : \n        if timeout >= 0 and ( time . time ( ) - tic ) > timeout : \n            break \n        time . sleep ( 1e-3 ) \n        self . spin ( ) \n    return len ( theids . intersection ( self . outstanding ) ) == 0 "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent += 1 \n        else : \n            if toktype == token . DEDENT : \n                indent -= 1 \n            else : \n                if toktype == token . NAME and ttext == 'class' : \n                    self . classdefs . add ( slineno ) \n                else : \n                    if toktype == token . OP and ttext == ':' : \n                        if not excluding and elineno in self . excluded : \n                            exclude_indent = indent \n                            excluding = True \n                    else : \n                        if toktype == token . STRING and prev_toktype == token . INDENT : \n                            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n                        else : \n                            if toktype == token . NEWLINE : \n                                if first_line is not None and elineno != first_line : \n                                    rng = ( first_line , elineno ) \n                                    for l in range ( first_line , elineno + 1 ) : \n                                        self . multiline [ l ] = rng \n                                first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = 0 \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if bc . jump_to >= 0 : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = 0 \n    for bc in bytecodes : \n        start_new_chunk = False \n        first_chunk = False \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = True \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = True \n        else : \n            if bc . offset in jump_to : \n                start_new_chunk = True \n            else : \n                if bc . op in OPS_CHUNK_BEGIN : \n                    start_new_chunk = True \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if bc . jump_to >= 0 and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch -= 1 \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if bc . op == OP_BREAK_LOOP : \n                chunk . exits . add ( block_stack [ - 1 ] [ 1 ] ) \n            chunk = None \n        if bc . op == OP_END_FINALLY : \n            for block in reversed ( block_stack ) : \n                if block [ 0 ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ 1 ] ) \n                    break \n        if bc . op == OP_COMPARE_OP and bc . arg == COMPARE_EXCEPTION : \n            ignore_branch += 1 \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if penult . op == OP_LOAD_CONST and ult . op == OP_RETURN_VALUE : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if chunks [ - 1 ] . byte != penult . offset : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - 1 ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , False ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - 1 ] . length = bc . next_offset - chunks [ - 1 ] . byte \n        for i in range ( len ( chunks ) - 1 ) : \n            chunks [ i ] . length = chunks [ i + 1 ] . byte - chunks [ i ] . byte \n    return chunks "}
{"14275": "\ndef convert_to_this_nbformat ( nb , orig_version = 2 , orig_minor = 0 ) : \n    if orig_version == 1 : \n        nb = v2 . convert_to_this_nbformat ( nb ) \n        orig_version = 2 \n    if orig_version == 2 : \n        nb . nbformat = nbformat \n        nb . nbformat_minor = nbformat_minor \n        nb . orig_nbformat = 2 \n        return nb \n    else : \n        if orig_version == 3 : \n            if orig_minor != nbformat_minor : \n                nb . orig_nbformat_minor = orig_minor \n            nb . nbformat_minor = nbformat_minor \n            return nb \n        else : \n            raise ValueError ( 'Cannot convert a notebook from v%s to v3' % orig_version ) "}
{"14281": "\ndef _handle_pyout ( self , msg ) : \n    self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        content = msg [ 'content' ] \n        prompt_number = content . get ( 'execution_count' , 0 ) \n        data = content [ 'data' ] \n        if data . has_key ( 'text/html' ) : \n            self . _append_plain_text ( self . output_sep , True ) \n            self . _append_html ( self . _make_out_prompt ( prompt_number ) , True ) \n            html = data [ 'text/html' ] \n            self . _append_plain_text ( '\\n' , True ) \n            self . _append_html ( html + self . output_sep2 , True ) \n        else : \n            if data . has_key ( 'text/plain' ) : \n                self . _append_plain_text ( self . output_sep , True ) \n                self . _append_html ( self . _make_out_prompt ( prompt_number ) , True ) \n                text = data [ 'text/plain' ] \n                if \"\\n\" in text and not self . output_sep . endswith ( \"\\n\" ) : \n                    self . _append_plain_text ( '\\n' , True ) \n                self . _append_plain_text ( text + self . output_sep2 , True ) "}
{"14282": "\ndef _handle_display_data ( self , msg ) : \n    self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        source = msg [ 'content' ] [ 'source' ] \n        data = msg [ 'content' ] [ 'data' ] \n        metadata = msg [ 'content' ] [ 'metadata' ] \n        if data . has_key ( 'text/html' ) : \n            html = data [ 'text/html' ] \n            self . _append_html ( html , True ) \n        else : \n            if data . has_key ( 'text/plain' ) : \n                text = data [ 'text/plain' ] \n                self . _append_plain_text ( text , True ) \n        self . _append_plain_text ( u'\\n' , True ) "}
{"14287": "\ndef set_default_style ( self , colors = 'lightbg' ) : \n    colors = colors . lower ( ) \n    if colors == 'lightbg' : \n        self . style_sheet = styles . default_light_style_sheet \n        self . syntax_style = styles . default_light_syntax_style \n    else : \n        if colors == 'linux' : \n            self . style_sheet = styles . default_dark_style_sheet \n            self . syntax_style = styles . default_dark_syntax_style \n        else : \n            if colors == 'nocolor' : \n                self . style_sheet = styles . default_bw_style_sheet \n                self . syntax_style = styles . default_bw_syntax_style \n            else : \n                raise KeyError ( \"No such color scheme: %s\" % colors ) "}
{"14288": "\ndef _edit ( self , filename , line = None ) : \n    if self . custom_edit : \n        self . custom_edit_requested . emit ( filename , line ) \n    else : \n        if not self . editor : \n            self . _append_plain_text ( 'No default editor available.\\n' 'Specify a GUI text editor in the `IPythonWidget.editor` ' 'configurable to enable the %edit magic' ) \n        else : \n            try : \n                filename = '\"%s\"' % filename \n                if line and self . editor_line : \n                    command = self . editor_line . format ( filename = filename , line = line ) \n                else : \n                    try : \n                        command = self . editor . format ( ) \n                    except KeyError : \n                        command = self . editor . format ( filename = filename ) \n                    else : \n                        command += ' ' + filename \n            except KeyError : \n                self . _append_plain_text ( 'Invalid editor command.\\n' ) \n            else : \n                try : \n                    Popen ( command , shell = True ) \n                except OSError : \n                    msg = 'Opening editor with command \"%s\" failed.\\n' \n                    self . _append_plain_text ( msg % command ) "}
{"14312": "\ndef reads_json ( s , ** kwargs ) : \n    nbf , minor , d = parse_json ( s , ** kwargs ) \n    if nbf == 1 : \n        nb = v1 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 1 ) \n    else : \n        if nbf == 2 : \n            nb = v2 . to_notebook_json ( d , ** kwargs ) \n            nb = v3 . convert_to_this_nbformat ( nb , orig_version = 2 ) \n        else : \n            if nbf == 3 : \n                nb = v3 . to_notebook_json ( d , ** kwargs ) \n                nb = v3 . convert_to_this_nbformat ( nb , orig_version = 3 , orig_minor = minor ) \n            else : \n                raise NBFormatError ( 'Unsupported JSON nbformat version: %i' % nbf ) \n    return nb "}
{"14313": "\ndef reads_py ( s , ** kwargs ) : \n    nbf , nbm , s = parse_py ( s , ** kwargs ) \n    if nbf == 2 : \n        nb = v2 . to_notebook_py ( s , ** kwargs ) \n    else : \n        if nbf == 3 : \n            nb = v3 . to_notebook_py ( s , ** kwargs ) \n        else : \n            raise NBFormatError ( 'Unsupported PY nbformat version: %i' % nbf ) \n    return nb "}
{"14314": "\ndef reads ( s , format , ** kwargs ) : \n    format = unicode ( format ) \n    if format == u'json' or format == u'ipynb' : \n        return reads_json ( s , ** kwargs ) \n    else : \n        if format == u'py' : \n            return reads_py ( s , ** kwargs ) \n        else : \n            raise NBFormatError ( 'Unsupported format: %s' % format ) "}
{"14315": "\ndef writes ( nb , format , ** kwargs ) : \n    format = unicode ( format ) \n    if format == u'json' or format == u'ipynb' : \n        return writes_json ( nb , ** kwargs ) \n    else : \n        if format == u'py' : \n            return writes_py ( nb , ** kwargs ) \n        else : \n            raise NBFormatError ( 'Unsupported format: %s' % format ) "}
{"14331": "\ndef convert_currency ( from_symbol , to_symbol , value , date ) : \n    if from_symbol == to_symbol : \n        return value \n    factor = conversion_factor ( from_symbol , to_symbol , date ) \n    if type ( value ) == float : \n        output = value * float ( factor ) \n    else : \n        if type ( value ) == Decimal : \n            output = Decimal ( format ( value * factor , '.%sf' % str ( PRICE_PRECISION ) ) ) \n        else : \n            if type ( value ) in [ np . float16 , np . float32 , np . float64 , np . float128 , np . float ] : \n                output = float ( value ) * float ( factor ) \n            else : \n                output = None \n    return output "}
{"14364": "\ndef copy ( self ) : \n    if self . _page_control is not None and self . _page_control . hasFocus ( ) : \n        self . _page_control . copy ( ) \n    else : \n        if self . _control . hasFocus ( ) : \n            text = self . _control . textCursor ( ) . selection ( ) . toPlainText ( ) \n            if text : \n                lines = map ( self . _transform_prompt , text . splitlines ( ) ) \n                text = '\\n' . join ( lines ) \n                QtGui . QApplication . clipboard ( ) . setText ( text ) \n        else : \n            self . log . debug ( \"frontend widget : unknown copy target\" ) "}
{"14369": "\ndef _event_filter_console_keypress ( self , event ) : \n    key = event . key ( ) \n    if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : \n        if key == QtCore . Qt . Key_C and self . _executing : \n            self . request_interrupt_kernel ( ) \n            return True \n        else : \n            if key == QtCore . Qt . Key_Period : \n                self . request_restart_kernel ( ) \n                return True \n    else : \n        if not event . modifiers ( ) & QtCore . Qt . AltModifier : \n            if key == QtCore . Qt . Key_Backspace : \n                col = self . _get_input_buffer_cursor_column ( ) \n                cursor = self . _control . textCursor ( ) \n                if col > 3 and not cursor . hasSelection ( ) : \n                    text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n                    if text . endswith ( '    ' ) and not text . strip ( ) : \n                        cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4 ) \n                        cursor . removeSelectedText ( ) \n                        return True \n    return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) "}
{"14374": "\ndef _handle_execute_reply ( self , msg ) : \n    self . log . debug ( \"execute: %s\" , msg . get ( 'content' , '' ) ) \n    msg_id = msg [ 'parent_header' ] [ 'msg_id' ] \n    info = self . _request_info [ 'execute' ] . get ( msg_id ) \n    self . _reading = False \n    if info and info . kind == 'user' and not self . _hidden : \n        self . kernel_manager . sub_channel . flush ( ) \n        if self . ansi_codes : \n            self . _ansi_processor . reset_sgr ( ) \n        content = msg [ 'content' ] \n        status = content [ 'status' ] \n        if status == 'ok' : \n            self . _process_execute_ok ( msg ) \n        else : \n            if status == 'error' : \n                self . _process_execute_error ( msg ) \n            else : \n                if status == 'aborted' : \n                    self . _process_execute_abort ( msg ) \n        self . _show_interpreter_prompt_for_reply ( msg ) \n        self . executed . emit ( msg ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    else : \n        if info and info . kind == 'silent_exec_callback' and not self . _hidden : \n            self . _handle_exec_callback ( msg ) \n            self . _request_info [ 'execute' ] . pop ( msg_id ) \n        else : \n            super ( FrontendWidget , self ) . _handle_execute_reply ( msg ) "}
{"14382": "\ndef interrupt_kernel ( self ) : \n    if self . custom_interrupt : \n        self . _reading = False \n        self . custom_interrupt_requested . emit ( ) \n    else : \n        if self . kernel_manager . has_kernel : \n            self . _reading = False \n            self . kernel_manager . interrupt_kernel ( ) \n        else : \n            self . _append_plain_text ( 'Kernel process is either remote or ' 'unspecified. Cannot interrupt.\\n' ) "}
{"14384": "\ndef restart_kernel ( self , message , now = False ) : \n    if self . custom_restart : \n        self . custom_restart_requested . emit ( ) \n    else : \n        if self . kernel_manager . has_kernel : \n            self . kernel_manager . hb_channel . pause ( ) \n            if self . confirm_restart : \n                buttons = QtGui . QMessageBox . Yes | QtGui . QMessageBox . No \n                result = QtGui . QMessageBox . question ( self , 'Restart kernel?' , message , buttons ) \n                do_restart = result == QtGui . QMessageBox . Yes \n            else : \n                do_restart = True \n            if do_restart : \n                try : \n                    self . kernel_manager . restart_kernel ( now = now ) \n                except RuntimeError : \n                    self . _append_plain_text ( 'Kernel started externally. ' 'Cannot restart.\\n' , before_prompt = True ) \n                else : \n                    self . reset ( ) \n            else : \n                self . kernel_manager . hb_channel . unpause ( ) \n        else : \n            self . _append_plain_text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\\n' , before_prompt = True ) "}
{"14397": "\ndef latex_to_png ( s , encode = False , backend = 'mpl' ) : \n    if backend == 'mpl' : \n        f = latex_to_png_mpl \n    else : \n        if backend == 'dvipng' : \n            f = latex_to_png_dvipng \n        else : \n            raise ValueError ( 'No such backend {0}' . format ( backend ) ) \n    bin_data = f ( s ) \n    if encode and bin_data : \n        bin_data = encodestring ( bin_data ) \n    return bin_data "}
{"14400": "\ndef check_if_exists ( self ) : \n    if self . req is None : \n        return False \n    try : \n        self . satisfied_by = pkg_resources . get_distribution ( self . req ) \n    except pkg_resources . DistributionNotFound : \n        return False \n    except pkg_resources . VersionConflict : \n        existing_dist = pkg_resources . get_distribution ( self . req . project_name ) \n        if self . use_user_site : \n            if dist_in_usersite ( existing_dist ) : \n                self . conflicts_with = existing_dist \n            else : \n                if running_under_virtualenv ( ) and dist_in_site_packages ( existing_dist ) : \n                    raise InstallationError ( \"Will not install to the user site because it will lack sys.path precedence to %s in %s\" % ( existing_dist . project_name , existing_dist . location ) ) \n        else : \n            self . conflicts_with = existing_dist \n    return True "}
{"14403": "\ndef as_dict ( self , attrs = [ ] , ad_value = None ) : \n    excluded_names = set ( [ 'send_signal' , 'suspend' , 'resume' , 'terminate' , 'kill' , 'wait' , 'is_running' , 'as_dict' , 'parent' , 'get_children' , 'nice' ] ) \n    retdict = dict ( ) \n    for name in set ( attrs or dir ( self ) ) : \n        if name . startswith ( '_' ) : \n            continue \n        if name . startswith ( 'set_' ) : \n            continue \n        if name in excluded_names : \n            continue \n        try : \n            attr = getattr ( self , name ) \n            if callable ( attr ) : \n                if name == 'get_cpu_percent' : \n                    ret = attr ( interval = 0 ) \n                else : \n                    ret = attr ( ) \n            else : \n                ret = attr \n        except AccessDenied : \n            ret = ad_value \n        except NotImplementedError : \n            if attrs : \n                raise \n            continue \n        if name . startswith ( 'get' ) : \n            if name [ 3 ] == '_' : \n                name = name [ 4 : ] \n            else : \n                if name == 'getcwd' : \n                    name = 'cwd' \n        retdict [ name ] = ret \n    return retdict "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = True \n    if esc == ESC_QUOTE : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    else : \n        if esc == ESC_QUOTE2 : \n            newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n        else : \n            if esc == ESC_PAREN : \n                newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n            else : \n                if force_auto : \n                    do_rewrite = not the_rest . startswith ( '(' ) \n                else : \n                    if not the_rest : \n                        do_rewrite = ( self . shell . autocall >= 2 ) \n                    else : \n                        if the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n                            do_rewrite = False \n                        else : \n                            do_rewrite = True \n                if do_rewrite : \n                    if the_rest . endswith ( ';' ) : \n                        newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) \n                    else : \n                        newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n                else : \n                    normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n                    return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14441": "\ndef handle ( self , line_info ) : \n    normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n    line = line_info . line \n    try : \n        codeop . compile_command ( line ) \n    except SyntaxError : \n        if line [ 0 ] == ESC_HELP : \n            line = line [ 1 : ] \n        else : \n            if line [ - 1 ] == ESC_HELP : \n                line = line [ : - 1 ] \n        if line : \n            self . shell . magic ( 'pinfo %s' % line_info . ifun ) \n        else : \n            self . shell . show_usage ( ) \n        return '' \n    except : \n        raise \n        return normal_handler . handle ( line_info ) \n    else : \n        return normal_handler . handle ( line_info ) "}
{"14442": "\ndef eventFilter ( self , obj , event ) : \n    if obj == self . _text_edit : \n        etype = event . type ( ) \n        if etype == QtCore . QEvent . KeyPress : \n            key = event . key ( ) \n            if key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return ) : \n                self . hide ( ) \n            else : \n                if key == QtCore . Qt . Key_Escape : \n                    self . hide ( ) \n                    return True \n        else : \n            if etype == QtCore . QEvent . FocusOut : \n                self . hide ( ) \n            else : \n                if etype == QtCore . QEvent . Enter : \n                    self . _hide_timer . stop ( ) \n                else : \n                    if etype == QtCore . QEvent . Leave : \n                        self . _leave_event_hide ( ) \n    return super ( CallTipWidget , self ) . eventFilter ( obj , event ) "}
{"14455": "\ndef default_aliases ( ) : \n    if os . name == 'posix' : \n        default_aliases = [ ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'mv' , 'mv -i' ) , ( 'rm' , 'rm -i' ) , ( 'cp' , 'cp -i' ) , ( 'cat' , 'cat' ) , ] \n        if sys . platform . startswith ( 'linux' ) : \n            ls_aliases = [ ( 'ls' , 'ls -F --color' ) , ( 'll' , 'ls -F -o --color' ) , ( 'lf' , 'ls -F -o --color %l | grep ^-' ) , ( 'lk' , 'ls -F -o --color %l | grep ^l' ) , ( 'ldir' , 'ls -F -o --color %l | grep /$' ) , ( 'lx' , 'ls -F -o --color %l | grep ^-..x' ) , ] \n        else : \n            ls_aliases = [ ( 'ls' , 'ls -F' ) , ( 'll' , 'ls -F -l' ) , ( 'lf' , 'ls -F -l %l | grep ^-' ) , ( 'lk' , 'ls -F -l %l | grep ^l' ) , ( 'ldir' , 'ls -F -l %l | grep /$' ) , ( 'lx' , 'ls -F -l %l | grep ^-..x' ) , ] \n        default_aliases = default_aliases + ls_aliases \n    else : \n        if os . name in [ 'nt' , 'dos' ] : \n            default_aliases = [ ( 'ls' , 'dir /on' ) , ( 'ddir' , 'dir /ad /on' ) , ( 'ldir' , 'dir /ad /on' ) , ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'echo' , 'echo' ) , ( 'ren' , 'ren' ) , ( 'copy' , 'copy' ) , ] \n        else : \n            default_aliases = [ ] \n    return default_aliases "}
{"14464": "\ndef split_string ( self , string ) : \n    self . actions = [ ] \n    start = 0 \n    last_char = '\\n' if len ( string ) > 0 and string [ - 1 ] == '\\n' else None \n    string = string [ : - 1 ] if last_char is not None else string \n    for match in ANSI_OR_SPECIAL_PATTERN . finditer ( string ) : \n        raw = string [ start : match . start ( ) ] \n        substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n        if substring or self . actions : \n            yield substring \n            self . actions = [ ] \n        start = match . end ( ) \n        groups = filter ( lambda x : x is not None , match . groups ( ) ) \n        g0 = groups [ 0 ] \n        if g0 == '\\a' : \n            self . actions . append ( BeepAction ( 'beep' ) ) \n            yield None \n            self . actions = [ ] \n        else : \n            if g0 == '\\r' : \n                self . actions . append ( CarriageReturnAction ( 'carriage-return' ) ) \n                yield None \n                self . actions = [ ] \n            else : \n                if g0 == '\\b' : \n                    self . actions . append ( BackSpaceAction ( 'backspace' ) ) \n                    yield None \n                    self . actions = [ ] \n                else : \n                    if g0 == '\\n' or g0 == '\\r\\n' : \n                        self . actions . append ( NewLineAction ( 'newline' ) ) \n                        yield g0 \n                        self . actions = [ ] \n                    else : \n                        params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] \n                        if g0 . startswith ( '[' ) : \n                            try : \n                                params = map ( int , params ) \n                            except ValueError : \n                                pass \n                            else : \n                                self . set_csi_code ( groups [ 2 ] , params ) \n                        else : \n                            if g0 . startswith ( ']' ) : \n                                self . set_osc_code ( params ) \n    raw = string [ start : ] \n    substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n    if substring or self . actions : \n        yield substring \n    if last_char is not None : \n        self . actions . append ( NewLineAction ( 'newline' ) ) \n        yield last_char "}
{"14465": "\ndef get_color ( self , color , intensity = 0 ) : \n    if color is None : \n        return None \n    if color < 8 and intensity > 0 : \n        color += 8 \n    constructor = self . color_map . get ( color , None ) \n    if isinstance ( constructor , basestring ) : \n        return QtGui . QColor ( constructor ) \n    else : \n        if isinstance ( constructor , ( tuple , list ) ) : \n            return QtGui . QColor ( * constructor ) \n    return None "}
{"14474": "\ndef eventFilter ( self , obj , event ) : \n    etype = event . type ( ) \n    if etype == QtCore . QEvent . KeyPress : \n        key = event . key ( ) \n        if self . _control_key_down ( event . modifiers ( ) ) and key in self . _ctrl_down_remap : \n            new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , self . _ctrl_down_remap [ key ] , QtCore . Qt . NoModifier ) \n            QtGui . qApp . sendEvent ( obj , new_event ) \n            return True \n        else : \n            if obj == self . _control : \n                return self . _event_filter_console_keypress ( event ) \n            else : \n                if obj == self . _page_control : \n                    return self . _event_filter_page_keypress ( event ) \n    else : \n        if etype == QtCore . QEvent . MouseButtonRelease and event . button ( ) == QtCore . Qt . MidButton and obj == self . _control . viewport ( ) : \n            cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n            self . _control . setTextCursor ( cursor ) \n            self . paste ( QtGui . QClipboard . Selection ) \n            return True \n        else : \n            if etype == QtCore . QEvent . Resize and not self . _filter_resize : \n                self . _filter_resize = True \n                QtGui . qApp . sendEvent ( obj , event ) \n                self . _adjust_scrollbars ( ) \n                self . _filter_resize = False \n                return True \n            else : \n                if etype == QtCore . QEvent . ShortcutOverride and self . override_shortcuts and self . _control_key_down ( event . modifiers ( ) ) and event . key ( ) in self . _shortcuts : \n                    event . accept ( ) \n                else : \n                    if etype == QtCore . QEvent . DragEnter and obj == self . _control . viewport ( ) and event . source ( ) == self . _control . viewport ( ) : \n                        self . _filter_drag = True \n                    else : \n                        if etype == QtCore . QEvent . DragLeave and obj == self . _control . viewport ( ) and self . _filter_drag : \n                            cursor = self . _control . textCursor ( ) \n                            cursor . clearSelection ( ) \n                            self . _control . setTextCursor ( cursor ) \n                            self . _filter_drag = False \n                        else : \n                            if etype == QtCore . QEvent . Drop and obj == self . _control . viewport ( ) : \n                                cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n                                if self . _in_buffer ( cursor . position ( ) ) : \n                                    text = event . mimeData ( ) . text ( ) \n                                    self . _insert_plain_text_into_buffer ( cursor , text ) \n                                QtGui . qApp . sendEvent ( obj , QtGui . QDragLeaveEvent ( ) ) \n                                return True \n                            else : \n                                if etype in self . _pager_scroll_events and obj == self . _page_control : \n                                    self . _page_control . repaint ( ) \n                                    return True \n    return super ( ConsoleWidget , self ) . eventFilter ( obj , event ) "}
{"14480": "\ndef execute ( self , source = None , hidden = False , interactive = False ) : \n    if source is None : \n        source = self . input_buffer \n        if not hidden : \n            source += '\\n' \n    else : \n        if not hidden : \n            self . input_buffer = source \n    complete = self . _is_complete ( source , interactive ) \n    if hidden : \n        if complete : \n            self . _execute ( source , hidden ) \n        else : \n            error = 'Incomplete noninteractive input: \"%s\"' \n            raise RuntimeError ( error % source ) \n    else : \n        if complete : \n            self . _append_plain_text ( '\\n' ) \n            self . _input_buffer_executing = self . input_buffer \n            self . _executing = True \n            self . _prompt_finished ( ) \n            self . _control . document ( ) . setMaximumBlockCount ( self . buffer_size ) \n            self . _control . setUndoRedoEnabled ( False ) \n            self . _execute ( source , hidden ) \n        else : \n            cursor = self . _get_end_cursor ( ) \n            cursor . beginEditBlock ( ) \n            cursor . insertText ( '\\n' ) \n            self . _insert_continuation_prompt ( cursor ) \n            cursor . endEditBlock ( ) \n            self . _control . moveCursor ( QtGui . QTextCursor . End ) \n    return complete "}
{"14487": "\ndef reset_font ( self ) : \n    if sys . platform == 'win32' : \n        fallback = 'Courier' \n    else : \n        if sys . platform == 'darwin' : \n            fallback = 'Monaco' \n        else : \n            fallback = 'Monospace' \n    font = get_font ( self . font_family , fallback ) \n    if self . font_size : \n        font . setPointSize ( self . font_size ) \n    else : \n        font . setPointSize ( QtGui . qApp . font ( ) . pointSize ( ) ) \n    font . setStyleHint ( QtGui . QFont . TypeWriter ) \n    self . _set_font ( font ) "}
{"14493": "\ndef _complete_with_items ( self , cursor , items ) : \n    self . _cancel_completion ( ) \n    if len ( items ) == 1 : \n        cursor . setPosition ( self . _control . textCursor ( ) . position ( ) , QtGui . QTextCursor . KeepAnchor ) \n        cursor . insertText ( items [ 0 ] ) \n    else : \n        if len ( items ) > 1 : \n            current_pos = self . _control . textCursor ( ) . position ( ) \n            prefix = commonprefix ( items ) \n            if prefix : \n                cursor . setPosition ( current_pos , QtGui . QTextCursor . KeepAnchor ) \n                cursor . insertText ( prefix ) \n                current_pos = cursor . position ( ) \n            cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( prefix ) ) \n            self . _completion_widget . show_items ( cursor , items ) "}
{"14496": "\ndef _create_control ( self ) : \n    if self . custom_control : \n        control = self . custom_control ( ) \n    else : \n        if self . kind == 'plain' : \n            control = QtGui . QPlainTextEdit ( ) \n        else : \n            if self . kind == 'rich' : \n                control = QtGui . QTextEdit ( ) \n                control . setAcceptRichText ( False ) \n    control . installEventFilter ( self ) \n    control . viewport ( ) . installEventFilter ( self ) \n    control . customContextMenuRequested . connect ( self . _custom_context_menu_requested ) \n    control . copyAvailable . connect ( self . copy_available ) \n    control . redoAvailable . connect ( self . redo_available ) \n    control . undoAvailable . connect ( self . undo_available ) \n    layout = control . document ( ) . documentLayout ( ) \n    layout . documentSizeChanged . disconnect ( ) \n    layout . documentSizeChanged . connect ( self . _adjust_scrollbars ) \n    control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , True ) \n    control . setContextMenuPolicy ( QtCore . Qt . CustomContextMenu ) \n    control . setReadOnly ( True ) \n    control . setUndoRedoEnabled ( False ) \n    control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) \n    return control "}
{"14497": "\ndef _create_page_control ( self ) : \n    if self . custom_page_control : \n        control = self . custom_page_control ( ) \n    else : \n        if self . kind == 'plain' : \n            control = QtGui . QPlainTextEdit ( ) \n        else : \n            if self . kind == 'rich' : \n                control = QtGui . QTextEdit ( ) \n    control . installEventFilter ( self ) \n    viewport = control . viewport ( ) \n    viewport . installEventFilter ( self ) \n    control . setReadOnly ( True ) \n    control . setUndoRedoEnabled ( False ) \n    control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) \n    return control "}
{"14498": "\ndef _event_filter_page_keypress ( self , event ) : \n    key = event . key ( ) \n    ctrl_down = self . _control_key_down ( event . modifiers ( ) ) \n    alt_down = event . modifiers ( ) & QtCore . Qt . AltModifier \n    if ctrl_down : \n        if key == QtCore . Qt . Key_O : \n            self . _control . setFocus ( ) \n            intercept = True \n    else : \n        if alt_down : \n            if key == QtCore . Qt . Key_Greater : \n                self . _page_control . moveCursor ( QtGui . QTextCursor . End ) \n                intercepted = True \n            else : \n                if key == QtCore . Qt . Key_Less : \n                    self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n                    intercepted = True \n        else : \n            if key in ( QtCore . Qt . Key_Q , QtCore . Qt . Key_Escape ) : \n                if self . _splitter : \n                    self . _page_control . hide ( ) \n                    self . _control . setFocus ( ) \n                else : \n                    self . layout ( ) . setCurrentWidget ( self . _control ) \n                return True \n            else : \n                if key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return , QtCore . Qt . Key_Tab ) : \n                    new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageDown , QtCore . Qt . NoModifier ) \n                    QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n                    return True \n                else : \n                    if key == QtCore . Qt . Key_Backspace : \n                        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageUp , QtCore . Qt . NoModifier ) \n                        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n                        return True \n    return False "}
{"14508": "\ndef _insert_plain_text ( self , cursor , text ) : \n    cursor . beginEditBlock ( ) \n    if self . ansi_codes : \n        for substring in self . _ansi_processor . split_string ( text ) : \n            for act in self . _ansi_processor . actions : \n                if act . action == 'erase' and act . area == 'screen' : \n                    cursor . select ( QtGui . QTextCursor . Document ) \n                    cursor . removeSelectedText ( ) \n                else : \n                    if act . action == 'scroll' and act . unit == 'page' : \n                        cursor . insertText ( '\\n' ) \n                        cursor . endEditBlock ( ) \n                        self . _set_top_cursor ( cursor ) \n                        cursor . joinPreviousEditBlock ( ) \n                        cursor . deletePreviousChar ( ) \n                    else : \n                        if act . action == 'carriage-return' : \n                            cursor . movePosition ( cursor . StartOfLine , cursor . KeepAnchor ) \n                        else : \n                            if act . action == 'beep' : \n                                QtGui . qApp . beep ( ) \n                            else : \n                                if act . action == 'backspace' : \n                                    if not cursor . atBlockStart ( ) : \n                                        cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor ) \n                                else : \n                                    if act . action == 'newline' : \n                                        cursor . movePosition ( cursor . EndOfLine ) \n            format = self . _ansi_processor . get_format ( ) \n            selection = cursor . selectedText ( ) \n            if len ( selection ) == 0 : \n                cursor . insertText ( substring , format ) \n            else : \n                if substring is not None : \n                    if len ( substring ) >= len ( selection ) : \n                        cursor . insertText ( substring , format ) \n                    else : \n                        old_text = selection [ len ( substring ) : ] \n                        cursor . insertText ( substring + old_text , format ) \n                        cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor , len ( old_text ) ) \n    else : \n        cursor . insertText ( text ) \n    cursor . endEditBlock ( ) "}
{"14511": "\ndef _page ( self , text , html = False ) : \n    line_height = QtGui . QFontMetrics ( self . font ) . height ( ) \n    minlines = self . _control . viewport ( ) . height ( ) / line_height \n    if self . paging != 'none' and re . match ( \"(?:[^\\n]*\\n){%i}\" % minlines , text ) : \n        if self . paging == 'custom' : \n            self . custom_page_requested . emit ( text ) \n        else : \n            self . _page_control . clear ( ) \n            cursor = self . _page_control . textCursor ( ) \n            if html : \n                self . _insert_html ( cursor , text ) \n            else : \n                self . _insert_plain_text ( cursor , text ) \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            self . _page_control . viewport ( ) . resize ( self . _control . size ( ) ) \n            if self . _splitter : \n                self . _page_control . show ( ) \n                self . _page_control . setFocus ( ) \n            else : \n                self . layout ( ) . setCurrentWidget ( self . _page_control ) \n    else : \n        if html : \n            self . _append_html ( text ) \n        else : \n            self . _append_plain_text ( text ) "}
{"14530": "\ndef handle_pong ( self , msg ) : \n    current = str_to_bytes ( str ( self . lifetime ) ) \n    last = str_to_bytes ( str ( self . last_ping ) ) \n    if msg [ 1 ] == current : \n        delta = time . time ( ) - self . tic \n        self . responses . add ( msg [ 0 ] ) \n    else : \n        if msg [ 1 ] == last : \n            delta = time . time ( ) - self . tic + ( self . lifetime - self . last_ping ) \n            self . log . warn ( \"heartbeat::heart %r missed a beat, and took %.2f ms to respond\" , msg [ 0 ] , 1000 * delta ) \n            self . responses . add ( msg [ 0 ] ) \n        else : \n            self . log . warn ( \"heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)\" , msg [ 1 ] , self . lifetime ) "}
{"14538": "\ndef nt_quote_arg ( arg ) : \n    result = [ ] \n    needquote = False \n    nb = 0 \n    needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) \n    if needquote : \n        result . append ( '\"' ) \n    for c in arg : \n        if c == '\\\\' : \n            nb += 1 \n        else : \n            if c == '\"' : \n                result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) \n                nb = 0 \n            else : \n                if nb : \n                    result . append ( '\\\\' * nb ) \n                    nb = 0 \n                result . append ( c ) \n    if nb : \n        result . append ( '\\\\' * nb ) \n    if needquote : \n        result . append ( '\\\\' * nb ) \n        result . append ( '\"' ) \n    return '' . join ( result ) "}
{"14539": "\ndef check_conflicts ( self , dist ) : \n    return dist \n    from imp import find_module , get_suffixes \n    from glob import glob \n    blockers = [ ] \n    names = dict . fromkeys ( dist . _get_metadata ( 'top_level.txt' ) ) \n    exts = { '.pyc' : 1 , '.pyo' : 1 } \n    for ext , mode , typ in get_suffixes ( ) : \n        exts [ ext ] = 1 \n    for path , files in expand_paths ( [ self . install_dir ] + self . all_site_dirs ) : \n        for filename in files : \n            base , ext = os . path . splitext ( filename ) \n            if base in names : \n                if not ext : \n                    try : \n                        f , filename , descr = find_module ( base , [ path ] ) \n                    except ImportError : \n                        continue \n                    else : \n                        if f : \n                            f . close ( ) \n                        if filename not in blockers : \n                            blockers . append ( filename ) \n                else : \n                    if ext in exts and base != 'site' : \n                        blockers . append ( os . path . join ( path , filename ) ) \n    if blockers : \n        self . found_conflicts ( dist , blockers ) \n    return dist "}
{"14551": "\ndef filefind ( filename , path_dirs = None ) : \n    filename = filename . strip ( '\"' ) . strip ( \"'\" ) \n    if os . path . isabs ( filename ) and os . path . isfile ( filename ) : \n        return filename \n    if path_dirs is None : \n        path_dirs = ( \"\" , ) \n    else : \n        if isinstance ( path_dirs , basestring ) : \n            path_dirs = ( path_dirs , ) \n    for path in path_dirs : \n        if path == '.' : \n            path = os . getcwdu ( ) \n        testname = expand_path ( os . path . join ( path , filename ) ) \n        if os . path . isfile ( testname ) : \n            return os . path . abspath ( testname ) \n    raise IOError ( \"File %r does not exist in any of the search paths: %r\" % ( filename , path_dirs ) ) "}
{"14554": "\ndef get_ipython_dir ( ) : \n    env = os . environ \n    pjoin = os . path . join \n    ipdir_def = '.ipython' \n    xdg_def = 'ipython' \n    home_dir = get_home_dir ( ) \n    xdg_dir = get_xdg_dir ( ) \n    if 'IPYTHON_DIR' in env : \n        warnings . warn ( 'The environment variable IPYTHON_DIR is deprecated. ' 'Please use IPYTHONDIR instead.' ) \n    ipdir = env . get ( 'IPYTHONDIR' , env . get ( 'IPYTHON_DIR' , None ) ) \n    if ipdir is None : \n        home_ipdir = pjoin ( home_dir , ipdir_def ) \n        if xdg_dir : \n            xdg_ipdir = pjoin ( xdg_dir , xdg_def ) \n            if _writable_dir ( xdg_ipdir ) or not _writable_dir ( home_ipdir ) : \n                ipdir = xdg_ipdir \n        if ipdir is None : \n            ipdir = home_ipdir \n    ipdir = os . path . normpath ( os . path . expanduser ( ipdir ) ) \n    if os . path . exists ( ipdir ) and not _writable_dir ( ipdir ) : \n        warnings . warn ( \"IPython dir '%s' is not a writable location,\" \" using a temp directory.\" % ipdir ) \n        ipdir = tempfile . mkdtemp ( ) \n    else : \n        if not os . path . exists ( ipdir ) : \n            parent = ipdir . rsplit ( os . path . sep , 1 ) [ 0 ] \n            if not _writable_dir ( parent ) : \n                warnings . warn ( \"IPython parent '%s' is not a writable location,\" \" using a temp directory.\" % parent ) \n                ipdir = tempfile . mkdtemp ( ) \n    return py3compat . cast_unicode ( ipdir , fs_encoding ) "}
{"14586": "\ndef _get_range_session ( self , start = 1 , stop = None , raw = True , output = False ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if start < 0 : \n        start += n \n    if not stop or ( stop > n ) : \n        stop = n \n    else : \n        if stop < 0 : \n            stop += n \n    for i in range ( start , stop ) : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( 0 , i , line ) "}
{"14595": "\ndef format_lines ( statements , lines ) : \n    pairs = [ ] \n    i = 0 \n    j = 0 \n    start = None \n    statements = sorted ( statements ) \n    lines = sorted ( lines ) \n    while i < len ( statements ) and j < len ( lines ) : \n        if statements [ i ] == lines [ j ] : \n            if start == None : \n                start = lines [ j ] \n            end = lines [ j ] \n            j += 1 \n        else : \n            if start : \n                pairs . append ( ( start , end ) ) \n                start = None \n        i += 1 \n    if start : \n        pairs . append ( ( start , end ) ) \n    ret = ', ' . join ( map ( nice_pair , pairs ) ) \n    return ret "}
{"14598": "\ndef join_regex ( regexes ) : \n    if len ( regexes ) > 1 : \n        return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) \n    else : \n        if regexes : \n            return regexes [ 0 ] \n        else : \n            return \"\" "}
{"14600": "\ndef update ( self , v ) : \n    self . md5 . update ( to_bytes ( str ( type ( v ) ) ) ) \n    if isinstance ( v , string_class ) : \n        self . md5 . update ( to_bytes ( v ) ) \n    else : \n        if v is None : \n            pass \n        else : \n            if isinstance ( v , ( int , float ) ) : \n                self . md5 . update ( to_bytes ( str ( v ) ) ) \n            else : \n                if isinstance ( v , ( tuple , list ) ) : \n                    for e in v : \n                        self . update ( e ) \n                else : \n                    if isinstance ( v , dict ) : \n                        keys = v . keys ( ) \n                        for k in sorted ( keys ) : \n                            self . update ( k ) \n                            self . update ( v [ k ] ) \n                    else : \n                        for k in dir ( v ) : \n                            if k . startswith ( '__' ) : \n                                continue \n                            a = getattr ( v , k ) \n                            if inspect . isroutine ( a ) : \n                                continue \n                            self . update ( k ) \n                            self . update ( a ) "}
{"14616": "\ndef _run_cmd_line_code ( self ) : \n    if self . code_to_run : \n        line = self . code_to_run \n        try : \n            self . log . info ( \"Running code given at command line (c=): %s\" % line ) \n            self . shell . run_cell ( line , store_history = False ) \n        except : \n            self . log . warn ( \"Error in executing line in user namespace: %s\" % line ) \n            self . shell . showtraceback ( ) \n    else : \n        if self . file_to_run : \n            fname = self . file_to_run \n            try : \n                self . _exec_file ( fname ) \n            except : \n                self . log . warn ( \"Error in executing file in user namespace: %s\" % fname ) \n                self . shell . showtraceback ( ) "}
{"14627": "\ndef read ( self , directory ) : \n    usable = False \n    try : \n        status_file = os . path . join ( directory , self . STATUS_FILE ) \n        fstatus = open ( status_file , \"rb\" ) \n        try : \n            status = pickle . load ( fstatus ) \n        finally : \n            fstatus . close ( ) \n    except ( IOError , ValueError ) : \n        usable = False \n    else : \n        usable = True \n        if status [ 'format' ] != self . STATUS_FORMAT : \n            usable = False \n        else : \n            if status [ 'version' ] != coverage . __version__ : \n                usable = False \n    if usable : \n        self . files = status [ 'files' ] \n        self . settings = status [ 'settings' ] \n    else : \n        self . reset ( ) "}
{"14658": "\ndef get_text_query ( query_string , search_fields ) : \n    include_terms , exclude_terms = get_text_tokenizer ( query_string ) \n    include_q = get_query_includes ( include_terms , search_fields ) \n    exclude_q = get_query_excludes ( exclude_terms , search_fields ) \n    query = None \n    if include_q and exclude_q : \n        query = include_q & ~ exclude_q \n    else : \n        if not exclude_q : \n            query = include_q \n        else : \n            query = ~ exclude_q \n    return query "}
{"14664": "\ndef validateAttrib ( self , method , cls = None ) : \n    any = False \n    for group in self . attribs : \n        match = True \n        for key , value in group : \n            attr = get_method_attr ( method , cls , key ) \n            if callable ( value ) : \n                if not value ( key , method , cls ) : \n                    match = False \n                    break \n            else : \n                if value is True : \n                    if not bool ( attr ) : \n                        match = False \n                        break \n                else : \n                    if value is False : \n                        if bool ( attr ) : \n                            match = False \n                            break \n                    else : \n                        if type ( attr ) in ( list , tuple ) : \n                            if not str ( value ) . lower ( ) in [ str ( x ) . lower ( ) for x in attr ] : \n                                match = False \n                                break \n                        else : \n                            if ( value != attr and str ( value ) . lower ( ) != str ( attr ) . lower ( ) ) : \n                                match = False \n                                break \n        any = any or match \n    if any : \n        return None \n    return False "}
{"14701": "\ndef module_list ( path ) : \n    if path == '' : \n        path = '.' \n    if os . path . isdir ( path ) : \n        folder_list = os . listdir ( path ) \n    else : \n        if path . endswith ( '.egg' ) : \n            try : \n                folder_list = [ f for f in zipimporter ( path ) . _files ] \n            except : \n                folder_list = [ ] \n        else : \n            folder_list = [ ] \n    if not folder_list : \n        return [ ] \n    isfile = os . path . isfile \n    pjoin = os . path . join \n    basename = os . path . basename \n    def is_importable_file ( path ) : \n        name , extension = os . path . splitext ( path ) \n        return import_re . match ( path ) and py3compat . isidentifier ( name ) \n    folder_list = [ p for p in folder_list if isfile ( pjoin ( path , p , '__init__.py' ) ) or is_importable_file ( p ) ] \n    return [ basename ( p ) . split ( '.' ) [ 0 ] for p in folder_list ] "}
{"14726": "\ndef handle_unmet_dependency ( self , idents , parent ) : \n    engine = idents [ 0 ] \n    msg_id = parent [ 'msg_id' ] \n    job = self . pending [ engine ] . pop ( msg_id ) \n    job . blacklist . add ( engine ) \n    if job . blacklist == job . targets : \n        self . depending [ msg_id ] = job \n        self . fail_unreachable ( msg_id ) \n    else : \n        if not self . maybe_run ( job ) : \n            if msg_id not in self . all_failed : \n                self . save_unmet ( job ) \n    if self . hwm : \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            if self . loads [ idx ] == self . hwm - 1 : \n                self . update_graph ( None ) "}
{"14727": "\ndef update_graph ( self , dep_id = None , success = True ) : \n    jobs = self . graph . pop ( dep_id , [ ] ) \n    if dep_id is None or self . hwm and any ( [ load == self . hwm - 1 for load in self . loads ] ) : \n        jobs = self . depending . keys ( ) \n    for msg_id in sorted ( jobs , key = lambda msg_id : self . depending [ msg_id ] . timestamp ) : \n        job = self . depending [ msg_id ] \n        if job . after . unreachable ( self . all_completed , self . all_failed ) or job . follow . unreachable ( self . all_completed , self . all_failed ) : \n            self . fail_unreachable ( msg_id ) \n        else : \n            if job . after . check ( self . all_completed , self . all_failed ) : \n                if self . maybe_run ( job ) : \n                    self . depending . pop ( msg_id ) \n                    for mid in job . dependents : \n                        if mid in self . graph : \n                            self . graph [ mid ] . remove ( msg_id ) "}
{"14731": "\ndef log_write ( self , data , kind = 'input' ) : \n    if self . log_active and data : \n        write = self . logfile . write \n        if kind == 'input' : \n            if self . timestamp : \n                write ( str_to_unicode ( time . strftime ( '# %a, %d %b %Y %H:%M:%S\\n' , time . localtime ( ) ) ) ) \n            write ( data ) \n        else : \n            if kind == 'output' and self . log_output : \n                odata = u'\\n' . join ( [ u'#[Out]# %s' % s for s in data . splitlines ( ) ] ) \n                write ( u'%s\\n' % odata ) \n        self . logfile . flush ( ) "}
{"14753": "\ndef phys_tokens ( toks ) : \n    last_line = None \n    last_lineno = - 1 \n    last_ttype = None \n    for ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext in toks : \n        if last_lineno != elineno : \n            if last_line and last_line . endswith ( \"\\\\\\n\" ) : \n                inject_backslash = True \n                if last_ttype == tokenize . COMMENT : \n                    inject_backslash = False \n                else : \n                    if ttype == token . STRING : \n                        if \"\\n\" in ttext and ttext . split ( '\\n' , 1 ) [ 0 ] [ - 1 ] == '\\\\' : \n                            inject_backslash = False \n                if inject_backslash : \n                    ccol = len ( last_line . split ( \"\\n\" ) [ - 2 ] ) - 1 \n                    yield ( 99999 , \"\\\\\\n\" , ( slineno , ccol ) , ( slineno , ccol + 2 ) , last_line ) \n            last_line = ltext \n            last_ttype = ttype \n        yield ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext \n        last_lineno = elineno "}
{"14754": "\ndef source_token_lines ( source ) : \n    ws_tokens = set ( [ token . INDENT , token . DEDENT , token . NEWLINE , tokenize . NL ] ) \n    line = [ ] \n    col = 0 \n    source = source . expandtabs ( 8 ) . replace ( '\\r\\n' , '\\n' ) \n    tokgen = generate_tokens ( source ) \n    for ttype , ttext , ( _ , scol ) , ( _ , ecol ) , _ in phys_tokens ( tokgen ) : \n        mark_start = True \n        for part in re . split ( '(\\n)' , ttext ) : \n            if part == '\\n' : \n                yield line \n                line = [ ] \n                col = 0 \n                mark_end = False \n            else : \n                if part == '' : \n                    mark_end = False \n                else : \n                    if ttype in ws_tokens : \n                        mark_end = False \n                    else : \n                        if mark_start and scol > col : \n                            line . append ( ( \"ws\" , \" \" * ( scol - col ) ) ) \n                            mark_start = False \n                        tok_class = tokenize . tok_name . get ( ttype , 'xx' ) . lower ( ) [ : 3 ] \n                        if ttype == token . NAME and keyword . iskeyword ( ttext ) : \n                            tok_class = \"key\" \n                        line . append ( ( tok_class , part ) ) \n                        mark_end = True \n            scol = 0 \n        if mark_end : \n            col = ecol \n    if line : \n        yield line "}
{"14762": "\ndef parse_notifier_name ( name ) : \n    if isinstance ( name , str ) : \n        return [ name ] \n    else : \n        if name is None : \n            return [ 'anytrait' ] \n        else : \n            if isinstance ( name , ( list , tuple ) ) : \n                for n in name : \n                    assert isinstance ( n , str ) , \"names must be strings\" \n                return name "}
{"14781": "\ndef save_task_request ( self , idents , msg ) : \n    client_id = idents [ 0 ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::client %r sent invalid task message: %r\" , client_id , msg , exc_info = True ) \n        return \n    record = init_record ( msg ) \n    record [ 'client_uuid' ] = client_id . decode ( 'ascii' ) \n    record [ 'queue' ] = 'task' \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    self . pending . add ( msg_id ) \n    self . unassigned . add ( msg_id ) \n    try : \n        existing = self . db . get_record ( msg_id ) \n        if existing [ 'resubmitted' ] : \n            for key in ( 'submitted' , 'client_uuid' , 'buffers' ) : \n                record . pop ( key ) \n        for key , evalue in existing . iteritems ( ) : \n            if key . endswith ( 'buffers' ) : \n                continue \n            rvalue = record . get ( key , None ) \n            if evalue and rvalue and evalue != rvalue : \n                self . log . warn ( \"conflicting initial state for record: %r:%r <%r> %r\" , msg_id , rvalue , key , evalue ) \n            else : \n                if evalue and not rvalue : \n                    record [ key ] = evalue \n        try : \n            self . db . update_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error updating record %r\" , msg_id , exc_info = True ) \n    except KeyError : \n        try : \n            self . db . add_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error adding record %r\" , msg_id , exc_info = True ) \n    except Exception : \n        self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = True ) "}
{"14783": "\ndef save_iopub_message ( self , topics , msg ) : \n    try : \n        msg = self . session . unserialize ( msg , content = True ) \n    except Exception : \n        self . log . error ( \"iopub::invalid IOPub message\" , exc_info = True ) \n        return \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        self . log . warn ( \"iopub::IOPub message lacks parent: %r\" , msg ) \n        return \n    msg_id = parent [ 'msg_id' ] \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    content = msg [ 'content' ] \n    try : \n        rec = self . db . get_record ( msg_id ) \n    except KeyError : \n        rec = empty_record ( ) \n        rec [ 'msg_id' ] = msg_id \n        self . db . add_record ( msg_id , rec ) \n    d = { } \n    if msg_type == 'stream' : \n        name = content [ 'name' ] \n        s = rec [ name ] or '' \n        d [ name ] = s + content [ 'data' ] \n    else : \n        if msg_type == 'pyerr' : \n            d [ 'pyerr' ] = content \n        else : \n            if msg_type == 'pyin' : \n                d [ 'pyin' ] = content [ 'code' ] \n            else : \n                if msg_type in ( 'display_data' , 'pyout' ) : \n                    d [ msg_type ] = content \n                else : \n                    if msg_type == 'status' : \n                        pass \n                    else : \n                        self . log . warn ( \"unhandled iopub msg_type: %r\" , msg_type ) \n    if not d : \n        return \n    try : \n        self . db . update_record ( msg_id , d ) \n    except Exception : \n        self . log . error ( \"DB Error saving iopub message %r\" , msg_id , exc_info = True ) "}
{"14785": "\ndef register_engine ( self , reg , msg ) : \n    content = msg [ 'content' ] \n    try : \n        queue = cast_bytes ( content [ 'queue' ] ) \n    except KeyError : \n        self . log . error ( \"registration::queue not specified\" , exc_info = True ) \n        return \n    heart = content . get ( 'heartbeat' , None ) \n    if heart : \n        heart = cast_bytes ( heart ) \n    eid = self . _next_id \n    self . log . debug ( \"registration::register_engine(%i, %r, %r, %r)\" , eid , queue , reg , heart ) \n    content = dict ( id = eid , status = 'ok' ) \n    content . update ( self . engine_info ) \n    if queue in self . by_ident : \n        try : \n            raise KeyError ( \"queue_id %r in use\" % queue ) \n        except : \n            content = error . wrap_exception ( ) \n            self . log . error ( \"queue_id %r in use\" , queue , exc_info = True ) \n    else : \n        if heart in self . hearts : \n            try : \n                raise KeyError ( \"heart_id %r in use\" % heart ) \n            except : \n                self . log . error ( \"heart_id %r in use\" , heart , exc_info = True ) \n                content = error . wrap_exception ( ) \n        else : \n            for h , pack in self . incoming_registrations . iteritems ( ) : \n                if heart == h : \n                    try : \n                        raise KeyError ( \"heart_id %r in use\" % heart ) \n                    except : \n                        self . log . error ( \"heart_id %r in use\" , heart , exc_info = True ) \n                        content = error . wrap_exception ( ) \n                    break \n                else : \n                    if queue == pack [ 1 ] : \n                        try : \n                            raise KeyError ( \"queue_id %r in use\" % queue ) \n                        except : \n                            self . log . error ( \"queue_id %r in use\" , queue , exc_info = True ) \n                            content = error . wrap_exception ( ) \n                        break \n    msg = self . session . send ( self . query , \"registration_reply\" , content = content , ident = reg ) \n    if content [ 'status' ] == 'ok' : \n        if heart in self . heartmonitor . hearts : \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) \n            self . finish_registration ( heart ) \n        else : \n            purge = lambda : self . _purge_stalled_registration ( heart ) \n            dc = ioloop . DelayedCallback ( purge , self . registration_timeout , self . loop ) \n            dc . start ( ) \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) \n    else : \n        self . log . error ( \"registration::registration %i failed: %r\" , eid , content [ 'evalue' ] ) \n    return eid "}
{"14791": "\ndef get_results ( self , client_id , msg ) : \n    content = msg [ 'content' ] \n    msg_ids = sorted ( set ( content [ 'msg_ids' ] ) ) \n    statusonly = content . get ( 'status_only' , False ) \n    pending = [ ] \n    completed = [ ] \n    content = dict ( status = 'ok' ) \n    content [ 'pending' ] = pending \n    content [ 'completed' ] = completed \n    buffers = [ ] \n    if not statusonly : \n        try : \n            matches = self . db . find_records ( dict ( msg_id = { '$in' : msg_ids } ) ) \n            records = { } \n            for rec in matches : \n                records [ rec [ 'msg_id' ] ] = rec \n        except Exception : \n            content = error . wrap_exception ( ) \n            self . session . send ( self . query , \"result_reply\" , content = content , parent = msg , ident = client_id ) \n            return \n    else : \n        records = { } \n    for msg_id in msg_ids : \n        if msg_id in self . pending : \n            pending . append ( msg_id ) \n        else : \n            if msg_id in self . all_completed : \n                completed . append ( msg_id ) \n                if not statusonly : \n                    c , bufs = self . _extract_record ( records [ msg_id ] ) \n                    content [ msg_id ] = c \n                    buffers . extend ( bufs ) \n            else : \n                if msg_id in records : \n                    if rec [ 'completed' ] : \n                        completed . append ( msg_id ) \n                        c , bufs = self . _extract_record ( records [ msg_id ] ) \n                        content [ msg_id ] = c \n                        buffers . extend ( bufs ) \n                    else : \n                        pending . append ( msg_id ) \n                else : \n                    try : \n                        raise KeyError ( 'No such message: ' + msg_id ) \n                    except : \n                        content = error . wrap_exception ( ) \n                    break \n    self . session . send ( self . query , \"result_reply\" , content = content , parent = msg , ident = client_id , buffers = buffers ) "}
{"14801": "\ndef annotate_file ( self , cu , analysis ) : \n    if not cu . relative : \n        return \n    filename = cu . filename \n    source = cu . source_file ( ) \n    if self . directory : \n        dest_file = os . path . join ( self . directory , cu . flat_rootname ( ) ) \n        dest_file += \".py,cover\" \n    else : \n        dest_file = filename + \",cover\" \n    dest = open ( dest_file , 'w' ) \n    statements = sorted ( analysis . statements ) \n    missing = sorted ( analysis . missing ) \n    excluded = sorted ( analysis . excluded ) \n    lineno = 0 \n    i = 0 \n    j = 0 \n    covered = True \n    while True : \n        line = source . readline ( ) \n        if line == '' : \n            break \n        lineno += 1 \n        while i < len ( statements ) and statements [ i ] < lineno : \n            i += 1 \n        while j < len ( missing ) and missing [ j ] < lineno : \n            j += 1 \n        if i < len ( statements ) and statements [ i ] == lineno : \n            covered = j >= len ( missing ) or missing [ j ] > lineno \n        if self . blank_re . match ( line ) : \n            dest . write ( '  ' ) \n        else : \n            if self . else_re . match ( line ) : \n                if i >= len ( statements ) and j >= len ( missing ) : \n                    dest . write ( '! ' ) \n                else : \n                    if i >= len ( statements ) or j >= len ( missing ) : \n                        dest . write ( '> ' ) \n                    else : \n                        if statements [ i ] == missing [ j ] : \n                            dest . write ( '! ' ) \n                        else : \n                            dest . write ( '> ' ) \n            else : \n                if lineno in excluded : \n                    dest . write ( '- ' ) \n                else : \n                    if covered : \n                        dest . write ( '> ' ) \n                    else : \n                        dest . write ( '! ' ) \n        dest . write ( line ) \n    source . close ( ) \n    dest . close ( ) "}
{"14803": "\ndef squash_unicode ( obj ) : \n    if isinstance ( obj , dict ) : \n        for key in obj . keys ( ) : \n            obj [ key ] = squash_unicode ( obj [ key ] ) \n            if isinstance ( key , unicode ) : \n                obj [ squash_unicode ( key ) ] = obj . pop ( key ) \n    else : \n        if isinstance ( obj , list ) : \n            for i , v in enumerate ( obj ) : \n                obj [ i ] = squash_unicode ( v ) \n        else : \n            if isinstance ( obj , unicode ) : \n                obj = obj . encode ( 'utf8' ) \n    return obj "}
{"14808": "\ndef serialize ( self , msg , ident = None ) : \n    content = msg . get ( 'content' , { } ) \n    if content is None : \n        content = self . none \n    else : \n        if isinstance ( content , dict ) : \n            content = self . pack ( content ) \n        else : \n            if isinstance ( content , bytes ) : \n                pass \n            else : \n                if isinstance ( content , unicode ) : \n                    content = content . encode ( 'utf8' ) \n                else : \n                    raise TypeError ( \"Content incorrect type: %s\" % type ( content ) ) \n    real_message = [ self . pack ( msg [ 'header' ] ) , self . pack ( msg [ 'parent_header' ] ) , content ] \n    to_send = [ ] \n    if isinstance ( ident , list ) : \n        to_send . extend ( ident ) \n    else : \n        if ident is not None : \n            to_send . append ( ident ) \n    to_send . append ( DELIM ) \n    signature = self . sign ( real_message ) \n    to_send . append ( signature ) \n    to_send . extend ( real_message ) \n    return to_send "}
{"14809": "\ndef send ( self , stream , msg_or_type , content = None , parent = None , ident = None , buffers = None , subheader = None , track = False , header = None ) : \n    if not isinstance ( stream , ( zmq . Socket , ZMQStream ) ) : \n        raise TypeError ( \"stream must be Socket or ZMQStream, not %r\" % type ( stream ) ) \n    else : \n        if track and isinstance ( stream , ZMQStream ) : \n            raise TypeError ( \"ZMQStream cannot track messages\" ) \n    if isinstance ( msg_or_type , ( Message , dict ) ) : \n        msg = msg_or_type \n    else : \n        msg = self . msg ( msg_or_type , content = content , parent = parent , subheader = subheader , header = header ) \n    buffers = [ ] if buffers is None else buffers \n    to_send = self . serialize ( msg , ident ) \n    flag = 0 \n    if buffers : \n        flag = zmq . SNDMORE \n        _track = False \n    else : \n        _track = track \n    if track : \n        tracker = stream . send_multipart ( to_send , flag , copy = False , track = _track ) \n    else : \n        tracker = stream . send_multipart ( to_send , flag , copy = False ) \n    for b in buffers [ : - 1 ] : \n        stream . send ( b , flag , copy = False ) \n    if buffers : \n        if track : \n            tracker = stream . send ( buffers [ - 1 ] , copy = False , track = track ) \n        else : \n            tracker = stream . send ( buffers [ - 1 ] , copy = False ) \n    if self . debug : \n        pprint . pprint ( msg ) \n        pprint . pprint ( to_send ) \n        pprint . pprint ( buffers ) \n    msg [ 'tracker' ] = tracker \n    return msg "}
{"14820": "\ndef getargspec ( obj ) : \n    if inspect . isfunction ( obj ) : \n        func_obj = obj \n    else : \n        if inspect . ismethod ( obj ) : \n            func_obj = obj . im_func \n        else : \n            if hasattr ( obj , '__call__' ) : \n                func_obj = obj . __call__ \n            else : \n                raise TypeError ( 'arg is not a Python function' ) \n    args , varargs , varkw = inspect . getargs ( func_obj . func_code ) \n    return args , varargs , varkw , func_obj . func_defaults "}
{"14828": "\ndef pdoc ( self , obj , oname = '' , formatter = None ) : \n    head = self . __head \n    lines = [ ] \n    ds = getdoc ( obj ) \n    if formatter : \n        ds = formatter ( ds ) \n    if ds : \n        lines . append ( head ( \"Class Docstring:\" ) ) \n        lines . append ( indent ( ds ) ) \n    if inspect . isclass ( obj ) and hasattr ( obj , '__init__' ) : \n        init_ds = getdoc ( obj . __init__ ) \n        if init_ds is not None : \n            lines . append ( head ( \"Constructor Docstring:\" ) ) \n            lines . append ( indent ( init_ds ) ) \n    else : \n        if hasattr ( obj , '__call__' ) : \n            call_ds = getdoc ( obj . __call__ ) \n            if call_ds : \n                lines . append ( head ( \"Calling Docstring:\" ) ) \n                lines . append ( indent ( call_ds ) ) \n    if not lines : \n        self . noinfo ( 'documentation' , oname ) \n    else : \n        page . page ( '\\n' . join ( lines ) ) "}
{"14832": "\ndef pinfo ( self , obj , oname = '' , formatter = None , info = None , detail_level = 0 ) : \n    info = self . info ( obj , oname = oname , formatter = formatter , info = info , detail_level = detail_level ) \n    displayfields = [ ] \n    def add_fields ( fields ) : \n        for title , key in fields : \n            field = info [ key ] \n            if field is not None : \n                displayfields . append ( ( title , field . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields1 ) \n    if ( not py3compat . PY3 ) and isinstance ( obj , types . InstanceType ) and info [ 'base_class' ] : \n        displayfields . append ( ( \"Base Class\" , info [ 'base_class' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields2 ) \n    if info [ 'namespace' ] != 'Interactive' : \n        displayfields . append ( ( \"Namespace\" , info [ 'namespace' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields3 ) \n    if detail_level > 0 and info [ 'source' ] is not None : \n        displayfields . append ( ( \"Source\" , self . format ( py3compat . cast_bytes_py2 ( info [ 'source' ] ) ) ) ) \n    else : \n        if info [ 'docstring' ] is not None : \n            displayfields . append ( ( \"Docstring\" , info [ \"docstring\" ] ) ) \n    if info [ 'isclass' ] : \n        if info [ 'init_definition' ] or info [ 'init_docstring' ] : \n            displayfields . append ( ( \"Constructor information\" , \"\" ) ) \n            if info [ 'init_definition' ] is not None : \n                displayfields . append ( ( \" Definition\" , info [ 'init_definition' ] . rstrip ( ) ) ) \n            if info [ 'init_docstring' ] is not None : \n                displayfields . append ( ( \" Docstring\" , indent ( info [ 'init_docstring' ] ) ) ) \n    else : \n        add_fields ( self . pinfo_fields_obj ) \n    if displayfields : \n        page . page ( self . _format_fields ( displayfields ) ) "}
{"14833": "\ndef psearch ( self , pattern , ns_table , ns_search = [ ] , ignore_case = False , show_all = False ) : \n    type_pattern = 'all' \n    filter = '' \n    cmds = pattern . split ( ) \n    len_cmds = len ( cmds ) \n    if len_cmds == 1 : \n        filter = cmds [ 0 ] \n    else : \n        if len_cmds == 2 : \n            filter , type_pattern = cmds \n        else : \n            raise ValueError ( 'invalid argument string for psearch: <%s>' % pattern ) \n    for name in ns_search : \n        if name not in ns_table : \n            raise ValueError ( 'invalid namespace <%s>. Valid names: %s' % ( name , ns_table . keys ( ) ) ) \n    search_result , namespaces_seen = set ( ) , set ( ) \n    for ns_name in ns_search : \n        ns = ns_table [ ns_name ] \n        if id ( ns ) in namespaces_seen : \n            continue \n        namespaces_seen . add ( id ( ns ) ) \n        tmp_res = list_namespace ( ns , type_pattern , filter , ignore_case = ignore_case , show_all = show_all ) \n        search_result . update ( tmp_res ) \n    page . page ( '\\n' . join ( sorted ( search_result ) ) ) "}
{"14839": "\ndef load_all_modules_in_packages ( package_or_set_of_packages ) : \n    if isinstance ( package_or_set_of_packages , types . ModuleType ) : \n        packages = [ package_or_set_of_packages ] \n    else : \n        if isinstance ( package_or_set_of_packages , Iterable ) and not isinstance ( package_or_set_of_packages , ( dict , str ) ) : \n            packages = package_or_set_of_packages \n        else : \n            raise Exception ( \"This function only accepts a module reference, or an iterable of said objects\" ) \n    imported = packages . copy ( ) \n    for package in packages : \n        if not hasattr ( package , '__path__' ) : \n            raise Exception ( 'Package object passed in has no __path__ attribute. ' 'Make sure to pass in imported references to the packages in question.' ) \n        for module_finder , name , ispkg in pkgutil . walk_packages ( package . __path__ ) : \n            module_name = '{}.{}' . format ( package . __name__ , name ) \n            current_module = importlib . import_module ( module_name ) \n            imported . append ( current_module ) \n            if ispkg : \n                imported += load_all_modules_in_packages ( current_module ) \n    for module in imported : \n        dir ( module ) \n    return list ( { module . __name__ : module for module in imported } . values ( ) ) "}
{"14843": "\ndef format2 ( self , raw , out = None , scheme = '' ) : \n    string_output = 0 \n    if out == 'str' or self . out == 'str' or isinstance ( self . out , StringIO . StringIO ) : \n        out_old = self . out \n        self . out = StringIO . StringIO ( ) \n        string_output = 1 \n    else : \n        if out is not None : \n            self . out = out \n    if scheme == 'NoColor' : \n        error = False \n        self . out . write ( raw ) \n        if string_output : \n            return raw , error \n        else : \n            return None , error \n    colors = self . color_table [ scheme ] . colors \n    self . colors = colors \n    self . raw = raw . expandtabs ( ) . rstrip ( ) \n    self . lines = [ 0 , 0 ] \n    pos = 0 \n    raw_find = self . raw . find \n    lines_append = self . lines . append \n    while 1 : \n        pos = raw_find ( '\\n' , pos ) + 1 \n        if not pos : \n            break \n        lines_append ( pos ) \n    lines_append ( len ( self . raw ) ) \n    self . pos = 0 \n    text = StringIO . StringIO ( self . raw ) \n    error = False \n    try : \n        for atoken in generate_tokens ( text . readline ) : \n            self ( * atoken ) \n    except tokenize . TokenError as ex : \n        msg = ex . args [ 0 ] \n        line = ex . args [ 1 ] [ 0 ] \n        self . out . write ( \"%s\\n\\n*** ERROR: %s%s%s\\n\" % ( colors [ token . ERRORTOKEN ] , msg , self . raw [ self . lines [ line ] : ] , colors . normal ) ) \n        error = True \n    self . out . write ( colors . normal + '\\n' ) \n    if string_output : \n        output = self . out . getvalue ( ) \n        self . out = out_old \n        return ( output , error ) \n    return ( None , error ) "}
{"14847": "\ndef select_figure_format ( shell , fmt ) : \n    from matplotlib . figure import Figure \n    from IPython . zmq . pylab import backend_inline \n    svg_formatter = shell . display_formatter . formatters [ 'image/svg+xml' ] \n    png_formatter = shell . display_formatter . formatters [ 'image/png' ] \n    if fmt == 'png' : \n        svg_formatter . type_printers . pop ( Figure , None ) \n        png_formatter . for_type ( Figure , lambda fig : print_figure ( fig , 'png' ) ) \n    else : \n        if fmt == 'svg' : \n            png_formatter . type_printers . pop ( Figure , None ) \n            svg_formatter . for_type ( Figure , lambda fig : print_figure ( fig , 'svg' ) ) \n        else : \n            raise ValueError ( \"supported formats are: 'png', 'svg', not %r\" % fmt ) \n    backend_inline . _figure_format = fmt "}
{"14852": "\ndef _trace ( self , frame , event , arg_unused ) : \n    if self . stopped : \n        return \n    if 0 : \n        sys . stderr . write ( \"trace event: %s %r @%d\\n\" % ( event , frame . f_code . co_filename , frame . f_lineno ) ) \n    if self . last_exc_back : \n        if frame == self . last_exc_back : \n            if self . arcs and self . cur_file_data : \n                pair = ( self . last_line , - self . last_exc_firstlineno ) \n                self . cur_file_data [ pair ] = None \n            self . cur_file_data , self . last_line = self . data_stack . pop ( ) \n        self . last_exc_back = None \n    if event == 'call' : \n        self . data_stack . append ( ( self . cur_file_data , self . last_line ) ) \n        filename = frame . f_code . co_filename \n        if filename not in self . should_trace_cache : \n            tracename = self . should_trace ( filename , frame ) \n            self . should_trace_cache [ filename ] = tracename \n        else : \n            tracename = self . should_trace_cache [ filename ] \n        if tracename : \n            if tracename not in self . data : \n                self . data [ tracename ] = { } \n            self . cur_file_data = self . data [ tracename ] \n        else : \n            self . cur_file_data = None \n        self . last_line = - 1 \n    else : \n        if event == 'line' : \n            if self . cur_file_data is not None : \n                if self . arcs : \n                    self . cur_file_data [ ( self . last_line , frame . f_lineno ) ] = None \n                else : \n                    self . cur_file_data [ frame . f_lineno ] = None \n            self . last_line = frame . f_lineno \n        else : \n            if event == 'return' : \n                if self . arcs and self . cur_file_data : \n                    first = frame . f_code . co_firstlineno \n                    self . cur_file_data [ ( self . last_line , - first ) ] = None \n                self . cur_file_data , self . last_line = self . data_stack . pop ( ) \n            else : \n                if event == 'exception' : \n                    self . last_exc_back = frame . f_back \n                    self . last_exc_firstlineno = frame . f_code . co_firstlineno \n    return self . _trace "}
{"14866": "\ndef _source_for_file ( self , filename ) : \n    if not filename . endswith ( \".py\" ) : \n        if filename [ - 4 : - 1 ] == \".py\" : \n            filename = filename [ : - 1 ] \n        else : \n            if filename . endswith ( \"$py.class\" ) : \n                filename = filename [ : - 9 ] + \".py\" \n    return filename "}
{"14867": "\ndef _should_trace_with_reason ( self , filename , frame ) : \n    if not filename : \n        return None , \"empty string isn't a filename\" \n    if filename . startswith ( '<' ) : \n        return None , \"not a real filename\" \n    self . _check_for_packages ( ) \n    dunder_file = frame . f_globals . get ( '__file__' ) \n    if dunder_file : \n        filename = self . _source_for_file ( dunder_file ) \n    if filename . endswith ( \"$py.class\" ) : \n        filename = filename [ : - 9 ] + \".py\" \n    canonical = self . file_locator . canonical_filename ( filename ) \n    if self . source_match : \n        if not self . source_match . match ( canonical ) : \n            return None , \"falls outside the --source trees\" \n    else : \n        if self . include_match : \n            if not self . include_match . match ( canonical ) : \n                return None , \"falls outside the --include trees\" \n        else : \n            if self . pylib_match and self . pylib_match . match ( canonical ) : \n                return None , \"is in the stdlib\" \n            if self . cover_match and self . cover_match . match ( canonical ) : \n                return None , \"is part of coverage.py\" \n    if self . omit_match and self . omit_match . match ( canonical ) : \n        return None , \"is inside an --omit pattern\" \n    return canonical , \"because we love you\" "}
{"14893": "\ndef reload ( self ) : \n    if self . filename is not None : \n        with open ( self . filename , self . _read_flags ) as f : \n            self . data = f . read ( ) \n    else : \n        if self . url is not None : \n            try : \n                import urllib2 \n                response = urllib2 . urlopen ( self . url ) \n                self . data = response . read ( ) \n                encoding = None \n                for sub in response . headers [ 'content-type' ] . split ( ';' ) : \n                    sub = sub . strip ( ) \n                    if sub . startswith ( 'charset' ) : \n                        encoding = sub . split ( '=' ) [ - 1 ] . strip ( ) \n                        break \n                if encoding : \n                    self . data = self . data . decode ( encoding , 'replace' ) \n            except : \n                self . data = None "}
{"14899": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        self . log . critical ( 'Could not read pid file, cluster is probably not running.' ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    if not self . check_pid ( pid ) : \n        self . log . critical ( 'Cluster [pid=%r] is not running.' % pid ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    else : \n        if os . name == 'posix' : \n            sig = self . signal \n            self . log . info ( \"Stopping cluster [pid=%r] with [signal=%r]\" % ( pid , sig ) ) \n            try : \n                os . kill ( pid , sig ) \n            except OSError : \n                self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = True ) \n                self . remove_pid_file ( ) \n        else : \n            if os . name == 'nt' : \n                try : \n                    p = check_call ( [ 'taskkill' , '-pid' , str ( pid ) , '-t' , '-f' ] , stdout = PIPE , stderr = PIPE ) \n                except ( CalledProcessError , OSError ) : \n                    self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = True ) \n                self . remove_pid_file ( ) "}
