{"1": "\ndef dailymotion_download ( url , output_dir = '.' , merge = 1 , info_only = 0 , ** kwargs ) : \n    html = get_content ( rebuilt_url ( url ) ) \n    info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) \n    title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) \n    title = unicodize ( title ) \n    for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : \n        try : \n            real_url = info [ quality ] [ 1 ] [ \"url\" ] \n            if real_url : \n                break \n        except KeyError : \n            pass \n    mime , ext , size = url_info ( real_url ) \n    print_info ( site_info , title , mime , size ) \n    if not info_only : \n        download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge ) "}
{"2": "\ndef sina_download ( url , output_dir = '.' , merge = 1 , info_only = 0 , ** kwargs ) : \n    if 'news.sina.com.cn/zxt' in url : \n        sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n        return \n    vid = match1 ( url , r'vid=(\\d+)' ) \n    if vid is None : \n        video_page = get_content ( url ) \n        vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) \n        if hd_vid == '0' : \n            vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) \n            vid = vids [ - 1 ] \n    if vid is None : \n        vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) \n    if vid : \n        sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) \n    else : \n        vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) \n        if vkey is None : \n            vid = match1 ( url , r'#(\\d+)' ) \n            sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) \n            return \n        title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) \n        sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only ) "}
{"8": "\ndef vimeo_download_by_channel ( url , output_dir = '.' , merge = 0 , info_only = 0 , ** kwargs ) : \n    channel_id = match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) \n    vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , ** kwargs ) "}
{"13": "\ndef cbs_download ( url , output_dir = '.' , merge = 1 , info_only = 0 , ** kwargs ) : \n    html = get_content ( url ) \n    pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) \n    title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) \n    theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only ) "}
{"14": "\ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    elif 'info_only' in kwargs and kwargs [ 'info_only' ] : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n        else : \n            if 'index' not in kwargs : \n                self . p ( [ ] ) \n            else : \n                stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n                self . p_i ( stream_id ) \n    else : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n        else : \n            stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n        if 'index' not in kwargs : \n            self . p ( stream_id ) \n        else : \n            self . p_i ( stream_id ) \n        if stream_id in self . streams : \n            urls = self . streams [ stream_id ] [ 'src' ] \n            ext = self . streams [ stream_id ] [ 'container' ] \n            total_size = self . streams [ stream_id ] [ 'size' ] \n        else : \n            urls = self . dash_streams [ stream_id ] [ 'src' ] \n            ext = self . dash_streams [ stream_id ] [ 'container' ] \n            total_size = self . dash_streams [ stream_id ] [ 'size' ] \n        if not urls : \n            log . wtf ( '[Failed] Cannot extract video source.' ) \n        download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = 0 ) \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping captions.' ) \n            return \n        for lang in self . caption_tracks : \n            filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n            print ( 'Saving %s ... ' % filename , end = \"\" , flush = 1 ) \n            srt = self . caption_tracks [ lang ] \n            with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( srt ) \n            print ( 'Done.' ) "}
{"15": "\ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = 1 , info_only = 0 , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if sourceType == 'sina' : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'youku' : \n        youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n    elif sourceType == 'tudou' : \n        tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'qq' : \n        qq_download_by_vid ( sourceId , title , 1 , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'letv' : \n        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'zhuzhan' : \n        url = 'http://www.acfun.cn/v/ac' + vid \n        yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n        seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n        for t in seq : \n            if yk_streams . get ( t ) : \n                preferred = yk_streams [ t ] \n                break \n        size = 0 \n        for url in preferred [ 0 ] : \n            _ , _ , seg_size = url_info ( url ) \n            size += seg_size \n        if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : \n            ext = 'flv' \n        else : \n            ext = 'mp4' \n        print_info ( site_info , title , ext , size ) \n        if not info_only : \n            download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) \n    else : \n        raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping danmaku.' ) \n            return \n        try : \n            title = get_filename ( title ) \n            print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"18": "\ndef get_content ( url , headers = { } , decoded = 1 ) : \n    logging . debug ( 'get_content: %s' % url ) \n    req = request . Request ( url , headers = headers ) \n    if cookies : \n        cookies . add_cookie_header ( req ) \n        req . headers . update ( req . unredirected_hdrs ) \n    response = urlopen_with_retry ( req ) \n    data = response . read ( ) \n    content_encoding = response . getheader ( 'Content-Encoding' ) \n    if content_encoding == 'gzip' : \n        data = ungzip ( data ) \n    elif content_encoding == 'deflate' : \n        data = undeflate ( data ) \n    if decoded : \n        charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) \n        if charset is not None : \n            data = data . decode ( charset , 'ignore' ) \n        else : \n            data = data . decode ( 'utf-8' , 'ignore' ) \n    return data "}
{"19": "\ndef post_content ( url , headers = { } , post_data = { } , decoded = 1 , ** kwargs ) : \n    if kwargs . get ( 'post_data_raw' ) : \n        logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) \n    else : \n        logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) \n    req = request . Request ( url , headers = headers ) \n    if cookies : \n        cookies . add_cookie_header ( req ) \n        req . headers . update ( req . unredirected_hdrs ) \n    if kwargs . get ( 'post_data_raw' ) : \n        post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) \n    else : \n        post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) \n    response = urlopen_with_retry ( req , data = post_data_enc ) \n    data = response . read ( ) \n    content_encoding = response . getheader ( 'Content-Encoding' ) \n    if content_encoding == 'gzip' : \n        data = ungzip ( data ) \n    elif content_encoding == 'deflate' : \n        data = undeflate ( data ) \n    if decoded : \n        charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) \n        if charset is not None : \n            data = data . decode ( charset ) \n        else : \n            data = data . decode ( 'utf-8' ) \n    return data "}
{"23": "\ndef wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = 1 , info_only = 0 , ** kwargs ) : \n    for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : \n        for pIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : \n            wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) "}
{"24": "\ndef wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = 1 , info_only = 0 , ** kwargs ) : \n    html = json_api_content \n    title = _wanmen_get_title_by_json_topic_part ( html , tIndex , pIndex ) \n    bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , tIndex , pIndex ) \n    bokecc_download_by_id ( vid = bokeccID , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) "}
{"25": "\ndef has_task ( self , task_instance ) : \n    if task_instance . key in self . queued_tasks or task_instance . key in self . running : \n        return 1 "}
{"34": "\ndef get_conn ( self ) : \n    conn = self . get_connection ( self . mysql_conn_id ) \n    conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } \n    if not conn . port : \n        conn_config [ \"port\" ] = 3306 \n    else : \n        conn_config [ \"port\" ] = int ( conn . port ) \n    if conn . extra_dejson . get ( 'charset' , 0 ) : \n        conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] \n        if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : \n            conn_config [ \"use_unicode\" ] = 1 \n    if conn . extra_dejson . get ( 'cursor' , 0 ) : \n        if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor \n        elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor \n        elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor \n    local_infile = conn . extra_dejson . get ( 'local_infile' , 0 ) \n    if conn . extra_dejson . get ( 'ssl' , 0 ) : \n        dejson_ssl = conn . extra_dejson [ 'ssl' ] \n        if isinstance ( dejson_ssl , six . string_types ) : \n            dejson_ssl = json . loads ( dejson_ssl ) \n        conn_config [ 'ssl' ] = dejson_ssl \n    if conn . extra_dejson . get ( 'unix_socket' ) : \n        conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] \n    if local_infile : \n        conn_config [ \"local_infile\" ] = 1 \n    conn = MySQLdb . connect ( ** conn_config ) \n    return conn "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += 1 \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while 1 : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_ready_workers_running < num_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            elif num_workers_running > num_workers_expected : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                for _ in range ( excess ) : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess -= 1 \n                    wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n            elif num_workers_running == num_workers_expected : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10 ) \n                if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"48": "\ndef start_proxy ( self ) : \n    self . _download_sql_proxy_if_needed ( ) \n    if self . sql_proxy_process : \n        raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) \n    else : \n        command_to_run = [ self . sql_proxy_path ] \n        command_to_run . extend ( self . command_line_parameters ) \n        try : \n            self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) \n            os . makedirs ( self . cloud_sql_proxy_socket_directory ) \n        except OSError : \n            pass \n        command_to_run . extend ( self . _get_credential_parameters ( ) ) \n        self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) \n        self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        self . log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) \n        while 1 : \n            line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) \n            return_code = self . sql_proxy_process . poll ( ) \n            if line == '' and return_code is not None : \n                self . sql_proxy_process = None \n                raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) \n            if line != '' : \n                self . log . info ( line ) \n            if \"googleapi: Error\" in line or \"invalid instance name:\" in line : \n                self . stop_proxy ( ) \n                raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) \n            if \"Ready for new connections\" in line : \n                return "}
{"49": "\ndef stop_proxy ( self ) : \n    if not self . sql_proxy_process : \n        raise AirflowException ( \"The sql proxy is not started yet\" ) \n    else : \n        self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) \n        self . sql_proxy_process . kill ( ) \n        self . sql_proxy_process = None \n    self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) \n    shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = 1 ) \n    if self . sql_proxy_was_downloaded : \n        self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) \n        try : \n            os . remove ( self . sql_proxy_path ) \n        except OSError as e : \n            if not e . errno == errno . ENOENT : \n                raise \n    else : \n        self . log . info ( \"Skipped removing proxy - it was not downloaded: %s\" , self . sql_proxy_path ) \n    if os . path . isfile ( self . credentials_path ) : \n        self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) \n        os . remove ( self . credentials_path ) "}
{"64": "\ndef heartbeat ( self ) : \n    super ( SchedulerMetricsJob , self ) . heartbeat ( ) \n    session = settings . Session ( ) \n    TI = TaskInstance \n    successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) \n    session . commit ( ) \n    dagbag = DagBag ( SUBDIR ) \n    dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] \n    num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) \n    if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : \n        if len ( successful_tis ) == num_task_instances : \n            self . log . info ( \"All tasks processed! Printing stats.\" ) \n        else : \n            self . log . info ( \"Test timeout reached. Printing available stats.\" ) \n        self . print_stats ( ) \n        set_dags_paused_state ( 1 ) \n        sys . exit ( ) "}
{"66": "\ndef create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : \n    if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : \n        raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) \n    metric_fn , metric_keys = metric_fn_and_keys \n    if not callable ( metric_fn ) : \n        raise AirflowException ( \"`metric_fn` param must be callable.\" ) \n    if not callable ( validate_fn ) : \n        raise AirflowException ( \"`validate_fn` param must be callable.\" ) \n    if dag is not None and dag . default_args is not None : \n        default_args = dag . default_args \n        project_id = project_id or default_args . get ( 'project_id' ) \n        region = region or default_args . get ( 'region' ) \n        model_name = model_name or default_args . get ( 'model_name' ) \n        version_name = version_name or default_args . get ( 'version_name' ) \n        dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) \n    evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) \n    metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = 1 ) ) \n    evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) \n    evaluate_summary . set_upstream ( evaluate_prediction ) \n    def apply_validate_fn ( * args , ** kwargs ) : \n        prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] \n        scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) \n        if scheme != \"gs\" or not bucket or not obj : \n            raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) \n        summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) \n        gcs_hook = GoogleCloudStorageHook ( ) \n        summary = json . loads ( gcs_hook . download ( bucket , summary ) ) \n        return validate_fn ( summary ) \n    evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = 1 , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) \n    evaluate_validation . set_upstream ( evaluate_summary ) \n    return evaluate_prediction , evaluate_summary , evaluate_validation "}
{"76": "\ndef run_and_check ( self , session , prepped_request , extra_options ) : \n    extra_options = extra_options or { } \n    try : \n        response = session . send ( prepped_request , stream = extra_options . get ( \"stream\" , 0 ) , verify = extra_options . get ( \"verify\" , 1 ) , proxies = extra_options . get ( \"proxies\" , { } ) , cert = extra_options . get ( \"cert\" ) , timeout = extra_options . get ( \"timeout\" ) , allow_redirects = extra_options . get ( \"allow_redirects\" , 1 ) ) \n        if extra_options . get ( 'check_response' , 1 ) : \n            self . check_response ( response ) \n        return response \n    except requests . exceptions . ConnectionError as ex : \n        self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) \n        raise ex "}
{"86": "\ndef does_collection_exist ( self , collection_name , database_name = None ) : \n    if collection_name is None : \n        raise AirflowBadRequest ( \"Collection name cannot be None.\" ) \n    existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) \n    if len ( existing_container ) == 0 : \n        return 0 \n    return 1 "}
{"88": "\ndef does_database_exist ( self , database_name ) : \n    if database_name is None : \n        raise AirflowBadRequest ( \"Database name cannot be None.\" ) \n    existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) \n    if len ( existing_database ) == 0 : \n        return 0 \n    return 1 "}
{"109": "\ndef list_py_file_paths ( directory , safe_mode = 1 , include_examples = None ) : \n    if include_examples is None : \n        include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) \n    file_paths = [ ] \n    if directory is None : \n        return [ ] \n    elif os . path . isfile ( directory ) : \n        return [ directory ] \n    elif os . path . isdir ( directory ) : \n        patterns_by_dir = { } \n        for root , dirs , files in os . walk ( directory , followlinks = 1 ) : \n            patterns = patterns_by_dir . get ( root , [ ] ) \n            ignore_file = os . path . join ( root , '.airflowignore' ) \n            if os . path . isfile ( ignore_file ) : \n                with open ( ignore_file , 'r' ) as f : \n                    patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] \n            dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] \n            for d in dirs : \n                patterns_by_dir [ os . path . join ( root , d ) ] = patterns \n            for f in files : \n                try : \n                    file_path = os . path . join ( root , f ) \n                    if not os . path . isfile ( file_path ) : \n                        continue \n                    mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) \n                    if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : \n                        continue \n                    if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : \n                        continue \n                    might_contain_dag = 1 \n                    if safe_mode and not zipfile . is_zipfile ( file_path ) : \n                        with open ( file_path , 'rb' ) as fp : \n                            content = fp . read ( ) \n                            might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) \n                    if not might_contain_dag : \n                        continue \n                    file_paths . append ( file_path ) \n                except Exception : \n                    log = LoggingMixin ( ) . log \n                    log . exception ( \"Error while examining %s\" , f ) \n    if include_examples : \n        import airflow . example_dags \n        example_dag_folder = airflow . example_dags . __path__ [ 0 ] \n        file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , 0 ) ) \n    return file_paths "}
{"110": "\ndef construct_task_instance ( self , session = None , lock_for_update = 0 ) : \n    TI = airflow . models . TaskInstance \n    qry = session . query ( TI ) . filter ( TI . dag_id == self . _dag_id , TI . task_id == self . _task_id , TI . execution_date == self . _execution_date ) \n    if lock_for_update : \n        ti = qry . with_for_update ( ) . first ( ) \n    else : \n        ti = qry . first ( ) \n    return ti "}
{"115": "\ndef start_in_async ( self ) : \n    while 1 : \n        loop_start_time = time . time ( ) \n        if self . _signal_conn . poll ( ) : \n            agent_signal = self . _signal_conn . recv ( ) \n            if agent_signal == DagParsingSignal . TERMINATE_MANAGER : \n                self . terminate ( ) \n                break \n            elif agent_signal == DagParsingSignal . END_MANAGER : \n                self . end ( ) \n                sys . exit ( os . EX_OK ) \n        self . _refresh_dag_dir ( ) \n        simple_dags = self . heartbeat ( ) \n        for simple_dag in simple_dags : \n            self . _result_queue . put ( simple_dag ) \n        self . _print_stat ( ) \n        all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n        max_runs_reached = self . max_runs_reached ( ) \n        dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) \n        self . _stat_queue . put ( dag_parsing_stat ) \n        if max_runs_reached : \n            self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n            break \n        loop_duration = time . time ( ) - loop_start_time \n        if loop_duration < 1 : \n            sleep_length = 1 - loop_duration \n            self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep_length ) \n            time . sleep ( sleep_length ) "}
{"116": "\ndef start_in_sync ( self ) : \n    while 1 : \n        agent_signal = self . _signal_conn . recv ( ) \n        if agent_signal == DagParsingSignal . TERMINATE_MANAGER : \n            self . terminate ( ) \n            break \n        elif agent_signal == DagParsingSignal . END_MANAGER : \n            self . end ( ) \n            sys . exit ( os . EX_OK ) \n        elif agent_signal == DagParsingSignal . AGENT_HEARTBEAT : \n            self . _refresh_dag_dir ( ) \n            simple_dags = self . heartbeat ( ) \n            for simple_dag in simple_dags : \n                self . _result_queue . put ( simple_dag ) \n            self . _print_stat ( ) \n            all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n            max_runs_reached = self . max_runs_reached ( ) \n            dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) \n            self . _stat_queue . put ( dag_parsing_stat ) \n            self . wait_until_finished ( ) \n            self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) \n            if max_runs_reached : \n                self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n                self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) \n                break "}
{"124": "\ndef end ( self ) : \n    pids_to_kill = self . get_all_pids ( ) \n    if len ( pids_to_kill ) > 0 : \n        this_process = psutil . Process ( os . getpid ( ) ) \n        child_processes = [ x for x in this_process . children ( recursive = 1 ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        for child in child_processes : \n            self . log . info ( \"Terminating child PID: %s\" , child . pid ) \n            child . terminate ( ) \n        timeout = 5 \n        self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) \n        try : \n            psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) \n        except psutil . TimeoutExpired : \n            self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) \n        child_processes = [ x for x in this_process . children ( recursive = 1 ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        if len ( child_processes ) > 0 : \n            self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) \n            for child in child_processes : \n                self . log . info ( \"Killing child PID: %s\" , child . pid ) \n                child . kill ( ) \n                child . wait ( ) "}
{"137": "\ndef run_command ( command ) : \n    process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = 1 ) \n    output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] \n    if process . returncode != 0 : \n        raise AirflowConfigException ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) \n    return output "}
{"138": "\ndef remove_option ( self , section , option , remove_default = 1 ) : \n    if super ( ) . has_option ( section , option ) : \n        super ( ) . remove_option ( section , option ) \n    if self . airflow_defaults . has_option ( section , option ) and remove_default : \n        self . airflow_defaults . remove_option ( section , option ) "}
{"139": "\ndef getsection ( self , section ) : \n    if ( section not in self . _sections and section not in self . airflow_defaults . _sections ) : \n        return None \n    _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) \n    if section in self . _sections : \n        _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) \n    section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) \n    for env_var in sorted ( os . environ . keys ( ) ) : \n        if env_var . startswith ( section_prefix ) : \n            key = env_var . replace ( section_prefix , '' ) . lower ( ) \n            _section [ key ] = self . _get_env_var_option ( section , key ) \n    for key , val in iteritems ( _section ) : \n        try : \n            val = int ( val ) \n        except ValueError : \n            try : \n                val = float ( val ) \n            except ValueError : \n                if val . lower ( ) in ( 't' , 'true' ) : \n                    val = 1 \n                elif val . lower ( ) in ( 'f' , 'false' ) : \n                    val = 0 \n        _section [ key ] = val \n    return _section "}
{"148": "\ndef poll_operation_until_done ( self , name , polling_interval_in_seconds ) : \n    while 1 : \n        result = self . get_operation ( name ) \n        state = result [ 'metadata' ] [ 'common' ] [ 'state' ] \n        if state == 'PROCESSING' : \n            self . log . info ( 'Operation is processing. Re-polling state in {} seconds' . format ( polling_interval_in_seconds ) ) \n            time . sleep ( polling_interval_in_seconds ) \n        else : \n            return result "}
{"169": "\ndef get_conn ( self ) : \n    http_authorized = self . _authorize ( ) \n    return build ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = 0 ) "}
{"172": "\ndef _handle_databricks_operator_execution ( operator , hook , log , context ) : \n    if operator . do_xcom_push : \n        context [ 'ti' ] . xcom_push ( key = XCOM_RUN_ID_KEY , value = operator . run_id ) \n    log . info ( 'Run submitted with run_id: %s' , operator . run_id ) \n    run_page_url = hook . get_run_page_url ( operator . run_id ) \n    if operator . do_xcom_push : \n        context [ 'ti' ] . xcom_push ( key = XCOM_RUN_PAGE_URL_KEY , value = run_page_url ) \n    log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) \n    while 1 : \n        run_state = hook . get_run_state ( operator . run_id ) \n        if run_state . is_terminal : \n            if run_state . is_successful : \n                log . info ( '%s completed successfully.' , operator . task_id ) \n                log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) \n                return \n            else : \n                error_message = '{t} failed with terminal state: {s}' . format ( t = operator . task_id , s = run_state ) \n                raise AirflowException ( error_message ) \n        else : \n            log . info ( '%s in run state: %s' , operator . task_id , run_state ) \n            log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) \n            log . info ( 'Sleeping for %s seconds.' , operator . polling_period_seconds ) \n            time . sleep ( operator . polling_period_seconds ) "}
{"173": "\ndef run_cli ( self , pig , verbose = 1 ) : \n    with TemporaryDirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : \n        with NamedTemporaryFile ( dir = tmp_dir ) as f : \n            f . write ( pig . encode ( 'utf-8' ) ) \n            f . flush ( ) \n            fname = f . name \n            pig_bin = 'pig' \n            cmd_extra = [ ] \n            pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra \n            if self . pig_properties : \n                pig_properties_list = self . pig_properties . split ( ) \n                pig_cmd . extend ( pig_properties_list ) \n            if verbose : \n                self . log . info ( \"%s\" , \" \" . join ( pig_cmd ) ) \n            sp = subprocess . Popen ( pig_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = 1 ) \n            self . sp = sp \n            stdout = '' \n            for line in iter ( sp . stdout . readline , b'' ) : \n                stdout += line . decode ( 'utf-8' ) \n                if verbose : \n                    self . log . info ( line . strip ( ) ) \n            sp . wait ( ) \n            if sp . returncode : \n                raise AirflowException ( stdout ) \n            return stdout "}
{"177": "\ndef setdefault ( cls , key , default , deserialize_json = 0 ) : \n    obj = Variable . get ( key , default_var = None , deserialize_json = deserialize_json ) \n    if obj is None : \n        if default is not None : \n            Variable . set ( key , default , serialize_json = deserialize_json ) \n            return default \n        else : \n            raise ValueError ( 'Default Value must be set' ) \n    else : \n        return obj "}
{"178": "\ndef get_conn ( self ) : \n    authed_http = self . _authorize ( ) \n    return build ( 'ml' , 'v1' , http = authed_http , cache_discovery = 0 ) "}
{"180": "\ndef _get_job ( self , project_id , job_id ) : \n    job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) \n    request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) \n    while 1 : \n        try : \n            return request . execute ( ) \n        except HttpError as e : \n            if e . resp . status == 429 : \n                time . sleep ( 30 ) \n            else : \n                self . log . error ( 'Failed to get MLEngine job: {}' . format ( e ) ) \n                raise "}
{"181": "\ndef _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : \n    if interval <= 0 : \n        raise ValueError ( \"Interval must be > 0\" ) \n    while 1 : \n        job = self . _get_job ( project_id , job_id ) \n        if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : \n            return job \n        time . sleep ( interval ) "}
{"182": "\ndef create_version ( self , project_id , model_name , version_spec ) : \n    parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) \n    create_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . create ( parent = parent_name , body = version_spec ) \n    response = create_request . execute ( ) \n    get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) \n    return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , 0 ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None ) "}
{"185": "\ndef delete_version ( self , project_id , model_name , version_name ) : \n    full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) \n    delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) \n    response = delete_request . execute ( ) \n    get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) \n    return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , 0 ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None ) "}
{"188": "\ndef write_batch_data ( self , items ) : \n    dynamodb_conn = self . get_conn ( ) \n    try : \n        table = dynamodb_conn . Table ( self . table_name ) \n        with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : \n            for item in items : \n                batch . put_item ( Item = item ) \n        return 1 \n    except Exception as general_error : \n        raise AirflowException ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) ) "}
{"194": "\ndef trigger_dag ( dag_id ) : \n    data = request . get_json ( force = 1 ) \n    run_id = None \n    if 'run_id' in data : \n        run_id = data [ 'run_id' ] \n    conf = None \n    if 'conf' in data : \n        conf = data [ 'conf' ] \n    execution_date = None \n    if 'execution_date' in data and data [ 'execution_date' ] is not None : \n        execution_date = data [ 'execution_date' ] \n        try : \n            execution_date = timezone . parse ( execution_date ) \n        except ValueError : \n            error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) \n            _log . info ( error_message ) \n            response = jsonify ( { 'error' : error_message } ) \n            response . status_code = 400 \n            return response \n    try : \n        dr = trigger . trigger_dag ( dag_id , run_id , conf , execution_date ) \n    except AirflowException as err : \n        _log . error ( err ) \n        response = jsonify ( error = \"{}\" . format ( err ) ) \n        response . status_code = err . status_code \n        return response \n    if getattr ( g , 'user' , None ) : \n        _log . info ( \"User %s created %s\" , g . user , dr ) \n    response = jsonify ( message = \"Created {}\" . format ( dr ) ) \n    return response "}
{"198": "\ndef create_pool ( ) : \n    params = request . get_json ( force = 1 ) \n    try : \n        pool = pool_api . create_pool ( ** params ) \n    except AirflowException as err : \n        _log . error ( err ) \n        response = jsonify ( error = \"{}\" . format ( err ) ) \n        response . status_code = err . status_code \n        return response \n    else : \n        return jsonify ( pool . to_json ( ) ) "}
{"203": "\ndef get_logs ( self , resource_group , name , tail = 1000 ) : \n    logs = self . connection . container . list_logs ( resource_group , name , name , tail = tail ) \n    return logs . content . splitlines ( 1 ) "}
{"205": "\ndef exists ( self , resource_group , name ) : \n    for container in self . connection . container_groups . list_by_resource_group ( resource_group ) : \n        if container . name == name : \n            return 1 \n    return 0 "}
{"208": "\ndef poke ( self , context ) : \n    self . log . info ( 'RedisPubSubSensor checking for message on channels: %s' , self . channels ) \n    message = self . pubsub . get_message ( ) \n    self . log . info ( 'Message %s from channel %s' , message , self . channels ) \n    if message and message [ 'type' ] == 'message' : \n        context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) \n        self . pubsub . unsubscribe ( self . channels ) \n        return 1 \n    return 0 "}
{"209": "\ndef find ( dag_id = None , run_id = None , execution_date = None , state = None , external_trigger = None , no_backfills = 0 , session = None ) : \n    DR = DagRun \n    qry = session . query ( DR ) \n    if dag_id : \n        qry = qry . filter ( DR . dag_id == dag_id ) \n    if run_id : \n        qry = qry . filter ( DR . run_id == run_id ) \n    if execution_date : \n        if isinstance ( execution_date , list ) : \n            qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) \n        else : \n            qry = qry . filter ( DR . execution_date == execution_date ) \n    if state : \n        qry = qry . filter ( DR . state == state ) \n    if external_trigger is not None : \n        qry = qry . filter ( DR . external_trigger == external_trigger ) \n    if no_backfills : \n        from airflow . jobs import BackfillJob \n        qry = qry . filter ( DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) ) \n    dr = qry . order_by ( DR . execution_date ) . all ( ) \n    return dr "}
{"214": "\ndef update_state ( self , session = None ) : \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    self . log . debug ( \"Updating state for %s considering %s task(s)\" , self , len ( tis ) ) \n    for ti in list ( tis ) : \n        if ti . state == State . REMOVED : \n            tis . remove ( ti ) \n        else : \n            ti . task = dag . get_task ( ti . task_id ) \n    start_dttm = timezone . utcnow ( ) \n    unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) \n    none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) \n    none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) \n    if unfinished_tasks and none_depends_on_past and none_task_concurrency : \n        no_dependencies_met = 1 \n        for ut in unfinished_tasks : \n            old_state = ut . state \n            deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = 1 , ignore_in_retry_period = 1 , ignore_in_reschedule_period = 1 ) , session = session ) \n            if deps_met or old_state != ut . current_state ( session = session ) : \n                no_dependencies_met = 0 \n                break \n    duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 \n    Stats . timing ( \"dagrun.dependency-check.{}\" . format ( self . dag_id ) , duration ) \n    root_ids = [ t . task_id for t in dag . roots ] \n    roots = [ t for t in tis if t . task_id in root_ids ] \n    if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : \n        self . log . info ( 'Marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = 0 , reason = 'task_failure' , session = session ) \n    elif not unfinished_tasks and all ( r . state in ( State . SUCCESS , State . SKIPPED ) for r in roots ) : \n        self . log . info ( 'Marking run %s successful' , self ) \n        self . set_state ( State . SUCCESS ) \n        dag . handle_callback ( self , success = 1 , reason = 'success' , session = session ) \n    elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : \n        self . log . info ( 'Deadlock; marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = 0 , reason = 'all_tasks_deadlocked' , session = session ) \n    else : \n        self . set_state ( State . RUNNING ) \n    self . _emit_duration_stats_for_finished_state ( ) \n    session . merge ( self ) \n    session . commit ( ) \n    return self . state "}
{"217": "\ndef context_to_airflow_vars ( context , in_env_var_format = 0 ) : \n    params = dict ( ) \n    if in_env_var_format : \n        name_format = 'env_var_format' \n    else : \n        name_format = 'default' \n    task_instance = context . get ( 'task_instance' ) \n    if task_instance and task_instance . dag_id : \n        params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_ID' ] [ name_format ] ] = task_instance . dag_id \n    if task_instance and task_instance . task_id : \n        params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_TASK_ID' ] [ name_format ] ] = task_instance . task_id \n    if task_instance and task_instance . execution_date : \n        params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_EXECUTION_DATE' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) \n    dag_run = context . get ( 'dag_run' ) \n    if dag_run and dag_run . run_id : \n        params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_RUN_ID' ] [ name_format ] ] = dag_run . run_id \n    return params "}
{"221": "\ndef get_dag ( self , dag_id ) : \n    from airflow . models . dag import DagModel \n    root_dag_id = dag_id \n    if dag_id in self . dags : \n        dag = self . dags [ dag_id ] \n        if dag . is_subdag : \n            root_dag_id = dag . parent_dag . dag_id \n    orm_dag = DagModel . get_current ( root_dag_id ) \n    if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : \n        found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = 0 ) \n        if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : \n            return self . dags [ dag_id ] \n        elif dag_id in self . dags : \n            del self . dags [ dag_id ] \n    return self . dags . get ( dag_id ) "}
{"223": "\ndef bag_dag ( self , dag , parent_dag , root_dag ) : \n    dag . test_cycle ( ) \n    dag . resolve_template_files ( ) \n    dag . last_loaded = timezone . utcnow ( ) \n    for task in dag . tasks : \n        settings . policy ( task ) \n    subdags = dag . subdags \n    try : \n        for subdag in subdags : \n            subdag . full_filepath = dag . full_filepath \n            subdag . parent_dag = dag \n            subdag . is_subdag = 1 \n            self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) \n        self . dags [ dag . dag_id ] = dag \n        self . log . debug ( 'Loaded DAG %s' , dag ) \n    except AirflowDagCycleException as cycle_exception : \n        self . log . exception ( 'Exception bagging dag: %s' , dag . dag_id ) \n        if dag == root_dag : \n            for subdag in subdags : \n                if subdag . dag_id in self . dags : \n                    del self . dags [ subdag . dag_id ] \n        raise cycle_exception "}
{"224": "\ndef collect_dags ( self , dag_folder = None , only_if_updated = 1 , include_examples = configuration . conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'DAG_DISCOVERY_SAFE_MODE' ) ) : \n    start_dttm = timezone . utcnow ( ) \n    dag_folder = dag_folder or self . dag_folder \n    stats = [ ] \n    FileLoadStat = namedtuple ( 'FileLoadStat' , \"file duration dag_num task_num dags\" ) \n    dag_folder = correct_maybe_zipped ( dag_folder ) \n    for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : \n        try : \n            ts = timezone . utcnow ( ) \n            found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) \n            td = timezone . utcnow ( ) - ts \n            td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000 ) \n            stats . append ( FileLoadStat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] ) , ) ) \n        except Exception as e : \n            self . log . exception ( e ) \n    Stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , 1 ) \n    Stats . gauge ( 'dagbag_size' , len ( self . dags ) , 1 ) \n    Stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , 1 ) \n    self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = 1 ) "}
{"228": "\ndef poke ( self , context ) : \n    sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) \n    self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) \n    result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = 0 ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] \n    result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) \n    result = self . filter_for_filesize ( result , self . file_size ) \n    return bool ( result ) "}
{"229": "\ndef poke ( self , context ) : \n    sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) \n    result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = 1 ) ] \n    result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) \n    result = self . filter_for_filesize ( result , self . file_size ) \n    if self . be_empty : \n        self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) \n        return len ( result ) == 1 and result [ 0 ] [ 'path' ] == self . filepath \n    else : \n        self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) \n        result . pop ( 0 ) \n        return bool ( result ) and result [ 0 ] [ 'file_type' ] == 'f' "}
{"230": "\ndef clear_task_instances ( tis , session , activate_dag_runs = 1 , dag = None , ) : \n    job_ids = [ ] \n    for ti in tis : \n        if ti . state == State . RUNNING : \n            if ti . job_id : \n                ti . state = State . SHUTDOWN \n                job_ids . append ( ti . job_id ) \n        else : \n            task_id = ti . task_id \n            if dag and dag . has_task ( task_id ) : \n                task = dag . get_task ( task_id ) \n                task_retries = task . retries \n                ti . max_tries = ti . try_number + task_retries - 1 \n            else : \n                ti . max_tries = max ( ti . max_tries , ti . try_number - 1 ) \n            ti . state = State . NONE \n            session . merge ( ti ) \n    if job_ids : \n        from airflow . jobs import BaseJob as BJ \n        for job in session . query ( BJ ) . filter ( BJ . id . in_ ( job_ids ) ) . all ( ) : \n            job . state = State . SHUTDOWN \n    if activate_dag_runs and tis : \n        from airflow . models . dagrun import DagRun \n        drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , DagRun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) \n        for dr in drs : \n            dr . state = State . RUNNING \n            dr . start_date = timezone . utcnow ( ) "}
{"232": "\ndef generate_command ( dag_id , task_id , execution_date , mark_success = 0 , ignore_all_deps = 0 , ignore_depends_on_past = 0 , ignore_task_deps = 0 , ignore_ti_state = 0 , local = 0 , pickle_id = None , file_path = None , raw = 0 , job_id = None , pool = None , cfg_path = None ) : \n    iso = execution_date . isoformat ( ) \n    cmd = [ \"airflow\" , \"run\" , str ( dag_id ) , str ( task_id ) , str ( iso ) ] \n    cmd . extend ( [ \"--mark_success\" ] ) if mark_success else None \n    cmd . extend ( [ \"--pickle\" , str ( pickle_id ) ] ) if pickle_id else None \n    cmd . extend ( [ \"--job_id\" , str ( job_id ) ] ) if job_id else None \n    cmd . extend ( [ \"-A\" ] ) if ignore_all_deps else None \n    cmd . extend ( [ \"-i\" ] ) if ignore_task_deps else None \n    cmd . extend ( [ \"-I\" ] ) if ignore_depends_on_past else None \n    cmd . extend ( [ \"--force\" ] ) if ignore_ti_state else None \n    cmd . extend ( [ \"--local\" ] ) if local else None \n    cmd . extend ( [ \"--pool\" , pool ] ) if pool else None \n    cmd . extend ( [ \"--raw\" ] ) if raw else None \n    cmd . extend ( [ \"-sd\" , file_path ] ) if file_path else None \n    cmd . extend ( [ \"--cfg_path\" , cfg_path ] ) if cfg_path else None \n    return cmd "}
{"235": "\ndef refresh_from_db ( self , session = None , lock_for_update = 0 ) : \n    TI = TaskInstance \n    qry = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date ) \n    if lock_for_update : \n        ti = qry . with_for_update ( ) . first ( ) \n    else : \n        ti = qry . first ( ) \n    if ti : \n        self . state = ti . state \n        self . start_date = ti . start_date \n        self . end_date = ti . end_date \n        self . try_number = ti . _try_number \n        self . max_tries = ti . max_tries \n        self . hostname = ti . hostname \n        self . pid = ti . pid \n        self . executor_config = ti . executor_config \n    else : \n        self . state = None "}
{"238": "\ndef are_dependents_done ( self , session = None ) : \n    task = self . task \n    if not task . downstream_task_ids : \n        return 1 \n    ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance . execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) \n    count = ti [ 0 ] [ 0 ] \n    return count == len ( task . downstream_task_ids ) "}
{"241": "\ndef pool_full ( self , session ) : \n    if not self . task . pool : \n        return 0 \n    pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) \n    if not pool : \n        return 0 \n    open_slots = pool . open_slots ( session = session ) \n    return open_slots <= 0 "}
{"244": "\ndef xcom_pull ( self , task_ids = None , dag_id = None , key = XCOM_RETURN_KEY , include_prior_dates = 0 ) : \n    if dag_id is None : \n        dag_id = self . dag_id \n    pull_fn = functools . partial ( XCom . get_one , execution_date = self . execution_date , key = key , dag_id = dag_id , include_prior_dates = include_prior_dates ) \n    if is_container ( task_ids ) : \n        return tuple ( pull_fn ( task_id = t ) for t in task_ids ) \n    else : \n        return pull_fn ( task_id = task_ids ) "}
{"245": "\ndef init_run_context ( self , raw = 0 ) : \n    self . raw = raw \n    self . _set_context ( self ) "}
{"246": "\ndef close ( self ) : \n    if self . closed : \n        return \n    super ( ) . close ( ) \n    if not self . upload_on_close : \n        return \n    local_loc = os . path . join ( self . local_base , self . log_relative_path ) \n    remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) \n    if os . path . exists ( local_loc ) : \n        with open ( local_loc , 'r' ) as logfile : \n            log = logfile . read ( ) \n        self . wasb_write ( log , remote_loc , append = 1 ) \n        if self . delete_local_copy : \n            shutil . rmtree ( os . path . dirname ( local_loc ) ) \n    self . closed = 1 "}
{"247": "\ndef get_conn ( self ) : \n    if not self . _conn : \n        http_authorized = self . _authorize ( ) \n        self . _conn = build ( 'compute' , self . api_version , http = http_authorized , cache_discovery = 0 ) \n    return self . _conn "}
{"254": "\ndef _wait_for_operation_to_complete ( self , project_id , operation_name , zone = None ) : \n    service = self . get_conn ( ) \n    while 1 : \n        if zone is None : \n            operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) \n        else : \n            operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) \n        if operation_response . get ( \"status\" ) == GceOperationStatus . DONE : \n            error = operation_response . get ( \"error\" ) \n            if error : \n                code = operation_response . get ( \"httpErrorStatusCode\" ) \n                msg = operation_response . get ( \"httpErrorMessage\" ) \n                error_msg = str ( error . get ( \"errors\" ) ) [ 1 : - 1 ] \n                raise AirflowException ( \"{} {}: \" . format ( code , msg ) + error_msg ) \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) "}
{"255": "\ndef check_for_bucket ( self , bucket_name ) : \n    try : \n        self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) \n        return 1 \n    except ClientError as e : \n        self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) \n        return 0 "}
{"257": "\ndef check_for_prefix ( self , bucket_name , prefix , delimiter ) : \n    prefix = prefix + delimiter if prefix [ - 1 ] != delimiter else prefix \n    prefix_split = re . split ( r'(\\w+[{d}])$' . format ( d = delimiter ) , prefix , 1 ) \n    previous_level = prefix_split [ 0 ] \n    plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) \n    return 0 if plist is None else prefix in plist "}
{"258": "\ndef list_prefixes ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : \n    config = { 'PageSize' : page_size , 'MaxItems' : max_items , } \n    paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) \n    response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) \n    has_results = 0 \n    prefixes = [ ] \n    for page in response : \n        if 'CommonPrefixes' in page : \n            has_results = 1 \n            for p in page [ 'CommonPrefixes' ] : \n                prefixes . append ( p [ 'Prefix' ] ) \n    if has_results : \n        return prefixes "}
{"259": "\ndef list_keys ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : \n    config = { 'PageSize' : page_size , 'MaxItems' : max_items , } \n    paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) \n    response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) \n    has_results = 0 \n    keys = [ ] \n    for page in response : \n        if 'Contents' in page : \n            has_results = 1 \n            for k in page [ 'Contents' ] : \n                keys . append ( k [ 'Key' ] ) \n    if has_results : \n        return keys "}
{"260": "\ndef check_for_key ( self , key , bucket_name = None ) : \n    if not bucket_name : \n        ( bucket_name , key ) = self . parse_s3_url ( key ) \n    try : \n        self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) \n        return 1 \n    except ClientError as e : \n        self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) \n        return 0 "}
{"266": "\ndef load_file ( self , filename , key , bucket_name = None , replace = 0 , encrypt = 0 ) : \n    if not bucket_name : \n        ( bucket_name , key ) = self . parse_s3_url ( key ) \n    if not replace and self . check_for_key ( key , bucket_name ) : \n        raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) \n    extra_args = { } \n    if encrypt : \n        extra_args [ 'ServerSideEncryption' ] = \"AES256\" \n    client = self . get_conn ( ) \n    client . upload_file ( filename , bucket_name , key , ExtraArgs = extra_args ) "}
{"267": "\ndef load_string ( self , string_data , key , bucket_name = None , replace = 0 , encrypt = 0 , encoding = 'utf-8' ) : \n    self . load_bytes ( string_data . encode ( encoding ) , key = key , bucket_name = bucket_name , replace = replace , encrypt = encrypt ) "}
{"268": "\ndef load_bytes ( self , bytes_data , key , bucket_name = None , replace = 0 , encrypt = 0 ) : \n    if not bucket_name : \n        ( bucket_name , key ) = self . parse_s3_url ( key ) \n    if not replace and self . check_for_key ( key , bucket_name ) : \n        raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) \n    extra_args = { } \n    if encrypt : \n        extra_args [ 'ServerSideEncryption' ] = \"AES256\" \n    filelike_buffer = BytesIO ( bytes_data ) \n    client = self . get_conn ( ) \n    client . upload_fileobj ( filelike_buffer , bucket_name , key , ExtraArgs = extra_args ) "}
{"269": "\ndef load_file_obj ( self , file_obj , key , bucket_name = None , replace = 0 , encrypt = 0 ) : \n    if not bucket_name : \n        ( bucket_name , key ) = self . parse_s3_url ( key ) \n    if not replace and self . check_for_key ( key , bucket_name ) : \n        raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) \n    extra_args = { } \n    if encrypt : \n        extra_args [ 'ServerSideEncryption' ] = \"AES256\" \n    client = self . get_conn ( ) \n    client . upload_fileobj ( file_obj , bucket_name , key , ExtraArgs = extra_args ) "}
{"273": "\ndef send_email ( to , subject , html_content , files = None , dryrun = 0 , cc = None , bcc = None , mime_subtype = 'mixed' , sandbox_mode = 0 , ** kwargs ) : \n    if files is None : \n        files = [ ] \n    mail = Mail ( ) \n    from_email = kwargs . get ( 'from_email' ) or os . environ . get ( 'SENDGRID_MAIL_FROM' ) \n    from_name = kwargs . get ( 'from_name' ) or os . environ . get ( 'SENDGRID_MAIL_SENDER' ) \n    mail . from_email = Email ( from_email , from_name ) \n    mail . subject = subject \n    mail . mail_settings = MailSettings ( ) \n    if sandbox_mode : \n        mail . mail_settings . sandbox_mode = SandBoxMode ( enable = 1 ) \n    personalization = Personalization ( ) \n    to = get_email_address_list ( to ) \n    for to_address in to : \n        personalization . add_to ( Email ( to_address ) ) \n    if cc : \n        cc = get_email_address_list ( cc ) \n        for cc_address in cc : \n            personalization . add_cc ( Email ( cc_address ) ) \n    if bcc : \n        bcc = get_email_address_list ( bcc ) \n        for bcc_address in bcc : \n            personalization . add_bcc ( Email ( bcc_address ) ) \n    pers_custom_args = kwargs . get ( 'personalization_custom_args' , None ) \n    if isinstance ( pers_custom_args , dict ) : \n        for key in pers_custom_args . keys ( ) : \n            personalization . add_custom_arg ( CustomArg ( key , pers_custom_args [ key ] ) ) \n    mail . add_personalization ( personalization ) \n    mail . add_content ( Content ( 'text/html' , html_content ) ) \n    categories = kwargs . get ( 'categories' , [ ] ) \n    for cat in categories : \n        mail . add_category ( Category ( cat ) ) \n    for fname in files : \n        basename = os . path . basename ( fname ) \n        attachment = Attachment ( ) \n        attachment . type = mimetypes . guess_type ( basename ) [ 0 ] \n        attachment . filename = basename \n        attachment . disposition = \"attachment\" \n        attachment . content_id = '<{0}>' . format ( basename ) \n        with open ( fname , \"rb\" ) as f : \n            attachment . content = base64 . b64encode ( f . read ( ) ) . decode ( 'utf-8' ) \n        mail . add_attachment ( attachment ) \n    _post_sendgrid_mail ( mail . get ( ) ) "}
{"278": "\ndef is_valid_plugin ( plugin_obj , existing_plugins ) : \n    if ( inspect . isclass ( plugin_obj ) and issubclass ( plugin_obj , AirflowPlugin ) and ( plugin_obj is not AirflowPlugin ) ) : \n        plugin_obj . validate ( ) \n        return plugin_obj not in existing_plugins \n    return 0 "}
{"279": "\ndef skip ( self , dag_run , execution_date , tasks , session = None ) : \n    if not tasks : \n        return \n    task_ids = [ d . task_id for d in tasks ] \n    now = timezone . utcnow ( ) \n    if dag_run : \n        session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == dag_run . dag_id , TaskInstance . execution_date == dag_run . execution_date , TaskInstance . task_id . in_ ( task_ids ) ) . update ( { TaskInstance . state : State . SKIPPED , TaskInstance . start_date : now , TaskInstance . end_date : now } , synchronize_session = 0 ) \n        session . commit ( ) \n    else : \n        assert execution_date is not None , \"Execution date is None and no dag run\" \n        self . log . warning ( \"No DAG RUN present this should not happen\" ) \n        for task in tasks : \n            ti = TaskInstance ( task , execution_date = execution_date ) \n            ti . state = State . SKIPPED \n            ti . start_date = now \n            ti . end_date = now \n            session . merge ( ti ) \n        session . commit ( ) "}
{"281": "\ndef check_for_file ( self , file_path ) : \n    try : \n        files = self . connection . glob ( file_path , details = 0 , invalidate_cache = 1 ) \n        return len ( files ) == 1 \n    except FileNotFoundError : \n        return 0 "}
{"282": "\ndef upload_file ( self , local_path , remote_path , nthreads = 64 , overwrite = 1 , buffersize = 4194304 , blocksize = 4194304 ) : \n    multithread . ADLUploader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize ) "}
{"285": "\ndef uncompress_file ( input_file_name , file_extension , dest_dir ) : \n    if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : \n        raise NotImplementedError ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file_extension ) ) \n    if file_extension . lower ( ) == '.gz' : \n        fmodule = gzip . GzipFile \n    elif file_extension . lower ( ) == '.bz2' : \n        fmodule = bz2 . BZ2File \n    with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = 0 ) as f_uncompressed : \n        shutil . copyfileobj ( f_compressed , f_uncompressed ) \n    return f_uncompressed . name "}
{"292": "\ndef _do_api_call ( self , endpoint_info , json ) : \n    method , endpoint = endpoint_info \n    url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) \n    if 'token' in self . databricks_conn . extra_dejson : \n        self . log . info ( 'Using token auth.' ) \n        auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) \n    else : \n        self . log . info ( 'Using basic auth.' ) \n        auth = ( self . databricks_conn . login , self . databricks_conn . password ) \n    if method == 'GET' : \n        request_func = requests . get \n    elif method == 'POST' : \n        request_func = requests . post \n    else : \n        raise AirflowException ( 'Unexpected HTTP Method: ' + method ) \n    attempt_num = 1 \n    while 1 : \n        try : \n            response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) \n            response . raise_for_status ( ) \n            return response . json ( ) \n        except requests_exceptions . RequestException as e : \n            if not _retryable_error ( e ) : \n                raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) \n            self . _log_request_error ( attempt_num , e ) \n        if attempt_num == self . retry_limit : \n            raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) \n        attempt_num += 1 \n        sleep ( self . retry_delay ) "}
{"293": "\ndef get_conn ( self ) : \n    if not self . conn : \n        connection = self . get_connection ( self . conn_id ) \n        extras = connection . extra_dejson \n        self . conn = Salesforce ( username = connection . login , password = connection . password , security_token = extras [ 'security_token' ] , instance_url = connection . host , sandbox = extras . get ( 'sandbox' , 0 ) ) \n    return self . conn "}
{"299": "\ndef write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = 0 , record_time_added = 0 ) : \n    fmt = fmt . lower ( ) \n    if fmt not in [ 'csv' , 'json' , 'ndjson' ] : \n        raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) \n    df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) \n    df . columns = [ column . lower ( ) for column in df . columns ] \n    if coerce_to_timestamp and df . shape [ 0 ] > 0 : \n        object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] \n        self . log . info ( \"Coercing timestamps for: %s\" , object_name ) \n        schema = self . describe_object ( object_name ) \n        possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] \n        df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) \n    if record_time_added : \n        fetched_time = time . time ( ) \n        df [ \"time_fetched_from_salesforce\" ] = fetched_time \n    if fmt == \"csv\" : \n        self . log . info ( \"Cleaning data and writing to CSV\" ) \n        possible_strings = df . columns [ df . dtypes == \"object\" ] \n        df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) \n        df . to_csv ( filename , index = 0 ) \n    elif fmt == \"json\" : \n        df . to_json ( filename , \"records\" , date_unit = \"s\" ) \n    elif fmt == \"ndjson\" : \n        df . to_json ( filename , \"records\" , lines = 1 , date_unit = \"s\" ) \n    return df "}
{"300": "\ndef get_conn ( self ) : \n    if self . client is not None : \n        return self . client \n    options = self . extras \n    if options . get ( 'ssl' , 0 ) : \n        options . update ( { 'ssl_cert_reqs' : CERT_NONE } ) \n    self . client = MongoClient ( self . uri , ** options ) \n    return self . client "}
{"302": "\ndef replace_many ( self , mongo_collection , docs , filter_docs = None , mongo_db = None , upsert = 0 , collation = None , ** kwargs ) : \n    collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) \n    if not filter_docs : \n        filter_docs = [ { '_id' : doc [ '_id' ] } for doc in docs ] \n    requests = [ ReplaceOne ( filter_docs [ i ] , docs [ i ] , upsert = upsert , collation = collation ) for i in range ( len ( docs ) ) ] \n    return collection . bulk_write ( requests , ** kwargs ) "}
{"303": "\ndef has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = 0 ) : \n    mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = 1 ) \n    return len ( mail_attachments ) > 0 "}
{"304": "\ndef retrieve_mail_attachments ( self , name , mail_folder = 'INBOX' , check_regex = 0 , latest_only = 0 , not_found_mode = 'raise' ) : \n    mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) \n    if not mail_attachments : \n        self . _handle_not_found_mode ( not_found_mode ) \n    return mail_attachments "}
{"305": "\ndef download_mail_attachments ( self , name , local_output_directory , mail_folder = 'INBOX' , check_regex = 0 , latest_only = 0 , not_found_mode = 'raise' ) : \n    mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) \n    if not mail_attachments : \n        self . _handle_not_found_mode ( not_found_mode ) \n    self . _create_files ( mail_attachments , local_output_directory ) "}
{"306": "\ndef get_attachments_by_name ( self , name , check_regex , find_first = 0 ) : \n    attachments = [ ] \n    for part in self . mail . walk ( ) : \n        mail_part = MailPart ( part ) \n        if mail_part . is_attachment ( ) : \n            found_attachment = mail_part . has_matching_name ( name ) if check_regex else mail_part . has_equal_name ( name ) \n            if found_attachment : \n                file_name , file_payload = mail_part . get_file ( ) \n                self . log . info ( 'Found attachment: {}' . format ( file_name ) ) \n                attachments . append ( ( file_name , file_payload ) ) \n                if find_first : \n                    break \n    return attachments "}
{"307": "\ndef get_file ( self ) : \n    return self . part . get_filename ( ) , self . part . get_payload ( decode = 1 ) "}
{"310": "\ndef send_email ( to , subject , html_content , files = None , dryrun = 0 , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , ** kwargs ) : \n    path , attr = configuration . conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , 1 ) \n    module = importlib . import_module ( path ) \n    backend = getattr ( module , attr ) \n    to = get_email_address_list ( to ) \n    to = \", \" . join ( to ) \n    return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , ** kwargs ) "}
{"311": "\ndef send_email_smtp ( to , subject , html_content , files = None , dryrun = 0 , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , ** kwargs ) : \n    smtp_mail_from = configuration . conf . get ( 'smtp' , 'SMTP_MAIL_FROM' ) \n    to = get_email_address_list ( to ) \n    msg = MIMEMultipart ( mime_subtype ) \n    msg [ 'Subject' ] = subject \n    msg [ 'From' ] = smtp_mail_from \n    msg [ 'To' ] = \", \" . join ( to ) \n    recipients = to \n    if cc : \n        cc = get_email_address_list ( cc ) \n        msg [ 'CC' ] = \", \" . join ( cc ) \n        recipients = recipients + cc \n    if bcc : \n        bcc = get_email_address_list ( bcc ) \n        recipients = recipients + bcc \n    msg [ 'Date' ] = formatdate ( localtime = 1 ) \n    mime_text = MIMEText ( html_content , 'html' , mime_charset ) \n    msg . attach ( mime_text ) \n    for fname in files or [ ] : \n        basename = os . path . basename ( fname ) \n        with open ( fname , \"rb\" ) as f : \n            part = MIMEApplication ( f . read ( ) , Name = basename ) \n            part [ 'Content-Disposition' ] = 'attachment; filename=\"%s\"' % basename \n            part [ 'Content-ID' ] = '<%s>' % basename \n            msg . attach ( part ) \n    send_MIME_email ( smtp_mail_from , recipients , msg , dryrun ) "}
{"317": "\ndef delete_file ( self , container_name , blob_name , is_prefix = 0 , ignore_if_missing = 0 , ** kwargs ) : \n    if is_prefix : \n        blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , ** kwargs ) ] \n    elif self . check_for_blob ( container_name , blob_name ) : \n        blobs_to_delete = [ blob_name ] \n    else : \n        blobs_to_delete = [ ] \n    if not ignore_if_missing and len ( blobs_to_delete ) == 0 : \n        raise AirflowException ( 'Blob(s) not found: {}' . format ( blob_name ) ) \n    for blob_uri in blobs_to_delete : \n        self . log . info ( \"Deleting blob: \" + blob_uri ) \n        self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , ** kwargs ) "}
{"319": "\ndef get_conn ( self ) : \n    if self . conn is None : \n        params = self . get_connection ( self . ftp_conn_id ) \n        pasv = params . extra_dejson . get ( \"passive\" , 1 ) \n        self . conn = ftplib . FTP ( params . host , params . login , params . password ) \n        self . conn . set_pasv ( pasv ) \n    return self . conn "}
{"320": "\ndef list_directory ( self , path , nlst = 0 ) : \n    conn = self . get_conn ( ) \n    conn . cwd ( path ) \n    files = conn . nlst ( ) \n    return files "}
{"336": "\ndef upload ( self , bucket_name , object_name , filename , mime_type = 'application/octet-stream' , gzip = 0 ) : \n    if gzip : \n        filename_gz = filename + '.gz' \n        with open ( filename , 'rb' ) as f_in : \n            with gz . open ( filename_gz , 'wb' ) as f_out : \n                shutil . copyfileobj ( f_in , f_out ) \n                filename = filename_gz \n    client = self . get_conn ( ) \n    bucket = client . get_bucket ( bucket_name = bucket_name ) \n    blob = bucket . blob ( blob_name = object_name ) \n    blob . upload_from_filename ( filename = filename , content_type = mime_type ) \n    if gzip : \n        os . remove ( filename ) \n    self . log . info ( 'File %s uploaded to %s in %s bucket' , filename , object_name , bucket_name ) "}
{"338": "\ndef is_updated_after ( self , bucket_name , object_name , ts ) : \n    client = self . get_conn ( ) \n    bucket = storage . Bucket ( client = client , name = bucket_name ) \n    blob = bucket . get_blob ( blob_name = object_name ) \n    blob . reload ( ) \n    blob_update_time = blob . updated \n    if blob_update_time is not None : \n        import dateutil . tz \n        if not ts . tzinfo : \n            ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) \n        self . log . info ( \"Verify object date: %s > %s\" , blob_update_time , ts ) \n        if blob_update_time > ts : \n            return 1 \n    return 0 "}
{"340": "\ndef list ( self , bucket_name , versions = None , max_results = None , prefix = None , delimiter = None ) : \n    client = self . get_conn ( ) \n    bucket = client . get_bucket ( bucket_name = bucket_name ) \n    ids = [ ] \n    pageToken = None \n    while 1 : \n        blobs = bucket . list_blobs ( max_results = max_results , page_token = pageToken , prefix = prefix , delimiter = delimiter , versions = versions ) \n        blob_names = [ ] \n        for blob in blobs : \n            blob_names . append ( blob . name ) \n        prefixes = blobs . prefixes \n        if prefixes : \n            ids += list ( prefixes ) \n        else : \n            ids += blob_names \n        pageToken = blobs . next_page_token \n        if pageToken is None : \n            break \n    return ids "}
{"346": "\ndef secondary_training_status_changed ( current_job_description , prev_job_description ) : \n    current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) \n    if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == 0 : \n        return 0 \n    prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None \n    last_message = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and len ( prev_job_secondary_status_transitions ) > 0 else '' \n    message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] \n    return message != last_message "}
{"348": "\ndef tar_and_s3_upload ( self , path , key , bucket ) : \n    with tempfile . TemporaryFile ( ) as temp_file : \n        if os . path . isdir ( path ) : \n            files = [ os . path . join ( path , name ) for name in os . listdir ( path ) ] \n        else : \n            files = [ path ] \n        with tarfile . open ( mode = 'w:gz' , fileobj = temp_file ) as tar_file : \n            for f in files : \n                tar_file . add ( f , arcname = os . path . basename ( f ) ) \n        temp_file . seek ( 0 ) \n        self . s3_hook . load_file_obj ( temp_file , key , bucket , replace = 1 ) "}
{"350": "\ndef check_s3_url ( self , s3url ) : \n    bucket , key = S3Hook . parse_s3_url ( s3url ) \n    if not self . s3_hook . check_for_bucket ( bucket_name = bucket ) : \n        raise AirflowException ( \"The input S3 Bucket {} does not exist \" . format ( bucket ) ) \n    if key and not self . s3_hook . check_for_key ( key = key , bucket_name = bucket ) and not self . s3_hook . check_for_prefix ( prefix = key , bucket_name = bucket , delimiter = '/' ) : \n        raise AirflowException ( \"The input S3 Key \" \"or Prefix {} does not exist in the Bucket {}\" . format ( s3url , bucket ) ) \n    return 1 "}
{"352": "\ndef create_training_job ( self , config , wait_for_completion = 1 , print_log = 1 , check_interval = 30 , max_ingestion_time = None ) : \n    self . check_training_config ( config ) \n    response = self . get_conn ( ) . create_training_job ( ** config ) \n    if print_log : \n        self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) \n    elif wait_for_completion : \n        describe_response = self . check_status ( config [ 'TrainingJobName' ] , 'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) \n        billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) \n    return response "}
{"353": "\ndef create_tuning_job ( self , config , wait_for_completion = 1 , check_interval = 30 , max_ingestion_time = None ) : \n    self . check_tuning_config ( config ) \n    response = self . get_conn ( ) . create_hyper_parameter_tuning_job ( ** config ) \n    if wait_for_completion : \n        self . check_status ( config [ 'HyperParameterTuningJobName' ] , 'HyperParameterTuningJobStatus' , self . describe_tuning_job , check_interval , max_ingestion_time ) \n    return response "}
{"354": "\ndef create_transform_job ( self , config , wait_for_completion = 1 , check_interval = 30 , max_ingestion_time = None ) : \n    self . check_s3_url ( config [ 'TransformInput' ] [ 'DataSource' ] [ 'S3DataSource' ] [ 'S3Uri' ] ) \n    response = self . get_conn ( ) . create_transform_job ( ** config ) \n    if wait_for_completion : \n        self . check_status ( config [ 'TransformJobName' ] , 'TransformJobStatus' , self . describe_transform_job , check_interval , max_ingestion_time ) \n    return response "}
{"355": "\ndef create_endpoint ( self , config , wait_for_completion = 1 , check_interval = 30 , max_ingestion_time = None ) : \n    response = self . get_conn ( ) . create_endpoint ( ** config ) \n    if wait_for_completion : \n        self . check_status ( config [ 'EndpointName' ] , 'EndpointStatus' , self . describe_endpoint , check_interval , max_ingestion_time , non_terminal_states = self . endpoint_non_terminal_states ) \n    return response "}
{"357": "\ndef check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : \n    if not non_terminal_states : \n        non_terminal_states = self . non_terminal_states \n    sec = 0 \n    running = 1 \n    while running : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        try : \n            response = describe_function ( job_name ) \n            status = response [ key ] \n            self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) \n        except KeyError : \n            raise AirflowException ( 'Could not get status of the SageMaker job' ) \n        except ClientError : \n            raise AirflowException ( 'AWS request failed, check logs for more info' ) \n        if status in non_terminal_states : \n            running = 1 \n        elif status in self . failed_states : \n            raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) \n        else : \n            running = 0 \n        if max_ingestion_time and sec > max_ingestion_time : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    self . log . info ( 'SageMaker Job Compeleted' ) \n    response = describe_function ( job_name ) \n    return response "}
{"358": "\ndef check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : \n    sec = 0 \n    description = self . describe_training_job ( job_name ) \n    self . log . info ( secondary_training_status_message ( description , None ) ) \n    instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] \n    status = description [ 'TrainingJobStatus' ] \n    stream_names = [ ] \n    positions = { } \n    job_already_completed = status not in non_terminal_states \n    state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE \n    last_describe_job_call = time . time ( ) \n    last_description = description \n    while 1 : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) \n        if state == LogState . COMPLETE : \n            break \n        if max_ingestion_time and sec > max_ingestion_time : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    if wait_for_completion : \n        status = last_description [ 'TrainingJobStatus' ] \n        if status in failed_states : \n            reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) \n            raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) \n        billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) "}
{"360": "\ndef run_migrations_offline ( ) : \n    context . configure ( url = settings . SQL_ALCHEMY_CONN , target_metadata = target_metadata , literal_binds = 1 , compare_type = COMPARE_TYPE ) \n    with context . begin_transaction ( ) : \n        context . run_migrations ( ) "}
{"361": "\ndef run_migrations_online ( ) : \n    connectable = settings . engine \n    with connectable . connect ( ) as connection : \n        context . configure ( connection = connection , transaction_per_migration = 1 , target_metadata = target_metadata , compare_type = COMPARE_TYPE , ) \n        with context . begin_transaction ( ) : \n            context . run_migrations ( ) "}
{"369": "\ndef load_df ( self , df , table , field_dict = None , delimiter = ',' , encoding = 'utf8' , pandas_kwargs = None , ** kwargs ) : \n    def _infer_field_types_from_df ( df ) : \n        DTYPE_KIND_HIVE_TYPE = { 'b' : 'BOOLEAN' , 'i' : 'BIGINT' , 'u' : 'BIGINT' , 'f' : 'DOUBLE' , 'c' : 'STRING' , 'M' : 'TIMESTAMP' , 'O' : 'STRING' , 'S' : 'STRING' , 'U' : 'STRING' , 'V' : 'STRING' } \n        d = OrderedDict ( ) \n        for col , dtype in df . dtypes . iteritems ( ) : \n            d [ col ] = DTYPE_KIND_HIVE_TYPE [ dtype . kind ] \n        return d \n    if pandas_kwargs is None : \n        pandas_kwargs = { } \n    with TemporaryDirectory ( prefix = 'airflow_hiveop_' ) as tmp_dir : \n        with NamedTemporaryFile ( dir = tmp_dir , mode = \"w\" ) as f : \n            if field_dict is None : \n                field_dict = _infer_field_types_from_df ( df ) \n            df . to_csv ( path_or_buf = f , sep = delimiter , header = 0 , index = 0 , encoding = encoding , date_format = \"%Y-%m-%d %H:%M:%S\" , ** pandas_kwargs ) \n            f . flush ( ) \n            return self . load_file ( filepath = f . name , table = table , delimiter = delimiter , field_dict = field_dict , ** kwargs ) "}
{"370": "\ndef load_file ( self , filepath , table , delimiter = \",\" , field_dict = None , create = 1 , overwrite = 1 , partition = None , recreate = 0 , tblproperties = None ) : \n    hql = '' \n    if recreate : \n        hql += \"DROP TABLE IF EXISTS {table};\\n\" . format ( table = table ) \n    if create or recreate : \n        if field_dict is None : \n            raise ValueError ( \"Must provide a field dict when creating a table\" ) \n        fields = \",\\n    \" . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) \n        hql += \"CREATE TABLE IF NOT EXISTS {table} (\\n{fields})\\n\" . format ( table = table , fields = fields ) \n        if partition : \n            pfields = \",\\n    \" . join ( [ p + \" STRING\" for p in partition ] ) \n            hql += \"PARTITIONED BY ({pfields})\\n\" . format ( pfields = pfields ) \n        hql += \"ROW FORMAT DELIMITED\\n\" \n        hql += \"FIELDS TERMINATED BY '{delimiter}'\\n\" . format ( delimiter = delimiter ) \n        hql += \"STORED AS textfile\\n\" \n        if tblproperties is not None : \n            tprops = \", \" . join ( [ \"'{0}'='{1}'\" . format ( k , v ) for k , v in tblproperties . items ( ) ] ) \n            hql += \"TBLPROPERTIES({tprops})\\n\" . format ( tprops = tprops ) \n    hql += \";\" \n    self . log . info ( hql ) \n    self . run_cli ( hql ) \n    hql = \"LOAD DATA LOCAL INPATH '{filepath}' \" . format ( filepath = filepath ) \n    if overwrite : \n        hql += \"OVERWRITE \" \n    hql += \"INTO TABLE {table} \" . format ( table = table ) \n    if partition : \n        pvals = \", \" . join ( [ \"{0}='{1}'\" . format ( k , v ) for k , v in partition . items ( ) ] ) \n        hql += \"PARTITION ({pvals})\" . format ( pvals = pvals ) \n    hql += ';\\n' \n    self . log . info ( hql ) \n    self . run_cli ( hql ) "}
{"373": "\ndef table_exists ( self , table_name , db = 'default' ) : \n    try : \n        self . get_table ( table_name , db ) \n        return 1 \n    except Exception : \n        return 0 "}
{"376": "\ndef to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = 1 , fetch_size = 1000 , hive_conf = None ) : \n    results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) \n    header = next ( results_iter ) \n    message = None \n    i = 0 \n    with open ( csv_filepath , 'wb' ) as f : \n        writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) \n        try : \n            if output_header : \n                self . log . debug ( 'Cursor description is %s' , header ) \n                writer . writerow ( [ c [ 0 ] for c in header ] ) \n            for i , row in enumerate ( results_iter , 1 ) : \n                writer . writerow ( row ) \n                if i % fetch_size == 0 : \n                    self . log . info ( \"Written %s rows so far.\" , i ) \n        except ValueError as exception : \n            message = str ( exception ) \n    if message : \n        os . remove ( csv_filepath ) \n        raise ValueError ( message ) \n    self . log . info ( \"Done. Loaded a total of %s rows.\" , i ) "}
{"387": "\ndef get_service ( self ) : \n    http_authorized = self . _authorize ( ) \n    return build ( 'bigquery' , 'v2' , http = http_authorized , cache_discovery = 0 ) "}
{"388": "\ndef table_exists ( self , project_id , dataset_id , table_id ) : \n    service = self . get_service ( ) \n    try : \n        service . tables ( ) . get ( projectId = project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) \n        return 1 \n    except HttpError as e : \n        if e . resp [ 'status' ] == '404' : \n            return 0 \n        raise "}
{"391": "\ndef cancel_query ( self ) : \n    jobs = self . service . jobs ( ) \n    if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : \n        self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) \n        if self . location : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) \n        else : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) \n    else : \n        self . log . info ( 'No running BigQuery jobs to cancel.' ) \n        return \n    max_polling_attempts = 12 \n    polling_attempts = 0 \n    job_complete = 0 \n    while polling_attempts < max_polling_attempts and not job_complete : \n        polling_attempts = polling_attempts + 1 \n        job_complete = self . poll_job_complete ( self . running_job_id ) \n        if job_complete : \n            self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) \n        elif polling_attempts == max_polling_attempts : \n            self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) \n        else : \n            self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) \n            time . sleep ( 5 ) "}
{"392": "\ndef run_table_delete ( self , deletion_dataset_table , ignore_if_missing = 0 ) : \n    deletion_project , deletion_dataset , deletion_table = _split_tablename ( table_input = deletion_dataset_table , default_project_id = self . project_id ) \n    try : \n        self . service . tables ( ) . delete ( projectId = deletion_project , datasetId = deletion_dataset , tableId = deletion_table ) . execute ( num_retries = self . num_retries ) \n        self . log . info ( 'Deleted table %s:%s.%s.' , deletion_project , deletion_dataset , deletion_table ) \n    except HttpError : \n        if not ignore_if_missing : \n            raise Exception ( 'Table deletion failed. Table does not exist.' ) \n        else : \n            self . log . info ( 'Table does not exist. Skipping.' ) "}
{"393": "\ndef run_table_upsert ( self , dataset_id , table_resource , project_id = None ) : \n    table_id = table_resource [ 'tableReference' ] [ 'tableId' ] \n    project_id = project_id if project_id is not None else self . project_id \n    tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id ) . execute ( num_retries = self . num_retries ) \n    while 1 : \n        for table in tables_list_resp . get ( 'tables' , [ ] ) : \n            if table [ 'tableReference' ] [ 'tableId' ] == table_id : \n                self . log . info ( 'Table %s:%s.%s exists, updating.' , project_id , dataset_id , table_id ) \n                return self . service . tables ( ) . update ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) \n        if 'nextPageToken' in tables_list_resp : \n            tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id , pageToken = tables_list_resp [ 'nextPageToken' ] ) . execute ( num_retries = self . num_retries ) \n        else : \n            self . log . info ( 'Table %s:%s.%s does not exist. creating.' , project_id , dataset_id , table_id ) \n            return self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = self . num_retries ) "}
{"397": "\ndef insert_all ( self , project_id , dataset_id , table_id , rows , ignore_unknown_values = 0 , skip_invalid_rows = 0 , fail_on_error = 0 ) : \n    dataset_project_id = project_id if project_id else self . project_id \n    body = { \"rows\" : rows , \"ignoreUnknownValues\" : ignore_unknown_values , \"kind\" : \"bigquery#tableDataInsertAllRequest\" , \"skipInvalidRows\" : skip_invalid_rows , } \n    try : \n        self . log . info ( 'Inserting %s row(s) into Table %s:%s.%s' , len ( rows ) , dataset_project_id , dataset_id , table_id ) \n        resp = self . service . tabledata ( ) . insertAll ( projectId = dataset_project_id , datasetId = dataset_id , tableId = table_id , body = body ) . execute ( num_retries = self . num_retries ) \n        if 'insertErrors' not in resp : \n            self . log . info ( 'All row(s) inserted successfully: %s:%s.%s' , dataset_project_id , dataset_id , table_id ) \n        else : \n            error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}' . format ( len ( resp [ 'insertErrors' ] ) , dataset_project_id , dataset_id , table_id , resp [ 'insertErrors' ] ) \n            if fail_on_error : \n                raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( error_msg ) ) \n            self . log . info ( error_msg ) \n    except HttpError as err : \n        raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) "}
{"400": "\ndef next ( self ) : \n    if not self . job_id : \n        return None \n    if len ( self . buffer ) == 0 : \n        if self . all_pages_loaded : \n            return None \n        query_results = ( self . service . jobs ( ) . getQueryResults ( projectId = self . project_id , jobId = self . job_id , pageToken = self . page_token ) . execute ( num_retries = self . num_retries ) ) \n        if 'rows' in query_results and query_results [ 'rows' ] : \n            self . page_token = query_results . get ( 'pageToken' ) \n            fields = query_results [ 'schema' ] [ 'fields' ] \n            col_types = [ field [ 'type' ] for field in fields ] \n            rows = query_results [ 'rows' ] \n            for dict_row in rows : \n                typed_row = ( [ _bq_cast ( vs [ 'v' ] , col_types [ idx ] ) for idx , vs in enumerate ( dict_row [ 'f' ] ) ] ) \n                self . buffer . append ( typed_row ) \n            if not self . page_token : \n                self . all_pages_loaded = 1 \n        else : \n            self . page_token = None \n            self . job_id = None \n            self . page_token = None \n            return None \n    return self . buffer . pop ( 0 ) "}
{"405": "\ndef run_command ( self , run_with = None , join_args = 0 ) : \n    run_with = run_with or [ ] \n    cmd = [ \" \" . join ( self . _command ) ] if join_args else self . _command \n    full_cmd = run_with + cmd \n    self . log . info ( 'Running: %s' , full_cmd ) \n    proc = subprocess . Popen ( full_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = 1 , close_fds = 1 , env = os . environ . copy ( ) , preexec_fn = os . setsid ) \n    log_reader = threading . Thread ( target = self . _read_task_logs , args = ( proc . stdout , ) , ) \n    log_reader . daemon = 1 \n    log_reader . start ( ) \n    return proc "}
{"406": "\ndef on_finish ( self ) : \n    if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : \n        if self . run_as_user : \n            subprocess . call ( [ 'sudo' , 'rm' , self . _cfg_path ] , close_fds = 1 ) \n        else : \n            os . remove ( self . _cfg_path ) "}
{"407": "\ndef _main ( ) : \n    usage = \"usage: nvd3.py [options]\" \n    parser = OptionParser ( usage = usage , version = ( \"python-nvd3 - Charts generator with \" \"nvd3.js and d3.js\" ) ) \n    parser . add_option ( \"-q\" , \"--quiet\" , action = \"store_false\" , dest = \"verbose\" , default = 1 , help = \"don't print messages to stdout\" ) \n    ( options , args ) = parser . parse_args ( ) "}
{"411": "\ndef create_x_axis ( self , name , label = None , format = None , date = 0 , custom_format = 0 ) : \n    axis = { } \n    if custom_format and format : \n        axis [ 'tickFormat' ] = format \n    elif format : \n        if format == 'AM_PM' : \n            axis [ 'tickFormat' ] = \"function(d) { return get_am_pm(parseInt(d)); }\" \n        else : \n            axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format \n    if label : \n        axis [ 'axisLabel' ] = \"'\" + label + \"'\" \n    if date : \n        self . dateformat = format \n        axis [ 'tickFormat' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parseInt(d))) }\\n\" \"\" % self . dateformat ) \n        if name [ 0 ] == 'x' : \n            self . x_axis_date = 1 \n    self . axislist [ name ] = axis \n    if name == \"xAxis\" and self . focus_enable : \n        self . axislist [ 'x2Axis' ] = axis "}
{"412": "\ndef create_y_axis ( self , name , label = None , format = None , custom_format = 0 ) : \n    axis = { } \n    if custom_format and format : \n        axis [ 'tickFormat' ] = format \n    elif format : \n        axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format \n    if label : \n        axis [ 'axisLabel' ] = \"'\" + label + \"'\" \n    self . axislist [ name ] = axis "}
{"415": "\ndef gzipped ( f ) : \n    \n    @ functools . wraps ( f ) \n    def view_func ( * args , ** kwargs ) : \n        \n        @ after_this_request \n        def zipper ( response ) : \n            accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) \n            if 'gzip' not in accept_encoding . lower ( ) : \n                return response \n            response . direct_passthrough = 0 \n            if ( response . status_code < 200 or response . status_code >= 300 or 'Content-Encoding' in response . headers ) : \n                return response \n            gzip_buffer = IO ( ) \n            gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) \n            gzip_file . write ( response . data ) \n            gzip_file . close ( ) \n            response . data = gzip_buffer . getvalue ( ) \n            response . headers [ 'Content-Encoding' ] = 'gzip' \n            response . headers [ 'Vary' ] = 'Accept-Encoding' \n            response . headers [ 'Content-Length' ] = len ( response . data ) \n            return response \n        return f ( * args , ** kwargs ) \n    return view_func "}
{"416": "\ndef get_last_dagrun ( dag_id , session , include_externally_triggered = 0 ) : \n    DR = DagRun \n    query = session . query ( DR ) . filter ( DR . dag_id == dag_id ) \n    if not include_externally_triggered : \n        query = query . filter ( DR . external_trigger == 0 ) \n    query = query . order_by ( DR . execution_date . desc ( ) ) \n    return query . first ( ) "}
{"417": "\ndef create_dagrun ( self , run_id , state , execution_date , start_date = None , external_trigger = 0 , conf = None , session = None ) : \n    return self . get_dag ( ) . create_dagrun ( run_id = run_id , state = state , execution_date = execution_date , start_date = start_date , external_trigger = external_trigger , conf = conf , session = session ) "}
{"432": "\ndef poll_query_status ( self , query_execution_id , max_tries = None ) : \n    try_number = 1 \n    final_query_state = None \n    while 1 : \n        query_state = self . check_query_status ( query_execution_id ) \n        if query_state is None : \n            self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) \n        elif query_state in self . INTERMEDIATE_STATES : \n            self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) \n        else : \n            self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) \n            final_query_state = query_state \n            break \n        if max_tries and try_number >= max_tries : \n            final_query_state = query_state \n            break \n        try_number += 1 \n        sleep ( self . sleep_time ) \n    return final_query_state "}
{"435": "\ndef call ( self , path , query = None , get_all_pages = 1 , side_loading = 0 ) : \n    zendesk = self . get_conn ( ) \n    first_request_successful = 0 \n    while not first_request_successful : \n        try : \n            results = zendesk . call ( path , query ) \n            first_request_successful = 1 \n        except RateLimitError as rle : \n            self . __handle_rate_limit_exception ( rle ) \n    keys = [ path . split ( \"/\" ) [ - 1 ] . split ( \".json\" ) [ 0 ] ] \n    next_page = results [ 'next_page' ] \n    if side_loading : \n        keys += query [ 'include' ] . split ( ',' ) \n    results = { key : results [ key ] for key in keys } \n    if get_all_pages : \n        while next_page is not None : \n            try : \n                next_url = next_page . split ( self . __url ) [ 1 ] \n                self . log . info ( \"Calling %s\" , next_url ) \n                more_res = zendesk . call ( next_url ) \n                for key in results : \n                    results [ key ] . extend ( more_res [ key ] ) \n                if next_page == more_res [ 'next_page' ] : \n                    break \n                else : \n                    next_page = more_res [ 'next_page' ] \n            except RateLimitError as rle : \n                self . __handle_rate_limit_exception ( rle ) \n            except ZendeskError as ze : \n                if b\"Use a start_time older than 5 minutes\" in ze . msg : \n                    break \n                else : \n                    raise ze \n    return results "}
{"440": "\ndef delete_cluster ( self , cluster_identifier , skip_final_cluster_snapshot = 1 , final_cluster_snapshot_identifier = '' ) : \n    response = self . get_conn ( ) . delete_cluster ( ClusterIdentifier = cluster_identifier , SkipFinalClusterSnapshot = skip_final_cluster_snapshot , FinalClusterSnapshotIdentifier = final_cluster_snapshot_identifier ) \n    return response [ 'Cluster' ] if response [ 'Cluster' ] else None "}
{"441": "\ndef describe_cluster_snapshots ( self , cluster_identifier ) : \n    response = self . get_conn ( ) . describe_cluster_snapshots ( ClusterIdentifier = cluster_identifier ) \n    if 'Snapshots' not in response : \n        return None \n    snapshots = response [ 'Snapshots' ] \n    snapshots = filter ( lambda x : x [ 'Status' ] , snapshots ) \n    snapshots . sort ( key = lambda x : x [ 'SnapshotCreateTime' ] , reverse = 1 ) \n    return snapshots "}
{"448": "\ndef execute ( self , context ) : \n    s3_conn = S3Hook ( self . s3_conn_id ) \n    if self . is_pipeline : \n        results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , mongo_db = self . mongo_db ) \n    else : \n        results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) \n    docs_str = self . _stringify ( self . transform ( results ) ) \n    s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) \n    return 1 "}
{"450": "\ndef create_pool ( name , slots , description , session = None ) : \n    if not ( name and name . strip ( ) ) : \n        raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) \n    try : \n        slots = int ( slots ) \n    except ValueError : \n        raise AirflowBadRequest ( \"Bad value for `slots`: %s\" % slots ) \n    session . expire_on_commit = 0 \n    pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) \n    if pool is None : \n        pool = Pool ( pool = name , slots = slots , description = description ) \n        session . add ( pool ) \n    else : \n        pool . slots = slots \n        pool . description = description \n    session . commit ( ) \n    return pool "}
{"462": "\ndef import_table ( self , table , target_dir = None , append = 0 , file_type = \"text\" , columns = None , split_by = None , where = None , direct = 0 , driver = None , extra_import_options = None ) : \n    cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) \n    cmd += [ \"--table\" , table ] \n    if columns : \n        cmd += [ \"--columns\" , columns ] \n    if where : \n        cmd += [ \"--where\" , where ] \n    self . Popen ( cmd ) "}
{"463": "\ndef import_query ( self , query , target_dir , append = 0 , file_type = \"text\" , split_by = None , direct = None , driver = None , extra_import_options = None ) : \n    cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) \n    cmd += [ \"--query\" , query ] \n    self . Popen ( cmd ) "}
{"467": "\ndef close ( self ) : \n    if self . closed : \n        return \n    super ( ) . close ( ) \n    if not self . upload_on_close : \n        return \n    local_loc = os . path . join ( self . local_base , self . log_relative_path ) \n    remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) \n    if os . path . exists ( local_loc ) : \n        with open ( local_loc , 'r' ) as logfile : \n            log = logfile . read ( ) \n        self . s3_write ( log , remote_loc ) \n    self . closed = 1 "}
{"468": "\ndef _get_init_containers ( self ) : \n    if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : \n        return [ ] \n    init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] \n    if self . kube_config . git_user : \n        init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) \n    if self . kube_config . git_password : \n        init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) \n    volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : 0 } ] \n    if self . kube_config . git_ssh_key_secret_name : \n        volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) \n        init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) \n    if self . kube_config . git_ssh_known_hosts_configmap_name : \n        volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) \n        init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) \n    else : \n        init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'false' } ) \n    return [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533 } , 'env' : init_environment , 'volumeMounts' : volume_mounts } ] "}
{"476": "\ndef done ( self ) : \n    if self . _process is None : \n        raise AirflowException ( \"Tried to see if it's done before starting!\" ) \n    if self . _done : \n        return 1 \n    if self . _result_queue and not self . _result_queue . empty ( ) : \n        self . _result = self . _result_queue . get_nowait ( ) \n        self . _done = 1 \n        self . log . debug ( \"Waiting for %s\" , self . _process ) \n        self . _process . join ( ) \n        return 1 \n    if self . _result_queue and not self . _process . is_alive ( ) : \n        self . _done = 1 \n        if not self . _result_queue . empty ( ) : \n            self . _result = self . _result_queue . get_nowait ( ) \n        self . log . debug ( \"Waiting for %s\" , self . _process ) \n        self . _process . join ( ) \n        return 1 \n    return 0 "}
{"479": "\ndef _process_task_instances ( self , dag , queue , session = None ) : \n    dag_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) \n    active_dag_runs = [ ] \n    for run in dag_runs : \n        self . log . info ( \"Examining DAG run %s\" , run ) \n        if run . execution_date > timezone . utcnow ( ) : \n            self . log . error ( \"Execution date is in future: %s\" , run . execution_date ) \n            continue \n        if len ( active_dag_runs ) >= dag . max_active_runs : \n            self . log . info ( \"Number of active dag runs reached max_active_run.\" ) \n            break \n        if run . is_backfill : \n            continue \n        run . dag = dag \n        run . verify_integrity ( session = session ) \n        run . update_state ( session = session ) \n        if run . state == State . RUNNING : \n            make_transient ( run ) \n            active_dag_runs . append ( run ) \n    for run in active_dag_runs : \n        self . log . debug ( \"Examining active DAG run: %s\" , run ) \n        tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) \n        for ti in tis : \n            task = dag . get_task ( ti . task_id ) \n            ti . task = task \n            if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = 1 ) , session = session ) : \n                self . log . debug ( 'Queuing task: %s' , ti ) \n                queue . append ( ti . key ) "}
{"480": "\ndef _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : \n    tis_changed = 0 \n    query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) \n    if self . using_sqlite : \n        tis_to_change = query . with_for_update ( ) . all ( ) \n        for ti in tis_to_change : \n            ti . set_state ( new_state , session = session ) \n            tis_changed += 1 \n    else : \n        subq = query . subquery ( ) \n        tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = 0 ) \n        session . commit ( ) \n    if tis_changed > 0 : \n        self . log . warning ( \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state ) "}
{"483": "\ndef _enqueue_task_instances_with_queued_state ( self , simple_dag_bag , simple_task_instances ) : \n    TI = models . TaskInstance \n    for simple_task_instance in simple_task_instances : \n        simple_dag = simple_dag_bag . get_dag ( simple_task_instance . dag_id ) \n        command = TI . generate_command ( simple_task_instance . dag_id , simple_task_instance . task_id , simple_task_instance . execution_date , local = 1 , mark_success = 0 , ignore_all_deps = 0 , ignore_depends_on_past = 0 , ignore_task_deps = 0 , ignore_ti_state = 0 , pool = simple_task_instance . pool , file_path = simple_dag . full_filepath , pickle_id = simple_dag . pickle_id ) \n        priority = simple_task_instance . priority_weight \n        queue = simple_task_instance . queue \n        self . log . info ( \"Sending %s to executor with priority %s and queue %s\" , simple_task_instance . key , priority , queue ) \n        self . executor . queue_command ( simple_task_instance , command , priority = priority , queue = queue ) "}
{"487": "\ndef process_file ( self , file_path , zombies , pickle_dags = 0 , session = None ) : \n    self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) \n    simple_dags = [ ] \n    try : \n        dagbag = models . DagBag ( file_path , include_examples = 0 ) \n    except Exception : \n        self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) \n        Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) \n        return [ ] \n    if len ( dagbag . dags ) > 0 : \n        self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) \n    else : \n        self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) \n        self . update_import_errors ( session , dagbag ) \n        return [ ] \n    for dag in dagbag . dags . values ( ) : \n        dag . sync_to_db ( ) \n    paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] \n    for dag_id in dagbag . dags : \n        if dag_id not in paused_dag_ids : \n            dag = dagbag . get_dag ( dag_id ) \n            pickle_id = None \n            if pickle_dags : \n                pickle_id = dag . pickle ( session ) . id \n            simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) \n    if len ( self . dag_ids ) > 0 : \n        dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] \n    else : \n        dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] \n    ti_keys_to_schedule = [ ] \n    self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) \n    for ti_key in ti_keys_to_schedule : \n        dag = dagbag . dags [ ti_key [ 0 ] ] \n        task = dag . get_task ( ti_key [ 1 ] ) \n        ti = models . TaskInstance ( task , ti_key [ 2 ] ) \n        ti . refresh_from_db ( session = session , lock_for_update = 1 ) \n        dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = 1 ) \n        if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = 1 ) : \n            ti . state = State . SCHEDULED \n        self . log . info ( \"Creating / updating %s in ORM\" , ti ) \n        session . merge ( ti ) \n    session . commit ( ) \n    try : \n        self . update_import_errors ( session , dagbag ) \n    except Exception : \n        self . log . exception ( \"Error logging import errors!\" ) \n    try : \n        dagbag . kill_zombies ( zombies ) \n    except Exception : \n        self . log . exception ( \"Error killing zombies!\" ) \n    return simple_dags "}
{"490": "\ndef _get_dag_run ( self , run_date , session = None ) : \n    run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) \n    respect_dag_max_active_limit = ( 1 if ( self . dag . schedule_interval and not self . dag . is_subdag ) else 0 ) \n    current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = 0 ) \n    run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) \n    if run is not None and len ( run ) > 0 : \n        run = run [ 0 ] \n        if run . state == State . RUNNING : \n            respect_dag_max_active_limit = 0 \n    else : \n        run = None \n    if ( respect_dag_max_active_limit and current_active_dag_count >= self . dag . max_active_runs ) : \n        return None \n    run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = 0 , session = session , conf = self . conf , ) \n    run . dag = self . dag \n    run . state = State . RUNNING \n    run . run_id = run_id \n    run . verify_integrity ( session = session ) \n    return run "}
{"495": "\ndef heartbeat_callback ( self , session = None ) : \n    if self . terminating : \n        self . task_runner . terminate ( ) \n        return \n    self . task_instance . refresh_from_db ( ) \n    ti = self . task_instance \n    fqdn = get_hostname ( ) \n    same_hostname = fqdn == ti . hostname \n    same_process = ti . pid == os . getpid ( ) \n    if ti . state == State . RUNNING : \n        if not same_hostname : \n            self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) \n            raise AirflowException ( \"Hostname of job runner does not match\" ) \n        elif not same_process : \n            current_pid = os . getpid ( ) \n            self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current_pid ) \n            raise AirflowException ( \"PID of job runner does not match\" ) \n    elif ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : \n        self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) \n        self . task_runner . terminate ( ) \n        self . terminating = 1 "}
{"510": "\ndef record_exists ( self , table , keys ) : \n    keyspace = self . keyspace \n    if '.' in table : \n        keyspace , table = table . split ( '.' , 1 ) \n    ks = \" AND \" . join ( \"{}=%({})s\" . format ( key , key ) for key in keys . keys ( ) ) \n    cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\" . format ( keyspace = keyspace , table = table , keys = ks ) \n    try : \n        rs = self . get_conn ( ) . execute ( cql , keys ) \n        return rs . one ( ) is not None \n    except Exception : \n        return 0 "}
{"512": "\ndef submit ( self , application = \"\" , ** kwargs ) : \n    spark_submit_cmd = self . _build_spark_submit_command ( application ) \n    if hasattr ( self , '_env' ) : \n        env = os . environ . copy ( ) \n        env . update ( self . _env ) \n        kwargs [ \"env\" ] = env \n    self . _submit_sp = subprocess . Popen ( spark_submit_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - 1 , universal_newlines = 1 , ** kwargs ) \n    self . _process_spark_submit_log ( iter ( self . _submit_sp . stdout . readline , '' ) ) \n    returncode = self . _submit_sp . wait ( ) \n    if returncode or ( self . _is_kubernetes and self . _spark_exit_code != 0 ) : \n        raise AirflowException ( \"Cannot execute: {}. Error code is: {}.\" . format ( spark_submit_cmd , returncode ) ) \n    self . log . debug ( \"Should track driver: {}\" . format ( self . _should_track_driver_status ) ) \n    if self . _should_track_driver_status : \n        if self . _driver_id is None : \n            raise AirflowException ( \"No driver id is known: something went wrong when executing \" + \"the spark submit command\" ) \n        self . _driver_status = \"SUBMITTED\" \n        self . _start_driver_status_tracking ( ) \n        if self . _driver_status != \"FINISHED\" : \n            raise AirflowException ( \"ERROR : Driver {} badly exited with status {}\" . format ( self . _driver_id , self . _driver_status ) ) "}
{"516": "\ndef _wait_for_task_ended ( self ) : \n    try : \n        waiter = self . client . get_waiter ( 'job_execution_complete' ) \n        waiter . config . max_attempts = sys . maxsize \n        waiter . wait ( jobs = [ self . jobId ] ) \n    except ValueError : \n        retry = 1 \n        retries = 0 \n        while retries < self . max_retries and retry : \n            self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) \n            response = self . client . describe_jobs ( jobs = [ self . jobId ] ) \n            if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : \n                retry = 0 \n            sleep ( 1 + pow ( retries * 0.1 , 2 ) ) \n            retries += 1 "}
{"519": "\ndef _write_local_schema_file ( self , cursor ) : \n    schema_str = None \n    schema_file_mime_type = 'application/json' \n    tmp_schema_file_handle = NamedTemporaryFile ( delete = 1 ) \n    if self . schema is not None and isinstance ( self . schema , string_types ) : \n        schema_str = self . schema . encode ( 'utf-8' ) \n    elif self . schema is not None and isinstance ( self . schema , list ) : \n        schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) \n    else : \n        schema = [ ] \n        for field in cursor . description : \n            field_name = field [ 0 ] \n            field_type = self . type_map ( field [ 1 ] ) \n            if field [ 6 ] or field_type == 'TIMESTAMP' : \n                field_mode = 'NULLABLE' \n            else : \n                field_mode = 'REQUIRED' \n            schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) \n        schema_str = json . dumps ( schema , sort_keys = 1 ) . encode ( 'utf-8' ) \n    tmp_schema_file_handle . write ( schema_str ) \n    self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) \n    schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } \n    return schema_file_to_upload "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = 0 \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = 1 \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > 0 : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if num > 0 : \n                    start_date += delta \n                else : \n                    start_date -= delta \n    return sorted ( dates ) "}
{"533": "\ndef _has_perm ( self , permission_name , view_menu_name ) : \n    if hasattr ( self , 'perms' ) : \n        if ( permission_name , view_menu_name ) in self . perms : \n            return 1 \n    self . _get_and_cache_perms ( ) \n    return ( permission_name , view_menu_name ) in self . perms "}
{"539": "\ndef get_fernet ( ) : \n    global _fernet \n    log = LoggingMixin ( ) . log \n    if _fernet : \n        return _fernet \n    try : \n        from cryptography . fernet import Fernet , MultiFernet , InvalidToken \n        global InvalidFernetToken \n        InvalidFernetToken = InvalidToken \n    except BuiltinImportError : \n        log . warning ( \"cryptography not found - values will not be stored encrypted.\" ) \n        _fernet = NullFernet ( ) \n        return _fernet \n    try : \n        fernet_key = configuration . conf . get ( 'core' , 'FERNET_KEY' ) \n        if not fernet_key : \n            log . warning ( \"empty cryptography key - values will not be stored encrypted.\" ) \n            _fernet = NullFernet ( ) \n        else : \n            _fernet = MultiFernet ( [ Fernet ( fernet_part . encode ( 'utf-8' ) ) for fernet_part in fernet_key . split ( ',' ) ] ) \n            _fernet . is_encrypted = 1 \n    except ( ValueError , TypeError ) as ve : \n        raise AirflowException ( \"Could not create Fernet object: {}\" . format ( ve ) ) \n    return _fernet "}
{"542": "\ndef poke ( self , context ) : \n    sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) \n    sqs_conn = sqs_hook . get_conn ( ) \n    self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) \n    messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) \n    self . log . info ( \"reveived message %s\" , str ( messages ) ) \n    if 'Messages' in messages and len ( messages [ 'Messages' ] ) > 0 : \n        entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] \n        result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) \n        if 'Successful' in result : \n            context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) \n            return 1 \n        else : \n            raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) \n    return 0 "}
{"543": "\ndef get_conn ( self ) : \n    effective_user = self . proxy_user \n    autoconfig = self . autoconfig \n    use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' \n    try : \n        connections = self . get_connections ( self . hdfs_conn_id ) \n        if not effective_user : \n            effective_user = connections [ 0 ] . login \n        if not autoconfig : \n            autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , 0 ) \n        hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) \n    except AirflowException : \n        if not autoconfig : \n            raise \n    if autoconfig : \n        client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) \n    elif len ( connections ) == 1 : \n        client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    elif len ( connections ) > 1 : \n        nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] \n        client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    else : \n        raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) \n    return client "}
{"545": "\ndef check_for_path ( self , hdfs_path ) : \n    conn = self . get_conn ( ) \n    status = conn . status ( hdfs_path , strict = 0 ) \n    return bool ( status ) "}
{"546": "\ndef load_file ( self , source , destination , overwrite = 1 , parallelism = 1 , ** kwargs ) : \n    conn = self . get_conn ( ) \n    conn . upload ( hdfs_path = destination , local_path = source , overwrite = overwrite , n_threads = parallelism , ** kwargs ) \n    self . log . debug ( \"Uploaded file %s to %s\" , source , destination ) "}
{"553": "\ndef run ( self , sql , autocommit = 0 , parameters = None ) : \n    if isinstance ( sql , basestring ) : \n        sql = [ sql ] \n    with closing ( self . get_conn ( ) ) as conn : \n        if self . supports_autocommit : \n            self . set_autocommit ( conn , autocommit ) \n        with closing ( conn . cursor ( ) ) as cur : \n            for s in sql : \n                if parameters is not None : \n                    self . log . info ( \"{} with parameters {}\" . format ( s , parameters ) ) \n                    cur . execute ( s , parameters ) \n                else : \n                    self . log . info ( s ) \n                    cur . execute ( s ) \n        if not self . get_autocommit ( conn ) : \n            conn . commit ( ) "}
{"555": "\ndef insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 , replace = 0 ) : \n    if target_fields : \n        target_fields = \", \" . join ( target_fields ) \n        target_fields = \"({})\" . format ( target_fields ) \n    else : \n        target_fields = '' \n    i = 0 \n    with closing ( self . get_conn ( ) ) as conn : \n        if self . supports_autocommit : \n            self . set_autocommit ( conn , 0 ) \n        conn . commit ( ) \n        with closing ( conn . cursor ( ) ) as cur : \n            for i , row in enumerate ( rows , 1 ) : \n                lst = [ ] \n                for cell in row : \n                    lst . append ( self . _serialize_cell ( cell , conn ) ) \n                values = tuple ( lst ) \n                placeholders = [ \"%s\" , ] * len ( values ) \n                if not replace : \n                    sql = \"INSERT INTO \" \n                else : \n                    sql = \"REPLACE INTO \" \n                sql += \"{0} {1} VALUES ({2})\" . format ( table , target_fields , \",\" . join ( placeholders ) ) \n                cur . execute ( sql , values ) \n                if commit_every and i % commit_every == 0 : \n                    conn . commit ( ) \n                    self . log . info ( \"Loaded %s into %s rows so far\" , i , table ) \n        conn . commit ( ) \n    self . log . info ( \"Done loading. Loaded a total of %s rows\" , i ) "}
{"561": "\ndef _get_credentials ( self ) : \n    key_path = self . _get_field ( 'key_path' , 0 ) \n    keyfile_dict = self . _get_field ( 'keyfile_dict' , 0 ) \n    scope = self . _get_field ( 'scope' , None ) \n    if scope : \n        scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] \n    else : \n        scopes = _DEFAULT_SCOPES \n    if not key_path and not keyfile_dict : \n        self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) \n        credentials , _ = google . auth . default ( scopes = scopes ) \n    elif key_path : \n        if key_path . endswith ( '.json' ) : \n            self . log . debug ( 'Getting connection using JSON key file %s' % key_path ) \n            credentials = ( google . oauth2 . service_account . Credentials . from_service_account_file ( key_path , scopes = scopes ) ) \n        elif key_path . endswith ( '.p12' ) : \n            raise AirflowException ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) \n        else : \n            raise AirflowException ( 'Unrecognised extension for key file.' ) \n    else : \n        try : \n            keyfile_dict = json . loads ( keyfile_dict ) \n            keyfile_dict [ 'private_key' ] = keyfile_dict [ 'private_key' ] . replace ( '\\\\n' , '\\n' ) \n            credentials = ( google . oauth2 . service_account . Credentials . from_service_account_info ( keyfile_dict , scopes = scopes ) ) \n        except json . decoder . JSONDecodeError : \n            raise AirflowException ( 'Invalid key JSON.' ) \n    return credentials . with_subject ( self . delegate_to ) if self . delegate_to else credentials "}
{"567": "\ndef to_tensor ( pic ) : \n    if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : \n        raise TypeError ( 'pic should be PIL Image or ndarray. Got {}' . format ( type ( pic ) ) ) \n    if isinstance ( pic , np . ndarray ) : \n        if pic . ndim == 2 : \n            pic = pic [ : , : , None ] \n        img = torch . from_numpy ( pic . transpose ( ( 2 , 0 , 1 ) ) ) \n        if isinstance ( img , torch . ByteTensor ) : \n            return img . float ( ) . div ( 255 ) \n        else : \n            return img \n    if accimage is not None and isinstance ( pic , accimage . Image ) : \n        nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) \n        pic . copyto ( nppic ) \n        return torch . from_numpy ( nppic ) \n    if pic . mode == 'I' : \n        img = torch . from_numpy ( np . array ( pic , np . int32 , copy = 0 ) ) \n    elif pic . mode == 'I;16' : \n        img = torch . from_numpy ( np . array ( pic , np . int16 , copy = 0 ) ) \n    elif pic . mode == 'F' : \n        img = torch . from_numpy ( np . array ( pic , np . float32 , copy = 0 ) ) \n    elif pic . mode == '1' : \n        img = 255 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = 0 ) ) \n    else : \n        img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) \n    if pic . mode == 'YCbCr' : \n        nchannel = 3 \n    elif pic . mode == 'I;16' : \n        nchannel = 1 \n    else : \n        nchannel = len ( pic . mode ) \n    img = img . view ( pic . size [ 1 ] , pic . size [ 0 ] , nchannel ) \n    img = img . transpose ( 0 , 1 ) . transpose ( 0 , 2 ) . contiguous ( ) \n    if isinstance ( img , torch . ByteTensor ) : \n        return img . float ( ) . div ( 255 ) \n    else : \n        return img "}
{"568": "\ndef normalize ( tensor , mean , std , inplace = 0 ) : \n    if not _is_tensor_image ( tensor ) : \n        raise TypeError ( 'tensor is not a torch image.' ) \n    if not inplace : \n        tensor = tensor . clone ( ) \n    mean = torch . as_tensor ( mean , dtype = torch . float32 , device = tensor . device ) \n    std = torch . as_tensor ( std , dtype = torch . float32 , device = tensor . device ) \n    tensor . sub_ ( mean [ : , None , None ] ) . div_ ( std [ : , None , None ] ) \n    return tensor "}
{"582": "\ndef rotate ( img , angle , resample = 0 , expand = 0 , center = None ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    return img . rotate ( angle , resample , expand , center ) "}
{"585": "\ndef save_image ( tensor , filename , nrow = 8 , padding = 2 , normalize = 0 , range = None , scale_each = 0 , pad_value = 0 ) : \n    from PIL import Image \n    grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value , normalize = normalize , range = range , scale_each = scale_each ) \n    ndarr = grid . mul_ ( 255 ) . add_ ( 0.5 ) . clamp_ ( 0 , 255 ) . permute ( 1 , 2 , 0 ) . to ( 'cpu' , torch . uint8 ) . numpy ( ) \n    im = Image . fromarray ( ndarr ) \n    im . save ( filename ) "}
{"590": "\ndef accuracy ( output , target , topk = ( 1 , ) ) : \n    with torch . no_grad ( ) : \n        maxk = max ( topk ) \n        batch_size = target . size ( 0 ) \n        _ , pred = output . topk ( maxk , 1 , 1 , 1 ) \n        pred = pred . t ( ) \n        correct = pred . eq ( target [ None ] ) \n        res = [ ] \n        for k in topk : \n            correct_k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) \n            res . append ( correct_k * ( 100.0 / batch_size ) ) \n        return res "}
{"591": "\ndef setup_for_distributed ( is_master ) : \n    import builtins as __builtin__ \n    builtin_print = __builtin__ . print \n    def print ( * args , ** kwargs ) : \n        force = kwargs . pop ( 'force' , 0 ) \n        if is_master or force : \n            builtin_print ( * args , ** kwargs ) \n    __builtin__ . print = print "}
{"593": "\ndef list_dir ( root , prefix = 0 ) : \n    root = os . path . expanduser ( root ) \n    directories = list ( filter ( lambda p : os . path . isdir ( os . path . join ( root , p ) ) , os . listdir ( root ) ) ) \n    if prefix is 1 : \n        directories = [ os . path . join ( root , d ) for d in directories ] \n    return directories "}
{"594": "\ndef list_files ( root , suffix , prefix = 0 ) : \n    root = os . path . expanduser ( root ) \n    files = list ( filter ( lambda p : os . path . isfile ( os . path . join ( root , p ) ) and p . endswith ( suffix ) , os . listdir ( root ) ) ) \n    if prefix is 1 : \n        files = [ os . path . join ( root , d ) for d in files ] \n    return files "}
{"595": "\ndef download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) : \n    import requests \n    url = \"https://docs.google.com/uc?export=download\" \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = file_id \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        print ( 'Using downloaded and verified file: ' + fpath ) \n    else : \n        session = requests . Session ( ) \n        response = session . get ( url , params = { 'id' : file_id } , stream = 1 ) \n        token = _get_confirm_token ( response ) \n        if token : \n            params = { 'id' : file_id , 'confirm' : token } \n            response = session . get ( url , params = params , stream = 1 ) \n        _save_response_content ( response , fpath ) "}
{"602": "\ndef download ( self ) : \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    for url in self . urls : \n        filename = url . rpartition ( '/' ) [ 2 ] \n        file_path = os . path . join ( self . raw_folder , filename ) \n        download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) \n        self . extract_gzip ( gzip_path = file_path , remove_finished = 1 ) \n    print ( 'Processing...' ) \n    training_set = ( read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) ) \n    test_set = ( read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) ) \n    with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : \n        torch . save ( training_set , f ) \n    with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : \n        torch . save ( test_set , f ) \n    print ( 'Done!' ) "}
{"606": "\ndef preferences ( ) : \n    if request . method == 'POST' : \n        resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) \n        try : \n            request . preferences . parse_form ( request . form ) \n        except ValidationException : \n            request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) \n            return resp \n        return request . preferences . save ( resp ) \n    image_proxy = request . preferences . get_value ( 'image_proxy' ) \n    lang = request . preferences . get_value ( 'language' ) \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    allowed_plugins = request . preferences . plugins . get_enabled ( ) \n    stats = { } \n    for c in categories : \n        for e in categories [ c ] : \n            stats [ e . name ] = { 'time' : None , 'warn_timeout' : 0 , 'warn_time' : 0 } \n            if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : \n                stats [ e . name ] [ 'warn_timeout' ] = 1 \n            stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) \n    for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : \n        stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) \n        if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : \n            stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = 1 \n    return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = 1 ) "}
{"610": "\ndef custom_gradient ( fx , gx , x , fx_gx_manually_stopped = 0 , name = None ) : \n    def maybe_stop ( x ) : \n        if fx_gx_manually_stopped : \n            return x \n        return tf . stop_gradient ( x ) \n    with tf . compat . v1 . name_scope ( name , 'custom_gradient' , [ fx , gx , x ] ) : \n        fx = tf . convert_to_tensor ( value = fx , name = 'fx' ) \n        with tf . control_dependencies ( [ fx ] ) : \n            if is_list_like ( x ) : \n                x = [ identity ( x_ , name = 'x' ) for x_ in x ] \n            else : \n                x = [ identity ( x , name = 'x' ) ] \n            if is_list_like ( gx ) : \n                gx = [ identity ( gx_ , dtype = fx . dtype , name = 'gx' ) for gx_ in gx ] \n            else : \n                gx = [ identity ( gx , dtype = fx . dtype , name = 'gx' ) ] \n            override_grad = [ ] \n            for x_ , gx_ in zip ( x , gx ) : \n                equal_shape = tf . compat . v1 . assert_equal ( tf . shape ( input = x_ ) , tf . shape ( input = gx_ ) , message = 'Each `x` must have the same shape as each `gx`.' ) \n                with tf . control_dependencies ( [ equal_shape ] ) : \n                    zeros_like_x_ = x_ - tf . stop_gradient ( x_ ) \n                    override_grad . append ( tf . reduce_sum ( input_tensor = maybe_stop ( gx_ ) * zeros_like_x_ ) ) \n            override_grad = sum ( override_grad ) \n            override_grad /= tf . cast ( tf . size ( input = fx ) , dtype = fx . dtype . base_dtype ) \n            return maybe_stop ( fx ) + override_grad "}
{"621": "\ndef _max_mask_non_finite ( x , axis = - 1 , keepdims = 0 , mask = 0 ) : \n    m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) \n    needs_masking = ~ np . isfinite ( m ) \n    if needs_masking . ndim > 0 : \n        m [ needs_masking ] = mask \n    elif needs_masking : \n        m = mask \n    return m "}
{"634": "\ndef _maybe_check_valid_map_values ( map_values , validate_args ) : \n    assertions = [ ] \n    message = 'Rank of map_values must be 1.' \n    if tensorshape_util . rank ( map_values . shape ) is not None : \n        if tensorshape_util . rank ( map_values . shape ) != 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) \n    message = 'Size of map_values must be greater than 0.' \n    if tensorshape_util . num_elements ( map_values . shape ) is not None : \n        if tensorshape_util . num_elements ( map_values . shape ) == 0 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , 1 , message = 'map_values is not strictly increasing.' ) ) \n    return assertions "}
{"646": "\ndef _create_scale_operator ( self , identity_multiplier , diag , tril , perturb_diag , perturb_factor , shift , validate_args , dtype ) : \n    identity_multiplier = _as_tensor ( identity_multiplier , \"identity_multiplier\" , dtype ) \n    diag = _as_tensor ( diag , \"diag\" , dtype ) \n    tril = _as_tensor ( tril , \"tril\" , dtype ) \n    perturb_diag = _as_tensor ( perturb_diag , \"perturb_diag\" , dtype ) \n    perturb_factor = _as_tensor ( perturb_factor , \"perturb_factor\" , dtype ) \n    shape_hint = None \n    if perturb_factor is not None : \n        shape_hint = distribution_util . dimension_size ( perturb_factor , axis = - 2 ) \n    if self . _is_only_identity_multiplier : \n        if validate_args : \n            return distribution_util . with_dependencies ( [ assert_util . assert_none_equal ( identity_multiplier , tf . zeros ( [ ] , identity_multiplier . dtype ) , [ \"identity_multiplier should be non-zero.\" ] ) ] , identity_multiplier ) \n        return identity_multiplier \n    scale = distribution_util . make_tril_scale ( loc = shift , scale_tril = tril , scale_diag = diag , scale_identity_multiplier = identity_multiplier , validate_args = validate_args , assert_positive = 0 , shape_hint = shape_hint ) \n    if perturb_factor is not None : \n        return tf . linalg . LinearOperatorLowRankUpdate ( scale , u = perturb_factor , diag_update = perturb_diag , is_diag_update_positive = perturb_diag is None , is_non_singular = 1 , is_self_adjoint = 1 , is_positive_definite = 1 , is_square = 1 ) \n    return scale "}
{"650": "\ndef entropy_lower_bound ( self , name = \"entropy_lower_bound\" ) : \n    with self . _name_scope ( name ) : \n        with tf . control_dependencies ( self . _assertions ) : \n            distribution_entropies = [ d . entropy ( ) for d in self . components ] \n            cat_probs = self . _cat_probs ( log_probs = 0 ) \n            partial_entropies = [ c_p * m for ( c_p , m ) in zip ( cat_probs , distribution_entropies ) ] \n            return tf . add_n ( partial_entropies ) "}
{"652": "\ndef _maybe_validate_args ( outcomes , logits , probs , validate_args ) : \n    assertions = [ ] \n    def validate_equal_last_dim ( tensor_a , tensor_b , message ) : \n        if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : \n            if tensor_a . shape [ - 1 ] != tensor_b . shape [ - 1 ] : \n                raise ValueError ( message ) \n        elif validate_args : \n            assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - 1 ] , tf . shape ( input = tensor_b ) [ - 1 ] , message = message ) ) \n    if logits is not None : \n        validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) \n    if probs is not None : \n        validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) \n    message = 'Rank of outcomes must be 1.' \n    if outcomes . shape . ndims is not None : \n        if outcomes . shape . ndims != 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) \n    message = 'Size of outcomes must be greater than 0.' \n    if outcomes . shape . num_elements ( ) is not None : \n        if outcomes . shape . num_elements ( ) == 0 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , 1 , message = 'outcomes is not strictly increasing.' ) ) \n    return assertions "}
{"655": "\ndef covertype ( ) : \n    import sklearn . datasets \n    data = sklearn . datasets . covtype . fetch_covtype ( ) \n    features = data . data \n    labels = data . target \n    features -= features . mean ( 0 ) \n    features /= features . std ( 0 ) \n    features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) \n    features = tf . cast ( features , dtype = tf . float32 ) \n    _ , counts = np . unique ( labels , return_counts = 1 ) \n    specific_category = np . argmax ( counts ) \n    labels = ( labels == specific_category ) \n    labels = tf . cast ( labels , dtype = tf . int32 ) \n    return features , labels "}
{"656": "\ndef cholesky_covariance ( x , sample_axis = 0 , keepdims = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'cholesky_covariance' , values = [ x , sample_axis ] ) : \n        sample_axis = tf . convert_to_tensor ( value = sample_axis , dtype = tf . int32 ) \n        cov = covariance ( x , sample_axis = sample_axis , event_axis = - 1 , keepdims = keepdims ) \n        return tf . linalg . cholesky ( cov ) "}
{"657": "\ndef stddev ( x , sample_axis = 0 , keepdims = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'stddev' , values = [ x , sample_axis ] ) : \n        return tf . sqrt ( variance ( x , sample_axis = sample_axis , keepdims = keepdims ) ) "}
{"658": "\ndef variance ( x , sample_axis = 0 , keepdims = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'variance' , values = [ x , sample_axis ] ) : \n        return covariance ( x , y = None , sample_axis = sample_axis , event_axis = None , keepdims = keepdims ) "}
{"665": "\ndef sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = 1 , seed = None , name = None ) : \n    if dim < 1 or dim > _MAX_DIMENSION : \n        raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) \n    if ( num_results is None ) == ( sequence_indices is None ) : \n        raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) \n    if not dtype . is_floating : \n        raise ValueError ( 'dtype must be of `float`-type' ) \n    with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : \n        if num_results is not None : \n            num_results = tf . convert_to_tensor ( value = num_results ) \n        if sequence_indices is not None : \n            sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) \n        indices = _get_indices ( num_results , sequence_indices , dtype ) \n        radixes = tf . constant ( _PRIMES [ 0 : dim ] , dtype = dtype , shape = [ dim , 1 ] ) \n        max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) \n        max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) \n        exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , 1 ] ) \n        weight_mask = exponents_by_axes >= max_sizes_by_axes \n        capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) \n        weights = radixes ** capped_exponents \n        coeffs = tf . math . floordiv ( indices , weights ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs %= radixes \n        if not randomized : \n            coeffs /= radixes \n            return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) \n        coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs /= radixes \n        base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        zero_correction = tf . random . uniform ( [ dim , 1 ] , seed = stream ( ) , dtype = dtype ) \n        zero_correction /= radixes ** max_sizes_by_axes \n        return base_values + tf . reshape ( zero_correction , [ - 1 ] ) "}
{"669": "\ndef _primes_less_than ( n ) : \n    small_primes = np . array ( ( 2 , 3 , 5 ) ) \n    if n <= 6 : \n        return small_primes [ small_primes < n ] \n    sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) \n    sieve [ 0 ] = 0 \n    m = int ( n ** 0.5 ) // 3 + 1 \n    for i in range ( m ) : \n        if not sieve [ i ] : \n            continue \n        k = 3 * i + 1 | 1 \n        sieve [ k ** 2 // 3 : : 2 * k ] = 0 \n        sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = 0 \n    return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ] "}
{"678": "\ndef quadrature_scheme_softmaxnormal_gauss_hermite ( normal_loc , normal_scale , quadrature_size , validate_args = 0 , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_softmaxnormal_gauss_hermite\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        npdt = dtype_util . as_numpy_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = npdt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs /= np . linalg . norm ( probs , ord = 1 , keepdims = 1 ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = npdt ) \n        grid = softmax ( - distribution_util . pad ( ( normal_loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * normal_scale [ ... , tf . newaxis ] * grid ) , axis = - 2 , front = 1 ) , axis = - 2 ) \n        return grid , probs "}
{"679": "\ndef quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = 0 , name = None ) : \n    with tf . name_scope ( name or \"softmax_normal_grid_and_probs\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        dt = dtype_util . base_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) \n        def _get_batch_ndims ( ) : \n            ndims = tensorshape_util . rank ( dist . batch_shape ) \n            if ndims is None : \n                ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] \n            return ndims \n        batch_ndims = _get_batch_ndims ( ) \n        def _get_final_shape ( qs ) : \n            bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , 1 ) \n            num_components = tf . compat . dimension_value ( bs [ - 1 ] ) \n            if num_components is not None : \n                num_components += 1 \n            tail = tf . TensorShape ( [ num_components , qs ] ) \n            return bs [ : - 1 ] . concatenate ( tail ) \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + 1 ) ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"691": "\ndef posterior_marginals ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_marginals\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                log_transition = self . _log_trans \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_adjoint_prob = tf . zeros_like ( log_init ) \n                def forward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_vector_matrix ( log_previous_step , log_transition ) + log_prob_observation \n                log_prob = log_init + observation_log_probs [ 0 ] \n                forward_log_probs = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = log_prob , name = \"forward_log_probs\" ) \n                forward_log_probs = tf . concat ( [ [ log_prob ] , forward_log_probs ] , axis = 0 ) \n                def backward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_matrix_vector ( log_transition , log_prob_observation + log_previous_step ) \n                backward_log_adjoint_probs = tf . scan ( backward_step , observation_log_probs [ 1 : ] , initializer = log_adjoint_prob , reverse = 1 , name = \"backward_log_adjoint_probs\" ) \n                total_log_prob = tf . reduce_logsumexp ( input_tensor = forward_log_probs [ - 1 ] , axis = - 1 ) \n                backward_log_adjoint_probs = tf . concat ( [ backward_log_adjoint_probs , [ log_adjoint_prob ] ] , axis = 0 ) \n                log_likelihoods = forward_log_probs + backward_log_adjoint_probs \n                marginal_log_probs = distribution_util . move_dimension ( log_likelihoods - total_log_prob [ ... , tf . newaxis ] , 0 , - 2 ) \n                return categorical . Categorical ( logits = marginal_log_probs ) "}
{"692": "\ndef posterior_mode ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_mode\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_prob = log_init + observation_log_probs [ 0 ] \n                if self . _num_steps == 1 : \n                    most_likely_end = tf . argmax ( input = log_prob , axis = - 1 ) \n                    return most_likely_end [ ... , tf . newaxis ] \n                def forward_step ( previous_step_pair , log_prob_observation ) : \n                    log_prob_previous = previous_step_pair [ 0 ] \n                    log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) \n                    most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2 ) \n                    max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) \n                    return ( max_log_p_given_successor , most_likely_given_successor ) \n                forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) \n                most_likely_end = tf . argmax ( input = forward_log_probs [ - 1 ] , axis = - 1 ) \n                def backward_step ( most_likely_successor , most_likely_given_successor ) : \n                    return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) \n                backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = 1 ) \n                most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) \n                return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 ) "}
{"693": "\ndef _choose_random_direction ( current_state_parts , batch_rank , seed = None ) : \n    seed_gen = distributions . SeedStream ( seed , salt = '_choose_random_direction' ) \n    rnd_direction_parts = [ tf . random . normal ( tf . shape ( input = current_state_part ) , dtype = tf . float32 , seed = seed_gen ( ) ) for current_state_part in current_state_parts ] \n    sum_squares = sum ( tf . reduce_sum ( input_tensor = rnd_direction ** 2. , axis = tf . range ( batch_rank , tf . rank ( rnd_direction ) ) , keepdims = 1 ) for rnd_direction in rnd_direction_parts ) \n    rnd_direction_parts = [ rnd_direction / tf . sqrt ( sum_squares ) for rnd_direction in rnd_direction_parts ] \n    return rnd_direction_parts "}
{"697": "\ndef one_step ( self , current_state , previous_kernel_results ) : \n    with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'slice' , 'one_step' ) , values = [ self . step_size , self . max_doublings , self . _seed_stream , current_state , previous_kernel_results . target_log_prob ] ) : \n        with tf . compat . v1 . name_scope ( 'initialize' ) : \n            [ current_state_parts , step_sizes , current_target_log_prob ] = _prepare_args ( self . target_log_prob_fn , current_state , self . step_size , previous_kernel_results . target_log_prob , maybe_expand = 1 ) \n            max_doublings = tf . convert_to_tensor ( value = self . max_doublings , dtype = tf . int32 , name = 'max_doublings' ) \n        independent_chain_ndims = distribution_util . prefer_static_rank ( current_target_log_prob ) \n        [ next_state_parts , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] = _sample_next ( self . target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , independent_chain_ndims , seed = self . _seed_stream ( ) ) \n        def maybe_flatten ( x ) : \n            return x if mcmc_util . is_list_like ( current_state ) else x [ 0 ] \n        return [ maybe_flatten ( next_state_parts ) , SliceSamplerKernelResults ( target_log_prob = next_target_log_prob , bounds_satisfied = bounds_satisfied , direction = direction , upper_bounds = upper_bounds , lower_bounds = lower_bounds ) , ] "}
{"698": "\ndef _build_trainable_posterior ( param , initial_loc_fn ) : \n    loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = 1 ) \n    scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = 1 ) ) \n    q = tfd . Normal ( loc = loc , scale = scale ) \n    if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : \n        q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) \n    return tfd . TransformedDistribution ( q , param . bijector ) "}
{"699": "\ndef build_factored_variational_loss ( model , observed_time_series , init_batch_shape = ( ) , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'build_factored_variational_loss' , values = [ observed_time_series ] ) as name : \n        seed = tfd . SeedStream ( seed , salt = 'StructuralTimeSeries_build_factored_variational_loss' ) \n        variational_distributions = collections . OrderedDict ( ) \n        variational_samples = [ ] \n        for param in model . parameters : \n            def initial_loc_fn ( param ) : \n                return sample_uniform_initial_state ( param , return_constrained = 1 , init_sample_shape = init_batch_shape , seed = seed ( ) ) \n            q = _build_trainable_posterior ( param , initial_loc_fn = initial_loc_fn ) \n            variational_distributions [ param . name ] = q \n            variational_samples . append ( q . sample ( seed = seed ( ) ) ) \n        observed_time_series = sts_util . pad_batch_dimension_for_multiple_chains ( observed_time_series , model , chain_batch_shape = init_batch_shape ) \n        log_prob_fn = model . joint_log_prob ( observed_time_series ) \n        expected_log_joint = log_prob_fn ( * variational_samples ) \n        entropy = tf . reduce_sum ( input_tensor = [ - q . log_prob ( sample ) for ( q , sample ) in zip ( variational_distributions . values ( ) , variational_samples ) ] , axis = 0 ) \n        variational_loss = - ( expected_log_joint + entropy ) \n    return variational_loss , variational_distributions "}
{"700": "\ndef _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : \n    optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer \n    def train_loop_body ( step ) : \n        train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) \n        return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) \n    minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : step < num_steps , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = 1 ) [ 0 ] \n    return minimize_op "}
{"704": "\ndef factored_joint_mvn ( distributions ) : \n    graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] \n    with tf . compat . v1 . name_scope ( 'factored_joint_mvn' , values = graph_parents ) : \n        dtype = tf . debugging . assert_same_float_dtype ( distributions ) \n        broadcast_ones = tf . ones ( broadcast_batch_shape ( distributions ) , dtype = dtype ) [ ... , tf . newaxis ] \n        return MultivariateNormalLinearOperator ( loc = tf . concat ( [ mvn . mean ( ) * broadcast_ones for mvn in distributions ] , axis = - 1 ) , scale = tfl . LinearOperatorBlockDiag ( [ mvn . scale for mvn in distributions ] , is_square = 1 ) ) "}
{"722": "\ndef visualize_qualitative_analysis ( inputs , model , samples = 1 , batch_size = 3 , length = 8 ) : \n    average = lambda dist : tf . reduce_mean ( input_tensor = dist . mean ( ) , axis = 0 ) \n    with tf . compat . v1 . name_scope ( \"val_reconstruction\" ) : \n        reconstruct = functools . partial ( model . reconstruct , inputs = inputs , samples = samples ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( ) ) ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( sample_static = 1 ) ) , name = \"static_prior\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( sample_dynamic = 1 ) ) , name = \"dynamic_prior\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( swap_static = 1 ) ) , name = \"swap_static\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( swap_dynamic = 1 ) ) , name = \"swap_dynamic\" ) \n    with tf . compat . v1 . name_scope ( \"generation\" ) : \n        generate = functools . partial ( model . generate , batch_size = batch_size , length = length , samples = samples ) \n        image_summary ( average ( generate ( fix_static = 1 ) ) , \"fix_static\" ) \n        image_summary ( average ( generate ( fix_dynamic = 1 ) ) , \"fix_dynamic\" ) "}
{"729": "\ndef generate ( self , batch_size , length , samples = 1 , fix_static = 0 , fix_dynamic = 0 ) : \n    static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) \n    dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) \n    likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) \n    return likelihood "}
{"730": "\ndef reconstruct ( self , inputs , samples = 1 , sample_static = 0 , sample_dynamic = 0 , swap_static = 0 , swap_dynamic = 0 , fix_static = 0 , fix_dynamic = 0 ) : \n    batch_size = tf . shape ( input = inputs ) [ - 5 ] \n    length = len ( tf . unstack ( inputs , axis = - 4 ) ) \n    features = self . compressor ( inputs ) \n    if sample_static : \n        static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) \n    else : \n        static_sample , _ = self . sample_static_posterior ( features , samples ) \n    if swap_static : \n        static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) \n    if sample_dynamic : \n        dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) \n    else : \n        dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) \n    if swap_dynamic : \n        dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) \n    likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) \n    return likelihood "}
{"731": "\ndef sample_static_prior ( self , samples , batch_size , fixed = 0 ) : \n    dist = self . static_prior ( ) \n    if fixed : \n        sample = dist . sample ( ( samples , 1 ) ) + tf . zeros ( [ batch_size , 1 ] ) \n    else : \n        sample = dist . sample ( ( samples , batch_size ) ) \n    return sample , dist "}
{"732": "\ndef sample_dynamic_prior ( self , samples , batch_size , length , fixed = 0 ) : \n    if fixed : \n        sample_batch_size = 1 \n    else : \n        sample_batch_size = batch_size \n    sample , state = self . dynamic_prior . zero_state ( [ samples , sample_batch_size ] ) \n    locs = [ ] \n    scale_diags = [ ] \n    sample_list = [ ] \n    for _ in range ( length ) : \n        dist , state = self . dynamic_prior ( sample , state ) \n        sample = dist . sample ( ) \n        locs . append ( dist . parameters [ \"loc\" ] ) \n        scale_diags . append ( dist . parameters [ \"scale_diag\" ] ) \n        sample_list . append ( sample ) \n    sample = tf . stack ( sample_list , axis = 2 ) \n    loc = tf . stack ( locs , axis = 2 ) \n    scale_diag = tf . stack ( scale_diags , axis = 2 ) \n    if fixed : \n        sample = sample + tf . zeros ( [ batch_size , 1 , 1 ] ) \n    return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) "}
{"737": "\ndef _compute_min_event_ndims ( bijector_list , compute_forward = 1 ) : \n    min_event_ndims = 0 \n    rank_changed_adjusted_max_min_event_ndims = 0 \n    if compute_forward : \n        bijector_list = reversed ( bijector_list ) \n    for b in bijector_list : \n        if compute_forward : \n            current_min_event_ndims = b . forward_min_event_ndims \n            current_inverse_min_event_ndims = b . inverse_min_event_ndims \n        else : \n            current_min_event_ndims = b . inverse_min_event_ndims \n            current_inverse_min_event_ndims = b . forward_min_event_ndims \n        if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims : \n            min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) \n        number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions \n    return min_event_ndims "}
{"739": "\ndef _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = 0 , name = None ) : \n    if direction == 'ASCENDING' : \n        pass \n    elif direction == 'DESCENDING' : \n        values = np . negative ( values ) \n    else : \n        raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' ) "}
{"740": "\ndef _sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = 0 , name = None ) : \n    if direction == 'ASCENDING' : \n        pass \n    elif direction == 'DESCENDING' : \n        values = np . negative ( values ) \n    else : \n        raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) \n    if direction == 'DESCENDING' : \n        return np . negative ( result ) \n    return result "}
{"749": "\ndef benchmark_text_messages_hmc ( num_results = int ( 3e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 ) : \n    if not tf . executing_eagerly ( ) : \n        tf . compat . v1 . reset_default_graph ( ) \n    count_data = tf . cast ( tf . concat ( [ tfd . Poisson ( rate = 15. ) . sample ( 43 ) , tfd . Poisson ( rate = 25. ) . sample ( 31 ) ] , axis = 0 ) , dtype = tf . float32 ) \n    if tf . executing_eagerly ( ) : \n        count_data = count_data . numpy ( ) \n    else : \n        with tf . compat . v1 . Session ( ) : \n            count_data = count_data . eval ( ) \n    def unnormalized_log_posterior ( lambda1 , lambda2 , tau ) : \n        return text_messages_joint_log_prob ( count_data , lambda1 , lambda2 , tau ) \n    if tf . executing_eagerly ( ) : \n        sample_chain = tf . function ( tfp . mcmc . sample_chain ) \n    else : \n        sample_chain = tfp . mcmc . sample_chain \n    step_size = tf . compat . v2 . Variable ( name = 'step_size' , initial_value = tf . constant ( 0.05 , dtype = tf . float32 ) , trainable = 0 ) \n    def computation ( ) : \n        initial_chain_state = [ tf . constant ( count_data . mean ( ) , name = 'init_lambda1' ) , tf . constant ( count_data . mean ( ) , name = 'init_lambda2' ) , tf . constant ( 0.5 , name = 'init_tau' ) , ] \n        unconstraining_bijectors = [ tfp . bijectors . Exp ( ) , tfp . bijectors . Exp ( ) , tfp . bijectors . Sigmoid ( ) , ] \n        _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = initial_chain_state , kernel = tfp . mcmc . TransformedTransitionKernel ( inner_kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_log_posterior , num_leapfrog_steps = num_leapfrog_steps , step_size = step_size , step_size_update_fn = tfp . mcmc . make_simple_step_size_update_policy ( num_burnin_steps ) , state_gradients_are_stopped = 1 ) , bijector = unconstraining_bijectors ) ) \n        return kernel_results . inner_results . is_accepted \n    is_accepted_tensor = computation ( ) \n    if not tf . executing_eagerly ( ) : \n        session = tf . compat . v1 . Session ( ) \n        session . run ( tf . compat . v1 . global_variables_initializer ( ) ) \n        session . run ( is_accepted_tensor ) \n    start_time = time . time ( ) \n    if tf . executing_eagerly ( ) : \n        is_accepted = computation ( ) \n    else : \n        is_accepted = session . run ( is_accepted_tensor ) \n    wall_time = time . time ( ) - start_time \n    num_accepted = np . sum ( is_accepted ) \n    acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) \n    return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time ) "}
{"751": "\ndef get_marginal_distribution ( self , index_points = None ) : \n    with self . _name_scope ( 'get_marginal_distribution' ) : \n        index_points = self . _get_index_points ( index_points ) \n        covariance = self . _compute_covariance ( index_points ) \n        loc = self . _mean_fn ( index_points ) \n        if self . _is_univariate_marginal ( index_points ) : \n            scale = tf . sqrt ( covariance ) \n            loc = tf . squeeze ( loc , axis = - 1 ) \n            return normal . Normal ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' ) \n        else : \n            scale = tf . linalg . LinearOperatorLowerTriangular ( tf . linalg . cholesky ( _add_diagonal_shift ( covariance , self . jitter ) ) , is_non_singular = 1 , name = 'GaussianProcessScaleLinearOperator' ) \n            return mvn_linear_operator . MultivariateNormalLinearOperator ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' ) "}
{"755": "\ndef bootstrap_results ( self , state ) : \n    def loss ( ) : \n        q = self . _flattened_variational_distribution ( ) \n        samples = q . sample ( self . train_batch_size ) \n        return tf . reduce_mean ( input_tensor = q . log_prob ( samples ) - self . _flattened_target_log_prob ( samples ) , axis = - 1 ) \n    lr = tf . convert_to_tensor ( value = self . learning_rate , dtype = self . _dtype ) \n    dtype = lr . dtype \n    learning_rate = tf . compat . v2 . optimizers . schedules . PiecewiseConstantDecay ( list ( self . num_train_steps * np . array ( [ 0.2 , 0.8 ] ) . astype ( dtype . as_numpy_dtype ( ) ) ) , [ lr , lr * 0.1 , lr * 0.01 ] ) \n    opt = tf . compat . v2 . optimizers . Adam ( learning_rate ) \n    \n    @ tf . function ( autograph = 0 ) \n    def train_step ( ) : \n        with tf . GradientTape ( ) as tape : \n            loss_val = loss ( ) \n        vals = tape . watched_variables ( ) \n        grads = tape . gradient ( loss_val , vals ) \n        grads_and_vals = list ( zip ( grads , vals ) ) \n        opt . apply_gradients ( grads_and_vals ) \n        return loss_val \n    for step in range ( self . num_train_steps ) : \n        loss_val = train_step ( ) \n        tf . debugging . assert_all_finite ( loss_val , 'NeuTra loss is NaN at step {}' . format ( step ) ) \n        if self . train_debug_fn : \n            self . train_debug_fn ( self , step , loss_val ) \n    state_parts = tf . nest . flatten ( state ) \n    flat_state_shapes = tf . nest . flatten ( self . state_shape ) \n    batch_shape = tf . shape ( input = state_parts [ 0 ] ) [ : - flat_state_shapes [ 0 ] . ndims ] \n    return self . _kernel . bootstrap_results ( self . _flattened_variational_distribution ( ) . sample ( batch_shape , seed = self . seed ) ) "}
{"759": "\ndef _distributional_transform ( self , x ) : \n    if tensorshape_util . rank ( x . shape ) is None : \n        raise ValueError ( \"Distributional transform does not support inputs of \" \"undefined rank.\" ) \n    if isinstance ( self . _components_distribution , independent . Independent ) : \n        univariate_components = self . _components_distribution . distribution \n    else : \n        univariate_components = self . _components_distribution \n    with tf . control_dependencies ( [ assert_util . assert_equal ( univariate_components . is_scalar_event ( ) , 1 , message = \"`univariate_components` must have scalar event\" ) ] ) : \n        x_padded = self . _pad_sample_dims ( x ) \n        log_prob_x = univariate_components . log_prob ( x_padded ) \n        cdf_x = univariate_components . cdf ( x_padded ) \n        cumsum_log_prob_x = tf . reshape ( tf . math . cumsum ( tf . reshape ( log_prob_x , [ - 1 , self . _event_size ] ) , exclusive = 1 , axis = - 1 ) , tf . shape ( input = log_prob_x ) ) \n        logits_mix_prob = distribution_utils . pad_mixture_dimensions ( self . mixture_distribution . logits , self , self . mixture_distribution , self . _event_ndims ) \n        log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x \n        component_axis = tensorshape_util . rank ( x . shape ) - self . _event_ndims \n        posterior_weights_x = tf . nn . softmax ( log_posterior_weights_x , axis = component_axis ) \n        return tf . reduce_sum ( input_tensor = posterior_weights_x * cdf_x , axis = component_axis ) "}
{"766": "\ndef _numpy_text ( tensor , is_repr = 0 ) : \n    if tensor . dtype . is_numpy_compatible : \n        text = repr ( tensor . numpy ( ) ) if is_repr else str ( tensor . numpy ( ) ) \n    else : \n        text = \"<unprintable>\" \n    if \"\\n\" in text : \n        text = \"\\n\" + text \n    return text "}
{"773": "\ndef real_nvp_default_template ( hidden_layers , shift_only = 0 , activation = tf . nn . relu , name = None , * args , ** kwargs ) : \n    with tf . compat . v2 . name_scope ( name or \"real_nvp_default_template\" ) : \n        def _fn ( x , output_units , ** condition_kwargs ) : \n            if condition_kwargs : \n                raise NotImplementedError ( \"Conditioning not implemented in the default template.\" ) \n            if tensorshape_util . rank ( x . shape ) == 1 : \n                x = x [ tf . newaxis , ... ] \n                reshape_output = lambda x : x [ 0 ] \n            else : \n                reshape_output = lambda x : x \n            for units in hidden_layers : \n                x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , ** kwargs ) \n            x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2 ) * output_units , activation = None , * args , ** kwargs ) \n            if shift_only : \n                return reshape_output ( x ) , None \n            shift , log_scale = tf . split ( x , 2 , axis = - 1 ) \n            return reshape_output ( shift ) , reshape_output ( log_scale ) \n        return tf . compat . v1 . make_template ( \"real_nvp_default_template\" , _fn ) "}
{"783": "\ndef _potential_scale_reduction_single_state ( state , independent_chain_ndims ) : \n    with tf . compat . v1 . name_scope ( 'potential_scale_reduction_single_state' , values = [ state , independent_chain_ndims ] ) : \n        state = tf . convert_to_tensor ( value = state , name = 'state' ) \n        sample_ndims = 1 \n        sample_axis = tf . range ( 0 , sample_ndims ) \n        chain_axis = tf . range ( sample_ndims , sample_ndims + independent_chain_ndims ) \n        sample_and_chain_axis = tf . range ( 0 , sample_ndims + independent_chain_ndims ) \n        n = _axis_size ( state , sample_axis ) \n        m = _axis_size ( state , chain_axis ) \n        b_div_n = _reduce_variance ( tf . reduce_mean ( input_tensor = state , axis = sample_axis , keepdims = 1 ) , sample_and_chain_axis , biased = 0 ) \n        w = tf . reduce_mean ( input_tensor = _reduce_variance ( state , sample_axis , keepdims = 1 , biased = 1 ) , axis = sample_and_chain_axis ) \n        sigma_2_plus = w + b_div_n \n        return ( ( m + 1. ) / m ) * sigma_2_plus / w - ( n - 1. ) / ( m * n ) "}
{"786": "\ndef quadrature_scheme_lognormal_gauss_hermite ( loc , scale , quadrature_size , validate_args = 0 , name = None ) : \n    with tf . name_scope ( name or \"vector_diffeomixture_quadrature_gauss_hermite\" ) : \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        npdt = dtype_util . as_numpy_dtype ( loc . dtype ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs /= np . linalg . norm ( probs , ord = 1 , keepdims = 1 ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = loc . dtype ) \n        grid = ( loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * scale [ ... , tf . newaxis ] * grid ) \n        return grid , probs "}
{"787": "\ndef quadrature_scheme_lognormal_quantiles ( loc , scale , quadrature_size , validate_args = 0 , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_lognormal_quantiles\" ) : \n        dist = transformed_distribution . TransformedDistribution ( distribution = normal . Normal ( loc = loc , scale = scale ) , bijector = exp_bijector . Exp ( ) , validate_args = validate_args ) \n        batch_ndims = tensorshape_util . rank ( dist . batch_shape ) \n        if batch_ndims is None : \n            batch_ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        new_shape = tensorshape_util . concatenate ( dist . batch_shape , [ quadrature_size ] ) \n        tensorshape_util . set_shape ( grid , new_shape ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"788": "\ndef merge ( self , x = None , y = None , ildj = None , kwargs = None , mapping = None ) : \n    if mapping is None : \n        mapping = _Mapping ( x = x , y = y , ildj = ildj , kwargs = kwargs ) \n    elif any ( arg is not None for arg in [ x , y , ildj , kwargs ] ) : \n        raise ValueError ( \"Cannot simultaneously specify mapping and individual \" \"arguments.\" ) \n    return _Mapping ( x = self . _merge ( self . x , mapping . x ) , y = self . _merge ( self . y , mapping . y ) , ildj = self . _merge ( self . ildj , mapping . ildj ) , kwargs = self . _merge ( self . kwargs , mapping . kwargs , use_equals = 1 ) ) "}
{"790": "\ndef _merge ( self , old , new , use_equals = 0 ) : \n    if old is None : \n        return new \n    if new is None : \n        return old \n    if ( old == new ) if use_equals else ( old is new ) : \n        return old \n    raise ValueError ( \"Incompatible values: %s != %s\" % ( old , new ) ) "}
{"792": "\ndef _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : \n        step_size = tf . convert_to_tensor ( value = step_size ) \n        dtype = step_size . dtype . base_dtype \n        output_shape = tf . concat ( ( [ max_doublings + 1 ] , batch_shape ) , axis = 0 ) \n        expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . sample ( sample_shape = output_shape , seed = seed ) \n        width_multipliers = tf . cast ( 2 ** tf . range ( 0 , max_doublings + 1 ) , dtype = dtype ) \n        widths_shape = tf . concat ( ( [ max_doublings + 1 ] , tf . ones_like ( batch_shape ) ) , axis = 0 ) \n        width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) \n        widths = width_multipliers * step_size \n        left_increments = tf . cumsum ( widths * expand_left , exclusive = 1 , axis = 0 ) \n        return left_increments , widths "}
{"807": "\ndef default_loc_scale_fn ( is_singular = 0 , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : \n    def _fn ( dtype , shape , name , trainable , add_variable_fn ) : \n        loc = add_variable_fn ( name = name + '_loc' , shape = shape , initializer = loc_initializer , regularizer = loc_regularizer , constraint = loc_constraint , dtype = dtype , trainable = trainable ) \n        if is_singular : \n            return loc , None \n        untransformed_scale = add_variable_fn ( name = name + '_untransformed_scale' , shape = shape , initializer = untransformed_scale_initializer , regularizer = untransformed_scale_regularizer , constraint = untransformed_scale_constraint , dtype = dtype , trainable = trainable ) \n        scale = ( np . finfo ( dtype . as_numpy_dtype ) . eps + tf . nn . softplus ( untransformed_scale ) ) \n        return loc , scale \n    return _fn "}
{"808": "\ndef default_mean_field_normal_fn ( is_singular = 0 , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : \n    loc_scale_fn = default_loc_scale_fn ( is_singular = is_singular , loc_initializer = loc_initializer , untransformed_scale_initializer = untransformed_scale_initializer , loc_regularizer = loc_regularizer , untransformed_scale_regularizer = untransformed_scale_regularizer , loc_constraint = loc_constraint , untransformed_scale_constraint = untransformed_scale_constraint ) \n    def _fn ( dtype , shape , name , trainable , add_variable_fn ) : \n        loc , scale = loc_scale_fn ( dtype , shape , name , trainable , add_variable_fn ) \n        if scale is None : \n            dist = tfd . Deterministic ( loc = loc ) \n        else : \n            dist = tfd . Normal ( loc = loc , scale = scale ) \n        batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) \n        return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) \n    return _fn "}
{"837": "\ndef _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : \n    ds = [ ] \n    values_out = [ ] \n    seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) \n    gen = self . _model ( ) \n    index = 0 \n    d = next ( gen ) \n    try : \n        while 1 : \n            actual_distribution = d . distribution if isinstance ( d , self . Root ) else d \n            ds . append ( actual_distribution ) \n            if ( value is not None and len ( value ) > index and value [ index ] is not None ) : \n                seed ( ) \n                next_value = value [ index ] \n            else : \n                next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) \n            values_out . append ( next_value ) \n            index += 1 \n            d = gen . send ( next_value ) \n    except StopIteration : \n        pass \n    return ds , values_out "}
{"841": "\ndef newsgroups_dataset ( directory , split_name , num_words , shuffle_and_repeat ) : \n    data = np . load ( download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) ) \n    data = data [ : - 1 ] \n    num_documents = data . shape [ 0 ] \n    indices = np . array ( [ ( row_idx , column_idx ) for row_idx , row in enumerate ( data ) for column_idx in row ] ) \n    sparse_matrix = scipy . sparse . coo_matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num_documents , num_words ) , dtype = np . float32 ) \n    sparse_matrix = sparse_matrix . tocsr ( ) \n    dataset = tf . data . Dataset . range ( num_documents ) \n    if shuffle_and_repeat : \n        dataset = dataset . shuffle ( num_documents ) . repeat ( ) \n    def get_row_py_func ( idx ) : \n        def get_row_python ( idx_py ) : \n            return np . squeeze ( np . array ( sparse_matrix [ idx_py ] . todense ( ) ) , axis = 0 ) \n        py_func = tf . compat . v1 . py_func ( get_row_python , [ idx ] , tf . float32 , stateful = 0 ) \n        py_func . set_shape ( ( num_words , ) ) \n        return py_func \n    dataset = dataset . map ( get_row_py_func ) \n    return dataset "}
{"843": "\ndef build_input_fns ( data_dir , batch_size ) : \n    with open ( download ( data_dir , \"vocab.pkl\" ) , \"r\" ) as f : \n        words_to_idx = pickle . load ( f ) \n    num_words = len ( words_to_idx ) \n    vocabulary = [ None ] * num_words \n    for word , idx in words_to_idx . items ( ) : \n        vocabulary [ idx ] = word \n    def train_input_fn ( ) : \n        dataset = newsgroups_dataset ( data_dir , \"train\" , num_words , shuffle_and_repeat = 1 ) \n        dataset = dataset . batch ( batch_size ) . prefetch ( 32 ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    def eval_input_fn ( ) : \n        dataset = newsgroups_dataset ( data_dir , \"test\" , num_words , shuffle_and_repeat = 0 ) \n        dataset = dataset . batch ( batch_size ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    return train_input_fn , eval_input_fn , vocabulary "}
{"845": "\ndef add_ema_control_dependencies ( vector_quantizer , one_hot_assignments , codes , commitment_loss , decay ) : \n    updated_ema_count = moving_averages . assign_moving_average ( vector_quantizer . ema_count , tf . reduce_sum ( input_tensor = one_hot_assignments , axis = [ 0 , 1 ] ) , decay , zero_debias = 0 ) \n    updated_ema_means = moving_averages . assign_moving_average ( vector_quantizer . ema_means , tf . reduce_sum ( input_tensor = tf . expand_dims ( codes , 2 ) * tf . expand_dims ( one_hot_assignments , 3 ) , axis = [ 0 , 1 ] ) , decay , zero_debias = 0 ) \n    perturbed_ema_count = updated_ema_count + 1e-5 \n    with tf . control_dependencies ( [ commitment_loss ] ) : \n        update_means = tf . compat . v1 . assign ( vector_quantizer . codebook , updated_ema_means / perturbed_ema_count [ ... , tf . newaxis ] ) \n        with tf . control_dependencies ( [ update_means ] ) : \n            return tf . identity ( commitment_loss ) "}
{"846": "\ndef save_imgs ( x , fname ) : \n    n = x . shape [ 0 ] \n    fig = figure . Figure ( figsize = ( n , 1 ) , frameon = 0 ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( 1 , n , i + 1 ) \n        ax . imshow ( x [ i ] . squeeze ( ) , interpolation = \"none\" , cmap = cm . get_cmap ( \"binary\" ) ) \n        ax . axis ( \"off\" ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved %s\" % fname ) "}
{"856": "\ndef _assert_same_base_type ( items , expected_type = None ) : \n    original_expected_type = expected_type \n    mismatch = 0 \n    for item in items : \n        if item is not None : \n            item_type = base_dtype ( item . dtype ) \n            if not expected_type : \n                expected_type = item_type \n            elif expected_type != item_type : \n                mismatch = 1 \n                break \n    if mismatch : \n        expected_type = original_expected_type \n        original_item_str = None \n        get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) \n        for item in items : \n            if item is not None : \n                item_type = base_dtype ( item . dtype ) \n                if not expected_type : \n                    expected_type = item_type \n                    original_item_str = get_name ( item ) \n                elif expected_type != item_type : \n                    raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) \n        return expected_type \n    else : \n        return expected_type "}
{"858": "\ndef minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = 0 , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = 1 , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : \n        ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) \n        domain_dtype = simplex . dtype \n        ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) \n        closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) \n        def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : \n            ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , ** closure_kwargs ) \n            return ( converged , iterations + 1 , next_simplex , next_objective , num_evaluations + evaluations ) \n        initial_args = ( 0 , 0 , simplex , objective_at_simplex , num_evaluations ) \n        def _is_converged ( converged , num_iterations , * ignored_args ) : \n            not_converged = tf . logical_not ( converged ) \n            return ( not_converged if max_iterations is None else ( not_converged & ( num_iterations < max_iterations ) ) ) \n        ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) \n        order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = 1 ) \n        best_index = order [ 0 ] \n        return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = 0 , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = 1 ) \n        ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex \n        face_centroid /= tf . cast ( dim , domain_dtype ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = 1 \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return ( 1 , current_simplex , current_objective_values , 0 ) \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = objective_at_reflected < best_objective_value \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = 0 ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) "}
{"860": "\ndef _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , objective_at_reflected ) : \n    def _replace_worst_with_reflected ( ) : \n        next_simplex = _replace_at_index ( simplex , worst_index , reflected ) \n        next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) \n        return 0 , next_simplex , next_objective_values , 0 \n    return _replace_worst_with_reflected "}
{"861": "\ndef _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : \n    def _expand_and_maybe_replace ( ) : \n        expanded = face_centroid + expansion * ( reflected - face_centroid ) \n        expanded_objective_value = objective_function ( expanded ) \n        expanded_is_better = ( expanded_objective_value < objective_at_reflected ) \n        accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) \n        accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) \n        next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) \n        next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) \n        next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) \n        return 0 , next_simplex , next_objective_at_simplex , 1 \n    return _expand_and_maybe_replace "}
{"862": "\ndef _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : \n    def _contraction ( ) : \n        contracted = face_centroid + contraction * ( reflected - face_centroid ) \n        objective_at_contracted = objective_function ( contracted ) \n        is_contracted_acceptable = objective_at_contracted <= objective_at_reflected \n        def _accept_contraction ( ) : \n            next_simplex = _replace_at_index ( simplex , worst_index , contracted ) \n            objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) \n            return ( 0 , next_simplex , objective_at_next_simplex , 1 ) \n        def _reject_contraction ( ) : \n            return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) \n        return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) \n    return _contraction "}
{"863": "\ndef _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : \n    best_vertex = simplex [ best_index ] \n    shrunk_simplex = best_vertex + shrinkage * ( simplex - best_vertex ) \n    objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) \n    return ( 0 , shrunk_simplex , objective_at_shrunk_simplex , evals ) "}
{"874": "\ndef from_config ( cls , config ) : \n    return cls ( ** { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate_args' : config . get ( 'validate_args' , 0 ) , } ) "}
{"875": "\ndef _matmul ( a , b , transpose_a = 0 , transpose_b = 0 , adjoint_a = 0 , adjoint_b = 0 , a_is_sparse = 0 , b_is_sparse = 0 , name = None ) : \n    if a_is_sparse or b_is_sparse : \n        raise NotImplementedError ( 'Numpy backend does not support sparse matmul.' ) \n    if transpose_a or adjoint_a : \n        a = _matrix_transpose ( a , conjugate = adjoint_a ) \n    if transpose_b or adjoint_b : \n        b = _matrix_transpose ( b , conjugate = adjoint_b ) \n    return np . matmul ( a , b ) "}
{"882": "\ndef _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : \n    x = self . _maybe_rotate_dims ( x , rotate_right = 1 ) \n    log_prob = self . distribution . log_prob ( x , ** distribution_kwargs ) \n    if self . _is_maybe_event_override : \n        log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) \n    log_prob += tf . cast ( ildj , log_prob . dtype ) \n    if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : \n        tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) \n    return log_prob "}
{"883": "\ndef _finish_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : \n    x = self . _maybe_rotate_dims ( x , rotate_right = 1 ) \n    prob = self . distribution . prob ( x , ** distribution_kwargs ) \n    if self . _is_maybe_event_override : \n        prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) \n    prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) \n    if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : \n        tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) \n    return prob "}
{"884": "\ndef _maybe_rotate_dims ( self , x , rotate_right = 0 ) : \n    needs_rotation_const = tf . get_static_value ( self . _needs_rotation ) \n    if needs_rotation_const is not None and not needs_rotation_const : \n        return x \n    ndims = prefer_static . rank ( x ) \n    n = ( ndims - self . _rotate_ndims ) if rotate_right else self . _rotate_ndims \n    perm = prefer_static . concat ( [ prefer_static . range ( n , ndims ) , prefer_static . range ( 0 , n ) ] , axis = 0 ) \n    return tf . transpose ( a = x , perm = perm ) "}
{"892": "\ndef fit ( model_matrix , response , model , model_coefficients_start = None , predicted_linear_response_start = None , l2_regularizer = None , dispersion = None , offset = None , convergence_criteria_fn = None , learning_rate = None , fast_unsafe_numerics = 1 , maximum_iterations = None , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , dispersion , offset , learning_rate , maximum_iterations ] \n    with tf . compat . v1 . name_scope ( name , 'fit' , graph_deps ) : \n        [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset , ] = prepare_args ( model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset ) \n        if convergence_criteria_fn is None : \n            convergence_criteria_fn = ( convergence_criteria_small_relative_norm_weights_change ( ) ) \n        def _body ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous ) : \n            model_coefficients_next , predicted_linear_response_next = fit_one_step ( model_matrix , response , model , model_coefficients_previous , predicted_linear_response_previous , l2_regularizer , dispersion , offset , learning_rate , fast_unsafe_numerics ) \n            is_converged_next = convergence_criteria_fn ( is_converged_previous = is_converged_previous , iter_ = iter_ , model_coefficients_previous = model_coefficients_previous , predicted_linear_response_previous = predicted_linear_response_previous , model_coefficients_next = model_coefficients_next , predicted_linear_response_next = predicted_linear_response_next , response = response , model = model , dispersion = dispersion ) \n            return [ is_converged_next , iter_ + 1 , model_coefficients_next , predicted_linear_response_next , ] \n        [ is_converged , iter_ , model_coefficients , predicted_linear_response , ] = tf . while_loop ( cond = lambda is_converged , * args : tf . logical_not ( is_converged ) , body = _body , loop_vars = [ tf . zeros ( [ ] , np . bool ) , tf . zeros ( [ ] , np . int32 ) , model_coefficients_start , predicted_linear_response_start , ] , maximum_iterations = maximum_iterations ) \n        return [ model_coefficients , predicted_linear_response , is_converged , iter_ ] "}
{"900": "\ndef case ( pred_fn_pairs , default = None , exclusive = 0 , name = 'smart_case' ) : \n    return control_flow_ops . _case_helper ( cond , pred_fn_pairs , default , exclusive , name , allow_python_preds = 1 ) "}
{"903": "\ndef make_tril_scale ( loc = None , scale_tril = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = 0 , assert_positive = 0 , name = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( tf . linalg . diag_part ( x ) , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( tf . linalg . diag_part ( x ) , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) , ] , x ) \n    with tf . name_scope ( name or \"make_tril_scale\" ) : \n        dtype = dtype_util . common_dtype ( [ loc , scale_tril , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_tril = _convert_to_tensor ( scale_tril , name = \"scale_tril\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n    if scale_tril is not None : \n        scale_tril = tf . linalg . band_part ( scale_tril , - 1 , 0 ) \n        tril_diag = tf . linalg . diag_part ( scale_tril ) \n        if scale_diag is not None : \n            tril_diag += scale_diag \n        if scale_identity_multiplier is not None : \n            tril_diag += scale_identity_multiplier [ ... , tf . newaxis ] \n        scale_tril = tf . linalg . set_diag ( scale_tril , tril_diag ) \n        return tf . linalg . LinearOperatorLowerTriangular ( tril = _maybe_attach_assertion ( scale_tril ) , is_non_singular = 1 , is_self_adjoint = 0 , is_positive_definite = assert_positive ) \n    return make_diag_scale ( loc = loc , scale_diag = scale_diag , scale_identity_multiplier = scale_identity_multiplier , shape_hint = shape_hint , validate_args = validate_args , assert_positive = assert_positive , name = name ) "}
{"904": "\ndef make_diag_scale ( loc = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = 0 , assert_positive = 0 , name = None , dtype = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( x , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( x , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) ] , x ) \n    with tf . name_scope ( name or \"make_diag_scale\" ) : \n        if dtype is None : \n            dtype = dtype_util . common_dtype ( [ loc , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n        if scale_diag is not None : \n            if scale_identity_multiplier is not None : \n                scale_diag += scale_identity_multiplier [ ... , tf . newaxis ] \n            return tf . linalg . LinearOperatorDiag ( diag = _maybe_attach_assertion ( scale_diag ) , is_non_singular = 1 , is_self_adjoint = 1 , is_positive_definite = assert_positive ) \n        if loc is None and shape_hint is None : \n            raise ValueError ( \"Cannot infer `event_shape` unless `loc` or \" \"`shape_hint` is specified.\" ) \n        num_rows = shape_hint \n        del shape_hint \n        if num_rows is None : \n            num_rows = tf . compat . dimension_value ( loc . shape [ - 1 ] ) \n            if num_rows is None : \n                num_rows = tf . shape ( input = loc ) [ - 1 ] \n        if scale_identity_multiplier is None : \n            return tf . linalg . LinearOperatorIdentity ( num_rows = num_rows , dtype = dtype , is_self_adjoint = 1 , is_positive_definite = 1 , assert_proper_shapes = validate_args ) \n        return tf . linalg . LinearOperatorScaledIdentity ( num_rows = num_rows , multiplier = _maybe_attach_assertion ( scale_identity_multiplier ) , is_non_singular = 1 , is_self_adjoint = 1 , is_positive_definite = assert_positive , assert_proper_shapes = validate_args ) "}
{"907": "\ndef maybe_check_scalar_distribution ( distribution , expected_base_dtype , validate_args ) : \n    if distribution . dtype != expected_base_dtype : \n        raise TypeError ( \"dtype mismatch; \" \"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\" . format ( dtype_util . name ( distribution . dtype ) , dtype_util . name ( expected_base_dtype ) ) ) \n    if validate_args and ( distribution . reparameterization_type != reparameterization . FULLY_REPARAMETERIZED ) : \n        raise ValueError ( \"Base distribution should be reparameterized or be \" \"a function of non-trainable variables; \" \"distribution.reparameterization_type = \\\"{}\\\" \" \"!= \\\"FULLY_REPARAMETERIZED\\\".\" . format ( distribution . reparameterization_type ) ) \n    with tf . name_scope ( \"check_distribution\" ) : \n        assertions = [ ] \n        def check_is_scalar ( is_scalar , name ) : \n            is_scalar_ = tf . get_static_value ( is_scalar ) \n            if is_scalar_ is not None : \n                if not is_scalar_ : \n                    raise ValueError ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) \n            elif validate_args : \n                assertions . append ( assert_util . assert_equal ( is_scalar , 1 , message = ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) ) ) \n        check_is_scalar ( distribution . is_scalar_event ( ) , \"is_scalar_event\" ) \n        check_is_scalar ( distribution . is_scalar_batch ( ) , \"is_scalar_batch\" ) \n        return assertions "}
{"912": "\ndef same_dynamic_shape ( a , b ) : \n    a = tf . convert_to_tensor ( value = a , name = \"a\" ) \n    b = tf . convert_to_tensor ( value = b , name = \"b\" ) \n    def all_shapes_equal ( ) : \n        return tf . reduce_all ( input_tensor = tf . equal ( tf . concat ( [ tf . shape ( input = a ) , tf . shape ( input = b ) ] , 0 ) , tf . concat ( [ tf . shape ( input = b ) , tf . shape ( input = a ) ] , 0 ) ) ) \n    return tf . cond ( pred = tf . equal ( tf . rank ( a ) , tf . rank ( b ) ) , true_fn = all_shapes_equal , false_fn = lambda : tf . constant ( 0 ) ) "}
{"914": "\ndef _is_known_unsigned_by_dtype ( dt ) : \n    return { tf . bool : 1 , tf . uint8 : 1 , tf . uint16 : 1 , } . get ( dt . base_dtype , 0 ) "}
{"915": "\ndef _is_known_signed_by_dtype ( dt ) : \n    return { tf . float16 : 1 , tf . float32 : 1 , tf . float64 : 1 , tf . int8 : 1 , tf . int16 : 1 , tf . int32 : 1 , tf . int64 : 1 , } . get ( dt . base_dtype , 0 ) "}
{"927": "\ndef process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : \n    with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : \n        if quadrature_grid_and_probs is None : \n            grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) \n            grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs /= np . linalg . norm ( probs , ord = 1 , keepdims = 1 ) \n            grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n            probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) \n            return grid , probs \n        grid , probs = tuple ( quadrature_grid_and_probs ) \n        grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) \n        probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = 1 , name = \"probs\" ) \n        def _static_event_size ( x ) : \n            return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) \n        m , n = _static_event_size ( probs ) , _static_event_size ( grid ) \n        if m is not None and n is not None : \n            if m != n : \n                raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) \n        elif validate_args : \n            assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] \n            with tf . control_dependencies ( assertions ) : \n                grid = tf . identity ( grid ) \n                probs = tf . identity ( probs ) \n        return grid , probs "}
{"929": "\ndef expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = 0 ) : \n    with tf . name_scope ( op_name or \"expand_to_vector\" ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is None : \n            if validate_args : \n                x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = \"Input is neither scalar nor vector.\" ) ] , x ) \n            ndims = tf . rank ( x ) \n            expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) \n            return tf . reshape ( x , expanded_shape ) \n        elif ndims == 0 : \n            x_const = tf . get_static_value ( x ) \n            if x_const is not None : \n                return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) \n            else : \n                return tf . reshape ( x , [ 1 ] ) \n        elif ndims != 1 : \n            raise ValueError ( \"Input is neither scalar nor vector.\" ) \n        return x "}
{"933": "\ndef _event_shape ( self , shape , static_perm_to_shape ) : \n    rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) \n    if tensorshape_util . rank ( shape ) is None or rightmost_ is None : \n        return tf . TensorShape ( None ) \n    if tensorshape_util . rank ( shape ) < rightmost_ : \n        raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) \n    perm_ = tf . get_static_value ( self . perm , partial = 1 ) \n    if perm_ is None : \n        return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) \n    if sum ( p is None for p in perm_ ) == 1 : \n        present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) \n        for i , p in enumerate ( present [ 1 : ] ) : \n            if i != p : \n                perm_ = [ i if p is None else p for p in perm_ ] \n                break \n    return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) ) "}
{"939": "\ndef _augment_sample_shape ( partial_batch_dist , full_sample_and_batch_shape , validate_args = 0 ) : \n    full_ndims = distribution_util . prefer_static_shape ( full_sample_and_batch_shape ) [ 0 ] \n    partial_batch_ndims = ( tensorshape_util . rank ( partial_batch_dist . batch_shape ) if tensorshape_util . rank ( partial_batch_dist . batch_shape ) is not None else distribution_util . prefer_static_shape ( partial_batch_dist . batch_shape_tensor ( ) ) [ 0 ] ) \n    num_broadcast_dims = full_ndims - partial_batch_ndims \n    expected_partial_batch_shape = ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    expected_partial_batch_shape_static = tf . get_static_value ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    num_broadcast_dims_static = tf . get_static_value ( num_broadcast_dims ) \n    if num_broadcast_dims_static is not None : \n        if num_broadcast_dims_static < 0 : \n            raise ValueError ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions\" . format ( partial_batch_dist ) ) \n    if ( expected_partial_batch_shape_static is not None and tensorshape_util . is_fully_defined ( partial_batch_dist . batch_shape ) ) : \n        if ( partial_batch_dist . batch_shape and any ( expected_partial_batch_shape_static != tensorshape_util . as_list ( partial_batch_dist . batch_shape ) ) ) : \n            raise NotImplementedError ( \"Broadcasting is not supported; \" \"unexpected batch shape \" \"(expected {}, saw {}).\" . format ( expected_partial_batch_shape_static , partial_batch_dist . batch_shape ) ) \n    runtime_assertions = [ ] \n    if validate_args : \n        runtime_assertions . append ( assert_util . assert_greater_equal ( tf . convert_to_tensor ( value = num_broadcast_dims , dtype = tf . int32 ) , tf . zeros ( ( ) , dtype = tf . int32 ) , message = ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions.\" . format ( partial_batch_dist ) ) ) ) \n        runtime_assertions . append ( assert_util . assert_equal ( expected_partial_batch_shape , partial_batch_dist . batch_shape_tensor ( ) , message = ( \"Broadcasting is not supported; \" \"unexpected batch shape.\" ) , name = \"assert_batch_shape_same\" ) ) \n    with tf . control_dependencies ( runtime_assertions ) : \n        return full_sample_and_batch_shape [ : num_broadcast_dims ] "}
{"941": "\ndef backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) : \n    tmp_gain_cov = transition_matrix . matmul ( filtered_cov ) \n    predicted_cov_chol = tf . linalg . cholesky ( predicted_cov ) \n    gain_transpose = tf . linalg . cholesky_solve ( predicted_cov_chol , tmp_gain_cov ) \n    posterior_mean = ( filtered_mean + tf . linalg . matmul ( gain_transpose , next_posterior_mean - predicted_mean , adjoint_a = 1 ) ) \n    posterior_cov = ( filtered_cov + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( next_posterior_cov - predicted_cov , gain_transpose ) , adjoint_a = 1 ) ) \n    return ( posterior_mean , posterior_cov ) "}
{"943": "\ndef linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : \n    observation_size_is_static_and_scalar = ( tf . compat . dimension_value ( observation_matrix . shape [ - 2 ] ) == 1 ) \n    x_expected = _propagate_mean ( prior_mean , observation_matrix , observation_noise ) \n    tmp_obs_cov = observation_matrix . matmul ( prior_cov ) \n    predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = 1 ) + observation_noise . covariance ( ) ) \n    if observation_size_is_static_and_scalar : \n        gain_transpose = tmp_obs_cov / predicted_obs_cov \n    else : \n        predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) \n        gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) \n    posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = 1 ) ) \n    tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = 1 ) \n    tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + 1 ) \n    posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = 1 ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = 1 ) ) \n    if observation_size_is_static_and_scalar : \n        predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , 0 ] , scale = tf . sqrt ( predicted_obs_cov [ ... , 0 ] ) ) , reinterpreted_batch_ndims = 1 ) \n        predictive_dist . covariance = lambda : predicted_obs_cov \n    else : \n        predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , 0 ] , scale_tril = predicted_obs_cov_chol ) \n    return posterior_mean , posterior_cov , predictive_dist "}
{"947": "\ndef build_kalman_sample_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep , full_sample_and_batch_shape , stream , validate_args = 0 ) : \n    def sample_step ( sampled_prev , t ) : \n        latent_prev , _ = sampled_prev \n        transition_matrix = get_transition_matrix_for_timestep ( t - 1 ) \n        transition_noise = get_transition_noise_for_timestep ( t - 1 ) \n        latent_pred = transition_matrix . matmul ( latent_prev ) \n        latent_sampled = latent_pred + transition_noise . sample ( sample_shape = _augment_sample_shape ( transition_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] \n        observation_matrix = get_observation_matrix_for_timestep ( t ) \n        observation_noise = get_observation_noise_for_timestep ( t ) \n        observation_pred = observation_matrix . matmul ( latent_sampled ) \n        observation_sampled = observation_pred + observation_noise . sample ( sample_shape = _augment_sample_shape ( observation_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] \n        return ( latent_sampled , observation_sampled ) \n    return sample_step "}
{"949": "\ndef _propagate_cov ( cov , linop , dist ) : \n    return linop . matmul ( linop . matmul ( cov ) , adjoint_arg = 1 ) + dist . covariance ( ) "}
{"950": "\ndef backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : \n    with tf . name_scope ( \"backward_pass\" ) : \n        filtered_means = tf . convert_to_tensor ( value = filtered_means , name = \"filtered_means\" ) \n        filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = \"filtered_covs\" ) \n        predicted_means = tf . convert_to_tensor ( value = predicted_means , name = \"predicted_means\" ) \n        predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = \"predicted_covs\" ) \n        filtered_means = distribution_util . move_dimension ( filtered_means , - 2 , 0 ) \n        filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3 , 0 ) \n        predicted_means = distribution_util . move_dimension ( predicted_means , - 2 , 0 ) \n        predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3 , 0 ) \n        filtered_means = filtered_means [ ... , tf . newaxis ] \n        predicted_means = predicted_means [ ... , tf . newaxis ] \n        initial_backward_mean = predicted_means [ - 1 , ... ] \n        initial_backward_cov = predicted_covs [ - 1 , ... ] \n        num_timesteps = tf . shape ( input = filtered_means ) [ 0 ] \n        initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - 1 ) \n        update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) \n        posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = 1 ) \n        posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , 0 ] , 0 , - 2 ) \n        posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , 0 , - 3 ) \n        return ( posterior_means , posterior_covs ) "}
{"958": "\ndef _rotate ( self , samples ) : \n    event_dim = ( tf . compat . dimension_value ( self . event_shape [ 0 ] ) or self . _event_shape_tensor ( ) [ 0 ] ) \n    basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event_dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , \n    u = tf . nn . l2_normalize ( basis - self . mean_direction , axis = - 1 ) \n    return samples - 2 * tf . reduce_sum ( input_tensor = samples * u , axis = - 1 , keepdims = 1 ) * u "}
{"964": "\ndef is_namedtuple_like ( x ) : \n    try : \n        for fn in x . _fields : \n            _ = getattr ( x , fn ) \n        return 1 \n    except AttributeError : \n        return 0 "}
{"969": "\ndef maybe_call_fn_and_grads ( fn , fn_arg_list , result = None , grads = None , check_non_none_grads = 1 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'maybe_call_fn_and_grads' , [ fn_arg_list , result , grads ] ) : \n        fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) \n        result , grads = _value_and_gradients ( fn , fn_arg_list , result , grads ) \n        if not all ( r . dtype . is_floating for r in ( result if is_list_like ( result ) else [ result ] ) ) : \n            raise TypeError ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) \n        if len ( fn_arg_list ) != len ( grads ) : \n            raise ValueError ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) \n        if check_non_none_grads and any ( g is None for g in grads ) : \n            raise ValueError ( 'Encountered `None` gradient.\\n' '  fn_arg_list: {}\\n' '  grads: {}' . format ( fn_arg_list , grads ) ) \n        return result , grads "}
{"974": "\ndef enable_store_parameters_in_results ( kernel ) : \n    kernel_stack = [ ] \n    while hasattr ( kernel , 'parameters' ) and 'inner_kernel' in kernel . parameters : \n        kernel_stack . append ( kernel ) \n        kernel = kernel . parameters [ 'inner_kernel' ] \n    def _recreate_kernel ( kernel , parameters ) : \n        new_parameters = kernel . parameters . copy ( ) \n        new_parameters . update ( parameters ) \n        if 'store_parameters_in_results' in new_parameters : \n            new_parameters [ 'store_parameters_in_results' ] = 1 \n        with deprecation . silence ( ) : \n            return type ( kernel ) ( ** new_parameters ) \n    if hasattr ( kernel , 'parameters' ) : \n        kernel = _recreate_kernel ( kernel , { } ) \n    for outer_kernel in reversed ( kernel_stack ) : \n        outer_kernel = _recreate_kernel ( outer_kernel , { 'inner_kernel' : kernel } ) \n        kernel = outer_kernel \n    return kernel "}
{"976": "\ndef _replace_event_shape_in_tensorshape ( input_tensorshape , event_shape_in , event_shape_out ) : \n    event_shape_in_ndims = tensorshape_util . num_elements ( event_shape_in . shape ) \n    if tensorshape_util . rank ( input_tensorshape ) is None or event_shape_in_ndims is None : \n        return tf . TensorShape ( None ) , 0 \n    input_non_event_ndims = tensorshape_util . rank ( input_tensorshape ) - event_shape_in_ndims \n    if input_non_event_ndims < 0 : \n        raise ValueError ( 'Input has fewer ndims ({}) than event shape ndims ({}).' . format ( tensorshape_util . rank ( input_tensorshape ) , event_shape_in_ndims ) ) \n    input_non_event_tensorshape = input_tensorshape [ : input_non_event_ndims ] \n    input_event_tensorshape = input_tensorshape [ input_non_event_ndims : ] \n    event_shape_in_ = tf . get_static_value ( event_shape_in ) \n    is_validated = ( tensorshape_util . is_fully_defined ( input_event_tensorshape ) and event_shape_in_ is not None ) \n    if is_validated : \n        input_event_shape_ = np . int32 ( input_event_tensorshape ) \n        mask = event_shape_in_ >= 0 \n        explicit_input_event_shape_ = input_event_shape_ [ mask ] \n        explicit_event_shape_in_ = event_shape_in_ [ mask ] \n        if not all ( explicit_input_event_shape_ == explicit_event_shape_in_ ) : \n            raise ValueError ( 'Input `event_shape` does not match `event_shape_in`. ' '({} vs {}).' . format ( input_event_shape_ , event_shape_in_ ) ) \n    event_tensorshape_out = tensorshape_util . constant_value_as_shape ( event_shape_out ) \n    if tensorshape_util . rank ( event_tensorshape_out ) is None : \n        output_tensorshape = tf . TensorShape ( None ) \n    else : \n        output_tensorshape = tensorshape_util . concatenate ( input_non_event_tensorshape , event_tensorshape_out ) \n    return output_tensorshape , is_validated "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        for replica in range ( self . num_replica ) : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n        target_log_probs = tf . stack ( target_log_probs , axis = 0 ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ 0 ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = 0 , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return i < exchange_proposed_n \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio \n            for k in range ( num_state_parts ) : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n            return i + 1 , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ] "}
{"1005": "\ndef make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : \n    if step_counter is None and num_adaptation_steps is not None : \n        step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - 1 , dtype = np . int32 ) , dtype = tf . int32 , trainable = 0 , use_resource = 1 ) \n    def step_size_simple_update_fn ( step_size_var , kernel_results ) : \n        if kernel_results is None : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ tf . identity ( ss ) for ss in step_size_var ] \n            return tf . identity ( step_size_var ) \n        log_n = tf . math . log ( tf . cast ( tf . size ( input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) \n        log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n \n        adjustment = tf . where ( log_mean_accept_ratio < tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) \n        def build_assign_op ( ) : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] \n            return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) \n        if num_adaptation_steps is None : \n            return build_assign_op ( ) \n        else : \n            with tf . control_dependencies ( [ step_counter . assign_add ( 1 ) ] ) : \n                return tf . cond ( pred = step_counter < num_adaptation_steps , true_fn = build_assign_op , false_fn = lambda : step_size_var ) \n    return step_size_simple_update_fn "}
{"1006": "\ndef _leapfrog_integrator_one_step ( target_log_prob_fn , independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts , state_gradients_are_stopped = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'hmc_leapfrog_integrator_one_step' , [ independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts ] ) : \n        proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( current_momentum_parts , step_sizes , current_target_log_prob_grad_parts ) ] \n        proposed_state_parts = [ x + tf . cast ( eps , v . dtype ) * v for x , eps , v in zip ( current_state_parts , step_sizes , proposed_momentum_parts ) ] \n        if state_gradients_are_stopped : \n            proposed_state_parts = [ tf . stop_gradient ( x ) for x in proposed_state_parts ] \n        [ proposed_target_log_prob , proposed_target_log_prob_grad_parts , ] = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , proposed_state_parts ) \n        if not proposed_target_log_prob . dtype . is_floating : \n            raise TypeError ( '`target_log_prob_fn` must produce a `Tensor` ' 'with `float` `dtype`.' ) \n        if any ( g is None for g in proposed_target_log_prob_grad_parts ) : \n            raise ValueError ( 'Encountered `None` gradient. Does your target `target_log_prob_fn` ' 'access all `tf.Variable`s via `tf.get_variable`?\\n' '  current_state_parts: {}\\n' '  proposed_state_parts: {}\\n' '  proposed_target_log_prob_grad_parts: {}' . format ( current_state_parts , proposed_state_parts , proposed_target_log_prob_grad_parts ) ) \n        proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( proposed_momentum_parts , step_sizes , proposed_target_log_prob_grad_parts ) ] \n        return [ proposed_momentum_parts , proposed_state_parts , proposed_target_log_prob , proposed_target_log_prob_grad_parts , ] "}
{"1015": "\ndef sample_chain ( num_results , current_state , previous_kernel_results = None , kernel = None , num_burnin_steps = 0 , num_steps_between_results = 0 , trace_fn = lambda current_state , kernel_results : kernel_results , return_final_kernel_results = 0 , parallel_iterations = 10 , name = None , ) : \n    if not kernel . is_calibrated : \n        warnings . warn ( \"supplied `TransitionKernel` is not calibrated. Markov \" \"chain may not converge to intended target distribution.\" ) \n    with tf . compat . v1 . name_scope ( name , \"mcmc_sample_chain\" , [ num_results , num_burnin_steps , num_steps_between_results ] ) : \n        num_results = tf . convert_to_tensor ( value = num_results , dtype = tf . int32 , name = \"num_results\" ) \n        num_burnin_steps = tf . convert_to_tensor ( value = num_burnin_steps , dtype = tf . int32 , name = \"num_burnin_steps\" ) \n        num_steps_between_results = tf . convert_to_tensor ( value = num_steps_between_results , dtype = tf . int32 , name = \"num_steps_between_results\" ) \n        current_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = \"current_state\" ) , current_state ) \n        if previous_kernel_results is None : \n            previous_kernel_results = kernel . bootstrap_results ( current_state ) \n        if trace_fn is None : \n            trace_fn = lambda * args : ( ) \n            no_trace = 1 \n        else : \n            no_trace = 0 \n        if trace_fn is sample_chain . __defaults__ [ 4 ] : \n            warnings . warn ( \"Tracing all kernel results by default is deprecated. Set \" \"the `trace_fn` argument to None (the future default \" \"value) or an explicit callback that traces the values \" \"you are interested in.\" ) \n        def _trace_scan_fn ( state_and_results , num_steps ) : \n            next_state , current_kernel_results = mcmc_util . smart_for_loop ( loop_num_iter = num_steps , body_fn = kernel . one_step , initial_loop_vars = list ( state_and_results ) , parallel_iterations = parallel_iterations ) \n            return next_state , current_kernel_results \n        ( _ , final_kernel_results ) , ( all_states , trace ) = mcmc_util . trace_scan ( loop_fn = _trace_scan_fn , initial_state = ( current_state , previous_kernel_results ) , elems = tf . one_hot ( indices = 0 , depth = num_results , on_value = 1 + num_burnin_steps , off_value = 1 + num_steps_between_results , dtype = tf . int32 ) , trace_fn = lambda state_and_results : ( state_and_results [ 0 ] , trace_fn ( * state_and_results ) ) , parallel_iterations = parallel_iterations ) \n        if return_final_kernel_results : \n            return CheckpointableStatesAndTrace ( all_states = all_states , trace = trace , final_kernel_results = final_kernel_results ) \n        else : \n            if no_trace : \n                return all_states \n            else : \n                return StatesAndTrace ( all_states = all_states , trace = trace ) "}
{"1027": "\ndef create_sprites_dataset ( characters , actions , directions , channels = 3 , length = 8 , shuffle = 0 , fake_data = 0 ) : \n    if fake_data : \n        dummy_image = tf . random . normal ( [ HEIGHT , WIDTH , CHANNELS ] ) \n    else : \n        basedir = download_sprites ( ) \n    action_names = [ action . name for action in actions ] \n    action_metadata = [ ( action . start_row , action . frames ) for action in actions ] \n    direction_rows = [ direction . row_offset for direction in directions ] \n    chars = tf . data . Dataset . from_tensor_slices ( characters ) \n    act_names = tf . data . Dataset . from_tensor_slices ( action_names ) . repeat ( ) \n    acts_metadata = tf . data . Dataset . from_tensor_slices ( action_metadata ) . repeat ( ) \n    dir_rows = tf . data . Dataset . from_tensor_slices ( direction_rows ) . repeat ( ) \n    if shuffle : \n        chars = chars . shuffle ( len ( characters ) ) \n    dataset = tf . data . Dataset . zip ( ( chars , act_names , acts_metadata , dir_rows ) ) \n    skin_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( SKIN_COLORS ) ) \n    hair_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( HAIRSTYLES ) ) \n    top_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( TOPS ) ) \n    pants_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( PANTS ) ) \n    action_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( action_names ) ) \n    def process_example ( attrs , act_name , act_metadata , dir_row_offset ) : \n        skin_name = attrs [ 0 ] \n        hair_name = attrs [ 1 ] \n        top_name = attrs [ 2 ] \n        pants_name = attrs [ 3 ] \n        if fake_data : \n            char = dummy_image \n        else : \n            skin = read_image ( basedir + os . sep + skin_name ) \n            hair = read_image ( basedir + os . sep + hair_name ) \n            top = read_image ( basedir + os . sep + top_name ) \n            pants = read_image ( basedir + os . sep + pants_name ) \n            char = create_character ( skin , hair , top , pants ) \n        if shuffle : \n            seq = create_random_seq ( char , act_metadata , dir_row_offset , length ) \n        else : \n            seq = create_seq ( char , act_metadata , dir_row_offset , length ) \n        seq = seq [ ... , : channels ] \n        skin_idx = skin_table . lookup ( skin_name ) \n        hair_idx = hair_table . lookup ( hair_name ) \n        top_idx = top_table . lookup ( top_name ) \n        pants_idx = pants_table . lookup ( pants_name ) \n        act_idx = action_table . lookup ( act_name ) \n        return ( seq , skin_idx , hair_idx , top_idx , pants_idx , act_idx , skin_name , hair_name , top_name , pants_name , act_name ) \n    dataset = dataset . map ( process_example ) \n    return dataset "}
{"1032": "\ndef count_integers ( arr , weights = None , minlength = None , maxlength = None , axis = None , dtype = tf . int32 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'count_integers' , values = [ arr , weights , minlength , maxlength , axis ] ) : \n        if axis is None : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        arr = tf . convert_to_tensor ( value = arr , dtype = tf . int32 , name = 'arr' ) \n        arr_ndims = _get_static_ndims ( arr , expect_static = 1 ) \n        axis = _make_static_axis_non_negative_list ( axis , arr_ndims ) \n        not_axis = sorted ( set ( range ( arr_ndims ) ) . difference ( axis ) ) \n        if not not_axis : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        flat_arr = _move_dims_to_flat_end ( arr , not_axis , arr_ndims , right_end = 0 ) \n        if weights is None : \n            def one_bincount ( arr_slice ) : \n                return tf . math . bincount ( arr_slice , weights = None , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = flat_arr , dtype = dtype ) \n        else : \n            weights = tf . convert_to_tensor ( value = weights , name = 'weights' ) \n            _get_static_ndims ( weights , expect_static = 1 , expect_ndims = arr_ndims ) \n            flat_weights = _move_dims_to_flat_end ( weights , not_axis , arr_ndims , right_end = 0 ) \n            def one_bincount ( arr_and_weights_slices ) : \n                arr_slice , weights_slice = arr_and_weights_slices \n                return tf . math . bincount ( arr_slice , weights = weights_slice , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = [ flat_arr , flat_weights ] , dtype = weights . dtype ) \n        flat_counts_t = tf . transpose ( a = flat_counts , perm = [ 1 , 0 ] ) \n        _get_static_ndims ( flat_counts_t , expect_ndims = 2 , expect_static = 1 ) \n        not_axis_shape = tf . gather ( tf . shape ( input = arr ) , indices = not_axis ) \n        out_shape = tf . concat ( [ [ - 1 ] , not_axis_shape ] , axis = 0 ) \n        return tf . reshape ( flat_counts_t , out_shape ) "}
{"1033": "\ndef find_bins ( x , edges , extend_lower_interval = 0 , extend_upper_interval = 0 , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : \n        in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) \n        if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and tf . compat . dimension_value ( edges . shape [ 0 ] ) < 2 ) : \n            raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) \n        flattening_x = edges . shape . ndims == 1 and x . shape . ndims > 1 \n        if flattening_x : \n            x_orig_shape = tf . shape ( input = x ) \n            x = tf . reshape ( x , [ - 1 ] ) \n        if dtype is None : \n            dtype = in_type \n        dtype = tf . as_dtype ( dtype ) \n        x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) \n        edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) \n        searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None \n        almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) \n        almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) \n        bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( input = edges ) [ 0 ] - 2 , dtype ) ) \n        if not extend_lower_interval : \n            low_fill = np . nan if dtype . is_floating else - 1 \n            bins = tf . where ( x < tf . expand_dims ( edges [ 0 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) \n        if not extend_upper_interval : \n            up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ 0 ] - 1 \n            bins = tf . where ( x > tf . expand_dims ( edges [ - 1 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) \n        if flattening_x : \n            bins = tf . reshape ( bins , x_orig_shape ) \n        return bins "}
{"1034": "\ndef histogram ( x , edges , axis = None , extend_lower_interval = 0 , extend_upper_interval = 0 , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : \n        in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) \n        if axis is None : \n            x = tf . reshape ( x , shape = [ - 1 ] ) \n        else : \n            x_ndims = _get_static_ndims ( x , expect_static = 1 , expect_ndims_at_least = 1 ) \n            axis = _make_static_axis_non_negative_list ( axis , x_ndims ) \n            if not axis : \n                raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) \n            x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = 0 ) \n        bins = find_bins ( x , edges = edges , extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) \n        counts = count_integers ( bins , minlength = tf . shape ( input = edges ) [ 0 ] - 1 , maxlength = tf . shape ( input = edges ) [ 0 ] - 1 , axis = 0 , dtype = dtype or in_dtype ) \n        n_edges = tf . compat . dimension_value ( edges . shape [ 0 ] ) \n        if n_edges is not None : \n            counts . set_shape ( tf . TensorShape ( [ n_edges - 1 ] ) . concatenate ( counts . shape [ 1 : ] ) ) \n        return counts "}
{"1035": "\ndef quantiles ( x , num_quantiles , axis = None , interpolation = None , keep_dims = 0 , validate_args = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'quantiles' , values = [ x , num_quantiles , axis ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        return percentile ( x , q = tf . linspace ( tf . convert_to_tensor ( value = 0 , dtype = tf . float64 ) , tf . convert_to_tensor ( value = 100 , dtype = tf . float64 ) , num = num_quantiles + 1 ) , axis = axis , interpolation = interpolation , keep_dims = keep_dims , validate_args = validate_args , preserve_gradients = 0 ) "}
{"1036": "\ndef _get_static_ndims ( x , expect_static = 0 , expect_ndims = None , expect_ndims_no_more_than = None , expect_ndims_at_least = None ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        shape_const = tf . get_static_value ( tf . shape ( input = x ) ) \n        if shape_const is not None : \n            ndims = shape_const . ndim \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected argument `x` to have statically defined `ndims`.  Found: ' % x ) \n        return \n    if expect_ndims is not None : \n        ndims_message = ( 'Expected argument `x` to have ndims %s.  Found tensor %s' % ( expect_ndims , x ) ) \n        if ndims != expect_ndims : \n            raise ValueError ( ndims_message ) \n    if expect_ndims_at_least is not None : \n        ndims_at_least_message = ( 'Expected argument `x` to have ndims >= %d.  Found tensor %s' % ( expect_ndims_at_least , x ) ) \n        if ndims < expect_ndims_at_least : \n            raise ValueError ( ndims_at_least_message ) \n    if expect_ndims_no_more_than is not None : \n        ndims_no_more_than_message = ( 'Expected argument `x` to have ndims <= %d.  Found tensor %s' % ( expect_ndims_no_more_than , x ) ) \n        if ndims > expect_ndims_no_more_than : \n            raise ValueError ( ndims_no_more_than_message ) \n    return ndims "}
{"1039": "\ndef _move_dims_to_flat_end ( x , axis , x_ndims , right_end = 1 ) : \n    if not axis : \n        return x \n    other_dims = sorted ( set ( range ( x_ndims ) ) . difference ( axis ) ) \n    perm = other_dims + list ( axis ) if right_end else list ( axis ) + other_dims \n    x_permed = tf . transpose ( a = x , perm = perm ) \n    if x . shape . is_fully_defined ( ) : \n        x_shape = x . shape . as_list ( ) \n        other_shape = [ x_shape [ i ] for i in other_dims ] \n        end_shape = [ np . prod ( [ x_shape [ i ] for i in axis ] ) ] \n        full_shape = ( other_shape + end_shape if right_end else end_shape + other_shape ) \n    else : \n        other_shape = tf . gather ( tf . shape ( input = x ) , other_dims ) \n        full_shape = tf . concat ( [ other_shape , [ - 1 ] ] if right_end else [ [ - 1 ] , other_shape ] , axis = 0 ) \n    return tf . reshape ( x_permed , shape = full_shape ) "}
{"1042": "\ndef amari_alpha ( logu , alpha = 1. , self_normalized = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"amari_alpha\" , [ logu ] ) : \n        if alpha is None or tf . is_tensor ( alpha ) : \n            raise TypeError ( \"`alpha` cannot be `None` or `Tensor` type.\" ) \n        if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : \n            raise TypeError ( \"`self_normalized` cannot be `None` or `Tensor` type.\" ) \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        if alpha == 0. : \n            f = - logu \n        elif alpha == 1. : \n            f = tf . exp ( logu ) * logu \n        else : \n            f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) \n        if not self_normalized : \n            return f \n        if alpha == 0. : \n            return f + tf . math . expm1 ( logu ) \n        elif alpha == 1. : \n            return f - tf . math . expm1 ( logu ) \n        else : \n            return f - tf . math . expm1 ( logu ) / ( alpha - 1. ) "}
{"1043": "\ndef kl_reverse ( logu , self_normalized = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"kl_reverse\" , [ logu ] ) : \n        return amari_alpha ( logu , alpha = 0. , self_normalized = self_normalized ) "}
{"1044": "\ndef jensen_shannon ( logu , self_normalized = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"jensen_shannon\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        npdt = logu . dtype . as_numpy_dtype \n        y = tf . nn . softplus ( logu ) \n        if self_normalized : \n            y -= np . log ( 2 ) . astype ( npdt ) \n        return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y "}
{"1048": "\ndef t_power ( logu , t , self_normalized = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"t_power\" , [ logu , t ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = \"t\" ) \n        fu = tf . math . expm1 ( t * logu ) \n        if self_normalized : \n            fu -= t * tf . math . expm1 ( logu ) \n        fu *= tf . where ( tf . logical_and ( 0. < t , t < 1. ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) \n        return fu "}
{"1051": "\ndef modified_gan ( logu , self_normalized = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"chi_square\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        y = tf . nn . softplus ( logu ) - logu \n        if self_normalized : \n            y += 0.5 * tf . math . expm1 ( logu ) \n        return y "}
{"1055": "\ndef csiszar_vimco_helper ( logu , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"csiszar_vimco_helper\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( 1 ) [ 0 ] ) \n        if n is None : \n            n = tf . shape ( input = logu ) [ 0 ] \n            log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) \n            nm1 = tf . cast ( n - 1 , dtype = logu . dtype ) \n        else : \n            log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) \n            nm1 = np . asarray ( n - 1 , dtype = logu . dtype . as_numpy_dtype ) \n        log_max_u = tf . reduce_max ( input_tensor = logu , axis = 0 ) \n        log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = 0 ) \n        d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) \n        d_ok = tf . not_equal ( d , 0. ) \n        safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) \n        d_ok_result = logu + tfd . softplus_inverse ( safe_d ) \n        inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) \n        is_positive_and_largest = tf . logical_and ( logu > 0. , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) \n        log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( input = logu ) , - inf ) , logu ) , axis = 0 , keepdims = 1 ) \n        log_lomsum_u = tf . tile ( log_lomsum_u , multiples = 1 + tf . pad ( tensor = [ n - 1 ] , paddings = [ [ 0 , tf . rank ( logu ) - 1 ] ] ) ) \n        d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( input = d ) , - inf ) ) \n        log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) \n        looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = 0 ) - logu ) / nm1 \n        log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = 0 ) \n        log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n \n        log_sooavg_u = log_soosum_u - log_n \n        log_avg_u . set_shape ( logu . shape . with_rank_at_least ( 1 ) [ 1 : ] ) \n        log_sooavg_u . set_shape ( logu . shape ) \n        return log_avg_u , log_sooavg_u "}
{"1056": "\ndef _assert_ndims_statically ( x , expect_ndims = None , expect_ndims_at_least = None , expect_static = 0 ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) \n        return \n    if expect_ndims is not None and ndims != expect_ndims : \n        raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) \n    if expect_ndims_at_least is not None and ndims < expect_ndims_at_least : \n        raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) ) "}
{"1068": "\ndef pad_shape_right_with_ones ( x , ndims ) : \n    if not ( isinstance ( ndims , int ) and ndims >= 0 ) : \n        raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) \n    if ndims == 0 : \n        return x \n    x = tf . convert_to_tensor ( value = x ) \n    original_shape = x . shape \n    new_shape = distribution_util . pad ( tf . shape ( input = x ) , axis = 0 , back = 1 , value = 1 , count = ndims ) \n    x = tf . reshape ( x , new_shape ) \n    x . set_shape ( original_shape . concatenate ( [ 1 ] * ndims ) ) \n    return x "}
{"1074": "\ndef _get_search_direction ( state ) : \n    num_elements = tf . minimum ( state . num_iterations , distribution_util . prefer_static_shape ( state . position_deltas ) [ 0 ] ) \n    def _two_loop_algorithm ( ) : \n        position_deltas = state . position_deltas [ - num_elements : ] \n        gradient_deltas = state . gradient_deltas [ - num_elements : ] \n        inv_rhos = tf . reduce_sum ( input_tensor = gradient_deltas * position_deltas , axis = - 1 ) \n        def first_loop ( acc , args ) : \n            _ , q_direction = acc \n            position_delta , gradient_delta , inv_rho = args \n            alpha = tf . reduce_sum ( input_tensor = position_delta * q_direction , axis = - 1 ) / inv_rho \n            direction_delta = tf . expand_dims ( alpha , axis = - 1 ) * gradient_delta \n            return ( alpha , q_direction - direction_delta ) \n        zero = tf . zeros_like ( inv_rhos [ 0 ] ) \n        alphas , q_directions = tf . scan ( first_loop , [ position_deltas , gradient_deltas , inv_rhos ] , initializer = ( zero , state . objective_gradient ) , reverse = 1 ) \n        gamma_k = inv_rhos [ - 1 ] / tf . reduce_sum ( input_tensor = gradient_deltas [ - 1 ] * gradient_deltas [ - 1 ] , axis = - 1 ) \n        r_direction = tf . expand_dims ( gamma_k , axis = - 1 ) * q_directions [ 0 ] \n        def second_loop ( r_direction , args ) : \n            alpha , position_delta , gradient_delta , inv_rho = args \n            beta = tf . reduce_sum ( input_tensor = gradient_delta * r_direction , axis = - 1 ) / inv_rho \n            direction_delta = tf . expand_dims ( alpha - beta , axis = - 1 ) * position_delta \n            return r_direction + direction_delta \n        r_directions = tf . scan ( second_loop , [ alphas , position_deltas , gradient_deltas , inv_rhos ] , initializer = r_direction ) \n        return - r_directions [ - 1 ] \n    return prefer_static . cond ( tf . equal ( num_elements , 0 ) , ( lambda : - state . objective_gradient ) , _two_loop_algorithm ) "}
{"1079": "\ndef _uniform_correlation_like_matrix ( num_rows , batch_shape , dtype , seed ) : \n    num_entries = num_rows * ( num_rows + 1 ) / 2 \n    ones = tf . ones ( shape = [ num_entries ] , dtype = dtype ) \n    unifs = uniform . Uniform ( - ones , ones ) . sample ( batch_shape , seed = seed ) \n    tril = util . fill_triangular ( unifs ) \n    symmetric = tril + tf . linalg . matrix_transpose ( tril ) \n    diagonal_ones = tf . ones ( shape = util . pad ( batch_shape , axis = 0 , back = 1 , value = num_rows ) , dtype = dtype ) \n    return tf . linalg . set_diag ( symmetric , diagonal_ones ) "}
{"1086": "\ndef minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    if initial_population is None and initial_position is None : \n        raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) \n    if initial_population is not None and initial_position is not None : \n        raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) \n    with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : \n        ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) \n        def evolve_body ( loop_vars ) : \n            next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) \n            converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) \n            failed = _check_failure ( next_population_values ) \n            return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] \n        def evolve_cond ( loop_vars ) : \n            should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and loop_vars . num_iterations >= max_iterations ) ) \n            return ~ should_stop \n        initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = 0 ) , failed = tf . convert_to_tensor ( value = 0 ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) \n        final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] \n        best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) \n        final_population = final_state . population \n        if not was_iterable : \n            final_population = final_population [ 0 ] \n            best_position = best_position [ 0 ] \n        return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations ) "}
{"1087": "\ndef _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) : \n    was_iterable = 0 \n    if initial_position is not None : \n        initial_position , was_iterable = _ensure_list ( initial_position ) \n    if initial_population is not None : \n        initial_population , was_iterable = _ensure_list ( initial_population ) \n    population = _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed = seed ) \n    differential_weight = tf . convert_to_tensor ( value = differential_weight , dtype = population [ 0 ] . dtype . base_dtype ) \n    crossover_prob = tf . convert_to_tensor ( value = crossover_prob ) \n    population_values = objective_function ( * population ) \n    if max_iterations is not None : \n        max_iterations = tf . convert_to_tensor ( value = max_iterations ) \n    func_tolerance = tf . convert_to_tensor ( value = func_tolerance , dtype = population_values . dtype . base_dtype ) \n    position_tolerance = tf . convert_to_tensor ( value = position_tolerance , dtype = population [ 0 ] . dtype . base_dtype ) \n    return ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) "}
{"1091": "\ndef _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : \n    sizes = [ tf . cast ( tf . size ( input = x ) , dtype = tf . float64 ) for x in population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) \n    force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , 1 ] , seed = seed_stream ( ) ) \n    recombinants = [ ] \n    for i , population_part in enumerate ( population ) : \n        pop_part_flat = tf . reshape ( population_part , [ population_size , - 1 ] ) \n        mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - 1 ] ) \n        part_size = tf . size ( input = population_part ) // population_size \n        force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = 0 , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = 1 , off_value = 0 , dtype = tf . bool ) \n        group_mask = tf . math . equal ( force_crossover_group , i ) \n        force_crossovers &= group_mask \n        do_binary_crossover = tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) < crossover_prob \n        do_binary_crossover |= force_crossovers \n        recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) \n        recombinant = tf . reshape ( recombinant_flat , tf . shape ( input = population_part ) ) \n        recombinants . append ( recombinant ) \n    return recombinants "}
{"1094": "\ndef _ensure_list ( tensor_or_list ) : \n    if isinstance ( tensor_or_list , ( list , tuple ) ) : \n        return list ( tensor_or_list ) , 1 \n    return [ tensor_or_list ] , 0 "}
{"1098": "\ndef build_input_pipeline ( train_images , batch_size ) : \n    training_dataset = tf . data . Dataset . from_tensor_slices ( train_images ) \n    training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = 1 ) . repeat ( ) . batch ( batch_size ) \n    training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) \n    images = training_iterator . get_next ( ) \n    return images "}
{"1105": "\ndef matrix_rank ( a , tol = None , validate_args = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : \n        a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        s = tf . linalg . svd ( a , compute_uv = 0 ) \n        if tol is None : \n            if a . shape [ - 2 : ] . is_fully_defined ( ) : \n                m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) \n            else : \n                m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2 : ] ) \n            eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps \n            tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = 1 ) ) \n        return tf . reduce_sum ( input_tensor = tf . cast ( s > tol , tf . int32 ) , axis = - 1 ) "}
{"1106": "\ndef pinv ( a , rcond = None , validate_args = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : \n        a = tf . convert_to_tensor ( value = a , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        dtype = a . dtype . as_numpy_dtype \n        if rcond is None : \n            def get_dim_size ( dim ) : \n                if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : \n                    return tf . compat . dimension_value ( a . shape [ dim ] ) \n                return tf . shape ( input = a ) [ dim ] \n            num_rows = get_dim_size ( - 2 ) \n            num_cols = get_dim_size ( - 1 ) \n            if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : \n                max_rows_cols = float ( max ( num_rows , num_cols ) ) \n            else : \n                max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) \n            rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps \n        rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) \n        [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = 0 , compute_uv = 1 ) \n        cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) \n        singular_values = tf . where ( singular_values > cutoff [ ... , tf . newaxis ] , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) \n        a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = 1 ) \n        if a . shape . ndims is not None : \n            a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2 ] ] ) ) \n        return a_pinv "}
{"1107": "\ndef lu_solve ( lower_upper , perm , rhs , validate_args = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) \n        assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n                rhs = tf . identity ( rhs ) \n        if rhs . shape . ndims == 2 and perm . shape . ndims == 1 : \n            permuted_rhs = tf . gather ( rhs , perm , axis = - 2 ) \n        else : \n            rhs_shape = tf . shape ( input = rhs ) \n            broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2 ] , tf . shape ( input = perm ) [ : - 1 ] ) \n            d , m = rhs_shape [ - 2 ] , rhs_shape [ - 1 ] \n            rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = 0 ) \n            broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) \n            broadcast_rhs = tf . reshape ( broadcast_rhs , [ - 1 , d , m ] ) \n            broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - 1 ] ) \n            broadcast_perm = tf . reshape ( broadcast_perm , [ - 1 , d ] ) \n            broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) \n            broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) \n            broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - 1 ) \n            permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) \n            permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) \n        lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( tf . shape ( input = lower_upper ) [ : - 1 ] , dtype = lower_upper . dtype ) ) \n        return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = 0 ) "}
{"1108": "\ndef lu_matrix_inverse ( lower_upper , perm , validate_args = 0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_matrix_inverse' , [ lower_upper , perm ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n        shape = tf . shape ( input = lower_upper ) \n        return lu_solve ( lower_upper , perm , rhs = tf . eye ( shape [ - 1 ] , batch_shape = shape [ : - 2 ] , dtype = lower_upper . dtype ) , validate_args = 0 ) "}
{"1113": "\ndef _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : \n    mean , variance , grad_mean = model ( linear_response ) \n    is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) \n    def _mask_if_invalid ( x , mask ) : \n        mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) \n        return tf . where ( is_valid , x , mask ) \n    v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) \n    grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = 1 ) \n    fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) \n    return - grad_log_likelihood , fim_middle "}
{"1117": "\ndef masked_dense ( inputs , units , num_blocks = None , exclusive = 0 , kernel_initializer = None , reuse = None , name = None , * args , ** kwargs ) : \n    input_depth = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( inputs . shape , 1 ) [ - 1 ] ) \n    if input_depth is None : \n        raise NotImplementedError ( \"Rightmost dimension must be known prior to graph execution.\" ) \n    mask = _gen_mask ( num_blocks , input_depth , units , MASK_EXCLUSIVE if exclusive else MASK_INCLUSIVE ) . T \n    if kernel_initializer is None : \n        kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) \n    def masked_initializer ( shape , dtype = None , partition_info = None ) : \n        return mask * kernel_initializer ( shape , dtype , partition_info ) \n    with tf . compat . v2 . name_scope ( name or \"masked_dense\" ) : \n        layer = tf . compat . v1 . layers . Dense ( units , kernel_initializer = masked_initializer , kernel_constraint = lambda x : mask * x , name = name , dtype = dtype_util . base_dtype ( inputs . dtype ) , _scope = name , _reuse = reuse , * args , ** kwargs ) \n        return layer . apply ( inputs ) "}
{"1129": "\ndef _best_order ( g ) : \n    def _explore ( u ) : \n        if u . depth < 0 : \n            return \n        if not u . parents : \n            result . append ( ( u . name , u . parents ) ) \n            u . depth = - 1 \n            return \n        b = ( u . name , [ ] ) \n        result . append ( b ) \n        u . depth = - 1 \n        d = 0 \n        for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : \n            n0 = len ( result ) \n            _explore ( v ) \n            n1 = len ( result ) \n            b [ 1 ] . extend ( [ '_' ] * d + [ v . name ] ) \n            d = n1 - n0 - 1 \n    g = _depth ( g ) \n    result = [ ] \n    for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = 1 ) : \n        _explore ( u ) \n    return tuple ( reversed ( result ) ) "}
{"1132": "\ndef variational_loss ( self , observations , observation_index_points = None , kl_weight = 1. , name = 'variational_loss' ) : \n    with tf . name_scope ( name or 'variational_gp_loss' ) : \n        if observation_index_points is None : \n            observation_index_points = self . _index_points \n        observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = self . _dtype , name = 'observation_index_points' ) \n        observations = tf . convert_to_tensor ( value = observations , dtype = self . _dtype , name = 'observations' ) \n        kl_weight = tf . convert_to_tensor ( value = kl_weight , dtype = self . _dtype , name = 'kl_weight' ) \n        kzx = self . kernel . matrix ( self . _inducing_index_points , observation_index_points ) \n        kzx_linop = tf . linalg . LinearOperatorFullMatrix ( kzx ) \n        loc = ( self . _mean_fn ( observation_index_points ) + kzx_linop . matvec ( self . _kzz_inv_varloc , adjoint = 1 ) ) \n        likelihood = independent . Independent ( normal . Normal ( loc = loc , scale = tf . sqrt ( self . _observation_noise_variance + self . _jitter ) , name = 'NormalLikelihood' ) , reinterpreted_batch_ndims = 1 ) \n        obs_ll = likelihood . log_prob ( observations ) \n        chol_kzz_linop = tf . linalg . LinearOperatorLowerTriangular ( self . _chol_kzz ) \n        chol_kzz_inv_kzx = chol_kzz_linop . solve ( kzx ) \n        kzz_inv_kzx = chol_kzz_linop . solve ( chol_kzz_inv_kzx , adjoint = 1 ) \n        kxx_diag = tf . linalg . diag_part ( self . kernel . matrix ( observation_index_points , observation_index_points ) ) \n        ktilde_trace_term = ( tf . reduce_sum ( input_tensor = kxx_diag , axis = - 1 ) - tf . reduce_sum ( input_tensor = chol_kzz_inv_kzx ** 2 , axis = [ - 2 , - 1 ] ) ) \n        other_trace_term = tf . reduce_sum ( input_tensor = ( self . _variational_inducing_observations_posterior . scale . matmul ( kzz_inv_kzx ) ** 2 ) , axis = [ - 2 , - 1 ] ) \n        trace_term = ( .5 * ( ktilde_trace_term + other_trace_term ) / self . _observation_noise_variance ) \n        inducing_prior = gaussian_process . GaussianProcess ( kernel = self . _kernel , mean_fn = self . _mean_fn , index_points = self . _inducing_index_points , observation_noise_variance = self . _observation_noise_variance ) \n        kl_term = kl_weight * kullback_leibler . kl_divergence ( self . _variational_inducing_observations_posterior , inducing_prior ) \n        lower_bound = ( obs_ll - trace_term - kl_term ) \n        return - tf . reduce_mean ( input_tensor = lower_bound ) "}
{"1133": "\ndef optimal_variational_posterior ( kernel , inducing_index_points , observation_index_points , observations , observation_noise_variance , mean_fn = None , jitter = 1e-6 , name = None ) : \n    with tf . name_scope ( name or 'optimal_variational_posterior' ) : \n        dtype = dtype_util . common_dtype ( [ inducing_index_points , observation_index_points , observations , observation_noise_variance , jitter ] , tf . float32 ) \n        inducing_index_points = tf . convert_to_tensor ( value = inducing_index_points , dtype = dtype , name = 'inducing_index_points' ) \n        observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = dtype , name = 'observation_index_points' ) \n        observations = tf . convert_to_tensor ( value = observations , dtype = dtype , name = 'observations' ) \n        observation_noise_variance = tf . convert_to_tensor ( value = observation_noise_variance , dtype = dtype , name = 'observation_noise_variance' ) \n        jitter = tf . convert_to_tensor ( value = jitter , dtype = dtype , name = 'jitter' ) \n        if mean_fn is None : \n            mean_fn = lambda x : tf . zeros ( [ 1 ] , dtype = dtype ) \n        else : \n            if not callable ( mean_fn ) : \n                raise ValueError ( '`mean_fn` must be a Python callable' ) \n        kzz = kernel . matrix ( inducing_index_points , inducing_index_points ) \n        kzx = kernel . matrix ( inducing_index_points , observation_index_points ) \n        noise_var_inv = tf . math . reciprocal ( observation_noise_variance ) \n        sigma_inv = _add_diagonal_shift ( kzz + noise_var_inv * tf . matmul ( kzx , kzx , adjoint_b = 1 ) , jitter ) \n        chol_sigma_inv = tf . linalg . cholesky ( sigma_inv ) \n        kzx_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzx ) \n        kzx_obs = kzx_lin_op . matvec ( observations - mean_fn ( observation_index_points ) ) \n        kzz_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzz ) \n        loc = ( mean_fn ( inducing_index_points ) + noise_var_inv * kzz_lin_op . matvec ( _solve_cholesky_factored_system_vec ( chol_sigma_inv , kzx_obs ) ) ) \n        chol_sigma_inv_lin_op = tf . linalg . LinearOperatorLowerTriangular ( chol_sigma_inv ) \n        scale = chol_sigma_inv_lin_op . solve ( kzz ) \n        return loc , scale "}
{"1139": "\ndef _is_empty_observation_data ( feature_ndims , observation_index_points , observations ) : \n    if observation_index_points is None and observations is None : \n        return 1 \n    num_obs = tf . compat . dimension_value ( observation_index_points . shape [ - ( feature_ndims + 1 ) ] ) \n    if num_obs is not None and num_obs == 0 : \n        return 1 \n    return 0 "}
{"1142": "\ndef set_checkpoint ( self , checkpoint_trigger , checkpoint_path , isOverWrite = 1 ) : \n    if not os . path . exists ( checkpoint_path ) : \n        mkpath ( checkpoint_path ) \n    callBigDlFunc ( self . bigdl_type , \"setCheckPoint\" , self . value , checkpoint_trigger , checkpoint_path , isOverWrite ) "}
{"1154": "\ndef fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , validation_data = None , distributed = 1 ) : \n    if distributed : \n        if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : \n            training_data = to_sample_rdd ( x , y ) \n            if validation_data : \n                validation_data = to_sample_rdd ( * validation_data ) \n        elif ( isinstance ( x , RDD ) or isinstance ( x , DataSet ) ) and not y : \n            training_data = x \n        else : \n            raise TypeError ( \"Unsupported training data type: %s\" % type ( x ) ) \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , training_data , batch_size , nb_epoch , validation_data ) \n    else : \n        if validation_data : \n            val_x = [ JTensor . from_ndarray ( x ) for x in to_list ( validation_data [ 0 ] ) ] \n            val_y = JTensor . from_ndarray ( validation_data [ 1 ] ) \n        else : \n            val_x , val_y = None , None \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , [ JTensor . from_ndarray ( x ) for x in to_list ( x ) ] , JTensor . from_ndarray ( y ) , batch_size , nb_epoch , val_x , val_y , multiprocessing . cpu_count ( ) ) "}
{"1156": "\ndef predict ( self , x , distributed = 1 ) : \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            features = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            features = x \n        else : \n            raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) \n        return self . predict_distributed ( features ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . predict_local ( x ) \n        else : \n            raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) "}
{"1164": "\ndef _to_java_object_rdd ( rdd ) : \n    rdd = rdd . _reserialize ( AutoBatchedSerializer ( PickleSerializer ( ) ) ) \n    return rdd . ctx . _jvm . org . apache . spark . bigdl . api . python . BigDLSerDe . pythonToJava ( rdd . _jrdd , 1 ) "}
{"1171": "\ndef get_image ( self , float_key = \"floats\" , to_chw = 1 ) : \n    return self . image_frame . get_image ( float_key , to_chw ) "}
{"1172": "\ndef get_image ( self , float_key = \"floats\" , to_chw = 1 ) : \n    tensors = callBigDlFunc ( self . bigdl_type , \"localImageFrameToImageTensor\" , self . value , float_key , to_chw ) \n    return map ( lambda tensor : tensor . to_ndarray ( ) , tensors ) "}
{"1175": "\ndef predict ( self , x , batch_size = None , verbose = None , is_distributed = 0 ) : \n    if batch_size or verbose : \n        raise Exception ( \"we don't support batch_size or verbose for now\" ) \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            input = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            input = x \n        return self . bmodel . predict ( input ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . bmodel . predict_local ( x ) \n    raise Exception ( \"not supported type: %s\" % x ) "}
{"1176": "\ndef fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , verbose = 1 , callbacks = None , validation_split = 0. , validation_data = None , shuffle = 1 , class_weight = None , sample_weight = None , initial_epoch = 0 , is_distributed = 0 ) : \n    if callbacks : \n        raise Exception ( \"We don't support callbacks in fit for now\" ) \n    if class_weight : \n        unsupport_exp ( \"class_weight\" ) \n    if sample_weight : \n        unsupport_exp ( \"sample_weight\" ) \n    if initial_epoch != 0 : \n        unsupport_exp ( \"initial_epoch\" ) \n    if shuffle != 1 : \n        unsupport_exp ( \"shuffle\" ) \n    if validation_split != 0. : \n        unsupport_exp ( \"validation_split\" ) \n    bopt = self . __create_optimizer ( x = x , y = y , batch_size = batch_size , nb_epoch = nb_epoch , validation_data = validation_data , is_distributed = is_distributed ) \n    bopt . optimize ( ) "}
{"1184": "\ndef training ( self , is_training = 1 ) : \n    if is_training : \n        callJavaFunc ( self . value . training ) \n    else : \n        callJavaFunc ( self . value . evaluate ) \n    return self "}
{"1186": "\ndef load_keras ( json_path = None , hdf5_path = None , by_name = 0 ) : \n    import os \n    try : \n        import tensorflow \n    except ImportError : \n        os . environ [ 'KERAS_BACKEND' ] = \"theano\" \n        try : \n            from theano import ifelse \n        except ImportError : \n            raise Exception ( \"No backend is found for Keras. \" \"Please install either tensorflow or theano.\" ) \n    from bigdl . keras . converter import DefinitionLoader , WeightLoader \n    if json_path and not hdf5_path : \n        return DefinitionLoader . from_json_path ( json_path ) \n    elif json_path and hdf5_path : \n        return WeightLoader . load_weights_from_json_hdf5 ( json_path , hdf5_path , by_name = by_name ) \n    elif hdf5_path and not json_path : \n        kmodel , bmodel = DefinitionLoader . from_hdf5_path ( hdf5_path ) \n        WeightLoader . load_weights_from_kmodel ( bmodel , kmodel ) \n        return bmodel "}
{"1188": "\ndef load_weights_from_json_hdf5 ( def_json , weights_hdf5 , by_name = 0 ) : \n    bmodel = DefinitionLoader . from_json_path ( def_json ) \n    def_value = BCommon . text_from_path ( def_json ) \n    kmodel = model_from_json ( def_value ) \n    WeightLoader . load_weights_from_hdf5 ( bmodel , kmodel , weights_hdf5 , by_name ) \n    return bmodel "}
{"1196": "\ndef is_spark_below_2_2 ( ) : \n    import pyspark \n    if ( hasattr ( pyspark , \"version\" ) ) : \n        full_version = pyspark . version . __version__ \n        parts = full_version . split ( \".\" ) \n        spark_version = parts [ 0 ] + \".\" + parts [ 1 ] \n        if ( compare_version ( spark_version , \"2.2\" ) >= 0 ) : \n            return 0 \n    return 1 "}
{"1200": "\ndef precompute_future_symbols ( trie , n , allow_spaces = 0 ) : \n    if n == 0 : \n        return \n    if trie . is_terminated and trie . precompute_symbols : \n        return \n    for index , final in enumerate ( trie . final ) : \n        trie . data [ index ] = [ set ( ) for i in range ( n ) ] \n    for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n        node_data [ 0 ] = set ( trie . _get_letters ( index ) ) \n        if allow_spaces and final : \n            node_data [ 0 ] . add ( \" \" ) \n    for d in range ( 1 , n ) : \n        for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n            children = set ( trie . _get_children ( index ) ) \n            for child in children : \n                node_data [ d ] |= trie . data [ child ] [ d - 1 ] \n            if allow_spaces and final : \n                node_data [ d ] |= trie . data [ trie . root ] [ d - 1 ] \n    trie . terminated = 1 "}
{"1201": "\ndef simple_attention ( memory , att_size , mask , keep_prob = 1.0 , scope = \"simple_attention\" ) : \n    with tf . variable_scope ( scope ) : \n        BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) \n        memory_do = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) \n        logits = tf . layers . dense ( tf . layers . dense ( memory_do , att_size , activation = tf . nn . tanh ) , 1 , use_bias = 0 ) \n        logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) \n        att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) \n        res = tf . reduce_sum ( att_weights * memory , axis = 1 ) \n        return res "}
{"1202": "\ndef attention ( inputs , state , att_size , mask , scope = \"attention\" ) : \n    with tf . variable_scope ( scope ) : \n        u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) \n        logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , 1 , use_bias = 0 ) \n        logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) \n        att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) \n        res = tf . reduce_sum ( att_weights * inputs , axis = 1 ) \n        return res , logits "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = 0 ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if possible_matches > 0 : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if possible_matches_by_order [ i ] > 0 : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if min ( precisions ) > 0 : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1204": "\ndef _get_log_file ( self ) : \n    log_dir : Path = Path ( self . config [ 'log_path' ] ) . expanduser ( ) . resolve ( ) / self . agent_name \n    log_dir . mkdir ( parents = 1 , exist_ok = 1 ) \n    log_file_path = Path ( log_dir , f'{self._get_timestamp_utc_str()}_{self.agent_name}.log' ) \n    log_file = open ( log_file_path , 'a' , buffering = 1 , encoding = 'utf8' ) \n    return log_file "}
{"1207": "\ndef dump_weights ( tf_save_dir , outfile , options ) : \n    def _get_outname ( tf_name ) : \n        outname = re . sub ( ':0$' , '' , tf_name ) \n        outname = outname . lstrip ( 'lm/' ) \n        outname = re . sub ( '/rnn/' , '/RNN/' , outname ) \n        outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) \n        outname = re . sub ( '/cell_' , '/Cell' , outname ) \n        outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) \n        if '/RNN/' in outname : \n            if 'projection' in outname : \n                outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) \n            else : \n                outname = re . sub ( '/kernel' , '/W_0' , outname ) \n                outname = re . sub ( '/bias' , '/B' , outname ) \n        return outname \n    ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) \n    config = tf . ConfigProto ( allow_soft_placement = 1 ) \n    with tf . Graph ( ) . as_default ( ) : \n        with tf . Session ( config = config ) as sess : \n            with tf . variable_scope ( 'lm' ) : \n                LanguageModel ( options , 0 ) \n                loader = tf . train . Saver ( ) \n                loader . restore ( sess , ckpt_file ) \n            with h5py . File ( outfile , 'w' ) as fout : \n                for v in tf . trainable_variables ( ) : \n                    if v . name . find ( 'softmax' ) >= 0 : \n                        continue \n                    outname = _get_outname ( v . name ) \n                    shape = v . get_shape ( ) . as_list ( ) \n                    dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) \n                    values = sess . run ( [ v ] ) [ 0 ] \n                    dset [ ... ] = values "}
{"1209": "\ndef train_evaluate_model_from_config ( config : Union [ str , Path , dict ] , iterator : Union [ DataLearningIterator , DataFittingIterator ] = None , * , to_train : bool = 1 , evaluation_targets : Optional [ Iterable [ str ] ] = None , to_validate : Optional [ bool ] = None , download : bool = 0 , start_epoch_num : Optional [ int ] = None , recursive : bool = 0 ) -> Dict [ str , Dict [ str , float ] ] : \n    config = parse_config ( config ) \n    if download : \n        deep_download ( config ) \n    if to_train and recursive : \n        for subconfig in get_all_elems_from_json ( config [ 'chainer' ] , 'config_path' ) : \n            log . info ( f'Training \"{subconfig}\"' ) \n            train_evaluate_model_from_config ( subconfig , download = 0 , recursive = 1 ) \n    import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) \n    if iterator is None : \n        try : \n            data = read_data_by_config ( config ) \n        except ConfigError as e : \n            to_train = 0 \n            log . warning ( f'Skipping training. {e.message}' ) \n        else : \n            iterator = get_iterator_from_config ( config , data ) \n    if 'train' not in config : \n        log . warning ( 'Train config is missing. Populating with default values' ) \n    train_config = config . get ( 'train' ) \n    if start_epoch_num is not None : \n        train_config [ 'start_epoch_num' ] = start_epoch_num \n    if 'evaluation_targets' not in train_config and ( 'validate_best' in train_config or 'test_best' in train_config ) : \n        log . warning ( '\"validate_best\" and \"test_best\" parameters are deprecated.' ' Please, use \"evaluation_targets\" list instead' ) \n        train_config [ 'evaluation_targets' ] = [ ] \n        if train_config . pop ( 'validate_best' , 1 ) : \n            train_config [ 'evaluation_targets' ] . append ( 'valid' ) \n        if train_config . pop ( 'test_best' , 1 ) : \n            train_config [ 'evaluation_targets' ] . append ( 'test' ) \n    trainer_class = get_model ( train_config . pop ( 'class_name' , 'nn_trainer' ) ) \n    trainer = trainer_class ( config [ 'chainer' ] , ** train_config ) \n    if to_train : \n        trainer . train ( iterator ) \n    res = { } \n    if iterator is not None : \n        if to_validate is not None : \n            if evaluation_targets is None : \n                log . warning ( '\"to_validate\" parameter is deprecated and will be removed in future versions.' ' Please, use \"evaluation_targets\" list instead' ) \n                evaluation_targets = [ 'test' ] \n                if to_validate : \n                    evaluation_targets . append ( 'valid' ) \n            else : \n                log . warn ( 'Both \"evaluation_targets\" and \"to_validate\" parameters are specified.' ' \"to_validate\" is deprecated and will be ignored' ) \n        res = trainer . evaluate ( iterator , evaluation_targets , print_reports = 1 ) \n        trainer . get_chainer ( ) . destroy ( ) \n    res = { k : v [ 'metrics' ] for k , v in res . items ( ) } \n    return res "}
{"1210": "\ndef interact_alice ( agent : Agent ) : \n    data = request . get_json ( ) \n    text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) \n    payload = data [ 'request' ] . get ( 'payload' ) \n    session_id = data [ 'session' ] [ 'session_id' ] \n    user_id = data [ 'session' ] [ 'user_id' ] \n    message_id = data [ 'session' ] [ 'message_id' ] \n    dialog_id = DialogID ( user_id , session_id ) \n    response = { 'response' : { 'end_session' : 1 , 'text' : '' } , \"session\" : { 'session_id' : session_id , 'message_id' : message_id , 'user_id' : user_id } , 'version' : '1.0' } \n    agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ 0 ] \n    if isinstance ( agent_response , RichMessage ) : \n        response [ 'response' ] [ 'text' ] = '\\n' . join ( [ j [ 'content' ] for j in agent_response . json ( ) if j [ 'type' ] == 'plain_text' ] ) \n    else : \n        response [ 'response' ] [ 'text' ] = str ( agent_response ) \n    return jsonify ( response ) , 200 "}
{"1213": "\ndef _config_session ( ) : \n    config = tf . ConfigProto ( ) \n    config . gpu_options . allow_growth = 1 \n    config . gpu_options . visible_device_list = '0' \n    return tf . Session ( config = config ) "}
{"1218": "\ndef process_word ( word : str , to_lower : bool = 0 , append_case : Optional [ str ] = None ) -> Tuple [ str ] : \n    if all ( x . isupper ( ) for x in word ) and len ( word ) > 1 : \n        uppercase = \"<ALL_UPPER>\" \n    elif word [ 0 ] . isupper ( ) : \n        uppercase = \"<FIRST_UPPER>\" \n    else : \n        uppercase = None \n    if to_lower : \n        word = word . lower ( ) \n    if word . isdigit ( ) : \n        answer = [ \"<DIGIT>\" ] \n    elif word . startswith ( \"http://\" ) or word . startswith ( \"www.\" ) : \n        answer = [ \"<HTTP>\" ] \n    else : \n        answer = list ( word ) \n    if to_lower and uppercase is not None : \n        if append_case == \"first\" : \n            answer = [ uppercase ] + answer \n        elif append_case == \"last\" : \n            answer = answer + [ uppercase ] \n    return tuple ( answer ) "}
{"1219": "\ndef stacked_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = 0 , use_dilation = 0 , training_ph = None , add_l2_losses = 0 ) : \n    l2_reg = tf . nn . l2_loss if add_l2_losses else None \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        if use_dilation : \n            dilation_rate = 2 ** n_layer \n        else : \n            dilation_rate = 1 \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) , kernel_regularizer = l2_reg ) \n        if use_batch_norm : \n            assert training_ph is not None \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        units = tf . nn . relu ( units ) \n    return units "}
{"1220": "\ndef bi_rnn ( units : tf . Tensor , n_hidden : List , cell_type = 'gru' , seq_lengths = None , trainable_initial_states = 0 , use_peepholes = 0 , name = 'Bi-' ) : \n    with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : \n        if cell_type == 'gru' : \n            forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) \n                initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        elif cell_type == 'lstm' : \n            forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_fw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) \n                initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_bw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        else : \n            raise RuntimeError ( 'cell_type must be either \"gru\" or \"lstm\"s' ) \n        ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths , initial_state_fw = initial_state_fw , initial_state_bw = initial_state_bw ) \n    kernels = [ var for var in forward_cell . trainable_variables + backward_cell . trainable_variables if 'kernel' in var . name ] \n    for kernel in kernels : \n        tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) \n    return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) "}
{"1221": "\ndef stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = 0 , name = 'RNN_layer' ) : \n    for n , n_hidden in enumerate ( n_hidden_list ) : \n        with tf . variable_scope ( name + '_' + str ( n ) ) : \n            if cell_type == 'gru' : \n                forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n                backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n            elif cell_type == 'lstm' : \n                forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n                backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n            else : \n                raise RuntimeError ( 'cell_type must be either gru or lstm' ) \n            ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) \n            units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) \n            if cell_type == 'gru' : \n                last_units = tf . concat ( [ fw , bw ] , axis = 1 ) \n            else : \n                ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw \n                c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) \n                h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) \n                last_units = ( h , c ) \n    return units , last_units "}
{"1222": "\ndef stacked_highway_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = 0 , use_dilation = 0 , training_ph = None ) : \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        input_units = units \n        if input_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : \n            input_units = tf . layers . dense ( input_units , n_hidden ) \n        if use_dilation : \n            dilation_rate = 2 ** n_layer \n        else : \n            dilation_rate = 1 \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) ) \n        if use_batch_norm : \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        sigmoid_gate = tf . layers . dense ( input_units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) \n        input_units = sigmoid_gate * input_units + ( 1 - sigmoid_gate ) * units \n        input_units = tf . nn . relu ( input_units ) \n    units = input_units \n    return units "}
{"1223": "\ndef embedding_layer ( token_indices = None , token_embedding_matrix = None , n_tokens = None , token_embedding_dim = None , name : str = None , trainable = 1 ) : \n    if token_embedding_matrix is not None : \n        tok_mat = token_embedding_matrix \n        if trainable : \n            Warning ( 'Matrix of embeddings is passed to the embedding_layer, ' 'possibly there is a pre-trained embedding matrix. ' 'Embeddings paramenters are set to Trainable!' ) \n    else : \n        tok_mat = np . random . randn ( n_tokens , token_embedding_dim ) . astype ( np . float32 ) / np . sqrt ( token_embedding_dim ) \n    tok_emb_mat = tf . Variable ( tok_mat , name = name , trainable = trainable ) \n    embedded_tokens = tf . nn . embedding_lookup ( tok_emb_mat , token_indices ) \n    return embedded_tokens "}
{"1224": "\ndef cudnn_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = 0 , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = 0 ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        gru = tf . contrib . cudnn_rnn . CudnnGRU ( num_layers = n_layers , num_units = n_hidden ) \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = input_initial_h or init_h \n        h , h_last = gru ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , ) ) \n        h = tf . transpose ( h , ( 1 , 0 , 2 ) ) \n        h_last = tf . squeeze ( h_last , axis = 0 ) [ - 1 ] \n        if seq_lengths is not None : \n            indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n            h_last = tf . gather_nd ( h , indices ) \n        return h , h_last "}
{"1225": "\ndef cudnn_compatible_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = 0 , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = 0 ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = input_initial_h or init_h \n        with tf . variable_scope ( 'cudnn_gru' , reuse = reuse ) : \n            def single_cell ( ) : \n                return tf . contrib . cudnn_rnn . CudnnCompatibleGRUCell ( n_hidden ) \n            cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) \n            units = tf . transpose ( units , ( 1 , 0 , 2 ) ) \n            h , h_last = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = 1 , initial_state = tuple ( tf . unstack ( initial_h , axis = 0 ) ) ) \n            h = tf . transpose ( h , ( 1 , 0 , 2 ) ) \n            h_last = h_last [ - 1 ] \n            if seq_lengths is not None : \n                indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n                h_last = tf . gather_nd ( h , indices ) \n            return h , h_last "}
{"1226": "\ndef cudnn_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = 0 ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        lstm = tf . contrib . cudnn_rnn . CudnnLSTM ( num_layers = n_layers , num_units = n_hidden ) \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n            init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) \n            init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = initial_h or init_h \n        initial_c = initial_c or init_c \n        h , ( h_last , c_last ) = lstm ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , initial_c ) ) \n        h = tf . transpose ( h , ( 1 , 0 , 2 ) ) \n        h_last = h_last [ - 1 ] \n        c_last = c_last [ - 1 ] \n        if seq_lengths is not None : \n            indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n            h_last = tf . gather_nd ( h , indices ) \n        return h , ( h_last , c_last ) "}
{"1227": "\ndef cudnn_compatible_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = 0 ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n            init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) \n            init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = initial_h or init_h \n        initial_c = initial_c or init_c \n        with tf . variable_scope ( 'cudnn_lstm' , reuse = reuse ) : \n            def single_cell ( ) : \n                return tf . contrib . cudnn_rnn . CudnnCompatibleLSTMCell ( n_hidden ) \n            cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) \n            units = tf . transpose ( units , ( 1 , 0 , 2 ) ) \n            init = tuple ( [ tf . nn . rnn_cell . LSTMStateTuple ( ic , ih ) for ih , ic in zip ( tf . unstack ( initial_h , axis = 0 ) , tf . unstack ( initial_c , axis = 0 ) ) ] ) \n            h , state = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = 1 , initial_state = init ) \n            h = tf . transpose ( h , ( 1 , 0 , 2 ) ) \n            h_last = state [ - 1 ] . h \n            c_last = state [ - 1 ] . c \n            if seq_lengths is not None : \n                indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n                h_last = tf . gather_nd ( h , indices ) \n            return h , ( h_last , c_last ) "}
{"1228": "\ndef cudnn_bi_gru ( units , n_hidden , seq_lengths = None , n_layers = 1 , trainable_initial_states = 0 , name = 'cudnn_bi_gru' , reuse = 0 ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if seq_lengths is None : \n            seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] \n        with tf . variable_scope ( 'Forward' ) : \n            h_fw , h_last_fw = cudnn_gru_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) \n        with tf . variable_scope ( 'Backward' ) : \n            reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) \n            h_bw , h_last_bw = cudnn_gru_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) \n            h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) \n    return ( h_fw , h_bw ) , ( h_last_fw , h_last_bw ) "}
{"1229": "\ndef cudnn_bi_lstm ( units , n_hidden , seq_lengths = None , n_layers = 1 , trainable_initial_states = 0 , name = 'cudnn_bi_gru' , reuse = 0 ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if seq_lengths is None : \n            seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] \n        with tf . variable_scope ( 'Forward' ) : \n            h_fw , ( h_fw_last , c_fw_last ) = cudnn_lstm_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) \n        with tf . variable_scope ( 'Backward' ) : \n            reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) \n            h_bw , ( h_bw_last , c_bw_last ) = cudnn_lstm_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) \n            h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) \n        return ( h_fw , h_bw ) , ( ( h_fw_last , c_fw_last ) , ( h_bw_last , c_bw_last ) ) "}
{"1230": "\ndef cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2 , keep_prob = 1.0 , concat_stacked_outputs = 0 , trainable_initial_states = 0 , name = 'cudnn_stacked_bi_gru' , reuse = 0 ) : \n    if seq_lengths is None : \n        seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] \n    outputs = [ units ] \n    with tf . variable_scope ( name , reuse = reuse ) : \n        for n in range ( n_stacks ) : \n            if n == 0 : \n                inputs = outputs [ - 1 ] \n            else : \n                inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) \n            ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = 1 , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) \n            outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) \n    if concat_stacked_outputs : \n        return tf . concat ( outputs [ 1 : ] , axis = 2 ) \n    return outputs [ - 1 ] "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = 0 ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = 0 \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) \n        for _ in range ( self . char_conv_layers - 1 ) : \n            curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if self . conv_dropout > 0.0 : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n        curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ += curr_filters_number \n    if len ( conv_outputs ) > 1 : \n        conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ 0 ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) \n    if self . intermediate_dropout > 0.0 : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    for i in range ( self . char_highway_layers - 1 ) : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if self . highway_dropout > 0.0 : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1234": "\ndef _build_basic_network ( self , word_outputs ) : \n    if self . word_dropout > 0.0 : \n        lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) \n    else : \n        lstm_outputs = word_outputs \n    for j in range ( self . word_lstm_layers - 1 ) : \n        lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = 1 , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = 1 , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = \"softmax\" , activity_regularizer = self . regularizer ) , name = \"p\" ) ( lstm_outputs ) \n    return pre_outputs , lstm_outputs "}
{"1236": "\ndef predict_on_batch ( self , data : Union [ list , tuple ] , return_indexes : bool = 0 ) -> List [ List [ str ] ] : \n    X = self . _transform_batch ( data ) \n    objects_number , lengths = len ( X [ 0 ] ) , [ len ( elem ) for elem in data [ 0 ] ] \n    Y = self . model_ . predict_on_batch ( X ) \n    labels = np . argmax ( Y , axis = - 1 ) \n    answer : List [ List [ str ] ] = [ None ] * objects_number \n    for i , ( elem , length ) in enumerate ( zip ( labels , lengths ) ) : \n        elem = elem [ : length ] \n        answer [ i ] = elem if return_indexes else self . tags . idxs2toks ( elem ) \n    return answer "}
{"1239": "\ndef bleu_advanced ( y_true : List [ Any ] , y_predicted : List [ Any ] , weights : Tuple = ( 1 , ) , smoothing_function = SMOOTH . method1 , auto_reweigh = 0 , penalty = 1 ) -> float : \n    bleu_measure = sentence_bleu ( [ y_true ] , y_predicted , weights , smoothing_function , auto_reweigh ) \n    hyp_len = len ( y_predicted ) \n    hyp_lengths = hyp_len \n    ref_lengths = closest_ref_length ( [ y_true ] , hyp_len ) \n    bpenalty = brevity_penalty ( ref_lengths , hyp_lengths ) \n    if penalty is 1 or bpenalty == 0 : \n        return bleu_measure \n    return bleu_measure / bpenalty "}
{"1242": "\ndef verify_certs_chain ( certs_chain : List [ crypto . X509 ] , amazon_cert : crypto . X509 ) -> bool : \n    store = crypto . X509Store ( ) \n    for cert in certs_chain : \n        store . add_cert ( cert ) \n    default_verify_paths = ssl . get_default_verify_paths ( ) \n    default_verify_file = default_verify_paths . cafile \n    default_verify_file = Path ( default_verify_file ) . resolve ( ) if default_verify_file else None \n    default_verify_path = default_verify_paths . capath \n    default_verify_path = Path ( default_verify_path ) . resolve ( ) if default_verify_path else None \n    ca_files = [ ca_file for ca_file in default_verify_path . iterdir ( ) ] if default_verify_path else [ ] \n    if default_verify_file : \n        ca_files . append ( default_verify_file ) \n    for ca_file in ca_files : \n        ca_file : Path \n        if ca_file . is_file ( ) : \n            with ca_file . open ( 'r' , encoding = 'ascii' ) as crt_f : \n                ca_certs_txt = crt_f . read ( ) \n                ca_certs = extract_certs ( ca_certs_txt ) \n                for cert in ca_certs : \n                    store . add_cert ( cert ) \n    ssl_context = ssl . create_default_context ( ) \n    der_certs = ssl_context . get_ca_certs ( binary_form = 1 ) \n    pem_certs = '\\n' . join ( [ ssl . DER_cert_to_PEM_cert ( der_cert ) for der_cert in der_certs ] ) \n    ca_certs = extract_certs ( pem_certs ) \n    for ca_cert in ca_certs : \n        store . add_cert ( ca_cert ) \n    store_context = crypto . X509StoreContext ( store , amazon_cert ) \n    try : \n        store_context . verify_certificate ( ) \n        result = 1 \n    except crypto . X509StoreContextError : \n        result = 0 \n    return result "}
{"1243": "\ndef verify_signature ( amazon_cert : crypto . X509 , signature : str , request_body : bytes ) -> bool : \n    signature = base64 . b64decode ( signature ) \n    try : \n        crypto . verify ( amazon_cert , signature , request_body , 'sha1' ) \n        result = 1 \n    except crypto . Error : \n        result = 0 \n    return result "}
{"1249": "\ndef main ( ) : \n    args = parser . parse_args ( ) \n    path = get_settings_path ( ) \n    if args . default : \n        if populate_settings_dir ( force = 1 ) : \n            print ( f'Populated {path} with default settings files' ) \n        else : \n            print ( f'{path} is already a default settings directory' ) \n    else : \n        print ( f'Current DeepPavlov settings path: {path}' ) "}
{"1259": "\ndef populate_settings_dir ( force : bool = 0 ) -> bool : \n    res = 0 \n    if _default_settings_path == _settings_path : \n        return res \n    for src in list ( _default_settings_path . glob ( '**/*.json' ) ) : \n        dest = _settings_path / src . relative_to ( _default_settings_path ) \n        if not force and dest . exists ( ) : \n            continue \n        res = 1 \n        dest . parent . mkdir ( parents = 1 , exist_ok = 1 ) \n        shutil . copy ( src , dest ) \n    return res "}
{"1263": "\ndef search ( self , word , d , allow_spaces = 1 , return_cost = 1 ) : \n    if not all ( ( c in self . alphabet or ( c == \" \" and self . allow_spaces ) ) for c in word ) : \n        return [ ] \n    return self . _trie_search ( word , d , allow_spaces = allow_spaces , return_cost = return_cost ) "}
{"1264": "\ndef _make_default_operation_costs ( self , allow_spaces = 0 ) : \n    self . operation_costs = dict ( ) \n    self . operation_costs [ \"\" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } \n    for a in self . alphabet : \n        current_costs = { c : 1.0 for c in self . alphabet } \n        current_costs [ a ] = 0.0 \n        current_costs [ \"\" ] = 1.0 \n        if allow_spaces : \n            current_costs [ \" \" ] = 1.0 \n        self . operation_costs [ a ] = current_costs \n    for a , b in itertools . permutations ( self . alphabet , 2 ) : \n        self . operation_costs [ a + b ] = { b + a : 1.0 } \n    if allow_spaces : \n        self . operation_costs [ \" \" ] = { c : 1.0 for c in self . alphabet } \n        self . operation_costs [ \" \" ] [ \"\" ] = 1.0 "}
{"1270": "\ndef _handle_launch ( self , request : dict ) -> dict : \n    response = { 'response' : { 'shouldEndSession' : 0 , 'outputSpeech' : { 'type' : 'PlainText' , 'text' : self . config [ 'start_message' ] } , 'card' : { 'type' : 'Simple' , 'content' : self . config [ 'start_message' ] } } } \n    response = self . _generate_response ( response , request ) \n    return response "}
{"1271": "\ndef _handle_unsupported ( self , request : dict ) -> dict : \n    response = { 'response' : { 'shouldEndSession' : 0 , 'outputSpeech' : { 'type' : 'PlainText' , 'text' : self . config [ 'unsupported_message' ] } , 'card' : { 'type' : 'Simple' , 'content' : self . config [ 'unsupported_message' ] } } } \n    response = self . _generate_response ( response , request ) \n    return response "}
{"1274": "\ndef build_model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load_trained : bool = 0 , download : bool = 0 , serialized : Optional [ bytes ] = None ) -> Chainer : \n    config = parse_config ( config ) \n    if serialized : \n        serialized : list = pickle . loads ( serialized ) \n    if download : \n        deep_download ( config ) \n    import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) \n    model_config = config [ 'chainer' ] \n    model = Chainer ( model_config [ 'in' ] , model_config [ 'out' ] , model_config . get ( 'in_y' ) ) \n    for component_config in model_config [ 'pipe' ] : \n        if load_trained and ( 'fit_on' in component_config or 'in_y' in component_config ) : \n            try : \n                component_config [ 'load_path' ] = component_config [ 'save_path' ] \n            except KeyError : \n                log . warning ( 'No \"save_path\" parameter for the {} component, so \"load_path\" will not be renewed' . format ( component_config . get ( 'class_name' , component_config . get ( 'ref' , 'UNKNOWN' ) ) ) ) \n        if serialized and 'in' in component_config : \n            component_serialized = serialized . pop ( 0 ) \n        else : \n            component_serialized = None \n        component = from_params ( component_config , mode = mode , serialized = component_serialized ) \n        if 'in' in component_config : \n            c_in = component_config [ 'in' ] \n            c_out = component_config [ 'out' ] \n            in_y = component_config . get ( 'in_y' , None ) \n            main = component_config . get ( 'main' , 0 ) \n            model . append ( component , c_in , c_out , in_y , main ) \n    return model "}
{"1275": "\ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while 1 : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if len ( model . out_params ) > 1 : \n            pred = zip ( * pred ) \n        print ( '>>' , * pred ) "}
{"1276": "\ndef predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : \n    if file_path is None or file_path == '-' : \n        if sys . stdin . isatty ( ) : \n            raise RuntimeError ( 'To process data from terminal please use interact mode' ) \n        f = sys . stdin \n    else : \n        f = open ( file_path , encoding = 'utf8' ) \n    model : Chainer = build_model ( config ) \n    args_count = len ( model . in_x ) \n    while 1 : \n        batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) \n        if not batch : \n            break \n        args = [ ] \n        for i in range ( args_count ) : \n            args . append ( batch [ i : : args_count ] ) \n        res = model ( * args ) \n        if len ( model . out_params ) == 1 : \n            res = [ res ] \n        for res in zip ( * res ) : \n            res = json . dumps ( res , ensure_ascii = 0 ) \n            print ( res , flush = 1 ) \n    if f is not sys . stdin : \n        f . close ( ) "}
{"1277": "\ndef read_infile ( infile : Union [ Path , str ] , from_words = 0 , word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , tag_column : int = TAG_COLUMN , max_sents : int = - 1 , read_only_words : bool = 0 ) -> List [ Tuple [ List , Union [ List , None ] ] ] : \n    answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] \n    if from_words : \n        word_column , read_only_words = 0 , 1 \n    with open ( infile , \"r\" , encoding = \"utf8\" ) as fin : \n        for line in fin : \n            line = line . strip ( ) \n            if line . startswith ( \"#\" ) : \n                continue \n            if line == \"\" : \n                if len ( curr_word_sent ) > 0 : \n                    if read_only_words : \n                        curr_tag_sent = None \n                    answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n                curr_tag_sent , curr_word_sent = [ ] , [ ] \n                if len ( answer ) == max_sents : \n                    break \n                continue \n            splitted = line . split ( \"\\t\" ) \n            index = splitted [ 0 ] \n            if not from_words and not index . isdigit ( ) : \n                continue \n            curr_word_sent . append ( splitted [ word_column ] ) \n            if not read_only_words : \n                pos , tag = splitted [ pos_column ] , splitted [ tag_column ] \n                tag = pos if tag == \"_\" else \"{},{}\" . format ( pos , tag ) \n                curr_tag_sent . append ( tag ) \n        if len ( curr_word_sent ) > 0 : \n            if read_only_words : \n                curr_tag_sent = None \n            answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n    return answer "}
{"1289": "\ndef main ( ) : \n    args = parser . parse_args ( ) \n    run_ms_bot_framework_server ( agent_generator = make_agent , app_id = args . ms_id , app_secret = args . ms_secret , stateful = 1 ) "}
{"1290": "\ndef download ( dest_file_path : [ List [ Union [ str , Path ] ] ] , source_url : str , force_download = 1 ) : \n    if isinstance ( dest_file_path , list ) : \n        dest_file_paths = [ Path ( path ) for path in dest_file_path ] \n    else : \n        dest_file_paths = [ Path ( dest_file_path ) . absolute ( ) ] \n    if not force_download : \n        to_check = list ( dest_file_paths ) \n        dest_file_paths = [ ] \n        for p in to_check : \n            if p . exists ( ) : \n                log . info ( f'File already exists in {p}' ) \n            else : \n                dest_file_paths . append ( p ) \n    if dest_file_paths : \n        cache_dir = os . getenv ( 'DP_CACHE_DIR' ) \n        cached_exists = 0 \n        if cache_dir : \n            first_dest_path = Path ( cache_dir ) / md5 ( source_url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] \n            cached_exists = first_dest_path . exists ( ) \n        else : \n            first_dest_path = dest_file_paths . pop ( ) \n        if not cached_exists : \n            first_dest_path . parent . mkdir ( parents = 1 , exist_ok = 1 ) \n            simple_download ( source_url , first_dest_path ) \n        else : \n            log . info ( f'Found cached {source_url} in {first_dest_path}' ) \n        for dest_path in dest_file_paths : \n            dest_path . parent . mkdir ( parents = 1 , exist_ok = 1 ) \n            shutil . copy ( str ( first_dest_path ) , str ( dest_path ) ) "}
{"1292": "\ndef download_decompress ( url : str , download_path : [ Path , str ] , extract_paths = None ) : \n    file_name = Path ( urlparse ( url ) . path ) . name \n    download_path = Path ( download_path ) \n    if extract_paths is None : \n        extract_paths = [ download_path ] \n    elif isinstance ( extract_paths , list ) : \n        extract_paths = [ Path ( path ) for path in extract_paths ] \n    else : \n        extract_paths = [ Path ( extract_paths ) ] \n    cache_dir = os . getenv ( 'DP_CACHE_DIR' ) \n    extracted = 0 \n    if cache_dir : \n        cache_dir = Path ( cache_dir ) \n        url_hash = md5 ( url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] \n        arch_file_path = cache_dir / url_hash \n        extracted_path = cache_dir / ( url_hash + '_extracted' ) \n        extracted = extracted_path . exists ( ) \n        if not extracted and not arch_file_path . exists ( ) : \n            simple_download ( url , arch_file_path ) \n    else : \n        arch_file_path = download_path / file_name \n        simple_download ( url , arch_file_path ) \n        extracted_path = extract_paths . pop ( ) \n    if not extracted : \n        log . info ( 'Extracting {} archive into {}' . format ( arch_file_path , extracted_path ) ) \n        extracted_path . mkdir ( parents = 1 , exist_ok = 1 ) \n        if file_name . endswith ( '.tar.gz' ) : \n            untar ( arch_file_path , extracted_path ) \n        elif file_name . endswith ( '.gz' ) : \n            ungzip ( arch_file_path , extracted_path / Path ( file_name ) . with_suffix ( '' ) . name ) \n        elif file_name . endswith ( '.zip' ) : \n            with zipfile . ZipFile ( arch_file_path , 'r' ) as zip_ref : \n                zip_ref . extractall ( extracted_path ) \n        else : \n            raise RuntimeError ( f'Trying to extract an unknown type of archive {file_name}' ) \n        if not cache_dir : \n            arch_file_path . unlink ( ) \n    for extract_path in extract_paths : \n        for src in extracted_path . iterdir ( ) : \n            dest = extract_path / src . name \n            if src . is_dir ( ) : \n                copytree ( src , dest ) \n            else : \n                extract_path . mkdir ( parents = 1 , exist_ok = 1 ) \n                shutil . copy ( str ( src ) , str ( dest ) ) "}
{"1295": "\ndef set_query_parameter ( url , param_name , param_value ) : \n    scheme , netloc , path , query_string , fragment = urlsplit ( url ) \n    query_params = parse_qs ( query_string ) \n    query_params [ param_name ] = [ param_value ] \n    new_query_string = urlencode ( query_params , doseq = 1 ) \n    return urlunsplit ( ( scheme , netloc , path , new_query_string , fragment ) ) "}
{"1296": "\ndef alexa ( self ) -> dict : \n    response = { 'response' : { 'shouldEndSession' : 0 , 'outputSpeech' : { 'type' : 'PlainText' , 'text' : self . content } , 'card' : { 'type' : 'Simple' , 'content' : self . content } } } \n    return response "}
{"1303": "\ndef check_gpu_existence ( ) : \n    global _gpu_available \n    if _gpu_available is None : \n        sess_config = tf . ConfigProto ( ) \n        sess_config . gpu_options . allow_growth = 1 \n        try : \n            with tf . Session ( config = sess_config ) : \n                device_list = device_lib . list_local_devices ( ) \n                _gpu_available = any ( device . device_type == 'GPU' for device in device_list ) \n        except AttributeError as e : \n            log . warning ( f'Got an AttributeError `{e}`, assuming documentation building' ) \n            _gpu_available = 0 \n    return _gpu_available "}
{"1308": "\ndef run ( self ) -> None : \n    while 1 : \n        request = self . input_queue . get ( ) \n        response = self . _handle_request ( request ) \n        self . output_queue . put ( response ) "}
{"1311": "\ndef _verify_request ( self , signature_chain_url : str , signature : str , request_body : bytes ) -> bool : \n    if signature_chain_url not in self . valid_certificates . keys ( ) : \n        amazon_cert : X509 = verify_cert ( signature_chain_url ) \n        if amazon_cert : \n            amazon_cert_lifetime : timedelta = self . config [ 'amazon_cert_lifetime' ] \n            expiration_timestamp = datetime . utcnow ( ) + amazon_cert_lifetime \n            validated_cert = ValidatedCert ( cert = amazon_cert , expiration_timestamp = expiration_timestamp ) \n            self . valid_certificates [ signature_chain_url ] = validated_cert \n            log . info ( f'Certificate {signature_chain_url} validated' ) \n        else : \n            log . error ( f'Certificate {signature_chain_url} validation failed' ) \n            return 0 \n    else : \n        validated_cert : ValidatedCert = self . valid_certificates [ signature_chain_url ] \n        amazon_cert : X509 = validated_cert . cert \n    if verify_signature ( amazon_cert , signature , request_body ) : \n        result = 1 \n    else : \n        log . error ( f'Failed signature verification for request: {request_body.decode(\"utf-8\", \"replace\")}' ) \n        result = 0 \n    return result "}
{"1319": "\ndef shutdown ( self , prompt = 0 ) : \n    if not self . is_running ( ) : \n        return \n    assert_is_type ( prompt , bool ) \n    if prompt : \n        question = \"Are you sure you want to shutdown the H2O instance running at %s (Y/N)? \" % h2o . connection ( ) . base_url \n        response = input ( question ) \n    else : \n        response = \"Y\" \n    if response . lower ( ) in { \"y\" , \"yes\" } : \n        h2o . api ( \"POST /3/Shutdown\" ) \n        h2o . connection ( ) . close ( ) "}
{"1320": "\ndef is_running ( self ) : \n    try : \n        if h2o . connection ( ) . local_server and not h2o . connection ( ) . local_server . is_running ( ) : \n            return 0 \n        h2o . api ( \"GET /\" ) \n        return 1 \n    except ( H2OConnectionError , H2OServerError ) : \n        return 0 "}
{"1321": "\ndef show_status ( self , detailed = 0 ) : \n    if self . _retrieved_at + self . REFRESH_INTERVAL < time . time ( ) : \n        new_info = h2o . api ( \"GET /3/Cloud\" ) \n        self . _fill_from_h2ocluster ( new_info ) \n    ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) \n    allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) \n    free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) \n    unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) \n    status = \"locked\" if self . locked else \"accepting new members\" \n    if unhealthy_nodes == 0 : \n        status += \", healthy\" \n    else : \n        status += \", %d nodes are not healthy\" % unhealthy_nodes \n    api_extensions = self . list_api_extensions ( ) \n    H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) \n    if detailed : \n        keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] \n        header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] \n        table = [ [ k ] for k in keys ] \n        for node in self . nodes : \n            for i , k in enumerate ( keys ) : \n                table [ i ] . append ( node [ k ] ) \n        H2ODisplay ( table = table , header = header ) "}
{"1327": "\ndef summary ( self , key , column = \"C1\" , timeoutSecs = 10 , ** kwargs ) : \n    params_dict = { } \n    h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'summary' , 1 ) \n    result = self . do_json_request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeoutSecs , params = params_dict ) \n    h2o_sandbox . check_sandbox_for_errors ( ) \n    return result "}
{"1328": "\ndef delete_frame ( self , key , ignoreMissingKey = 1 , timeoutSecs = 60 , ** kwargs ) : \n    assert key is not None , '\"key\" parameter is null' \n    result = self . do_json_request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) \n    if not ignoreMissingKey and 'f00b4r' in result : \n        raise ValueError ( 'Frame key not found: ' + key ) \n    return result "}
{"1329": "\ndef model_builders ( self , algo = None , timeoutSecs = 10 , ** kwargs ) : \n    params_dict = { } \n    h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'model_builders' , 0 ) \n    request = '3/ModelBuilders.json' \n    if algo : \n        request += \"/\" + algo \n    result = self . do_json_request ( request , timeout = timeoutSecs , params = params_dict ) \n    h2o_sandbox . check_sandbox_for_errors ( ) \n    return result "}
{"1330": "\ndef validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60 , ** kwargs ) : \n    assert algo is not None , '\"algo\" parameter is null' \n    assert parameters is not None , '\"parameters\" parameter is null' \n    model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) \n    assert model_builders is not None , \"/ModelBuilders REST call failed\" \n    assert algo in model_builders [ 'model_builders' ] \n    builder = model_builders [ 'model_builders' ] [ algo ] \n    if training_frame is not None : \n        frames = self . frames ( key = training_frame ) \n        assert frames is not None , \"/Frames/{0} REST call failed\" . format ( training_frame ) \n        key_name = frames [ 'frames' ] [ 0 ] [ 'key' ] [ 'name' ] \n        assert key_name == training_frame , \"/Frames/{0} returned Frame {1} rather than Frame {2}\" . format ( training_frame , key_name , training_frame ) \n        parameters [ 'training_frame' ] = training_frame \n    result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + \"/parameters\" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = 1 , noExtraErrorCheck = 1 ) \n    verboseprint ( \"model parameters validation: \" + repr ( result ) ) \n    return result "}
{"1333": "\ndef delete_model ( self , key , ignoreMissingKey = 1 , timeoutSecs = 60 , ** kwargs ) : \n    assert key is not None , '\"key\" parameter is null' \n    result = self . do_json_request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) \n    if not ignoreMissingKey and 'f00b4r' in result : \n        raise ValueError ( 'Model key not found: ' + key ) \n    verboseprint ( \"delete_model result:\" , dump_json ( result ) ) \n    return result "}
{"1334": "\ndef _tabulate ( self , tablefmt = \"simple\" , rollups = 0 , rows = 10 ) : \n    if not self . is_valid ( ) : \n        self . fill ( rows = rows ) \n    d = collections . OrderedDict ( ) \n    if rollups : \n        col = next ( iter ( viewvalues ( self . _data ) ) ) \n        lrows = len ( col [ 'data' ] ) \n        d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) \n    for k , v in viewitems ( self . _data ) : \n        x = v [ 'data' ] \n        t = v [ \"type\" ] \n        if t == \"enum\" : \n            domain = v [ 'domain' ] \n            x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] \n        elif t == \"time\" : \n            x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] \n        if rollups : \n            mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None \n            maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None \n            if v [ 'type' ] == \"enum\" : \n                v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None \n            x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x \n        d [ k ] = x \n    return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt ) "}
{"1340": "\ndef wait_for_ssh ( ips , port = 22 , skipAlive = 1 , requiredsuccess = 3 ) : \n    log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) \n    for ip in ips : \n        if not skipAlive or not ssh_live ( ip , port ) : \n            log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) \n            count = 0 \n            while count < requiredsuccess : \n                if ssh_live ( ip , port ) : \n                    count += 1 \n                else : \n                    count = 0 \n                time . sleep ( 1 ) \n                h2o_cmd . dot ( ) "}
{"1344": "\ndef _wrap ( text , wrap_at = 120 , indent = 4 ) : \n    out = \"\" \n    curr_line_length = indent \n    space_needed = 0 \n    for word in text . split ( ) : \n        if curr_line_length + len ( word ) > wrap_at : \n            out += \"\\n\" + \" \" * indent \n            curr_line_length = indent \n            space_needed = 0 \n        if space_needed : \n            out += \" \" \n            curr_line_length += 1 \n        out += word \n        curr_line_length += len ( word ) \n        space_needed = 1 \n    return out "}
{"1345": "\ndef join ( self ) : \n    self . _future = 0 \n    self . _job . poll ( ) \n    model_key = self . _job . dest_key \n    self . _job = None \n    model_json = h2o . api ( \"GET /%d/Models/%s\" % ( self . _rest_version , model_key ) ) [ \"models\" ] [ 0 ] \n    self . _resolve_model ( model_key , model_json ) "}
{"1346": "\ndef train ( self , x = None , y = None , training_frame = None , offset_column = None , fold_column = None , weights_column = None , validation_frame = None , max_runtime_secs = None , ignored_columns = None , model_id = None , verbose = 0 ) : \n    self . _train ( x = x , y = y , training_frame = training_frame , offset_column = offset_column , fold_column = fold_column , weights_column = weights_column , validation_frame = validation_frame , max_runtime_secs = max_runtime_secs , ignored_columns = ignored_columns , model_id = model_id , verbose = verbose ) "}
{"1347": "\ndef fit ( self , X , y = None , ** params ) : \n    stk = inspect . stack ( ) [ 1 : ] \n    warn = 1 \n    for s in stk : \n        mod = inspect . getmodule ( s [ 0 ] ) \n        if mod : \n            warn = \"sklearn\" not in mod . __name__ \n            if not warn : \n                break \n    if warn : \n        warnings . warn ( \"\\n\\n\\t`fit` is not recommended outside of the sklearn framework. Use `train` instead.\" , UserWarning , stacklevel = 2 ) \n    training_frame = X . cbind ( y ) if y is not None else X \n    x = X . names \n    y = y . names [ 0 ] if y is not None else None \n    self . train ( x , y , training_frame , ** params ) \n    return self "}
{"1348": "\ndef get_params ( self , deep = 1 ) : \n    out = dict ( ) \n    for key , value in self . parms . items ( ) : \n        if deep and isinstance ( value , H2OEstimator ) : \n            deep_items = list ( value . get_params ( ) . items ( ) ) \n            out . update ( ( key + \"__\" + k , val ) for k , val in deep_items ) \n        out [ key ] = value \n    return out "}
{"1349": "\ndef signal_handler ( signum , stackframe ) : \n    global g_runner \n    global g_handling_signal \n    if g_handling_signal : \n        return \n    g_handling_signal = 1 \n    print ( \"\" ) \n    print ( \"----------------------------------------------------------------------\" ) \n    print ( \"\" ) \n    print ( \"SIGNAL CAUGHT (\" + str ( signum ) + \").  TEARING DOWN CLOUDS.\" ) \n    print ( \"\" ) \n    print ( \"----------------------------------------------------------------------\" ) \n    g_runner . terminate ( ) "}
{"1358": "\ndef roc ( self , train = 0 , valid = 0 , xval = 0 ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in viewitems ( tm ) : \n        if v is not None : \n            m [ k ] = ( v . fprs , v . tprs ) \n    return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m "}
{"1372": "\ndef check ( self , var ) : \n    if not isinstance ( var , _str_type ) : \n        return 0 \n    return _enum_mangle ( var ) in self . _consts "}
{"1374": "\ndef _read_config ( self ) : \n    self . _config_loaded = 1 \n    conf = [ ] \n    for f in self . _candidate_log_files ( ) : \n        if os . path . isfile ( f ) : \n            self . _logger . info ( \"Reading config file %s\" % f ) \n            section_rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) \n            keyvalue_rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) \n            with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config_file : \n                section_name = None \n                for lineno , line in enumerate ( config_file ) : \n                    line = line . strip ( ) \n                    if line == \"\" or line . startswith ( \"#\" ) : \n                        continue \n                    m1 = section_rx . match ( line ) \n                    if m1 : \n                        section_name = m1 . group ( 1 ) \n                        continue \n                    m2 = keyvalue_rx . match ( line ) \n                    if m2 : \n                        lng = m2 . group ( 1 ) \n                        key = m2 . group ( 2 ) \n                        val = m2 . group ( 3 ) . strip ( ) \n                        if lng and lng . lower ( ) != \"py:\" : \n                            continue \n                        if section_name : \n                            key = section_name + \".\" + key \n                        if key in H2OConfigReader . _allowed_config_keys : \n                            conf . append ( ( key , val ) ) \n                        else : \n                            self . _logger . error ( \"Key %s is not a valid config key\" % key ) \n                        continue \n                    self . _logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) \n            self . _config = dict ( conf ) \n            return "}
{"1375": "\ndef _candidate_log_files ( ) : \n    relpath = \".h2oconfig\" \n    prevpath = None \n    while 1 : \n        abspath = os . path . abspath ( relpath ) \n        if abspath == prevpath : \n            break \n        prevpath = abspath \n        relpath = \"../\" + relpath \n        yield abspath \n    yield os . path . expanduser ( \"~/.h2oconfig\" ) "}
{"1376": "\ndef execute ( self , progress_fn , print_verbose_info = None ) : \n    assert_is_type ( progress_fn , FunctionType , GeneratorType , MethodType ) \n    if isinstance ( progress_fn , GeneratorType ) : \n        progress_fn = ( lambda g : lambda : next ( g ) ) ( progress_fn ) \n    self . _next_poll_time = 0 \n    self . _t0 = time . time ( ) \n    self . _x0 = 0 \n    self . _v0 = 0.01 \n    self . _ve = 0.01 \n    progress = 0 \n    status = None \n    try : \n        while 1 : \n            now = time . time ( ) \n            if self . _next_poll_time <= now : \n                res = progress_fn ( ) \n                assert_is_type ( res , ( numeric , numeric ) , numeric ) \n                if not isinstance ( res , tuple ) : \n                    res = ( res , - 1 ) \n                now = time . time ( ) \n                self . _store_model_progress ( res , now ) \n                self . _recalculate_model_parameters ( now ) \n            progress = min ( self . _compute_progress_at_time ( now ) [ 0 ] , 1 ) \n            if progress == 1 and self . _get_real_progress ( ) >= 1 : \n                break \n            result = self . _widget . render ( progress ) \n            assert_is_type ( result , RenderResult ) \n            time0 = result . next_time \n            time1 = self . _get_time_at_progress ( result . next_progress ) \n            next_render_time = min ( time0 , time1 ) \n            self . _draw ( result . rendered ) \n            wait_time = min ( next_render_time , self . _next_poll_time ) - now \n            if wait_time > 0 : \n                time . sleep ( wait_time ) \n                if print_verbose_info is not None : \n                    print_verbose_info ( progress ) \n    except KeyboardInterrupt : \n        status = \"cancelled\" \n    except StopIteration as e : \n        status = str ( e ) \n    result = self . _widget . render ( progress = progress , status = status ) \n    self . _draw ( result . rendered , final = 1 ) \n    if status == \"cancelled\" : \n        raise StopIteration ( status ) "}
{"1383": "\ndef _draw ( self , txt , final = 0 ) : \n    if not self . _file_mode : \n        sys . stdout . write ( \"\\r\" ) \n    sys . stdout . write ( txt ) \n    if final and not isinstance ( self . _widget , _HiddenWidget ) : \n        sys . stdout . write ( \"\\n\" ) \n    else : \n        if not self . _file_mode : \n            sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) "}
{"1384": "\ndef _compute_widget_sizes ( self ) : \n    wl = [ 0 ] * len ( self . _widgets ) \n    flex_count = 0 \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            flex_count += 1 \n        else : \n            wl [ i ] = widget . render ( 1 ) . length \n    remaining_width = self . _width - sum ( wl ) \n    remaining_width -= len ( self . _widgets ) - 1 \n    if remaining_width < 10 * flex_count : \n        if self . _file_mode : \n            remaining_width = 10 * flex_count \n        else : \n            widget0 = self . _widgets [ 0 ] \n            if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( 0 ) . length >= 10 * flex_count : \n                remaining_width += widget0 . render ( 0 ) . length + 1 \n                self . _to_render = widget0 . render ( 0 ) . rendered + \"\\n\" \n                self . _widgets = self . _widgets [ 1 : ] \n            if remaining_width < 10 * flex_count : \n                self . _file_mode = 1 \n                remaining_width = 10 * flex_count \n    remaining_width = max ( remaining_width , 10 * flex_count ) \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            target_length = int ( remaining_width / flex_count ) \n            result = widget . render ( 1 , target_length ) \n            wl [ i ] = result . length \n            remaining_width -= result . length \n            flex_count -= 1 \n    return wl "}
{"1388": "\ndef get_frame ( frame_id , rows = 10 , rows_offset = 0 , cols = - 1 , full_cols = - 1 , cols_offset = 0 , light = 0 ) : \n    fr = H2OFrame ( ) \n    fr . _ex . _cache . _id = frame_id \n    try : \n        fr . _ex . _cache . fill ( rows = rows , rows_offset = rows_offset , cols = cols , full_cols = full_cols , cols_offset = cols_offset , light = light ) \n    except EnvironmentError : \n        return None \n    return fr "}
{"1389": "\ndef refresh ( self ) : \n    self . _ex . _cache . flush ( ) \n    self . _frame ( fill_cache = 1 ) "}
{"1390": "\ndef type ( self , col ) : \n    assert_is_type ( col , int , str ) \n    if not self . _ex . _cache . types_valid ( ) or not self . _ex . _cache . names_valid ( ) : \n        self . _ex . _cache . flush ( ) \n        self . _frame ( fill_cache = 1 ) \n    types = self . _ex . _cache . types \n    if is_type ( col , str ) : \n        if col in types : \n            return types [ col ] \n    else : \n        names = self . _ex . _cache . names \n        if - len ( names ) <= col < len ( names ) : \n            return types [ names [ col ] ] \n    raise H2OValueError ( \"Column '%r' does not exist in the frame\" % col ) "}
{"1392": "\ndef summary ( self , return_data = 0 ) : \n    if not self . _has_content ( ) : \n        print ( \"This H2OFrame is empty and not initialized.\" ) \n        return self . _ex . _cache . _data ; \n    if not self . _ex . _cache . is_valid ( ) : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    if not return_data : \n        if self . nrows == 0 : \n            print ( \"This H2OFrame is empty.\" ) \n        elif H2ODisplay . _in_ipy ( ) : \n            import IPython . display \n            IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , 1 ) , raw = 1 ) \n        else : \n            print ( self . _ex . _cache . _tabulate ( \"simple\" , 1 ) ) \n    else : \n        return self . _ex . _cache . _data "}
{"1393": "\ndef describe ( self , chunk_summary = 0 ) : \n    if self . _has_content ( ) : \n        res = h2o . api ( \"GET /3/Frames/%s\" % self . frame_id , data = { \"row_count\" : 10 } ) [ \"frames\" ] [ 0 ] \n        self . _ex . _cache . _fill_data ( res ) \n        print ( \"Rows:{}\" . format ( self . nrow ) ) \n        print ( \"Cols:{}\" . format ( self . ncol ) ) \n        if chunk_summary : \n            res [ \"chunk_summary\" ] . show ( ) \n            res [ \"distribution_summary\" ] . show ( ) \n        print ( \"\\n\" ) \n    self . summary ( ) "}
{"1394": "\ndef head ( self , rows = 10 , cols = 200 ) : \n    assert_is_type ( rows , int ) \n    assert_is_type ( cols , int ) \n    nrows = min ( self . nrows , rows ) \n    ncols = min ( self . ncols , cols ) \n    newdt = self [ : nrows , : ncols ] \n    return newdt . _frame ( rows = nrows , cols = cols , fill_cache = 1 ) "}
{"1396": "\ndef levels ( self ) : \n    lol = H2OFrame . _expr ( expr = ExprNode ( \"levels\" , self ) ) . as_data_frame ( 0 ) \n    lol . pop ( 0 ) \n    lol = list ( zip ( * lol ) ) \n    return [ [ ll for ll in l if ll != '' ] for l in lol ] "}
{"1399": "\ndef set_levels ( self , levels ) : \n    assert_is_type ( levels , [ str ] ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"setDomain\" , self , 0 , levels ) , cache = self . _ex . _cache ) "}
{"1406": "\ndef structure ( self ) : \n    df = self . as_data_frame ( use_pandas = 0 ) \n    cn = df . pop ( 0 ) \n    nr = self . nrow \n    nc = self . ncol \n    width = max ( [ len ( c ) for c in cn ] ) \n    isfactor = self . isfactor ( ) \n    numlevels = self . nlevels ( ) \n    lvls = self . levels ( ) \n    print ( \"H2OFrame: '{}' \\nDimensions: {} obs. of {} variables\" . format ( self . frame_id , nr , nc ) ) \n    for i in range ( nc ) : \n        print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) \n        if isfactor [ i ] : \n            nl = numlevels [ i ] \n            print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) \n        else : \n            print ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as_list ( self [ : 10 , i ] , 0 ) [ 1 : ] ) ) ) "}
{"1407": "\ndef as_data_frame ( self , use_pandas = 1 , header = 1 ) : \n    if can_use_pandas ( ) and use_pandas : \n        import pandas \n        return pandas . read_csv ( StringIO ( self . get_frame_data ( ) ) , low_memory = 0 , skip_blank_lines = 0 ) \n    from h2o . utils . csv . readers import reader \n    frame = [ row for row in reader ( StringIO ( self . get_frame_data ( ) ) ) ] \n    if not header : \n        frame . pop ( 0 ) \n    return frame "}
{"1417": "\ndef merge ( self , other , all_x = 0 , all_y = 0 , by_x = None , by_y = None , method = \"auto\" ) : \n    if by_x is None and by_y is None : \n        common_names = list ( set ( self . names ) & set ( other . names ) ) \n        if not common_names : \n            raise H2OValueError ( \"No columns in common to merge on!\" ) \n    if by_x is None : \n        by_x = [ self . names . index ( c ) for c in common_names ] \n    else : \n        by_x = _getValidCols ( by_x , self ) \n    if by_y is None : \n        by_y = [ other . names . index ( c ) for c in common_names ] \n    else : \n        by_y = _getValidCols ( by_y , other ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"merge\" , self , other , all_x , all_y , by_x , by_y , method ) ) "}
{"1420": "\ndef var ( self , y = None , na_rm = 0 , use = None ) : \n    symmetric = 0 \n    if y is None : \n        y = self \n        symmetric = 1 \n    if use is None : \n        use = \"complete.obs\" if na_rm else \"everything\" \n    if self . nrow == 1 or ( self . ncol == 1 and y . ncol == 1 ) : \n        return ExprNode ( \"var\" , self , y , use , symmetric ) . _eager_scalar ( ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"var\" , self , y , use , symmetric ) ) . _frame ( ) "}
{"1421": "\ndef cor ( self , y = None , na_rm = 0 , use = None ) : \n    assert_is_type ( y , H2OFrame , None ) \n    assert_is_type ( na_rm , bool ) \n    assert_is_type ( use , None , \"everything\" , \"all.obs\" , \"complete.obs\" ) \n    if y is None : \n        y = self \n    if use is None : \n        use = \"complete.obs\" if na_rm else \"everything\" \n    if self . nrow == 1 or ( self . ncol == 1 and y . ncol == 1 ) : \n        return ExprNode ( \"cor\" , self , y , use ) . _eager_scalar ( ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"cor\" , self , y , use ) ) . _frame ( ) "}
{"1430": "\ndef table ( self , data2 = None , dense = 1 ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"table\" , self , data2 , dense ) ) if data2 is not None else H2OFrame . _expr ( expr = ExprNode ( \"table\" , self , dense ) ) "}
{"1431": "\ndef hist ( self , breaks = \"sturges\" , plot = 1 , ** kwargs ) : \n    server = kwargs . pop ( \"server\" ) if \"server\" in kwargs else 0 \n    assert_is_type ( breaks , int , [ numeric ] , Enum ( \"sturges\" , \"rice\" , \"sqrt\" , \"doane\" , \"fd\" , \"scott\" ) ) \n    assert_is_type ( plot , bool ) \n    assert_is_type ( server , bool ) \n    if kwargs : \n        raise H2OValueError ( \"Unknown parameters to hist(): %r\" % kwargs ) \n    hist = H2OFrame . _expr ( expr = ExprNode ( \"hist\" , self , breaks ) ) . _frame ( ) \n    if plot : \n        try : \n            import matplotlib \n            if server : \n                matplotlib . use ( \"Agg\" , warn = 0 ) \n            import matplotlib . pyplot as plt \n        except ImportError : \n            print ( \"ERROR: matplotlib is required to make the histogram plot. \" \"Set `plot` to False, if a plot is not desired.\" ) \n            return \n        hist [ \"widths\" ] = hist [ \"breaks\" ] . difflag1 ( ) \n        lefts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"breaks\" ] , use_pandas = 0 ) [ 2 : ] ] \n        widths = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"widths\" ] , use_pandas = 0 ) [ 2 : ] ] \n        counts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"counts\" ] , use_pandas = 0 ) [ 2 : ] ] \n        plt . xlabel ( self . names [ 0 ] ) \n        plt . ylabel ( \"Frequency\" ) \n        plt . title ( \"Histogram of %s\" % self . names [ 0 ] ) \n        plt . bar ( left = lefts , width = widths , height = counts , bottom = 0 ) \n        if not server : \n            plt . show ( ) \n    else : \n        hist [ \"density\" ] = hist [ \"counts\" ] / ( hist [ \"breaks\" ] . difflag1 ( ) * hist [ \"counts\" ] . sum ( ) ) \n        return hist "}
{"1432": "\ndef isax ( self , num_words , max_cardinality , optimize_card = 0 , ** kwargs ) : \n    if num_words <= 0 : \n        raise H2OValueError ( \"num_words must be greater than 0\" ) \n    if max_cardinality <= 0 : \n        raise H2OValueError ( \"max_cardinality must be greater than 0\" ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"isax\" , self , num_words , max_cardinality , optimize_card ) ) "}
{"1433": "\ndef sub ( self , pattern , replacement , ignore_case = 0 ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"replacefirst\" , self , pattern , replacement , ignore_case ) ) "}
{"1435": "\ndef grep ( self , pattern , ignore_case = 0 , invert = 0 , output_logical = 0 ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"grep\" , self , pattern , ignore_case , invert , output_logical ) ) "}
{"1442": "\ndef cut ( self , breaks , labels = None , include_lowest = 0 , right = 1 , dig_lab = 3 ) : \n    assert_is_type ( breaks , [ numeric ] ) \n    if self . ncols != 1 : \n        raise H2OValueError ( \"Single-column frame is expected\" ) \n    if self . types [ self . names [ 0 ] ] not in { \"int\" , \"real\" } : \n        raise H2OValueError ( \"A numeric column is expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"cut\" , self , breaks , labels , include_lowest , right , dig_lab ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . types = { k : \"enum\" for k in self . names } \n    return fr "}
{"1443": "\ndef idxmax ( self , skipna = 1 , axis = 0 ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"which.max\" , self , skipna , axis ) ) "}
{"1445": "\ndef parse_text ( text ) : \n    assert isinstance ( text , _str_type ) , \"`text` parameter should be a string, got %r\" % type ( text ) \n    gen = iter ( text . splitlines ( 1 ) ) \n    readline = gen . next if hasattr ( gen , \"next\" ) else gen . __next__ \n    return Code ( _tokenize ( readline ) ) "}
{"1449": "\ndef size ( self , train = 0 , valid = 0 , xval = 0 ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in tm . items ( ) : \n        m [ k ] = None if v is None else [ v [ 2 ] for v in v . _metric_json [ \"centroid_stats\" ] . cell_values ] \n    return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m "}
{"1452": "\ndef connect ( server = None , url = None , ip = None , port = None , https = None , verify_ssl_certificates = None , auth = None , proxy = None , cookies = None , verbose = 1 , config = None ) : \n    global h2oconn \n    if config : \n        if \"connect_params\" in config : \n            h2oconn = _connect_with_conf ( config [ \"connect_params\" ] ) \n        else : \n            h2oconn = _connect_with_conf ( config ) \n    else : \n        h2oconn = H2OConnection . open ( server = server , url = url , ip = ip , port = port , https = https , auth = auth , verify_ssl_certificates = verify_ssl_certificates , proxy = proxy , cookies = cookies , verbose = verbose ) \n        if verbose : \n            h2oconn . cluster . show_status ( ) \n    return h2oconn "}
{"1457": "\ndef import_file ( path = None , destination_frame = None , parse = 1 , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , pattern = None , skipped_columns = None , custom_non_data_line_markers = None ) : \n    coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) \n    natype = U ( str , [ str ] ) \n    assert_is_type ( path , str , [ str ] ) \n    assert_is_type ( pattern , str , None ) \n    assert_is_type ( destination_frame , str , None ) \n    assert_is_type ( parse , bool ) \n    assert_is_type ( header , - 1 , 0 , 1 ) \n    assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == 1 ) ) \n    assert_is_type ( col_names , [ str ] , None ) \n    assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) \n    assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) \n    assert isinstance ( skipped_columns , ( type ( None ) , list ) ) , \"The skipped_columns should be an list of column names!\" \n    check_frame_id ( destination_frame ) \n    patharr = path if isinstance ( path , list ) else [ path ] \n    if any ( os . path . split ( p ) [ 0 ] == \"~\" for p in patharr ) : \n        raise H2OValueError ( \"Paths relative to a current user (~) are not valid in the server environment. \" \"Please use absolute paths if possible.\" ) \n    if not parse : \n        return lazy_import ( path , pattern ) \n    else : \n        return H2OFrame ( ) . _import_parse ( path , pattern , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns , custom_non_data_line_markers ) "}
{"1458": "\ndef import_hive_table ( database = None , table = None , partitions = None , allow_multi_format = 0 ) : \n    assert_is_type ( database , str , None ) \n    assert_is_type ( table , str ) \n    assert_is_type ( partitions , [ [ str ] ] , None ) \n    p = { \"database\" : database , \"table\" : table , \"partitions\" : partitions , \"allow_multi_format\" : allow_multi_format } \n    j = H2OJob ( api ( \"POST /3/ImportHiveTable\" , data = p ) , \"Import Hive Table\" ) . poll ( ) \n    return get_frame ( j . dest_key ) "}
{"1459": "\ndef import_sql_table ( connection_url , table , username , password , columns = None , optimize = 1 , fetch_mode = None ) : \n    assert_is_type ( connection_url , str ) \n    assert_is_type ( table , str ) \n    assert_is_type ( username , str ) \n    assert_is_type ( password , str ) \n    assert_is_type ( columns , [ str ] , None ) \n    assert_is_type ( optimize , bool ) \n    assert_is_type ( fetch_mode , str , None ) \n    p = { \"connection_url\" : connection_url , \"table\" : table , \"username\" : username , \"password\" : password , \"fetch_mode\" : fetch_mode } \n    if columns : \n        p [ \"columns\" ] = \", \" . join ( columns ) \n    j = H2OJob ( api ( \"POST /99/ImportSQLTable\" , data = p ) , \"Import SQL Table\" ) . poll ( ) \n    return get_frame ( j . dest_key ) "}
{"1460": "\ndef import_sql_select ( connection_url , select_query , username , password , optimize = 1 , use_temp_table = None , temp_table_name = None , fetch_mode = None ) : \n    assert_is_type ( connection_url , str ) \n    assert_is_type ( select_query , str ) \n    assert_is_type ( username , str ) \n    assert_is_type ( password , str ) \n    assert_is_type ( optimize , bool ) \n    assert_is_type ( use_temp_table , bool , None ) \n    assert_is_type ( temp_table_name , str , None ) \n    assert_is_type ( fetch_mode , str , None ) \n    p = { \"connection_url\" : connection_url , \"select_query\" : select_query , \"username\" : username , \"password\" : password , \"use_temp_table\" : use_temp_table , \"temp_table_name\" : temp_table_name , \"fetch_mode\" : fetch_mode } \n    j = H2OJob ( api ( \"POST /99/ImportSQLTable\" , data = p ) , \"Import SQL Table\" ) . poll ( ) \n    return get_frame ( j . dest_key ) "}
{"1462": "\ndef deep_copy ( data , xid ) : \n    assert_is_type ( data , H2OFrame ) \n    assert_is_type ( xid , str ) \n    assert_satisfies ( xid , xid != data . frame_id ) \n    check_frame_id ( xid ) \n    duplicate = data . apply ( lambda x : x ) \n    duplicate . _ex = ExprNode ( \"assign\" , xid , duplicate ) . _eval_driver ( 0 ) \n    duplicate . _ex . _cache . _id = xid \n    duplicate . _ex . _children = None \n    return duplicate "}
{"1466": "\ndef download_pojo ( model , path = \"\" , get_jar = 1 , jar_name = \"\" ) : \n    assert_is_type ( model , ModelBase ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if not model . have_pojo : \n        raise H2OValueError ( \"Export to POJO not supported\" ) \n    if path == \"\" : \n        java_code = api ( \"GET /3/Models.java/%s\" % model . model_id ) \n        print ( java_code ) \n        return None \n    else : \n        filename = api ( \"GET /3/Models.java/%s\" % model . model_id , save_to = path ) \n        if get_jar : \n            if jar_name == \"\" : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) \n            else : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , jar_name ) ) \n        return filename "}
{"1469": "\ndef export_file ( frame , path , force = 0 , parts = 1 ) : \n    assert_is_type ( frame , H2OFrame ) \n    assert_is_type ( path , str ) \n    assert_is_type ( force , bool ) \n    assert_is_type ( parts , int ) \n    H2OJob ( api ( \"POST /3/Frames/%s/export\" % ( frame . frame_id ) , data = { \"path\" : path , \"num_parts\" : parts , \"force\" : force } ) , \"Export File\" ) . poll ( ) "}
{"1470": "\ndef as_list ( data , use_pandas = 1 , header = 1 ) : \n    assert_is_type ( data , H2OFrame ) \n    assert_is_type ( use_pandas , bool ) \n    assert_is_type ( header , bool ) \n    return H2OFrame . as_data_frame ( data , use_pandas = use_pandas , header = header ) "}
{"1471": "\ndef demo ( funcname , interactive = 1 , echo = 1 , test = 0 ) : \n    import h2o . demos as h2odemo \n    assert_is_type ( funcname , str ) \n    assert_is_type ( interactive , bool ) \n    assert_is_type ( echo , bool ) \n    assert_is_type ( test , bool ) \n    demo_function = getattr ( h2odemo , funcname , None ) \n    if demo_function and type ( demo_function ) is type ( demo ) : \n        demo_function ( interactive , echo , test ) \n    else : \n        print ( \"Demo for %s is not available.\" % funcname ) "}
{"1474": "\ndef _put_key ( file_path , dest_key = None , overwrite = 1 ) : \n    ret = api ( \"POST /3/PutKey?destination_key={}&overwrite={}\" . format ( dest_key if dest_key else '' , overwrite ) , filename = file_path ) \n    return ret [ \"destination_key\" ] "}
{"1480": "\ndef mojo_predict_pandas ( dataframe , mojo_zip_path , genmodel_jar_path = None , classpath = None , java_options = None , verbose = 0 ) : \n    tmp_dir = tempfile . mkdtemp ( ) \n    try : \n        if not can_use_pandas ( ) : \n            raise RuntimeException ( 'Cannot import pandas' ) \n        import pandas \n        assert_is_type ( dataframe , pandas . DataFrame ) \n        input_csv_path = os . path . join ( tmp_dir , 'input.csv' ) \n        prediction_csv_path = os . path . join ( tmp_dir , 'prediction.csv' ) \n        dataframe . to_csv ( input_csv_path ) \n        mojo_predict_csv ( input_csv_path = input_csv_path , mojo_zip_path = mojo_zip_path , output_csv_path = prediction_csv_path , genmodel_jar_path = genmodel_jar_path , classpath = classpath , java_options = java_options , verbose = verbose ) \n        return pandas . read_csv ( prediction_csv_path ) \n    finally : \n        shutil . rmtree ( tmp_dir ) "}
{"1481": "\ndef mojo_predict_csv ( input_csv_path , mojo_zip_path , output_csv_path = None , genmodel_jar_path = None , classpath = None , java_options = None , verbose = 0 ) : \n    default_java_options = '-Xmx4g -XX:ReservedCodeCacheSize=256m' \n    prediction_output_file = 'prediction.csv' \n    java = H2OLocalServer . _find_java ( ) \n    H2OLocalServer . _check_java ( java = java , verbose = verbose ) \n    if verbose : \n        print ( \"input_csv:\\t%s\" % input_csv_path ) \n    if not os . path . isfile ( input_csv_path ) : \n        raise RuntimeError ( \"Input csv cannot be found at %s\" % input_csv_path ) \n    mojo_zip_path = os . path . abspath ( mojo_zip_path ) \n    if verbose : \n        print ( \"mojo_zip:\\t%s\" % mojo_zip_path ) \n    if not os . path . isfile ( mojo_zip_path ) : \n        raise RuntimeError ( \"MOJO zip cannot be found at %s\" % mojo_zip_path ) \n    parent_dir = os . path . dirname ( mojo_zip_path ) \n    if output_csv_path is None : \n        output_csv_path = os . path . join ( parent_dir , prediction_output_file ) \n    if genmodel_jar_path is None : \n        genmodel_jar_path = os . path . join ( parent_dir , gen_model_file_name ) \n    if verbose : \n        print ( \"genmodel_jar:\\t%s\" % genmodel_jar_path ) \n    if not os . path . isfile ( genmodel_jar_path ) : \n        raise RuntimeError ( \"Genmodel jar cannot be found at %s\" % genmodel_jar_path ) \n    if verbose and output_csv_path is not None : \n        print ( \"output_csv:\\t%s\" % output_csv_path ) \n    if classpath is None : \n        classpath = genmodel_jar_path \n    if verbose : \n        print ( \"classpath:\\t%s\" % classpath ) \n    if java_options is None : \n        java_options = default_java_options \n    if verbose : \n        print ( \"java_options:\\t%s\" % java_options ) \n    cmd = [ java ] \n    for option in java_options . split ( ' ' ) : \n        cmd += [ option ] \n    cmd += [ \"-cp\" , classpath , h2o_predictor_class , \"--mojo\" , mojo_zip_path , \"--input\" , input_csv_path , '--output' , output_csv_path , '--decimal' ] \n    if verbose : \n        cmd_str = \" \" . join ( cmd ) \n        print ( \"java cmd:\\t%s\" % cmd_str ) \n    subprocess . check_call ( cmd , shell = 0 ) \n    with open ( output_csv_path ) as csv_file : \n        result = list ( csv . DictReader ( csv_file ) ) \n    return result "}
{"1482": "\ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack \n            print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) \n            print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) \n            print ( \"        ^^^^ %s\" % message ) \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = 1 \n        return decorator_invisible \n    return deprecated_decorator "}
{"1483": "\ndef join ( self ) : \n    self . _future = 0 \n    self . _job . poll ( ) \n    self . _job = None "}
{"1485": "\ndef summary ( self , header = 1 ) : \n    table = [ ] \n    for model in self . models : \n        model_summary = model . _model_json [ \"output\" ] [ \"model_summary\" ] \n        r_values = list ( model_summary . cell_values [ 0 ] ) \n        r_values [ 0 ] = model . model_id \n        table . append ( r_values ) \n    print ( ) \n    if header : \n        print ( 'Grid Summary:' ) \n    print ( ) \n    H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ 1 : ] , numalign = \"left\" , stralign = \"left\" ) "}
{"1487": "\ndef get_hyperparams ( self , id , display = 1 ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ 0 ] ) \n    res = [ model . params [ h ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ h ] [ 'actual' ] , list ) else model . params [ h ] [ 'actual' ] for h in self . hyper_params ] \n    if display : \n        print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) \n    return res "}
{"1488": "\ndef get_hyperparams_dict ( self , id , display = 1 ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    model_params = dict ( ) \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ 0 ] ) \n    for param_name in self . hyper_names : \n        model_params [ param_name ] = model . params [ param_name ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ param_name ] [ 'actual' ] , list ) else model . params [ param_name ] [ 'actual' ] \n    if display : \n        print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) \n    return model_params "}
{"1490": "\ndef F1 ( self , thresholds = None , train = 0 , valid = 0 , xval = 0 ) : \n    return { model . model_id : model . F1 ( thresholds , train , valid , xval ) for model in self . models } "}
{"1491": "\ndef varimp ( self , use_pandas = 0 ) : \n    model = self . _model_json [ \"output\" ] \n    if \"importance\" in list ( model . keys ( ) ) and model [ \"importance\" ] : \n        vals = model [ \"importance\" ] . cell_values \n        header = model [ \"importance\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        print ( \"Warning: This model doesn't have importances of components.\" ) "}
{"1492": "\ndef proj_archetypes ( self , test_data , reverse_transform = 0 ) : \n    if test_data is None or test_data . nrow == 0 : \n        raise ValueError ( \"Must specify test data\" ) \n    j = h2o . api ( \"POST /3/Predictions/models/%s/frames/%s\" % ( self . model_id , test_data . frame_id ) , data = { \"project_archetypes\" : 1 , \"reverse_transform\" : reverse_transform } ) \n    return h2o . get_frame ( j [ \"model_metrics\" ] [ 0 ] [ \"predictions\" ] [ \"frame_id\" ] [ \"name\" ] ) "}
{"1493": "\ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = 0 ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) \n    if type == \"barplot\" : \n        plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) \n    elif type == \"lines\" : \n        plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1500": "\ndef start_logging ( self , dest = None ) : \n    assert_is_type ( dest , None , str , type ( sys . stdout ) ) \n    if dest is None : \n        dest = os . path . join ( tempfile . mkdtemp ( ) , \"h2o-connection.log\" ) \n    self . _print ( \"Now logging all API requests to file %r\" % dest ) \n    self . _is_logging = 1 \n    self . _logging_dest = dest "}
{"1507": "\ndef _print ( self , msg , flush = 0 , end = \"\\n\" ) : \n    if self . _verbose : \n        print2 ( msg , end = end , flush = flush ) "}
{"1508": "\ndef get_automl ( project_name ) : \n    automl_json = h2o . api ( \"GET /99/AutoML/%s\" % project_name ) \n    project_name = automl_json [ \"project_name\" ] \n    leaderboard_list = [ key [ \"name\" ] for key in automl_json [ 'leaderboard' ] [ 'models' ] ] \n    if leaderboard_list is not None and len ( leaderboard_list ) > 0 : \n        leader_id = leaderboard_list [ 0 ] \n    else : \n        leader_id = None \n    leader = h2o . get_model ( leader_id ) \n    is_progress = H2OJob . __PROGRESS_BAR__ \n    h2o . no_progress ( ) \n    try : \n        leaderboard = h2o . H2OFrame ( automl_json [ \"leaderboard_table\" ] . cell_values , column_names = automl_json [ \"leaderboard_table\" ] . col_header ) \n    except Exception as ex : \n        raise ex \n    finally : \n        if is_progress is 1 : \n            h2o . show_progress ( ) \n    leaderboard = leaderboard [ 1 : ] \n    automl_dict = { 'project_name' : project_name , \"leader\" : leader , \"leaderboard\" : leaderboard } \n    return automl_dict "}
{"1509": "\ndef download_pojo ( self , path = \"\" , get_genmodel_jar = 0 , genmodel_name = \"\" ) : \n    return h2o . download_pojo ( self . leader , path , get_jar = get_genmodel_jar , jar_name = genmodel_name ) "}
{"1510": "\ndef download_mojo ( self , path = \".\" , get_genmodel_jar = 0 , genmodel_name = \"\" ) : \n    return ModelBase . download_mojo ( self . leader , path , get_genmodel_jar , genmodel_name ) "}
{"1511": "\ndef fit ( self , X , y = None , ** params ) : \n    if isinstance ( self . parms [ \"center\" ] , ( tuple , list ) ) : \n        self . _means = self . parms [ \"center\" ] \n    if isinstance ( self . parms [ \"scale\" ] , ( tuple , list ) ) : \n        self . _stds = self . parms [ \"scale\" ] \n    if self . means is None and self . parms [ \"center\" ] : \n        self . _means = X . mean ( return_frame = 1 ) . getrow ( ) \n    else : \n        self . _means = 0 \n    if self . stds is None and self . parms [ \"scale\" ] : \n        self . _stds = X . sd ( ) \n    else : \n        self . _stds = 0 \n    return self "}
{"1515": "\ndef find_node_name ( each_line , temp_func_list ) : \n    global g_node_name \n    global g_failed_test_info_dict \n    if g_node_name in each_line : \n        temp_strings = each_line . split ( ) \n        [ start , found , endstr ] = each_line . partition ( g_node_name ) \n        if found : \n            temp_strings = endstr . split ( ) \n            g_failed_test_info_dict [ \"6.node_name\" ] = extract_true_string ( temp_strings [ 1 ] ) \n            temp_func_list . remove ( find_node_name ) \n    return 1 "}
{"1516": "\ndef find_git_hash_branch ( each_line , temp_func_list ) : \n    global g_git_hash_branch \n    global g_failed_test_info_dict \n    if g_git_hash_branch in each_line : \n        [ start , found , endstr ] = each_line . partition ( g_git_hash_branch ) \n        temp_strings = endstr . strip ( ) . split ( ) \n        if len ( temp_strings ) > 1 : \n            g_failed_test_info_dict [ \"4.git_hash\" ] = temp_strings [ 0 ] \n            g_failed_test_info_dict [ \"5.git_branch\" ] = temp_strings [ 1 ] \n        temp_func_list . remove ( find_git_hash_branch ) \n    return 1 "}
{"1517": "\ndef find_build_timeout ( each_line , temp_func_list ) : \n    global g_build_timeout \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    if g_build_timeout in each_line : \n        g_failed_test_info_dict [ \"8.build_timeout\" ] = 'Yes' \n        g_failure_occurred = 1 \n        return 0 \n    else : \n        return 1 "}
{"1518": "\ndef find_build_failure ( each_line , temp_func_list ) : \n    global g_build_success \n    global g_build_success_tests \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    global g_build_failed_message \n    for ind in range ( 0 , len ( g_build_failed_message ) ) : \n        if g_build_failed_message [ ind ] in each_line . lower ( ) : \n            if ( ( ind == 0 ) and ( len ( g_failed_jobs ) > 0 ) ) : \n                continue \n            else : \n                g_failure_occurred = 1 \n                g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' \n                temp_func_list . remove ( find_build_failure ) \n                return 0 \n    return 1 "}
{"1519": "\ndef find_build_id ( each_line , temp_func_list ) : \n    global g_before_java_file \n    global g_java_filenames \n    global g_build_id_text \n    global g_jenkins_url \n    global g_output_filename \n    global g_output_pickle_filename \n    if g_build_id_text in each_line : \n        [ startStr , found , endStr ] = each_line . partition ( g_build_id_text ) \n        g_failed_test_info_dict [ \"2.build_id\" ] = endStr . strip ( ) \n        temp_func_list . remove ( find_build_id ) \n        g_jenkins_url = os . path . join ( 'http://' , g_jenkins_url , 'view' , g_view_name , 'job' , g_failed_test_info_dict [ \"1.jobName\" ] , g_failed_test_info_dict [ \"2.build_id\" ] , 'artifact' ) \n    return 1 "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = 0 \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if len ( found ) > 0 : \n                    if len ( g_current_testname ) > 0 : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( len ( temp_strings ) >= 6 ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : \n                if g_toContinue == 1 : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = 0 \n            else : \n                if g_toContinue : \n                    tempMessage += each_line \n            if ( ( len ( temp_strings ) > 5 ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) \n                if found and ( len ( endStr . strip ( ) ) > 0 ) : \n                    tempMessage += endStr \n                    messageType = temp_strings [ 5 ] \n                    g_toContinue = 1 \n        java_file . close ( ) "}
{"1522": "\ndef save_dict ( ) : \n    global g_test_root_dir \n    global g_output_filename_failed_tests \n    global g_output_filename_passed_tests \n    global g_output_pickle_filename \n    global g_failed_test_info_dict \n    if \"2.build_id\" not in g_failed_test_info_dict . keys ( ) : \n        g_failed_test_info_dict [ \"2.build_id\" ] = \"unknown\" \n    build_id = g_failed_test_info_dict [ \"2.build_id\" ] \n    g_output_filename_failed_tests = g_output_filename_failed_tests + '_build_' + build_id + '_failed_tests.log' \n    g_output_filename_passed_tests = g_output_filename_passed_tests + '_build_' + build_id + '_passed_tests.log' \n    g_output_pickle_filename = g_output_pickle_filename + '_build_' + build_id + '.pickle' \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    with open ( g_output_pickle_filename , 'wb' ) as test_file : \n        pickle . dump ( g_failed_test_info_dict , test_file ) \n    text_file_failed_tests = open ( g_output_filename_failed_tests , 'w' ) \n    text_file_passed_tests = None \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    write_passed_tests = 0 \n    if ( \"passed_tests_info *********\" in allKeys ) : \n        text_file_passed_tests = open ( g_output_filename_passed_tests , 'w' ) \n        write_passed_tests = 1 \n    for keyName in allKeys : \n        val = g_failed_test_info_dict [ keyName ] \n        if isinstance ( val , list ) : \n            if ( len ( val ) == 3 ) : \n                if keyName == \"failed_tests_info *********\" : \n                    write_test_java_message ( keyName , val , text_file_failed_tests ) \n                if keyName == \"passed_tests_info *********\" : \n                    write_test_java_message ( keyName , val , text_file_passed_tests ) \n            elif ( len ( val ) == 2 ) : \n                write_java_message ( keyName , val , text_file_failed_tests ) \n                if write_passed_tests : \n                    write_java_message ( keyName , val , text_file_passed_tests ) \n        else : \n            write_general_build_message ( keyName , val , text_file_failed_tests ) \n            if write_passed_tests : \n                write_general_build_message ( keyName , val , text_file_passed_tests ) \n    text_file_failed_tests . close ( ) \n    if write_passed_tests : \n        text_file_passed_tests . close ( ) "}
{"1528": "\ndef find_synonyms ( self , word , count = 20 ) : \n    j = h2o . api ( \"GET /3/Word2VecSynonyms\" , data = { 'model' : self . model_id , 'word' : word , 'count' : count } ) \n    return OrderedDict ( sorted ( zip ( j [ 'synonyms' ] , j [ 'scores' ] ) , key = lambda t : t [ 1 ] , reverse = 1 ) ) "}
{"1529": "\ndef poll ( self , verbose_model_scoring_history = 0 ) : \n    try : \n        hidden = not H2OJob . __PROGRESS_BAR__ \n        pb = ProgressBar ( title = self . _job_type + \" progress\" , hidden = hidden ) \n        if verbose_model_scoring_history : \n            pb . execute ( self . _refresh_job_status , print_verbose_info = lambda x : self . _print_verbose_info ( ) if int ( x * 10 ) % 5 == 0 else \" \" ) \n        else : \n            pb . execute ( self . _refresh_job_status ) \n    except StopIteration as e : \n        if str ( e ) == \"cancelled\" : \n            h2o . api ( \"POST /3/Jobs/%s/cancel\" % self . job_key ) \n            self . status = \"CANCELLED\" \n    assert self . status in { \"DONE\" , \"CANCELLED\" , \"FAILED\" } or self . _poll_count <= 0 , \"Polling finished while the job has status %s\" % self . status \n    if self . warnings : \n        for w in self . warnings : \n            warnings . warn ( w ) \n    if self . status == \"CANCELLED\" : \n        raise H2OJobCancelled ( \"Job<%s> was cancelled by the user.\" % self . job_key ) \n    if self . status == \"FAILED\" : \n        if ( isinstance ( self . job , dict ) ) and ( \"stacktrace\" in list ( self . job ) ) : \n            raise EnvironmentError ( \"Job with key {} failed with an exception: {}\\nstacktrace: \" \"\\n{}\" . format ( self . job_key , self . exception , self . job [ \"stacktrace\" ] ) ) \n        else : \n            raise EnvironmentError ( \"Job with key %s failed with an exception: %s\" % ( self . job_key , self . exception ) ) \n    return self "}
{"1530": "\ndef to_pojo ( self , pojo_name = \"\" , path = \"\" , get_jar = 1 ) : \n    assert_is_type ( pojo_name , str ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if pojo_name == \"\" : \n        pojo_name = \"AssemblyPOJO_\" + str ( uuid . uuid4 ( ) ) \n    java = h2o . api ( \"GET /99/Assembly.java/%s/%s\" % ( self . id , pojo_name ) ) \n    file_path = path + \"/\" + pojo_name + \".java\" \n    if path == \"\" : \n        print ( java ) \n    else : \n        with open ( file_path , 'w' , encoding = \"utf-8\" ) as f : \n            f . write ( java ) \n    if get_jar and path != \"\" : \n        h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) "}
{"1538": "\ndef varimp ( self , use_pandas = 0 ) : \n    model = self . _model_json [ \"output\" ] \n    if self . algo == 'glm' or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : \n        if self . algo == 'glm' : \n            tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values \n            maxVal = 0 \n            sum = 0 \n            for item in tempvals : \n                sum = sum + item [ 1 ] \n                if item [ 1 ] > maxVal : \n                    maxVal = item [ 1 ] \n            vals = [ ] \n            for item in tempvals : \n                tempT = ( item [ 0 ] , item [ 1 ] , item [ 1 ] / maxVal , item [ 1 ] / sum ) \n                vals . append ( tempT ) \n            header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] \n        else : \n            vals = model [ \"variable_importances\" ] . cell_values \n            header = model [ \"variable_importances\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        print ( \"Warning: This model doesn't have variable importances\" ) "}
{"1539": "\ndef residual_degrees_of_freedom ( self , train = 0 , valid = 0 , xval = 0 ) : \n    if xval : \n        raise H2OValueError ( \"Cross-validation metrics are not available.\" ) \n    if not train and not valid : \n        train = 1 \n    if train and valid : \n        train = 1 \n    if train : \n        return self . _model_json [ \"output\" ] [ \"training_metrics\" ] . residual_degrees_of_freedom ( ) \n    else : \n        return self . _model_json [ \"output\" ] [ \"validation_metrics\" ] . residual_degrees_of_freedom ( ) "}
{"1541": "\ndef download_pojo ( self , path = \"\" , get_genmodel_jar = 0 , genmodel_name = \"\" ) : \n    assert_is_type ( path , str ) \n    assert_is_type ( get_genmodel_jar , bool ) \n    path = path . rstrip ( \"/\" ) \n    return h2o . download_pojo ( self , path , get_jar = get_genmodel_jar , jar_name = genmodel_name ) "}
{"1542": "\ndef download_mojo ( self , path = \".\" , get_genmodel_jar = 0 , genmodel_name = \"\" ) : \n    assert_is_type ( path , str ) \n    assert_is_type ( get_genmodel_jar , bool ) \n    if not self . have_mojo : \n        raise H2OValueError ( \"Export to MOJO not supported\" ) \n    if get_genmodel_jar : \n        if genmodel_name == \"\" : \n            h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) \n        else : \n            h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , genmodel_name ) ) \n    return h2o . api ( \"GET /3/Models/%s/mojo\" % self . model_id , save_to = path ) "}
{"1543": "\ndef save_model_details ( self , path = \"\" , force = 0 ) : \n    assert_is_type ( path , str ) \n    assert_is_type ( force , bool ) \n    path = os . path . join ( os . getcwd ( ) if path == \"\" else path , self . model_id + \".json\" ) \n    return h2o . api ( \"GET /99/Models/%s/json\" % self . model_id , data = { \"dir\" : path , \"force\" : force } ) [ \"dir\" ] "}
{"1546": "\ndef gbm ( interactive = 1 , echo = 1 , testing = 0 ) : \n    def demo_body ( go ) : \n        go ( ) \n        h2o . init ( ) \n        go ( ) \n        prostate = h2o . load_dataset ( \"prostate\" ) \n        go ( ) \n        prostate . describe ( ) \n        go ( ) \n        train , test = prostate . split_frame ( ratios = [ 0.70 ] ) \n        go ( ) \n        train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) \n        test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) \n        go ( ) \n        from h2o . estimators import H2OGradientBoostingEstimator \n        prostate_gbm = H2OGradientBoostingEstimator ( distribution = \"bernoulli\" , ntrees = 10 , max_depth = 8 , min_rows = 10 , learn_rate = 0.2 ) \n        prostate_gbm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training_frame = train ) \n        go ( ) \n        prostate_gbm . show ( ) \n        go ( ) \n        predictions = prostate_gbm . predict ( test ) \n        predictions . show ( ) \n        go ( ) \n        from h2o . tree import H2OTree , H2ONode \n        tree = H2OTree ( prostate_gbm , 0 , \"0\" ) \n        len ( tree ) \n        tree . left_children \n        tree . right_children \n        tree . root_node . show ( ) \n        go ( ) \n        performance = prostate_gbm . model_performance ( test ) \n        performance . show ( ) \n    _run_demo ( demo_body , interactive , echo , testing ) "}
{"1547": "\ndef deeplearning ( interactive = 1 , echo = 1 , testing = 0 ) : \n    def demo_body ( go ) : \n        go ( ) \n        h2o . init ( ) \n        go ( ) \n        prostate = h2o . load_dataset ( \"prostate\" ) \n        go ( ) \n        prostate . describe ( ) \n        go ( ) \n        train , test = prostate . split_frame ( ratios = [ 0.70 ] ) \n        go ( ) \n        train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) \n        test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) \n        go ( ) \n        from h2o . estimators import H2ODeepLearningEstimator \n        prostate_dl = H2ODeepLearningEstimator ( activation = \"Tanh\" , hidden = [ 10 , 10 , 10 ] , epochs = 10000 ) \n        prostate_dl . train ( x = list ( set ( prostate . col_names ) - { \"ID\" , \"CAPSULE\" } ) , y = \"CAPSULE\" , training_frame = train ) \n        go ( ) \n        prostate_dl . show ( ) \n        go ( ) \n        predictions = prostate_dl . predict ( test ) \n        predictions . show ( ) \n        go ( ) \n        performance = prostate_dl . model_performance ( test ) \n        performance . show ( ) \n    _run_demo ( demo_body , interactive , echo , testing ) "}
{"1548": "\ndef glm ( interactive = 1 , echo = 1 , testing = 0 ) : \n    def demo_body ( go ) : \n        go ( ) \n        h2o . init ( ) \n        go ( ) \n        prostate = h2o . load_dataset ( \"prostate\" ) \n        go ( ) \n        prostate . describe ( ) \n        go ( ) \n        train , test = prostate . split_frame ( ratios = [ 0.70 ] ) \n        go ( ) \n        train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) \n        test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) \n        go ( ) \n        from h2o . estimators import H2OGeneralizedLinearEstimator \n        prostate_glm = H2OGeneralizedLinearEstimator ( family = \"binomial\" , alpha = [ 0.5 ] ) \n        prostate_glm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training_frame = train ) \n        go ( ) \n        prostate_glm . show ( ) \n        go ( ) \n        predictions = prostate_glm . predict ( test ) \n        predictions . show ( ) \n        go ( ) \n        performance = prostate_glm . model_performance ( test ) \n        performance . show ( ) \n    _run_demo ( demo_body , interactive , echo , testing ) "}
{"1551": "\ndef show ( self , header = 1 ) : \n    if header and self . _table_header : \n        print ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            print ( self . _table_description ) \n    print ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if nr > 20 : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if nr > 20 and can_use_pandas ( ) : \n        print ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1552": "\ndef start ( jar_path = None , nthreads = - 1 , enable_assertions = 1 , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = 1 , jvm_custom_args = None , bind_to_localhost = 1 ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and min_mem_size > max_mem_size : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( port [ - 1 ] == \"+\" and port [ : - 1 ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - 1 ] ) \n            port = 0 \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        print ( \"Attempting to start a local H2O server...\" ) \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        print ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1555": "\ndef hit_ratio_table ( self , train = 0 , valid = 0 , xval = 0 ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in zip ( list ( tm . keys ( ) ) , list ( tm . values ( ) ) ) : \n        m [ k ] = None if v is None else v . hit_ratio_table ( ) \n    return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m "}
{"1566": "\ndef add_new_message ( ) : \n    global g_new_messages_to_exclude \n    global g_dict_changed \n    new_message_dict = extract_message_to_dict ( g_new_messages_to_exclude ) \n    if new_message_dict : \n        g_dict_changed = 1 \n        update_message_dict ( new_message_dict , 1 ) "}
{"1568": "\ndef extract_message_to_dict ( filename ) : \n    message_dict = { } \n    if os . path . isfile ( filename ) : \n        with open ( filename , 'r' ) as wfile : \n            key = \"\" \n            val = \"\" \n            startMess = 0 \n            while 1 : \n                each_line = wfile . readline ( ) \n                if not each_line : \n                    if startMess : \n                        add_to_dict ( val . strip ( ) , key , message_dict ) \n                    break \n                if \"keyname\" in each_line . lower ( ) : \n                    temp_strings = each_line . strip ( ) . split ( '=' ) \n                    if ( len ( temp_strings ) > 1 ) : \n                        if startMess : \n                            add_to_dict ( val . strip ( ) , key , message_dict ) \n                            val = \"\" \n                        key = temp_strings [ 1 ] . strip ( ) \n                        startMess = 0 \n                if ( len ( each_line ) > 1 ) and startMess : \n                    val += each_line \n                if \"ignoredmessage\" in each_line . lower ( ) : \n                    startMess = 1 \n                    temp_mess = each_line . split ( '=' ) \n                    if ( len ( temp_mess ) > 1 ) : \n                        val = temp_mess [ 1 ] \n    return message_dict "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if len ( argv ) < 2 : \n        usage ( ) \n    i = 1 \n    while ( i < len ( argv ) ) : \n        s = argv [ i ] \n        if ( s == \"--inputfileadd\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        elif ( s == \"--inputfilerm\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_old_messages_to_remove = argv [ i ] \n        elif ( s == \"--loadjavamessage\" ) : \n            i += 1 \n            if i > len ( argv ) : \n                usage ( ) \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == \"--savejavamessage\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_save_java_message_filename = argv [ i ] \n        elif ( s == '--printjavamessage' ) : \n            i += 1 \n            g_print_java_messages = 1 \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == '--help' ) : \n            usage ( ) \n        else : \n            unknown_arg ( s ) \n        i += 1 "}
{"1576": "\ndef transform ( self , data , allow_timestamps = 0 ) : \n    assert_is_type ( data , H2OFrame ) \n    assert_is_type ( allow_timestamps , bool ) \n    return H2OFrame . _expr ( ExprNode ( \"mojo.pipeline.transform\" , self . pipeline_id [ 0 ] , data , allow_timestamps ) ) "}
{"1579": "\ndef plot ( self , type = \"roc\" , server = 0 ) : \n    assert_is_type ( type , \"roc\" ) \n    try : \n        imp . find_module ( 'matplotlib' ) \n        import matplotlib \n        if server : \n            matplotlib . use ( 'Agg' , warn = 0 ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    if type == \"roc\" : \n        plt . xlabel ( 'False Positive Rate (FPR)' ) \n        plt . ylabel ( 'True Positive Rate (TPR)' ) \n        plt . title ( 'ROC Curve' ) \n        plt . text ( 0.5 , 0.5 , r'AUC={0:.4f}' . format ( self . _metric_json [ \"AUC\" ] ) ) \n        plt . plot ( self . fprs , self . tprs , 'b--' ) \n        plt . axis ( [ 0 , 1 , 0 , 1 ] ) \n        if not server : \n            plt . show ( ) "}
{"1581": "\ndef available ( ) : \n    builder_json = h2o . api ( \"GET /3/ModelBuilders\" , data = { \"algo\" : \"deepwater\" } ) \n    visibility = builder_json [ \"model_builders\" ] [ \"deepwater\" ] [ \"visibility\" ] \n    if visibility == \"Experimental\" : \n        print ( \"Cannot build a Deep Water model - no backend found.\" ) \n        return 0 \n    else : \n        return 1 "}
{"1586": "\ndef comment_user ( self , user_id , amount = None ) : \n    if not self . check_user ( user_id , filter_closed_acc = 1 ) : \n        return 0 \n    self . logger . info ( \"Going to comment user_%s's feed:\" % user_id ) \n    user_id = self . convert_to_user_id ( user_id ) \n    medias = self . get_user_medias ( user_id , is_comment = 1 ) \n    if not medias : \n        self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) \n        return 0 \n    return self . comment_medias ( medias [ : amount ] ) "}
{"1587": "\ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while 1 : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if login == username . strip ( ) : \n                    return login , password \n        print ( \"Which account do you want to use? (Type number)\" ) \n        for ind , ( login , password ) in enumerate ( lines ) : \n            print ( \"%d: %s\" % ( ind + 1 , login ) ) \n        print ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) \n        print ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if ind == 0 : \n                add_credentials ( ) \n                continue \n            elif ind == - 1 : \n                delete_credentials ( ) \n                check_secret ( ) \n                continue \n            elif 0 <= ind - 1 < len ( lines ) : \n                return lines [ ind - 1 ] \n        except Exception : \n            print ( \"Wrong input, enter the number of the account to use.\" ) "}
{"1588": "\ndef like_user ( self , user_id , amount = None , filtration = 1 ) : \n    if filtration : \n        if not self . check_user ( user_id ) : \n            return 0 \n    self . logger . info ( \"Liking user_%s's feed:\" % user_id ) \n    user_id = self . convert_to_user_id ( user_id ) \n    medias = self . get_user_medias ( user_id , filtration = filtration ) \n    if not medias : \n        self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) \n        return 0 \n    return self . like_medias ( medias [ : amount ] ) "}
{"1590": "\ndef check_not_bot ( self , user_id ) : \n    self . small_delay ( ) \n    user_id = self . convert_to_user_id ( user_id ) \n    if not user_id : \n        return 0 \n    if user_id in self . whitelist : \n        return 1 \n    if user_id in self . blacklist : \n        return 0 \n    user_info = self . get_user_info ( user_id ) \n    if not user_info : \n        return 1 \n    skipped = self . skipped_file \n    if \"following_count\" in user_info and user_info [ \"following_count\" ] > self . max_following_to_block : \n        msg = 'following_count > bot.max_following_to_block, skipping!' \n        self . console_print ( msg , 'red' ) \n        skipped . append ( user_id ) \n        return 0 \n    if search_stop_words_in_user ( self , user_info ) : \n        msg = '`bot.search_stop_words_in_user` found in user, skipping!' \n        skipped . append ( user_id ) \n        return 0 \n    return 1 "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = 0 ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if sys . version_info [ 0 ] < 3 : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        print ( str ( exception ) ) \n        return [ ] "}
{"1593": "\ndef defer ( self ) : \n    self . _is_live ( 'defer' ) \n    try : \n        self . message . modify ( 1 , 1 ) \n    except Exception as e : \n        raise MessageSettleFailed ( \"defer\" , e ) "}
{"1594": "\ndef download ( self , resource_group_name , virtual_wan_name , vpn_sites = None , output_blob_sas_url = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _download_initial ( resource_group_name = resource_group_name , virtual_wan_name = virtual_wan_name , vpn_sites = vpn_sites , output_blob_sas_url = output_blob_sas_url , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1596": "\ndef update_command ( self , resource_group_name , node_name , session , pssession , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _update_command_initial ( resource_group_name = resource_group_name , node_name = node_name , session = session , pssession = pssession , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'PowerShellCommandResults' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1597": "\ndef delete_by_id ( self , application_definition_id , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _delete_by_id_initial ( application_definition_id = application_definition_id , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1598": "\ndef create_or_update_by_id ( self , application_definition_id , parameters , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _create_or_update_by_id_initial ( application_definition_id = application_definition_id , parameters = parameters , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'ApplicationDefinition' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1602": "\ndef execute_script_actions ( self , resource_group_name , cluster_name , persist_on_success , script_actions = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _execute_script_actions_initial ( resource_group_name = resource_group_name , cluster_name = cluster_name , persist_on_success = persist_on_success , script_actions = script_actions , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1603": "\ndef check_front_door_name_availability ( self , name , type , custom_headers = None , raw = 0 , ** operation_config ) : \n    check_front_door_name_availability_input = models . CheckNameAvailabilityInput ( name = name , type = type ) \n    api_version = \"2018-08-01\" \n    url = self . check_front_door_name_availability . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_front_door_name_availability_input , 'CheckNameAvailabilityInput' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityOutput' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1604": "\ndef purge_deleted ( self , vault_name , location , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _purge_deleted_initial ( vault_name = vault_name , location = location , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1608": "\ndef get_azure_cli_credentials ( resource = None , with_tenant = 0 ) : \n    profile = get_cli_profile ( ) \n    cred , subscription_id , tenant_id = profile . get_login_credentials ( resource = resource ) \n    if with_tenant : \n        return cred , subscription_id , tenant_id \n    else : \n        return cred , subscription_id "}
{"1609": "\ndef resolve ( self , app_id , query , timezone_offset = None , verbose = None , staging = None , spell_check = None , bing_spell_check_subscription_key = None , log = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    url = self . resolve . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = 1 ) , 'appId' : self . _serialize . url ( \"app_id\" , app_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if timezone_offset is not None : \n        query_parameters [ 'timezoneOffset' ] = self . _serialize . query ( \"timezone_offset\" , timezone_offset , 'float' ) \n    if verbose is not None : \n        query_parameters [ 'verbose' ] = self . _serialize . query ( \"verbose\" , verbose , 'bool' ) \n    if staging is not None : \n        query_parameters [ 'staging' ] = self . _serialize . query ( \"staging\" , staging , 'bool' ) \n    if spell_check is not None : \n        query_parameters [ 'spellCheck' ] = self . _serialize . query ( \"spell_check\" , spell_check , 'bool' ) \n    if bing_spell_check_subscription_key is not None : \n        query_parameters [ 'bing-spell-check-subscription-key' ] = self . _serialize . query ( \"bing_spell_check_subscription_key\" , bing_spell_check_subscription_key , 'str' ) \n    if log is not None : \n        query_parameters [ 'log' ] = self . _serialize . query ( \"log\" , log , 'bool' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( query , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'LuisResult' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1610": "\ndef check_name_availability_local ( self , location , name , type , custom_headers = None , raw = 0 , ** operation_config ) : \n    check_name_availability = models . CheckNameAvailabilityRequest ( name = name , type = type ) \n    url = self . check_name_availability_local . metadata [ 'url' ] \n    path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) , 'location' : self . _serialize . url ( \"location\" , location , 'str' , max_length = 90 , min_length = 1 , pattern = r'^[-\\w\\._\\(\\)]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_name_availability , 'CheckNameAvailabilityRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityResponse' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1626": "\ndef verify_face_to_person ( self , face_id , person_id , person_group_id = None , large_person_group_id = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    body = models . VerifyFaceToPersonRequest ( face_id = face_id , person_group_id = person_group_id , large_person_group_id = large_person_group_id , person_id = person_id ) \n    url = self . verify_face_to_person . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( body , 'VerifyFaceToPersonRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'VerifyResult' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1627": "\ndef add ( self , job , job_add_options = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    timeout = None \n    if job_add_options is not None : \n        timeout = job_add_options . timeout \n    client_request_id = None \n    if job_add_options is not None : \n        client_request_id = job_add_options . client_request_id \n    return_client_request_id = None \n    if job_add_options is not None : \n        return_client_request_id = job_add_options . return_client_request_id \n    ocp_date = None \n    if job_add_options is not None : \n        ocp_date = job_add_options . ocp_date \n    url = self . add . metadata [ 'url' ] \n    path_format_arguments = { 'batchUrl' : self . _serialize . url ( \"self.config.batch_url\" , self . config . batch_url , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'int' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; odata=minimalmetadata; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    if client_request_id is not None : \n        header_parameters [ 'client-request-id' ] = self . _serialize . header ( \"client_request_id\" , client_request_id , 'str' ) \n    if return_client_request_id is not None : \n        header_parameters [ 'return-client-request-id' ] = self . _serialize . header ( \"return_client_request_id\" , return_client_request_id , 'bool' ) \n    if ocp_date is not None : \n        header_parameters [ 'ocp-date' ] = self . _serialize . header ( \"ocp_date\" , ocp_date , 'rfc-1123' ) \n    body_content = self . _serialize . body ( job , 'JobAddParameter' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 201 ] : \n        raise models . BatchErrorException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        client_raw_response . add_headers ( { 'client-request-id' : 'str' , 'request-id' : 'str' , 'ETag' : 'str' , 'Last-Modified' : 'rfc-1123' , 'DataServiceId' : 'str' , } ) \n        return client_raw_response "}
{"1628": "\ndef get_entry_properties_from_node ( entry , include_id , id_prefix_to_skip = None , use_title_as_id = 0 ) : \n    properties = { } \n    etag = entry . getAttributeNS ( METADATA_NS , 'etag' ) \n    if etag : \n        properties [ 'etag' ] = etag \n    for updated in _MinidomXmlToObject . get_child_nodes ( entry , 'updated' ) : \n        properties [ 'updated' ] = updated . firstChild . nodeValue \n    for name in _MinidomXmlToObject . get_children_from_path ( entry , 'author' , 'name' ) : \n        if name . firstChild is not None : \n            properties [ 'author' ] = name . firstChild . nodeValue \n    if include_id : \n        if use_title_as_id : \n            for title in _MinidomXmlToObject . get_child_nodes ( entry , 'title' ) : \n                properties [ 'name' ] = title . firstChild . nodeValue \n        else : \n            for id in _MinidomXmlToObject . get_child_nodes ( entry , 'id' ) : \n                properties [ 'name' ] = _get_readable_id ( id . firstChild . nodeValue , id_prefix_to_skip ) \n    return properties "}
{"1630": "\ndef _find_namespaces_from_child ( parent , child , namespaces ) : \n    for cur_child in parent . childNodes : \n        if cur_child is child : \n            return 1 \n        if _MinidomXmlToObject . _find_namespaces_from_child ( cur_child , child , namespaces ) : \n            for key in cur_child . attributes . keys ( ) : \n                if key . startswith ( 'xmlns:' ) or key == 'xmlns' : \n                    namespaces [ key ] = cur_child . attributes [ key ] \n            break \n    return 0 "}
{"1634": "\ndef xml_to_metrics ( xmlstr , object_type ) : \n    xmldoc = minidom . parseString ( xmlstr ) \n    return_obj = object_type ( ) \n    members = dict ( vars ( return_obj ) ) \n    for xml_entry in _MinidomXmlToObject . get_children_from_path ( xmldoc , 'entry' ) : \n        for node in _MinidomXmlToObject . get_children_from_path ( xml_entry , 'content' , 'properties' ) : \n            for name in members : \n                xml_name = _get_serialization_name ( name ) \n                children = _MinidomXmlToObject . get_child_nodes ( node , xml_name ) \n                if not children : \n                    continue \n                child = children [ 0 ] \n                node_type = child . getAttributeNS ( \"http://schemas.microsoft.com/ado/2007/08/dataservices/metadata\" , 'type' ) \n                node_value = _ServiceBusManagementXmlSerializer . odata_converter ( child . firstChild . nodeValue , node_type ) \n                setattr ( return_obj , name , node_value ) \n        for name , value in _MinidomXmlToObject . get_entry_properties_from_node ( xml_entry , include_id = 1 , use_title_as_id = 0 ) . items ( ) : \n            if name in members : \n                continue \n            setattr ( return_obj , name , value ) \n    return return_obj "}
{"1635": "\ndef replace_content ( self , resource_group_name , automation_account_name , runbook_name , runbook_content , custom_headers = None , raw = 0 , callback = None , polling = 1 , ** operation_config ) : \n    raw_result = self . _replace_content_initial ( resource_group_name = resource_group_name , automation_account_name = automation_account_name , runbook_name = runbook_name , runbook_content = runbook_content , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        header_dict = { 'location' : 'str' , } \n        deserialized = self . _deserialize ( 'object' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            client_raw_response . add_headers ( header_dict ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1636": "\ndef list_recommendations ( self , keywords = None , max_domain_recommendations = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . DomainRecommendationSearchParameters ( keywords = keywords , max_domain_recommendations = max_domain_recommendations ) \n    def internal_paging ( next_link = None , raw = 0 ) : \n        if not next_link : \n            url = self . list_recommendations . metadata [ 'url' ] \n            path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( parameters , 'DomainRecommendationSearchParameters' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = 0 , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . NameIdentifierPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . NameIdentifierPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1637": "\ndef update ( self , kb_id , update_kb , custom_headers = None , raw = 0 , ** operation_config ) : \n    url = self . update . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = 1 ) , 'kbId' : self . _serialize . url ( \"kb_id\" , kb_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( update_kb , 'UpdateKbOperationDTO' ) \n    request = self . _client . patch ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 202 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    header_dict = { } \n    if response . status_code == 202 : \n        deserialized = self . _deserialize ( 'Operation' , response ) \n        header_dict = { 'Location' : 'str' , } \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        client_raw_response . add_headers ( header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1638": "\ndef get_member_groups ( self , object_id , security_enabled_only , additional_properties = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . UserGetMemberGroupsParameters ( additional_properties = additional_properties , security_enabled_only = security_enabled_only ) \n    def internal_paging ( next_link = None , raw = 0 ) : \n        if not next_link : \n            url = self . get_member_groups . metadata [ 'url' ] \n            path_format_arguments = { 'objectId' : self . _serialize . url ( \"object_id\" , object_id , 'str' ) , 'tenantID' : self . _serialize . url ( \"self.config.tenant_id\" , self . config . tenant_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( parameters , 'UserGetMemberGroupsParameters' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = 0 , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . GraphErrorException ( self . _deserialize , response ) \n        return response \n    deserialized = models . StrPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . StrPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1639": "\ndef build_package_from_pr_number ( gh_token , sdk_id , pr_number , output_folder , * , with_comment = 0 ) : \n    con = Github ( gh_token ) \n    repo = con . get_repo ( sdk_id ) \n    sdk_pr = repo . get_pull ( pr_number ) \n    package_names = { f . filename . split ( '/' ) [ 0 ] for f in sdk_pr . get_files ( ) if f . filename . startswith ( \"azure\" ) } \n    absolute_output_folder = Path ( output_folder ) . resolve ( ) \n    with tempfile . TemporaryDirectory ( ) as temp_dir , manage_git_folder ( gh_token , Path ( temp_dir ) / Path ( \"sdk\" ) , sdk_id , pr_number = pr_number ) as sdk_folder : \n        for package_name in package_names : \n            _LOGGER . debug ( \"Build {}\" . format ( package_name ) ) \n            execute_simple_command ( [ \"python\" , \"./build_package.py\" , \"--dest\" , str ( absolute_output_folder ) , package_name ] , cwd = sdk_folder ) \n            _LOGGER . debug ( \"Build finished: {}\" . format ( package_name ) ) \n    if with_comment : \n        files = [ f . name for f in absolute_output_folder . iterdir ( ) ] \n        comment_message = None \n        dashboard = DashboardCommentableObject ( sdk_pr , \"(message created by the CI based on PR content)\" ) \n        try : \n            installation_message = build_installation_message ( sdk_pr ) \n            download_message = build_download_message ( sdk_pr , files ) \n            comment_message = installation_message + \"\\n\\n\" + download_message \n            dashboard . create_comment ( comment_message ) \n        except Exception : \n            _LOGGER . critical ( \"Unable to do PR comment:\\n%s\" , comment_message ) "}
{"1640": "\ndef import_data ( self , resource_group_name , name , files , format = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _import_data_initial ( resource_group_name = resource_group_name , name = name , files = files , format = format , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1641": "\ndef publish ( self , resource_group_name , automation_account_name , runbook_name , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _publish_initial ( resource_group_name = resource_group_name , automation_account_name = automation_account_name , runbook_name = runbook_name , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            client_raw_response . add_headers ( { 'location' : 'str' , } ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1643": "\ndef replace ( self , word_alterations , custom_headers = None , raw = 0 , ** operation_config ) : \n    word_alterations1 = models . WordAlterationsDTO ( word_alterations = word_alterations ) \n    url = self . replace . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( word_alterations1 , 'WordAlterationsDTO' ) \n    request = self . _client . put ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 204 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1644": "\ndef add_value ( self , secret_resource_name , secret_value_resource_name , name , value = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    secret_value_resource_description = models . SecretValueResourceDescription ( name = name , value = value ) \n    url = self . add_value . metadata [ 'url' ] \n    path_format_arguments = { 'secretResourceName' : self . _serialize . url ( \"secret_resource_name\" , secret_resource_name , 'str' , skip_quote = 1 ) , 'secretValueResourceName' : self . _serialize . url ( \"secret_value_resource_name\" , secret_value_resource_name , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( secret_value_resource_description , 'SecretValueResourceDescription' ) \n    request = self . _client . put ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 , 201 , 202 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'SecretValueResourceDescription' , response ) \n    if response . status_code == 201 : \n        deserialized = self . _deserialize ( 'SecretValueResourceDescription' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1648": "\ndef create_storage_account ( self , service_name , description , label , affinity_group = None , location = None , geo_replication_enabled = None , extended_properties = None , account_type = 'Standard_GRS' ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'description' , description ) \n    _validate_not_none ( 'label' , label ) \n    if affinity_group is None and location is None : \n        raise ValueError ( 'location or affinity_group must be specified' ) \n    if affinity_group is not None and location is not None : \n        raise ValueError ( 'Only one of location or affinity_group needs to be specified' ) \n    if geo_replication_enabled == 0 : \n        account_type = 'Standard_LRS' \n    return self . _perform_post ( self . _get_storage_service_path ( ) , _XmlSerializer . create_storage_service_input_to_xml ( service_name , description , label , affinity_group , location , account_type , extended_properties ) , as_async = 1 ) "}
{"1649": "\ndef update_storage_account ( self , service_name , description = None , label = None , geo_replication_enabled = None , extended_properties = None , account_type = 'Standard_GRS' ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    if geo_replication_enabled == 0 : \n        account_type = 'Standard_LRS' \n    return self . _perform_put ( self . _get_storage_service_path ( service_name ) , _XmlSerializer . update_storage_service_input_to_xml ( description , label , account_type , extended_properties ) ) "}
{"1650": "\ndef delete_storage_account ( self , service_name ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    return self . _perform_delete ( self . _get_storage_service_path ( service_name ) , as_async = 1 ) "}
{"1652": "\ndef get_hosted_service_properties ( self , service_name , embed_detail = 0 ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'embed_detail' , embed_detail ) \n    return self . _perform_get ( self . _get_hosted_service_path ( service_name ) + '?embed-detail=' + _str ( embed_detail ) . lower ( ) , HostedService ) "}
{"1653": "\ndef create_hosted_service ( self , service_name , label , description = None , location = None , affinity_group = None , extended_properties = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'label' , label ) \n    if affinity_group is None and location is None : \n        raise ValueError ( 'location or affinity_group must be specified' ) \n    if affinity_group is not None and location is not None : \n        raise ValueError ( 'Only one of location or affinity_group needs to be specified' ) \n    return self . _perform_post ( self . _get_hosted_service_path ( ) , _XmlSerializer . create_hosted_service_to_xml ( service_name , label , description , location , affinity_group , extended_properties ) , as_async = 1 ) "}
{"1654": "\ndef delete_hosted_service ( self , service_name , complete = 0 ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    path = self . _get_hosted_service_path ( service_name ) \n    if complete == 1 : \n        path = path + '?comp=media' \n    return self . _perform_delete ( path , as_async = 1 ) "}
{"1655": "\ndef create_deployment ( self , service_name , deployment_slot , name , package_url , label , configuration , start_deployment = 0 , treat_warnings_as_error = 0 , extended_properties = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_slot' , deployment_slot ) \n    _validate_not_none ( 'name' , name ) \n    _validate_not_none ( 'package_url' , package_url ) \n    _validate_not_none ( 'label' , label ) \n    _validate_not_none ( 'configuration' , configuration ) \n    return self . _perform_post ( self . _get_deployment_path_using_slot ( service_name , deployment_slot ) , _XmlSerializer . create_deployment_to_xml ( name , package_url , label , configuration , start_deployment , treat_warnings_as_error , extended_properties ) , as_async = 1 ) "}
{"1656": "\ndef delete_deployment ( self , service_name , deployment_name , delete_vhd = 0 ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    path = self . _get_deployment_path_using_name ( service_name , deployment_name ) \n    if delete_vhd : \n        path += '?comp=media' \n    return self . _perform_delete ( path , as_async = 1 ) "}
{"1657": "\ndef swap_deployment ( self , service_name , production , source_deployment ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'production' , production ) \n    _validate_not_none ( 'source_deployment' , source_deployment ) \n    return self . _perform_post ( self . _get_hosted_service_path ( service_name ) , _XmlSerializer . swap_deployment_to_xml ( production , source_deployment ) , as_async = 1 ) "}
{"1658": "\ndef change_deployment_configuration ( self , service_name , deployment_name , configuration , treat_warnings_as_error = 0 , mode = 'Auto' , extended_properties = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'configuration' , configuration ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name , deployment_name ) + '/?comp=config' , _XmlSerializer . change_deployment_to_xml ( configuration , treat_warnings_as_error , mode , extended_properties ) , as_async = 1 ) "}
{"1659": "\ndef update_deployment_status ( self , service_name , deployment_name , status ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'status' , status ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name , deployment_name ) + '/?comp=status' , _XmlSerializer . update_deployment_status_to_xml ( status ) , as_async = 1 ) "}
{"1660": "\ndef upgrade_deployment ( self , service_name , deployment_name , mode , package_url , configuration , label , force , role_to_upgrade = None , extended_properties = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'mode' , mode ) \n    _validate_not_none ( 'package_url' , package_url ) \n    _validate_not_none ( 'configuration' , configuration ) \n    _validate_not_none ( 'label' , label ) \n    _validate_not_none ( 'force' , force ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name , deployment_name ) + '/?comp=upgrade' , _XmlSerializer . upgrade_deployment_to_xml ( mode , package_url , configuration , label , role_to_upgrade , force , extended_properties ) , as_async = 1 ) "}
{"1661": "\ndef walk_upgrade_domain ( self , service_name , deployment_name , upgrade_domain ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'upgrade_domain' , upgrade_domain ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name , deployment_name ) + '/?comp=walkupgradedomain' , _XmlSerializer . walk_upgrade_domain_to_xml ( upgrade_domain ) , as_async = 1 ) "}
{"1662": "\ndef reboot_role_instance ( self , service_name , deployment_name , role_instance_name ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_instance_name' , role_instance_name ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name , deployment_name ) + '/roleinstances/' + _str ( role_instance_name ) + '?comp=reboot' , '' , as_async = 1 ) "}
{"1663": "\ndef delete_role_instances ( self , service_name , deployment_name , role_instance_names ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_instance_names' , role_instance_names ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name , deployment_name ) + '/roleinstances/?comp=delete' , _XmlSerializer . role_instances_to_xml ( role_instance_names ) , as_async = 1 ) "}
{"1667": "\ndef add_service_certificate ( self , service_name , data , certificate_format , password = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'data' , data ) \n    _validate_not_none ( 'certificate_format' , certificate_format ) \n    _validate_not_none ( 'password' , password ) \n    return self . _perform_post ( '/' + self . subscription_id + '/services/hostedservices/' + _str ( service_name ) + '/certificates' , _XmlSerializer . certificate_file_to_xml ( data , certificate_format , password ) , as_async = 1 ) "}
{"1668": "\ndef delete_service_certificate ( self , service_name , thumbalgorithm , thumbprint ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'thumbalgorithm' , thumbalgorithm ) \n    _validate_not_none ( 'thumbprint' , thumbprint ) \n    return self . _perform_delete ( '/' + self . subscription_id + '/services/hostedservices/' + _str ( service_name ) + '/certificates/' + _str ( thumbalgorithm ) + '-' + _str ( thumbprint ) , as_async = 1 ) "}
{"1676": "\ndef create_reserved_ip_address ( self , name , label = None , location = None ) : \n    _validate_not_none ( 'name' , name ) \n    return self . _perform_post ( self . _get_reserved_ip_path ( ) , _XmlSerializer . create_reserved_ip_to_xml ( name , label , location ) , as_async = 1 ) "}
{"1677": "\ndef delete_reserved_ip_address ( self , name ) : \n    _validate_not_none ( 'name' , name ) \n    return self . _perform_delete ( self . _get_reserved_ip_path ( name ) , as_async = 1 ) "}
{"1678": "\ndef associate_reserved_ip_address ( self , name , service_name , deployment_name , virtual_ip_name = None ) : \n    _validate_not_none ( 'name' , name ) \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    return self . _perform_post ( self . _get_reserved_ip_path_for_association ( name ) , _XmlSerializer . associate_reserved_ip_to_xml ( service_name , deployment_name , virtual_ip_name ) , as_async = 1 , x_ms_version = '2015-02-01' ) "}
{"1679": "\ndef disassociate_reserved_ip_address ( self , name , service_name , deployment_name , virtual_ip_name = None ) : \n    _validate_not_none ( 'name' , name ) \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    return self . _perform_post ( self . _get_reserved_ip_path_for_disassociation ( name ) , _XmlSerializer . associate_reserved_ip_to_xml ( service_name , deployment_name , virtual_ip_name ) , as_async = 1 , x_ms_version = '2015-02-01' ) "}
{"1682": "\ndef create_virtual_machine_deployment ( self , service_name , deployment_name , deployment_slot , label , role_name , system_config , os_virtual_hard_disk , network_config = None , availability_set_name = None , data_virtual_hard_disks = None , role_size = None , role_type = 'PersistentVMRole' , virtual_network_name = None , resource_extension_references = None , provision_guest_agent = None , vm_image_name = None , media_location = None , dns_servers = None , reserved_ip_name = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'deployment_slot' , deployment_slot ) \n    _validate_not_none ( 'label' , label ) \n    _validate_not_none ( 'role_name' , role_name ) \n    return self . _perform_post ( self . _get_deployment_path_using_name ( service_name ) , _XmlSerializer . virtual_machine_deployment_to_xml ( deployment_name , deployment_slot , label , role_name , system_config , os_virtual_hard_disk , role_type , network_config , availability_set_name , data_virtual_hard_disks , role_size , virtual_network_name , resource_extension_references , provision_guest_agent , vm_image_name , media_location , dns_servers , reserved_ip_name ) , as_async = 1 ) "}
{"1683": "\ndef add_role ( self , service_name , deployment_name , role_name , system_config , os_virtual_hard_disk , network_config = None , availability_set_name = None , data_virtual_hard_disks = None , role_size = None , role_type = 'PersistentVMRole' , resource_extension_references = None , provision_guest_agent = None , vm_image_name = None , media_location = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    return self . _perform_post ( self . _get_role_path ( service_name , deployment_name ) , _XmlSerializer . add_role_to_xml ( role_name , system_config , os_virtual_hard_disk , role_type , network_config , availability_set_name , data_virtual_hard_disks , role_size , resource_extension_references , provision_guest_agent , vm_image_name , media_location ) , as_async = 1 ) "}
{"1684": "\ndef update_role ( self , service_name , deployment_name , role_name , os_virtual_hard_disk = None , network_config = None , availability_set_name = None , data_virtual_hard_disks = None , role_size = None , role_type = 'PersistentVMRole' , resource_extension_references = None , provision_guest_agent = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    return self . _perform_put ( self . _get_role_path ( service_name , deployment_name , role_name ) , _XmlSerializer . update_role_to_xml ( role_name , os_virtual_hard_disk , role_type , network_config , availability_set_name , data_virtual_hard_disks , role_size , resource_extension_references , provision_guest_agent ) , as_async = 1 ) "}
{"1685": "\ndef delete_role ( self , service_name , deployment_name , role_name , complete = 0 ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    path = self . _get_role_path ( service_name , deployment_name , role_name ) \n    if complete == 1 : \n        path = path + '?comp=media' \n    return self . _perform_delete ( path , as_async = 1 ) "}
{"1686": "\ndef capture_role ( self , service_name , deployment_name , role_name , post_capture_action , target_image_name , target_image_label , provisioning_configuration = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    _validate_not_none ( 'post_capture_action' , post_capture_action ) \n    _validate_not_none ( 'target_image_name' , target_image_name ) \n    _validate_not_none ( 'target_image_label' , target_image_label ) \n    return self . _perform_post ( self . _get_role_instance_operations_path ( service_name , deployment_name , role_name ) , _XmlSerializer . capture_role_to_xml ( post_capture_action , target_image_name , target_image_label , provisioning_configuration ) , as_async = 1 ) "}
{"1687": "\ndef start_role ( self , service_name , deployment_name , role_name ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    return self . _perform_post ( self . _get_role_instance_operations_path ( service_name , deployment_name , role_name ) , _XmlSerializer . start_role_operation_to_xml ( ) , as_async = 1 ) "}
{"1688": "\ndef start_roles ( self , service_name , deployment_name , role_names ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_names' , role_names ) \n    return self . _perform_post ( self . _get_roles_operations_path ( service_name , deployment_name ) , _XmlSerializer . start_roles_operation_to_xml ( role_names ) , as_async = 1 ) "}
{"1689": "\ndef restart_role ( self , service_name , deployment_name , role_name ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    return self . _perform_post ( self . _get_role_instance_operations_path ( service_name , deployment_name , role_name ) , _XmlSerializer . restart_role_operation_to_xml ( ) , as_async = 1 ) "}
{"1690": "\ndef shutdown_role ( self , service_name , deployment_name , role_name , post_shutdown_action = 'Stopped' ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    _validate_not_none ( 'post_shutdown_action' , post_shutdown_action ) \n    return self . _perform_post ( self . _get_role_instance_operations_path ( service_name , deployment_name , role_name ) , _XmlSerializer . shutdown_role_operation_to_xml ( post_shutdown_action ) , as_async = 1 ) "}
{"1691": "\ndef shutdown_roles ( self , service_name , deployment_name , role_names , post_shutdown_action = 'Stopped' ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_names' , role_names ) \n    _validate_not_none ( 'post_shutdown_action' , post_shutdown_action ) \n    return self . _perform_post ( self . _get_roles_operations_path ( service_name , deployment_name ) , _XmlSerializer . shutdown_roles_operation_to_xml ( role_names , post_shutdown_action ) , as_async = 1 ) "}
{"1692": "\ndef add_dns_server ( self , service_name , deployment_name , dns_server_name , address ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'dns_server_name' , dns_server_name ) \n    _validate_not_none ( 'address' , address ) \n    return self . _perform_post ( self . _get_dns_server_path ( service_name , deployment_name ) , _XmlSerializer . dns_server_to_xml ( dns_server_name , address ) , as_async = 1 ) "}
{"1693": "\ndef update_dns_server ( self , service_name , deployment_name , dns_server_name , address ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'dns_server_name' , dns_server_name ) \n    _validate_not_none ( 'address' , address ) \n    return self . _perform_put ( self . _get_dns_server_path ( service_name , deployment_name , dns_server_name ) , _XmlSerializer . dns_server_to_xml ( dns_server_name , address ) , as_async = 1 ) "}
{"1694": "\ndef delete_dns_server ( self , service_name , deployment_name , dns_server_name ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'dns_server_name' , dns_server_name ) \n    return self . _perform_delete ( self . _get_dns_server_path ( service_name , deployment_name , dns_server_name ) , as_async = 1 ) "}
{"1696": "\ndef replicate_vm_image ( self , vm_image_name , regions , offer , sku , version ) : \n    _validate_not_none ( 'vm_image_name' , vm_image_name ) \n    _validate_not_none ( 'regions' , regions ) \n    _validate_not_none ( 'offer' , offer ) \n    _validate_not_none ( 'sku' , sku ) \n    _validate_not_none ( 'version' , version ) \n    return self . _perform_put ( self . _get_replication_path_using_vm_image_name ( vm_image_name ) , _XmlSerializer . replicate_image_to_xml ( regions , offer , sku , version ) , as_async = 1 , x_ms_version = '2015-04-01' ) "}
{"1697": "\ndef unreplicate_vm_image ( self , vm_image_name ) : \n    _validate_not_none ( 'vm_image_name' , vm_image_name ) \n    return self . _perform_put ( self . _get_unreplication_path_using_vm_image_name ( vm_image_name ) , None , as_async = 1 , x_ms_version = '2015-04-01' ) "}
{"1698": "\ndef share_vm_image ( self , vm_image_name , permission ) : \n    _validate_not_none ( 'vm_image_name' , vm_image_name ) \n    _validate_not_none ( 'permission' , permission ) \n    path = self . _get_sharing_path_using_vm_image_name ( vm_image_name ) \n    query = '&permission=' + permission \n    path = path + '?' + query . lstrip ( '&' ) \n    return self . _perform_put ( path , None , as_async = 1 , x_ms_version = '2015-04-01' ) "}
{"1699": "\ndef create_vm_image ( self , vm_image ) : \n    _validate_not_none ( 'vm_image' , vm_image ) \n    _validate_not_none ( 'vm_image.name' , vm_image . name ) \n    _validate_not_none ( 'vm_image.label' , vm_image . label ) \n    _validate_not_none ( 'vm_image.os_disk_configuration.os_state' , vm_image . os_disk_configuration . os_state ) \n    _validate_not_none ( 'vm_image.os_disk_configuration.os' , vm_image . os_disk_configuration . os ) \n    _validate_not_none ( 'vm_image.os_disk_configuration.media_link' , vm_image . os_disk_configuration . media_link ) \n    return self . _perform_post ( self . _get_vm_image_path ( ) , _XmlSerializer . create_vm_image_to_xml ( vm_image ) , as_async = 1 ) "}
{"1700": "\ndef delete_vm_image ( self , vm_image_name , delete_vhd = 0 ) : \n    _validate_not_none ( 'vm_image_name' , vm_image_name ) \n    path = self . _get_vm_image_path ( vm_image_name ) \n    if delete_vhd : \n        path += '?comp=media' \n    return self . _perform_delete ( path , as_async = 1 ) "}
{"1702": "\ndef update_vm_image ( self , vm_image_name , vm_image ) : \n    _validate_not_none ( 'vm_image_name' , vm_image_name ) \n    _validate_not_none ( 'vm_image' , vm_image ) \n    return self . _perform_put ( self . _get_vm_image_path ( vm_image_name ) , _XmlSerializer . update_vm_image_to_xml ( vm_image ) , as_async = 1 ) "}
{"1703": "\ndef add_os_image ( self , label , media_link , name , os ) : \n    _validate_not_none ( 'label' , label ) \n    _validate_not_none ( 'media_link' , media_link ) \n    _validate_not_none ( 'name' , name ) \n    _validate_not_none ( 'os' , os ) \n    return self . _perform_post ( self . _get_image_path ( ) , _XmlSerializer . os_image_to_xml ( label , media_link , name , os ) , as_async = 1 ) "}
{"1704": "\ndef update_os_image ( self , image_name , label , media_link , name , os ) : \n    _validate_not_none ( 'image_name' , image_name ) \n    _validate_not_none ( 'label' , label ) \n    _validate_not_none ( 'media_link' , media_link ) \n    _validate_not_none ( 'name' , name ) \n    _validate_not_none ( 'os' , os ) \n    return self . _perform_put ( self . _get_image_path ( image_name ) , _XmlSerializer . os_image_to_xml ( label , media_link , name , os ) , as_async = 1 ) "}
{"1705": "\ndef update_os_image_from_image_reference ( self , image_name , os_image ) : \n    _validate_not_none ( 'image_name' , image_name ) \n    _validate_not_none ( 'os_image' , os_image ) \n    return self . _perform_put ( self . _get_image_path ( image_name ) , _XmlSerializer . update_os_image_to_xml ( os_image ) , as_async = 1 ) "}
{"1706": "\ndef delete_os_image ( self , image_name , delete_vhd = 0 ) : \n    _validate_not_none ( 'image_name' , image_name ) \n    path = self . _get_image_path ( image_name ) \n    if delete_vhd : \n        path += '?comp=media' \n    return self . _perform_delete ( path , as_async = 1 ) "}
{"1708": "\ndef add_data_disk ( self , service_name , deployment_name , role_name , lun , host_caching = None , media_link = None , disk_label = None , disk_name = None , logical_disk_size_in_gb = None , source_media_link = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    _validate_not_none ( 'lun' , lun ) \n    return self . _perform_post ( self . _get_data_disk_path ( service_name , deployment_name , role_name ) , _XmlSerializer . data_virtual_hard_disk_to_xml ( host_caching , disk_label , disk_name , lun , logical_disk_size_in_gb , media_link , source_media_link ) , as_async = 1 ) "}
{"1709": "\ndef update_data_disk ( self , service_name , deployment_name , role_name , lun , host_caching = None , media_link = None , updated_lun = None , disk_label = None , disk_name = None , logical_disk_size_in_gb = None ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    _validate_not_none ( 'lun' , lun ) \n    return self . _perform_put ( self . _get_data_disk_path ( service_name , deployment_name , role_name , lun ) , _XmlSerializer . data_virtual_hard_disk_to_xml ( host_caching , disk_label , disk_name , updated_lun , logical_disk_size_in_gb , media_link , None ) , as_async = 1 ) "}
{"1710": "\ndef delete_data_disk ( self , service_name , deployment_name , role_name , lun , delete_vhd = 0 ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    _validate_not_none ( 'lun' , lun ) \n    path = self . _get_data_disk_path ( service_name , deployment_name , role_name , lun ) \n    if delete_vhd : \n        path += '?comp=media' \n    return self . _perform_delete ( path , as_async = 1 ) "}
{"1713": "\ndef delete_disk ( self , disk_name , delete_vhd = 0 ) : \n    _validate_not_none ( 'disk_name' , disk_name ) \n    path = self . _get_disk_path ( disk_name ) \n    if delete_vhd : \n        path += '?comp=media' \n    return self . _perform_delete ( path ) "}
{"1714": "\ndef summarize_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    from_parameter = None \n    if query_options is not None : \n        from_parameter = query_options . from_property \n    to = None \n    if query_options is not None : \n        to = query_options . to \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    url = self . summarize_for_management_group . metadata [ 'url' ] \n    path_format_arguments = { 'policyStatesSummaryResource' : self . _serialize . url ( \"self.policy_states_summary_resource\" , self . policy_states_summary_resource , 'str' ) , 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = 0 ) \n    if from_parameter is not None : \n        query_parameters [ '$from' ] = self . _serialize . query ( \"from_parameter\" , from_parameter , 'iso-8601' ) \n    if to is not None : \n        query_parameters [ '$to' ] = self . _serialize . query ( \"to\" , to , 'iso-8601' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . QueryFailureException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'SummarizeResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1718": "\ndef create_or_update ( self , resource_group_name , vm_scale_set_name , parameters , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _create_or_update_initial ( resource_group_name = resource_group_name , vm_scale_set_name = vm_scale_set_name , parameters = parameters , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'VirtualMachineScaleSet' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1719": "\ndef convert_to_single_placement_group ( self , resource_group_name , vm_scale_set_name , active_placement_group_id = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . VMScaleSetConvertToSinglePlacementGroupInput ( active_placement_group_id = active_placement_group_id ) \n    url = self . convert_to_single_placement_group . metadata [ 'url' ] \n    path_format_arguments = { 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' ) , 'vmScaleSetName' : self . _serialize . url ( \"vm_scale_set_name\" , vm_scale_set_name , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'VMScaleSetConvertToSinglePlacementGroupInput' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        exp = CloudError ( response ) \n        exp . request_id = response . headers . get ( 'x-ms-request-id' ) \n        raise exp \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1720": "\ndef screen_text ( self , text_content_type , text_content , language = None , autocorrect = 0 , pii = 0 , list_id = None , classify = 0 , custom_headers = None , raw = 0 , callback = None , ** operation_config ) : \n    url = self . screen_text . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if language is not None : \n        query_parameters [ 'language' ] = self . _serialize . query ( \"language\" , language , 'str' ) \n    if autocorrect is not None : \n        query_parameters [ 'autocorrect' ] = self . _serialize . query ( \"autocorrect\" , autocorrect , 'bool' ) \n    if pii is not None : \n        query_parameters [ 'PII' ] = self . _serialize . query ( \"pii\" , pii , 'bool' ) \n    if list_id is not None : \n        query_parameters [ 'listId' ] = self . _serialize . query ( \"list_id\" , list_id , 'str' ) \n    if classify is not None : \n        query_parameters [ 'classify' ] = self . _serialize . query ( \"classify\" , classify , 'bool' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'text/plain' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    header_parameters [ 'Content-Type' ] = self . _serialize . header ( \"text_content_type\" , text_content_type , 'str' ) \n    body_content = self . _client . stream_upload ( text_content , callback ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'Screen' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1721": "\ndef create_key ( self , vault_base_url , key_name , kty , key_size = None , key_ops = None , key_attributes = None , tags = None , curve = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . KeyCreateParameters ( kty = kty , key_size = key_size , key_ops = key_ops , key_attributes = key_attributes , tags = tags , curve = curve ) \n    url = self . create_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = 1 ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyCreateParameters' ) \n    request = self . _client . post ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1722": "\ndef import_key ( self , vault_base_url , key_name , key , hsm = None , key_attributes = None , tags = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . KeyImportParameters ( hsm = hsm , key = key , key_attributes = key_attributes , tags = tags ) \n    url = self . import_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = 1 ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyImportParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1723": "\ndef update_key ( self , vault_base_url , key_name , key_version , key_ops = None , key_attributes = None , tags = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . KeyUpdateParameters ( key_ops = key_ops , key_attributes = key_attributes , tags = tags ) \n    url = self . update_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = 1 ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' ) , 'key-version' : self . _serialize . url ( \"key_version\" , key_version , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyUpdateParameters' ) \n    request = self . _client . patch ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1724": "\ndef set_secret ( self , vault_base_url , secret_name , value , tags = None , content_type = None , secret_attributes = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameters = models . SecretSetParameters ( value = value , tags = tags , content_type = content_type , secret_attributes = secret_attributes ) \n    url = self . set_secret . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = 1 ) , 'secret-name' : self . _serialize . url ( \"secret_name\" , secret_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'SecretSetParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'SecretBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1725": "\ndef set_certificate_issuer ( self , vault_base_url , issuer_name , provider , credentials = None , organization_details = None , attributes = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    parameter = models . CertificateIssuerSetParameters ( provider = provider , credentials = credentials , organization_details = organization_details , attributes = attributes ) \n    url = self . set_certificate_issuer . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = 1 ) , 'issuer-name' : self . _serialize . url ( \"issuer_name\" , issuer_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameter , 'CertificateIssuerSetParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'IssuerBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1732": "\ndef get_deadletter_receiver ( self , transfer_deadletter = 0 , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if int ( prefetch ) < 0 or int ( prefetch ) > 50000 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if transfer_deadletter : \n        entity_uri = self . mgmt_client . format_transfer_dead_letter_queue_name ( self . entity_uri ) \n    else : \n        entity_uri = self . mgmt_client . format_dead_letter_queue_name ( self . entity_uri ) \n    return Receiver ( handler_id , entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1740": "\ndef get_certificates ( self , vault_base_url , maxresults = None , include_pending = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    def internal_paging ( next_link = None , raw = 0 ) : \n        if not next_link : \n            url = self . get_certificates . metadata [ 'url' ] \n            path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = 1 ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            if maxresults is not None : \n                query_parameters [ 'maxresults' ] = self . _serialize . query ( \"maxresults\" , maxresults , 'int' , maximum = 25 , minimum = 1 ) \n            if include_pending is not None : \n                query_parameters [ 'includePending' ] = self . _serialize . query ( \"include_pending\" , include_pending , 'bool' ) \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        request = self . _client . get ( url , query_parameters ) \n        response = self . _client . send ( request , header_parameters , stream = 0 , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . KeyVaultErrorException ( self . _deserialize , response ) \n        return response \n    deserialized = models . CertificateItemPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . CertificateItemPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1754": "\ndef create ( env_dir , system_site_packages = 0 , clear = 0 , symlinks = 0 , with_pip = 0 , prompt = None ) : \n    builder = ExtendedEnvBuilder ( system_site_packages = system_site_packages , clear = clear , symlinks = symlinks , with_pip = with_pip , prompt = prompt ) \n    builder . create ( env_dir ) \n    return builder . context "}
{"1755": "\ndef create_venv_with_package ( packages ) : \n    with tempfile . TemporaryDirectory ( ) as tempdir : \n        myenv = create ( tempdir , with_pip = 1 ) \n        pip_call = [ myenv . env_exe , \"-m\" , \"pip\" , \"install\" , ] \n        subprocess . check_call ( pip_call + [ '-U' , 'pip' ] ) \n        if packages : \n            subprocess . check_call ( pip_call + packages ) \n        yield myenv "}
{"1769": "\ndef list_agreements ( self , name , include_privacy = None , for_transfer = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    agreement_option = models . TopLevelDomainAgreementOption ( include_privacy = include_privacy , for_transfer = for_transfer ) \n    def internal_paging ( next_link = None , raw = 0 ) : \n        if not next_link : \n            url = self . list_agreements . metadata [ 'url' ] \n            path_format_arguments = { 'name' : self . _serialize . url ( \"name\" , name , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( agreement_option , 'TopLevelDomainAgreementOption' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = 0 , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . TldLegalAgreementPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . TldLegalAgreementPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1770": "\nasync def close ( self , exception = None ) : \n    self . running = 0 \n    if self . error : \n        return \n    if isinstance ( exception , ServiceBusError ) : \n        self . error = exception \n    elif exception : \n        self . error = ServiceBusError ( str ( exception ) ) \n    else : \n        self . error = ServiceBusError ( \"This message handler is now closed.\" ) \n    await self . _handler . close_async ( ) "}
{"1771": "\nasync def close ( self , exception = None ) : \n    if not self . running : \n        return \n    self . running = 0 \n    self . receiver_shutdown = 1 \n    self . _used . set ( ) \n    await super ( Receiver , self ) . close ( exception = exception ) "}
{"1775": "\ndef merge ( self , reservation_order_id , sources = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _merge_initial ( reservation_order_id = reservation_order_id , sources = sources , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( '[ReservationResponse]' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1777": "\ndef purge ( self , resource_group_name , workspace_name , table , filters , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _purge_initial ( resource_group_name = resource_group_name , workspace_name = workspace_name , table = table , filters = filters , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'object' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1778": "\ndef _error_handler ( error ) : \n    if error . condition == b'com.microsoft:server-busy' : \n        return errors . ErrorAction ( retry = 1 , backoff = 4 ) \n    if error . condition == b'com.microsoft:timeout' : \n        return errors . ErrorAction ( retry = 1 , backoff = 2 ) \n    if error . condition == b'com.microsoft:operation-cancelled' : \n        return errors . ErrorAction ( retry = 1 ) \n    if error . condition == b\"com.microsoft:container-close\" : \n        return errors . ErrorAction ( retry = 1 , backoff = 4 ) \n    if error . condition in _NO_RETRY_ERRORS : \n        return errors . ErrorAction ( retry = 0 ) \n    return errors . ErrorAction ( retry = 1 ) "}
{"1779": "\ndef create_queue ( self , queue_name , queue = None , fail_on_exist = 0 ) : \n    _validate_not_none ( 'queue_name' , queue_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( queue_name ) + '' \n    request . body = _get_request_body ( _convert_queue_to_xml ( queue ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return 1 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return 0 \n    else : \n        self . _perform_request ( request ) \n        return 1 "}
{"1780": "\ndef delete_queue ( self , queue_name , fail_not_exist = 0 ) : \n    _validate_not_none ( 'queue_name' , queue_name ) \n    request = HTTPRequest ( ) \n    request . method = 'DELETE' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( queue_name ) + '' \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_not_exist : \n        try : \n            self . _perform_request ( request ) \n            return 1 \n        except AzureHttpError as ex : \n            _dont_fail_not_exist ( ex ) \n            return 0 \n    else : \n        self . _perform_request ( request ) \n        return 1 "}
{"1782": "\ndef create_topic ( self , topic_name , topic = None , fail_on_exist = 0 ) : \n    _validate_not_none ( 'topic_name' , topic_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( topic_name ) + '' \n    request . body = _get_request_body ( _convert_topic_to_xml ( topic ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return 1 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return 0 \n    else : \n        self . _perform_request ( request ) \n        return 1 "}
{"1784": "\ndef create_rule ( self , topic_name , subscription_name , rule_name , rule = None , fail_on_exist = 0 ) : \n    _validate_not_none ( 'topic_name' , topic_name ) \n    _validate_not_none ( 'subscription_name' , subscription_name ) \n    _validate_not_none ( 'rule_name' , rule_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( topic_name ) + '/subscriptions/' + _str ( subscription_name ) + '/rules/' + _str ( rule_name ) + '' \n    request . body = _get_request_body ( _convert_rule_to_xml ( rule ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return 1 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return 0 \n    else : \n        self . _perform_request ( request ) \n        return 1 "}
{"1787": "\ndef create_subscription ( self , topic_name , subscription_name , subscription = None , fail_on_exist = 0 ) : \n    _validate_not_none ( 'topic_name' , topic_name ) \n    _validate_not_none ( 'subscription_name' , subscription_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( topic_name ) + '/subscriptions/' + _str ( subscription_name ) + '' \n    request . body = _get_request_body ( _convert_subscription_to_xml ( subscription ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return 1 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return 0 \n    else : \n        self . _perform_request ( request ) \n        return 1 "}
{"1794": "\ndef receive_queue_message ( self , queue_name , peek_lock = 1 , timeout = 60 ) : \n    if peek_lock : \n        return self . peek_lock_queue_message ( queue_name , timeout ) \n    return self . read_delete_queue_message ( queue_name , timeout ) "}
{"1795": "\ndef receive_subscription_message ( self , topic_name , subscription_name , peek_lock = 1 , timeout = 60 ) : \n    if peek_lock : \n        return self . peek_lock_subscription_message ( topic_name , subscription_name , timeout ) \n    return self . read_delete_subscription_message ( topic_name , subscription_name , timeout ) "}
{"1796": "\ndef create_event_hub ( self , hub_name , hub = None , fail_on_exist = 0 ) : \n    _validate_not_none ( 'hub_name' , hub_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( hub_name ) + '?api-version=2014-01' \n    request . body = _get_request_body ( _convert_event_hub_to_xml ( hub ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return 1 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return 0 \n    else : \n        self . _perform_request ( request ) \n        return 1 "}
{"1805": "\ndef reset_service_principal_profile ( self , resource_group_name , resource_name , client_id , secret = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _reset_service_principal_profile_initial ( resource_group_name = resource_group_name , resource_name = resource_name , client_id = client_id , secret = secret , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1811": "\ndef get_cluster_health ( self , nodes_health_state_filter = 0 , applications_health_state_filter = 0 , events_health_state_filter = 0 , exclude_health_statistics = 0 , include_system_application_health_statistics = 0 , timeout = 60 , custom_headers = None , raw = 0 , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_cluster_health . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1812": "\ndef get_cluster_health_using_policy ( self , nodes_health_state_filter = 0 , applications_health_state_filter = 0 , events_health_state_filter = 0 , exclude_health_statistics = 0 , include_system_application_health_statistics = 0 , timeout = 60 , application_health_policy_map = None , cluster_health_policy = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    cluster_health_policies = None \n    if application_health_policy_map is not None or cluster_health_policy is not None : \n        cluster_health_policies = models . ClusterHealthPolicies ( application_health_policy_map = application_health_policy_map , cluster_health_policy = cluster_health_policy ) \n    api_version = \"6.0\" \n    url = self . get_cluster_health_using_policy . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if cluster_health_policies is not None : \n        body_content = self . _serialize . body ( cluster_health_policies , 'ClusterHealthPolicies' ) \n    else : \n        body_content = None \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1813": "\ndef unprovision_application_type ( self , application_type_name , application_type_version , timeout = 60 , async_parameter = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    unprovision_application_type_description_info = models . UnprovisionApplicationTypeDescriptionInfo ( application_type_version = application_type_version , async_property = async_parameter ) \n    api_version = \"6.0\" \n    url = self . unprovision_application_type . metadata [ 'url' ] \n    path_format_arguments = { 'applicationTypeName' : self . _serialize . url ( \"application_type_name\" , application_type_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( unprovision_application_type_description_info , 'UnprovisionApplicationTypeDescriptionInfo' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 , 202 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1814": "\ndef get_repair_task_list ( self , task_id_filter = None , state_filter = None , executor_filter = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_repair_task_list . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if task_id_filter is not None : \n        query_parameters [ 'TaskIdFilter' ] = self . _serialize . query ( \"task_id_filter\" , task_id_filter , 'str' ) \n    if state_filter is not None : \n        query_parameters [ 'StateFilter' ] = self . _serialize . query ( \"state_filter\" , state_filter , 'int' ) \n    if executor_filter is not None : \n        query_parameters [ 'ExecutorFilter' ] = self . _serialize . query ( \"executor_filter\" , executor_filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( '[RepairTask]' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1815": "\ndef submit_property_batch ( self , name_id , timeout = 60 , operations = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    property_batch_description_list = models . PropertyBatchDescriptionList ( operations = operations ) \n    api_version = \"6.0\" \n    url = self . submit_property_batch . metadata [ 'url' ] \n    path_format_arguments = { 'nameId' : self . _serialize . url ( \"name_id\" , name_id , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( property_batch_description_list , 'PropertyBatchDescriptionList' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 , 409 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'SuccessfulPropertyBatchInfo' , response ) \n    if response . status_code == 409 : \n        deserialized = self . _deserialize ( 'FailedPropertyBatchInfo' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1817": "\ndef start_web_site_network_trace_operation ( self , resource_group_name , name , duration_in_seconds = None , max_frame_length = None , sas_url = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _start_web_site_network_trace_operation_initial ( resource_group_name = resource_group_name , name = name , duration_in_seconds = duration_in_seconds , max_frame_length = max_frame_length , sas_url = sas_url , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( '[NetworkTrace]' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1818": "\ndef list_slot_differences_slot ( self , resource_group_name , name , slot , target_slot , preserve_vnet , custom_headers = None , raw = 0 , ** operation_config ) : \n    slot_swap_entity = models . CsmSlotEntity ( target_slot = target_slot , preserve_vnet = preserve_vnet ) \n    def internal_paging ( next_link = None , raw = 0 ) : \n        if not next_link : \n            url = self . list_slot_differences_slot . metadata [ 'url' ] \n            path_format_arguments = { 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' , max_length = 90 , min_length = 1 , pattern = r'^[-\\w\\._\\(\\)]+[^\\.]$' ) , 'name' : self . _serialize . url ( \"name\" , name , 'str' ) , 'slot' : self . _serialize . url ( \"slot\" , slot , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( slot_swap_entity , 'CsmSlotEntity' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = 0 , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . SlotDifferencePaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . SlotDifferencePaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1819": "\ndef swap_slot_slot ( self , resource_group_name , name , slot , target_slot , preserve_vnet , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _swap_slot_slot_initial ( resource_group_name = resource_group_name , name = name , slot = slot , target_slot = target_slot , preserve_vnet = preserve_vnet , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1820": "\ndef get_by_type ( self , app_id , event_type , timespan = None , filter = None , search = None , orderby = None , select = None , skip = None , top = None , format = None , count = None , apply = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    url = self . get_by_type . metadata [ 'url' ] \n    path_format_arguments = { 'appId' : self . _serialize . url ( \"app_id\" , app_id , 'str' ) , 'eventType' : self . _serialize . url ( \"event_type\" , event_type , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if timespan is not None : \n        query_parameters [ 'timespan' ] = self . _serialize . query ( \"timespan\" , timespan , 'str' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    if search is not None : \n        query_parameters [ '$search' ] = self . _serialize . query ( \"search\" , search , 'str' ) \n    if orderby is not None : \n        query_parameters [ '$orderby' ] = self . _serialize . query ( \"orderby\" , orderby , 'str' ) \n    if select is not None : \n        query_parameters [ '$select' ] = self . _serialize . query ( \"select\" , select , 'str' ) \n    if skip is not None : \n        query_parameters [ '$skip' ] = self . _serialize . query ( \"skip\" , skip , 'int' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' ) \n    if format is not None : \n        query_parameters [ '$format' ] = self . _serialize . query ( \"format\" , format , 'str' ) \n    if count is not None : \n        query_parameters [ '$count' ] = self . _serialize . query ( \"count\" , count , 'bool' ) \n    if apply is not None : \n        query_parameters [ '$apply' ] = self . _serialize . query ( \"apply\" , apply , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'EventsResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1821": "\ndef add_face_from_stream ( self , large_face_list_id , image , user_data = None , target_face = None , custom_headers = None , raw = 0 , callback = None , ** operation_config ) : \n    url = self . add_face_from_stream . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = 1 ) , 'largeFaceListId' : self . _serialize . url ( \"large_face_list_id\" , large_face_list_id , 'str' , max_length = 64 , pattern = r'^[a-z0-9-_]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if user_data is not None : \n        query_parameters [ 'userData' ] = self . _serialize . query ( \"user_data\" , user_data , 'str' , max_length = 1024 ) \n    if target_face is not None : \n        query_parameters [ 'targetFace' ] = self . _serialize . query ( \"target_face\" , target_face , '[int]' , div = ',' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/octet-stream' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _client . stream_upload ( image , callback ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'PersistedFace' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1822": "\ndef _handle_redirect ( self , r , ** kwargs ) : \n    if r . is_redirect : \n        self . _thread_local . auth_attempted = 0 "}
{"1823": "\ndef create_and_start_migration ( self , resource_group_name , namespace_name , target_namespace , post_migration_name , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _create_and_start_migration_initial ( resource_group_name = resource_group_name , namespace_name = namespace_name , target_namespace = target_namespace , post_migration_name = post_migration_name , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'MigrationConfigProperties' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1824": "\ndef publish_events ( self , topic_hostname , events , custom_headers = None , raw = 0 , ** operation_config ) : \n    url = self . publish_events . metadata [ 'url' ] \n    path_format_arguments = { 'topicHostname' : self . _serialize . url ( \"topic_hostname\" , topic_hostname , 'str' , skip_quote = 1 ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( events , '[EventGridEvent]' ) \n    request = self . _client . post ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise HttpOperationError ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1825": "\ndef move_resources ( self , source_resource_group_name , resources = None , target_resource_group = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _move_resources_initial ( source_resource_group_name = source_resource_group_name , resources = resources , target_resource_group = target_resource_group , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1827": "\ndef list_query_results_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    def internal_paging ( next_link = None , raw = 0 ) : \n        if not next_link : \n            url = self . list_query_results_for_management_group . metadata [ 'url' ] \n            path_format_arguments = { 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) , 'policyTrackedResourcesResource' : self . _serialize . url ( \"self.policy_tracked_resources_resource\" , self . policy_tracked_resources_resource , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n            if top is not None : \n                query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = 0 ) \n            if filter is not None : \n                query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        request = self . _client . post ( url , query_parameters , header_parameters ) \n        response = self . _client . send ( request , stream = 0 , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . QueryFailureException ( self . _deserialize , response ) \n        return response \n    deserialized = models . PolicyTrackedResourcePaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . PolicyTrackedResourcePaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1828": "\ndef create_queue ( self , queue_name , lock_duration = 30 , max_size_in_megabytes = None , requires_duplicate_detection = 0 , requires_session = 0 , default_message_time_to_live = None , dead_lettering_on_message_expiration = 0 , duplicate_detection_history_time_window = None , max_delivery_count = None , enable_batched_operations = None ) : \n    queue_properties = Queue ( lock_duration = \"PT{}S\" . format ( int ( lock_duration ) ) , max_size_in_megabytes = max_size_in_megabytes , requires_duplicate_detection = requires_duplicate_detection , requires_session = requires_session , default_message_time_to_live = default_message_time_to_live , dead_lettering_on_message_expiration = dead_lettering_on_message_expiration , duplicate_detection_history_time_window = duplicate_detection_history_time_window , max_delivery_count = max_delivery_count , enable_batched_operations = enable_batched_operations ) \n    try : \n        return self . mgmt_client . create_queue ( queue_name , queue = queue_properties , fail_on_exist = 1 ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) "}
{"1829": "\ndef delete_queue ( self , queue_name , fail_not_exist = 0 ) : \n    try : \n        return self . mgmt_client . delete_queue ( queue_name , fail_not_exist = fail_not_exist ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) \n    except azure . common . AzureMissingResourceHttpError as e : \n        raise ServiceBusResourceNotFound ( \"Specificed queue '{}' does not exist.\" . format ( queue_name ) , e ) "}
{"1830": "\ndef create_topic ( self , topic_name , default_message_time_to_live = None , max_size_in_megabytes = None , requires_duplicate_detection = None , duplicate_detection_history_time_window = None , enable_batched_operations = None ) : \n    topic_properties = Topic ( max_size_in_megabytes = max_size_in_megabytes , requires_duplicate_detection = requires_duplicate_detection , default_message_time_to_live = default_message_time_to_live , duplicate_detection_history_time_window = duplicate_detection_history_time_window , enable_batched_operations = enable_batched_operations ) \n    try : \n        return self . mgmt_client . create_topic ( topic_name , topic = topic_properties , fail_on_exist = 1 ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) "}
{"1831": "\ndef delete_topic ( self , topic_name , fail_not_exist = 0 ) : \n    try : \n        return self . mgmt_client . delete_topic ( topic_name , fail_not_exist = fail_not_exist ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) \n    except azure . common . AzureMissingResourceHttpError as e : \n        raise ServiceBusResourceNotFound ( \"Specificed queue does not exist.\" , e ) "}
{"1832": "\ndef create_subscription ( self , topic_name , subscription_name , lock_duration = 30 , requires_session = None , default_message_time_to_live = None , dead_lettering_on_message_expiration = None , dead_lettering_on_filter_evaluation_exceptions = None , enable_batched_operations = None , max_delivery_count = None ) : \n    sub_properties = Subscription ( lock_duration = \"PT{}S\" . format ( int ( lock_duration ) ) , requires_session = requires_session , default_message_time_to_live = default_message_time_to_live , dead_lettering_on_message_expiration = dead_lettering_on_message_expiration , dead_lettering_on_filter_evaluation_exceptions = dead_lettering_on_filter_evaluation_exceptions , max_delivery_count = max_delivery_count , enable_batched_operations = enable_batched_operations ) \n    try : \n        return self . mgmt_client . create_subscription ( topic_name , subscription_name , subscription = sub_properties , fail_on_exist = 1 ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) "}
{"1834": "\ndef get_properties ( self ) : \n    try : \n        self . entity = self . _get_entity ( ) \n        self . properties = dict ( self . entity ) \n        if hasattr ( self . entity , 'requires_session' ) : \n            self . requires_session = self . entity . requires_session \n        return self . properties \n    except AzureServiceBusResourceNotFound : \n        raise ServiceBusResourceNotFound ( \"Specificed queue does not exist.\" ) \n    except azure . common . AzureHttpError : \n        self . entity = None \n        self . properties = { } \n        self . requires_session = 0 \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace not found\" , e ) "}
{"1835": "\ndef expired ( self ) : \n    if self . locked_until and self . locked_until <= datetime . datetime . now ( ) : \n        return 1 \n    return 0 "}
{"1836": "\ndef create ( self , resource_group_name , node_name , session , user_name = None , password = None , retention_period = None , credential_data_format = None , encryption_certificate_thumbprint = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _create_initial ( resource_group_name = resource_group_name , node_name = node_name , session = session , user_name = user_name , password = password , retention_period = retention_period , credential_data_format = credential_data_format , encryption_certificate_thumbprint = encryption_certificate_thumbprint , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'SessionResource' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1837": "\ndef create_subscription ( self , billing_account_name , invoice_section_name , body , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _create_subscription_initial ( billing_account_name = billing_account_name , invoice_section_name = invoice_section_name , body = body , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        header_dict = { 'Location' : 'str' , 'Retry-After' : 'int' , } \n        deserialized = self . _deserialize ( 'SubscriptionCreationResult' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            client_raw_response . add_headers ( header_dict ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1838": "\ndef export_request_rate_by_interval ( self , parameters , location , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _export_request_rate_by_interval_initial ( parameters = parameters , location = location , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'LogAnalyticsOperationResult' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , lro_options = { 'final-state-via' : 'azure-async-operation' } , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1842": "\ndef build_config ( config : Dict [ str , Any ] ) -> Dict [ str , str ] : \n    result = config . copy ( ) \n    is_stable = result . pop ( \"is_stable\" , 0 ) \n    if is_stable : \n        result [ \"classifier\" ] = \"Development Status :: 5 - Production/Stable\" \n    else : \n        result [ \"classifier\" ] = \"Development Status :: 4 - Beta\" \n    package_name = result [ \"package_name\" ] \n    result [ \"package_nspkg\" ] = result . pop ( \"package_nspkg\" , package_name [ : package_name . rindex ( '-' ) ] + \"-nspkg\" ) \n    result [ 'is_arm' ] = result . pop ( \"is_arm\" , 1 ) \n    result [ 'need_msrestazure' ] = result . pop ( \"need_msrestazure\" , 1 ) \n    package_parts = result [ \"package_nspkg\" ] [ : - len ( '-nspkg' ) ] . split ( '-' ) \n    result [ 'nspkg_names' ] = [ \".\" . join ( package_parts [ : i + 1 ] ) for i in range ( len ( package_parts ) ) ] \n    result [ 'init_names' ] = [ \"/\" . join ( package_parts [ : i + 1 ] ) + \"/__init__.py\" for i in range ( len ( package_parts ) ) ] \n    return result "}
{"1843": "\ndef reset_password ( self , user_name , reset_password_payload , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _reset_password_initial ( user_name = user_name , reset_password_payload = reset_password_payload , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1844": "\ndef start_environment ( self , user_name , environment_id , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _start_environment_initial ( user_name = user_name , environment_id = environment_id , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1845": "\ndef _create_message ( response , service_instance ) : \n    respbody = response . body \n    custom_properties = { } \n    broker_properties = None \n    message_type = None \n    message_location = None \n    for name , value in response . headers : \n        if name . lower ( ) == 'brokerproperties' : \n            broker_properties = json . loads ( value ) \n        elif name . lower ( ) == 'content-type' : \n            message_type = value \n        elif name . lower ( ) == 'location' : \n            message_location = value \n        elif name . lower ( ) not in [ 'transfer-encoding' , 'server' , 'date' , 'strict-transport-security' ] : \n            if '\"' in value : \n                value = value [ 1 : - 1 ] . replace ( '\\\\\"' , '\"' ) \n                try : \n                    custom_properties [ name ] = datetime . strptime ( value , '%a, %d %b %Y %H:%M:%S GMT' ) \n                except ValueError : \n                    custom_properties [ name ] = value \n            elif value . lower ( ) == 'true' : \n                custom_properties [ name ] = 1 \n            elif value . lower ( ) == 'false' : \n                custom_properties [ name ] = 0 \n            else : \n                try : \n                    float_value = float ( value ) \n                    if str ( int ( float_value ) ) == value : \n                        custom_properties [ name ] = int ( value ) \n                    else : \n                        custom_properties [ name ] = float_value \n                except ValueError : \n                    pass \n    if message_type is None : \n        message = Message ( respbody , service_instance , message_location , custom_properties , 'application/atom+xml;type=entry;charset=utf-8' , broker_properties ) \n    else : \n        message = Message ( respbody , service_instance , message_location , custom_properties , message_type , broker_properties ) \n    return message "}
{"1846": "\ndef _convert_etree_element_to_rule ( entry_element ) : \n    rule = Rule ( ) \n    rule_element = entry_element . find ( './atom:content/sb:RuleDescription' , _etree_sb_feed_namespaces ) \n    if rule_element is not None : \n        filter_element = rule_element . find ( './sb:Filter' , _etree_sb_feed_namespaces ) \n        if filter_element is not None : \n            rule . filter_type = filter_element . attrib . get ( _make_etree_ns_attr_name ( _etree_sb_feed_namespaces [ 'i' ] , 'type' ) , None ) \n            sql_exp_element = filter_element . find ( './sb:SqlExpression' , _etree_sb_feed_namespaces ) \n            if sql_exp_element is not None : \n                rule . filter_expression = sql_exp_element . text \n        action_element = rule_element . find ( './sb:Action' , _etree_sb_feed_namespaces ) \n        if action_element is not None : \n            rule . action_type = action_element . attrib . get ( _make_etree_ns_attr_name ( _etree_sb_feed_namespaces [ 'i' ] , 'type' ) , None ) \n            sql_exp_element = action_element . find ( './sb:SqlExpression' , _etree_sb_feed_namespaces ) \n            if sql_exp_element is not None : \n                rule . action_expression = sql_exp_element . text \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , 1 , '/rules' ) . items ( ) : \n        setattr ( rule , name , value ) \n    return rule "}
{"1847": "\ndef _convert_etree_element_to_queue ( entry_element ) : \n    queue = Queue ( ) \n    invalid_queue = 1 \n    queue_element = entry_element . find ( './atom:content/sb:QueueDescription' , _etree_sb_feed_namespaces ) \n    if queue_element is not None : \n        mappings = [ ( 'LockDuration' , 'lock_duration' , None ) , ( 'MaxSizeInMegabytes' , 'max_size_in_megabytes' , int ) , ( 'RequiresDuplicateDetection' , 'requires_duplicate_detection' , _parse_bool ) , ( 'RequiresSession' , 'requires_session' , _parse_bool ) , ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'DeadLetteringOnMessageExpiration' , 'dead_lettering_on_message_expiration' , _parse_bool ) , ( 'DuplicateDetectionHistoryTimeWindow' , 'duplicate_detection_history_time_window' , None ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'MaxDeliveryCount' , 'max_delivery_count' , int ) , ( 'MessageCount' , 'message_count' , int ) , ( 'SizeInBytes' , 'size_in_bytes' , int ) , ] \n        for mapping in mappings : \n            if _read_etree_element ( queue_element , mapping [ 0 ] , queue , mapping [ 1 ] , mapping [ 2 ] ) : \n                invalid_queue = 0 \n    if invalid_queue : \n        raise AzureServiceBusResourceNotFound ( _ERROR_QUEUE_NOT_FOUND ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , 1 ) . items ( ) : \n        setattr ( queue , name , value ) \n    return queue "}
{"1848": "\ndef _convert_etree_element_to_topic ( entry_element ) : \n    topic = Topic ( ) \n    invalid_topic = 1 \n    topic_element = entry_element . find ( './atom:content/sb:TopicDescription' , _etree_sb_feed_namespaces ) \n    if topic_element is not None : \n        mappings = [ ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'MaxSizeInMegabytes' , 'max_size_in_megabytes' , int ) , ( 'RequiresDuplicateDetection' , 'requires_duplicate_detection' , _parse_bool ) , ( 'DuplicateDetectionHistoryTimeWindow' , 'duplicate_detection_history_time_window' , None ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'SizeInBytes' , 'size_in_bytes' , int ) , ] \n        for mapping in mappings : \n            if _read_etree_element ( topic_element , mapping [ 0 ] , topic , mapping [ 1 ] , mapping [ 2 ] ) : \n                invalid_topic = 0 \n    if invalid_topic : \n        raise AzureServiceBusResourceNotFound ( _ERROR_TOPIC_NOT_FOUND ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , 1 ) . items ( ) : \n        setattr ( topic , name , value ) \n    return topic "}
{"1849": "\ndef _convert_etree_element_to_subscription ( entry_element ) : \n    subscription = Subscription ( ) \n    subscription_element = entry_element . find ( './atom:content/sb:SubscriptionDescription' , _etree_sb_feed_namespaces ) \n    if subscription_element is not None : \n        mappings = [ ( 'LockDuration' , 'lock_duration' , None ) , ( 'RequiresSession' , 'requires_session' , _parse_bool ) , ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'DeadLetteringOnFilterEvaluationExceptions' , 'dead_lettering_on_filter_evaluation_exceptions' , _parse_bool ) , ( 'DeadLetteringOnMessageExpiration' , 'dead_lettering_on_message_expiration' , _parse_bool ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'MaxDeliveryCount' , 'max_delivery_count' , int ) , ( 'MessageCount' , 'message_count' , int ) , ] \n        for mapping in mappings : \n            _read_etree_element ( subscription_element , mapping [ 0 ] , subscription , mapping [ 1 ] , mapping [ 2 ] ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , 1 , '/subscriptions' ) . items ( ) : \n        setattr ( subscription , name , value ) \n    return subscription "}
{"1850": "\ndef create ( self , resource_group_name , account_name , certificate_name , parameters , if_match = None , if_none_match = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    raw_result = self . _create_initial ( resource_group_name = resource_group_name , account_name = account_name , certificate_name = certificate_name , parameters = parameters , if_match = if_match , if_none_match = if_none_match , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    if raw : \n        return raw_result \n    def long_running_send ( ) : \n        return raw_result . response \n    def get_long_running_status ( status_link , headers = None ) : \n        request = self . _client . get ( status_link ) \n        if headers : \n            request . headers . update ( headers ) \n        header_parameters = { } \n        header_parameters [ 'x-ms-client-request-id' ] = raw_result . response . request . headers [ 'x-ms-client-request-id' ] \n        return self . _client . send ( request , header_parameters , stream = 0 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if response . status_code not in [ 200 ] : \n            exp = CloudError ( response ) \n            exp . request_id = response . headers . get ( 'x-ms-request-id' ) \n            raise exp \n        header_dict = { 'ETag' : 'str' , } \n        deserialized = self . _deserialize ( 'Certificate' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            client_raw_response . add_headers ( header_dict ) \n            return client_raw_response \n        return deserialized \n    long_running_operation_timeout = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    return AzureOperationPoller ( long_running_send , get_long_running_output , get_long_running_status , long_running_operation_timeout ) "}
{"1851": "\ndef delete ( self , resource_group_name , account_name , certificate_name , custom_headers = None , raw = 0 , ** operation_config ) : \n    raw_result = self . _delete_initial ( resource_group_name = resource_group_name , account_name = account_name , certificate_name = certificate_name , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    if raw : \n        return raw_result \n    def long_running_send ( ) : \n        return raw_result . response \n    def get_long_running_status ( status_link , headers = None ) : \n        request = self . _client . get ( status_link ) \n        if headers : \n            request . headers . update ( headers ) \n        header_parameters = { } \n        header_parameters [ 'x-ms-client-request-id' ] = raw_result . response . request . headers [ 'x-ms-client-request-id' ] \n        return self . _client . send ( request , header_parameters , stream = 0 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if response . status_code not in [ 200 , 202 , 204 ] : \n            exp = CloudError ( response ) \n            exp . request_id = response . headers . get ( 'x-ms-request-id' ) \n            raise exp \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            client_raw_response . add_headers ( { 'Location' : 'str' , 'Retry-After' : 'int' , } ) \n            return client_raw_response \n    long_running_operation_timeout = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    return AzureOperationPoller ( long_running_send , get_long_running_output , get_long_running_status , long_running_operation_timeout ) "}
{"1852": "\ndef get_client_from_cli_profile ( client_class , ** kwargs ) : \n    cloud = get_cli_active_cloud ( ) \n    parameters = { } \n    if 'credentials' not in kwargs or 'subscription_id' not in kwargs : \n        resource , _ = _client_resource ( client_class , cloud ) \n        credentials , subscription_id , tenant_id = get_azure_cli_credentials ( resource = resource , with_tenant = 1 ) \n        parameters . update ( { 'credentials' : kwargs . get ( 'credentials' , credentials ) , 'subscription_id' : kwargs . get ( 'subscription_id' , subscription_id ) } ) \n    args = get_arg_spec ( client_class . __init__ ) . args \n    if 'adla_job_dns_suffix' in args and 'adla_job_dns_suffix' not in kwargs : \n        parameters [ 'adla_job_dns_suffix' ] = cloud . suffixes . azure_datalake_analytics_catalog_and_job_endpoint \n    elif 'base_url' in args and 'base_url' not in kwargs : \n        _ , base_url = _client_resource ( client_class , cloud ) \n        if base_url : \n            parameters [ 'base_url' ] = base_url \n        else : \n            parameters [ 'base_url' ] = cloud . endpoints . resource_manager \n    if 'tenant_id' in args and 'tenant_id' not in kwargs : \n        parameters [ 'tenant_id' ] = tenant_id \n    parameters . update ( kwargs ) \n    return _instantiate_client ( client_class , ** parameters ) "}
{"1856": "\ndef get_entry_properties_from_element ( element , include_id , id_prefix_to_skip = None , use_title_as_id = 0 ) : \n    properties = { } \n    etag = element . attrib . get ( _make_etree_ns_attr_name ( _etree_entity_feed_namespaces [ 'm' ] , 'etag' ) , None ) \n    if etag is not None : \n        properties [ 'etag' ] = etag \n    updated = element . findtext ( './atom:updated' , '' , _etree_entity_feed_namespaces ) \n    if updated : \n        properties [ 'updated' ] = updated \n    author_name = element . findtext ( './atom:author/atom:name' , '' , _etree_entity_feed_namespaces ) \n    if author_name : \n        properties [ 'author' ] = author_name \n    if include_id : \n        if use_title_as_id : \n            title = element . findtext ( './atom:title' , '' , _etree_entity_feed_namespaces ) \n            if title : \n                properties [ 'name' ] = title \n        else : \n            element_id = element . findtext ( './atom:id' , '' , _etree_entity_feed_namespaces ) \n            if element_id : \n                properties [ 'name' ] = _get_readable_id ( element_id , id_prefix_to_skip ) \n    return properties "}
{"1857": "\ndef delete ( self , resource_group_name , if_match , provisioning_service_name , certificate_name , certificatename = None , certificateraw_bytes = None , certificateis_verified = None , certificatepurpose = None , certificatecreated = None , certificatelast_updated = None , certificatehas_private_key = None , certificatenonce = None , custom_headers = None , raw = 0 , ** operation_config ) : \n    url = self . delete . metadata [ 'url' ] \n    path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) , 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' ) , 'provisioningServiceName' : self . _serialize . url ( \"provisioning_service_name\" , provisioning_service_name , 'str' ) , 'certificateName' : self . _serialize . url ( \"certificate_name\" , certificate_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if certificatename is not None : \n        query_parameters [ 'certificate.name' ] = self . _serialize . query ( \"certificatename\" , certificatename , 'str' ) \n    if certificateraw_bytes is not None : \n        query_parameters [ 'certificate.rawBytes' ] = self . _serialize . query ( \"certificateraw_bytes\" , certificateraw_bytes , 'bytearray' ) \n    if certificateis_verified is not None : \n        query_parameters [ 'certificate.isVerified' ] = self . _serialize . query ( \"certificateis_verified\" , certificateis_verified , 'bool' ) \n    if certificatepurpose is not None : \n        query_parameters [ 'certificate.purpose' ] = self . _serialize . query ( \"certificatepurpose\" , certificatepurpose , 'str' ) \n    if certificatecreated is not None : \n        query_parameters [ 'certificate.created' ] = self . _serialize . query ( \"certificatecreated\" , certificatecreated , 'iso-8601' ) \n    if certificatelast_updated is not None : \n        query_parameters [ 'certificate.lastUpdated' ] = self . _serialize . query ( \"certificatelast_updated\" , certificatelast_updated , 'iso-8601' ) \n    if certificatehas_private_key is not None : \n        query_parameters [ 'certificate.hasPrivateKey' ] = self . _serialize . query ( \"certificatehas_private_key\" , certificatehas_private_key , 'bool' ) \n    if certificatenonce is not None : \n        query_parameters [ 'certificate.nonce' ] = self . _serialize . query ( \"certificatenonce\" , certificatenonce , 'str' ) \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    header_parameters [ 'If-Match' ] = self . _serialize . header ( \"if_match\" , if_match , 'str' ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    request = self . _client . delete ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , stream = 0 , ** operation_config ) \n    if response . status_code not in [ 200 , 204 ] : \n        raise models . ErrorDetailsException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1866": "\ndef delete_site ( self , webspace_name , website_name , delete_empty_server_farm = 0 , delete_metrics = 0 ) : \n    path = self . _get_sites_details_path ( webspace_name , website_name ) \n    query = '' \n    if delete_empty_server_farm : \n        query += '&deleteEmptyServerFarm=true' \n    if delete_metrics : \n        query += '&deleteMetrics=true' \n    if query : \n        path = path + '?' + query . lstrip ( '&' ) \n    return self . _perform_delete ( path ) "}
{"1867": "\ndef update_site ( self , webspace_name , website_name , state = None ) : \n    xml = _XmlSerializer . update_website_to_xml ( state ) \n    return self . _perform_put ( self . _get_sites_details_path ( webspace_name , website_name ) , xml , as_async = 1 ) "}
{"1868": "\ndef restart_site ( self , webspace_name , website_name ) : \n    return self . _perform_post ( self . _get_restart_path ( webspace_name , website_name ) , None , as_async = 1 ) "}
{"1873": "\ndef update_policies ( self , resource_group_name , registry_name , quarantine_policy = None , trust_policy = None , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _update_policies_initial ( resource_group_name = resource_group_name , registry_name = registry_name , quarantine_policy = quarantine_policy , trust_policy = trust_policy , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        deserialized = self . _deserialize ( 'RegistryPolicies' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            return client_raw_response \n        return deserialized \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1874": "\ndef create_cloud_service ( self , cloud_service_id , label , description , geo_region ) : \n    _validate_not_none ( 'cloud_service_id' , cloud_service_id ) \n    _validate_not_none ( 'label' , label ) \n    _validate_not_none ( 'description' , description ) \n    _validate_not_none ( 'geo_region' , geo_region ) \n    path = self . _get_cloud_services_path ( cloud_service_id ) \n    body = _SchedulerManagementXmlSerializer . create_cloud_service_to_xml ( label , description , geo_region ) \n    return self . _perform_put ( path , body , as_async = 1 ) "}
{"1877": "\ndef complete_restore ( self , location_name , operation_id , last_backup_name , custom_headers = None , raw = 0 , polling = 1 , ** operation_config ) : \n    raw_result = self . _complete_restore_initial ( location_name = location_name , operation_id = operation_id , last_backup_name = last_backup_name , custom_headers = custom_headers , raw = 1 , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            return client_raw_response \n    lro_delay = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    if polling is 1 : \n        polling_method = ARMPolling ( lro_delay , ** operation_config ) \n    elif polling is 0 : \n        polling_method = NoPolling ( ) \n    else : \n        polling_method = polling \n    return LROPoller ( self . _client , raw_result , get_long_running_output , polling_method ) "}
{"1879": "\nasync def send_pending_messages ( self ) : \n    if not self . running : \n        await self . open ( ) \n    try : \n        pending = self . _handler . _pending_messages [ : ] \n        await self . _handler . wait_async ( ) \n        results = [ ] \n        for m in pending : \n            if m . state == constants . MessageState . SendFailed : \n                results . append ( ( 0 , MessageSendFailed ( m . _response ) ) ) \n            else : \n                results . append ( ( 1 , None ) ) \n        return results \n    except Exception as e : \n        raise MessageSendFailed ( e ) "}
{"1891": "\ndef segments ( self , ** kwargs ) : \n    segmentBase = self . segmentBase or self . walk_back_get_attr ( \"segmentBase\" ) \n    segmentLists = self . segmentList or self . walk_back_get_attr ( \"segmentList\" ) \n    segmentTemplate = self . segmentTemplate or self . walk_back_get_attr ( \"segmentTemplate\" ) \n    if segmentTemplate : \n        for segment in segmentTemplate . segments ( RepresentationID = self . id , Bandwidth = int ( self . bandwidth * 1000 ) , ** kwargs ) : \n            if segment . init : \n                yield segment \n            else : \n                yield segment \n    elif segmentLists : \n        for segmentList in segmentLists : \n            for segment in segmentList . segments : \n                yield segment \n    else : \n        yield Segment ( self . base_url , 0 , 1 , 1 ) "}
{"1894": "\ndef queue ( self , queue_ , value ) : \n    while not self . closed : \n        try : \n            queue_ . put ( value , block = 1 , timeout = 1 ) \n            return \n        except queue . Full : \n            continue "}
{"1895": "\ndef _pv_params ( cls , session , pvswf , pv , ** request_params ) : \n    try : \n        data , hdntl = pv . split ( \";\" ) \n    except ValueError : \n        data = pv \n        hdntl = \"\" \n    cache = Cache ( filename = \"stream.json\" ) \n    key = \"akamaihd-player:\" + pvswf \n    cached = cache . get ( key ) \n    request_params = deepcopy ( request_params ) \n    headers = request_params . pop ( \"headers\" , { } ) \n    if cached : \n        headers [ \"If-Modified-Since\" ] = cached [ \"modified\" ] \n    swf = session . http . get ( pvswf , headers = headers , ** request_params ) \n    if cached and swf . status_code == 304 : \n        hash = cached [ \"hash\" ] \n    else : \n        hash = sha256 ( ) \n        hash . update ( swfdecompress ( swf . content ) ) \n        hash = base64 . b64encode ( hash . digest ( ) ) . decode ( \"ascii\" ) \n        modified = swf . headers . get ( \"Last-Modified\" , \"\" ) \n        if len ( modified ) < 40 : \n            cache . set ( key , dict ( hash = hash , modified = modified ) ) \n    msg = \"st=0~exp=9999999999~acl=*~data={0}!{1}\" . format ( data , hash ) \n    auth = hmac . new ( AKAMAIHD_PV_KEY , msg . encode ( \"ascii\" ) , sha256 ) \n    pvtoken = \"{0}~hmac={1}\" . format ( msg , auth . hexdigest ( ) ) \n    params = [ ( \"pvtoken\" , pvtoken ) ] \n    params . extend ( parse_qsl ( hdntl , keep_blank_values = 1 ) ) \n    return params "}
{"1899": "\ndef parse_xml ( data , name = \"XML\" , ignore_ns = 0 , exception = PluginError , schema = None , invalid_char_entities = 0 ) : \n    if is_py2 and isinstance ( data , unicode ) : \n        data = data . encode ( \"utf8\" ) \n    elif is_py3 and isinstance ( data , str ) : \n        data = bytearray ( data , \"utf8\" ) \n    if ignore_ns : \n        data = re . sub ( br\"[\\t ]xmlns=\\\"(.+?)\\\"\" , b\"\" , data ) \n    if invalid_char_entities : \n        data = re . sub ( br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)' , b'&amp;' , data ) \n    try : \n        tree = ET . fromstring ( data ) \n    except Exception as err : \n        snippet = repr ( data ) \n        if len ( snippet ) > 35 : \n            snippet = snippet [ : 35 ] + \" ...\" \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        tree = schema . validate ( tree , name = name , exception = exception ) \n    return tree "}
{"1904": "\ndef parse_manifest ( cls , session , url_or_manifest , ** args ) : \n    ret = { } \n    if url_or_manifest . startswith ( '<?xml' ) : \n        mpd = MPD ( parse_xml ( url_or_manifest , ignore_ns = 1 ) ) \n    else : \n        res = session . http . get ( url_or_manifest , ** args ) \n        url = res . url \n        urlp = list ( urlparse ( url ) ) \n        urlp [ 2 ] , _ = urlp [ 2 ] . rsplit ( \"/\" , 1 ) \n        mpd = MPD ( session . http . xml ( res , ignore_ns = 1 ) , base_url = urlunparse ( urlp ) , url = url ) \n    video , audio = [ ] , [ ] \n    for aset in mpd . periods [ 0 ] . adaptationSets : \n        if aset . contentProtection : \n            raise PluginError ( \"{} is protected by DRM\" . format ( url ) ) \n        for rep in aset . representations : \n            if rep . mimeType . startswith ( \"video\" ) : \n                video . append ( rep ) \n            elif rep . mimeType . startswith ( \"audio\" ) : \n                audio . append ( rep ) \n    if not video : \n        video = [ None ] \n    if not audio : \n        audio = [ None ] \n    locale = session . localization \n    locale_lang = locale . language \n    lang = None \n    available_languages = set ( ) \n    for aud in audio : \n        if aud and aud . lang : \n            available_languages . add ( aud . lang ) \n            try : \n                if locale . explicit and aud . lang and Language . get ( aud . lang ) == locale_lang : \n                    lang = aud . lang \n            except LookupError : \n                continue \n    if not lang : \n        lang = audio [ 0 ] and audio [ 0 ] . lang \n    log . debug ( \"Available languages for DASH audio streams: {0} (using: {1})\" . format ( \", \" . join ( available_languages ) or \"NONE\" , lang or \"n/a\" ) ) \n    if len ( available_languages ) > 1 : \n        audio = list ( filter ( lambda a : a . lang is None or a . lang == lang , audio ) ) \n    for vid , aud in itertools . product ( video , audio ) : \n        stream = DASHStream ( session , mpd , vid , aud , ** args ) \n        stream_name = [ ] \n        if vid : \n            stream_name . append ( \"{:0.0f}{}\" . format ( vid . height or vid . bandwidth_rounded , \"p\" if vid . height else \"k\" ) ) \n        if audio and len ( audio ) > 1 : \n            stream_name . append ( \"a{:0.0f}k\" . format ( aud . bandwidth ) ) \n        ret [ '+' . join ( stream_name ) ] = stream \n    return ret "}
{"1913": "\ndef login ( self ) : \n    email = self . get_option ( \"email\" ) \n    password = self . get_option ( \"password\" ) \n    if email and password : \n        res = self . session . http . get ( self . login_url ) \n        csrf_match = self . csrf_re . search ( res . text ) \n        token = csrf_match and csrf_match . group ( 1 ) \n        self . logger . debug ( \"Attempting login as {0} (token={1})\" , email , token ) \n        res = self . session . http . post ( self . login_url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow_redirects = 0 , raise_for_status = 0 , headers = { \"Referer\" : self . login_url } ) \n        if res . status_code != 302 : \n            self . logger . error ( \"Failed to login to LiveEdu account: {0}\" , email ) "}
{"1916": "\ndef iter_chunks ( self , fd = None , buf = None , skip_header = None ) : \n    timestamps = dict ( self . timestamps_add ) \n    tag_iterator = self . iter_tags ( fd = fd , buf = buf , skip_header = skip_header ) \n    if not self . flv_header_written : \n        analyzed_tags = self . analyze_tags ( tag_iterator ) \n    else : \n        analyzed_tags = [ ] \n    for tag in chain ( analyzed_tags , tag_iterator ) : \n        if not self . flv_header_written : \n            flv_header = Header ( has_video = self . has_video , has_audio = self . has_audio ) \n            yield flv_header . serialize ( ) \n            self . flv_header_written = 1 \n        if self . verify_tag ( tag ) : \n            self . adjust_tag_gap ( tag ) \n            self . adjust_tag_timestamp ( tag ) \n            if self . duration : \n                norm_timestamp = tag . timestamp / 1000 \n                if norm_timestamp > self . duration : \n                    break \n            yield tag . serialize ( ) \n            timestamps [ tag . type ] = tag . timestamp \n    if not self . flatten_timestamps : \n        self . timestamps_add = timestamps \n    self . tags = [ ] "}
{"1922": "\ndef output_stream_http ( plugin , initial_streams , external = 0 , port = 0 ) : \n    global output \n    if not external : \n        if not args . player : \n            console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) \n        title = create_title ( plugin ) \n        server = create_http_server ( ) \n        player = output = PlayerOutput ( args . player , args = args . player_args , filename = server . url , quiet = not args . verbose_player , title = title ) \n        try : \n            log . info ( \"Starting player: {0}\" , args . player ) \n            if player : \n                player . open ( ) \n        except OSError as err : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n    else : \n        server = create_http_server ( host = None , port = port ) \n        player = None \n        log . info ( \"Starting server, access with one of:\" ) \n        for url in server . urls : \n            log . info ( \" \" + url ) \n    for req in iter_http_requests ( server , player ) : \n        user_agent = req . headers . get ( \"User-Agent\" ) or \"unknown player\" \n        log . info ( \"Got HTTP request from {0}\" . format ( user_agent ) ) \n        stream_fd = prebuffer = None \n        while not stream_fd and ( not player or player . running ) : \n            try : \n                streams = initial_streams or fetch_streams ( plugin ) \n                initial_streams = None \n                for stream_name in ( resolve_stream_name ( streams , s ) for s in args . stream ) : \n                    if stream_name in streams : \n                        stream = streams [ stream_name ] \n                        break \n                else : \n                    log . info ( \"Stream not available, will re-fetch \" \"streams in 10 sec\" ) \n                    sleep ( 10 ) \n                    continue \n            except PluginError as err : \n                log . error ( u\"Unable to fetch new streams: {0}\" , err ) \n                continue \n            try : \n                log . info ( \"Opening stream: {0} ({1})\" , stream_name , type ( stream ) . shortname ( ) ) \n                stream_fd , prebuffer = open_stream ( stream ) \n            except StreamError as err : \n                log . error ( \"{0}\" , err ) \n        if stream_fd and prebuffer : \n            log . debug ( \"Writing stream to player\" ) \n            read_stream ( stream_fd , server , prebuffer ) \n        server . close ( 1 ) \n    player . close ( ) \n    server . close ( ) "}
{"1923": "\ndef output_stream_passthrough ( plugin , stream ) : \n    global output \n    title = create_title ( plugin ) \n    filename = '\"{0}\"' . format ( stream_to_url ( stream ) ) \n    output = PlayerOutput ( args . player , args = args . player_args , filename = filename , call = 1 , quiet = not args . verbose_player , title = title ) \n    try : \n        log . info ( \"Starting player: {0}\" , args . player ) \n        output . open ( ) \n    except OSError as err : \n        console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n        return 0 \n    return 1 "}
{"1925": "\ndef output_stream ( plugin , stream ) : \n    global output \n    success_open = 0 \n    for i in range ( args . retry_open ) : \n        try : \n            stream_fd , prebuffer = open_stream ( stream ) \n            success_open = 1 \n            break \n        except StreamError as err : \n            log . error ( \"Try {0}/{1}: Could not open stream {2} ({3})\" , i + 1 , args . retry_open , stream , err ) \n    if not success_open : \n        console . exit ( \"Could not open stream {0}, tried {1} times, exiting\" , stream , args . retry_open ) \n    output = create_output ( plugin ) \n    try : \n        output . open ( ) \n    except ( IOError , OSError ) as err : \n        if isinstance ( output , PlayerOutput ) : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n        else : \n            console . exit ( \"Failed to open output: {0} ({1})\" , args . output , err ) \n    with closing ( output ) : \n        log . debug ( \"Writing stream to output\" ) \n        read_stream ( stream_fd , output , prebuffer ) \n    return 1 "}
{"1927": "\ndef handle_stream ( plugin , streams , stream_name ) : \n    stream_name = resolve_stream_name ( streams , stream_name ) \n    stream = streams [ stream_name ] \n    if args . subprocess_cmdline : \n        if isinstance ( stream , StreamProcess ) : \n            try : \n                cmdline = stream . cmdline ( ) \n            except StreamError as err : \n                console . exit ( \"{0}\" , err ) \n            console . msg ( \"{0}\" , cmdline ) \n        else : \n            console . exit ( \"The stream specified cannot be translated to a command\" ) \n    elif console . json : \n        console . msg_json ( stream ) \n    elif args . stream_url : \n        try : \n            console . msg ( \"{0}\" , stream . to_url ( ) ) \n        except TypeError : \n            console . exit ( \"The stream specified cannot be translated to a URL\" ) \n    else : \n        alt_streams = list ( filter ( lambda k : stream_name + \"_alt\" in k , sorted ( streams . keys ( ) ) ) ) \n        file_output = args . output or args . stdout \n        for stream_name in [ stream_name ] + alt_streams : \n            stream = streams [ stream_name ] \n            stream_type = type ( stream ) . shortname ( ) \n            if stream_type in args . player_passthrough and not file_output : \n                log . info ( \"Opening stream: {0} ({1})\" , stream_name , stream_type ) \n                success = output_stream_passthrough ( plugin , stream ) \n            elif args . player_external_http : \n                return output_stream_http ( plugin , streams , external = 1 , port = args . player_external_http_port ) \n            elif args . player_continuous_http and not file_output : \n                return output_stream_http ( plugin , streams ) \n            else : \n                log . info ( \"Opening stream: {0} ({1})\" , stream_name , stream_type ) \n                success = output_stream ( plugin , stream ) \n            if success : \n                break "}
{"1936": "\ndef setup_args ( parser , config_files = [ ] , ignore_unknown = 0 ) : \n    global args \n    arglist = sys . argv [ 1 : ] \n    for config_file in filter ( os . path . isfile , config_files ) : \n        arglist . insert ( 0 , \"@\" + config_file ) \n    args , unknown = parser . parse_known_args ( arglist ) \n    if unknown and not ignore_unknown : \n        msg = gettext ( 'unrecognized arguments: %s' ) \n        parser . error ( msg % ' ' . join ( unknown ) ) \n    if args . stream : \n        args . stream = [ stream . lower ( ) for stream in args . stream ] \n    if not args . url and args . url_param : \n        args . url = args . url_param "}
{"1938": "\ndef setup_http_session ( ) : \n    if args . http_proxy : \n        streamlink . set_option ( \"http-proxy\" , args . http_proxy ) \n    if args . https_proxy : \n        streamlink . set_option ( \"https-proxy\" , args . https_proxy ) \n    if args . http_cookie : \n        streamlink . set_option ( \"http-cookies\" , dict ( args . http_cookie ) ) \n    if args . http_header : \n        streamlink . set_option ( \"http-headers\" , dict ( args . http_header ) ) \n    if args . http_query_param : \n        streamlink . set_option ( \"http-query-params\" , dict ( args . http_query_param ) ) \n    if args . http_ignore_env : \n        streamlink . set_option ( \"http-trust-env\" , 0 ) \n    if args . http_no_ssl_verify : \n        streamlink . set_option ( \"http-ssl-verify\" , 0 ) \n    if args . http_disable_dh : \n        streamlink . set_option ( \"http-disable-dh\" , 1 ) \n    if args . http_ssl_cert : \n        streamlink . set_option ( \"http-ssl-cert\" , args . http_ssl_cert ) \n    if args . http_ssl_cert_crt_key : \n        streamlink . set_option ( \"http-ssl-cert\" , tuple ( args . http_ssl_cert_crt_key ) ) \n    if args . http_timeout : \n        streamlink . set_option ( \"http-timeout\" , args . http_timeout ) \n    if args . http_cookies : \n        streamlink . set_option ( \"http-cookies\" , args . http_cookies ) \n    if args . http_headers : \n        streamlink . set_option ( \"http-headers\" , args . http_headers ) \n    if args . http_query_params : \n        streamlink . set_option ( \"http-query-params\" , args . http_query_params ) "}
{"1948": "\ndef resolve_url ( self , url , follow_redirect = 1 ) : \n    url = update_scheme ( \"http://\" , url ) \n    available_plugins = [ ] \n    for name , plugin in self . plugins . items ( ) : \n        if plugin . can_handle_url ( url ) : \n            available_plugins . append ( plugin ) \n    available_plugins . sort ( key = lambda x : x . priority ( url ) , reverse = 1 ) \n    if available_plugins : \n        return available_plugins [ 0 ] ( url ) \n    if follow_redirect : \n        try : \n            res = self . http . head ( url , allow_redirects = 1 , acceptable_status = [ 501 ] ) \n            if res . status_code == 501 : \n                res = self . http . get ( url , stream = 1 ) \n            if res . url != url : \n                return self . resolve_url ( res . url , follow_redirect = follow_redirect ) \n        except PluginError : \n            pass \n    raise NoPluginError "}
{"1951": "\ndef startswith ( string ) : \n    def starts_with ( value ) : \n        validate ( text , value ) \n        if not value . startswith ( string ) : \n            raise ValueError ( \"'{0}' does not start with '{1}'\" . format ( value , string ) ) \n        return 1 \n    return starts_with "}
{"1952": "\ndef endswith ( string ) : \n    def ends_with ( value ) : \n        validate ( text , value ) \n        if not value . endswith ( string ) : \n            raise ValueError ( \"'{0}' does not end with '{1}'\" . format ( value , string ) ) \n        return 1 \n    return ends_with "}
{"1953": "\ndef contains ( string ) : \n    def contains_str ( value ) : \n        validate ( text , value ) \n        if string not in value : \n            raise ValueError ( \"'{0}' does not contain '{1}'\" . format ( value , string ) ) \n        return 1 \n    return contains_str "}
{"1957": "\ndef url ( ** attributes ) : \n    def check_url ( value ) : \n        validate ( text , value ) \n        parsed = urlparse ( value ) \n        if not parsed . netloc : \n            raise ValueError ( \"'{0}' is not a valid URL\" . format ( value ) ) \n        for name , schema in attributes . items ( ) : \n            if not _hasattr ( parsed , name ) : \n                raise ValueError ( \"Invalid URL attribute '{0}'\" . format ( name ) ) \n            try : \n                validate ( schema , _getattr ( parsed , name ) ) \n            except ValueError as err : \n                raise ValueError ( \"Unable to validate URL attribute '{0}': {1}\" . format ( name , err ) ) \n        return 1 \n    if attributes . get ( \"scheme\" ) == \"http\" : \n        attributes [ \"scheme\" ] = any ( \"http\" , \"https\" ) \n    return check_url "}
{"1963": "\ndef dologin ( self , email , password , emailauth = \"\" , emailsteamid = \"\" , captchagid = \"-1\" , captcha_text = \"\" , twofactorcode = \"\" ) : \n    epassword , rsatimestamp = self . encrypt_password ( email , password ) \n    login_data = { 'username' : email , \"password\" : epassword , \"emailauth\" : emailauth , \"loginfriendlyname\" : \"Streamlink\" , \"captchagid\" : captchagid , \"captcha_text\" : captcha_text , \"emailsteamid\" : emailsteamid , \"rsatimestamp\" : rsatimestamp , \"remember_login\" : 1 , \"donotcache\" : self . donotcache , \"twofactorcode\" : twofactorcode } \n    res = self . session . http . post ( self . _dologin_url , data = login_data ) \n    resp = self . session . http . json ( res , schema = self . _dologin_schema ) \n    if not resp [ u\"success\" ] : \n        if resp . get ( u\"captcha_needed\" ) : \n            captchagid = resp [ u\"captcha_gid\" ] \n            log . error ( \"Captcha result required, open this URL to see the captcha: {}\" . format ( self . _captcha_url . format ( captchagid ) ) ) \n            try : \n                captcha_text = self . input_ask ( \"Captcha text\" ) \n            except FatalPluginError : \n                captcha_text = None \n            if not captcha_text : \n                return 0 \n        else : \n            if resp . get ( u\"emailauth_needed\" ) : \n                if not emailauth : \n                    try : \n                        emailauth = self . input_ask ( \"Email auth code required\" ) \n                    except FatalPluginError : \n                        emailauth = None \n                    if not emailauth : \n                        return 0 \n                else : \n                    raise SteamLoginFailed ( \"Email auth key error\" ) \n            if resp . get ( u\"requires_twofactor\" ) : \n                try : \n                    twofactorcode = self . input_ask ( \"Two factor auth code required\" ) \n                except FatalPluginError : \n                    twofactorcode = None \n                if not twofactorcode : \n                    return 0 \n            if resp . get ( u\"message\" ) : \n                raise SteamLoginFailed ( resp [ u\"message\" ] ) \n        return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u\"emailsteamid\" , u\"\" ) , captcha_text = captcha_text , captchagid = captchagid , twofactorcode = twofactorcode ) \n    elif resp . get ( \"login_complete\" ) : \n        return 1 \n    else : \n        log . error ( \"Something when wrong when logging in to Steam\" ) \n        return 0 "}
{"1966": "\ndef _login ( self , username , password ) : \n    self . logger . debug ( 'login ...' ) \n    res = self . session . http . get ( self . login_url ) \n    input_list = self . _input_re . findall ( res . text ) \n    if not input_list : \n        raise PluginError ( 'Missing input data on login website.' ) \n    data = { } \n    for _input_data in input_list : \n        try : \n            _input_name = self . _name_re . search ( _input_data ) . group ( 1 ) \n        except AttributeError : \n            continue \n        try : \n            _input_value = self . _value_re . search ( _input_data ) . group ( 1 ) \n        except AttributeError : \n            _input_value = '' \n        data [ _input_name ] = _input_value \n    login_data = { 'ctl00$Login1$UserName' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$LoginButton.x' : '0' , 'ctl00$Login1$LoginButton.y' : '0' } \n    data . update ( login_data ) \n    res = self . session . http . post ( self . login_url , data = data ) \n    for cookie in self . session . http . cookies : \n        self . _session_attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) \n    if self . _session_attributes . get ( 'ASP.NET_SessionId' ) and self . _session_attributes . get ( '.abportail1' ) : \n        self . logger . debug ( 'New session data' ) \n        self . set_expires_time_cache ( ) \n        return 1 \n    else : \n        self . logger . error ( 'Failed to login, check your username/password' ) \n        return 0 "}
{"1968": "\ndef _api_call ( self , entrypoint , params = None , schema = None ) : \n    url = self . _api_url . format ( entrypoint ) \n    params = params or { } \n    if self . session_id : \n        params . update ( { \"session_id\" : self . session_id } ) \n    else : \n        params . update ( { \"device_id\" : self . device_id , \"device_type\" : self . _access_type , \"access_token\" : self . _access_token , \"version\" : self . _version_code } ) \n    params . update ( { \"locale\" : self . locale . replace ( '_' , '' ) , } ) \n    if self . session_id : \n        params [ \"session_id\" ] = self . session_id \n    res = self . session . http . post ( url , data = params , headers = self . headers , verify = 0 ) \n    json_res = self . session . http . json ( res , schema = _api_schema ) \n    if json_res [ \"error\" ] : \n        err_msg = json_res . get ( \"message\" , \"Unknown error\" ) \n        err_code = json_res . get ( \"code\" , \"unknown_error\" ) \n        raise CrunchyrollAPIError ( err_msg , err_code ) \n    data = json_res . get ( \"data\" ) \n    if schema : \n        data = schema . validate ( data , name = \"API response\" ) \n    return data "}
{"1987": "\ndef mnemonic ( self , index , verbose = 0 ) : \n    if index < 16 : \n        return [ 'last' , '2last' , '3last' , '4last' , 'last-1' , 'last+1' , 'last-2' , 'last+2' , 'last-3' , 'last+3' , '2last-1' , '2last+1' , '2last-2' , '2last+2' , '2last-3' , '2last+3' ] [ index ] \n    if index < 16 + self . NDIRECT : \n        return str ( index - 16 ) \n    index -= self . NDIRECT + 16 \n    hcode = index >> self . NPOSTFIX \n    lcode = index & ( 1 << self . NPOSTFIX ) - 1 \n    if self . NPOSTFIX : \n        formatString = '1{0}{1}{2:0{3}b}{4:+d}' \n    else : \n        formatString = '1{0}{1}{4:+d}' \n    return formatString . format ( hcode & 1 , 'x' * ( 2 + hcode >> 1 ) if hcode < 13 or verbose else '[{}*x]' . format ( 2 + hcode >> 1 ) , lcode , self . NPOSTFIX , self . NDIRECT + 1 - ( 4 << self . NPOSTFIX ) ) "}
{"1988": "\ndef compileActions ( self ) : \n    import re \n    self . actionList = actions = [ None ] * 121 \n    actions [ 73 ] = \"b' the '+w+b' of the '\" \n    actionLines = self . actionTable . splitlines ( ) \n    colonPositions = [ m . start ( ) for m in re . finditer ( ':' , actionLines [ 1 ] ) ] + [ 100 ] \n    columns = [ ( colonPositions [ i ] - 3 , colonPositions [ i + 1 ] - 3 ) for i in range ( len ( colonPositions ) - 1 ) ] \n    for line in self . actionTable . splitlines ( keepends = 0 ) : \n        for start , end in columns : \n            action = line [ start : end ] \n            if not action or action . isspace ( ) : \n                continue \n            index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] \n            assert colon == ':' \n            action = action . rstrip ( ) \n            action = action . replace ( '_' , ' ' ) \n            wPos = action . index ( 'w' ) \n            action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) \n            action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) \n            action = action . replace ( \".U\" , \".upper()\" ) \n            actions [ int ( index ) ] = action "}
{"1991": "\ndef processStream ( self ) : \n    print ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10 ) ) \n    print ( 'Stream header' . center ( 60 , '-' ) ) \n    self . windowSize = self . verboseRead ( WindowSizeAlphabet ( ) ) \n    print ( 'Metablock header' . center ( 60 , '=' ) ) \n    self . ISLAST = 0 \n    self . output = bytearray ( ) \n    while not self . ISLAST : \n        self . ISLAST = self . verboseRead ( BoolCode ( 'LAST' , description = \"Last block\" ) ) \n        if self . ISLAST : \n            if self . verboseRead ( BoolCode ( 'EMPTY' , description = \"Empty block\" ) ) : \n                break \n        if self . metablockLength ( ) : \n            continue \n        if not self . ISLAST and self . uncompressed ( ) : \n            continue \n        print ( 'Block type descriptors' . center ( 60 , '-' ) ) \n        self . numberOfBlockTypes = { } \n        self . currentBlockCounts = { } \n        self . blockTypeCodes = { } \n        self . blockCountCodes = { } \n        for blockType in ( L , I , D ) : \n            self . blockType ( blockType ) \n        print ( 'Distance code parameters' . center ( 60 , '-' ) ) \n        self . NPOSTFIX , self . NDIRECT = self . verboseRead ( DistanceParamAlphabet ( ) ) \n        self . readLiteralContextModes ( ) \n        print ( 'Context maps' . center ( 60 , '-' ) ) \n        self . cmaps = { } \n        numberOfTrees = { I : self . numberOfBlockTypes [ I ] } \n        for blockType in ( L , D ) : \n            numberOfTrees [ blockType ] = self . contextMap ( blockType ) \n        print ( 'Prefix code lists' . center ( 60 , '-' ) ) \n        self . prefixCodes = { } \n        for blockType in ( L , I , D ) : \n            self . readPrefixArray ( blockType , numberOfTrees [ blockType ] ) \n        self . metablock ( ) "}
{"1992": "\ndef metablockLength ( self ) : \n    self . MLEN = self . verboseRead ( MetablockLengthAlphabet ( ) ) \n    if self . MLEN : \n        return 0 \n    self . verboseRead ( ReservedAlphabet ( ) ) \n    MSKIP = self . verboseRead ( SkipLengthAlphabet ( ) ) \n    self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n    self . stream . pos += 8 * MSKIP \n    print ( \"Skipping to {:x}\" . format ( self . stream . pos >> 3 ) ) \n    return 1 "}
{"1999": "\ndef arrow_table_from_vaex_df ( ds , column_names = None , selection = None , strings = 1 , virtual = 0 ) : \n    names = [ ] \n    arrays = [ ] \n    for name , array in ds . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : \n        names . append ( name ) \n        arrays . append ( arrow_array_from_numpy_array ( array ) ) \n    return pyarrow . Table . from_arrays ( arrays , names ) "}
{"2001": "\ndef add_virtual_columns_cartesian_velocities_to_pmvr ( self , x = \"x\" , y = \"y\" , z = \"z\" , vx = \"vx\" , vy = \"vy\" , vz = \"vz\" , vr = \"vr\" , pm_long = \"pm_long\" , pm_lat = \"pm_lat\" , distance = None ) : \n    if distance is None : \n        distance = \"sqrt({x}**2+{y}**2+{z}**2)\" . format ( ** locals ( ) ) \n    k = 4.74057 \n    self . add_variable ( \"k\" , k , overwrite = 0 ) \n    self . add_virtual_column ( vr , \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( pm_long , \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)/{distance}/k\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( pm_lat , \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( ({x}**2+{y}**2+{z}**2) * sqrt({x}**2+{y}**2) )/k\" . format ( ** locals ( ) ) ) "}
{"2002": "\ndef add_virtual_columns_proper_motion2vperpendicular ( self , distance = \"distance\" , pm_long = \"pm_l\" , pm_lat = \"pm_b\" , vl = \"vl\" , vb = \"vb\" , propagate_uncertainties = 0 , radians = 0 ) : \n    k = 4.74057 \n    self . add_variable ( \"k\" , k , overwrite = 0 ) \n    self . add_virtual_column ( vl , \"k*{pm_long}*{distance}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( vb , \"k* {pm_lat}*{distance}\" . format ( ** locals ( ) ) ) \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ vl ] , self [ vb ] ] ) "}
{"2004": "\ndef value_counts ( self , dropna = 0 , dropnull = 1 , ascending = 0 , progress = 0 ) : \n    from pandas import Series \n    dtype = self . dtype \n    transient = self . transient or self . ds . filtered or self . ds . is_masked ( self . expression ) \n    if self . dtype == str_type and not transient : \n        ar = self . ds . columns [ self . expression ] \n        if not isinstance ( ar , ColumnString ) : \n            transient = 1 \n    counter_type = counter_type_from_dtype ( self . dtype , transient ) \n    counters = [ None ] * self . ds . executor . thread_pool . nthreads \n    def map ( thread_index , i1 , i2 , ar ) : \n        if counters [ thread_index ] is None : \n            counters [ thread_index ] = counter_type ( ) \n        if dtype == str_type : \n            previous_ar = ar \n            ar = _to_string_sequence ( ar ) \n            if not transient : \n                assert ar is previous_ar . string_sequence \n        if np . ma . isMaskedArray ( ar ) : \n            mask = np . ma . getmaskarray ( ar ) \n            counters [ thread_index ] . update ( ar , mask ) \n        else : \n            counters [ thread_index ] . update ( ar ) \n        return 0 \n    def reduce ( a , b ) : \n        return a + b \n    self . ds . map_reduce ( map , reduce , [ self . expression ] , delay = 0 , progress = progress , name = 'value_counts' , info = 1 , to_numpy = 0 ) \n    counters = [ k for k in counters if k is not None ] \n    counter0 = counters [ 0 ] \n    for other in counters [ 1 : ] : \n        counter0 . merge ( other ) \n    value_counts = counter0 . extract ( ) \n    index = np . array ( list ( value_counts . keys ( ) ) ) \n    counts = np . array ( list ( value_counts . values ( ) ) ) \n    order = np . argsort ( counts ) \n    if not ascending : \n        order = order [ : : - 1 ] \n    counts = counts [ order ] \n    index = index [ order ] \n    if not dropna or not dropnull : \n        index = index . tolist ( ) \n        counts = counts . tolist ( ) \n        if not dropna and counter0 . nan_count : \n            index = [ np . nan ] + index \n            counts = [ counter0 . nan_count ] + counts \n        if not dropnull and counter0 . null_count : \n            index = [ 'null' ] + index \n            counts = [ counter0 . null_count ] + counts \n    return Series ( counts , index = index ) "}
{"2005": "\ndef map ( self , mapper , nan_mapping = None , null_mapping = None ) : \n    assert isinstance ( mapper , collectionsAbc . Mapping ) , \"mapper should be a dict like object\" \n    df = self . ds \n    mapper_keys = np . array ( list ( mapper . keys ( ) ) ) \n    key_set = df . _set ( self . expression ) \n    found_keys = key_set . keys ( ) \n    mapper_has_nan = any ( [ key != key for key in mapper_keys ] ) \n    if not set ( mapper_keys ) . issuperset ( found_keys ) : \n        missing = set ( found_keys ) . difference ( mapper_keys ) \n        missing0 = list ( missing ) [ 0 ] \n        if missing0 == missing0 : \n            raise ValueError ( 'Missing values in mapper: %s' % missing ) \n    choices = [ mapper [ key ] for key in found_keys ] \n    if key_set . has_nan : \n        if mapper_has_nan : \n            choices = [ mapper [ np . nan ] ] + choices \n        else : \n            choices = [ nan_mapping ] + choices \n    if key_set . has_null : \n        choices = [ null_mapping ] + choices \n    choices = np . array ( choices ) \n    key_set_name = df . add_variable ( 'map_key_set' , key_set , unique = 1 ) \n    choices_name = df . add_variable ( 'map_choices' , choices , unique = 1 ) \n    expr = '_choose(_ordinal_values({}, {}), {})' . format ( self , key_set_name , choices_name ) \n    return Expression ( df , expr ) "}
{"2012": "\ndef from_pandas ( df , name = \"pandas\" , copy_index = 1 , index_name = \"index\" ) : \n    import six \n    vaex_df = vaex . dataframe . DataFrameArrays ( name ) \n    def add ( name , column ) : \n        values = column . values \n        try : \n            vaex_df . add_column ( name , values ) \n        except Exception as e : \n            print ( \"could not convert column %s, error: %r, will try to convert it to string\" % ( name , e ) ) \n            try : \n                values = values . astype ( \"S\" ) \n                vaex_df . add_column ( name , values ) \n            except Exception as e : \n                print ( \"Giving up column %s, error: %r\" % ( name , e ) ) \n    for name in df . columns : \n        add ( name , df [ name ] ) \n    if copy_index : \n        add ( index_name , df . index ) \n    return vaex_df "}
{"2013": "\ndef from_csv ( filename_or_buffer , copy_index = 1 , ** kwargs ) : \n    import pandas as pd \n    return from_pandas ( pd . read_csv ( filename_or_buffer , ** kwargs ) , copy_index = copy_index ) "}
{"2014": "\ndef server ( url , ** kwargs ) : \n    from vaex . remote import ServerRest \n    url = urlparse ( url ) \n    if url . scheme == \"ws\" : \n        websocket = 1 \n    else : \n        websocket = 0 \n    assert url . scheme in [ \"ws\" , \"http\" ] \n    port = url . port \n    base_path = url . path \n    hostname = url . hostname \n    return vaex . remote . ServerRest ( hostname , base_path = base_path , port = port , websocket = websocket , ** kwargs ) "}
{"2019": "\ndef evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = 0 ) : \n    expression = _ensure_strings_from_expressions ( expression ) \n    result = self . server . _call_dataset ( \"evaluate\" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) \n    return result "}
{"2022": "\ndef _task ( self , task , progressbar = 0 ) : \n    if self . delay : \n        return self . executor . schedule ( task ) \n    else : \n        import vaex . utils \n        callback = None \n        try : \n            if progressbar == 1 : \n                def update ( fraction ) : \n                    bar . update ( fraction ) \n                    return 1 \n                bar = vaex . utils . progressbar ( task . name ) \n                callback = self . executor . signal_progress . connect ( update ) \n            elif progressbar : \n                callback = self . executor . signal_progress . connect ( progressbar ) \n            result = self . executor . run ( task ) \n            if progressbar == 1 : \n                bar . finish ( ) \n                sys . stdout . write ( '\\n' ) \n            return result \n        finally : \n            if callback : \n                self . executor . signal_progress . disconnect ( callback ) "}
{"2025": "\ndef clear ( self , event ) : \n    if self . useblit : \n        self . background = ( self . canvas . copy_from_bbox ( self . canvas . figure . bbox ) ) \n    for line in self . vlines + self . hlines : \n        line . set_visible ( 0 ) \n    self . ellipse . set_visible ( 0 ) "}
{"2030": "\ndef nop ( self , expression , progress = 0 , delay = 0 ) : \n    expression = _ensure_string_from_expression ( expression ) \n    def map ( ar ) : \n        pass \n    def reduce ( a , b ) : \n        pass \n    return self . map_reduce ( map , reduce , [ expression ] , delay = delay , progress = progress , name = 'nop' , to_numpy = 0 ) "}
{"2031": "\ndef first ( self , expression , order_expression , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , edges = 0 , progress = None ) : \n    return self . _compute_agg ( 'first' , expression , binby , limits , shape , selection , delay , edges , progress , extra_expressions = [ order_expression ] ) \n    logger . debug ( \"count(%r, binby=%r, limits=%r)\" , expression , binby , limits ) \n    logger . debug ( \"count(%r, binby=%r, limits=%r)\" , expression , binby , limits ) \n    expression = _ensure_strings_from_expressions ( expression ) \n    order_expression = _ensure_string_from_expression ( order_expression ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    \n    @ delayed \n    def finish ( * counts ) : \n        counts = np . asarray ( counts ) \n        return vaex . utils . unlistify ( waslist , counts ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , delay = 1 , shape = shape ) \n    stats = [ self . _first_calculation ( expression , order_expression , binby = binby , limits = limits , shape = shape , selection = selection , edges = edges , progressbar = progressbar ) for expression in expressions ] \n    var = finish ( * stats ) \n    return self . _delay ( delay , var ) "}
{"2032": "\ndef mean ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , progress = None , edges = 0 ) : \n    return self . _compute_agg ( 'mean' , expression , binby , limits , shape , selection , delay , edges , progress ) \n    logger . debug ( \"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\" , expression , binby , limits , shape , selection , delay ) \n    expression = _ensure_strings_from_expressions ( expression ) \n    selection = _ensure_strings_from_expressions ( selection ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    \n    @ delayed \n    def calculate ( expression , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_ADD_WEIGHT_MOMENTS_01 , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"mean for %s\" % expression ) \n        return task \n    \n    @ delayed \n    def finish ( * stats_args ) : \n        stats = np . array ( stats_args ) \n        counts = stats [ ... , 0 ] \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            mean = stats [ ... , 1 ] / counts \n        return vaex . utils . unlistify ( waslist , mean ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , delay = 1 ) \n    stats = [ calculate ( expression , limits ) for expression in expressions ] \n    var = finish ( * stats ) \n    return self . _delay ( delay , var ) "}
{"2033": "\ndef sum ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , progress = None , edges = 0 ) : \n    return self . _compute_agg ( 'sum' , expression , binby , limits , shape , selection , delay , edges , progress ) \n    \n    @ delayed \n    def finish ( * sums ) : \n        return vaex . utils . unlistify ( waslist , sums ) \n    expression = _ensure_strings_from_expressions ( expression ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , delay = 1 ) \n    sums = [ self . _sum_calculation ( expression , binby = binby , limits = limits , shape = shape , selection = selection , progressbar = progressbar ) for expression in expressions ] \n    s = finish ( * sums ) \n    return self . _delay ( delay , s ) "}
{"2034": "\ndef std ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , progress = None ) : \n    \n    @ delayed \n    def finish ( var ) : \n        return var ** 0.5 \n    return self . _delay ( delay , finish ( self . var ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = 1 , progress = progress ) ) ) "}
{"2035": "\ndef cov ( self , x , y = None , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , progress = None ) : \n    selection = _ensure_strings_from_expressions ( selection ) \n    if y is None : \n        if not _issequence ( x ) : \n            raise ValueError ( \"if y argument is not given, x is expected to be sequence, not %r\" , x ) \n        expressions = x \n    else : \n        expressions = [ x , y ] \n    N = len ( expressions ) \n    binby = _ensure_list ( binby ) \n    shape = _expand_shape ( shape , len ( binby ) ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , selection = selection , delay = 1 ) \n    \n    @ delayed \n    def calculate ( expressions , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weights = expressions , op = tasks . OP_COV , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"covariance values for %r\" % expressions ) \n        return task \n    \n    @ delayed \n    def finish ( values ) : \n        N = len ( expressions ) \n        counts = values [ ... , : N ] \n        sums = values [ ... , N : 2 * N ] \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            means = sums / counts \n        meansxy = means [ ... , None ] * means [ ... , None , : ] \n        counts = values [ ... , 2 * N : 2 * N + N ** 2 ] \n        sums = values [ ... , 2 * N + N ** 2 : ] \n        shape = counts . shape [ : - 1 ] + ( N , N ) \n        counts = counts . reshape ( shape ) \n        sums = sums . reshape ( shape ) \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            moments2 = sums / counts \n        cov_matrix = moments2 - meansxy \n        return cov_matrix \n    progressbar = vaex . utils . progressbars ( progress ) \n    values = calculate ( expressions , limits ) \n    cov_matrix = finish ( values ) \n    return self . _delay ( delay , cov_matrix ) "}
{"2036": "\ndef minmax ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , progress = None ) : \n    \n    @ delayed \n    def finish ( * minmax_list ) : \n        value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) \n        value = value . astype ( dtype0 ) \n        return value \n    \n    @ delayed \n    def calculate ( expression , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_MIN_MAX , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"minmax for %s\" % expression ) \n        return task \n    \n    @ delayed \n    def finish ( * minmax_list ) : \n        value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) \n        value = value . astype ( dtype0 ) \n        return value \n    expression = _ensure_strings_from_expressions ( expression ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    dtypes = [ self . dtype ( expr ) for expr in expressions ] \n    dtype0 = dtypes [ 0 ] \n    if not all ( [ k . kind == dtype0 . kind for k in dtypes ] ) : \n        raise ValueError ( \"cannot mix datetime and non-datetime expressions\" ) \n    progressbar = vaex . utils . progressbars ( progress , name = \"minmaxes\" ) \n    limits = self . limits ( binby , limits , selection = selection , delay = 1 ) \n    all_tasks = [ calculate ( expression , limits ) for expression in expressions ] \n    result = finish ( * all_tasks ) \n    return self . _delay ( delay , result ) "}
{"2037": "\ndef min ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = 0 , delay = 0 , progress = None , edges = 0 ) : \n    return self . _compute_agg ( 'min' , expression , binby , limits , shape , selection , delay , edges , progress ) \n    \n    @ delayed \n    def finish ( result ) : \n        return result [ ... , 0 ] \n    return self . _delay ( delay , finish ( self . minmax ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = delay , progress = progress ) ) ) "}
{"2038": "\ndef median_approx ( self , expression , percentage = 50. , binby = [ ] , limits = None , shape = default_shape , percentile_shape = 256 , percentile_limits = \"minmax\" , selection = 0 , delay = 0 ) : \n    return self . percentile_approx ( expression , 50 , binby = binby , limits = limits , shape = shape , percentile_shape = percentile_shape , percentile_limits = percentile_limits , selection = selection , delay = delay ) "}
{"2039": "\ndef plot_widget ( self , x , y , z = None , grid = None , shape = 256 , limits = None , what = \"count(*)\" , figsize = None , f = \"identity\" , figure_key = None , fig = None , axes = None , xlabel = None , ylabel = None , title = None , show = 1 , selection = [ None , 1 ] , colormap = \"afmhot\" , grid_limits = None , normalize = \"normalize\" , grid_before = None , what_kwargs = { } , type = \"default\" , scales = None , tool_select = 0 , bq_cleanup = 1 , backend = \"bqplot\" , ** kwargs ) : \n    import vaex . jupyter . plot \n    backend = vaex . jupyter . plot . create_backend ( backend ) \n    cls = vaex . jupyter . plot . get_type ( type ) \n    x = _ensure_strings_from_expressions ( x ) \n    y = _ensure_strings_from_expressions ( y ) \n    z = _ensure_strings_from_expressions ( z ) \n    for name in 'vx vy vz' . split ( ) : \n        if name in kwargs : \n            kwargs [ name ] = _ensure_strings_from_expressions ( kwargs [ name ] ) \n    plot2d = cls ( backend = backend , dataset = self , x = x , y = y , z = z , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , grid_before = grid_before , grid_limits = grid_limits , normalize = normalize , colormap = colormap , what_kwargs = what_kwargs , ** kwargs ) \n    if show : \n        plot2d . show ( ) \n    return plot2d "}
{"2040": "\ndef healpix_count ( self , expression = None , healpix_expression = None , healpix_max_level = 12 , healpix_level = 8 , binby = None , limits = None , shape = default_shape , delay = 0 , progress = None , selection = None ) : \n    import healpy as hp \n    if healpix_expression is None : \n        if self . ucds . get ( \"source_id\" , None ) == 'meta.id;meta.main' : \n            healpix_expression = \"source_id/34359738368\" \n    if healpix_expression is None : \n        raise ValueError ( \"no healpix_expression given, and was unable to guess\" ) \n    reduce_level = healpix_max_level - healpix_level \n    NSIDE = 2 ** healpix_level \n    nmax = hp . nside2npix ( NSIDE ) \n    scaling = 4 ** reduce_level \n    expr = \"%s/%s\" % ( healpix_expression , scaling ) \n    binby = [ expr ] + ( [ ] if binby is None else _ensure_list ( binby ) ) \n    shape = ( nmax , ) + _expand_shape ( shape , len ( binby ) - 1 ) \n    epsilon = 1. / scaling / 2 \n    limits = [ [ - epsilon , nmax - epsilon ] ] + ( [ ] if limits is None else limits ) \n    return self . count ( expression , binby = binby , limits = limits , shape = shape , delay = delay , progress = progress , selection = selection ) "}
{"2041": "\ndef healpix_plot ( self , healpix_expression = \"source_id/34359738368\" , healpix_max_level = 12 , healpix_level = 8 , what = \"count(*)\" , selection = None , grid = None , healpix_input = \"equatorial\" , healpix_output = \"galactic\" , f = None , colormap = \"afmhot\" , grid_limits = None , image_size = 800 , nest = 1 , figsize = None , interactive = 0 , title = \"\" , smooth = None , show = 0 , colorbar = 1 , rotation = ( 0 , 0 , 0 ) , ** kwargs ) : \n    import healpy as hp \n    import pylab as plt \n    if grid is None : \n        reduce_level = healpix_max_level - healpix_level \n        NSIDE = 2 ** healpix_level \n        nmax = hp . nside2npix ( NSIDE ) \n        scaling = 4 ** reduce_level \n        epsilon = 1. / scaling / 2 \n        grid = self . _stat ( what = what , binby = \"%s/%s\" % ( healpix_expression , scaling ) , limits = [ - epsilon , nmax - epsilon ] , shape = nmax , selection = selection ) \n    if grid_limits : \n        grid_min , grid_max = grid_limits \n    else : \n        grid_min = grid_max = None \n    f_org = f \n    f = _parse_f ( f ) \n    if smooth : \n        if nest : \n            grid = hp . reorder ( grid , inp = \"NEST\" , out = \"RING\" ) \n            nest = 0 \n        grid = hp . smoothing ( grid , sigma = np . radians ( smooth ) ) \n    fgrid = f ( grid ) \n    coord_map = dict ( equatorial = 'C' , galactic = 'G' , ecliptic = \"E\" ) \n    fig = plt . gcf ( ) \n    if figsize is not None : \n        fig . set_size_inches ( * figsize ) \n    what_label = what \n    if f_org : \n        what_label = f_org + \" \" + what_label \n    f = hp . mollzoom if interactive else hp . mollview \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" ) \n        coord = coord_map [ healpix_input ] , coord_map [ healpix_output ] \n        if coord_map [ healpix_input ] == coord_map [ healpix_output ] : \n            coord = None \n        f ( fgrid , unit = what_label , rot = rotation , nest = nest , title = title , coord = coord , cmap = colormap , hold = 1 , xsize = image_size , min = grid_min , max = grid_max , cbar = colorbar , ** kwargs ) \n    if show : \n        plt . show ( ) "}
{"2042": "\ndef plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = \"count(*)\" , shape = 128 , selection = [ None , 1 ] , f = None , vcount_limits = None , smooth_pre = None , smooth_post = None , grid_limits = None , normalize = \"normalize\" , colormap = \"afmhot\" , figure_key = None , fig = None , lighting = 1 , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level_width = 0.1 , show = 1 , ** kwargs ) : \n    import vaex . ext . ipyvolume \n    cls = vaex . ext . ipyvolume . PlotDefault \n    plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , smooth_pre = smooth_pre , smooth_post = smooth_post , grid_limits = grid_limits , vcount_limits = vcount_limits , normalize = normalize , colormap = colormap , ** kwargs ) \n    if show : \n        plot3d . show ( ) \n    return plot3d "}
{"2043": "\ndef dtype ( self , expression , internal = 0 ) : \n    expression = _ensure_string_from_expression ( expression ) \n    if expression in self . variables : \n        return np . float64 ( 1 ) . dtype \n    elif expression in self . columns . keys ( ) : \n        column = self . columns [ expression ] \n        data = column [ 0 : 1 ] \n        dtype = data . dtype \n    else : \n        data = self . evaluate ( expression , 0 , 1 , filtered = 0 ) \n        dtype = data . dtype \n    if not internal : \n        if dtype != str_type : \n            if dtype . kind in 'US' : \n                return str_type \n            if dtype . kind == 'O' : \n                if isinstance ( data [ 0 ] , six . string_types ) : \n                    return str_type \n    return dtype "}
{"2044": "\ndef get_private_dir ( self , create = 0 ) : \n    if self . is_local ( ) : \n        name = os . path . abspath ( self . path ) . replace ( os . path . sep , \"_\" ) [ : 250 ] \n        name = name . replace ( \":\" , \"_\" ) \n    else : \n        server = self . server \n        name = \"%s_%s_%s_%s\" % ( server . hostname , server . port , server . base_path . replace ( \"/\" , \"_\" ) , self . name ) \n    dir = os . path . join ( vaex . utils . get_private_dir ( ) , \"dfs\" , name ) \n    if create and not os . path . exists ( dir ) : \n        os . makedirs ( dir ) \n    return dir "}
{"2045": "\ndef state_get ( self ) : \n    virtual_names = list ( self . virtual_columns . keys ( ) ) + list ( self . variables . keys ( ) ) \n    units = { key : str ( value ) for key , value in self . units . items ( ) } \n    ucds = { key : value for key , value in self . ucds . items ( ) if key in virtual_names } \n    descriptions = { key : value for key , value in self . descriptions . items ( ) } \n    import vaex . serialize \n    def check ( key , value ) : \n        if not vaex . serialize . can_serialize ( value . f ) : \n            warnings . warn ( 'Cannot serialize function for virtual column {} (use vaex.serialize.register)' . format ( key ) ) \n            return 0 \n        return 1 \n    def clean ( value ) : \n        return vaex . serialize . to_dict ( value . f ) \n    functions = { key : clean ( value ) for key , value in self . functions . items ( ) if check ( key , value ) } \n    virtual_columns = { key : value for key , value in self . virtual_columns . items ( ) } \n    selections = { name : self . get_selection ( name ) for name , history in self . selection_histories . items ( ) } \n    selections = { name : selection . to_dict ( ) if selection is not None else None for name , selection in selections . items ( ) } \n    state = dict ( virtual_columns = virtual_columns , column_names = self . column_names , renamed_columns = self . _renamed_columns , variables = self . variables , functions = functions , selections = selections , ucds = ucds , units = units , descriptions = descriptions , description = self . description , active_range = [ self . _index_start , self . _index_end ] ) \n    return state "}
{"2046": "\ndef state_set ( self , state , use_active_range = 0 ) : \n    self . description = state [ 'description' ] \n    if use_active_range : \n        self . _index_start , self . _index_end = state [ 'active_range' ] \n    self . _length_unfiltered = self . _index_end - self . _index_start \n    if 'renamed_columns' in state : \n        for old , new in state [ 'renamed_columns' ] : \n            self . _rename ( old , new ) \n    for name , value in state [ 'functions' ] . items ( ) : \n        self . add_function ( name , vaex . serialize . from_dict ( value ) ) \n    if 'column_names' in state : \n        self . column_names = [ ] \n        self . virtual_columns = collections . OrderedDict ( ) \n        for name , value in state [ 'virtual_columns' ] . items ( ) : \n            self [ name ] = self . _expr ( value ) \n        self . column_names = state [ 'column_names' ] \n    else : \n        self . virtual_columns = collections . OrderedDict ( ) \n        for name , value in state [ 'virtual_columns' ] . items ( ) : \n            self [ name ] = self . _expr ( value ) \n    self . variables = state [ 'variables' ] \n    import astropy \n    units = { key : astropy . units . Unit ( value ) for key , value in state [ \"units\" ] . items ( ) } \n    self . units . update ( units ) \n    for name , selection_dict in state [ 'selections' ] . items ( ) : \n        if selection_dict is None : \n            selection = None \n        else : \n            selection = selections . selection_from_dict ( selection_dict ) \n        self . set_selection ( selection , name = name ) "}
{"2047": "\ndef remove_virtual_meta ( self ) : \n    dir = self . get_private_dir ( create = 1 ) \n    path = os . path . join ( dir , \"virtual_meta.yaml\" ) \n    try : \n        if os . path . exists ( path ) : \n            os . remove ( path ) \n        if not os . listdir ( dir ) : \n            os . rmdir ( dir ) \n    except : \n        logger . exception ( \"error while trying to remove %s or %s\" , path , dir ) "}
{"2048": "\ndef write_virtual_meta ( self ) : \n    path = os . path . join ( self . get_private_dir ( create = 1 ) , \"virtual_meta.yaml\" ) \n    virtual_names = list ( self . virtual_columns . keys ( ) ) + list ( self . variables . keys ( ) ) \n    units = { key : str ( value ) for key , value in self . units . items ( ) if key in virtual_names } \n    ucds = { key : value for key , value in self . ucds . items ( ) if key in virtual_names } \n    descriptions = { key : value for key , value in self . descriptions . items ( ) if key in virtual_names } \n    meta_info = dict ( virtual_columns = self . virtual_columns , variables = self . variables , ucds = ucds , units = units , descriptions = descriptions ) \n    vaex . utils . write_json_or_yaml ( path , meta_info ) "}
{"2049": "\ndef write_meta ( self ) : \n    path = os . path . join ( self . get_private_dir ( create = 1 ) , \"meta.yaml\" ) \n    units = { key : str ( value ) for key , value in self . units . items ( ) } \n    meta_info = dict ( description = self . description , ucds = self . ucds , units = units , descriptions = self . descriptions , ) \n    vaex . utils . write_json_or_yaml ( path , meta_info ) "}
{"2050": "\ndef subspaces ( self , expressions_list = None , dimensions = None , exclude = None , ** kwargs ) : \n    if dimensions is not None : \n        expressions_list = list ( itertools . combinations ( self . get_column_names ( ) , dimensions ) ) \n        if exclude is not None : \n            import six \n            def excluded ( expressions ) : \n                if callable ( exclude ) : \n                    return exclude ( expressions ) \n                elif isinstance ( exclude , six . string_types ) : \n                    return exclude in expressions \n                elif isinstance ( exclude , ( list , tuple ) ) : \n                    for e in exclude : \n                        if isinstance ( e , six . string_types ) : \n                            if e in expressions : \n                                return 1 \n                        elif isinstance ( e , ( list , tuple ) ) : \n                            if set ( e ) . issubset ( expressions ) : \n                                return 1 \n                        else : \n                            raise ValueError ( \"elements of exclude should contain a string or a sequence of strings\" ) \n                else : \n                    raise ValueError ( \"exclude should contain a string, a sequence of strings, or should be a callable\" ) \n                return 0 \n            expressions_list = [ expr for expr in expressions_list if not excluded ( expr ) ] \n        logger . debug ( \"expression list generated: %r\" , expressions_list ) \n    import vaex . legacy \n    return vaex . legacy . Subspaces ( [ self ( * expressions , ** kwargs ) for expressions in expressions_list ] ) "}
{"2051": "\ndef set_variable ( self , name , expression_or_value , write = 1 ) : \n    self . variables [ name ] = expression_or_value "}
{"2053": "\ndef _evaluate_selection_mask ( self , name = \"default\" , i1 = None , i2 = None , selection = None , cache = 0 ) : \n    i1 = i1 or 0 \n    i2 = i2 or len ( self ) \n    scope = scopes . _BlockScopeSelection ( self , i1 , i2 , selection , cache = cache ) \n    return scope . evaluate ( name ) "}
{"2054": "\ndef to_dict ( self , column_names = None , selection = None , strings = 1 , virtual = 0 ) : \n    return dict ( self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) ) "}
{"2055": "\ndef to_copy ( self , column_names = None , selection = None , strings = 1 , virtual = 0 , selections = 1 ) : \n    if column_names : \n        column_names = _ensure_strings_from_expressions ( column_names ) \n    df = vaex . from_items ( * self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = 0 ) ) \n    if virtual : \n        for name , value in self . virtual_columns . items ( ) : \n            df . add_virtual_column ( name , value ) \n    if selections : \n        for key , value in self . selection_histories . items ( ) : \n            if key != FILTER_SELECTION_NAME : \n                df . selection_histories [ key ] = list ( value ) \n        for key , value in self . selection_history_indices . items ( ) : \n            if key != FILTER_SELECTION_NAME : \n                df . selection_history_indices [ key ] = value \n    df . functions . update ( self . functions ) \n    df . copy_metadata ( self ) \n    return df "}
{"2056": "\ndef to_pandas_df ( self , column_names = None , selection = None , strings = 1 , virtual = 0 , index_name = None ) : \n    import pandas as pd \n    data = self . to_dict ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) \n    if index_name is not None : \n        if index_name in data : \n            index = data . pop ( index_name ) \n        else : \n            index = self . evaluate ( index_name , selection = selection ) \n    else : \n        index = None \n    df = pd . DataFrame ( data = data , index = index ) \n    if index is not None : \n        df . index . name = index_name \n    return df "}
{"2057": "\ndef to_arrow_table ( self , column_names = None , selection = None , strings = 1 , virtual = 0 ) : \n    from vaex_arrow . convert import arrow_table_from_vaex_df \n    return arrow_table_from_vaex_df ( self , column_names , selection , strings , virtual ) "}
{"2058": "\ndef to_astropy_table ( self , column_names = None , selection = None , strings = 1 , virtual = 0 , index = None ) : \n    from astropy . table import Table , Column , MaskedColumn \n    meta = dict ( ) \n    meta [ \"name\" ] = self . name \n    meta [ \"description\" ] = self . description \n    table = Table ( meta = meta ) \n    for name , data in self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : \n        if self . dtype ( name ) == str_type : \n            data = np . array ( data ) . astype ( 'U' ) \n        meta = dict ( ) \n        if name in self . ucds : \n            meta [ \"ucd\" ] = self . ucds [ name ] \n        if np . ma . isMaskedArray ( data ) : \n            cls = MaskedColumn \n        else : \n            cls = Column \n        table [ name ] = cls ( data , unit = self . unit ( name ) , description = self . descriptions . get ( name ) , meta = meta ) \n    return table "}
{"2060": "\ndef rename_column ( self , name , new_name , unique = 0 , store_in_state = 1 ) : \n    new_name = vaex . utils . find_valid_name ( new_name , used = [ ] if not unique else list ( self ) ) \n    data = self . columns . get ( name ) \n    if data is not None : \n        del self . columns [ name ] \n        self . column_names [ self . column_names . index ( name ) ] = new_name \n        self . columns [ new_name ] = data \n    else : \n        expression = self . virtual_columns [ name ] \n        del self . virtual_columns [ name ] \n        self . virtual_columns [ new_name ] = expression \n    if store_in_state : \n        self . _renamed_columns . append ( ( name , new_name ) ) \n    for d in [ self . ucds , self . units , self . descriptions ] : \n        if name in d : \n            d [ new_name ] = d [ name ] \n            del d [ name ] \n    return new_name "}
{"2061": "\ndef add_virtual_columns_cartesian_to_polar ( self , x = \"x\" , y = \"y\" , radius_out = \"r_polar\" , azimuth_out = \"phi_polar\" , propagate_uncertainties = 0 , radians = 0 ) : \n    x = self [ x ] \n    y = self [ y ] \n    if radians : \n        to_degrees = \"\" \n    else : \n        to_degrees = \"*180/pi\" \n    r = np . sqrt ( x ** 2 + y ** 2 ) \n    self [ radius_out ] = r \n    phi = np . arctan2 ( y , x ) \n    if not radians : \n        phi = phi * 180 / np . pi \n    self [ azimuth_out ] = phi \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ radius_out ] , self [ azimuth_out ] ] ) "}
{"2063": "\ndef add_virtual_columns_cartesian_velocities_to_polar ( self , x = \"x\" , y = \"y\" , vx = \"vx\" , radius_polar = None , vy = \"vy\" , vr_out = \"vr_polar\" , vazimuth_out = \"vphi_polar\" , propagate_uncertainties = 0 , ) : \n    x = self . _expr ( x ) \n    y = self . _expr ( y ) \n    vx = self . _expr ( vx ) \n    vy = self . _expr ( vy ) \n    if radius_polar is None : \n        radius_polar = np . sqrt ( x ** 2 + y ** 2 ) \n    radius_polar = self . _expr ( radius_polar ) \n    self [ vr_out ] = ( x * vx + y * vy ) / radius_polar \n    self [ vazimuth_out ] = ( x * vy - y * vx ) / radius_polar \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ vr_out ] , self [ vazimuth_out ] ] ) "}
{"2064": "\ndef add_virtual_columns_polar_velocities_to_cartesian ( self , x = 'x' , y = 'y' , azimuth = None , vr = 'vr_polar' , vazimuth = 'vphi_polar' , vx_out = 'vx' , vy_out = 'vy' , propagate_uncertainties = 0 ) : \n    x = self . _expr ( x ) \n    y = self . _expr ( y ) \n    vr = self . _expr ( vr ) \n    vazimuth = self . _expr ( vazimuth ) \n    if azimuth is not None : \n        azimuth = self . _expr ( azimuth ) \n        azimuth = np . deg2rad ( azimuth ) \n    else : \n        azimuth = np . arctan2 ( y , x ) \n    azimuth = self . _expr ( azimuth ) \n    self [ vx_out ] = vr * np . cos ( azimuth ) - vazimuth * np . sin ( azimuth ) \n    self [ vy_out ] = vr * np . sin ( azimuth ) + vazimuth * np . cos ( azimuth ) \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ vx_out ] , self [ vy_out ] ] ) "}
{"2065": "\ndef add_virtual_columns_rotation ( self , x , y , xnew , ynew , angle_degrees , propagate_uncertainties = 0 ) : \n    x = _ensure_string_from_expression ( x ) \n    y = _ensure_string_from_expression ( y ) \n    theta = np . radians ( angle_degrees ) \n    matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) ] , [ np . sin ( theta ) , np . cos ( theta ) ] ] ) \n    m = matrix_name = x + \"_\" + y + \"_rot\" \n    for i in range ( 2 ) : \n        for j in range ( 2 ) : \n            self . set_variable ( matrix_name + \"_%d%d\" % ( i , j ) , matrix [ i , j ] . item ( ) ) \n    self [ xnew ] = self . _expr ( \"{m}_00 * {x} + {m}_01 * {y}\" . format ( ** locals ( ) ) ) \n    self [ ynew ] = self . _expr ( \"{m}_10 * {x} + {m}_11 * {y}\" . format ( ** locals ( ) ) ) \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xnew ] , self [ ynew ] ] ) "}
{"2066": "\ndef add_virtual_columns_spherical_to_cartesian ( self , alpha , delta , distance , xname = \"x\" , yname = \"y\" , zname = \"z\" , propagate_uncertainties = 0 , center = [ 0 , 0 , 0 ] , center_name = \"solar_position\" , radians = 0 ) : \n    alpha = self . _expr ( alpha ) \n    delta = self . _expr ( delta ) \n    distance = self . _expr ( distance ) \n    if not radians : \n        alpha = alpha * self . _expr ( 'pi' ) / 180 \n        delta = delta * self . _expr ( 'pi' ) / 180 \n    if center [ 0 ] : \n        self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance + center [ 0 ] \n    else : \n        self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance \n    if center [ 1 ] : \n        self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance + center [ 1 ] \n    else : \n        self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance \n    if center [ 2 ] : \n        self [ zname ] = np . sin ( delta ) * distance + center [ 2 ] \n    else : \n        self [ zname ] = np . sin ( delta ) * distance \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xname ] , self [ yname ] , self [ zname ] ] ) "}
{"2067": "\ndef add_virtual_columns_cartesian_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , alpha = \"l\" , delta = \"b\" , distance = \"distance\" , radians = 0 , center = None , center_name = \"solar_position\" ) : \n    transform = \"\" if radians else \"*180./pi\" \n    if center is not None : \n        self . add_variable ( center_name , center ) \n    if center is not None and center [ 0 ] != 0 : \n        x = \"({x} - {center_name}[0])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 1 ] != 0 : \n        y = \"({y} - {center_name}[1])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 2 ] != 0 : \n        z = \"({z} - {center_name}[2])\" . format ( ** locals ( ) ) \n    self . add_virtual_column ( distance , \"sqrt({x}**2 + {y}**2 + {z}**2)\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( alpha , \"arctan2({y}, {x}){transform}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( delta , \"(-arccos({z}/{distance})+pi/2){transform}\" . format ( ** locals ( ) ) ) "}
{"2068": "\ndef add_virtual_column ( self , name , expression , unique = 0 ) : \n    type = \"change\" if name in self . virtual_columns else \"add\" \n    expression = _ensure_string_from_expression ( expression ) \n    if name in self . get_column_names ( virtual = 0 ) : \n        renamed = '__' + vaex . utils . find_valid_name ( name , used = self . get_column_names ( ) ) \n        expression = self . _rename ( name , renamed , expression ) [ 0 ] . expression \n    name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else self . get_column_names ( ) ) \n    self . virtual_columns [ name ] = expression \n    self . column_names . append ( name ) \n    self . _save_assign_expression ( name ) \n    self . signal_column_changed . emit ( self , name , \"add\" ) "}
{"2070": "\ndef add_variable ( self , name , expression , overwrite = 1 , unique = 1 ) : \n    if unique or overwrite or name not in self . variables : \n        existing_names = self . get_column_names ( virtual = 0 ) + list ( self . variables . keys ( ) ) \n        name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else existing_names ) \n        self . variables [ name ] = expression \n        self . signal_variable_changed . emit ( self , name , \"add\" ) \n        if unique : \n            return name "}
{"2074": "\ndef describe ( self , strings = 1 , virtual = 1 , selection = None ) : \n    import pandas as pd \n    N = len ( self ) \n    columns = { } \n    for feature in self . get_column_names ( strings = strings , virtual = virtual ) [ : ] : \n        dtype = str ( self . dtype ( feature ) ) if self . dtype ( feature ) != str else 'str' \n        if self . dtype ( feature ) == str_type or self . dtype ( feature ) . kind in [ 'S' , 'U' , 'O' ] : \n            count = self . count ( feature , selection = selection , delay = 1 ) \n            self . execute ( ) \n            count = count . get ( ) \n            columns [ feature ] = ( ( dtype , count , N - count , '--' , '--' , '--' , '--' ) ) \n        else : \n            count = self . count ( feature , selection = selection , delay = 1 ) \n            mean = self . mean ( feature , selection = selection , delay = 1 ) \n            std = self . std ( feature , selection = selection , delay = 1 ) \n            minmax = self . minmax ( feature , selection = selection , delay = 1 ) \n            self . execute ( ) \n            count , mean , std , minmax = count . get ( ) , mean . get ( ) , std . get ( ) , minmax . get ( ) \n            count = int ( count ) \n            columns [ feature ] = ( ( dtype , count , N - count , mean , std , minmax [ 0 ] , minmax [ 1 ] ) ) \n    return pd . DataFrame ( data = columns , index = [ 'dtype' , 'count' , 'missing' , 'mean' , 'std' , 'min' , 'max' ] ) "}
{"2077": "\ndef get_column_names ( self , virtual = 1 , strings = 1 , hidden = 0 , regex = None ) : \n    def column_filter ( name ) : \n        if regex and not re . match ( regex , name ) : \n            return 0 \n        if not virtual and name in self . virtual_columns : \n            return 0 \n        if not strings and ( self . dtype ( name ) == str_type or self . dtype ( name ) . type == np . string_ ) : \n            return 0 \n        if not hidden and name . startswith ( '__' ) : \n            return 0 \n        return 1 \n    return [ name for name in self . column_names if column_filter ( name ) ] "}
{"2078": "\ndef trim ( self , inplace = 0 ) : \n    df = self if inplace else self . copy ( ) \n    for name in df : \n        column = df . columns . get ( name ) \n        if column is not None : \n            if self . _index_start == 0 and len ( column ) == self . _index_end : \n                pass \n            else : \n                if isinstance ( column , np . ndarray ) : \n                    df . columns [ name ] = column [ self . _index_start : self . _index_end ] \n                else : \n                    df . columns [ name ] = column . trim ( self . _index_start , self . _index_end ) \n    df . _length_original = self . length_unfiltered ( ) \n    df . _length_unfiltered = df . _length_original \n    df . _index_start = 0 \n    df . _index_end = df . _length_original \n    df . _active_fraction = 1 \n    return df "}
{"2081": "\ndef sample ( self , n = None , frac = None , replace = 0 , weights = None , random_state = None ) : \n    self = self . extract ( ) \n    if type ( random_state ) == int or random_state is None : \n        random_state = np . random . RandomState ( seed = random_state ) \n    if n is None and frac is None : \n        n = 1 \n    elif frac is not None : \n        n = int ( round ( frac * len ( self ) ) ) \n    weights_values = None \n    if weights is not None : \n        weights_values = self . evaluate ( weights ) \n        weights_values = weights_values / self . sum ( weights ) \n    indices = random_state . choice ( len ( self ) , n , replace = replace , p = weights_values ) \n    return self . take ( indices ) "}
{"2082": "\ndef split_random ( self , frac , random_state = None ) : \n    self = self . extract ( ) \n    if type ( random_state ) == int or random_state is None : \n        random_state = np . random . RandomState ( seed = random_state ) \n    indices = random_state . choice ( len ( self ) , len ( self ) , replace = 0 ) \n    return self . take ( indices ) . split ( frac ) "}
{"2084": "\ndef sort ( self , by , ascending = 1 , kind = 'quicksort' ) : \n    self = self . trim ( ) \n    values = self . evaluate ( by , filtered = 0 ) \n    indices = np . argsort ( values , kind = kind ) \n    if not ascending : \n        indices = indices [ : : - 1 ] . copy ( ) \n    return self . take ( indices ) "}
{"2085": "\ndef materialize ( self , virtual_column , inplace = 0 ) : \n    df = self . trim ( inplace = inplace ) \n    virtual_column = _ensure_string_from_expression ( virtual_column ) \n    if virtual_column not in df . virtual_columns : \n        raise KeyError ( 'Virtual column not found: %r' % virtual_column ) \n    ar = df . evaluate ( virtual_column , filtered = 0 ) \n    del df [ virtual_column ] \n    df . add_column ( virtual_column , ar ) \n    return df "}
{"2090": "\ndef select_non_missing ( self , drop_nan = 1 , drop_masked = 1 , column_names = None , mode = \"replace\" , name = \"default\" ) : \n    column_names = column_names or self . get_column_names ( virtual = 0 ) \n    def create ( current ) : \n        return selections . SelectionDropNa ( drop_nan , drop_masked , column_names , current , mode ) \n    self . _selection ( create , name ) "}
{"2091": "\ndef dropna ( self , drop_nan = 1 , drop_masked = 1 , column_names = None ) : \n    copy = self . copy ( ) \n    copy . select_non_missing ( drop_nan = drop_nan , drop_masked = drop_masked , column_names = column_names , name = FILTER_SELECTION_NAME , mode = 'and' ) \n    return copy "}
{"2094": "\ndef select_circle ( self , x , y , xc , yc , r , mode = \"replace\" , name = \"default\" , inclusive = 1 ) : \n    if inclusive : \n        expr = ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 <= r ** 2 \n    else : \n        expr = ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 < r ** 2 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2095": "\ndef select_ellipse ( self , x , y , xc , yc , width , height , angle = 0 , mode = \"replace\" , name = \"default\" , radians = 0 , inclusive = 1 ) : \n    if radians : \n        pass \n    else : \n        alpha = np . deg2rad ( angle ) \n    xr = width / 2 \n    yr = height / 2 \n    r = max ( xr , yr ) \n    a = xr / r \n    b = yr / r \n    expr = \"(({x}-{xc})*cos({alpha})+({y}-{yc})*sin({alpha}))**2/{a}**2 + (({x}-{xc})*sin({alpha})-({y}-{yc})*cos({alpha}))**2/{b}**2 <= {r}**2\" . format ( ** locals ( ) ) \n    if inclusive : \n        expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 <= r ** 2 \n    else : \n        expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 < r ** 2 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2098": "\ndef set_selection ( self , selection , name = \"default\" , executor = None ) : \n    def create ( current ) : \n        return selection \n    self . _selection ( create , name , executor = executor , execute_fully = 1 ) "}
{"2099": "\ndef _selection ( self , create_selection , name , executor = None , execute_fully = 0 ) : \n    selection_history = self . selection_histories [ name ] \n    previous_index = self . selection_history_indices [ name ] \n    current = selection_history [ previous_index ] if selection_history else None \n    selection = create_selection ( current ) \n    executor = executor or self . executor \n    selection_history . append ( selection ) \n    self . selection_history_indices [ name ] += 1 \n    del selection_history [ self . selection_history_indices [ name ] : - 1 ] \n    if 0 : \n        if self . is_local ( ) : \n            if selection : \n                result = vaex . promise . Promise . fulfilled ( None ) \n                self . signal_selection_changed . emit ( self ) \n            else : \n                result = vaex . promise . Promise . fulfilled ( None ) \n                self . signal_selection_changed . emit ( self ) \n        else : \n            self . signal_selection_changed . emit ( self ) \n            result = vaex . promise . Promise . fulfilled ( None ) \n    self . signal_selection_changed . emit ( self ) \n    result = vaex . promise . Promise . fulfilled ( None ) \n    logger . debug ( \"select selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) \n    return result "}
{"2100": "\ndef _find_valid_name ( self , initial_name ) : \n    return vaex . utils . find_valid_name ( initial_name , used = self . get_column_names ( hidden = 1 ) ) "}
{"2103": "\ndef categorize ( self , column , labels = None , check = 1 ) : \n    column = _ensure_string_from_expression ( column ) \n    if check : \n        vmin , vmax = self . minmax ( column ) \n        if labels is None : \n            N = int ( vmax + 1 ) \n            labels = list ( map ( str , range ( N ) ) ) \n        if ( vmax - vmin ) >= len ( labels ) : \n            raise ValueError ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) \n    self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) ) "}
{"2104": "\ndef ordinal_encode ( self , column , values = None , inplace = 0 ) : \n    column = _ensure_string_from_expression ( column ) \n    df = self if inplace else self . copy ( ) \n    df_unfiltered = df . copy ( ) \n    df_unfiltered . select_nothing ( name = FILTER_SELECTION_NAME ) \n    df_unfiltered . _length_unfiltered = df . _length_original \n    df_unfiltered . set_active_range ( 0 , df . _length_original ) \n    found_values , codes = df_unfiltered . unique ( column , return_inverse = 1 ) \n    if values is None : \n        values = found_values \n    else : \n        translation = np . zeros ( len ( found_values ) , dtype = np . uint64 ) \n        missing_value = len ( found_values ) \n        for i , found_value in enumerate ( found_values ) : \n            try : \n                found_value = found_value . decode ( 'ascii' ) \n            except : \n                pass \n            if found_value not in values : \n                translation [ i ] = missing_value \n            else : \n                translation [ i ] = values . index ( found_value ) \n        codes = translation [ codes ] \n        if missing_value in translation : \n            codes = np . ma . masked_array ( codes , codes == missing_value ) \n    original_column = df . rename_column ( column , '__original_' + column , unique = 1 ) \n    labels = [ str ( k ) for k in values ] \n    df . add_column ( column , codes ) \n    df . _categories [ column ] = dict ( labels = labels , N = len ( values ) , values = values ) \n    return df "}
{"2106": "\ndef length ( self , selection = 0 ) : \n    if selection : \n        return 0 if self . mask is None else np . sum ( self . mask ) \n    else : \n        return len ( self ) "}
{"2109": "\ndef export_hdf5 ( self , path , column_names = None , byteorder = \"=\" , shuffle = 0 , selection = 0 , progress = None , virtual = 0 , sort = None , ascending = 1 ) : \n    import vaex . export \n    vaex . export . export_hdf5 ( self , path , column_names , byteorder , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) "}
{"2112": "\ndef register_function ( scope = None , as_property = 0 , name = None ) : \n    prefix = '' \n    if scope : \n        prefix = scope + \"_\" \n        if scope not in scopes : \n            raise KeyError ( \"unknown scope\" ) \n    def wrapper ( f , name = name ) : \n        name = name or f . __name__ \n        if name . startswith ( prefix ) : \n            name = name [ len ( prefix ) : ] \n        full_name = prefix + name \n        if scope : \n            def closure ( name = name , full_name = full_name , function = f ) : \n                def wrapper ( self , * args , ** kwargs ) : \n                    lazy_func = getattr ( self . expression . ds . func , full_name ) \n                    args = ( self . expression , ) + args \n                    return lazy_func ( * args , ** kwargs ) \n                return functools . wraps ( function ) ( wrapper ) \n            if as_property : \n                setattr ( scopes [ scope ] , name , property ( closure ( ) ) ) \n            else : \n                setattr ( scopes [ scope ] , name , closure ( ) ) \n        else : \n            def closure ( name = name , full_name = full_name , function = f ) : \n                def wrapper ( self , * args , ** kwargs ) : \n                    lazy_func = getattr ( self . ds . func , full_name ) \n                    args = ( self , ) + args \n                    return lazy_func ( * args , ** kwargs ) \n                return functools . wraps ( function ) ( wrapper ) \n            setattr ( vaex . expression . Expression , name , closure ( ) ) \n        vaex . expression . expression_namespace [ prefix + name ] = f \n        return f \n    return wrapper "}
{"2113": "\ndef fillna ( ar , value , fill_nan = 1 , fill_masked = 1 ) : \n    ar = ar if not isinstance ( ar , column . Column ) else ar . to_numpy ( ) \n    if ar . dtype . kind in 'O' and fill_nan : \n        strings = ar . astype ( str ) \n        mask = strings == 'nan' \n        ar = ar . copy ( ) \n        ar [ mask ] = value \n    elif ar . dtype . kind in 'f' and fill_nan : \n        mask = np . isnan ( ar ) \n        if np . any ( mask ) : \n            ar = ar . copy ( ) \n            ar [ mask ] = value \n    if fill_masked and np . ma . isMaskedArray ( ar ) : \n        mask = ar . mask \n        if np . any ( mask ) : \n            ar = ar . data . copy ( ) \n            ar [ mask ] = value \n    return ar "}
{"2128": "\ndef str_contains ( x , pattern , regex = 1 ) : \n    return _to_string_sequence ( x ) . search ( pattern , regex ) "}
{"2129": "\ndef str_count ( x , pat , regex = 0 ) : \n    return _to_string_sequence ( x ) . count ( pat , regex ) "}
{"2130": "\ndef str_find ( x , sub , start = 0 , end = None ) : \n    return _to_string_sequence ( x ) . find ( sub , start , 0 if end is None else end , end is None , 1 ) "}
{"2137": "\ndef str_rfind ( x , sub , start = 0 , end = None ) : \n    return _to_string_sequence ( x ) . find ( sub , start , 0 if end is None else end , end is None , 0 ) "}
{"2139": "\ndef str_rjust ( x , width , fillchar = ' ' ) : \n    sl = _to_string_sequence ( x ) . pad ( width , fillchar , 1 , 0 ) \n    return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) "}
{"2147": "\ndef store_properties ( fh , props , comment = None , timestamp = 1 ) : \n    if comment is not None : \n        write_comment ( fh , comment ) \n    if timestamp : \n        write_comment ( fh , time . strftime ( '%a %b %d %H:%M:%S %Z %Y' ) ) \n    if hasattr ( props , 'keys' ) : \n        for key in props : \n            write_property ( fh , key , props [ key ] ) \n    else : \n        for key , value in props : \n            write_property ( fh , key , value ) "}
{"2150": "\ndef iter_properties ( fh , comments = 0 ) : \n    for line in _property_lines ( fh ) : \n        key , value = _split_key_value ( line ) \n        if key is not COMMENT : \n            key = _unescape ( key ) \n        elif not comments : \n            continue \n        yield key , _unescape ( value ) "}
{"2160": "\ndef midi_to_note ( midi , octave = 1 , cents = 0 ) : \n    if cents and not octave : \n        raise ParameterError ( 'Cannot encode cents without octave information.' ) \n    if not np . isscalar ( midi ) : \n        return [ midi_to_note ( x , octave = octave , cents = cents ) for x in midi ] \n    note_map = [ 'C' , 'C#' , 'D' , 'D#' , 'E' , 'F' , 'F#' , 'G' , 'G#' , 'A' , 'A#' , 'B' ] \n    note_num = int ( np . round ( midi ) ) \n    note_cents = int ( 100 * np . around ( midi - note_num , 2 ) ) \n    note = note_map [ note_num % 12 ] \n    if octave : \n        note = '{:s}{:0d}' . format ( note , int ( note_num / 12 ) - 1 ) \n    if cents : \n        note = '{:s}{:+02d}' . format ( note , note_cents ) \n    return note "}
{"2161": "\ndef hz_to_mel ( frequencies , htk = 0 ) : \n    frequencies = np . asanyarray ( frequencies ) \n    if htk : \n        return 2595.0 * np . log10 ( 1.0 + frequencies / 700.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    mels = ( frequencies - f_min ) / f_sp \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if frequencies . ndim : \n        log_t = ( frequencies >= min_log_hz ) \n        mels [ log_t ] = min_log_mel + np . log ( frequencies [ log_t ] / min_log_hz ) / logstep \n    elif frequencies >= min_log_hz : \n        mels = min_log_mel + np . log ( frequencies / min_log_hz ) / logstep \n    return mels "}
{"2162": "\ndef mel_to_hz ( mels , htk = 0 ) : \n    mels = np . asanyarray ( mels ) \n    if htk : \n        return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    freqs = f_min + f_sp * mels \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if mels . ndim : \n        log_t = ( mels >= min_log_mel ) \n        freqs [ log_t ] = min_log_hz * np . exp ( logstep * ( mels [ log_t ] - min_log_mel ) ) \n    elif mels >= min_log_mel : \n        freqs = min_log_hz * np . exp ( logstep * ( mels - min_log_mel ) ) \n    return freqs "}
{"2163": "\ndef fft_frequencies ( sr = 22050 , n_fft = 2048 ) : \n    return np . linspace ( 0 , float ( sr ) / 2 , int ( 1 + n_fft // 2 ) , endpoint = 1 ) "}
{"2165": "\ndef mel_frequencies ( n_mels = 128 , fmin = 0.0 , fmax = 11025.0 , htk = 0 ) : \n    min_mel = hz_to_mel ( fmin , htk = htk ) \n    max_mel = hz_to_mel ( fmax , htk = htk ) \n    mels = np . linspace ( min_mel , max_mel , n_mels ) \n    return mel_to_hz ( mels , htk = htk ) "}
{"2169": "\ndef hybrid_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = 1 , pad_mode = 'reflect' , res_type = None ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) \n    lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , window = window ) \n    pseudo_filters = 2.0 ** np . ceil ( np . log2 ( lengths ) ) < 2 * hop_length \n    n_bins_pseudo = int ( np . sum ( pseudo_filters ) ) \n    n_bins_full = n_bins - n_bins_pseudo \n    cqt_resp = [ ] \n    if n_bins_pseudo > 0 : \n        fmin_pseudo = np . min ( freqs [ pseudo_filters ] ) \n        cqt_resp . append ( pseudo_cqt ( y , sr , hop_length = hop_length , fmin = fmin_pseudo , n_bins = n_bins_pseudo , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode ) ) \n    if n_bins_full > 0 : \n        cqt_resp . append ( np . abs ( cqt ( y , sr , hop_length = hop_length , fmin = fmin , n_bins = n_bins_full , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode , res_type = res_type ) ) ) \n    return __trim_stack ( cqt_resp , n_bins ) "}
{"2170": "\ndef pseudo_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = 1 , pad_mode = 'reflect' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = hop_length , window = window ) \n    fft_basis = np . abs ( fft_basis ) \n    D = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , pad_mode = pad_mode ) ) \n    C = fft_basis . dot ( D ) \n    if scale : \n        C /= np . sqrt ( n_fft ) \n    else : \n        lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n        C *= np . sqrt ( lengths [ : , np . newaxis ] / n_fft ) \n    return C "}
{"2171": "\ndef icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = 1 , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if hop_length > min ( lengths ) : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    for octave in range ( n_octaves - 1 , - 1 , - 1 ) : \n        slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , - ( octave ) * bins_per_octave - 1 ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] \n        oct_hop = hop_length // 2 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , 1 , 2 , scale = 1 , res_type = res_type , fix = 0 ) \n            y [ : len ( y_oct ) ] += y_oct \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2172": "\ndef __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : \n    basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = 1 , window = window ) \n    n_fft = basis . shape [ 1 ] \n    if ( hop_length is not None and n_fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) : \n        n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) \n    basis *= lengths [ : , np . newaxis ] / float ( n_fft ) \n    fft = get_fftlib ( ) \n    fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2 ) + 1 ] \n    fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) \n    return fft_basis , n_fft , lengths "}
{"2176": "\ndef __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : \n    downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) \n    if downsample_count > 0 and res_type == 'kaiser_fast' : \n        downsample_factor = 2 ** ( downsample_count ) \n        hop_length //= downsample_factor \n        if len ( y ) < downsample_factor : \n            raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) \n        new_sr = sr / float ( downsample_factor ) \n        y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = 1 ) \n        if not scale : \n            y *= np . sqrt ( downsample_factor ) \n        sr = new_sr \n    return y , sr , hop_length "}
{"2180": "\ndef viterbi_discriminative ( prob , transition , p_state = None , p_init = None , return_logp = 0 ) : \n    n_states , n_steps = prob . shape \n    if transition . shape != ( n_states , n_states ) : \n        raise ParameterError ( 'transition.shape={}, must be ' '(n_states, n_states)={}' . format ( transition . shape , ( n_states , n_states ) ) ) \n    if np . any ( transition < 0 ) or not np . allclose ( transition . sum ( axis = 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid transition matrix: must be non-negative ' 'and sum to 1 on each row.' ) \n    if np . any ( prob < 0 ) or not np . allclose ( prob . sum ( axis = 0 ) , 1 ) : \n        raise ParameterError ( 'Invalid probability values: each column must ' 'sum to 1 and be non-negative' ) \n    states = np . zeros ( n_steps , dtype = int ) \n    values = np . zeros ( ( n_steps , n_states ) , dtype = float ) \n    ptr = np . zeros ( ( n_steps , n_states ) , dtype = int ) \n    epsilon = np . finfo ( prob . dtype ) . tiny \n    if p_state is None : \n        p_state = np . empty ( n_states ) \n        p_state . fill ( 1. / n_states ) \n    elif p_state . shape != ( n_states , ) : \n        raise ParameterError ( 'Marginal distribution p_state must have shape (n_states,). ' 'Got p_state.shape={}' . format ( p_state . shape ) ) \n    elif np . any ( p_state < 0 ) or not np . allclose ( p_state . sum ( axis = - 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid marginal state distribution: ' 'p_state={}' . format ( p_state ) ) \n    log_trans = np . log ( transition + epsilon ) \n    log_marginal = np . log ( p_state + epsilon ) \n    log_prob = np . log ( prob . T + epsilon ) - log_marginal \n    if p_init is None : \n        p_init = np . empty ( n_states ) \n        p_init . fill ( 1. / n_states ) \n    elif np . any ( p_init < 0 ) or not np . allclose ( p_init . sum ( ) , 1 ) : \n        raise ParameterError ( 'Invalid initial state distribution: ' 'p_init={}' . format ( p_init ) ) \n    log_p_init = np . log ( p_init + epsilon ) \n    _viterbi ( log_prob , log_trans , log_p_init , states , values , ptr ) \n    if return_logp : \n        return states , values [ - 1 , states [ - 1 ] ] \n    return states "}
{"2184": "\ndef transition_local ( n_states , width , window = 'triangle' , wrap = 0 ) : \n    if not isinstance ( n_states , int ) or n_states <= 1 : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    width = np . asarray ( width , dtype = int ) \n    if width . ndim == 0 : \n        width = np . tile ( width , n_states ) \n    if width . shape != ( n_states , ) : \n        raise ParameterError ( 'width={} must have length equal to n_states={}' . format ( width , n_states ) ) \n    if np . any ( width < 1 ) : \n        raise ParameterError ( 'width={} must be at least 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    for i , width_i in enumerate ( width ) : \n        trans_row = pad_center ( get_window ( window , width_i , fftbins = 0 ) , n_states ) \n        trans_row = np . roll ( trans_row , n_states // 2 + i + 1 ) \n        if not wrap : \n            trans_row [ min ( n_states , i + width_i // 2 + 1 ) : ] = 0 \n            trans_row [ : max ( 0 , i - width_i // 2 ) ] = 0 \n        transition [ i ] = trans_row \n    transition /= transition . sum ( axis = 1 , keepdims = 1 ) \n    return transition "}
{"2185": "\ndef onset_detect ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , backtrack = 0 , energy = None , units = 'frames' , ** kwargs ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) \n    onset_envelope -= onset_envelope . min ( ) \n    if not onset_envelope . any ( ) : \n        return np . array ( [ ] , dtype = np . int ) \n    onset_envelope /= onset_envelope . max ( ) \n    kwargs . setdefault ( 'pre_max' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'post_max' , 0.00 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'pre_avg' , 0.10 * sr // hop_length ) \n    kwargs . setdefault ( 'post_avg' , 0.10 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'wait' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'delta' , 0.07 ) \n    onsets = util . peak_pick ( onset_envelope , ** kwargs ) \n    if backtrack : \n        if energy is None : \n            energy = onset_envelope \n        onsets = onset_backtrack ( onsets , energy ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) \n    elif units == 'time' : \n        onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return onsets "}
{"2186": "\ndef onset_strength ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = 0 , center = 1 , feature = None , aggregate = None , centering = None , ** kwargs ) : \n    if aggregate is 0 : \n        raise ParameterError ( 'aggregate={} cannot be False when computing full-spectrum onset strength.' ) \n    odf_all = onset_strength_multi ( y = y , sr = sr , S = S , lag = lag , max_size = max_size , ref = ref , detrend = detrend , center = center , feature = feature , aggregate = aggregate , channels = None , ** kwargs ) \n    return odf_all [ 0 ] "}
{"2187": "\ndef onset_backtrack ( events , energy ) : \n    minima = np . flatnonzero ( ( energy [ 1 : - 1 ] <= energy [ : - 2 ] ) & ( energy [ 1 : - 1 ] < energy [ 2 : ] ) ) \n    minima = util . fix_frames ( 1 + minima , x_min = 0 ) \n    return minima [ util . match_events ( events , minima , right = 0 ) ] "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = 0 , center = 1 , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if lag < 1 or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if max_size < 1 or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048 ) \n    hop_length = kwargs . get ( 'hop_length' , 512 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if max_size == 1 : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) \n    elif ref . shape != S . shape : \n        raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = 1 \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = 0 \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = 0 ) \n    pad_width = lag \n    if center : \n        pad_width += n_fft // ( 2 * hop_length ) \n    onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - 1 ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ 1 ] ] \n    return onset_env "}
{"2190": "\ndef write_wav ( path , y , sr , norm = 0 ) : \n    util . valid_audio ( y , mono = 0 ) \n    if norm and np . issubdtype ( y . dtype , np . floating ) : \n        wav = util . normalize ( y , norm = np . inf , axis = None ) \n    else : \n        wav = y \n    if wav . ndim > 1 and wav . shape [ 0 ] == 2 : \n        wav = wav . T \n    scipy . io . wavfile . write ( path , sr , wav ) "}
{"2191": "\ndef cmap ( data , robust = 1 , cmap_seq = 'magma' , cmap_bool = 'gray_r' , cmap_div = 'coolwarm' ) : \n    data = np . atleast_1d ( data ) \n    if data . dtype == 'bool' : \n        return get_cmap ( cmap_bool ) \n    data = data [ np . isfinite ( data ) ] \n    if robust : \n        min_p , max_p = 2 , 98 \n    else : \n        min_p , max_p = 0 , 100 \n    max_val = np . percentile ( data , max_p ) \n    min_val = np . percentile ( data , min_p ) \n    if min_val >= 0 or max_val <= 0 : \n        return get_cmap ( cmap_seq ) \n    return get_cmap ( cmap_div ) "}
{"2192": "\ndef waveplot ( y , sr = 22050 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , max_sr = 1000 , ax = None , ** kwargs ) : \n    util . valid_audio ( y , mono = 0 ) \n    if not ( isinstance ( max_sr , int ) and max_sr > 0 ) : \n        raise ParameterError ( 'max_sr must be a non-negative integer' ) \n    target_sr = sr \n    hop_length = 1 \n    if max_points is not None : \n        if max_points <= 0 : \n            raise ParameterError ( 'max_points must be strictly positive' ) \n        if max_points < y . shape [ - 1 ] : \n            target_sr = min ( max_sr , ( sr * y . shape [ - 1 ] ) // max_points ) \n        hop_length = sr // target_sr \n        if y . ndim == 1 : \n            y = __envelope ( y , hop_length ) \n        else : \n            y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) \n    if y . ndim > 1 : \n        y_top = y [ 0 ] \n        y_bottom = - y [ 1 ] \n    else : \n        y_top = y \n        y_bottom = - y \n    axes = __check_axes ( ax ) \n    kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) \n    locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , sr = sr , hop_length = hop_length ) \n    out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) \n    axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) \n    if x_axis == 'time' : \n        axes . xaxis . set_major_formatter ( TimeFormatter ( lag = 0 ) ) \n        axes . xaxis . set_label_text ( 'Time' ) \n    elif x_axis is None or x_axis in [ 'off' , 'none' ] : \n        axes . set_xticks ( [ ] ) \n    else : \n        raise ParameterError ( 'Unknown x_axis value: {}' . format ( x_axis ) ) \n    return out "}
{"2200": "\ndef __coord_chroma ( n , bins_per_octave = 12 , ** _kwargs ) : \n    return np . linspace ( 0 , ( 12.0 * n ) / bins_per_octave , num = n + 1 , endpoint = 1 ) "}
{"2203": "\ndef piptrack ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = None , fmin = 150.0 , fmax = 4000.0 , threshold = 0.1 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' , ref = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    S = np . abs ( S ) \n    fmin = np . maximum ( fmin , 0 ) \n    fmax = np . minimum ( fmax , float ( sr ) / 2 ) \n    fft_freqs = time_frequency . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    avg = 0.5 * ( S [ 2 : ] - S [ : - 2 ] ) \n    shift = 2 * S [ 1 : - 1 ] - S [ 2 : ] - S [ : - 2 ] \n    shift = avg / ( shift + ( np . abs ( shift ) < util . tiny ( shift ) ) ) \n    avg = np . pad ( avg , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    shift = np . pad ( shift , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    dskew = 0.5 * avg * shift \n    pitches = np . zeros_like ( S ) \n    mags = np . zeros_like ( S ) \n    freq_mask = ( ( fmin <= fft_freqs ) & ( fft_freqs < fmax ) ) . reshape ( ( - 1 , 1 ) ) \n    if ref is None : \n        ref = np . max \n    if six . callable ( ref ) : \n        ref_value = threshold * ref ( S , axis = 0 ) \n    else : \n        ref_value = np . abs ( ref ) \n    idx = np . argwhere ( freq_mask & util . localmax ( S * ( S > ref_value ) ) ) \n    pitches [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( ( idx [ : , 0 ] + shift [ idx [ : , 0 ] , idx [ : , 1 ] ] ) * float ( sr ) / n_fft ) \n    mags [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( S [ idx [ : , 0 ] , idx [ : , 1 ] ] + dskew [ idx [ : , 0 ] , idx [ : , 1 ] ] ) \n    return pitches , mags "}
{"2209": "\ndef remix ( y , intervals , align_zeros = 1 ) : \n    util . valid_audio ( y , mono = 0 ) \n    y_out = [ ] \n    if align_zeros : \n        y_mono = core . to_mono ( y ) \n        zeros = np . nonzero ( core . zero_crossings ( y_mono ) ) [ - 1 ] \n        zeros = np . append ( zeros , [ len ( y_mono ) ] ) \n    clip = [ slice ( None ) ] * y . ndim \n    for interval in intervals : \n        if align_zeros : \n            interval = zeros [ util . match_events ( interval , zeros ) ] \n        clip [ - 1 ] = slice ( interval [ 0 ] , interval [ 1 ] ) \n        y_out . append ( y [ tuple ( clip ) ] ) \n    return np . concatenate ( y_out , axis = - 1 ) "}
{"2215": "\ndef _spectrogram ( y = None , S = None , n_fft = 2048 , hop_length = 512 , power = 1 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' ) : \n    if S is not None : \n        n_fft = 2 * ( S . shape [ 0 ] - 1 ) \n    else : \n        S = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , win_length = win_length , center = center , window = window , pad_mode = pad_mode ) ) ** power \n    return S , n_fft "}
{"2217": "\ndef decompose ( S , n_components = None , transformer = None , sort = 0 , fit = 1 , ** kwargs ) : \n    if transformer is None : \n        if fit is 0 : \n            raise ParameterError ( 'fit must be True if transformer is None' ) \n        transformer = sklearn . decomposition . NMF ( n_components = n_components , ** kwargs ) \n    if n_components is None : \n        n_components = S . shape [ 0 ] \n    if fit : \n        activations = transformer . fit_transform ( S . T ) . T \n    else : \n        activations = transformer . transform ( S . T ) . T \n    components = transformer . components_ . T \n    if sort : \n        components , idx = util . axis_sort ( components , index = 1 ) \n        activations = activations [ idx ] \n    return components , activations "}
{"2218": "\ndef nn_filter ( S , rec = None , aggregate = None , axis = - 1 , ** kwargs ) : \n    if aggregate is None : \n        aggregate = np . mean \n    if rec is None : \n        kwargs = dict ( kwargs ) \n        kwargs [ 'sparse' ] = 1 \n        rec = segment . recurrence_matrix ( S , axis = axis , ** kwargs ) \n    elif not scipy . sparse . issparse ( rec ) : \n        rec = scipy . sparse . csr_matrix ( rec ) \n    if rec . shape [ 0 ] != S . shape [ axis ] or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'Invalid self-similarity matrix shape ' 'rec.shape={} for S.shape={}' . format ( rec . shape , S . shape ) ) \n    return __nn_filter_helper ( rec . data , rec . indices , rec . indptr , S . swapaxes ( 0 , axis ) , aggregate ) . swapaxes ( 0 , axis ) "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = 0 , norm = 1 , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2 \n    if norm is not None and norm != 1 and norm != np . inf : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    for i in range ( n_mels ) : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2 ] / fdiff [ i + 1 ] \n        weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) \n    if norm == 1 : \n        enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) \n        weights *= enorm [ : , np . newaxis ] \n    if not np . all ( ( mel_f [ : - 2 ] == 0 ) | ( weights . max ( axis = 1 ) > 0 ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2221": "\ndef chroma ( sr , n_fft , n_chroma = 12 , A440 = 440.0 , ctroct = 5.0 , octwidth = 2 , norm = 2 , base_c = 1 , dtype = np . float32 ) : \n    wts = np . zeros ( ( n_chroma , n_fft ) ) \n    frequencies = np . linspace ( 0 , sr , n_fft , endpoint = 0 ) [ 1 : ] \n    frqbins = n_chroma * hz_to_octs ( frequencies , A440 ) \n    frqbins = np . concatenate ( ( [ frqbins [ 0 ] - 1.5 * n_chroma ] , frqbins ) ) \n    binwidthbins = np . concatenate ( ( np . maximum ( frqbins [ 1 : ] - frqbins [ : - 1 ] , 1.0 ) , [ 1 ] ) ) \n    D = np . subtract . outer ( frqbins , np . arange ( 0 , n_chroma , dtype = 'd' ) ) . T \n    n_chroma2 = np . round ( float ( n_chroma ) / 2 ) \n    D = np . remainder ( D + n_chroma2 + 10 * n_chroma , n_chroma ) - n_chroma2 \n    wts = np . exp ( - 0.5 * ( 2 * D / np . tile ( binwidthbins , ( n_chroma , 1 ) ) ) ** 2 ) \n    wts = util . normalize ( wts , norm = norm , axis = 0 ) \n    if octwidth is not None : \n        wts *= np . tile ( np . exp ( - 0.5 * ( ( ( frqbins / n_chroma - ctroct ) / octwidth ) ** 2 ) ) , ( n_chroma , 1 ) ) \n    if base_c : \n        wts = np . roll ( wts , - 3 , axis = 0 ) \n    return np . ascontiguousarray ( wts [ : , : int ( 1 + n_fft / 2 ) ] , dtype = dtype ) "}
{"2223": "\ndef constant_q ( sr , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , window = 'hann' , filter_scale = 1 , pad_fft = 1 , norm = 1 , dtype = np . complex64 , ** kwargs ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    lengths = constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - 1 ) \n    freqs = Q * sr / lengths \n    filters = [ ] \n    for ilen , freq in zip ( lengths , freqs ) : \n        sig = np . exp ( np . arange ( - ilen // 2 , ilen // 2 , dtype = float ) * 1j * 2 * np . pi * freq / sr ) \n        sig = sig * __float_window ( window ) ( len ( sig ) ) \n        sig = util . normalize ( sig , norm = norm ) \n        filters . append ( sig ) \n    max_len = max ( lengths ) \n    if pad_fft : \n        max_len = int ( 2.0 ** ( np . ceil ( np . log2 ( max_len ) ) ) ) \n    else : \n        max_len = int ( np . ceil ( max_len ) ) \n    filters = np . asarray ( [ util . pad_center ( filt , max_len , ** kwargs ) for filt in filters ] , dtype = dtype ) \n    return filters , np . asarray ( lengths ) "}
{"2225": "\ndef cq_to_chroma ( n_input , bins_per_octave = 12 , n_chroma = 12 , fmin = None , window = None , base_c = 1 , dtype = np . float32 ) : \n    n_merge = float ( bins_per_octave ) / n_chroma \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if np . mod ( n_merge , 1 ) != 0 : \n        raise ParameterError ( 'Incompatible CQ merge: ' 'input bins must be an ' 'integer multiple of output bins.' ) \n    cq_to_ch = np . repeat ( np . eye ( n_chroma ) , n_merge , axis = 1 ) \n    cq_to_ch = np . roll ( cq_to_ch , - int ( n_merge // 2 ) , axis = 1 ) \n    n_octaves = np . ceil ( np . float ( n_input ) / bins_per_octave ) \n    cq_to_ch = np . tile ( cq_to_ch , int ( n_octaves ) ) [ : , : n_input ] \n    midi_0 = np . mod ( hz_to_midi ( fmin ) , 12 ) \n    if base_c : \n        roll = midi_0 \n    else : \n        roll = midi_0 - 9 \n    roll = int ( np . round ( roll * ( n_chroma / 12. ) ) ) \n    cq_to_ch = np . roll ( cq_to_ch , roll , axis = 0 ) . astype ( dtype ) \n    if window is not None : \n        cq_to_ch = scipy . signal . convolve ( cq_to_ch , np . atleast_2d ( window ) , mode = 'same' ) \n    return cq_to_ch "}
{"2227": "\ndef get_window ( window , Nx , fftbins = 1 ) : \n    if six . callable ( window ) : \n        return window ( Nx ) \n    elif ( isinstance ( window , ( six . string_types , tuple ) ) or np . isscalar ( window ) ) : \n        return scipy . signal . get_window ( window , Nx , fftbins = fftbins ) \n    elif isinstance ( window , ( np . ndarray , list ) ) : \n        if len ( window ) == Nx : \n            return np . asarray ( window ) \n        raise ParameterError ( 'Window size mismatch: ' '{:d} != {:d}' . format ( len ( window ) , Nx ) ) \n    else : \n        raise ParameterError ( 'Invalid window specification: {}' . format ( window ) ) "}
{"2228": "\ndef _multirate_fb ( center_freqs = None , sample_rates = None , Q = 25.0 , passband_ripple = 1 , stopband_attenuation = 50 , ftype = 'ellip' , flayout = 'ba' ) : \n    if center_freqs is None : \n        raise ParameterError ( 'center_freqs must be provided.' ) \n    if sample_rates is None : \n        raise ParameterError ( 'sample_rates must be provided.' ) \n    if center_freqs . shape != sample_rates . shape : \n        raise ParameterError ( 'Number of provided center_freqs and sample_rates must be equal.' ) \n    nyquist = 0.5 * sample_rates \n    filter_bandwidths = center_freqs / float ( Q ) \n    filterbank = [ ] \n    for cur_center_freq , cur_nyquist , cur_bw in zip ( center_freqs , nyquist , filter_bandwidths ) : \n        passband_freqs = [ cur_center_freq - 0.5 * cur_bw , cur_center_freq + 0.5 * cur_bw ] / cur_nyquist \n        stopband_freqs = [ cur_center_freq - cur_bw , cur_center_freq + cur_bw ] / cur_nyquist \n        cur_filter = scipy . signal . iirdesign ( passband_freqs , stopband_freqs , passband_ripple , stopband_attenuation , analog = 0 , ftype = ftype , output = flayout ) \n        filterbank . append ( cur_filter ) \n    return filterbank , sample_rates "}
{"2232": "\ndef diagonal_filter ( window , n , slope = 1.0 , angle = None , zero_mean = 0 ) : \n    if angle is None : \n        angle = np . arctan ( slope ) \n    win = np . diag ( get_window ( window , n , fftbins = 0 ) ) \n    if not np . isclose ( angle , np . pi / 4 ) : \n        win = scipy . ndimage . rotate ( win , 45 - angle * 180 / np . pi , order = 5 , prefilter = 0 ) \n    np . clip ( win , 0 , None , out = win ) \n    win /= win . sum ( ) \n    if zero_mean : \n        win -= win . mean ( ) \n    return win "}
{"2233": "\ndef spectral_centroid ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , freq = None , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with real-valued input' ) \n    elif np . any ( S < 0 ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    return np . sum ( freq * util . normalize ( S , norm = 1 , axis = 0 ) , axis = 0 , keepdims = 1 ) "}
{"2234": "\ndef spectral_rolloff ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' , freq = None , roll_percent = 0.85 ) : \n    if not 0.0 < roll_percent < 1.0 : \n        raise ParameterError ( 'roll_percent must lie in the range (0, 1)' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with real-valued input' ) \n    elif np . any ( S < 0 ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    total_energy = np . cumsum ( S , axis = 0 ) \n    threshold = roll_percent * total_energy [ - 1 ] \n    ind = np . where ( total_energy < threshold , np . nan , 1 ) \n    return np . nanmin ( ind * freq , axis = 0 , keepdims = 1 ) "}
{"2235": "\ndef spectral_flatness ( y = None , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' , amin = 1e-10 , power = 2.0 ) : \n    if amin <= 0 : \n        raise ParameterError ( 'amin must be strictly positive' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 1. , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with real-valued input' ) \n    elif np . any ( S < 0 ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with non-negative energies' ) \n    S_thresh = np . maximum ( amin , S ** power ) \n    gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = 0 , keepdims = 1 ) ) \n    amean = np . mean ( S_thresh , axis = 0 , keepdims = 1 ) \n    return gmean / amean "}
{"2236": "\ndef poly_features ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' , order = 1 , freq = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        coefficients = np . polyfit ( freq , S , order ) \n    else : \n        coefficients = np . concatenate ( [ [ np . polyfit ( freq [ : , i ] , S [ : , i ] , order ) ] for i in range ( S . shape [ 1 ] ) ] , axis = 0 ) . T \n    return coefficients "}
{"2237": "\ndef zero_crossing_rate ( y , frame_length = 2048 , hop_length = 512 , center = 1 , ** kwargs ) : \n    util . valid_audio ( y ) \n    if center : \n        y = np . pad ( y , int ( frame_length // 2 ) , mode = 'edge' ) \n    y_framed = util . frame ( y , frame_length , hop_length ) \n    kwargs [ 'axis' ] = 0 \n    kwargs . setdefault ( 'pad' , 0 ) \n    crossings = zero_crossings ( y_framed , ** kwargs ) \n    return np . mean ( crossings , axis = 0 , keepdims = 1 ) "}
{"2238": "\ndef chroma_stft ( y = None , sr = 22050 , S = None , norm = np . inf , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' , tuning = None , ** kwargs ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 2 , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    n_chroma = kwargs . get ( 'n_chroma' , 12 ) \n    if tuning is None : \n        tuning = estimate_tuning ( S = S , sr = sr , bins_per_octave = n_chroma ) \n    if 'A440' not in kwargs : \n        kwargs [ 'A440' ] = 440.0 * 2.0 ** ( float ( tuning ) / n_chroma ) \n    chromafb = filters . chroma ( sr , n_fft , ** kwargs ) \n    raw_chroma = np . dot ( chromafb , S ) \n    return util . normalize ( raw_chroma , norm = norm , axis = 0 ) "}
{"2240": "\ndef melspectrogram ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = 1 , pad_mode = 'reflect' , power = 2.0 , ** kwargs ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = power , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    mel_basis = filters . mel ( sr , n_fft , ** kwargs ) \n    return np . dot ( mel_basis , S ) "}
{"2243": "\ndef __match_intervals ( intervals_from , intervals_to , strict = 1 ) : \n    start_index = np . argsort ( intervals_to [ : , 0 ] ) \n    end_index = np . argsort ( intervals_to [ : , 1 ] ) \n    start_sorted = intervals_to [ start_index , 0 ] \n    end_sorted = intervals_to [ end_index , 1 ] \n    search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) \n    search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) \n    output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) \n    for i in range ( len ( intervals_from ) ) : \n        query = intervals_from [ i ] \n        after_query = search_ends [ i ] \n        before_query = search_starts [ i ] \n        candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) \n        if len ( candidates ) > 0 : \n            output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) \n        elif strict : \n            raise ParameterError \n        else : \n            dist_before = np . inf \n            dist_after = np . inf \n            if search_starts [ i ] > 0 : \n                dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] \n            if search_ends [ i ] + 1 < len ( intervals_to ) : \n                dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] \n            if dist_before < dist_after : \n                output [ i ] = end_index [ search_starts [ i ] - 1 ] \n            else : \n                output [ i ] = start_index [ search_ends [ i ] + 1 ] \n    return output "}
{"2244": "\ndef match_intervals ( intervals_from , intervals_to , strict = 1 ) : \n    if len ( intervals_from ) == 0 or len ( intervals_to ) == 0 : \n        raise ParameterError ( 'Attempting to match empty interval list' ) \n    valid_intervals ( intervals_from ) \n    valid_intervals ( intervals_to ) \n    try : \n        return __match_intervals ( intervals_from , intervals_to , strict = strict ) \n    except ParameterError : \n        six . reraise ( ParameterError , ParameterError ( 'Unable to match intervals with strict={}' . format ( strict ) ) , sys . exc_info ( ) [ 2 ] ) "}
{"2245": "\ndef match_events ( events_from , events_to , left = 1 , right = 1 ) : \n    if len ( events_from ) == 0 or len ( events_to ) == 0 : \n        raise ParameterError ( 'Attempting to match empty event list' ) \n    if not ( left or right ) and not np . all ( np . in1d ( events_from , events_to ) ) : \n        raise ParameterError ( 'Cannot match events with left=right=False ' 'and events_from is not contained ' 'in events_to' ) \n    if ( not left ) and max ( events_to ) < max ( events_from ) : \n        raise ParameterError ( 'Cannot match events with left=False ' 'and max(events_to) < max(events_from)' ) \n    if ( not right ) and min ( events_to ) > min ( events_from ) : \n        raise ParameterError ( 'Cannot match events with right=False ' 'and min(events_to) > min(events_from)' ) \n    output = np . empty_like ( events_from , dtype = np . int ) \n    return __match_events_helper ( output , events_from , events_to , left , right ) "}
{"2246": "\ndef salience ( S , freqs , h_range , weights = None , aggregate = None , filter_peaks = 1 , fill_value = np . nan , kind = 'linear' , axis = 0 ) : \n    if aggregate is None : \n        aggregate = np . average \n    if weights is None : \n        weights = np . ones ( ( len ( h_range ) , ) ) \n    else : \n        weights = np . array ( weights , dtype = float ) \n    S_harm = interp_harmonics ( S , freqs , h_range , kind = kind , axis = axis ) \n    if aggregate is np . average : \n        S_sal = aggregate ( S_harm , axis = 0 , weights = weights ) \n    else : \n        S_sal = aggregate ( S_harm , axis = 0 ) \n    if filter_peaks : \n        S_peaks = scipy . signal . argrelmax ( S , axis = 0 ) \n        S_out = np . empty ( S . shape ) \n        S_out . fill ( fill_value ) \n        S_out [ S_peaks [ 0 ] , S_peaks [ 1 ] ] = S_sal [ S_peaks [ 0 ] , S_peaks [ 1 ] ] \n        S_sal = S_out \n    return S_sal "}
{"2248": "\ndef harmonics_1d ( harmonic_out , x , freqs , h_range , kind = 'linear' , fill_value = 0 , axis = 0 ) : \n    f_interp = scipy . interpolate . interp1d ( freqs , x , kind = kind , axis = axis , copy = 0 , bounds_error = 0 , fill_value = fill_value ) \n    idx_out = [ slice ( None ) ] * harmonic_out . ndim \n    interp_axis = 1 + ( axis % x . ndim ) \n    for h_index , harmonic in enumerate ( h_range ) : \n        idx_out [ 0 ] = h_index \n        for f_index , frequency in enumerate ( freqs ) : \n            idx_out [ interp_axis ] = f_index \n            harmonic_out [ tuple ( idx_out ) ] = f_interp ( harmonic * frequency ) "}
{"2250": "\ndef load ( path , sr = 22050 , mono = 1 , offset = 0.0 , duration = None , dtype = np . float32 , res_type = 'kaiser_best' ) : \n    try : \n        with sf . SoundFile ( path ) as sf_desc : \n            sr_native = sf_desc . samplerate \n            if offset : \n                sf_desc . seek ( int ( offset * sr_native ) ) \n            if duration is not None : \n                frame_duration = int ( duration * sr_native ) \n            else : \n                frame_duration = - 1 \n            y = sf_desc . read ( frames = frame_duration , dtype = dtype , always_2d = 0 ) . T \n    except RuntimeError as exc : \n        y , sr_native = __audioread_load ( path , offset , duration , dtype ) \n    if mono : \n        y = to_mono ( y ) \n    if sr is not None : \n        y = resample ( y , sr_native , sr , res_type = res_type ) \n    else : \n        sr = sr_native \n    return y , sr "}
{"2252": "\ndef to_mono ( y ) : \n    util . valid_audio ( y , mono = 0 ) \n    if y . ndim > 1 : \n        y = np . mean ( y , axis = 0 ) \n    return y "}
{"2253": "\ndef resample ( y , orig_sr , target_sr , res_type = 'kaiser_best' , fix = 1 , scale = 0 , ** kwargs ) : \n    util . valid_audio ( y , mono = 0 ) \n    if orig_sr == target_sr : \n        return y \n    ratio = float ( target_sr ) / orig_sr \n    n_samples = int ( np . ceil ( y . shape [ - 1 ] * ratio ) ) \n    if res_type in ( 'scipy' , 'fft' ) : \n        y_hat = scipy . signal . resample ( y , n_samples , axis = - 1 ) \n    elif res_type == 'polyphase' : \n        if int ( orig_sr ) != orig_sr or int ( target_sr ) != target_sr : \n            raise ParameterError ( 'polyphase resampling is only supported for integer-valued sampling rates.' ) \n        orig_sr = int ( orig_sr ) \n        target_sr = int ( target_sr ) \n        gcd = np . gcd ( orig_sr , target_sr ) \n        y_hat = scipy . signal . resample_poly ( y , target_sr // gcd , orig_sr // gcd , axis = - 1 ) \n    else : \n        y_hat = resampy . resample ( y , orig_sr , target_sr , filter = res_type , axis = - 1 ) \n    if fix : \n        y_hat = util . fix_length ( y_hat , n_samples , ** kwargs ) \n    if scale : \n        y_hat /= np . sqrt ( ratio ) \n    return np . ascontiguousarray ( y_hat , dtype = y . dtype ) "}
{"2255": "\ndef lpc ( y , order ) : \n    if not isinstance ( order , int ) or order < 1 : \n        raise ParameterError ( \"order must be an integer > 0\" ) \n    util . valid_audio ( y , mono = 1 ) \n    return __lpc ( y , order ) "}
{"2256": "\ndef clicks ( times = None , frames = None , sr = 22050 , hop_length = 512 , click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : \n    if times is None : \n        if frames is None : \n            raise ParameterError ( 'either \"times\" or \"frames\" must be provided' ) \n        positions = frames_to_samples ( frames , hop_length = hop_length ) \n    else : \n        positions = time_to_samples ( times , sr = sr ) \n    if click is not None : \n        util . valid_audio ( click , mono = 1 ) \n    else : \n        if click_duration <= 0 : \n            raise ParameterError ( 'click_duration must be strictly positive' ) \n        if click_freq <= 0 : \n            raise ParameterError ( 'click_freq must be strictly positive' ) \n        angular_freq = 2 * np . pi * click_freq / float ( sr ) \n        click = np . logspace ( 0 , - 10 , num = int ( np . round ( sr * click_duration ) ) , base = 2.0 ) \n        click *= np . sin ( angular_freq * np . arange ( len ( click ) ) ) \n    if length is None : \n        length = positions . max ( ) + click . shape [ 0 ] \n    else : \n        if length < 1 : \n            raise ParameterError ( 'length must be a positive integer' ) \n        positions = positions [ positions < length ] \n    click_signal = np . zeros ( length , dtype = np . float32 ) \n    for start in positions : \n        end = start + click . shape [ 0 ] \n        if end >= length : \n            click_signal [ start : ] += click [ : length - start ] \n        else : \n            click_signal [ start : end ] += click \n    return click_signal "}
{"2258": "\ndef chirp ( fmin , fmax , sr = 22050 , length = None , duration = None , linear = 0 , phi = None ) : \n    if fmin is None or fmax is None : \n        raise ParameterError ( 'both \"fmin\" and \"fmax\" must be provided' ) \n    period = 1.0 / sr \n    if length is None : \n        if duration is None : \n            raise ParameterError ( 'either \"length\" or \"duration\" must be provided' ) \n    else : \n        duration = period * length \n    if phi is None : \n        phi = - np . pi * 0.5 \n    method = 'linear' if linear else 'logarithmic' \n    return scipy . signal . chirp ( np . arange ( duration , step = period ) , fmin , duration , fmax , method = method , phi = phi / np . pi * 180 , ) "}
{"2261": "\ndef process_arguments ( args ) : \n    parser = argparse . ArgumentParser ( description = 'Time stretching example' ) \n    parser . add_argument ( 'input_file' , action = 'store' , help = 'path to the input file (wav, mp3, etc)' ) \n    parser . add_argument ( 'output_file' , action = 'store' , help = 'path to the stretched output (wav)' ) \n    parser . add_argument ( '-s' , '--speed' , action = 'store' , type = float , default = 2.0 , required = 0 , help = 'speed' ) \n    return vars ( parser . parse_args ( args ) ) "}
{"2263": "\ndef beat_track ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , start_bpm = 120.0 , tightness = 100 , trim = 1 , bpm = None , units = 'frames' ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset . onset_strength ( y = y , sr = sr , hop_length = hop_length , aggregate = np . median ) \n    if not onset_envelope . any ( ) : \n        return ( 0 , np . array ( [ ] , dtype = int ) ) \n    if bpm is None : \n        bpm = tempo ( onset_envelope = onset_envelope , sr = sr , hop_length = hop_length , start_bpm = start_bpm ) [ 0 ] \n    beats = __beat_tracker ( onset_envelope , bpm , float ( sr ) / hop_length , tightness , trim ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        beats = core . frames_to_samples ( beats , hop_length = hop_length ) \n    elif units == 'time' : \n        beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return ( bpm , beats ) "}
{"2266": "\ndef __beat_track_dp ( localscore , period , tightness ) : \n    backlink = np . zeros_like ( localscore , dtype = int ) \n    cumscore = np . zeros_like ( localscore ) \n    window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) \n    if tightness <= 0 : \n        raise ParameterError ( 'tightness must be strictly positive' ) \n    txwt = - tightness * ( np . log ( - window / period ) ** 2 ) \n    first_beat = 1 \n    for i , score_i in enumerate ( localscore ) : \n        z_pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) \n        candidates = txwt . copy ( ) \n        candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] \n        beat_location = np . argmax ( candidates ) \n        cumscore [ i ] = score_i + candidates [ beat_location ] \n        if first_beat and score_i < 0.01 * localscore . max ( ) : \n            backlink [ i ] = - 1 \n        else : \n            backlink [ i ] = window [ beat_location ] \n            first_beat = 0 \n        window = window + 1 \n    return backlink , cumscore "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = 1 , axis = - 1 ) : \n    axis = np . abs ( axis ) \n    if rec . ndim != 2 or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = 1 - axis \n        lag_format = rec . format \n        if axis == 0 : \n            rec = rec . tocsc ( ) \n        elif axis in ( - 1 , 1 ) : \n            rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    else : \n        if pad : \n            padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] \n            padding [ ( 1 - axis ) ] = ( 0 , t ) \n            lag = np . pad ( rec , padding , mode = 'constant' ) \n        else : \n            lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2270": "\ndef timelag_filter ( function , pad = 1 , index = 0 ) : \n    def __my_filter ( wrapped_f , * args , ** kwargs ) : \n        args = list ( args ) \n        args [ index ] = recurrence_to_lag ( args [ index ] , pad = pad ) \n        result = wrapped_f ( * args , ** kwargs ) \n        return lag_to_recurrence ( result ) \n    return decorator ( __my_filter , function ) "}
{"2271": "\ndef subsegment ( data , frames , n_segments = 4 , axis = - 1 ) : \n    frames = util . fix_frames ( frames , x_min = 0 , x_max = data . shape [ axis ] , pad = 1 ) \n    if n_segments < 1 : \n        raise ParameterError ( 'n_segments must be a positive integer' ) \n    boundaries = [ ] \n    idx_slices = [ slice ( None ) ] * data . ndim \n    for seg_start , seg_end in zip ( frames [ : - 1 ] , frames [ 1 : ] ) : \n        idx_slices [ axis ] = slice ( seg_start , seg_end ) \n        boundaries . extend ( seg_start + agglomerative ( data [ tuple ( idx_slices ) ] , min ( seg_end - seg_start , n_segments ) , axis = axis ) ) \n    return np . ascontiguousarray ( boundaries ) "}
{"2273": "\ndef path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7 , zero_mean = 0 , clip = 1 , ** kwargs ) : \n    if min_ratio is None : \n        min_ratio = 1. / max_ratio \n    elif min_ratio > max_ratio : \n        raise ParameterError ( 'min_ratio={} cannot exceed max_ratio={}' . format ( min_ratio , max_ratio ) ) \n    R_smooth = None \n    for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2 ) : \n        kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) \n        if R_smooth is None : \n            R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) \n        else : \n            np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , out = R_smooth ) \n    if clip : \n        np . clip ( R_smooth , 0 , None , out = R_smooth ) \n    return R_smooth "}
{"2276": "\ndef valid_audio ( y , mono = 1 ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and y . ndim != 1 : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    elif y . ndim > 2 or y . ndim == 0 : \n        raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return 1 "}
{"2279": "\ndef axis_sort ( S , axis = - 1 , index = 0 , value = None ) : \n    if value is None : \n        value = np . argmax \n    if S . ndim != 2 : \n        raise ParameterError ( 'axis_sort is only defined for 2D arrays' ) \n    bin_idx = value ( S , axis = np . mod ( 1 - axis , S . ndim ) ) \n    idx = np . argsort ( bin_idx ) \n    sort_slice = [ slice ( None ) ] * S . ndim \n    sort_slice [ axis ] = idx \n    if index : \n        return S [ tuple ( sort_slice ) ] , idx \n    else : \n        return S [ tuple ( sort_slice ) ] "}
{"2280": "\ndef normalize ( S , norm = np . inf , axis = 0 , threshold = None , fill = None ) : \n    if threshold is None : \n        threshold = tiny ( S ) \n    elif threshold <= 0 : \n        raise ParameterError ( 'threshold={} must be strictly ' 'positive' . format ( threshold ) ) \n    if fill not in [ None , 0 , 1 ] : \n        raise ParameterError ( 'fill={} must be None or boolean' . format ( fill ) ) \n    if not np . all ( np . isfinite ( S ) ) : \n        raise ParameterError ( 'Input must be finite' ) \n    mag = np . abs ( S ) . astype ( np . float ) \n    fill_norm = 1 \n    if norm == np . inf : \n        length = np . max ( mag , axis = axis , keepdims = 1 ) \n    elif norm == - np . inf : \n        length = np . min ( mag , axis = axis , keepdims = 1 ) \n    elif norm == 0 : \n        if fill is 1 : \n            raise ParameterError ( 'Cannot normalize with norm=0 and fill=True' ) \n        length = np . sum ( mag > 0 , axis = axis , keepdims = 1 , dtype = mag . dtype ) \n    elif np . issubdtype ( type ( norm ) , np . number ) and norm > 0 : \n        length = np . sum ( mag ** norm , axis = axis , keepdims = 1 ) ** ( 1. / norm ) \n        if axis is None : \n            fill_norm = mag . size ** ( - 1. / norm ) \n        else : \n            fill_norm = mag . shape [ axis ] ** ( - 1. / norm ) \n    elif norm is None : \n        return S \n    else : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    small_idx = length < threshold \n    Snorm = np . empty_like ( S ) \n    if fill is None : \n        length [ small_idx ] = 1.0 \n        Snorm [ : ] = S / length \n    elif fill : \n        length [ small_idx ] = np . nan \n        Snorm [ : ] = S / length \n        Snorm [ np . isnan ( Snorm ) ] = fill_norm \n    else : \n        length [ small_idx ] = np . inf \n        Snorm [ : ] = S / length \n    return Snorm "}
{"2283": "\ndef sparsify_rows ( x , quantile = 0.01 ) : \n    if x . ndim == 1 : \n        x = x . reshape ( ( 1 , - 1 ) ) \n    elif x . ndim > 2 : \n        raise ParameterError ( 'Input must have 2 or fewer dimensions. ' 'Provided x.shape={}.' . format ( x . shape ) ) \n    if not 0.0 <= quantile < 1 : \n        raise ParameterError ( 'Invalid quantile {:.2f}' . format ( quantile ) ) \n    x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    mags = np . abs ( x ) \n    norms = np . sum ( mags , axis = 1 , keepdims = 1 ) \n    mag_sort = np . sort ( mags , axis = 1 ) \n    cumulative_mag = np . cumsum ( mag_sort / norms , axis = 1 ) \n    threshold_idx = np . argmin ( cumulative_mag < quantile , axis = 1 ) \n    for i , j in enumerate ( threshold_idx ) : \n        idx = np . where ( mags [ i ] >= mag_sort [ i , j ] ) \n        x_sparse [ i , idx ] = x [ i , idx ] \n    return x_sparse . tocsr ( ) "}
{"2286": "\ndef index_to_slice ( idx , idx_min = None , idx_max = None , step = None , pad = 1 ) : \n    idx_fixed = fix_frames ( idx , idx_min , idx_max , pad = pad ) \n    return [ slice ( start , end , step ) for ( start , end ) in zip ( idx_fixed , idx_fixed [ 1 : ] ) ] "}
{"2287": "\ndef sync ( data , idx , aggregate = None , pad = 1 , axis = - 1 ) : \n    if aggregate is None : \n        aggregate = np . mean \n    shape = list ( data . shape ) \n    if np . all ( [ isinstance ( _ , slice ) for _ in idx ] ) : \n        slices = idx \n    elif np . all ( [ np . issubdtype ( type ( _ ) , np . integer ) for _ in idx ] ) : \n        slices = index_to_slice ( np . asarray ( idx ) , 0 , shape [ axis ] , pad = pad ) \n    else : \n        raise ParameterError ( 'Invalid index set: {}' . format ( idx ) ) \n    agg_shape = list ( shape ) \n    agg_shape [ axis ] = len ( slices ) \n    data_agg = np . empty ( agg_shape , order = 'F' if np . isfortran ( data ) else 'C' , dtype = data . dtype ) \n    idx_in = [ slice ( None ) ] * data . ndim \n    idx_agg = [ slice ( None ) ] * data_agg . ndim \n    for ( i , segment ) in enumerate ( slices ) : \n        idx_in [ axis ] = segment \n        idx_agg [ axis ] = i \n        data_agg [ tuple ( idx_agg ) ] = aggregate ( data [ tuple ( idx_in ) ] , axis = axis ) \n    return data_agg "}
{"2288": "\ndef softmask ( X , X_ref , power = 1 , split_zeros = 0 ) : \n    if X . shape != X_ref . shape : \n        raise ParameterError ( 'Shape mismatch: {}!={}' . format ( X . shape , X_ref . shape ) ) \n    if np . any ( X < 0 ) or np . any ( X_ref < 0 ) : \n        raise ParameterError ( 'X and X_ref must be non-negative' ) \n    if power <= 0 : \n        raise ParameterError ( 'power must be strictly positive' ) \n    dtype = X . dtype \n    if not np . issubdtype ( dtype , np . floating ) : \n        dtype = np . float32 \n    Z = np . maximum ( X , X_ref ) . astype ( dtype ) \n    bad_idx = ( Z < np . finfo ( dtype ) . tiny ) \n    Z [ bad_idx ] = 1 \n    if np . isfinite ( power ) : \n        mask = ( X / Z ) ** power \n        ref_mask = ( X_ref / Z ) ** power \n        good_idx = ~ bad_idx \n        mask [ good_idx ] /= mask [ good_idx ] + ref_mask [ good_idx ] \n        if split_zeros : \n            mask [ bad_idx ] = 0.5 \n        else : \n            mask [ bad_idx ] = 0.0 \n    else : \n        mask = X > X_ref \n    return mask "}
{"2290": "\ndef frames2video ( frame_dir , video_file , fps = 30 , fourcc = 'XVID' , filename_tmpl = '{:06d}.jpg' , start = 0 , end = 0 , show_progress = 1 ) : \n    if end == 0 : \n        ext = filename_tmpl . split ( '.' ) [ - 1 ] \n        end = len ( [ name for name in scandir ( frame_dir , ext ) ] ) \n    first_file = osp . join ( frame_dir , filename_tmpl . format ( start ) ) \n    check_file_exist ( first_file , 'The start frame not found: ' + first_file ) \n    img = cv2 . imread ( first_file ) \n    height , width = img . shape [ : 2 ] \n    resolution = ( width , height ) \n    vwriter = cv2 . VideoWriter ( video_file , VideoWriter_fourcc ( * fourcc ) , fps , resolution ) \n    def write_frame ( file_idx ) : \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        img = cv2 . imread ( filename ) \n        vwriter . write ( img ) \n    if show_progress : \n        track_progress ( write_frame , range ( start , end ) ) \n    else : \n        for i in range ( start , end ) : \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i ) ) \n            img = cv2 . imread ( filename ) \n            vwriter . write ( img ) \n    vwriter . release ( ) "}
{"2291": "\ndef read ( self ) : \n    if self . _cache : \n        img = self . _cache . get ( self . _position ) \n        if img is not None : \n            ret = 1 \n        else : \n            if self . _position != self . _get_real_position ( ) : \n                self . _set_real_position ( self . _position ) \n            ret , img = self . _vcap . read ( ) \n            if ret : \n                self . _cache . put ( self . _position , img ) \n    else : \n        ret , img = self . _vcap . read ( ) \n    if ret : \n        self . _position += 1 \n    return img "}
{"2293": "\ndef cvt2frames ( self , frame_dir , file_start = 0 , filename_tmpl = '{:06d}.jpg' , start = 0 , max_num = 0 , show_progress = 1 ) : \n    mkdir_or_exist ( frame_dir ) \n    if max_num == 0 : \n        task_num = self . frame_cnt - start \n    else : \n        task_num = min ( self . frame_cnt - start , max_num ) \n    if task_num <= 0 : \n        raise ValueError ( 'start must be less than total frame number' ) \n    if start > 0 : \n        self . _set_real_position ( start ) \n    def write_frame ( file_idx ) : \n        img = self . read ( ) \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        cv2 . imwrite ( filename , img ) \n    if show_progress : \n        track_progress ( write_frame , range ( file_start , file_start + task_num ) ) \n    else : \n        for i in range ( task_num ) : \n            img = self . read ( ) \n            if img is None : \n                break \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i + file_start ) ) \n            cv2 . imwrite ( filename , img ) "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50 , chunksize = 1 , skip_first = 0 , keep_order = 1 ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num -= nproc * chunksize * int ( skip_first ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if len ( results ) < nproc * chunksize : \n                continue \n            elif len ( results ) == nproc * chunksize : \n                prog_bar . start ( ) \n                continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2297": "\ndef imrotate ( img , angle , center = None , scale = 1.0 , border_value = 0 , auto_bound = 0 ) : \n    if center is not None and auto_bound : \n        raise ValueError ( '`auto_bound` conflicts with `center`' ) \n    h , w = img . shape [ : 2 ] \n    if center is None : \n        center = ( ( w - 1 ) * 0.5 , ( h - 1 ) * 0.5 ) \n    assert isinstance ( center , tuple ) \n    matrix = cv2 . getRotationMatrix2D ( center , - angle , scale ) \n    if auto_bound : \n        cos = np . abs ( matrix [ 0 , 0 ] ) \n        sin = np . abs ( matrix [ 0 , 1 ] ) \n        new_w = h * sin + w * cos \n        new_h = h * cos + w * sin \n        matrix [ 0 , 2 ] += ( new_w - w ) * 0.5 \n        matrix [ 1 , 2 ] += ( new_h - h ) * 0.5 \n        w = int ( np . round ( new_w ) ) \n        h = int ( np . round ( new_h ) ) \n    rotated = cv2 . warpAffine ( img , matrix , ( w , h ) , borderValue = border_value ) \n    return rotated "}
{"2304": "\ndef imresize ( img , size , return_scale = 0 , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    resized_img = cv2 . resize ( img , size , interpolation = interp_codes [ interpolation ] ) \n    if not return_scale : \n        return resized_img \n    else : \n        w_scale = size [ 0 ] / w \n        h_scale = size [ 1 ] / h \n        return resized_img , w_scale , h_scale "}
{"2305": "\ndef imresize_like ( img , dst_img , return_scale = 0 , interpolation = 'bilinear' ) : \n    h , w = dst_img . shape [ : 2 ] \n    return imresize ( img , ( w , h ) , return_scale , interpolation ) "}
{"2306": "\ndef imrescale ( img , scale , return_scale = 0 , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    if isinstance ( scale , ( float , int ) ) : \n        if scale <= 0 : \n            raise ValueError ( 'Invalid scale {}, must be positive.' . format ( scale ) ) \n        scale_factor = scale \n    elif isinstance ( scale , tuple ) : \n        max_long_edge = max ( scale ) \n        max_short_edge = min ( scale ) \n        scale_factor = min ( max_long_edge / max ( h , w ) , max_short_edge / min ( h , w ) ) \n    else : \n        raise TypeError ( 'Scale must be a number or tuple of int, but got {}' . format ( type ( scale ) ) ) \n    new_size = _scale_size ( ( w , h ) , scale_factor ) \n    rescaled_img = imresize ( img , new_size , interpolation = interpolation ) \n    if return_scale : \n        return rescaled_img , scale_factor \n    else : \n        return rescaled_img "}
{"2311": "\ndef imshow_bboxes ( img , bboxes , colors = 'green' , top_k = - 1 , thickness = 1 , show = 1 , win_name = '' , wait_time = 0 , out_file = None ) : \n    img = imread ( img ) \n    if isinstance ( bboxes , np . ndarray ) : \n        bboxes = [ bboxes ] \n    if not isinstance ( colors , list ) : \n        colors = [ colors for _ in range ( len ( bboxes ) ) ] \n    colors = [ color_val ( c ) for c in colors ] \n    assert len ( bboxes ) == len ( colors ) \n    for i , _bboxes in enumerate ( bboxes ) : \n        _bboxes = _bboxes . astype ( np . int32 ) \n        if top_k <= 0 : \n            _top_k = _bboxes . shape [ 0 ] \n        else : \n            _top_k = min ( top_k , _bboxes . shape [ 0 ] ) \n        for j in range ( _top_k ) : \n            left_top = ( _bboxes [ j , 0 ] , _bboxes [ j , 1 ] ) \n            right_bottom = ( _bboxes [ j , 2 ] , _bboxes [ j , 3 ] ) \n            cv2 . rectangle ( img , left_top , right_bottom , colors [ i ] , thickness = thickness ) \n    if show : \n        imshow ( img , win_name , wait_time ) \n    if out_file is not None : \n        imwrite ( img , out_file ) "}
{"2312": "\ndef flowread ( flow_or_path , quantize = 0 , concat_axis = 0 , * args , ** kwargs ) : \n    if isinstance ( flow_or_path , np . ndarray ) : \n        if ( flow_or_path . ndim != 3 ) or ( flow_or_path . shape [ - 1 ] != 2 ) : \n            raise ValueError ( 'Invalid flow with shape {}' . format ( flow_or_path . shape ) ) \n        return flow_or_path \n    elif not is_str ( flow_or_path ) : \n        raise TypeError ( '\"flow_or_path\" must be a filename or numpy array, not {}' . format ( type ( flow_or_path ) ) ) \n    if not quantize : \n        with open ( flow_or_path , 'rb' ) as f : \n            try : \n                header = f . read ( 4 ) . decode ( 'utf-8' ) \n            except Exception : \n                raise IOError ( 'Invalid flow file: {}' . format ( flow_or_path ) ) \n            else : \n                if header != 'PIEH' : \n                    raise IOError ( 'Invalid flow file: {}, header does not contain PIEH' . format ( flow_or_path ) ) \n            w = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            h = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            flow = np . fromfile ( f , np . float32 , w * h * 2 ) . reshape ( ( h , w , 2 ) ) \n    else : \n        assert concat_axis in [ 0 , 1 ] \n        cat_flow = imread ( flow_or_path , flag = 'unchanged' ) \n        if cat_flow . ndim != 2 : \n            raise IOError ( '{} is not a valid quantized flow file, its dimension is {}.' . format ( flow_or_path , cat_flow . ndim ) ) \n        assert cat_flow . shape [ concat_axis ] % 2 == 0 \n        dx , dy = np . split ( cat_flow , 2 , axis = concat_axis ) \n        flow = dequantize_flow ( dx , dy , * args , ** kwargs ) \n    return flow . astype ( np . float32 ) "}
{"2313": "\ndef flowwrite ( flow , filename , quantize = 0 , concat_axis = 0 , * args , ** kwargs ) : \n    if not quantize : \n        with open ( filename , 'wb' ) as f : \n            f . write ( 'PIEH' . encode ( 'utf-8' ) ) \n            np . array ( [ flow . shape [ 1 ] , flow . shape [ 0 ] ] , dtype = np . int32 ) . tofile ( f ) \n            flow = flow . astype ( np . float32 ) \n            flow . tofile ( f ) \n            f . flush ( ) \n    else : \n        assert concat_axis in [ 0 , 1 ] \n        dx , dy = quantize_flow ( flow , * args , ** kwargs ) \n        dxdy = np . concatenate ( ( dx , dy ) , axis = concat_axis ) \n        imwrite ( dxdy , filename ) "}
{"2314": "\ndef dequantize_flow ( dx , dy , max_val = 0.02 , denorm = 1 ) : \n    assert dx . shape == dy . shape \n    assert dx . ndim == 2 or ( dx . ndim == 3 and dx . shape [ - 1 ] == 1 ) \n    dx , dy = [ dequantize ( d , - max_val , max_val , 255 ) for d in [ dx , dy ] ] \n    if denorm : \n        dx *= dx . shape [ 1 ] \n        dy *= dx . shape [ 0 ] \n    flow = np . dstack ( ( dx , dy ) ) \n    return flow "}
{"2315": "\ndef load_state_dict ( module , state_dict , strict = 0 , logger = None ) : \n    unexpected_keys = [ ] \n    own_state = module . state_dict ( ) \n    for name , param in state_dict . items ( ) : \n        if name not in own_state : \n            unexpected_keys . append ( name ) \n            continue \n        if isinstance ( param , torch . nn . Parameter ) : \n            param = param . data \n        try : \n            own_state [ name ] . copy_ ( param ) \n        except Exception : \n            raise RuntimeError ( 'While copying the parameter named {}, ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {}.' . format ( name , own_state [ name ] . size ( ) , param . size ( ) ) ) \n    missing_keys = set ( own_state . keys ( ) ) - set ( state_dict . keys ( ) ) \n    err_msg = [ ] \n    if unexpected_keys : \n        err_msg . append ( 'unexpected key in source state_dict: {}\\n' . format ( ', ' . join ( unexpected_keys ) ) ) \n    if missing_keys : \n        err_msg . append ( 'missing keys in source state_dict: {}\\n' . format ( ', ' . join ( missing_keys ) ) ) \n    err_msg = '\\n' . join ( err_msg ) \n    if err_msg : \n        if strict : \n            raise RuntimeError ( err_msg ) \n        elif logger is not None : \n            logger . warn ( err_msg ) \n        else : \n            print ( err_msg ) "}
{"2316": "\ndef load_checkpoint ( model , filename , map_location = None , strict = 0 , logger = None ) : \n    if filename . startswith ( 'modelzoo://' ) : \n        import torchvision \n        model_urls = dict ( ) \n        for _ , name , ispkg in pkgutil . walk_packages ( torchvision . models . __path__ ) : \n            if not ispkg : \n                _zoo = import_module ( 'torchvision.models.{}' . format ( name ) ) \n                _urls = getattr ( _zoo , 'model_urls' ) \n                model_urls . update ( _urls ) \n        model_name = filename [ 11 : ] \n        checkpoint = model_zoo . load_url ( model_urls [ model_name ] ) \n    elif filename . startswith ( 'open-mmlab://' ) : \n        model_name = filename [ 13 : ] \n        checkpoint = model_zoo . load_url ( open_mmlab_model_urls [ model_name ] ) \n    elif filename . startswith ( ( 'http://' , 'https://' ) ) : \n        checkpoint = model_zoo . load_url ( filename ) \n    else : \n        if not osp . isfile ( filename ) : \n            raise IOError ( '{} is not a checkpoint file' . format ( filename ) ) \n        checkpoint = torch . load ( filename , map_location = map_location ) \n    if isinstance ( checkpoint , OrderedDict ) : \n        state_dict = checkpoint \n    elif isinstance ( checkpoint , dict ) and 'state_dict' in checkpoint : \n        state_dict = checkpoint [ 'state_dict' ] \n    else : \n        raise RuntimeError ( 'No state_dict found in checkpoint file {}' . format ( filename ) ) \n    if list ( state_dict . keys ( ) ) [ 0 ] . startswith ( 'module.' ) : \n        state_dict = { k [ 7 : ] : v for k , v in checkpoint [ 'state_dict' ] . items ( ) } \n    if hasattr ( model , 'module' ) : \n        load_state_dict ( model . module , state_dict , strict , logger ) \n    else : \n        load_state_dict ( model , state_dict , strict , logger ) \n    return checkpoint "}
{"2322": "\ndef register_hook ( self , hook , priority = 'NORMAL' ) : \n    assert isinstance ( hook , Hook ) \n    if hasattr ( hook , 'priority' ) : \n        raise ValueError ( '\"priority\" is a reserved attribute for hooks' ) \n    priority = get_priority ( priority ) \n    hook . priority = priority \n    inserted = 0 \n    for i in range ( len ( self . _hooks ) - 1 , - 1 , - 1 ) : \n        if priority >= self . _hooks [ i ] . priority : \n            self . _hooks . insert ( i + 1 , hook ) \n            inserted = 1 \n            break \n    if not inserted : \n        self . _hooks . insert ( 0 , hook ) "}
{"2325": "\ndef convert_video ( in_file , out_file , print_cmd = 0 , pre_options = '' , ** kwargs ) : \n    options = [ ] \n    for k , v in kwargs . items ( ) : \n        if isinstance ( v , bool ) : \n            if v : \n                options . append ( '-{}' . format ( k ) ) \n        elif k == 'log_level' : \n            assert v in [ 'quiet' , 'panic' , 'fatal' , 'error' , 'warning' , 'info' , 'verbose' , 'debug' , 'trace' ] \n            options . append ( '-loglevel {}' . format ( v ) ) \n        else : \n            options . append ( '-{} {}' . format ( k , v ) ) \n    cmd = 'ffmpeg -y {} -i {} {} {}' . format ( pre_options , in_file , ' ' . join ( options ) , out_file ) \n    if print_cmd : \n        print ( cmd ) \n    subprocess . call ( cmd , shell = 1 ) "}
{"2326": "\ndef resize_video ( in_file , out_file , size = None , ratio = None , keep_ar = 0 , log_level = 'info' , print_cmd = 0 , ** kwargs ) : \n    if size is None and ratio is None : \n        raise ValueError ( 'expected size or ratio must be specified' ) \n    elif size is not None and ratio is not None : \n        raise ValueError ( 'size and ratio cannot be specified at the same time' ) \n    options = { 'log_level' : log_level } \n    if size : \n        if not keep_ar : \n            options [ 'vf' ] = 'scale={}:{}' . format ( size [ 0 ] , size [ 1 ] ) \n        else : \n            options [ 'vf' ] = ( 'scale=w={}:h={}:force_original_aspect_ratio' '=decrease' . format ( size [ 0 ] , size [ 1 ] ) ) \n    else : \n        if not isinstance ( ratio , tuple ) : \n            ratio = ( ratio , ratio ) \n        options [ 'vf' ] = 'scale=\"trunc(iw*{}):trunc(ih*{})\"' . format ( ratio [ 0 ] , ratio [ 1 ] ) \n    convert_video ( in_file , out_file , print_cmd , ** options ) "}
{"2327": "\ndef cut_video ( in_file , out_file , start = None , end = None , vcodec = None , acodec = None , log_level = 'info' , print_cmd = 0 , ** kwargs ) : \n    options = { 'log_level' : log_level } \n    if vcodec is None : \n        options [ 'vcodec' ] = 'copy' \n    if acodec is None : \n        options [ 'acodec' ] = 'copy' \n    if start : \n        options [ 'ss' ] = start \n    else : \n        start = 0 \n    if end : \n        options [ 't' ] = end - start \n    convert_video ( in_file , out_file , print_cmd , ** options ) "}
{"2328": "\ndef concat_video ( video_list , out_file , vcodec = None , acodec = None , log_level = 'info' , print_cmd = 0 , ** kwargs ) : \n    _ , tmp_filename = tempfile . mkstemp ( suffix = '.txt' , text = 1 ) \n    with open ( tmp_filename , 'w' ) as f : \n        for filename in video_list : \n            f . write ( 'file {}\\n' . format ( osp . abspath ( filename ) ) ) \n    options = { 'log_level' : log_level } \n    if vcodec is None : \n        options [ 'vcodec' ] = 'copy' \n    if acodec is None : \n        options [ 'acodec' ] = 'copy' \n    convert_video ( tmp_filename , out_file , print_cmd , pre_options = '-f concat -safe 0' , ** options ) \n    os . remove ( tmp_filename ) "}
{"2335": "\ndef imwrite ( img , file_path , params = None , auto_mkdir = 1 ) : \n    if auto_mkdir : \n        dir_name = osp . abspath ( osp . dirname ( file_path ) ) \n        mkdir_or_exist ( dir_name ) \n    return cv2 . imwrite ( file_path , img , params ) "}
{"2336": "\ndef bgr2gray ( img , keepdim = 0 ) : \n    out_img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) \n    if keepdim : \n        out_img = out_img [ ... , None ] \n    return out_img "}
{"2339": "\ndef is_seq_of ( seq , expected_type , seq_type = None ) : \n    if seq_type is None : \n        exp_seq_type = collections_abc . Sequence \n    else : \n        assert isinstance ( seq_type , type ) \n        exp_seq_type = seq_type \n    if not isinstance ( seq , exp_seq_type ) : \n        return 0 \n    for item in seq : \n        if not isinstance ( item , expected_type ) : \n            return 0 \n    return 1 "}
{"2342": "\ndef average ( self , n = 0 ) : \n    assert n >= 0 \n    for key in self . val_history : \n        values = np . array ( self . val_history [ key ] [ - n : ] ) \n        nums = np . array ( self . n_history [ key ] [ - n : ] ) \n        avg = np . sum ( values * nums ) / np . sum ( nums ) \n        self . output [ key ] = avg \n    self . ready = 1 "}
{"2343": "\ndef scatter ( input , devices , streams = None ) : \n    if streams is None : \n        streams = [ None ] * len ( devices ) \n    if isinstance ( input , list ) : \n        chunk_size = ( len ( input ) - 1 ) // len ( devices ) + 1 \n        outputs = [ scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) ] \n        return outputs \n    elif isinstance ( input , torch . Tensor ) : \n        output = input . contiguous ( ) \n        stream = streams [ 0 ] if output . numel ( ) > 0 else None \n        with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : \n            output = output . cuda ( devices [ 0 ] , non_blocking = 1 ) \n        return output \n    else : \n        raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) ) "}
{"2346": "\ndef start ( self ) : \n    if not self . _is_running : \n        self . _t_start = time ( ) \n        self . _is_running = 1 \n    self . _t_last = time ( ) "}
{"2352": "\ndef accuracy ( output , target , topk = ( 1 , ) ) : \n    with torch . no_grad ( ) : \n        maxk = max ( topk ) \n        batch_size = target . size ( 0 ) \n        _ , pred = output . topk ( maxk , 1 , 1 , 1 ) \n        pred = pred . t ( ) \n        correct = pred . eq ( target . view ( 1 , - 1 ) . expand_as ( pred ) ) \n        res = [ ] \n        for k in topk : \n            correct_k = correct [ : k ] . view ( - 1 ) . float ( ) . sum ( 0 , keepdim = 1 ) \n            res . append ( correct_k . mul_ ( 100.0 / batch_size ) ) \n        return res "}
{"2359": "\nasync def multiple_request ( self , urls , is_gather = 0 , ** kwargs ) : \n    if is_gather : \n        resp_results = await asyncio . gather ( * [ self . handle_request ( self . request ( url = url , ** kwargs ) ) for url in urls ] , return_exceptions = 1 ) \n        for index , task_result in enumerate ( resp_results ) : \n            if not isinstance ( task_result , RuntimeError ) and task_result : \n                _ , response = task_result \n                response . index = index \n                yield response \n    else : \n        for index , url in enumerate ( urls ) : \n            _ , response = await self . handle_request ( self . request ( url = url , ** kwargs ) ) \n            response . index = index \n            yield response "}
{"2363": "\ndef parse_yaml_linenumbers ( data , filename ) : \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + 1 \n        return node \n    def construct_mapping ( node , deep = 0 ) : \n        if ANSIBLE_VERSION < 2 : \n            mapping = Constructor . construct_mapping ( loader , node , deep = deep ) \n        else : \n            mapping = AnsibleConstructor . construct_mapping ( loader , node , deep = deep ) \n        if hasattr ( node , '__line__' ) : \n            mapping [ LINE_NUMBER_KEY ] = node . __line__ \n        else : \n            mapping [ LINE_NUMBER_KEY ] = mapping . _line_number \n        mapping [ FILENAME_KEY ] = filename \n        return mapping \n    try : \n        if ANSIBLE_VERSION < 2 : \n            loader = yaml . Loader ( data ) \n        else : \n            import inspect \n            kwargs = { } \n            if 'vault_password' in inspect . getargspec ( AnsibleLoader . __init__ ) . args : \n                kwargs [ 'vault_password' ] = DEFAULT_VAULT_PASSWORD \n            loader = AnsibleLoader ( data , ** kwargs ) \n        loader . compose_node = compose_node \n        loader . construct_mapping = construct_mapping \n        data = loader . get_single_data ( ) \n    except ( yaml . parser . ParserError , yaml . scanner . ScannerError ) as e : \n        raise SystemExit ( \"Failed to parse YAML in %s: %s\" % ( filename , str ( e ) ) ) \n    return data "}
{"2367": "\ndef egg2dist ( self , egginfo_path , distinfo_path ) : \n    def adios ( p ) : \n        if os . path . exists ( p ) and not os . path . islink ( p ) and os . path . isdir ( p ) : \n            shutil . rmtree ( p ) \n        elif os . path . exists ( p ) : \n            os . unlink ( p ) \n    adios ( distinfo_path ) \n    if not os . path . exists ( egginfo_path ) : \n        import glob \n        pat = os . path . join ( os . path . dirname ( egginfo_path ) , '*.egg-info' ) \n        possible = glob . glob ( pat ) \n        err = \"Egg metadata expected at %s but not found\" % ( egginfo_path , ) \n        if possible : \n            alt = os . path . basename ( possible [ 0 ] ) \n            err += \" (%s found - possible misnamed archive file?)\" % ( alt , ) \n        raise ValueError ( err ) \n    if os . path . isfile ( egginfo_path ) : \n        pkginfo_path = egginfo_path \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , egginfo_path ) \n        os . mkdir ( distinfo_path ) \n    else : \n        pkginfo_path = os . path . join ( egginfo_path , 'PKG-INFO' ) \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , pkginfo_path ) \n        shutil . copytree ( egginfo_path , distinfo_path , ignore = lambda x , y : set ( ( 'PKG-INFO' , 'requires.txt' , 'SOURCES.txt' , 'not-zip-safe' , ) ) ) \n        dependency_links_path = os . path . join ( distinfo_path , 'dependency_links.txt' ) \n        with open ( dependency_links_path , 'r' ) as dependency_links_file : \n            dependency_links = dependency_links_file . read ( ) . strip ( ) \n        if not dependency_links : \n            adios ( dependency_links_path ) \n    write_pkg_info ( os . path . join ( distinfo_path , 'METADATA' ) , pkg_info ) \n    metadata_path = os . path . join ( distinfo_path , 'METADATA' ) \n    self . add_requirements ( metadata_path ) \n    metadata_json_path = os . path . join ( distinfo_path , 'metadata.json' ) \n    pymeta = pkginfo_to_dict ( metadata_path , distribution = self . distribution ) \n    if 'description' in pymeta : \n        description_filename = 'DESCRIPTION.rst' \n        description_text = pymeta . pop ( 'description' ) \n        description_path = os . path . join ( distinfo_path , description_filename ) \n        with open ( description_path , \"wb\" ) as description_file : \n            description_file . write ( description_text . encode ( 'utf-8' ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'description' ] = description_filename \n    license = self . license_file ( ) \n    if license : \n        license_filename = 'LICENSE.txt' \n        shutil . copy ( license , os . path . join ( self . distinfo_dir , license_filename ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'license' ] = license_filename \n    with open ( metadata_json_path , \"w\" ) as metadata_json : \n        json . dump ( pymeta , metadata_json , sort_keys = 1 ) \n    adios ( egginfo_path ) "}
{"2375": "\nasync def read ( self , keys : List [ str ] ) -> dict : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        if len ( keys ) > 0 : \n            parameters = [ { 'name' : f'@id{i}' , 'value' : f'{self.__sanitize_key(key)}' } for i , key in enumerate ( keys ) ] \n            parameter_sequence = ',' . join ( param . get ( 'name' ) for param in parameters ) \n            query = { \"query\" : f\"SELECT c.id, c.realId, c.document, c._etag \\FROM c WHERE c.id in ({parameter_sequence})\" , \"parameters\" : parameters } \n            options = { 'enableCrossPartitionQuery' : 1 } \n            results = list ( self . client . QueryItems ( self . __container_link , query , options ) ) \n            return { r . get ( 'realId' ) : self . __create_si ( r ) for r in results } \n        else : \n            raise Exception ( 'cosmosdb_storage.read(): \\provide at least one key' ) \n    except TypeError as e : \n        raise e "}
{"2376": "\nasync def write ( self , changes : Dict [ str , StoreItem ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for ( key , change ) in changes . items ( ) : \n            e_tag = change . e_tag \n            doc = { 'id' : self . __sanitize_key ( key ) , 'realId' : key , 'document' : self . __create_dict ( change ) } \n            if ( e_tag == '*' or not e_tag ) : \n                self . client . UpsertItem ( database_or_Container_link = self . __container_link , document = doc , options = { 'disableAutomaticIdGeneration' : 1 } ) \n            elif ( len ( e_tag ) > 0 ) : \n                access_condition = { 'type' : 'IfMatch' , 'condition' : e_tag } \n                self . client . ReplaceItem ( document_link = self . __item_link ( self . __sanitize_key ( key ) ) , new_document = doc , options = { 'accessCondition' : access_condition } ) \n            else : \n                raise Exception ( 'cosmosdb_storage.write(): etag missing' ) \n    except Exception as e : \n        raise e "}
{"2387": "\ndef supports_suggested_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : \n    max_actions = { Channels . facebook : 10 , Channels . skype : 10 , Channels . line : 13 , Channels . kik : 20 , Channels . telegram : 100 , Channels . slack : 100 , Channels . emulator : 100 , Channels . direct_line : 100 , Channels . webchat : 100 , } \n    return button_cnt <= max_actions [ channel_id ] if channel_id in max_actions else 0 "}
{"2388": "\ndef supports_card_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : \n    max_actions = { Channels . facebook : 3 , Channels . skype : 3 , Channels . ms_teams : 3 , Channels . line : 99 , Channels . slack : 100 , Channels . emulator : 100 , Channels . direct_line : 100 , Channels . webchat : 100 , Channels . cortana : 100 , } \n    return button_cnt <= max_actions [ channel_id ] if channel_id in max_actions else 0 "}
{"2390": "\ndef is_token_from_emulator ( auth_header : str ) -> bool : \n    if not auth_header : \n        return 0 \n    parts = auth_header . split ( ' ' ) \n    if len ( parts ) != 2 : \n        return 0 \n    auth_scheme = parts [ 0 ] \n    bearer_token = parts [ 1 ] \n    if auth_scheme != 'Bearer' : \n        return 0 \n    token = jwt . decode ( bearer_token , verify = 0 ) \n    if not token : \n        return 0 \n    issuer = token [ 'iss' ] \n    if not issuer : \n        return 0 \n    issuer_list = EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS . issuer \n    if issuer_list and not issuer in issuer_list : \n        return 0 \n    return 1 "}
{"2413": "\ndef plot_coherence ( xdata , ydata , std_error , fit , fit_function , xunit , exp_str , qubit_label ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'The function plot_coherence needs matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    plt . errorbar ( xdata , ydata , std_error , marker = '.' , markersize = 9 , c = 'b' , linestyle = '' ) \n    plt . plot ( xdata , fit_function ( xdata , * fit ) , c = 'r' , linestyle = '--' , label = ( exp_str + '= %s %s' % ( str ( round ( fit [ 1 ] ) ) , xunit ) ) ) \n    plt . xticks ( fontsize = 14 , rotation = 70 ) \n    plt . yticks ( fontsize = 14 ) \n    plt . xlabel ( 'time [%s]' % ( xunit ) , fontsize = 16 ) \n    plt . ylabel ( 'P(1)' , fontsize = 16 ) \n    plt . title ( exp_str + ' measurement of Q$_{%s}$' % ( str ( qubit_label ) ) , fontsize = 18 ) \n    plt . legend ( fontsize = 12 ) \n    plt . grid ( 1 ) \n    plt . show ( ) "}
{"2415": "\ndef plot_rb_data ( xdata , ydatas , yavg , yerr , fit , survival_prob , ax = None , show_plt = 1 ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'The function plot_rb_data needs matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if ax is None : \n        plt . figure ( ) \n        ax = plt . gca ( ) \n    for ydata in ydatas : \n        ax . plot ( xdata , ydata , color = 'gray' , linestyle = 'none' , marker = 'x' ) \n    ax . errorbar ( xdata , yavg , yerr = yerr , color = 'r' , linestyle = '--' , linewidth = 3 ) \n    ax . plot ( xdata , survival_prob ( xdata , * fit ) , color = 'blue' , linestyle = '-' , linewidth = 2 ) \n    ax . tick_params ( labelsize = 14 ) \n    ax . set_xlabel ( 'Clifford Length' , fontsize = 16 ) \n    ax . set_ylabel ( 'Z' , fontsize = 16 ) \n    ax . grid ( 1 ) \n    if show_plt : \n        plt . show ( ) "}
{"2424": "\ndef _best_subset ( self , n_qubits ) : \n    if n_qubits == 1 : \n        return np . array ( [ 0 ] ) \n    device_qubits = self . coupling_map . size ( ) \n    cmap = np . asarray ( self . coupling_map . get_edges ( ) ) \n    data = np . ones_like ( cmap [ : , 0 ] ) \n    sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , 0 ] , cmap [ : , 1 ] ) ) , shape = ( device_qubits , device_qubits ) ) . tocsr ( ) \n    best = 0 \n    best_map = None \n    for k in range ( sp_cmap . shape [ 0 ] ) : \n        bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = 0 , return_predecessors = 0 ) \n        connection_count = 0 \n        sub_graph = [ ] \n        for i in range ( n_qubits ) : \n            node_idx = bfs [ i ] \n            for j in range ( sp_cmap . indptr [ node_idx ] , sp_cmap . indptr [ node_idx + 1 ] ) : \n                node = sp_cmap . indices [ j ] \n                for counter in range ( n_qubits ) : \n                    if node == bfs [ counter ] : \n                        connection_count += 1 \n                        sub_graph . append ( [ node_idx , node ] ) \n                        break \n        if connection_count > best : \n            best = connection_count \n            best_map = bfs [ 0 : n_qubits ] \n            mapping = { } \n            for edge in range ( best_map . shape [ 0 ] ) : \n                mapping [ best_map [ edge ] ] = edge \n            new_cmap = [ [ mapping [ c [ 0 ] ] , mapping [ c [ 1 ] ] ] for c in sub_graph ] \n            rows = [ edge [ 0 ] for edge in new_cmap ] \n            cols = [ edge [ 1 ] for edge in new_cmap ] \n            data = [ 1 ] * len ( rows ) \n            sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , shape = ( n_qubits , n_qubits ) ) . tocsr ( ) \n            perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) \n            best_map = best_map [ perm ] \n    return best_map "}
{"2429": "\ndef _process_gate ( self , node , opaque = 0 ) : \n    self . gates [ node . name ] = { } \n    de_gate = self . gates [ node . name ] \n    de_gate [ \"print\" ] = 1 \n    de_gate [ \"opaque\" ] = opaque \n    de_gate [ \"n_args\" ] = node . n_args ( ) \n    de_gate [ \"n_bits\" ] = node . n_bits ( ) \n    if node . n_args ( ) > 0 : \n        de_gate [ \"args\" ] = [ element . name for element in node . arguments . children ] \n    else : \n        de_gate [ \"args\" ] = [ ] \n    de_gate [ \"bits\" ] = [ c . name for c in node . bitlist . children ] \n    if opaque : \n        de_gate [ \"body\" ] = None \n    else : \n        de_gate [ \"body\" ] = node . body "}
{"2447": "\ndef __partial_trace_vec ( vec , trace_systems , dimensions , reverse = 1 ) : \n    if reverse : \n        dimensions = dimensions [ : : - 1 ] \n        trace_systems = len ( dimensions ) - 1 - np . array ( trace_systems ) \n    rho = vec . reshape ( dimensions ) \n    rho = np . tensordot ( rho , rho . conj ( ) , axes = ( trace_systems , trace_systems ) ) \n    d = int ( np . sqrt ( np . product ( rho . shape ) ) ) \n    return rho . reshape ( d , d ) "}
{"2453": "\ndef concurrence ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == 1 : \n        rho = outer ( state ) \n    if len ( state ) != 4 : \n        raise Exception ( \"Concurrence is only defined for more than two qubits\" ) \n    YY = np . fliplr ( np . diag ( [ - 1 , 1 , 1 , - 1 ] ) ) \n    A = rho . dot ( YY ) . dot ( rho . conj ( ) ) . dot ( YY ) \n    w = la . eigh ( A , eigvals_only = 1 ) \n    w = np . sqrt ( np . maximum ( w , 0 ) ) \n    return max ( 0.0 , w [ - 1 ] - np . sum ( w [ 0 : - 1 ] ) ) "}
{"2464": "\ndef status ( self ) : \n    return BackendStatus ( backend_name = self . name ( ) , backend_version = __version__ , operational = 1 , pending_jobs = 0 , status_msg = '' ) "}
{"2465": "\ndef start ( self , iterations ) : \n    self . touched = 1 \n    self . iter = int ( iterations ) \n    self . t_start = time . time ( ) "}
{"2470": "\ndef quaternion_from_euler ( angles , order = 'yzy' ) : \n    angles = np . asarray ( angles , dtype = float ) \n    quat = quaternion_from_axis_rotation ( angles [ 0 ] , order [ 0 ] ) * ( quaternion_from_axis_rotation ( angles [ 1 ] , order [ 1 ] ) * quaternion_from_axis_rotation ( angles [ 2 ] , order [ 2 ] ) ) \n    quat . normalize ( inplace = 1 ) \n    return quat "}
{"2471": "\ndef normalize ( self , inplace = 0 ) : \n    if inplace : \n        nrm = self . norm ( ) \n        self . data /= nrm \n        return None \n    nrm = self . norm ( ) \n    data_copy = np . array ( self . data , copy = 1 ) \n    data_copy /= nrm \n    return Quaternion ( data_copy ) "}
{"2478": "\ndef is_square_matrix ( mat ) : \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return 0 \n    shape = mat . shape \n    return shape [ 0 ] == shape [ 1 ] "}
{"2479": "\ndef is_diagonal_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return 0 \n    return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol ) "}
{"2480": "\ndef is_symmetric_matrix ( op , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( op ) \n    if mat . ndim != 2 : \n        return 0 \n    return np . allclose ( mat , mat . T , rtol = rtol , atol = atol ) "}
{"2481": "\ndef is_hermitian_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return 0 \n    return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol ) "}
{"2482": "\ndef is_positive_semidefinite_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    if not is_hermitian_matrix ( mat , rtol = rtol , atol = atol ) : \n        return 0 \n    vals = np . linalg . eigvalsh ( mat ) \n    for v in vals : \n        if v < - atol : \n            return 0 \n    return 1 "}
{"2483": "\ndef is_identity_matrix ( mat , ignore_phase = 0 , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return 0 \n    if ignore_phase : \n        theta = np . angle ( mat [ 0 , 0 ] ) \n        mat = np . exp ( - 1j * theta ) * mat \n    iden = np . eye ( len ( mat ) ) \n    return np . allclose ( mat , iden , rtol = rtol , atol = atol ) "}
{"2484": "\ndef is_unitary_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    mat = np . conj ( mat . T ) . dot ( mat ) \n    return is_identity_matrix ( mat , ignore_phase = 0 , rtol = rtol , atol = atol ) "}
{"2507": "\ndef _hide_tick_lines_and_labels ( axis ) : \n    for item in axis . get_ticklines ( ) + axis . get_ticklabels ( ) : \n        item . set_visible ( 0 ) "}
{"2512": "\ndef render ( self , title = '' ) : \n    if self . _rendered : \n        self . axes . clear ( ) \n    self . _rendered = 1 \n    if not self . _ext_fig : \n        self . fig = plt . figure ( figsize = self . figsize ) \n    if not self . _ext_axes : \n        self . axes = Axes3D ( self . fig , azim = self . view [ 0 ] , elev = self . view [ 1 ] ) \n    if self . background : \n        self . axes . clear ( ) \n        self . axes . set_xlim3d ( - 1.3 , 1.3 ) \n        self . axes . set_ylim3d ( - 1.3 , 1.3 ) \n        self . axes . set_zlim3d ( - 1.3 , 1.3 ) \n    else : \n        self . plot_axes ( ) \n        self . axes . set_axis_off ( ) \n        self . axes . set_xlim3d ( - 0.7 , 0.7 ) \n        self . axes . set_ylim3d ( - 0.7 , 0.7 ) \n        self . axes . set_zlim3d ( - 0.7 , 0.7 ) \n    self . axes . grid ( 0 ) \n    self . plot_back ( ) \n    self . plot_points ( ) \n    self . plot_vectors ( ) \n    self . plot_front ( ) \n    self . plot_axes_labels ( ) \n    self . plot_annotations ( ) \n    self . axes . set_title ( title , fontsize = self . font_size , y = 1.08 ) "}
{"2515": "\ndef two_qubit_kak ( unitary_matrix , verify_gate_sequence = 0 ) : \n    warnings . warn ( \"two_qubit_kak function is now accessible under \" \"qiskit.quantum_info.synthesis\" , DeprecationWarning ) \n    return synthesis . two_qubit_kak ( unitary_matrix ) "}
{"2523": "\ndef compile ( circuits , backend , config = None , basis_gates = None , coupling_map = None , initial_layout = None , shots = 1024 , max_credits = 10 , seed = None , qobj_id = None , seed_mapper = None , pass_manager = None , memory = 0 ) : \n    warnings . warn ( 'qiskit.compile() is deprecated and will be removed in Qiskit Terra 0.9. ' 'Please use qiskit.compiler.transpile() to transform circuits ' 'and qiskit.compiler.assemble() to produce a runnable qobj.' , DeprecationWarning ) \n    new_circuits = transpile ( circuits , basis_gates = basis_gates , coupling_map = coupling_map , initial_layout = initial_layout , seed_transpiler = seed_mapper , backend = backend , pass_manager = pass_manager ) \n    qobj = assemble ( new_circuits , qobj_header = None , shots = shots , max_credits = max_credits , seed_simulator = seed , memory = memory , qobj_id = qobj_id , config = config ) \n    return qobj "}
{"2524": "\ndef _filter_deprecation_warnings ( ) : \n    deprecation_filter = ( 'always' , None , DeprecationWarning , re . compile ( r'^qiskit\\.*' , re . UNICODE ) , 0 ) \n    try : \n        warnings . _add_filter ( * deprecation_filter , append = 0 ) \n    except AttributeError : \n        pass \n    warnings . simplefilter ( 'ignore' , category = ChangedInMarshmallow3Warning ) "}
{"2525": "\ndef local_hardware_info ( ) : \n    results = { 'os' : platform . system ( ) , 'memory' : psutil . virtual_memory ( ) . total / ( 1024 ** 3 ) , 'cpus' : psutil . cpu_count ( logical = 0 ) or 1 } \n    return results "}
{"2526": "\ndef _has_connection ( hostname , port ) : \n    try : \n        host = socket . gethostbyname ( hostname ) \n        socket . create_connection ( ( host , port ) , 2 ) \n        return 1 \n    except Exception : \n        return 0 "}
{"2527": "\ndef _html_checker ( job_var , interval , status , header , _interval_set = 0 ) : \n    job_status = job_var . status ( ) \n    job_status_name = job_status . name \n    job_status_msg = job_status . value \n    status . value = header % ( job_status_msg ) \n    while job_status_name not in [ 'DONE' , 'CANCELLED' ] : \n        time . sleep ( interval ) \n        job_status = job_var . status ( ) \n        job_status_name = job_status . name \n        job_status_msg = job_status . value \n        if job_status_name == 'ERROR' : \n            break \n        else : \n            if job_status_name == 'QUEUED' : \n                job_status_msg += ' (%s)' % job_var . queue_position ( ) \n                if not _interval_set : \n                    interval = max ( job_var . queue_position ( ) , 2 ) \n            else : \n                if not _interval_set : \n                    interval = 2 \n            status . value = header % ( job_status_msg ) \n    status . value = header % ( job_status_msg ) "}
{"2532": "\ndef _fix_gaussian_width ( gaussian_samples , amp : float , center : float , sigma : float , zeroed_width : Union [ None , float ] = None , rescale_amp : bool = 0 , ret_scale_factor : bool = 0 ) -> np . ndarray : \n    if zeroed_width is None : \n        zeroed_width = 2 * ( center + 1 ) \n    zero_offset = gaussian ( np . array ( [ - zeroed_width / 2 ] ) , amp , center , sigma ) \n    gaussian_samples -= zero_offset \n    amp_scale_factor = 1. \n    if rescale_amp : \n        amp_scale_factor = amp / ( amp - zero_offset ) \n        gaussian_samples *= amp_scale_factor \n    if ret_scale_factor : \n        return gaussian_samples , amp_scale_factor \n    return gaussian_samples "}
{"2533": "\ndef gaussian ( times : np . ndarray , amp : complex , center : float , sigma : float , zeroed_width : Union [ None , float ] = None , rescale_amp : bool = 0 , ret_x : bool = 0 ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ] ] : \n    times = np . asarray ( times , dtype = np . complex_ ) \n    x = ( times - center ) / sigma \n    gauss = amp * np . exp ( - x ** 2 / 2 ) . astype ( np . complex_ ) \n    if zeroed_width is not None : \n        gauss = _fix_gaussian_width ( gauss , amp = amp , center = center , sigma = sigma , zeroed_width = zeroed_width , rescale_amp = rescale_amp ) \n    if ret_x : \n        return gauss , x \n    return gauss "}
{"2534": "\ndef gaussian_deriv ( times : np . ndarray , amp : complex , center : float , sigma : float , ret_gaussian : bool = 0 ) -> np . ndarray : \n    gauss , x = gaussian ( times , amp = amp , center = center , sigma = sigma , ret_x = 1 ) \n    gauss_deriv = - x / sigma * gauss \n    if ret_gaussian : \n        return gauss_deriv , gauss \n    return gauss_deriv "}
{"2535": "\ndef gaussian_square ( times : np . ndarray , amp : complex , center : float , width : float , sigma : float , zeroed_width : Union [ None , float ] = None ) -> np . ndarray : \n    square_start = center - width / 2 \n    square_stop = center + width / 2 \n    if zeroed_width : \n        zeroed_width = min ( width , zeroed_width ) \n        gauss_zeroed_width = zeroed_width - width \n    else : \n        gauss_zeroed_width = None \n    funclist = [ functools . partial ( gaussian , amp = amp , center = square_start , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = 1 ) , functools . partial ( gaussian , amp = amp , center = square_stop , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = 1 ) , functools . partial ( constant , amp = amp ) ] \n    condlist = [ times <= square_start , times >= square_stop ] \n    return np . piecewise ( times . astype ( np . complex_ ) , condlist , funclist ) "}
{"2538": "\ndef has_register ( self , register ) : \n    has_reg = 0 \n    if ( isinstance ( register , QuantumRegister ) and register in self . qregs ) : \n        has_reg = 1 \n    elif ( isinstance ( register , ClassicalRegister ) and register in self . cregs ) : \n        has_reg = 1 \n    return has_reg "}
{"2549": "\ndef draw ( self , scale = 0.7 , filename = None , style = None , output = 'text' , interactive = 0 , line_length = None , plot_barriers = 1 , reverse_bits = 0 , justify = None ) : \n    from qiskit . tools import visualization \n    return visualization . circuit_drawer ( self , scale = scale , filename = filename , style = style , output = output , interactive = interactive , line_length = line_length , plot_barriers = plot_barriers , reverse_bits = reverse_bits , justify = justify ) "}
{"2553": "\ndef num_connected_components ( self , unitary_only = 0 ) : \n    reg_offset = 0 \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset += reg . size \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( 1 if instr . control else 0 ) \n        if num_qargs >= 2 and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = 0 \n            if instr . control and not unitary_only : \n                creg = instr . control [ 0 ] \n                creg_int = reg_map [ creg . name ] \n                for coff in range ( creg . size ) : \n                    temp_int = creg_int + coff \n                    for k in range ( num_sub_graphs ) : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            for item in args : \n                reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] \n                for k in range ( num_sub_graphs ) : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            if num_touched > 1 : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                for idx in range ( num_sub_graphs ) : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs -= ( num_touched - 1 ) \n        if num_sub_graphs == 1 : \n            break \n    return num_sub_graphs "}
{"2556": "\ndef pulse_drawer ( samples , duration , dt = None , interp_method = 'None' , filename = None , interactive = 0 , dpi = 150 , nop = 1000 , size = ( 6 , 5 ) ) : \n    try : \n        from matplotlib import pyplot as plt \n    except ImportError : \n        raise ImportError ( 'pulse_drawer need matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if dt : \n        _dt = dt \n    else : \n        _dt = 1 \n    re_y = np . real ( samples ) \n    im_y = np . imag ( samples ) \n    image = plt . figure ( figsize = size ) \n    ax0 = image . add_subplot ( 111 ) \n    if interp_method == 'CubicSpline' : \n        time = np . arange ( 0 , duration + 1 ) * _dt + 0.5 * _dt \n        cs_ry = CubicSpline ( time [ : - 1 ] , re_y ) \n        cs_iy = CubicSpline ( time [ : - 1 ] , im_y ) \n        _time = np . linspace ( 0 , duration * _dt , nop ) \n        _re_y = cs_ry ( _time ) \n        _im_y = cs_iy ( _time ) \n    elif interp_method == 'None' : \n        time = np . arange ( 0 , duration + 1 ) * _dt \n        _time = np . r_ [ time [ 0 ] , np . repeat ( time [ 1 : - 1 ] , 2 ) , time [ - 1 ] ] \n        _re_y = np . repeat ( re_y , 2 ) \n        _im_y = np . repeat ( im_y , 2 ) \n    else : \n        raise QiskitError ( 'Invalid interpolation method \"%s\"' % interp_method ) \n    ax0 . fill_between ( x = _time , y1 = _re_y , y2 = np . zeros_like ( _time ) , facecolor = 'red' , alpha = 0.3 , edgecolor = 'red' , linewidth = 1.5 , label = 'real part' ) \n    ax0 . fill_between ( x = _time , y1 = _im_y , y2 = np . zeros_like ( _time ) , facecolor = 'blue' , alpha = 0.3 , edgecolor = 'blue' , linewidth = 1.5 , label = 'imaginary part' ) \n    ax0 . set_xlim ( 0 , duration * _dt ) \n    ax0 . grid ( b = 1 , linestyle = '-' ) \n    ax0 . legend ( bbox_to_anchor = ( 0.5 , 1.00 ) , loc = 'lower center' , ncol = 2 , frameon = 0 , fontsize = 14 ) \n    if filename : \n        image . savefig ( filename , dpi = dpi , bbox_inches = 'tight' ) \n    plt . close ( image ) \n    if image and interactive : \n        plt . show ( image ) \n    return image "}
{"2569": "\ndef is_connected ( self ) : \n    try : \n        return nx . is_weakly_connected ( self . graph ) \n    except nx . exception . NetworkXException : \n        return 0 "}
{"2570": "\ndef _compute_distance_matrix ( self ) : \n    if not self . is_connected ( ) : \n        raise CouplingError ( \"coupling graph not connected\" ) \n    lengths = nx . all_pairs_shortest_path_length ( self . graph . to_undirected ( as_view = 1 ) ) \n    lengths = dict ( lengths ) \n    size = len ( lengths ) \n    cmap = np . zeros ( ( size , size ) ) \n    for idx in range ( size ) : \n        cmap [ idx , np . fromiter ( lengths [ idx ] . keys ( ) , dtype = int ) ] = np . fromiter ( lengths [ idx ] . values ( ) , dtype = int ) \n    self . _dist_matrix = cmap "}
{"2577": "\ndef subscribe ( self , event , callback ) : \n    if not callable ( callback ) : \n        raise QiskitError ( \"Callback is not a callable!\" ) \n    if event not in self . _subscribers : \n        self . _subscribers [ event ] = [ ] \n    new_subscription = self . _Subscription ( event , callback ) \n    if new_subscription in self . _subscribers [ event ] : \n        return 0 \n    self . _subscribers [ event ] . append ( new_subscription ) \n    return 1 "}
{"2579": "\ndef unsubscribe ( self , event , callback ) : \n    try : \n        self . _subscribers [ event ] . remove ( self . _Subscription ( event , callback ) ) \n    except KeyError : \n        return 0 \n    return 1 "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if number_of_qubits < 5 : \n        temp_set = [ ] \n        if case == 'weight' : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) \n        elif case == 'tensor' : \n            for k in range ( 4 ** number_of_qubits ) : \n                z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                for j in range ( number_of_qubits ) : \n                    element = ( k // ( 4 ** j ) ) % 4 \n                    if element == 1 : \n                        x [ j ] = 1 \n                    elif element == 2 : \n                        z [ j ] = 1 \n                        x [ j ] = 1 \n                    elif element == 3 : \n                        z [ j ] = 1 \n                temp_set . append ( Pauli ( z , x ) ) \n            return temp_set \n        else : \n            raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2596": "\ndef from_label ( cls , label ) : \n    z = np . zeros ( len ( label ) , dtype = np . bool ) \n    x = np . zeros ( len ( label ) , dtype = np . bool ) \n    for i , char in enumerate ( label ) : \n        if char == 'X' : \n            x [ - i - 1 ] = 1 \n        elif char == 'Z' : \n            z [ - i - 1 ] = 1 \n        elif char == 'Y' : \n            z [ - i - 1 ] = 1 \n            x [ - i - 1 ] = 1 \n        elif char != 'I' : \n            raise QiskitError ( \"Pauli string must be only consisted of 'I', 'X', \" \"'Y' or 'Z' but you have {}.\" . format ( char ) ) \n    return cls ( z = z , x = x ) "}
{"2615": "\ndef _validate_measure_sampling ( self , experiment ) : \n    if self . _shots <= 1 : \n        self . _sample_measure = 0 \n        return \n    if hasattr ( experiment . config , 'allows_measure_sampling' ) : \n        self . _sample_measure = experiment . config . allows_measure_sampling \n    else : \n        measure_flag = 0 \n        for instruction in experiment . instructions : \n            if instruction . name == \"reset\" : \n                self . _sample_measure = 0 \n                return \n            if measure_flag : \n                if instruction . name not in [ \"measure\" , \"barrier\" , \"id\" , \"u0\" ] : \n                    self . _sample_measure = 0 \n                    return \n            elif instruction . name == \"measure\" : \n                measure_flag = 1 \n        self . _sample_measure = 1 "}
{"2617": "\ndef _run_job ( self , job_id , qobj ) : \n    self . _validate ( qobj ) \n    result_list = [ ] \n    self . _shots = qobj . config . shots \n    self . _memory = getattr ( qobj . config , 'memory' , 0 ) \n    self . _qobj_config = qobj . config \n    start = time . time ( ) \n    for experiment in qobj . experiments : \n        result_list . append ( self . run_experiment ( experiment ) ) \n    end = time . time ( ) \n    result = { 'backend_name' : self . name ( ) , 'backend_version' : self . _configuration . backend_version , 'qobj_id' : qobj . qobj_id , 'job_id' : job_id , 'results' : result_list , 'status' : 'COMPLETED' , 'success' : 1 , 'time_taken' : ( end - start ) , 'header' : qobj . header . as_dict ( ) } \n    return Result . from_dict ( result ) "}
{"2622": "\ndef _run_job ( self , job_id , qobj ) : \n    self . _validate ( qobj ) \n    result_list = [ ] \n    start = time . time ( ) \n    for experiment in qobj . experiments : \n        result_list . append ( self . run_experiment ( experiment ) ) \n    end = time . time ( ) \n    result = { 'backend_name' : self . name ( ) , 'backend_version' : self . _configuration . backend_version , 'qobj_id' : qobj . qobj_id , 'job_id' : job_id , 'results' : result_list , 'status' : 'COMPLETED' , 'success' : 1 , 'time_taken' : ( end - start ) , 'header' : qobj . header . as_dict ( ) } \n    return Result . from_dict ( result ) "}
{"2624": "\ndef _is_bit ( obj ) : \n    if isinstance ( obj , tuple ) and len ( obj ) == 2 : \n        if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : \n            return 1 \n    return 0 "}
{"2626": "\ndef has_overlap ( self , interval : 'Interval' ) -> bool : \n    if self . begin < interval . end and interval . begin < self . end : \n        return 1 \n    return 0 "}
{"2631": "\ndef is_mergeable_with ( self , timeslots : 'TimeslotCollection' ) -> bool : \n    for slot in timeslots . timeslots : \n        for interval in self . _table [ slot . channel ] : \n            if slot . interval . has_overlap ( interval ) : \n                return 0 \n    return 1 "}
{"2636": "\ndef iplot_state_paulivec ( rho , figsize = None , slider = 0 , show_legend = 0 ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"paulivec_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"paulivec_$divNumber\",                                      \"paulivec\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        figsize = ( 7 , 5 ) \n    options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] , 'slider' : int ( slider ) , 'show_legend' : int ( show_legend ) } \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    data_to_plot = [ ] \n    rho_data = process_data ( rho ) \n    data_to_plot . append ( dict ( data = rho_data ) ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : data_to_plot , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2644": "\ndef run ( self , dag ) : \n    self . _initialize_backend_prop ( ) \n    num_qubits = self . _create_program_graph ( dag ) \n    if num_qubits > len ( self . swap_graph ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = 1 ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = 1 ) : \n        self . pending_program_edges . append ( ( end1 , end2 ) ) \n    while self . pending_program_edges : \n        edge = self . _select_next_edge ( ) \n        q1_mapped = edge [ 0 ] in self . prog2hw \n        q2_mapped = edge [ 1 ] in self . prog2hw \n        if ( not q1_mapped ) and ( not q2_mapped ) : \n            best_hw_edge = self . _select_best_remaining_cx ( ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] \n            self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] \n            self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) \n            self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) \n        elif not q1_mapped : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        else : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) \n            self . prog2hw [ edge [ 1 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        new_edges = [ x for x in self . pending_program_edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] \n        self . pending_program_edges = new_edges \n    for qid in self . qarg_to_id . values ( ) : \n        if qid not in self . prog2hw : \n            self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] \n            self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) \n    layout = Layout ( ) \n    for q in dag . qubits ( ) : \n        pid = self . _qarg_to_id ( q ) \n        hwid = self . prog2hw [ pid ] \n        layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid \n    self . property_set [ 'layout' ] = layout "}
{"2666": "\ndef includes ( self , lo_freq : float ) -> bool : \n    if self . _lb <= lo_freq <= self . _ub : \n        return 1 \n    return 0 "}
{"2684": "\ndef gaussian ( duration : int , amp : complex , sigma : float , name : str = None ) -> SamplePulse : \n    center = duration / 2 \n    zeroed_width = duration + 2 \n    return _sampled_gaussian_pulse ( duration , amp , center , sigma , zeroed_width = zeroed_width , rescale_amp = 1 , name = name ) "}
{"2690": "\ndef rename_register ( self , regname , newname ) : \n    if regname == newname : \n        return \n    if newname in self . qregs or newname in self . cregs : \n        raise DAGCircuitError ( \"duplicate register name %s\" % newname ) \n    if regname not in self . qregs and regname not in self . cregs : \n        raise DAGCircuitError ( \"no register named %s\" % regname ) \n    if regname in self . qregs : \n        reg = self . qregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    if regname in self . cregs : \n        reg = self . cregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    for node in self . _multi_graph . nodes ( ) : \n        if node . type == \"in\" or node . type == \"out\" : \n            if node . name and regname in node . name : \n                node . name = newname \n        elif node . type == \"op\" : \n            qa = [ ] \n            for a in node . qargs : \n                if a [ 0 ] == regname : \n                    a = ( newname , a [ 1 ] ) \n                qa . append ( a ) \n            node . qargs = qa \n            ca = [ ] \n            for a in node . cargs : \n                if a [ 0 ] == regname : \n                    a = ( newname , a [ 1 ] ) \n                ca . append ( a ) \n            node . cargs = ca \n            if node . condition is not None : \n                if node . condition [ 0 ] == regname : \n                    node . condition = ( newname , node . condition [ 1 ] ) \n    for _ , _ , edge_data in self . _multi_graph . edges ( data = 1 ) : \n        if regname in edge_data [ 'name' ] : \n            edge_data [ 'name' ] = re . sub ( regname , newname , edge_data [ 'name' ] ) "}
{"2699": "\ndef _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = 1 ) : \n    add_regs = set ( ) \n    reg_frag_chk = { } \n    for v in keyregs . values ( ) : \n        reg_frag_chk [ v ] = { j : 0 for j in range ( len ( v ) ) } \n    for k in edge_map . keys ( ) : \n        if k [ 0 ] . name in keyregs : \n            reg_frag_chk [ k [ 0 ] ] [ k [ 1 ] ] = 1 \n    for k , v in reg_frag_chk . items ( ) : \n        s = set ( v . values ( ) ) \n        if len ( s ) == 2 : \n            raise DAGCircuitError ( \"edge_map fragments reg %s\" % k ) \n        elif s == set ( [ 0 ] ) : \n            if k in self . qregs . values ( ) or k in self . cregs . values ( ) : \n                raise DAGCircuitError ( \"unmapped duplicate reg %s\" % k ) \n            else : \n                add_regs . add ( k ) \n        else : \n            if valreg : \n                if not edge_map [ ( k , 0 ) ] [ 0 ] . name in valregs : \n                    size = max ( map ( lambda x : x [ 1 ] , filter ( lambda x : x [ 0 ] == edge_map [ ( k , 0 ) ] [ 0 ] , edge_map . values ( ) ) ) ) \n                    qreg = QuantumRegister ( size + 1 , edge_map [ ( k , 0 ) ] [ 0 ] . name ) \n                    add_regs . add ( qreg ) \n    return add_regs "}
{"2705": "\ndef _make_pred_succ_maps ( self , node ) : \n    pred_map = { e [ 2 ] [ 'wire' ] : e [ 0 ] for e in self . _multi_graph . in_edges ( nbunch = node , data = 1 ) } \n    succ_map = { e [ 2 ] [ 'wire' ] : e [ 1 ] for e in self . _multi_graph . out_edges ( nbunch = node , data = 1 ) } \n    return pred_map , succ_map "}
{"2708": "\ndef edges ( self , nodes = None ) : \n    for source_node , dest_node , edge_data in self . _multi_graph . edges ( nodes , data = 1 ) : \n        yield source_node , dest_node , edge_data "}
{"2725": "\ndef collect_runs ( self , namelist ) : \n    group_list = [ ] \n    topo_ops = list ( self . topological_op_nodes ( ) ) \n    nodes_seen = dict ( zip ( topo_ops , [ 0 ] * len ( topo_ops ) ) ) \n    for node in topo_ops : \n        if node . name in namelist and node . condition is None and not nodes_seen [ node ] : \n            group = [ node ] \n            nodes_seen [ node ] = 1 \n            s = list ( self . _multi_graph . successors ( node ) ) \n            while len ( s ) == 1 and s [ 0 ] . type == \"op\" and s [ 0 ] . name in namelist : \n                group . append ( s [ 0 ] ) \n                nodes_seen [ s [ 0 ] ] = 1 \n                s = list ( self . _multi_graph . successors ( s [ 0 ] ) ) \n            if len ( group ) >= 1 : \n                group_list . append ( tuple ( group ) ) \n    return set ( group_list ) "}
{"2726": "\ndef nodes_on_wire ( self , wire , only_ops = 0 ) : \n    current_node = self . input_map . get ( wire , None ) \n    if not current_node : \n        raise DAGCircuitError ( 'The given wire %s is not present in the circuit' % str ( wire ) ) \n    more_nodes = 1 \n    while more_nodes : \n        more_nodes = 0 \n        if current_node . type == 'op' or not only_ops : \n            yield current_node \n        for node , edges in self . _multi_graph . adj [ current_node ] . items ( ) : \n            if any ( wire == edge [ 'wire' ] for edge in edges . values ( ) ) : \n                current_node = node \n                more_nodes = 1 \n                break "}
{"2735": "\ndef marginal_counts ( counts , meas_qubits ) : \n    num_of_qubits = len ( list ( counts . keys ( ) ) [ 0 ] ) \n    qs = sorted ( meas_qubits , reverse = 1 ) \n    meas_keys = count_keys ( len ( qs ) ) \n    rgx = [ reduce ( lambda x , y : ( key [ qs . index ( y ) ] if y in qs else '\\\\d' ) + x , range ( num_of_qubits ) , '' ) for key in meas_keys ] \n    meas_counts = [ ] \n    for m in rgx : \n        c = 0 \n        for key , val in counts . items ( ) : \n            if match ( m , key ) : \n                c += val \n        meas_counts . append ( c ) \n    return dict ( zip ( meas_keys , meas_counts ) ) "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = 0 , quiet = 0 , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if len ( msg ) < msg_len : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif len ( msg ) > msg_len : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2744": "\ndef job_monitor ( job , interval = None , monitor_async = 0 , quiet = 0 , output = sys . stdout ) : \n    if interval is None : \n        _interval_set = 0 \n        interval = 2 \n    else : \n        _interval_set = 1 \n    if _NOTEBOOK_ENV : \n        if monitor_async : \n            try : \n                import ipywidgets as widgets \n            except ImportError : \n                raise ImportError ( 'These functions  need ipywidgets. ' 'Run \"pip install ipywidgets\" before.' ) \n            from qiskit . tools . jupyter . jupyter_magics import _html_checker \n            style = \"font-size:16px;\" \n            header = \"<p style='{style}'>Job Status: %s </p>\" . format ( style = style ) \n            status = widgets . HTML ( value = header % job . status ( ) . value ) \n            display ( status ) \n            thread = threading . Thread ( target = _html_checker , args = ( job , interval , status , header ) ) \n            thread . start ( ) \n        else : \n            _text_checker ( job , interval , _interval_set , quiet = quiet , output = output ) \n    else : \n        if monitor_async : \n            raise QiskitError ( 'monitor_async only available in Jupyter notebooks.' ) \n        _text_checker ( job , interval , _interval_set , quiet = quiet , output = output ) "}
{"2750": "\ndef plot_job_history ( jobs , interval = 'year' ) : \n    def get_date ( job ) : \n        return datetime . datetime . strptime ( job . creation_date ( ) , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    current_time = datetime . datetime . now ( ) \n    if interval == 'year' : \n        bins = [ ( current_time - datetime . timedelta ( days = k * 365 / 12 ) ) for k in range ( 12 ) ] \n    elif interval == 'month' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30 ) ] \n    elif interval == 'week' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7 ) ] \n    binned_jobs = [ 0 ] * len ( bins ) \n    if interval == 'year' : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . month == dat . month : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    else : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . day == dat . day and date . month == dat . month : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    nz_bins = [ ] \n    nz_idx = [ ] \n    for ind , val in enumerate ( binned_jobs ) : \n        if val != 0 : \n            nz_idx . append ( ind ) \n            nz_bins . append ( val ) \n    total_jobs = sum ( binned_jobs ) \n    colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , '#7a5195' , '#ef5675' , '#bc5090' ] \n    if interval == 'year' : \n        labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2 : ] , bins [ b ] . month ) for b in nz_idx ] \n    else : \n        labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] \n    fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 ) ) \n    ax . pie ( nz_bins [ : : - 1 ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14 } , rotatelabels = 1 , counterclock = 0 ) \n    ax . add_artist ( Circle ( ( 0 , 0 ) , 0.7 , color = 'white' , zorder = 1 ) ) \n    ax . text ( 0 , 0 , total_jobs , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = 26 ) \n    fig . tight_layout ( ) \n    return fig "}
{"2756": "\ndef execute ( experiments , backend , basis_gates = None , coupling_map = None , backend_properties = None , initial_layout = None , seed_transpiler = None , optimization_level = None , pass_manager = None , qobj_id = None , qobj_header = None , shots = 1024 , memory = 0 , max_credits = 10 , seed_simulator = None , default_qubit_los = None , default_meas_los = None , schedule_los = None , meas_level = 2 , meas_return = 'avg' , memory_slots = None , memory_slot_size = 100 , rep_time = None , parameter_binds = None , seed = None , seed_mapper = None , config = None , circuits = None , ** run_config ) : \n    if circuits is not None : \n        experiments = circuits \n        warnings . warn ( \"the `circuits` arg in `execute()` has been deprecated. \" \"please use `experiments`, which can handle both circuit \" \"and pulse Schedules\" , DeprecationWarning ) \n    experiments = transpile ( experiments , basis_gates = basis_gates , coupling_map = coupling_map , backend_properties = backend_properties , initial_layout = initial_layout , seed_transpiler = seed_transpiler , optimization_level = optimization_level , backend = backend , pass_manager = pass_manager , seed_mapper = seed_mapper , ) \n    qobj = assemble ( experiments , qobj_id = qobj_id , qobj_header = qobj_header , shots = shots , memory = memory , max_credits = max_credits , seed_simulator = seed_simulator , default_qubit_los = default_qubit_los , default_meas_los = default_meas_los , schedule_los = schedule_los , meas_level = meas_level , meas_return = meas_return , memory_slots = memory_slots , memory_slot_size = memory_slot_size , rep_time = rep_time , parameter_binds = parameter_binds , backend = backend , config = config , seed = seed , run_config = run_config ) \n    return backend . run ( qobj , ** run_config ) "}
{"2762": "\ndef assemble ( experiments , backend = None , qobj_id = None , qobj_header = None , shots = 1024 , memory = 0 , max_credits = None , seed_simulator = None , default_qubit_los = None , default_meas_los = None , schedule_los = None , meas_level = 2 , meas_return = 'avg' , memory_slots = None , memory_slot_size = 100 , rep_time = None , parameter_binds = None , config = None , seed = None , ** run_config ) : \n    if config : \n        warnings . warn ( 'config is not used anymore. Set all configs in ' 'run_config.' , DeprecationWarning ) \n        run_config = run_config or config \n    if seed : \n        warnings . warn ( 'seed is deprecated in favor of seed_simulator.' , DeprecationWarning ) \n        seed_simulator = seed_simulator or seed \n    experiments = experiments if isinstance ( experiments , list ) else [ experiments ] \n    qobj_id , qobj_header , run_config = _parse_run_args ( backend , qobj_id , qobj_header , shots , memory , max_credits , seed_simulator , default_qubit_los , default_meas_los , schedule_los , meas_level , meas_return , memory_slots , memory_slot_size , rep_time , parameter_binds , ** run_config ) \n    if all ( isinstance ( exp , QuantumCircuit ) for exp in experiments ) : \n        bound_experiments , run_config = _expand_parameters ( circuits = experiments , run_config = run_config ) \n        return assemble_circuits ( circuits = bound_experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) \n    elif all ( isinstance ( exp , Schedule ) for exp in experiments ) : \n        return assemble_schedules ( schedules = experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) \n    else : \n        raise QiskitError ( \"bad input to assemble() function; \" \"must be either circuits or schedules\" ) "}
{"2765": "\ndef process_fidelity ( channel1 , channel2 , require_cptp = 1 ) : \n    is_cptp1 = None \n    is_cptp2 = None \n    if isinstance ( channel1 , ( list , np . ndarray ) ) : \n        channel1 = Operator ( channel1 ) \n        if require_cptp : \n            is_cptp1 = channel1 . is_unitary ( ) \n    if isinstance ( channel2 , ( list , np . ndarray ) ) : \n        channel2 = Operator ( channel2 ) \n        if require_cptp : \n            is_cptp2 = channel2 . is_unitary ( ) \n    s1 = SuperOp ( channel1 ) \n    s2 = SuperOp ( channel2 ) \n    if require_cptp : \n        if is_cptp1 is None : \n            is_cptp1 = s1 . is_cptp ( ) \n        if not is_cptp1 : \n            raise QiskitError ( 'channel1 is not CPTP' ) \n        if is_cptp2 is None : \n            is_cptp2 = s2 . is_cptp ( ) \n        if not is_cptp2 : \n            raise QiskitError ( 'channel2 is not CPTP' ) \n    input_dim1 , output_dim1 = s1 . dim \n    input_dim2 , output_dim2 = s2 . dim \n    if input_dim1 != output_dim1 or input_dim2 != output_dim2 : \n        raise QiskitError ( 'Input channels must have same size input and output dimensions.' ) \n    if input_dim1 != input_dim2 : \n        raise QiskitError ( 'Input channels have different dimensions.' ) \n    fidelity = np . trace ( s1 . compose ( s2 . adjoint ( ) ) . data ) / ( input_dim1 ** 2 ) \n    return fidelity "}
{"2789": "\ndef parse_debug ( self , val ) : \n    if val is 1 : \n        self . parse_deb = 1 \n    elif val is 0 : \n        self . parse_deb = 0 \n    else : \n        raise QasmError ( \"Illegal debug value '\" + str ( val ) + \"' must be True or False.\" ) "}
{"2791": "\ndef run ( self , data ) : \n    ast = self . parser . parse ( data , debug = 1 ) \n    self . parser . parse ( data , debug = 1 ) \n    ast . to_string ( 0 ) "}
{"2792": "\ndef parse ( self ) : \n    if self . _filename : \n        with open ( self . _filename ) as ifile : \n            self . _data = ifile . read ( ) \n    with QasmParser ( self . _filename ) as qasm_p : \n        qasm_p . parse_debug ( 0 ) \n        return qasm_p . parse ( self . _data ) "}
{"2795": "\ndef projector ( state , flatten = 0 ) : \n    density_matrix = np . outer ( state . conjugate ( ) , state ) \n    if flatten : \n        return density_matrix . flatten ( order = 'F' ) \n    return density_matrix "}
{"2798": "\ndef backend_widget ( backend ) : \n    config = backend . configuration ( ) . to_dict ( ) \n    props = backend . properties ( ) . to_dict ( ) \n    name = widgets . HTML ( value = \"<h4>{name}</h4>\" . format ( name = backend . name ( ) ) , layout = widgets . Layout ( ) ) \n    n_qubits = config [ 'n_qubits' ] \n    qubit_count = widgets . HTML ( value = \"<h5><b>{qubits}</b></h5>\" . format ( qubits = n_qubits ) , layout = widgets . Layout ( justify_content = 'center' ) ) \n    cmap = widgets . Output ( layout = widgets . Layout ( min_width = '250px' , max_width = '250px' , max_height = '250px' , min_height = '250px' , justify_content = 'center' , align_items = 'center' , margin = '0px 0px 0px 0px' ) ) \n    with cmap : \n        _cmap_fig = plot_gate_map ( backend , plot_directed = 0 , label_qubits = 0 ) \n        if _cmap_fig is not None : \n            display ( _cmap_fig ) \n            plt . close ( _cmap_fig ) \n    pending = generate_jobs_pending_widget ( ) \n    is_oper = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify_content = 'center' ) ) \n    least_busy = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify_content = 'center' ) ) \n    t1_units = props [ 'qubits' ] [ 0 ] [ 0 ] [ 'unit' ] \n    avg_t1 = round ( sum ( [ q [ 0 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) \n    t1_widget = widgets . HTML ( value = \"<h5>{t1} {units}</h5>\" . format ( t1 = avg_t1 , units = t1_units ) , layout = widgets . Layout ( ) ) \n    t2_units = props [ 'qubits' ] [ 0 ] [ 1 ] [ 'unit' ] \n    avg_t2 = round ( sum ( [ q [ 1 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) \n    t2_widget = widgets . HTML ( value = \"<h5>{t2} {units}</h5>\" . format ( t2 = avg_t2 , units = t2_units ) , layout = widgets . Layout ( ) ) \n    out = widgets . VBox ( [ name , cmap , qubit_count , pending , least_busy , is_oper , t1_widget , t2_widget ] , layout = widgets . Layout ( display = 'inline-flex' , flex_flow = 'column' , align_items = 'center' ) ) \n    out . _is_alive = 1 \n    return out "}
{"2799": "\ndef update_backend_info ( self , interval = 60 ) : \n    my_thread = threading . currentThread ( ) \n    current_interval = 0 \n    started = 0 \n    all_dead = 0 \n    stati = [ None ] * len ( self . _backends ) \n    while getattr ( my_thread , \"do_run\" , 1 ) and not all_dead : \n        if current_interval == interval or started is 0 : \n            for ind , back in enumerate ( self . _backends ) : \n                _value = self . children [ ind ] . children [ 2 ] . value \n                _head = _value . split ( '<b>' ) [ 0 ] \n                try : \n                    _status = back . status ( ) \n                    stati [ ind ] = _status \n                except Exception : \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5 style='color:#ff5c49'>\" ) \n                    self . children [ ind ] . _is_alive = 0 \n                else : \n                    self . children [ ind ] . _is_alive = 1 \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5>\" ) \n            idx = list ( range ( len ( self . _backends ) ) ) \n            pending = [ s . pending_jobs for s in stati ] \n            _ , least_idx = zip ( * sorted ( zip ( pending , idx ) ) ) \n            for ind in least_idx : \n                if stati [ ind ] . operational : \n                    least_pending_idx = ind \n                    break \n            for var in idx : \n                if var == least_pending_idx : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n                self . children [ var ] . children [ 3 ] . children [ 1 ] . value = pending [ var ] \n                self . children [ var ] . children [ 3 ] . children [ 1 ] . max = max ( self . children [ var ] . children [ 3 ] . children [ 1 ] . max , pending [ var ] + 10 ) \n                if stati [ var ] . operational : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n            started = 1 \n            current_interval = 0 \n        time . sleep ( 1 ) \n        all_dead = not any ( [ wid . _is_alive for wid in self . children ] ) \n        current_interval += 1 "}
{"2810": "\ndef _get_validator ( name , schema = None , check_schema = 1 , validator_class = None , ** validator_kwargs ) : \n    if schema is None : \n        try : \n            schema = _SCHEMAS [ name ] \n        except KeyError : \n            raise SchemaValidationError ( \"Valid schema name or schema must \" \"be provided.\" ) \n    if name not in _VALIDATORS : \n        if validator_class is None : \n            validator_class = jsonschema . validators . validator_for ( schema ) \n        _VALIDATORS [ name ] = validator_class ( schema , ** validator_kwargs ) \n    validator = _VALIDATORS [ name ] \n    if check_schema : \n        validator . check_schema ( schema ) \n    return validator "}
{"2816": "\ndef _generate_latex_source ( circuit , filename = None , scale = 0.7 , style = None , reverse_bits = 0 , plot_barriers = 1 , justify = None ) : \n    qregs , cregs , ops = utils . _get_layered_instructions ( circuit , reverse_bits = reverse_bits , justify = justify ) \n    qcimg = _latex . QCircuitImage ( qregs , cregs , ops , scale , style = style , plot_barriers = plot_barriers , reverse_bits = reverse_bits ) \n    latex = qcimg . latex ( ) \n    if filename : \n        with open ( filename , 'w' ) as latex_file : \n            latex_file . write ( latex ) \n    return latex "}
{"2817": "\ndef _matplotlib_circuit_drawer ( circuit , scale = 0.7 , filename = None , style = None , plot_barriers = 1 , reverse_bits = 0 , justify = None ) : \n    qregs , cregs , ops = utils . _get_layered_instructions ( circuit , reverse_bits = reverse_bits , justify = justify ) \n    qcd = _matplotlib . MatplotlibDrawer ( qregs , cregs , ops , scale = scale , style = style , plot_barriers = plot_barriers , reverse_bits = reverse_bits ) \n    return qcd . draw ( filename ) "}
{"2825": "\ndef _compose_subsystem ( self , other , qargs , front = 0 ) : \n    input_dims = list ( self . input_dims ( ) ) \n    output_dims = list ( self . output_dims ( ) ) \n    if front : \n        num_indices = len ( self . input_dims ( ) ) \n        shift = 2 * len ( self . output_dims ( ) ) \n        right_mul = 1 \n        for pos , qubit in enumerate ( qargs ) : \n            input_dims [ qubit ] = other . _input_dims [ pos ] \n    else : \n        num_indices = len ( self . output_dims ( ) ) \n        shift = 0 \n        right_mul = 0 \n        for pos , qubit in enumerate ( qargs ) : \n            output_dims [ qubit ] = other . _output_dims [ pos ] \n    tensor = np . reshape ( self . data , self . _shape ) \n    mat = np . reshape ( other . data , other . _shape ) \n    indices = [ 2 * num_indices - 1 - qubit for qubit in qargs ] + [ num_indices - 1 - qubit for qubit in qargs ] \n    final_shape = [ np . product ( output_dims ) ** 2 , np . product ( input_dims ) ** 2 ] \n    data = np . reshape ( self . _einsum_matmul ( tensor , mat , indices , shift , right_mul ) , final_shape ) \n    return SuperOp ( data , input_dims , output_dims ) "}
{"2827": "\ndef run ( self , dag ) : \n    final_op_types = [ 'measure' , 'barrier' ] \n    final_ops = [ ] \n    for candidate_node in dag . named_nodes ( * final_op_types ) : \n        is_final_op = 1 \n        for _ , child_successors in dag . bfs_successors ( candidate_node ) : \n            if any ( suc . type == 'op' and suc . name not in final_op_types for suc in child_successors ) : \n                is_final_op = 0 \n                break \n        if is_final_op : \n            final_ops . append ( candidate_node ) \n    if not final_ops : \n        return dag \n    barrier_layer = DAGCircuit ( ) \n    for qreg in dag . qregs . values ( ) : \n        barrier_layer . add_qreg ( qreg ) \n    for creg in dag . cregs . values ( ) : \n        barrier_layer . add_creg ( creg ) \n    final_qubits = set ( final_op . qargs [ 0 ] for final_op in final_ops ) \n    barrier_layer . apply_operation_back ( Barrier ( len ( final_qubits ) ) , list ( final_qubits ) , [ ] ) \n    ordered_final_nodes = [ node for node in dag . topological_op_nodes ( ) if node in set ( final_ops ) ] \n    for final_node in ordered_final_nodes : \n        barrier_layer . apply_operation_back ( final_node . op , final_node . qargs , final_node . cargs ) \n    for final_op in final_ops : \n        dag . remove_op_node ( final_op ) \n    dag . extend_back ( barrier_layer ) \n    adjacent_pass = MergeAdjacentBarriers ( ) \n    return adjacent_pass . run ( dag ) "}
{"2842": "\ndef _einsum_matmul ( cls , tensor , mat , indices , shift = 0 , right_mul = 0 ) : \n    rank = tensor . ndim \n    rank_mat = mat . ndim \n    if rank_mat % 2 != 0 : \n        raise QiskitError ( \"Contracted matrix must have an even number of indices.\" ) \n    indices_tensor = list ( range ( rank ) ) \n    for j , index in enumerate ( indices ) : \n        indices_tensor [ index + shift ] = rank + j \n    mat_contract = list ( reversed ( range ( rank , rank + len ( indices ) ) ) ) \n    mat_free = [ index + shift for index in reversed ( indices ) ] \n    if right_mul : \n        indices_mat = mat_contract + mat_free \n    else : \n        indices_mat = mat_free + mat_contract \n    return np . einsum ( tensor , indices_tensor , mat , indices_mat ) "}
{"2850": "\ndef is_unitary ( self , atol = None , rtol = None ) : \n    try : \n        op = self . to_operator ( ) \n        return op . is_unitary ( atol = atol , rtol = rtol ) \n    except QiskitError : \n        return 0 "}
{"2856": "\ndef get_locale_map ( self , languages = None , locales = None , region = None , use_given_order = 0 , allow_conflicting_locales = 0 ) : \n    return OrderedDict ( self . _load_data ( languages = languages , locales = locales , region = region , use_given_order = use_given_order , allow_conflicting_locales = allow_conflicting_locales ) ) "}
{"2857": "\ndef get_locales ( self , languages = None , locales = None , region = None , use_given_order = 0 , allow_conflicting_locales = 0 ) : \n    for _ , locale in self . _load_data ( languages = languages , locales = locales , region = region , use_given_order = use_given_order , allow_conflicting_locales = allow_conflicting_locales ) : \n        yield locale "}
{"2858": "\ndef are_tokens_valid ( self , tokens ) : \n    match_relative_regex = self . _get_match_relative_regex_cache ( ) \n    for token in tokens : \n        if any ( [ match_relative_regex . match ( token ) , token in self , token . isdigit ( ) ] ) : \n            continue \n        else : \n            return 0 \n    else : \n        return 1 "}
{"2859": "\ndef split ( self , string , keep_formatting = 0 ) : \n    if not string : \n        return string \n    split_relative_regex = self . _get_split_relative_regex_cache ( ) \n    match_relative_regex = self . _get_match_relative_regex_cache ( ) \n    tokens = split_relative_regex . split ( string ) \n    for i , token in enumerate ( tokens ) : \n        if match_relative_regex . match ( token ) : \n            tokens [ i ] = [ token ] \n            continue \n        tokens [ i ] = self . _split_by_known_words ( token , keep_formatting ) \n    return list ( filter ( bool , chain ( * tokens ) ) ) "}
{"2862": "\ndef is_applicable ( self , date_string , strip_timezone = 0 , settings = None ) : \n    if strip_timezone : \n        date_string , _ = pop_tz_offset_from_string ( date_string , as_offset = 0 ) \n    date_string = self . _translate_numerals ( date_string ) \n    if settings . NORMALIZE : \n        date_string = normalize_unicode ( date_string ) \n    date_string = self . _simplify ( date_string , settings = settings ) \n    dictionary = self . _get_dictionary ( settings ) \n    date_tokens = dictionary . split ( date_string ) \n    return dictionary . are_tokens_valid ( date_tokens ) "}
{"2863": "\ndef translate ( self , date_string , keep_formatting = 0 , settings = None ) : \n    date_string = self . _translate_numerals ( date_string ) \n    if settings . NORMALIZE : \n        date_string = normalize_unicode ( date_string ) \n    date_string = self . _simplify ( date_string , settings = settings ) \n    dictionary = self . _get_dictionary ( settings ) \n    date_string_tokens = dictionary . split ( date_string , keep_formatting ) \n    relative_translations = self . _get_relative_translations ( settings = settings ) \n    for i , word in enumerate ( date_string_tokens ) : \n        word = word . lower ( ) \n        for pattern , replacement in relative_translations . items ( ) : \n            if pattern . match ( word ) : \n                date_string_tokens [ i ] = pattern . sub ( replacement , word ) \n        else : \n            if word in dictionary : \n                date_string_tokens [ i ] = dictionary [ word ] or '' \n    if \"in\" in date_string_tokens : \n        date_string_tokens = self . _clear_future_words ( date_string_tokens ) \n    return self . _join ( list ( filter ( bool , date_string_tokens ) ) , separator = \"\" if keep_formatting else \" \" , settings = settings ) "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n        self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if self . timeout > 120000 : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , 1 ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl' "}
{"2882": "\ndef execute ( self , cmd ) : \n    self . log . info ( \"Executing: %s\" , cmd ) \n    retcode = execute ( cmd , shell = 1 , poll_period = 0.1 , catch_out = self . catch_out ) [ 0 ] \n    if retcode : \n        raise RuntimeError ( \"Subprocess returned %s\" % retcode ) \n    return retcode "}
{"2888": "\ndef _feed ( self ) : \n    self . plan = StpdReader ( self . stpd_filename ) \n    if self . cached_stpd : \n        self . plan = list ( self . plan ) \n    for task in self . plan : \n        if self . quit . is_set ( ) : \n            logger . info ( \"Stop feeding: gonna quit\" ) \n            return \n        while 1 : \n            try : \n                self . task_queue . put ( task , timeout = 1 ) \n                break \n            except Full : \n                if self . quit . is_set ( ) or self . workers_finished : \n                    return \n                else : \n                    continue \n    workers_count = self . instances \n    logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers_count ) ) \n    retry_delay = 1 \n    for _ in range ( 5 ) : \n        try : \n            [ self . task_queue . put ( None , timeout = 1 ) for _ in xrange ( 0 , workers_count ) ] \n            break \n        except Full : \n            logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry_delay ) \n            time . sleep ( retry_delay ) \n            retry_delay *= 2 \n    try : \n        logger . info ( \"Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        logger . info ( \"All workers exited.\" ) \n        self . workers_finished = 1 \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . task_queue . close ( ) \n        self . results . close ( ) \n        self . quit . set ( ) \n        logger . info ( \"Going to quit. Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        self . workers_finished = 1 "}
{"2889": "\ndef init_logging ( self , log_filename = \"tank.log\" ) : \n    logger = logging . getLogger ( '' ) \n    self . log_filename = log_filename \n    self . core . add_artifact_file ( self . log_filename ) \n    file_handler = logging . FileHandler ( self . log_filename ) \n    file_handler . setLevel ( logging . DEBUG ) \n    file_handler . setFormatter ( logging . Formatter ( \"%(asctime)s [%(levelname)s] %(name)s %(message)s\" ) ) \n    logger . addHandler ( file_handler ) \n    console_handler = logging . StreamHandler ( sys . stdout ) \n    stderr_hdl = logging . StreamHandler ( sys . stderr ) \n    fmt_regular = logging . Formatter ( \"%(asctime)s %(levelname)s: %(message)s\" , \"%H:%M:%S\" ) \n    console_handler . setLevel ( logging . INFO ) \n    console_handler . setFormatter ( fmt_regular ) \n    stderr_hdl . setFormatter ( fmt_regular ) \n    f_err = SingleLevelFilter ( logging . ERROR , 1 ) \n    f_warn = SingleLevelFilter ( logging . WARNING , 1 ) \n    f_crit = SingleLevelFilter ( logging . CRITICAL , 1 ) \n    console_handler . addFilter ( f_err ) \n    console_handler . addFilter ( f_warn ) \n    console_handler . addFilter ( f_crit ) \n    logger . addHandler ( console_handler ) \n    f_info = SingleLevelFilter ( logging . INFO , 1 ) \n    f_debug = SingleLevelFilter ( logging . DEBUG , 1 ) \n    stderr_hdl . addFilter ( f_info ) \n    stderr_hdl . addFilter ( f_debug ) \n    logger . addHandler ( stderr_hdl ) "}
{"2891": "\ndef configure ( self , options ) : \n    self . options = options \n    if self . options . get ( 'lock_dir' , None ) : \n        self . core . set_option ( self . core . SECTION , \"lock_dir\" , self . options [ 'lock_dir' ] ) \n    if self . options . get ( 'ignore_lock' , None ) : \n        self . core . set_option ( self . core . SECTION , 'ignore_lock' , self . options [ 'ignore_lock' ] ) \n    while 1 : \n        try : \n            self . core . get_lock ( ) \n            break \n        except Exception as exc : \n            if self . options . get ( 'lock_fail' , None ) : \n                raise RuntimeError ( \"Lock file present, cannot continue\" ) \n            self . log . info ( \"Couldn't get lock. Will retry in 5 seconds... (%s)\" , str ( exc ) ) \n            time . sleep ( 5 ) \n    configs = self . get_default_configs ( ) \n    if self . options . get ( 'config' , None ) : \n        configs . append ( self . options [ 'config' ] ) \n    self . core . load_configs ( configs ) \n    self . __add_user_options ( ) \n    self . core . load_plugins ( ) \n    if self . options . get ( 'ignore_lock' , None ) : \n        self . core . set_option ( self . core . SECTION , self . IGNORE_LOCKS , \"1\" ) "}
{"2893": "\ndef _collect_data ( self , end = 0 ) : \n    data = get_nowait_from_queue ( self . results ) \n    stats = get_nowait_from_queue ( self . stats_results ) \n    logger . debug ( \"Data timestamps: %s\" % [ d . get ( 'ts' ) for d in data ] ) \n    logger . debug ( \"Stats timestamps: %s\" % [ d . get ( 'ts' ) for d in stats ] ) \n    for item in data : \n        ts = item [ 'ts' ] \n        if ts in self . stat_cache : \n            data_item = item \n            stat_item = self . stat_cache . pop ( ts ) \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . data_cache [ ts ] = item \n    for item in stats : \n        ts = item [ 'ts' ] \n        if ts in self . data_cache : \n            data_item = self . data_cache . pop ( ts ) \n            stat_item = item \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . stat_cache [ ts ] = item \n    if end and len ( self . data_cache ) > 0 : \n        logger . info ( 'Timestamps without stats:' ) \n        for ts , data_item in sorted ( self . data_cache . items ( ) , key = lambda i : i [ 0 ] ) : \n            logger . info ( ts ) \n            self . __notify_listeners ( data_item , StatsReader . stats_item ( ts , 0 , 0 ) ) "}
{"2895": "\ndef get_marker ( marker_type , enum_ammo = 0 ) : \n    try : \n        limit = int ( marker_type ) \n        if limit : \n            marker = __UriMarker ( limit ) \n        else : \n            def marker ( m ) : \n                return '' \n    except ValueError : \n        if marker_type in __markers : \n            marker = __markers [ marker_type ] \n        else : \n            raise NotImplementedError ( 'No such marker: \"%s\"' % marker_type ) \n    if enum_ammo : \n        marker = __Enumerator ( marker ) \n    return marker "}
{"2897": "\ndef start ( self ) : \n    logger . info ( 'Starting agent on localhost' ) \n    args = self . python . split ( ) + [ os . path . join ( self . workdir , self . AGENT_FILENAME ) , '--telegraf' , self . path [ 'TELEGRAF_LOCAL_PATH' ] , '--host' , self . host ] \n    if self . kill_old : \n        args . append ( self . kill_old ) \n    self . session = self . popen ( args ) \n    self . reader_thread = threading . Thread ( target = self . read_buffer ) \n    self . reader_thread . setDaemon ( 1 ) \n    return self . session "}
{"2898": "\ndef start ( self ) : \n    logger . info ( 'Starting agent: %s' , self . host ) \n    command = \"{python} {agent_path} --telegraf {telegraf_path} --host {host} {kill_old}\" . format ( python = self . python , agent_path = os . path . join ( self . path [ 'AGENT_REMOTE_FOLDER' ] , self . AGENT_FILENAME ) , telegraf_path = self . path [ 'TELEGRAF_REMOTE_PATH' ] , host = self . host , kill_old = self . kill_old ) \n    logger . debug ( 'Command to start agent: %s' , command ) \n    self . session = self . ssh . async_session ( command ) \n    self . reader_thread = threading . Thread ( target = self . read_buffer ) \n    self . reader_thread . setDaemon ( 1 ) \n    return self . session "}
{"2905": "\ndef create_startup_config ( self ) : \n    cfg_path = \"agent_startup_{}.cfg\" . format ( self . host ) \n    if os . path . isfile ( cfg_path ) : \n        logger . info ( 'Found agent startup config file in working directory with the same name as created for host %s.\\n' 'Creating new one via tempfile. This will affect predictable filenames for agent artefacts' , self . host ) \n        handle , cfg_path = tempfile . mkstemp ( '.cfg' , 'agent_' ) \n        os . close ( handle ) \n    try : \n        config = ConfigParser . RawConfigParser ( ) \n        config . add_section ( 'startup' ) \n        [ config . set ( 'startup' , \"cmd%s\" % idx , cmd ) for idx , cmd in enumerate ( self . startups ) ] \n        config . add_section ( 'shutdown' ) \n        [ config . set ( 'shutdown' , \"cmd%s\" % idx , cmd ) for idx , cmd in enumerate ( self . shutdowns ) ] \n        config . add_section ( 'source' ) \n        [ config . set ( 'source' , \"file%s\" % idx , path ) for idx , path in enumerate ( self . sources ) ] \n        with open ( cfg_path , 'w' ) as fds : \n            config . write ( fds ) \n    except Exception as exc : \n        logger . error ( 'Error trying to create monitoring startups config. Malformed? %s' , exc , exc_info = 1 ) \n    return cfg_path "}
{"2906": "\ndef __check_disk ( self ) : \n    cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" \n    cmd += self . core . artifacts_base_dir \n    cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" \n    res = execute ( cmd , 1 , 0.1 , 1 ) \n    logging . debug ( \"Result: %s\" , res ) \n    if not len ( res [ 1 ] ) : \n        self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) \n        return \n    disk_free = res [ 1 ] \n    self . log . debug ( \"Disk free space: %s/%s\" , disk_free . strip ( ) , self . disk_limit ) \n    if int ( disk_free . strip ( ) ) < self . disk_limit : \n        raise RuntimeError ( \"Not enough local resources: disk space less than %sMB in %s: %sMB\" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) ) "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ 1 if i == ' ' else 0 for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( 1 ) \n            elif is_space ( chunk ) : \n                result . append ( 1 ) \n            else : \n                result . append ( 0 ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result += chunk \n        else : \n            if left > 0 : \n                if len ( chunk ) <= left : \n                    result += chunk \n                    left -= len ( chunk ) \n                else : \n                    leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] \n                    was_cut = not is_empty ( leftover , markups ) \n                    if was_cut : \n                        result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' \n                    else : \n                        result += chunk [ : left ] \n                    left = 0 \n    return result "}
{"2919": "\ndef __make_writer_request ( self , params = None , json = None , http_method = \"POST\" , trace = 0 ) : \n    request = requests . Request ( http_method , self . writer_url , params = params , json = json , headers = { 'User-Agent' : self . user_agent } ) \n    ids = id_gen ( str ( uuid . uuid4 ( ) ) ) \n    network_timeouts = self . network_timeouts ( ) \n    maintenance_timeouts = self . maintenance_timeouts ( ) \n    while 1 : \n        try : \n            response = self . __send_single_request ( request , ids . next ( ) , trace = trace ) \n            return response \n        except ( Timeout , ConnectionError , ProtocolError ) : \n            logger . warn ( traceback . format_exc ( ) ) \n            try : \n                timeout = next ( network_timeouts ) \n                logger . warn ( \"Network error, will retry in %ss...\" % timeout ) \n                time . sleep ( timeout ) \n                continue \n            except StopIteration : \n                raise self . NetworkError ( ) \n        except self . UnderMaintenance as e : \n            try : \n                timeout = next ( maintenance_timeouts ) \n                logger . warn ( \"Writer is under maintenance, will retry in %ss...\" % timeout ) \n                time . sleep ( timeout ) \n                continue \n            except StopIteration : \n                raise e "}
{"2920": "\ndef load_plugins ( self ) : \n    logger . info ( \"Loading plugins...\" ) \n    for ( plugin_name , plugin_path , plugin_cfg ) in self . config . plugins : \n        logger . debug ( \"Loading plugin %s from %s\" , plugin_name , plugin_path ) \n        if plugin_path == \"yandextank.plugins.Overload\" : \n            logger . warning ( \"Deprecated plugin name: 'yandextank.plugins.Overload'\\n\" \"There is a new generic plugin now.\\n\" \"Correcting to 'yandextank.plugins.DataUploader overload'\" ) \n            plugin_path = \"yandextank.plugins.DataUploader overload\" \n        try : \n            plugin = il . import_module ( plugin_path ) \n        except ImportError : \n            logger . warning ( 'Plugin name %s path %s import error' , plugin_name , plugin_path ) \n            logger . debug ( 'Plugin name %s path %s import error' , plugin_name , plugin_path , exc_info = 1 ) \n            raise \n        try : \n            instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin_cfg , name = plugin_name ) \n        except AttributeError : \n            logger . warning ( 'Plugin %s classname should be `Plugin`' , plugin_name ) \n            raise \n        else : \n            self . register_plugin ( self . PLUGIN_PREFIX + plugin_name , instance ) \n    logger . debug ( \"Plugin instances: %s\" , self . _plugins ) "}
{"2923": "\ndef __collect_file ( self , filename , keep_original = 0 ) : \n    dest = self . artifacts_dir + '/' + os . path . basename ( filename ) \n    logger . debug ( \"Collecting file: %s to %s\" , filename , dest ) \n    if not filename or not os . path . exists ( filename ) : \n        logger . warning ( \"File not found to collect: %s\" , filename ) \n        return \n    if os . path . exists ( dest ) : \n        logger . warning ( \"File already exists: %s\" , dest ) \n        return \n    if keep_original : \n        shutil . copy ( filename , self . artifacts_dir ) \n    else : \n        shutil . move ( filename , self . artifacts_dir ) \n    os . chmod ( dest , 0o644 ) "}
{"2924": "\ndef add_artifact_file ( self , filename , keep_original = 0 ) : \n    if filename : \n        logger . debug ( \"Adding artifact file to collect (keep=%s): %s\" , keep_original , filename ) \n        self . artifact_files [ filename ] = keep_original "}
{"2933": "\ndef poll ( self ) : \n    start_time = time . time ( ) \n    for agent in self . agents : \n        for collect in agent . reader : \n            if not collect : \n                return 0 \n            for chunk in collect : \n                ts , prepared_results = chunk \n                if self . load_start_time and int ( ts ) >= self . load_start_time : \n                    ready_to_send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash_hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared_results } } } \n                    self . __collected_data . append ( ready_to_send ) \n    logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000 ) \n    collected_data_length = len ( self . __collected_data ) \n    if not self . first_data_received and self . __collected_data : \n        self . first_data_received = 1 \n        logger . info ( \"Monitoring received first data.\" ) \n    else : \n        self . send_collected_data ( ) \n    return collected_data_length "}
{"2937": "\ndef _decode_agents_data ( self , block ) : \n    collect = [ ] \n    if block : \n        for chunk in block . split ( '\\n' ) : \n            try : \n                if chunk : \n                    prepared_results = { } \n                    jsn = json . loads ( chunk ) \n                    for ts , values in jsn . iteritems ( ) : \n                        for key , value in values . iteritems ( ) : \n                            try : \n                                key_group , key_name = key . split ( '_' ) [ 0 ] . split ( '-' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) \n                            except : \n                                key_group , key_name = key . split ( '_' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) \n                            if key_group in decoder . diff_metrics . keys ( ) : \n                                if key_name in decoder . diff_metrics [ key_group ] : \n                                    decoded_key = decoder . find_common_names ( key ) \n                                    if self . prev_check : \n                                        try : \n                                            value = jsn [ ts ] [ key ] - self . prev_check [ key ] \n                                        except KeyError : \n                                            logger . debug ( 'There is no diff value for metric %s.\\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc_info = 1 ) \n                                            value = 0 \n                                        prepared_results [ decoded_key ] = value \n                                else : \n                                    decoded_key = decoder . find_common_names ( key ) \n                                    prepared_results [ decoded_key ] = value \n                            else : \n                                decoded_key = decoder . find_common_names ( key ) \n                                prepared_results [ decoded_key ] = value \n                        self . prev_check = jsn [ ts ] \n                        collect . append ( ( ts , prepared_results ) ) \n            except ValueError : \n                logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) \n                logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc_info = 1 ) \n                return [ ] \n            except BaseException : \n                logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc_info = 1 ) \n                return [ ] \n        if collect : \n            return collect "}
{"2957": "\ndef get_next_event ( process , queue ) : \n    while 1 : \n        try : \n            return queue . get ( block = 1 , timeout = TICK ) \n        except multiprocessing . queues . Empty : \n            if not process . is_alive ( ) : \n                try : \n                    return queue . get ( block = 0 ) \n                except multiprocessing . queues . Empty : \n                    return PROCESS_DEAD_AND_QUEUE_EMPTY \n    check . failed ( 'unreachable' ) "}
{"2958": "\ndef execute_pipeline_through_queue ( repository_info , pipeline_name , solid_subset , environment_dict , run_id , message_queue , reexecution_config , step_keys_to_execute , ) : \n    message_queue . put ( ProcessStartedSentinel ( os . getpid ( ) ) ) \n    run_config = RunConfig ( run_id , event_callback = message_queue . put , executor_config = InProcessExecutorConfig ( raise_on_error = 0 ) , reexecution_config = reexecution_config , step_keys_to_execute = step_keys_to_execute , ) \n    repository_container = RepositoryContainer ( repository_info ) \n    if repository_container . repo_error : \n        message_queue . put ( MultiprocessingError ( serializable_error_info_from_exc_info ( repository_container . repo_error ) ) ) \n        return \n    try : \n        result = execute_pipeline ( repository_container . repository . get_pipeline ( pipeline_name ) . build_sub_pipeline ( solid_subset ) , environment_dict , run_config = run_config , ) \n        return result \n    except : \n        error_info = serializable_error_info_from_exc_info ( sys . exc_info ( ) ) \n        message_queue . put ( MultiprocessingError ( error_info ) ) \n    finally : \n        message_queue . put ( MultiprocessingDone ( ) ) \n        message_queue . close ( ) "}
{"2959": "\ndef join ( self ) : \n    while 1 : \n        with self . _processes_lock : \n            if not self . _processes and self . _processing_semaphore . locked ( ) : \n                return 1 \n        gevent . sleep ( 0.1 ) "}
{"2960": "\ndef Field ( dagster_type , default_value = FIELD_NO_DEFAULT_PROVIDED , is_optional = INFER_OPTIONAL_COMPOSITE_FIELD , is_secret = 0 , description = None , ) : \n    config_type = resolve_to_config_type ( dagster_type ) \n    if not config_type : \n        raise DagsterInvalidDefinitionError ( ( 'Attempted to pass {value_repr} to a Field that expects a valid ' 'dagster type usable in config (e.g. Dict, NamedDict, Int, String et al).' ) . format ( value_repr = repr ( dagster_type ) ) ) \n    return FieldImpl ( config_type = resolve_to_config_type ( dagster_type ) , default_value = default_value , is_optional = is_optional , is_secret = is_secret , description = description , ) "}
{"2965": "\ndef construct_publish_comands ( additional_steps = None , nightly = 0 ) : \n    publish_commands = ( [ 'rm -rf dist' ] + ( additional_steps if additional_steps else [ ] ) + [ 'python setup.py sdist bdist_wheel{nightly}' . format ( nightly = ' --nightly' if nightly else '' ) , 'twine upload dist/*' , ] ) \n    return publish_commands "}
{"2970": "\ndef block ( self , text , prefix = '' ) : \n    wrapper = TextWrapper ( width = self . line_length - len ( self . current_indent_str ) , initial_indent = prefix , subsequent_indent = prefix , break_long_words = 0 , break_on_hyphens = 0 , ) \n    for line in wrapper . wrap ( text ) : \n        self . line ( line ) "}
{"2975": "\ndef user_code_context_manager ( user_fn , error_cls , msg ) : \n    check . callable_param ( user_fn , 'user_fn' ) \n    check . subclass_param ( error_cls , 'error_cls' , DagsterUserCodeExecutionError ) \n    with user_code_error_boundary ( error_cls , msg ) : \n        thing_or_gen = user_fn ( ) \n        gen = _ensure_gen ( thing_or_gen ) \n        try : \n            thing = next ( gen ) \n        except StopIteration : \n            check . failed ( 'Must yield one item. You did not yield anything.' ) \n        yield thing \n        stopped = 0 \n        try : \n            next ( gen ) \n        except StopIteration : \n            stopped = 1 \n        check . invariant ( stopped , 'Must yield one item. Yielded more than one item' ) "}
{"2977": "\ndef success ( self ) : \n    any_success = 0 \n    for step_event in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) : \n        if step_event . event_type == DagsterEventType . STEP_FAILURE : \n            return 0 \n        if step_event . event_type == DagsterEventType . STEP_SUCCESS : \n            any_success = 1 \n    return any_success "}
{"2982": "\ndef PermissiveDict ( fields = None ) : \n    if fields : \n        check_user_facing_fields_dict ( fields , 'PermissiveDict' ) \n    class _PermissiveDict ( _ConfigComposite ) : \n        def __init__ ( self ) : \n            key = 'PermissiveDict.' + str ( DictCounter . get_next_count ( ) ) \n            super ( _PermissiveDict , self ) . __init__ ( name = None , key = key , fields = fields or dict ( ) , description = 'A configuration dictionary with typed fields' , type_attributes = ConfigTypeAttributes ( is_builtin = 1 ) , ) \n        \n        @ property \n        def is_permissive_composite ( self ) : \n            return 1 \n    return _PermissiveDict "}
{"2990": "\ndef get_connection_params ( self ) : \n    valid_settings = { 'NAME' : 'name' , 'HOST' : 'host' , 'PORT' : 'port' , 'USER' : 'username' , 'PASSWORD' : 'password' , 'AUTH_SOURCE' : 'authSource' , 'AUTH_MECHANISM' : 'authMechanism' , 'ENFORCE_SCHEMA' : 'enforce_schema' , 'REPLICASET' : 'replicaset' , 'SSL' : 'ssl' , 'SSL_CERTFILE' : 'ssl_certfile' , 'SSL_CA_CERTS' : 'ssl_ca_certs' , 'READ_PREFERENCE' : 'read_preference' } \n    connection_params = { 'name' : 'djongo_test' , 'enforce_schema' : 1 } \n    for setting_name , kwarg in valid_settings . items ( ) : \n        try : \n            setting = self . settings_dict [ setting_name ] \n        except KeyError : \n            continue \n        if setting or setting is 0 : \n            connection_params [ kwarg ] = setting \n    return connection_params "}
{"3023": "\nasync def close ( self ) : \n    async with self . _lock : \n        for t in self . hashtables : \n            await t . close ( ) \n        if self . keys is not None : \n            await self . keys . close ( ) \n        self . _initialized = 0 "}
{"3028": "\ndef select_text ( text , reading = 0 , prefer = None ) : \n    if reading : \n        text = text [ 1 ] \n    else : \n        text = text [ 0 ] \n    if not isinstance ( text , strtype ) : \n        common = set ( text ) & set ( prefer or set ( ) ) \n        if len ( common ) == 1 : \n            text = common . pop ( ) \n        else : \n            text = text [ 0 ] \n    return text "}
{"3030": "\ndef parse_statement ( self ) : \n    self . _skip_whitespace_and_comments ( ) \n    if self . _current_token . kind == tokenize . ENDMARKER : \n        return None \n    stmt_loc = self . _current_location ( ignore_char_num = 1 ) \n    binding_key_or_keyword = self . _parse_selector ( ) \n    statement = None \n    if self . _current_token . value != '=' : \n        if binding_key_or_keyword == 'import' : \n            module = self . _parse_selector ( scoped = 0 ) \n            statement = ImportStatement ( module , stmt_loc ) \n        elif binding_key_or_keyword == 'include' : \n            str_loc = self . _current_location ( ) \n            success , filename = self . _maybe_parse_basic_type ( ) \n            if not success or not isinstance ( filename , str ) : \n                self . _raise_syntax_error ( 'Expected file path as string.' , str_loc ) \n            statement = IncludeStatement ( filename , stmt_loc ) \n        else : \n            self . _raise_syntax_error ( \"Expected '='.\" ) \n    else : \n        self . _advance_one_token ( ) \n        value = self . parse_value ( ) \n        scope , selector , arg_name = parse_binding_key ( binding_key_or_keyword ) \n        statement = BindingStatement ( scope , selector , arg_name , value , stmt_loc ) \n    assert statement , 'Internal parsing error.' \n    if ( self . _current_token . kind != tokenize . NEWLINE and self . _current_token . kind != tokenize . ENDMARKER ) : \n        self . _raise_syntax_error ( 'Expected newline.' ) \n    elif self . _current_token . kind == tokenize . NEWLINE : \n        self . _advance_one_token ( ) \n    return statement "}
{"3033": "\ndef _maybe_parse_configurable_reference ( self ) : \n    if self . _current_token . value != '@' : \n        return 0 , None \n    location = self . _current_location ( ) \n    self . _advance_one_token ( ) \n    scoped_name = self . _parse_selector ( allow_periods_in_scope = 1 ) \n    evaluate = 0 \n    if self . _current_token . value == '(' : \n        evaluate = 1 \n        self . _advance ( ) \n        if self . _current_token . value != ')' : \n            self . _raise_syntax_error ( \"Expected ')'.\" ) \n        self . _advance_one_token ( ) \n    self . _skip_whitespace_and_comments ( ) \n    with utils . try_with_location ( location ) : \n        reference = self . _delegate . configurable_reference ( scoped_name , evaluate ) \n    return 1 , reference "}
{"3038": "\ndef _decorate_fn_or_cls ( decorator , fn_or_cls , subclass = 0 ) : \n    if not inspect . isclass ( fn_or_cls ) : \n        return decorator ( _ensure_wrappability ( fn_or_cls ) ) \n    construction_fn = _find_class_construction_fn ( fn_or_cls ) \n    if subclass : \n        class DecoratedClass ( fn_or_cls ) : \n            __doc__ = fn_or_cls . __doc__ \n            __module__ = fn_or_cls . __module__ \n        DecoratedClass . __name__ = fn_or_cls . __name__ \n        if six . PY3 : \n            DecoratedClass . __qualname__ = fn_or_cls . __qualname__ \n        cls = DecoratedClass \n    else : \n        cls = fn_or_cls \n    decorated_fn = decorator ( _ensure_wrappability ( construction_fn ) ) \n    if construction_fn . __name__ == '__new__' : \n        decorated_fn = staticmethod ( decorated_fn ) \n    setattr ( cls , construction_fn . __name__ , decorated_fn ) \n    return cls "}
{"3040": "\ndef clear_config ( clear_constants = 0 ) : \n    _set_config_is_locked ( 0 ) \n    _CONFIG . clear ( ) \n    _SINGLETONS . clear ( ) \n    if clear_constants : \n        _CONSTANTS . clear ( ) \n    else : \n        saved_constants = _CONSTANTS . copy ( ) \n        _CONSTANTS . clear ( ) \n        for name , value in six . iteritems ( saved_constants ) : \n            constant ( name , value ) \n    _IMPORTED_MODULES . clear ( ) \n    _OPERATIVE_CONFIG . clear ( ) "}
{"3043": "\ndef _might_have_parameter ( fn_or_cls , arg_name ) : \n    if inspect . isclass ( fn_or_cls ) : \n        fn = _find_class_construction_fn ( fn_or_cls ) \n    else : \n        fn = fn_or_cls \n    while hasattr ( fn , '__wrapped__' ) : \n        fn = fn . __wrapped__ \n    arg_spec = _get_cached_arg_spec ( fn ) \n    if six . PY3 : \n        if arg_spec . varkw : \n            return 1 \n        return arg_name in arg_spec . args or arg_name in arg_spec . kwonlyargs \n    else : \n        if arg_spec . keywords : \n            return 1 \n        return arg_name in arg_spec . args "}
{"3048": "\ndef config_scope ( name_or_scope ) : \n    try : \n        valid_value = 1 \n        if isinstance ( name_or_scope , list ) : \n            new_scope = name_or_scope \n        elif name_or_scope and isinstance ( name_or_scope , six . string_types ) : \n            new_scope = current_scope ( ) \n            new_scope . extend ( name_or_scope . split ( '/' ) ) \n        else : \n            valid_value = name_or_scope in ( None , '' ) \n            new_scope = [ ] \n        _ACTIVE_SCOPES . append ( new_scope ) \n        scopes_are_valid = map ( config_parser . MODULE_RE . match , new_scope ) \n        if not valid_value or not all ( scopes_are_valid ) : \n            err_str = 'Invalid value for `name_or_scope`: {}.' \n            raise ValueError ( err_str . format ( name_or_scope ) ) \n        yield new_scope \n    finally : \n        _ACTIVE_SCOPES . pop ( ) "}
{"3051": "\ndef parse_config ( bindings , skip_unknown = 0 ) : \n    if isinstance ( bindings , ( list , tuple ) ) : \n        bindings = '\\n' . join ( bindings ) \n    _validate_skip_unknown ( skip_unknown ) \n    if isinstance ( skip_unknown , ( list , tuple ) ) : \n        skip_unknown = set ( skip_unknown ) \n    parser = config_parser . ConfigParser ( bindings , ParserDelegate ( skip_unknown ) ) \n    for statement in parser : \n        if isinstance ( statement , config_parser . BindingStatement ) : \n            scope , selector , arg_name , value , location = statement \n            if not arg_name : \n                macro_name = '{}/{}' . format ( scope , selector ) if scope else selector \n                with utils . try_with_location ( location ) : \n                    bind_parameter ( ( macro_name , 'gin.macro' , 'value' ) , value ) \n                continue \n            if not _should_skip ( selector , skip_unknown ) : \n                with utils . try_with_location ( location ) : \n                    bind_parameter ( ( scope , selector , arg_name ) , value ) \n        elif isinstance ( statement , config_parser . ImportStatement ) : \n            if skip_unknown : \n                try : \n                    __import__ ( statement . module ) \n                    _IMPORTED_MODULES . add ( statement . module ) \n                except ImportError : \n                    log_str = 'Skipping import of unknown module `%s` (skip_unknown=%r).' \n                    logging . info ( log_str , statement . module , skip_unknown ) \n            else : \n                with utils . try_with_location ( statement . location ) : \n                    __import__ ( statement . module ) \n                _IMPORTED_MODULES . add ( statement . module ) \n        elif isinstance ( statement , config_parser . IncludeStatement ) : \n            with utils . try_with_location ( statement . location ) : \n                parse_config_file ( statement . filename , skip_unknown ) \n        else : \n            raise AssertionError ( 'Unrecognized statement type {}.' . format ( statement ) ) "}
{"3053": "\ndef parse_config_file ( config_file , skip_unknown = 0 ) : \n    for reader , existence_check in _FILE_READERS : \n        if existence_check ( config_file ) : \n            with reader ( config_file ) as f : \n                parse_config ( f , skip_unknown = skip_unknown ) \n                return \n    raise IOError ( 'Unable to open file: {}' . format ( config_file ) ) "}
{"3054": "\ndef parse_config_files_and_bindings ( config_files , bindings , finalize_config = 1 , skip_unknown = 0 ) : \n    if config_files is None : \n        config_files = [ ] \n    if bindings is None : \n        bindings = '' \n    for config_file in config_files : \n        parse_config_file ( config_file , skip_unknown ) \n    parse_config ( bindings , skip_unknown ) \n    if finalize_config : \n        finalize ( ) "}
{"3056": "\ndef finalize ( ) : \n    if config_is_locked ( ) : \n        raise RuntimeError ( 'Finalize called twice (config already locked).' ) \n    bindings = { } \n    for hook in _FINALIZE_HOOKS : \n        new_bindings = hook ( _CONFIG ) \n        if new_bindings is not None : \n            for key , value in six . iteritems ( new_bindings ) : \n                pbk = ParsedBindingKey ( key ) \n                if pbk in bindings : \n                    err_str = 'Received conflicting updates when running {}.' \n                    raise ValueError ( err_str . format ( hook ) ) \n                bindings [ pbk ] = value \n    for pbk , value in six . iteritems ( bindings ) : \n        bind_parameter ( pbk , value ) \n    _set_config_is_locked ( 1 ) "}
{"3100": "\ndef remove_action ( self , action_name , action_id ) : \n    action = self . get_action ( action_name , action_id ) \n    if action is None : \n        return 0 \n    action . cancel ( ) \n    self . actions [ action_name ] . remove ( action ) \n    return 1 "}
{"3109": "\ndef update ( self , ** fields ) : \n    self . _for_write = 1 \n    if django . VERSION >= ( 2 , 0 ) : \n        query = self . query . chain ( UpdateQuery ) \n    else : \n        query = self . query . clone ( UpdateQuery ) \n    query . _annotations = None \n    query . add_update_values ( fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresReturningUpdateCompiler ( query , connection , self . db ) \n    with transaction . atomic ( using = self . db , savepoint = 0 ) : \n        rows = compiler . execute_sql ( CURSOR ) \n    self . _result_cache = None \n    for row in rows : \n        signals . update . send ( self . model , pk = row [ 0 ] ) \n    return len ( rows ) "}
{"3110": "\ndef bulk_insert ( self , rows , return_model = 0 ) : \n    if self . conflict_target or self . conflict_action : \n        compiler = self . _build_insert_compiler ( rows ) \n        objs = compiler . execute_sql ( return_id = 1 ) \n        if return_model : \n            return [ self . model ( ** dict ( r , ** k ) ) for r , k in zip ( rows , objs ) ] \n        else : \n            return [ dict ( r , ** k ) for r , k in zip ( rows , objs ) ] \n    return super ( ) . bulk_create ( [ self . model ( ** fields ) for fields in rows ] ) "}
{"3111": "\ndef insert ( self , ** fields ) : \n    if self . conflict_target or self . conflict_action : \n        compiler = self . _build_insert_compiler ( [ fields ] ) \n        rows = compiler . execute_sql ( return_id = 1 ) \n        pk_field_name = self . model . _meta . pk . name \n        return rows [ 0 ] [ pk_field_name ] \n    return super ( ) . create ( ** fields ) . pk "}
{"3112": "\ndef insert_and_get ( self , ** fields ) : \n    if not self . conflict_target and not self . conflict_action : \n        return super ( ) . create ( ** fields ) \n    compiler = self . _build_insert_compiler ( [ fields ] ) \n    rows = compiler . execute_sql ( return_id = 0 ) \n    columns = rows [ 0 ] \n    model_columns = { } \n    for field in self . model . _meta . local_concrete_fields : \n        model_columns [ field . column ] = field . attname \n    model_init_fields = { } \n    for column_name , column_value in columns . items ( ) : \n        try : \n            model_init_fields [ model_columns [ column_name ] ] = column_value \n        except KeyError : \n            pass \n    return self . model ( ** model_init_fields ) "}
{"3113": "\ndef _build_insert_compiler ( self , rows : List [ Dict ] ) : \n    objs = [ ] \n    field_count = len ( rows [ 0 ] ) \n    for index , row in enumerate ( rows ) : \n        if field_count != len ( row ) : \n            raise SuspiciousOperation ( ( 'In bulk upserts, you cannot have rows with different field ' 'configurations. Row {0} has a different field config than ' 'the first row.' ) . format ( index ) ) \n        objs . append ( self . model ( ** row ) ) \n    self . _for_write = 1 \n    insert_fields , update_fields = self . _get_upsert_fields ( rows [ 0 ] ) \n    query = PostgresInsertQuery ( self . model ) \n    query . conflict_action = self . conflict_action \n    query . conflict_target = self . conflict_target \n    query . index_predicate = self . index_predicate \n    query . values ( objs , insert_fields , update_fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresInsertCompiler ( query , connection , self . db ) \n    return compiler "}
{"3115": "\ndef _get_upsert_fields ( self , kwargs ) : \n    model_instance = self . model ( ** kwargs ) \n    insert_fields = [ ] \n    update_fields = [ ] \n    for field in model_instance . _meta . local_concrete_fields : \n        has_default = field . default != NOT_PROVIDED \n        if ( field . name in kwargs or field . column in kwargs ) : \n            insert_fields . append ( field ) \n            update_fields . append ( field ) \n            continue \n        elif has_default : \n            insert_fields . append ( field ) \n            continue \n        if field . primary_key is 1 and 'pk' in kwargs : \n            insert_fields . append ( field ) \n            update_fields . append ( field ) \n            continue \n        if self . _is_magical_field ( model_instance , field , is_insert = 1 ) : \n            insert_fields . append ( field ) \n        if self . _is_magical_field ( model_instance , field , is_insert = 0 ) : \n            update_fields . append ( field ) \n    return insert_fields , update_fields "}
{"3122": "\ndef add_join_conditions ( self , conditions : Dict [ str , Any ] ) -> None : \n    alias = self . get_initial_alias ( ) \n    opts = self . get_meta ( ) \n    for name , value in conditions . items ( ) : \n        parts = name . split ( LOOKUP_SEP ) \n        join_info = self . setup_joins ( parts , opts , alias , allow_many = 1 ) \n        self . trim_joins ( join_info [ 1 ] , join_info [ 3 ] , join_info [ 4 ] ) \n        target_table = join_info [ 3 ] [ - 1 ] \n        field = join_info [ 1 ] [ - 1 ] \n        join = self . alias_map . get ( target_table ) \n        if not join : \n            raise SuspiciousOperation ( ( 'Cannot add an extra join condition for \"%s\", there\\'s no' ' existing join to add it to.' ) % target_table ) \n        if not isinstance ( join , ConditionalJoin ) : \n            self . alias_map [ target_table ] = ConditionalJoin . from_join ( join ) \n            join = self . alias_map [ target_table ] \n        join . add_condition ( field , value ) "}
{"3124": "\ndef values ( self , objs : List , insert_fields : List , update_fields : List = [ ] ) : \n    self . insert_values ( insert_fields , objs , raw = 0 ) \n    self . update_fields = update_fields "}
{"3132": "\ndef prepare_database ( self ) : \n    super ( ) . prepare_database ( ) \n    with self . cursor ( ) as cursor : \n        try : \n            cursor . execute ( 'CREATE EXTENSION IF NOT EXISTS hstore' ) \n        except ProgrammingError : \n            logger . warning ( 'Failed to create \"hstore\" extension. ' 'Tables with hstore columns may fail to migrate. ' 'If hstore is needed, make sure you are connected ' 'to the database as a superuser ' 'or add the extension manually.' , exc_info = 1 ) "}
{"3135": "\ndef as_sql ( self , return_id = 0 ) : \n    queries = [ self . _rewrite_insert ( sql , params , return_id ) for sql , params in super ( ) . as_sql ( ) ] \n    return queries "}
{"3136": "\ndef _rewrite_insert ( self , sql , params , return_id = 0 ) : \n    returning = self . qn ( self . query . model . _meta . pk . attname ) if return_id else '*' \n    if self . query . conflict_action . value == 'UPDATE' : \n        return self . _rewrite_insert_update ( sql , params , returning ) \n    elif self . query . conflict_action . value == 'NOTHING' : \n        return self . _rewrite_insert_nothing ( sql , params , returning ) \n    raise SuspiciousOperation ( ( '%s is not a valid conflict action, specify ' 'ConflictAction.UPDATE or ConflictAction.NOTHING.' ) % str ( self . query . conflict_action ) ) "}
{"3156": "\ndef play ( self , board ) : \n    color = board . color \n    node = self \n    path = [ node ] \n    while 1 : \n        pos = node . select ( board ) \n        if pos == PASS : \n            break \n        board . move ( pos ) \n        child = node . pos_child [ pos ] \n        if not child : \n            child = node . pos_child [ pos ] = UCTNode ( ) \n            child . unexplored = board . useful_moves ( ) \n            child . pos = pos \n            child . parent = node \n            path . append ( child ) \n            break \n        path . append ( child ) \n        node = child \n    self . random_playout ( board ) \n    self . update_path ( board , color , path ) "}
{"3159": "\ndef filter_benchmarks ( benchmarks , bench_funcs , base_ver ) : \n    for bm in list ( benchmarks ) : \n        func = bench_funcs [ bm ] \n        if getattr ( func , '_python2_only' , 0 ) and ( 3 , 0 ) <= base_ver : \n            benchmarks . discard ( bm ) \n            logging . info ( \"Skipping Python2-only benchmark %s; \" \"not compatible with Python %s\" % ( bm , base_ver ) ) \n            continue \n    return benchmarks "}
{"3164": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) \n    fetching = 1 \n    page = 0 \n    nposts = 0 \n    since = int ( from_date . timestamp ( ) * 1000 ) \n    while fetching : \n        raw_posts = self . client . posts ( self . channel , page = page ) \n        posts_before = nposts \n        for post in self . _parse_posts ( raw_posts ) : \n            if post [ 'update_at' ] < since : \n                fetching = 0 \n                break \n            user_id = post [ 'user_id' ] \n            user = self . _get_or_fetch_user ( user_id ) \n            post [ 'user_data' ] = user \n            yield post \n            nposts += 1 \n        if fetching : \n            if posts_before == nposts : \n                fetching = 0 \n            else : \n                page += 1 \n    logger . info ( \"Fetch process completed: %s posts fetched\" , nposts ) "}
{"3169": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , archive = 1 ) \n    parser . parser . add_argument ( 'url' , help = \"URL of the RSS feed\" ) \n    return parser "}
{"3189": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = 1 , token_auth = 1 , archive = 1 ) \n    group = parser . parser . add_argument_group ( 'GitLab arguments' ) \n    group . add_argument ( '--enterprise-url' , dest = 'base_url' , help = \"Base URL for GitLab Enterprise instance\" ) \n    group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , action = 'store_true' , help = \"sleep for getting more rate\" ) \n    group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , default = MIN_RATE_LIMIT , type = int , help = \"sleep until reset when the rate limit \\                               reaches this value\" ) \n    group . add_argument ( '--blacklist-ids' , dest = 'blacklist_ids' , nargs = '*' , type = int , help = \"Ids of items that must not be retrieved.\" ) \n    group . add_argument ( '--max-retries' , dest = 'max_retries' , default = MAX_RETRIES , type = int , help = \"number of API call retries\" ) \n    group . add_argument ( '--sleep-time' , dest = 'sleep_time' , default = DEFAULT_SLEEP_TIME , type = int , help = \"sleeping time between API call retries\" ) \n    parser . parser . add_argument ( 'owner' , help = \"GitLab owner\" ) \n    parser . parser . add_argument ( 'repository' , help = \"GitLab repository\" ) \n    return parser "}
{"3195": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = 1 , token_auth = 1 , archive = 1 ) \n    action = parser . parser . _option_string_actions [ '--api-token' ] \n    action . required = 1 \n    group = parser . parser . add_argument_group ( 'Slack arguments' ) \n    group . add_argument ( '--max-items' , dest = 'max_items' , type = int , default = MAX_ITEMS , help = \"Maximum number of items requested on the same query\" ) \n    parser . parser . add_argument ( 'channel' , help = \"Slack channel identifier\" ) \n    return parser "}
{"3199": "\ndef parse_bug_activity ( raw_html ) : \n    def is_activity_empty ( bs ) : \n        EMPTY_ACTIVITY = \"No changes have been made to this (?:bug|issue) yet.\" \n        tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) \n        return tag is not None \n    def find_activity_table ( bs ) : \n        tables = bs . find_all ( 'table' ) \n        for tb in tables : \n            nheaders = len ( tb . tr . find_all ( 'th' , recursive = 0 ) ) \n            if nheaders == 5 : \n                return tb \n        raise ParseError ( cause = \"Table of bug activity not found.\" ) \n    def remove_tags ( bs ) : \n        HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] \n        for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : \n            tag . replaceWith ( tag . text ) \n    def format_text ( bs ) : \n        strings = [ s . strip ( ' \\n\\t' ) for s in bs . stripped_strings ] \n        s = ' ' . join ( strings ) \n        return s \n    bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) \n    if is_activity_empty ( bs ) : \n        fields = [ ] \n    else : \n        activity_tb = find_activity_table ( bs ) \n        remove_tags ( activity_tb ) \n        fields = activity_tb . find_all ( 'td' ) \n    while fields : \n        who = fields . pop ( 0 ) \n        when = fields . pop ( 0 ) \n        n = int ( who . get ( 'rowspan' ) ) \n        for _ in range ( n ) : \n            what = fields . pop ( 0 ) \n            removed = fields . pop ( 0 ) \n            added = fields . pop ( 0 ) \n            event = { 'Who' : format_text ( who ) , 'When' : format_text ( when ) , 'What' : format_text ( what ) , 'Removed' : format_text ( removed ) , 'Added' : format_text ( added ) } \n            yield event "}
{"3205": "\ndef fetch ( self , category = CATEGORY_EVENT , from_date = DEFAULT_DATETIME , to_date = None , filter_classified = 0 ) : \n    if not from_date : \n        from_date = DEFAULT_DATETIME \n    from_date = datetime_to_utc ( from_date ) \n    kwargs = { \"from_date\" : from_date , \"to_date\" : to_date } \n    items = super ( ) . fetch ( category , filter_classified = filter_classified , ** kwargs ) \n    return items "}
{"3206": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    logger . info ( \"Fetching events of '%s' group from %s to %s\" , self . group , str ( from_date ) , str ( to_date ) if to_date else '--' ) \n    to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None \n    nevents = 0 \n    stop_fetching = 0 \n    ev_pages = self . client . events ( self . group , from_date = from_date ) \n    for evp in ev_pages : \n        events = [ event for event in self . parse_json ( evp ) ] \n        for event in events : \n            event_id = event [ 'id' ] \n            event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) \n            event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) \n            event_ts = self . metadata_updated_on ( event ) \n            if to_date_ts and event_ts >= to_date_ts : \n                stop_fetching = 1 \n                continue \n            yield event \n            nevents += 1 \n        if stop_fetching : \n            break \n    logger . info ( \"Fetch process completed: %s events fetched\" , nevents ) "}
{"3210": "\ndef __fetch_question ( self , question ) : \n    html_question_items = [ ] \n    npages = 1 \n    next_request = 1 \n    while next_request : \n        try : \n            html_question = self . client . get_html_question ( question [ 'id' ] , npages ) \n            html_question_items . append ( html_question ) \n            tpages = self . ab_parser . parse_number_of_html_pages ( html_question ) \n            if npages == tpages : \n                next_request = 0 \n            npages = npages + 1 \n        except requests . exceptions . TooManyRedirects as e : \n            logger . warning ( \"%s, data not retrieved for question %s\" , e , question [ 'id' ] ) \n            next_request = 0 \n    return html_question_items "}
{"3213": "\ndef get_api_questions ( self , path ) : \n    npages = 1 \n    next_request = 1 \n    path = urijoin ( self . base_url , path ) \n    while next_request : \n        try : \n            params = { 'page' : npages , 'sort' : self . ORDER_API } \n            response = self . fetch ( path , payload = params ) \n            whole_page = response . text \n            raw_questions = json . loads ( whole_page ) \n            tpages = raw_questions [ 'pages' ] \n            logger . debug ( \"Fetching questions from '%s': page %s/%s\" , self . base_url , npages , tpages ) \n            if npages == tpages : \n                next_request = 0 \n            npages = npages + 1 \n            yield raw_questions \n        except requests . exceptions . TooManyRedirects as e : \n            logger . warning ( \"%s, data not retrieved for resource %s\" , e , path ) \n            next_request = 0 "}
{"3215": "\ndef get_comments ( self , post_id ) : \n    path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) \n    params = { 'post_id' : post_id , 'post_type' : 'answer' , 'avatar_size' : 0 } \n    headers = { 'X-Requested-With' : 'XMLHttpRequest' } \n    try : \n        response = self . fetch ( path , payload = params , headers = headers ) \n        raw = response . text \n    except requests . exceptions . HTTPError as ex : \n        if ex . response . status_code == 404 : \n            logger . debug ( \"Comments URL did not work. Using old URL schema.\" ) \n            self . _use_new_urls = 0 \n            path = urijoin ( self . base_url , self . COMMENTS_OLD ) \n            response = self . fetch ( path , payload = params , headers = headers ) \n            raw = response . text \n        elif ex . response . status_code == 500 : \n            logger . warning ( \"Comments not retrieved due to %s\" , ex ) \n            raw = '[]' \n        else : \n            raise ex \n    return raw "}
{"3217": "\ndef parse_answers ( html_question ) : \n    def parse_answer_container ( update_info ) : \n        container_info = { } \n        created = update_info [ 0 ] \n        answered_at = created . abbr . attrs [ \"title\" ] \n        container_info [ 'added_at' ] = str ( str_to_datetime ( answered_at ) . timestamp ( ) ) \n        container_info [ 'answered_by' ] = AskbotParser . parse_user_info ( created ) \n        try : \n            update_info [ 1 ] \n        except IndexError : \n            pass \n        else : \n            updated = update_info [ 1 ] \n            updated_at = updated . abbr . attrs [ \"title\" ] \n            container_info [ 'updated_at' ] = str ( str_to_datetime ( updated_at ) . timestamp ( ) ) \n            if AskbotParser . parse_user_info ( updated ) : \n                container_info [ 'updated_by' ] = AskbotParser . parse_user_info ( updated ) \n        return container_info \n    answer_list = [ ] \n    bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) \n    bs_answers = bs_question . select ( \"div.answer\" ) \n    for bs_answer in bs_answers : \n        answer_id = bs_answer . attrs [ \"data-post-id\" ] \n        votes_element = bs_answer . select ( \"div.vote-number\" ) [ 0 ] . text \n        accepted_answer = bs_answer . select ( \"div.answer-img-accept\" ) [ 0 ] . get ( 'title' ) . endswith ( \"correct\" ) \n        body = bs_answer . select ( \"div.post-body\" ) \n        update_info = body [ 0 ] . select ( \"div.post-update-info\" ) \n        answer_container = parse_answer_container ( update_info ) \n        body [ 0 ] . div . extract ( ) . select ( \"div.post-update-info-container\" ) \n        body = body [ 0 ] . get_text ( strip = 1 ) \n        answer = { 'id' : answer_id , 'score' : votes_element , 'summary' : body , 'accepted' : accepted_answer } \n        answer . update ( answer_container ) \n        answer_list . append ( answer ) \n    return answer_list "}
{"3228": "\ndef __execute_from_remote ( self , cmd ) : \n    result = None \n    retries = 0 \n    while retries < self . MAX_RETRIES : \n        try : \n            result = subprocess . check_output ( cmd , shell = 1 ) \n            break \n        except subprocess . CalledProcessError as ex : \n            logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) \n            time . sleep ( self . RETRY_WAIT * retries ) \n            retries += 1 \n    if result is None : \n        result = RuntimeError ( cmd + \" failed \" + str ( self . MAX_RETRIES ) + \" times. Giving up!\" ) \n    if self . archive : \n        cmd = self . sanitize_for_archive ( cmd ) \n        self . archive . store ( cmd , None , None , result ) \n    if isinstance ( result , RuntimeError ) : \n        raise result \n    return result "}
{"3229": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = 1 , archive = 1 ) \n    group = parser . parser . add_argument_group ( 'Gerrit arguments' ) \n    group . add_argument ( '--user' , dest = 'user' , help = \"Gerrit ssh user\" ) \n    group . add_argument ( '--max-reviews' , dest = 'max_reviews' , type = int , default = MAX_REVIEWS , help = \"Max number of reviews per ssh query.\" ) \n    group . add_argument ( '--blacklist-reviews' , dest = 'blacklist_reviews' , nargs = '*' , help = \"Wrong reviews that must not be retrieved.\" ) \n    group . add_argument ( '--disable-host-key-check' , dest = 'disable_host_key_check' , action = 'store_true' , help = \"Don't check remote host identity\" ) \n    group . add_argument ( '--ssh-port' , dest = 'port' , default = PORT , type = int , help = \"Set SSH port of the Gerrit server\" ) \n    parser . parser . add_argument ( 'hostname' , help = \"Hostname of the Gerrit server\" ) \n    return parser "}
{"3239": "\ndef __fetch_items ( self , path , payload ) : \n    page = 0 \n    url_next = path \n    fetch_data = 1 \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            raw_content = self . __send_request ( url_next , payload ) \n            content = json . loads ( raw_content ) \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code in [ 410 ] : \n                logger . warning ( \"Data is not available - %s\" , url_next ) \n                raw_content = '{\"total_size\": 0, \"start\": 0, \"entries\": []}' \n                content = json . loads ( raw_content ) \n            else : \n                raise e \n        if 'next_collection_link' in content : \n            url_next = content [ 'next_collection_link' ] \n            payload = None \n        else : \n            fetch_data = 0 \n        yield raw_content \n        page += 1 "}
{"3240": "\ndef subscriptions ( self , per_page = PER_PAGE ) : \n    url = urijoin ( GROUPSIO_API_URL , self . GET_SUBSCRIPTIONS ) \n    logger . debug ( \"Get groupsio paginated subscriptions from \" + url ) \n    keep_fetching = 1 \n    payload = { \"limit\" : per_page } \n    while keep_fetching : \n        r = self . __fetch ( url , payload ) \n        response_raw = r . json ( ) \n        subscriptions = response_raw [ 'data' ] \n        yield subscriptions \n        total_subscriptions = response_raw [ 'total_count' ] \n        logger . debug ( \"Subscriptions: %i/%i\" % ( response_raw [ 'end_item' ] , total_subscriptions ) ) \n        payload [ 'page_token' ] = response_raw [ 'next_page_token' ] \n        keep_fetching = response_raw [ 'has_more' ] "}
{"3243": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = 1 , token_auth = 1 ) \n    action = parser . parser . _option_string_actions [ '--api-token' ] \n    action . required = 1 \n    group = parser . parser . add_argument_group ( 'Groupsio arguments' ) \n    group . add_argument ( '--mboxes-path' , dest = 'mboxes_path' , help = \"Path where mbox files will be stored\" ) \n    group . add_argument ( '--no-verify' , dest = 'verify' , action = 'store_false' , help = \"Value 'True' enable SSL verification\" ) \n    parser . parser . add_argument ( 'group_name' , help = \"Name of the group on Groups.io\" ) \n    return parser "}
{"3245": "\ndef fetch ( backend_class , backend_args , category , filter_classified = 0 , manager = None ) : \n    init_args = find_signature_parameters ( backend_class . __init__ , backend_args ) \n    archive = manager . create_archive ( ) if manager else None \n    init_args [ 'archive' ] = archive \n    backend = backend_class ( ** init_args ) \n    if category : \n        backend_args [ 'category' ] = category \n    if filter_classified : \n        backend_args [ 'filter_classified' ] = filter_classified \n    fetch_args = find_signature_parameters ( backend . fetch , backend_args ) \n    items = backend . fetch ( ** fetch_args ) \n    try : \n        for item in items : \n            yield item \n    except Exception as e : \n        if manager : \n            archive_path = archive . archive_path \n            manager . remove_archive ( archive_path ) \n        raise e "}
{"3248": "\ndef fetch ( self , category , filter_classified = 0 , ** kwargs ) : \n    if category not in self . categories : \n        cause = \"%s category not valid for %s\" % ( category , self . __class__ . __name__ ) \n        raise BackendError ( cause = cause ) \n    if filter_classified and self . archive : \n        cause = \"classified fields filtering is not compatible with archiving items\" \n        raise BackendError ( cause = cause ) \n    if self . archive : \n        self . archive . init_metadata ( self . origin , self . __class__ . __name__ , self . version , category , kwargs ) \n    self . client = self . _init_client ( ) \n    for item in self . fetch_items ( category , ** kwargs ) : \n        if filter_classified : \n            item = self . filter_classified_data ( item ) \n        yield self . metadata ( item , filter_classified = filter_classified ) "}
{"3249": "\ndef fetch_from_archive ( self ) : \n    if not self . archive : \n        raise ArchiveError ( cause = \"archive instance was not provided\" ) \n    self . client = self . _init_client ( from_archive = 1 ) \n    for item in self . fetch_items ( self . archive . category , ** self . archive . backend_params ) : \n        yield self . metadata ( item ) "}
{"3252": "\ndef _set_auth_arguments ( self , basic_auth = 1 , token_auth = 0 ) : \n    group = self . parser . add_argument_group ( 'authentication arguments' ) \n    if basic_auth : \n        group . add_argument ( '-u' , '--backend-user' , dest = 'user' , help = \"backend user\" ) \n        group . add_argument ( '-p' , '--backend-password' , dest = 'password' , help = \"backend password\" ) \n    if token_auth : \n        group . add_argument ( '-t' , '--api-token' , dest = 'api_token' , help = \"backend authentication token / API key\" ) "}
{"3255": "\ndef run ( self ) : \n    backend_args = vars ( self . parsed_args ) \n    category = backend_args . pop ( 'category' , None ) \n    filter_classified = backend_args . pop ( 'filter_classified' , 0 ) \n    archived_since = backend_args . pop ( 'archived_since' , None ) \n    if self . archive_manager and self . parsed_args . fetch_archive : \n        items = fetch_from_archive ( self . BACKEND , backend_args , self . archive_manager , category , archived_since ) \n    else : \n        items = fetch ( self . BACKEND , backend_args , category , filter_classified = filter_classified , manager = self . archive_manager ) \n    try : \n        for item in items : \n            if self . json_line : \n                obj = json . dumps ( item , separators = ( ',' , ':' ) , sort_keys = 1 ) \n            else : \n                obj = json . dumps ( item , indent = 4 , sort_keys = 1 ) \n            self . outfile . write ( obj ) \n            self . outfile . write ( '\\n' ) \n    except IOError as e : \n        raise RuntimeError ( str ( e ) ) \n    except Exception as e : \n        raise RuntimeError ( str ( e ) ) "}
{"3258": "\ndef parse_mbox ( filepath ) : \n    mbox = _MBox ( filepath , create = 0 ) \n    for msg in mbox : \n        message = message_to_dict ( msg ) \n        yield message "}
{"3261": "\ndef _validate_message ( self , message ) : \n    if self . MESSAGE_ID_FIELD not in message : \n        logger . warning ( \"Field 'Message-ID' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return 0 \n    if not message [ self . MESSAGE_ID_FIELD ] : \n        logger . warning ( \"Field 'Message-ID' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return 0 \n    if self . DATE_FIELD not in message : \n        logger . warning ( \"Field 'Date' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return 0 \n    if not message [ self . DATE_FIELD ] : \n        logger . warning ( \"Field 'Date' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return 0 \n    try : \n        str_to_datetime ( message [ self . DATE_FIELD ] ) \n    except InvalidDateError : \n        logger . warning ( \"Invalid date %s in message %s; ignoring\" , message [ self . DATE_FIELD ] , message [ 'unixfrom' ] ) \n        return 0 \n    return 1 "}
{"3264": "\ndef fetch ( self , category = CATEGORY_COMMIT , from_date = DEFAULT_DATETIME , to_date = DEFAULT_LAST_DATETIME , branches = None , latest_items = 0 , no_update = 0 ) : \n    if not from_date : \n        from_date = DEFAULT_DATETIME \n    if not to_date : \n        to_date = DEFAULT_LAST_DATETIME \n    kwargs = { 'from_date' : from_date , 'to_date' : to_date , 'branches' : branches , 'latest_items' : latest_items , 'no_update' : no_update } \n    items = super ( ) . fetch ( category , ** kwargs ) \n    return items "}
{"3268": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = 1 , to_date = 1 ) \n    group = parser . parser . add_argument_group ( 'Git arguments' ) \n    group . add_argument ( '--branches' , dest = 'branches' , nargs = '+' , type = str , default = None , help = \"Fetch commits only from these branches\" ) \n    exgroup = group . add_mutually_exclusive_group ( ) \n    exgroup . add_argument ( '--git-path' , dest = 'git_path' , help = \"Path where the Git repository will be cloned\" ) \n    exgroup . add_argument ( '--git-log' , dest = 'git_log' , help = \"Path to the Git log file\" ) \n    exgroup_fetch = group . add_mutually_exclusive_group ( ) \n    exgroup_fetch . add_argument ( '--latest-items' , dest = 'latest_items' , action = 'store_true' , help = \"Fetch latest commits added to the repository\" ) \n    exgroup_fetch . add_argument ( '--no-update' , dest = 'no_update' , action = 'store_true' , help = \"Fetch all commits without updating the repository\" ) \n    parser . parser . add_argument ( 'uri' , help = \"URI of the Git log repository\" ) \n    return parser "}
{"3269": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        parsed = 0 \n        self . nline += 1 \n        while not parsed : \n            parsed = self . handlers [ self . state ] ( line ) \n            if self . state == self . COMMIT and self . commit : \n                commit = self . _build_commit ( ) \n                logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n                yield commit \n    if self . commit : \n        commit = self . _build_commit ( ) \n        logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n        yield commit "}
{"3272": "\ndef is_detached ( self ) : \n    cmd_sym = [ 'git' , 'symbolic-ref' , 'HEAD' ] \n    try : \n        self . _exec ( cmd_sym , cwd = self . dirpath , env = self . gitenv ) \n    except RepositoryError as e : \n        if e . msg . find ( \"ref HEAD is not a symbolic ref\" ) == - 1 : \n            raise e \n        return 1 \n    else : \n        return 0 "}
{"3278": "\ndef _fetch_pack ( self ) : \n    def prepare_refs ( refs ) : \n        return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] \n    def determine_wants ( refs ) : \n        remote_refs = prepare_refs ( self . _discover_refs ( remote = 1 ) ) \n        local_refs = prepare_refs ( self . _discover_refs ( ) ) \n        wants = [ ref for ref in remote_refs if ref not in local_refs ] \n        return wants \n    client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) \n    repo = dulwich . repo . Repo ( self . dirpath ) \n    fd = io . BytesIO ( ) \n    local_refs = self . _discover_refs ( ) \n    graph_walker = _GraphWalker ( local_refs ) \n    result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) \n    refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] \n    if len ( fd . getvalue ( ) ) > 0 : \n        fd . seek ( 0 ) \n        pack = repo . object_store . add_thin_pack ( fd . read , None ) \n        pack_name = pack . name ( ) . decode ( 'utf-8' ) \n    else : \n        pack_name = None \n    return ( pack_name , refs ) "}
{"3280": "\ndef _update_references ( self , refs ) : \n    new_refs = [ ref . refname for ref in refs ] \n    for old_ref in self . _discover_refs ( ) : \n        if not old_ref . refname . startswith ( 'refs/heads/' ) : \n            continue \n        if old_ref . refname in new_refs : \n            continue \n        self . _update_ref ( old_ref , delete = 1 ) \n    for new_ref in refs : \n        refname = new_ref . refname \n        if refname . endswith ( '^{}' ) : \n            logger . debug ( \"Annotated tag %s ignored for updating in sync process\" , refname ) \n            continue \n        elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : \n            logger . debug ( \"Reference %s not needed; ignored for updating in sync process\" , refname ) \n            continue \n        else : \n            self . _update_ref ( new_ref ) \n    cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] \n    self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) "}
{"3281": "\ndef _discover_refs ( self , remote = 0 ) : \n    if remote : \n        cmd_refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] \n        sep = '\\t' \n        ignored_error_codes = [ 2 ] \n    else : \n        if self . is_empty ( ) : \n            raise EmptyRepositoryError ( repository = self . uri ) \n        cmd_refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] \n        sep = ' ' \n        ignored_error_codes = [ 1 ] \n    outs = self . _exec ( cmd_refs , cwd = self . dirpath , env = self . gitenv , ignored_error_codes = ignored_error_codes ) \n    outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) \n    outs = outs . split ( '\\n' ) if outs else [ ] \n    refs = [ ] \n    for line in outs : \n        data = line . split ( sep ) \n        ref = GitRef ( data [ 0 ] , data [ 1 ] ) \n        refs . append ( ref ) \n    return refs "}
{"3282": "\ndef _update_ref ( self , ref , delete = 0 ) : \n    cmd = [ 'git' , 'update-ref' ] \n    if delete : \n        cmd . extend ( [ '-d' , ref . refname ] ) \n        action = 'deleted' \n    else : \n        cmd . extend ( [ ref . refname , ref . hash ] ) \n        action = 'updated to %s' % ref . hash \n    try : \n        self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) \n    except RepositoryError as e : \n        logger . warning ( \"Git %s ref could not be %s during sync process in %s (%s); skipped\" , ref . refname , action , self . uri , self . dirpath ) \n    else : \n        logger . debug ( \"Git %s ref %s in %s (%s)\" , ref . refname , action , self . uri , self . dirpath ) "}
{"3283": "\ndef _exec_nb ( self , cmd , cwd = None , env = None , encoding = 'utf-8' ) : \n    self . failed_message = None \n    logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) \n    try : \n        self . proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) \n        err_thread = threading . Thread ( target = self . _read_stderr , kwargs = { 'encoding' : encoding } , daemon = 1 ) \n        err_thread . start ( ) \n        for line in self . proc . stdout : \n            yield line . decode ( encoding , errors = 'surrogateescape' ) \n        err_thread . join ( ) \n        self . proc . communicate ( ) \n        self . proc . stdout . close ( ) \n        self . proc . stderr . close ( ) \n    except OSError as e : \n        err_thread . join ( ) \n        raise RepositoryError ( cause = str ( e ) ) \n    if self . proc . returncode != 0 : \n        cause = \"git command - %s (return code: %d)\" % ( self . failed_message , self . proc . returncode ) \n        raise RepositoryError ( cause = cause ) "}
{"3286": "\ndef fetch ( self , category = CATEGORY_TWEET , since_id = None , max_id = None , geocode = None , lang = None , include_entities = 1 , tweets_type = TWEET_TYPE_MIXED ) : \n    kwargs = { \"since_id\" : since_id , \"max_id\" : max_id , \"geocode\" : geocode , \"lang\" : lang , \"include_entities\" : include_entities , \"result_type\" : tweets_type } \n    items = super ( ) . fetch ( category , ** kwargs ) \n    return items "}
{"3288": "\ndef tweets ( self , query , since_id = None , max_id = None , geocode = None , lang = None , include_entities = 1 , result_type = TWEET_TYPE_MIXED ) : \n    resource = self . base_url \n    params = { 'q' : query , 'count' : self . max_items } \n    if since_id : \n        params [ 'since_id' ] = since_id \n    if max_id : \n        params [ 'max_id' ] = max_id \n    if geocode : \n        params [ 'geocode' ] = geocode \n    if lang : \n        params [ 'lang' ] = lang \n    params [ 'include_entities' ] = include_entities \n    params [ 'result_type' ] = result_type \n    while 1 : \n        raw_tweets = self . _fetch ( resource , params = params ) \n        tweets = json . loads ( raw_tweets ) \n        if not tweets [ 'statuses' ] : \n            break \n        params [ 'max_id' ] = tweets [ 'statuses' ] [ - 1 ] [ 'id' ] - 1 \n        yield tweets [ 'statuses' ] "}
{"3289": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , token_auth = 1 , archive = 1 ) \n    action = parser . parser . _option_string_actions [ '--api-token' ] \n    action . required = 1 \n    group = parser . parser . add_argument_group ( 'Twitter arguments' ) \n    group . add_argument ( '--max-items' , dest = 'max_items' , type = int , default = MAX_ITEMS , help = \"Maximum number of items requested on the same query\" ) \n    group . add_argument ( '--no-entities' , dest = 'include_entities' , action = 'store_false' , help = \" Exclude entities node\" ) \n    group . add_argument ( '--geo-code' , dest = 'geocode' , help = \"Select tweets by users located at latitude,longitude,radius\" ) \n    group . add_argument ( '--lang' , dest = 'lang' , help = \"Select tweets to the given language in ISO 639-1 code\" ) \n    group . add_argument ( '--tweets-type' , dest = 'tweets_type' , default = TWEET_TYPE_MIXED , help = \"Type of tweets returned. Default is 'mixed', others are 'recent' and 'popular'\" ) \n    group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , action = 'store_true' , help = \"sleep for getting more rate\" ) \n    group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , default = MIN_RATE_LIMIT , type = int , help = \"sleep until reset when the rate limit reaches this value\" ) \n    group . add_argument ( '--sleep-time' , dest = 'sleep_time' , default = SLEEP_TIME , type = int , help = \"minimun sleeping time to avoid too many request exception\" ) \n    parser . parser . add_argument ( 'query' , help = \"Search query including operators, max 500 chars\" ) \n    return parser "}
{"3317": "\ndef _need_check_tokens ( self ) : \n    if self . n_tokens <= 1 or self . rate_limit is None : \n        return 0 \n    elif self . last_rate_limit_checked is None : \n        self . last_rate_limit_checked = self . rate_limit \n        return 1 \n    approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 \n    if self . rate_limit <= approaching_limit : \n        self . last_rate_limit_checked = self . rate_limit \n        return 1 \n    ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) \n    if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : \n        self . last_rate_limit_checked = self . rate_limit \n        return 1 \n    elif ratio > 1.0 : \n        self . last_rate_limit_checked = self . rate_limit \n        return 0 \n    else : \n        return 0 "}
{"3323": "\ndef make_hashcode ( uri , payload , headers ) : \n    def dict_to_json_str ( data ) : \n        return json . dumps ( data , sort_keys = 1 ) \n    content = ':' . join ( [ uri , dict_to_json_str ( payload ) , dict_to_json_str ( headers ) ] ) \n    hashcode = hashlib . sha1 ( content . encode ( 'utf-8' ) ) \n    return hashcode . hexdigest ( ) "}
{"3334": "\ndef message_to_dict ( msg ) : \n    def parse_headers ( msg ) : \n        headers = { } \n        for header , value in msg . items ( ) : \n            hv = [ ] \n            for text , charset in email . header . decode_header ( value ) : \n                if type ( text ) == bytes : \n                    charset = charset if charset else 'utf-8' \n                    try : \n                        text = text . decode ( charset , errors = 'surrogateescape' ) \n                    except ( UnicodeError , LookupError ) : \n                        text = text . decode ( 'ascii' , errors = 'surrogateescape' ) \n                hv . append ( text ) \n            v = ' ' . join ( hv ) \n            headers [ header ] = v if v else None \n        return headers \n    def parse_payload ( msg ) : \n        body = { } \n        if not msg . is_multipart ( ) : \n            payload = decode_payload ( msg ) \n            subtype = msg . get_content_subtype ( ) \n            body [ subtype ] = [ payload ] \n        else : \n            for part in email . iterators . typed_subpart_iterator ( msg ) : \n                payload = decode_payload ( part ) \n                subtype = part . get_content_subtype ( ) \n                body . setdefault ( subtype , [ ] ) . append ( payload ) \n        return { k : '\\n' . join ( v ) for k , v in body . items ( ) } \n    def decode_payload ( msg_or_part ) : \n        charset = msg_or_part . get_content_charset ( 'utf-8' ) \n        payload = msg_or_part . get_payload ( decode = 1 ) \n        try : \n            payload = payload . decode ( charset , errors = 'surrogateescape' ) \n        except ( UnicodeError , LookupError ) : \n            payload = payload . decode ( 'ascii' , errors = 'surrogateescape' ) \n        return payload \n    message = requests . structures . CaseInsensitiveDict ( ) \n    if isinstance ( msg , mailbox . mboxMessage ) : \n        message [ 'unixfrom' ] = msg . get_from ( ) \n    else : \n        message [ 'unixfrom' ] = None \n    try : \n        for k , v in parse_headers ( msg ) . items ( ) : \n            message [ k ] = v \n        message [ 'body' ] = parse_payload ( msg ) \n    except UnicodeError as e : \n        raise ParseError ( cause = str ( e ) ) \n    return message "}
{"3341": "\ndef _call ( self , resource , params ) : \n    url = self . URL % { 'base' : self . base_url , 'resource' : resource } \n    if self . api_token : \n        params [ self . PKEY ] = self . api_token \n    logger . debug ( \"Redmine client requests: %s params: %s\" , resource , str ( params ) ) \n    r = self . fetch ( url , payload = params , verify = 0 ) \n    return r . text "}
{"3346": "\ndef filter_custom_fields ( fields ) : \n    custom_fields = { } \n    sorted_fields = [ field for field in fields if field [ 'custom' ] is 1 ] \n    for custom_field in sorted_fields : \n        custom_fields [ custom_field [ 'id' ] ] = custom_field \n    return custom_fields "}
{"3348": "\ndef get_items ( self , from_date , url , expand_fields = 1 ) : \n    start_at = 0 \n    req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n    issues = req . text \n    data = req . json ( ) \n    titems = data [ 'total' ] \n    nitems = data [ 'maxResults' ] \n    start_at += min ( nitems , titems ) \n    self . __log_status ( start_at , titems , url ) \n    while issues : \n        yield issues \n        issues = None \n        if data [ 'startAt' ] + nitems < titems : \n            req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n            data = req . json ( ) \n            start_at += nitems \n            issues = req . text \n            self . __log_status ( start_at , titems , url ) "}
{"3350": "\ndef get_comments ( self , issue_id ) : \n    url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , self . ISSUE , issue_id , self . COMMENT ) \n    comments = self . get_items ( DEFAULT_DATETIME , url , expand_fields = 0 ) \n    return comments "}
{"3357": "\ndef setup_cmd_parser ( cls ) : \n    parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = 1 , token_auth = 1 , archive = 1 ) \n    group = parser . parser . add_argument_group ( 'StackExchange arguments' ) \n    group . add_argument ( '--site' , dest = 'site' , required = 1 , help = \"StackExchange site\" ) \n    group . add_argument ( '--tagged' , dest = 'tagged' , help = \"filter items by question Tag\" ) \n    group . add_argument ( '--max-questions' , dest = 'max_questions' , type = int , default = MAX_QUESTIONS , help = \"Maximum number of questions requested in the same query\" ) \n    return parser "}
{"3365": "\ndef _filter_message_by_chats ( self , message , chats ) : \n    if chats is None : \n        return 1 \n    chat_id = message [ 'message' ] [ 'chat' ] [ 'id' ] \n    return chat_id in chats "}
{"3368": "\ndef metadata ( self , item , filter_classified = 0 ) : \n    item = super ( ) . metadata ( item , filter_classified = filter_classified ) \n    item [ 'offset' ] = item [ 'data' ] [ 'offset' ] \n    return item "}
{"3375": "\ndef setup_rate_limit_handler ( self , sleep_for_rate = 0 , min_rate_to_sleep = MIN_RATE_LIMIT , rate_limit_header = RATE_LIMIT_HEADER , rate_limit_reset_header = RATE_LIMIT_RESET_HEADER ) : \n    self . rate_limit = None \n    self . rate_limit_reset_ts = None \n    self . sleep_for_rate = sleep_for_rate \n    self . rate_limit_header = rate_limit_header \n    self . rate_limit_reset_header = rate_limit_reset_header \n    if min_rate_to_sleep > self . MAX_RATE_LIMIT : \n        msg = \"Minimum rate to sleep value exceeded (%d).\" \n        msg += \"High values might cause the client to sleep forever.\" \n        msg += \"Reset to %d.\" \n        self . min_rate_to_sleep = self . MAX_RATE_LIMIT \n        logger . warning ( msg , min_rate_to_sleep , self . MAX_RATE_LIMIT ) \n    else : \n        self . min_rate_to_sleep = min_rate_to_sleep "}
{"3391": "\ndef tasks ( self , from_date = DEFAULT_DATETIME ) : \n    ts = int ( datetime_to_utc ( from_date ) . timestamp ( ) ) or 1 \n    consts = { self . PMODIFIED_START : ts } \n    attachments = { self . PPROJECTS : 1 } \n    params = { self . PCONSTRAINTS : consts , self . PATTACHMENTS : attachments , self . PORDER : self . VOUTDATED , } \n    while 1 : \n        r = self . _call ( self . MANIPHEST_TASKS , params ) \n        yield r \n        j = json . loads ( r ) \n        after = j [ 'result' ] [ 'cursor' ] [ 'after' ] \n        if not after : \n            break \n        params [ self . PAFTER ] = after "}
{"3395": "\ndef _call ( self , method , params ) : \n    url = self . URL % { 'base' : self . base_url , 'method' : method } \n    params [ '__conduit__' ] = { 'token' : self . api_token } \n    data = { 'params' : json . dumps ( params , sort_keys = 1 ) , 'output' : 'json' , '__conduit__' : 1 } \n    logger . debug ( \"Phabricator Conduit client requests: %s params: %s\" , method , str ( data ) ) \n    r = self . fetch ( url , payload = data , method = HttpClient . POST , verify = 0 ) \n    result = r . json ( ) \n    if result [ 'error_code' ] : \n        raise ConduitError ( error = result [ 'error_info' ] , code = result [ 'error_code' ] ) \n    return r . text "}
{"3409": "\ndef complex_input_with_reference ( ) : \n    print ( \"\\ncomplex_input_with_reference ...\" ) \n    wps = WebProcessingService ( 'http://localhost:8094/wps' , verbose = verbose ) \n    processid = 'wordcount' \n    textdoc = ComplexDataInput ( \"http://www.gutenberg.org/files/28885/28885-h/28885-h.htm\" ) \n    inputs = [ ( \"text\" , textdoc ) ] \n    outputs = [ ( \"output\" , 1 , 'some/mime-type' ) ] \n    execution = wps . execute ( processid , inputs , output = outputs ) \n    monitorExecution ( execution ) \n    print ( 'percent complete' , execution . percentCompleted ) \n    print ( 'status message' , execution . statusMessage ) \n    for output in execution . processOutputs : \n        print ( 'identifier=%s, dataType=%s, data=%s, reference=%s' % ( output . identifier , output . dataType , output . data , output . reference ) ) "}
{"3470": "\ndef get_descriptors_in_module ( mdl , submodule = 1 ) : \n    __all__ = getattr ( mdl , \"__all__\" , None ) \n    if __all__ is None : \n        __all__ = dir ( mdl ) \n    all_values = ( getattr ( mdl , name ) for name in __all__ if name [ : 1 ] != \"_\" ) \n    if submodule : \n        for v in all_values : \n            if is_descriptor_class ( v ) : \n                yield v \n            if isinstance ( v , ModuleType ) : \n                for v in get_descriptors_in_module ( v , submodule = 1 ) : \n                    yield v \n    else : \n        for v in all_values : \n            if is_descriptor_class ( v ) : \n                yield v "}
{"3472": "\ndef register ( self , desc , version = None , ignore_3D = 0 ) : \n    if version is None : \n        version = __version__ \n    version = StrictVersion ( version ) \n    return self . _register ( desc , version , ignore_3D ) "}
{"3474": "\ndef is_descriptor_class ( desc , include_abstract = 0 ) : \n    return ( isinstance ( desc , type ) and issubclass ( desc , Descriptor ) and ( 1 if include_abstract else not inspect . isabstract ( desc ) ) ) "}
{"3484": "\ndef asdict ( self , rawkey = 0 ) : \n    if rawkey : \n        return dict ( self . items ( ) ) \n    else : \n        return { str ( k ) : v for k , v in self . items ( ) } "}
{"3490": "\ndef fail ( message , exc_info = None , status = 1 , stacktrace = 0 ) : \n    text = message \n    if exc_info : \n        text += str ( exc_info ) \n    error ( text ) \n    if stacktrace : \n        error ( traceback . format_exc ( ) ) \n    clean_tempfiles ( ) \n    if __name__ == '__main__' : \n        sys . exit ( status ) \n    else : \n        raise RuntimeError ( status ) "}
{"3507": "\ndef list_buckets ( self ) : \n    result = [ ] \n    for bucket in self . s3 . list_buckets ( ) . get ( 'Buckets' ) or [ ] : \n        result . append ( { 'name' : S3URL . combine ( 's3' , bucket [ 'Name' ] , '' ) , 'is_dir' : 1 , 'size' : 0 , 'last_modified' : bucket [ 'CreationDate' ] } ) \n    return result "}
{"3510": "\ndef source_expand ( self , source ) : \n    result = [ ] \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    for src in source : \n        tmp = self . opt . recursive \n        self . opt . recursive = 0 \n        result += [ f [ 'name' ] for f in self . s3walk ( src , 1 ) ] \n        self . opt . recursive = tmp \n    if ( len ( result ) == 0 ) and ( not self . opt . ignore_empty_source ) : \n        fail ( \"[Runtime Failure] Source doesn't exist.\" ) \n    return result "}
{"3519": "\ndef cp_files ( self , source , target , delete_source = 0 ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if target [ - 1 ] == PATH_SEP : \n        for src in source : \n            self . cp_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) , delete_source ) \n    else : \n        if len ( source ) > 1 : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n        elif len ( source ) == 1 : \n            self . cp_single_file ( pool , source [ 0 ] , target , delete_source ) \n        else : \n            pass \n    pool . join ( ) "}
{"3523": "\ndef file_hash ( self , filename , block_size = 2 ** 20 ) : \n    m = hashlib . md5 ( ) \n    with open ( filename , 'rb' ) as f : \n        while 1 : \n            data = f . read ( block_size ) \n            if not data : \n                break \n            m . update ( data ) \n    return m . hexdigest ( ) "}
{"3526": "\ndef sync_check ( self , md5cache , remoteKey ) : \n    if not remoteKey : \n        return 0 \n    if not os . path . exists ( md5cache . filename ) : \n        return 0 \n    localmd5 = md5cache . get_md5 ( ) \n    return ( 'ETag' in remoteKey and remoteKey [ 'ETag' ] == '\"%s\"' % localmd5 ) or ( 'md5' in remoteKey and remoteKey [ 'md5' ] == localmd5 ) or ( 'md5' in remoteKey [ 'Metadata' ] and remoteKey [ 'Metadata' ] [ 'md5' ] == localmd5 ) "}
{"3527": "\ndef partial_match ( self , path , filter_path ) : \n    if not path or not filter_path : \n        return 1 \n    if path [ - 1 ] == PATH_SEP : \n        path = path [ 0 : - 1 ] \n    if filter_path [ - 1 ] == PATH_SEP : \n        filter_path += '*' \n    pi = path . split ( PATH_SEP ) \n    fi = filter_path . split ( PATH_SEP ) \n    min_len = min ( len ( pi ) , len ( fi ) ) \n    matched = fnmatch . fnmatch ( PATH_SEP . join ( pi [ 0 : min_len ] ) , PATH_SEP . join ( fi [ 0 : min_len ] ) ) \n    return matched and ( self . opt . recursive or len ( pi ) <= len ( fi ) ) "}
{"3528": "\ndef s3walk ( self , s3url , s3dir , filter_path , result ) : \n    paginator = self . s3 . get_paginator ( 'list_objects' ) \n    filter_path_level = filter_path . count ( PATH_SEP ) \n    for page in paginator . paginate ( Bucket = s3url . bucket , Prefix = s3dir , Delimiter = PATH_SEP , PaginationConfig = { 'PageSize' : 1000 } ) : \n        for obj in page . get ( 'CommonPrefixes' ) or [ ] : \n            obj_name = obj [ 'Prefix' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or ( obj_name . count ( PATH_SEP ) != filter_path_level + 1 ) : \n                self . pool . s3walk ( s3url , obj_name , filter_path , result ) \n            else : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : 1 , 'size' : 0 , 'last_modified' : None } ) \n        for obj in page . get ( 'Contents' ) or [ ] : \n            obj_name = obj [ 'Key' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or obj_name . count ( PATH_SEP ) == filter_path_level : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : 0 , 'size' : obj [ 'Size' ] , 'last_modified' : obj [ 'LastModified' ] } ) "}
{"3536": "\ndef copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete_source = 0 ) : \n    if self . opt . dry_run : \n        message ( '%s => %s' % ( source , target ) ) \n        return \n    source_url = S3URL ( source ) \n    target_url = S3URL ( target ) \n    if not mpi : \n        obj = self . lookup ( source_url ) \n        fsize = int ( obj [ 'ContentLength' ] ) \n        if fsize < self . opt . max_singlepart_copy_size : \n            self . s3 . copy_object ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } ) \n            message ( '%s => %s' % ( source , target ) ) \n            if delete_source : \n                self . delete ( source ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , Metadata = obj [ 'Metadata' ] ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . copy ( * args , delete_source = delete_source ) \n        return \n    response = self . s3 . upload_part_copy ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } , CopySourceRange = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , UploadId = mpi . id , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'CopyPartResult' ] [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            if delete_source : \n                self . delete ( source ) \n            message ( '%s => %s' % ( source , target ) ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = source_url . bucket , Key = source_url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Copy failed: Unable to complete copy %s.' % source ) "}
{"3538": "\ndef validate ( self , format , args ) : \n    fmtMap = { 'cmd' : 'Command' , 's3' : 's3 path' , 'local' : 'local path' } \n    fmts = format . split ( '|' ) \n    if len ( fmts ) != len ( args ) : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    for i , fmt in enumerate ( fmts ) : \n        valid = 0 \n        for f in fmt . split ( ',' ) : \n            if f == 'cmd' and args [ i ] + '_handler' in CommandHandler . __dict__ : \n                valid = 1 \n            if f == 's3' and S3URL . is_valid ( args [ i ] ) : \n                valid = 1 \n            if f == 'local' and not S3URL . is_valid ( args [ i ] ) : \n                valid = 1 \n        if not valid : \n            raise InvalidArgument ( 'Invalid parameter: %s, %s expected' % ( args [ i ] , fmtMap [ fmt . split ( ',' ) [ 0 ] ] ) ) "}
{"3545": "\ndef dsync_handler ( self , args ) : \n    self . opt . recursive = 1 \n    self . opt . sync_check = 1 \n    self . opt . force = 1 \n    self . validate ( 'cmd|s3,local|s3,local' , args ) \n    source = args [ 1 ] \n    target = args [ 2 ] \n    self . s3handler ( ) . dsync_files ( source , target ) "}
{"3547": "\ndef mv_handler ( self , args ) : \n    self . validate ( 'cmd|s3|s3' , args ) \n    source = args [ 1 ] \n    target = args [ 2 ] \n    self . s3handler ( ) . cp_files ( source , target , delete_source = 1 ) "}
{"3555": "\ndef discover_gateways ( self ) : \n    _socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) \n    _socket . settimeout ( 5.0 ) \n    if self . _interface != 'any' : \n        _socket . bind ( ( self . _interface , 0 ) ) \n    for gateway in self . _gateways_config : \n        host = gateway . get ( 'host' ) \n        port = gateway . get ( 'port' ) \n        sid = gateway . get ( 'sid' ) \n        if not ( host and port and sid ) : \n            continue \n        try : \n            ip_address = socket . gethostbyname ( host ) \n            if gateway . get ( 'disable' ) : \n                _LOGGER . info ( 'Xiaomi Gateway %s is disabled by configuration' , sid ) \n                self . disabled_gateways . append ( ip_address ) \n                continue \n            _LOGGER . info ( 'Xiaomi Gateway %s configured at IP %s:%s' , sid , ip_address , port ) \n            self . gateways [ ip_address ] = XiaomiGateway ( ip_address , port , sid , gateway . get ( 'key' ) , self . _device_discovery_retries , self . _interface , gateway . get ( 'proto' ) ) \n        except OSError as error : \n            _LOGGER . error ( \"Could not resolve %s: %s\" , host , error ) \n    try : \n        _socket . sendto ( '{\"cmd\":\"whois\"}' . encode ( ) , ( self . MULTICAST_ADDRESS , self . GATEWAY_DISCOVERY_PORT ) ) \n        while 1 : \n            data , ( ip_add , _ ) = _socket . recvfrom ( 1024 ) \n            if len ( data ) is None or ip_add in self . gateways : \n                continue \n            if ip_add in self . gateways . keys ( ) or ip_add in self . disabled_gateways : \n                continue \n            resp = json . loads ( data . decode ( ) ) \n            if resp [ \"cmd\" ] != 'iam' : \n                _LOGGER . error ( \"Response does not match return cmd\" ) \n                continue \n            if resp [ \"model\" ] not in GATEWAY_MODELS : \n                _LOGGER . error ( \"Response must be gateway model\" ) \n                continue \n            disabled = 0 \n            gateway_key = None \n            for gateway in self . _gateways_config : \n                sid = gateway . get ( 'sid' ) \n                if sid is None or sid == resp [ \"sid\" ] : \n                    gateway_key = gateway . get ( 'key' ) \n                if sid and sid == resp [ 'sid' ] and gateway . get ( 'disable' ) : \n                    disabled = 1 \n            sid = resp [ \"sid\" ] \n            if disabled : \n                _LOGGER . info ( \"Xiaomi Gateway %s is disabled by configuration\" , sid ) \n                self . disabled_gateways . append ( ip_add ) \n            else : \n                _LOGGER . info ( 'Xiaomi Gateway %s found at IP %s' , sid , ip_add ) \n                self . gateways [ ip_add ] = XiaomiGateway ( ip_add , resp [ \"port\" ] , sid , gateway_key , self . _device_discovery_retries , self . _interface , resp [ \"proto_version\" ] if \"proto_version\" in resp else None ) \n    except socket . timeout : \n        _LOGGER . info ( \"Gateway discovery finished in 5 seconds\" ) \n        _socket . close ( ) "}
{"3556": "\ndef listen ( self ) : \n    _LOGGER . info ( 'Creating Multicast Socket' ) \n    self . _mcastsocket = self . _create_mcast_socket ( ) \n    self . _listening = 1 \n    thread = Thread ( target = self . _listen_to_msg , args = ( ) ) \n    self . _threads . append ( thread ) \n    thread . daemon = 1 \n    thread . start ( ) "}
{"3558": "\ndef push_data ( self , data ) : \n    if not _validate_data ( data ) : \n        return 0 \n    jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else _list2map ( data [ 'params' ] ) \n    if jdata is None : \n        return 0 \n    sid = data [ 'sid' ] \n    for func in self . callbacks [ sid ] : \n        func ( jdata , data ) \n    return 1 "}
{"3560": "\ndef exception_handler ( job , * exc_info ) : \n    job_info = job . to_dict ( ) \n    job_info [ 'data' ] = repr ( job_info [ 'data' ] ) \n    extra_data = { 'job' : job_info } \n    payload_data = { 'framework' : 'rq' } \n    rollbar . report_exc_info ( exc_info , extra_data = extra_data , payload_data = payload_data ) \n    return 1 "}
{"3561": "\ndef includeme ( config ) : \n    settings = config . registry . settings \n    config . add_tween ( 'rollbar.contrib.pyramid.rollbar_tween_factory' , over = EXCVIEW ) \n    if asbool ( settings . get ( 'rollbar.patch_debugtoolbar' , 1 ) ) : \n        patch_debugtoolbar ( settings ) \n    def hook ( request , data ) : \n        data [ 'framework' ] = 'pyramid' \n        if request : \n            request . environ [ 'rollbar.uuid' ] = data [ 'uuid' ] \n            if request . matched_route : \n                data [ 'context' ] = request . matched_route . name \n    rollbar . BASE_DATA_HOOK = hook \n    kw = parse_settings ( settings ) \n    access_token = kw . pop ( 'access_token' ) \n    environment = kw . pop ( 'environment' , 'production' ) \n    if kw . get ( 'scrub_fields' ) : \n        kw [ 'scrub_fields' ] = set ( [ str . strip ( x ) for x in kw . get ( 'scrub_fields' ) . split ( '\\n' ) if x ] ) \n    if kw . get ( 'exception_level_filters' ) : \n        r = DottedNameResolver ( ) \n        exception_level_filters = [ ] \n        for line in kw . get ( 'exception_level_filters' ) . split ( '\\n' ) : \n            if line : \n                dotted_path , level = line . split ( ) \n                try : \n                    cls = r . resolve ( dotted_path ) \n                    exception_level_filters . append ( ( cls , level ) ) \n                except ImportError : \n                    log . error ( 'Could not import %r' % dotted_path ) \n        kw [ 'exception_level_filters' ] = exception_level_filters \n    kw [ 'enabled' ] = asbool ( kw . get ( 'enabled' , 1 ) ) \n    rollbar . init ( access_token , environment , ** kw ) "}
{"3564": "\ndef init ( access_token , environment = 'production' , scrub_fields = None , url_fields = None , ** kw ) : \n    global SETTINGS , agent_log , _initialized , _transforms , _serialize_transform , _threads \n    if scrub_fields is not None : \n        SETTINGS [ 'scrub_fields' ] = list ( scrub_fields ) \n    if url_fields is not None : \n        SETTINGS [ 'url_fields' ] = list ( url_fields ) \n    SETTINGS = dict_merge ( SETTINGS , kw ) \n    if _initialized : \n        if not SETTINGS . get ( 'suppress_reinit_warning' ) : \n            log . warning ( 'Rollbar already initialized. Ignoring re-init.' ) \n        return \n    SETTINGS [ 'access_token' ] = access_token \n    SETTINGS [ 'environment' ] = environment \n    if SETTINGS . get ( 'allow_logging_basic_config' ) : \n        logging . basicConfig ( ) \n    if SETTINGS . get ( 'handler' ) == 'agent' : \n        agent_log = _create_agent_log ( ) \n    _serialize_transform = SerializableTransform ( safe_repr = SETTINGS [ 'locals' ] [ 'safe_repr' ] , whitelist_types = SETTINGS [ 'locals' ] [ 'whitelisted_types' ] ) \n    _transforms = [ ScrubRedactTransform ( ) , _serialize_transform , ScrubTransform ( suffixes = [ ( field , ) for field in SETTINGS [ 'scrub_fields' ] ] , redact_char = '*' ) , ScrubUrlTransform ( suffixes = [ ( field , ) for field in SETTINGS [ 'url_fields' ] ] , params_to_scrub = SETTINGS [ 'scrub_fields' ] ) ] \n    shortener_keys = [ ( 'request' , 'POST' ) , ( 'request' , 'json' ) , ( 'body' , 'request' , 'POST' ) , ( 'body' , 'request' , 'json' ) , ] \n    if SETTINGS [ 'locals' ] [ 'enabled' ] : \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'code' ) ) \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'args' , '*' ) ) \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'kwargs' , '*' ) ) \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'locals' , '*' ) ) \n    shortener_keys . extend ( SETTINGS [ 'shortener_keys' ] ) \n    shortener = ShortenerTransform ( safe_repr = SETTINGS [ 'locals' ] [ 'safe_repr' ] , keys = shortener_keys , ** SETTINGS [ 'locals' ] [ 'sizes' ] ) \n    _transforms . append ( shortener ) \n    _threads = queue . Queue ( ) \n    events . reset ( ) \n    filters . add_builtin_filters ( SETTINGS ) \n    _initialized = 1 "}
{"3582": "\ndef is_inside_lambda ( node : astroid . node_classes . NodeNG ) -> bool : \n    parent = node . parent \n    while parent is not None : \n        if isinstance ( parent , astroid . Lambda ) : \n            return 1 \n        parent = parent . parent \n    return 0 "}
{"3584": "\ndef clobber_in_except ( node : astroid . node_classes . NodeNG ) -> Tuple [ bool , Tuple [ str , str ] ] : \n    if isinstance ( node , astroid . AssignAttr ) : \n        return 1 , ( node . attrname , \"object %r\" % ( node . expr . as_string ( ) , ) ) \n    if isinstance ( node , astroid . AssignName ) : \n        name = node . name \n        if is_builtin ( name ) : \n            return ( 1 , ( name , \"builtins\" ) ) \n        stmts = node . lookup ( name ) [ 1 ] \n        if stmts and not isinstance ( stmts [ 0 ] . assign_type ( ) , ( astroid . Assign , astroid . AugAssign , astroid . ExceptHandler ) , ) : \n            return 1 , ( name , \"outer scope (line %s)\" % stmts [ 0 ] . fromlineno ) \n    return 0 , None "}
{"3585": "\ndef is_super ( node : astroid . node_classes . NodeNG ) -> bool : \n    if getattr ( node , \"name\" , None ) == \"super\" and node . root ( ) . name == BUILTINS_NAME : \n        return 1 \n    return 0 "}
{"3586": "\ndef is_error ( node : astroid . node_classes . NodeNG ) -> bool : \n    for child_node in node . get_children ( ) : \n        if isinstance ( child_node , astroid . Raise ) : \n            return 1 \n    return 0 "}
{"3587": "\ndef is_default_argument ( node : astroid . node_classes . NodeNG ) -> bool : \n    parent = node . scope ( ) \n    if isinstance ( parent , ( astroid . FunctionDef , astroid . Lambda ) ) : \n        for default_node in parent . args . defaults : \n            for default_name_node in default_node . nodes_of_class ( astroid . Name ) : \n                if default_name_node is node : \n                    return 1 \n    return 0 "}
{"3588": "\ndef is_func_decorator ( node : astroid . node_classes . NodeNG ) -> bool : \n    parent = node . parent \n    while parent is not None : \n        if isinstance ( parent , astroid . Decorators ) : \n            return 1 \n        if parent . is_statement or isinstance ( parent , ( astroid . Lambda , scoped_nodes . ComprehensionScope , scoped_nodes . ListComp ) , ) : \n            break \n        parent = parent . parent \n    return 0 "}
{"3589": "\ndef is_ancestor_name ( frame : astroid . node_classes . NodeNG , node : astroid . node_classes . NodeNG ) -> bool : \n    try : \n        bases = frame . bases \n    except AttributeError : \n        return 0 \n    for base in bases : \n        if node in base . nodes_of_class ( astroid . Name ) : \n            return 1 \n    return 0 "}
{"3594": "\ndef inherit_from_std_ex ( node : astroid . node_classes . NodeNG ) -> bool : \n    ancestors = node . ancestors ( ) if hasattr ( node , \"ancestors\" ) else [ ] \n    for ancestor in itertools . chain ( [ node ] , ancestors ) : \n        if ( ancestor . name in ( \"Exception\" , \"BaseException\" ) and ancestor . root ( ) . name == EXCEPTIONS_MODULE ) : \n            return 1 \n    return 0 "}
{"3595": "\ndef error_of_type ( handler : astroid . ExceptHandler , error_type ) -> bool : \n    def stringify_error ( error ) : \n        if not isinstance ( error , str ) : \n            return error . __name__ \n        return error \n    if not isinstance ( error_type , tuple ) : \n        error_type = ( error_type , ) \n    expected_errors = { stringify_error ( error ) for error in error_type } \n    if not handler . type : \n        return 1 \n    return handler . catch ( expected_errors ) "}
{"3596": "\ndef decorated_with_property ( node : astroid . FunctionDef ) -> bool : \n    if not node . decorators : \n        return 0 \n    for decorator in node . decorators . nodes : \n        if not isinstance ( decorator , astroid . Name ) : \n            continue \n        try : \n            if _is_property_decorator ( decorator ) : \n                return 1 \n        except astroid . InferenceError : \n            pass \n    return 0 "}
{"3597": "\ndef decorated_with ( func : astroid . FunctionDef , qnames : Iterable [ str ] ) -> bool : \n    decorators = func . decorators . nodes if func . decorators else [ ] \n    for decorator_node in decorators : \n        try : \n            if any ( i is not None and i . qname ( ) in qnames for i in decorator_node . infer ( ) ) : \n                return 1 \n        except astroid . InferenceError : \n            continue \n    return 0 "}
{"3599": "\ndef is_from_fallback_block ( node : astroid . node_classes . NodeNG ) -> bool : \n    context = find_try_except_wrapper_node ( node ) \n    if not context : \n        return 0 \n    if isinstance ( context , astroid . ExceptHandler ) : \n        other_body = context . parent . body \n        handlers = context . parent . handlers \n    else : \n        other_body = itertools . chain . from_iterable ( handler . body for handler in context . handlers ) \n        handlers = context . handlers \n    has_fallback_imports = any ( isinstance ( import_node , ( astroid . ImportFrom , astroid . Import ) ) for import_node in other_body ) \n    ignores_import_error = _except_handlers_ignores_exception ( handlers , ImportError ) \n    return ignores_import_error or has_fallback_imports "}
{"3601": "\ndef node_ignores_exception ( node : astroid . node_classes . NodeNG , exception = Exception ) -> bool : \n    managing_handlers = get_exception_handlers ( node , exception ) \n    if not managing_handlers : \n        return 0 \n    return any ( managing_handlers ) "}
{"3602": "\ndef class_is_abstract ( node : astroid . ClassDef ) -> bool : \n    for method in node . methods ( ) : \n        if method . parent . frame ( ) is node : \n            if method . is_abstract ( pass_is_abstract = 0 ) : \n                return 1 \n    return 0 "}
{"3605": "\ndef is_registered_in_singledispatch_function ( node : astroid . FunctionDef ) -> bool : \n    singledispatch_qnames = ( \"functools.singledispatch\" , \"singledispatch.singledispatch\" , ) \n    if not isinstance ( node , astroid . FunctionDef ) : \n        return 0 \n    decorators = node . decorators . nodes if node . decorators else [ ] \n    for decorator in decorators : \n        if not isinstance ( decorator , astroid . Call ) : \n            continue \n        func = decorator . func \n        if not isinstance ( func , astroid . Attribute ) or func . attrname != \"register\" : \n            continue \n        try : \n            func_def = next ( func . expr . infer ( ) ) \n        except astroid . InferenceError : \n            continue \n        if isinstance ( func_def , astroid . FunctionDef ) : \n            return decorated_with ( func_def , singledispatch_qnames ) \n    return 0 "}
{"3616": "\ndef _check_relative_import ( self , modnode , importnode , importedmodnode , importedasname ) : \n    if not self . linter . is_message_enabled ( \"relative-import\" ) : \n        return None \n    if importedmodnode . file is None : \n        return 0 \n    if modnode is importedmodnode : \n        return 0 \n    if modnode . absolute_import_activated ( ) or getattr ( importnode , \"level\" , None ) : \n        return 0 \n    if importedmodnode . name != importedasname : \n        self . add_message ( \"relative-import\" , args = ( importedasname , importedmodnode . name ) , node = importnode , ) \n        return None \n    return None "}
{"3631": "\ndef handle_message ( self , msg ) : \n    self . messages . append ( { \"type\" : msg . category , \"module\" : msg . module , \"obj\" : msg . obj , \"line\" : msg . line , \"column\" : msg . column , \"path\" : msg . path , \"symbol\" : msg . symbol , \"message\" : html . escape ( msg . msg or \"\" , quote = 0 ) , \"message-id\" : msg . msg_id , } ) "}
{"3635": "\ndef show_node ( self , node ) : \n    if self . config . show_builtin : \n        return 1 \n    return node . root ( ) . name != BUILTINS_NAME "}
{"3637": "\ndef get_ancestors ( self , node , level ) : \n    if level == 0 : \n        return \n    for ancestor in node . ancestors ( recurs = 0 ) : \n        if not self . show_node ( ancestor ) : \n            continue \n        yield ancestor "}
{"3644": "\ndef _is_owner_ignored ( owner , name , ignored_classes , ignored_modules ) : \n    ignored_modules = set ( ignored_modules ) \n    module_name = owner . root ( ) . name \n    module_qname = owner . root ( ) . qname ( ) \n    if any ( module_name in ignored_modules or module_qname in ignored_modules or fnmatch . fnmatch ( module_qname , ignore ) for ignore in ignored_modules ) : \n        return 1 \n    ignored_classes = set ( ignored_classes ) \n    if hasattr ( owner , \"qname\" ) : \n        qname = owner . qname ( ) \n    else : \n        qname = \"\" \n    return any ( ignore in ( name , qname ) for ignore in ignored_classes ) "}
{"3646": "\ndef _emit_no_member ( node , owner , owner_name , ignored_mixins = 1 , ignored_none = 1 ) : \n    if node_ignores_exception ( node , AttributeError ) : \n        return 0 \n    if ignored_none and isinstance ( owner , astroid . Const ) and owner . value is None : \n        return 0 \n    if is_super ( owner ) or getattr ( owner , \"type\" , None ) == \"metaclass\" : \n        return 0 \n    if ignored_mixins and owner_name [ - 5 : ] . lower ( ) == \"mixin\" : \n        return 0 \n    if isinstance ( owner , astroid . FunctionDef ) and owner . decorators : \n        return 0 \n    if isinstance ( owner , ( astroid . Instance , astroid . ClassDef ) ) : \n        if owner . has_dynamic_getattr ( ) : \n            try : \n                metaclass = owner . metaclass ( ) \n            except exceptions . MroError : \n                return 0 \n            if metaclass : \n                return metaclass . qname ( ) == \"enum.EnumMeta\" \n            return 0 \n        if not has_known_bases ( owner ) : \n            return 0 \n    if isinstance ( owner , objects . Super ) : \n        try : \n            owner . super_mro ( ) \n        except ( exceptions . MroError , exceptions . SuperError ) : \n            return 0 \n        if not all ( map ( has_known_bases , owner . type . mro ( ) ) ) : \n            return 0 \n    if isinstance ( owner , astroid . Module ) : \n        try : \n            owner . getattr ( \"__getattr__\" ) \n            return 0 \n        except astroid . NotFoundError : \n            pass \n    if node . attrname . startswith ( \"_\" + owner_name ) : \n        unmangled_name = node . attrname . split ( \"_\" + owner_name ) [ - 1 ] \n        try : \n            if owner . getattr ( unmangled_name , context = None ) is not None : \n                return 0 \n        except astroid . NotFoundError : \n            return 1 \n    return 1 "}
{"3649": "\ndef _no_context_variadic ( node , variadic_name , variadic_type , variadics ) : \n    statement = node . statement ( ) \n    for name in statement . nodes_of_class ( astroid . Name ) : \n        if name . name != variadic_name : \n            continue \n        inferred = safe_infer ( name ) \n        if isinstance ( inferred , ( astroid . List , astroid . Tuple ) ) : \n            length = len ( inferred . elts ) \n        elif isinstance ( inferred , astroid . Dict ) : \n            length = len ( inferred . items ) \n        else : \n            continue \n        inferred_statement = inferred . statement ( ) \n        if not length and isinstance ( inferred_statement , astroid . FunctionDef ) : \n            is_in_starred_context = _has_parent_of_type ( node , variadic_type , statement ) \n            used_as_starred_argument = _is_name_used_as_variadic ( name , variadics ) \n            if is_in_starred_context or used_as_starred_argument : \n                return 1 \n    return 0 "}
{"3651": "\ndef visit_assign ( self , node ) : \n    if not isinstance ( node . value , astroid . Call ) : \n        return \n    function_node = safe_infer ( node . value . func ) \n    funcs = ( astroid . FunctionDef , astroid . UnboundMethod , astroid . BoundMethod ) \n    if not ( isinstance ( function_node , funcs ) and function_node . root ( ) . fully_defined ( ) and not function_node . decorators ) : \n        return \n    if ( function_node . is_generator ( ) or function_node . is_abstract ( pass_is_abstract = 0 ) or isinstance ( function_node , astroid . AsyncFunctionDef ) ) : \n        return \n    returns = list ( function_node . nodes_of_class ( astroid . Return , skip_klass = astroid . FunctionDef ) ) \n    if not returns : \n        self . add_message ( \"assignment-from-no-return\" , node = node ) \n    else : \n        for rnode in returns : \n            if not ( isinstance ( rnode . value , astroid . Const ) and rnode . value . value is None or rnode . value is None ) : \n                break \n        else : \n            self . add_message ( \"assignment-from-none\" , node = node ) "}
{"3654": "\ndef interfaces ( node , herited = 1 , handler_func = _iface_hdlr ) : \n    try : \n        implements = bases . Instance ( node ) . getattr ( \"__implements__\" ) [ 0 ] \n    except exceptions . NotFoundError : \n        return \n    if not herited and implements . frame ( ) is not node : \n        return \n    found = set ( ) \n    missing = 0 \n    for iface in node_classes . unpack_infer ( implements ) : \n        if iface is astroid . Uninferable : \n            missing = 1 \n            continue \n        if iface not in found and handler_func ( iface ) : \n            found . add ( iface ) \n            yield iface \n    if missing : \n        raise exceptions . InferenceError ( ) "}
{"3658": "\ndef visit_assignname ( self , node ) : \n    if hasattr ( node , \"_handled\" ) : \n        return \n    node . _handled = 1 \n    if node . name in node . frame ( ) : \n        frame = node . frame ( ) \n    else : \n        frame = node . root ( ) \n    try : \n        if not hasattr ( frame , \"locals_type\" ) : \n            if isinstance ( frame , astroid . ClassDef ) : \n                self . visit_classdef ( frame ) \n            elif isinstance ( frame , astroid . FunctionDef ) : \n                self . visit_functiondef ( frame ) \n            else : \n                self . visit_module ( frame ) \n        current = frame . locals_type [ node . name ] \n        values = set ( node . infer ( ) ) \n        frame . locals_type [ node . name ] = list ( set ( current ) | values ) \n    except astroid . InferenceError : \n        pass "}
{"3661": "\ndef visit_importfrom ( self , node ) : \n    basename = node . modname \n    context_file = node . root ( ) . file \n    if context_file is not None : \n        relative = modutils . is_relative ( basename , context_file ) \n    else : \n        relative = 0 \n    for name in node . names : \n        if name [ 0 ] == \"*\" : \n            continue \n        fullname = \"%s.%s\" % ( basename , name [ 0 ] ) \n        if fullname . find ( \".\" ) > - 1 : \n            try : \n                fullname = modutils . get_module_part ( fullname , context_file ) \n            except ImportError : \n                continue \n        if fullname != basename : \n            self . _imported_module ( node , fullname , relative ) "}
{"3673": "\ndef _check_new_format ( self , node , func ) : \n    if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : \n        return \n    if node . starargs or node . kwargs : \n        return \n    try : \n        strnode = next ( func . bound . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : \n        return \n    try : \n        call_site = CallSite . from_call ( node ) \n    except astroid . InferenceError : \n        return \n    try : \n        fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) \n    except utils . IncompleteFormatString : \n        self . add_message ( \"bad-format-string\" , node = node ) \n        return \n    positional_arguments = call_site . positional_arguments \n    named_arguments = call_site . keyword_arguments \n    named_fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } \n    if num_args and manual_pos : \n        self . add_message ( \"format-combined-specification\" , node = node ) \n        return \n    check_args = 0 \n    num_args += sum ( 1 for field in named_fields if field == \"\" ) \n    if named_fields : \n        for field in named_fields : \n            if field and field not in named_arguments : \n                self . add_message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) \n        for field in named_arguments : \n            if field not in named_fields : \n                self . add_message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) \n        num_args = num_args or manual_pos \n        if positional_arguments or num_args : \n            empty = any ( 1 for field in named_fields if field == \"\" ) \n            if named_arguments or empty : \n                check_args = 1 \n    else : \n        check_args = 1 \n    if check_args : \n        num_args = num_args or manual_pos \n        if len ( positional_arguments ) > num_args : \n            self . add_message ( \"too-many-format-args\" , node = node ) \n        elif len ( positional_arguments ) < num_args : \n            self . add_message ( \"too-few-format-args\" , node = node ) \n    self . _detect_vacuous_formatting ( node , positional_arguments ) \n    self . _check_new_format_specifiers ( node , fields , named_arguments ) "}
{"3674": "\ndef process_non_raw_string_token ( self , prefix , string_body , start_row ) : \n    i = 0 \n    while 1 : \n        i = string_body . find ( \"\\\\\" , i ) \n        if i == - 1 : \n            break \n        next_char = string_body [ i + 1 ] \n        match = string_body [ i : i + 2 ] \n        if next_char in self . UNICODE_ESCAPE_CHARACTERS : \n            if \"u\" in prefix : \n                pass \n            elif ( _PY3K or self . _unicode_literals ) and \"b\" not in prefix : \n                pass \n            else : \n                self . add_message ( \"anomalous-unicode-escape-in-string\" , line = start_row , args = ( match , ) , ) \n        elif next_char not in self . ESCAPE_CHARACTERS : \n            self . add_message ( \"anomalous-backslash-in-string\" , line = start_row , args = ( match , ) ) \n        i += 2 "}
{"3687": "\ndef help_message ( self , msgids ) : \n    for msgid in msgids : \n        try : \n            for message_definition in self . get_message_definitions ( msgid ) : \n                print ( message_definition . format_help ( checkerref = 1 ) ) \n                print ( \"\" ) \n        except UnknownMessageError as ex : \n            print ( ex ) \n            print ( \"\" ) \n            continue "}
{"3688": "\ndef list_messages ( self ) : \n    messages = sorted ( self . _messages_definitions . values ( ) , key = lambda m : m . msgid ) \n    for message in messages : \n        if not message . may_be_emitted ( ) : \n            continue \n        print ( message . format_help ( checkerref = 0 ) ) \n    print ( \"\" ) "}
{"3695": "\ndef set_option ( self , optname , value , action = None , optdict = None ) : \n    if optname in self . _options_methods or optname in self . _bw_options_methods : \n        if value : \n            try : \n                meth = self . _options_methods [ optname ] \n            except KeyError : \n                meth = self . _bw_options_methods [ optname ] \n                warnings . warn ( \"%s is deprecated, replace it by %s\" % ( optname , optname . split ( \"-\" ) [ 0 ] ) , DeprecationWarning , ) \n            value = utils . _check_csv ( value ) \n            if isinstance ( value , ( list , tuple ) ) : \n                for _id in value : \n                    meth ( _id , ignore_unknown = 1 ) \n            else : \n                meth ( value ) \n            return \n    elif optname == \"output-format\" : \n        self . _reporter_name = value \n        if self . _reporters : \n            self . _load_reporter ( ) \n    try : \n        checkers . BaseTokenChecker . set_option ( self , optname , value , action , optdict ) \n    except config . UnsupportedAction : \n        print ( \"option %s can't be read from config file\" % optname , file = sys . stderr ) "}
{"3696": "\ndef register_checker ( self , checker ) : \n    assert checker . priority <= 0 , \"checker priority can't be >= 0\" \n    self . _checkers [ checker . name ] . append ( checker ) \n    for r_id , r_title , r_cb in checker . reports : \n        self . register_report ( r_id , r_title , r_cb , checker ) \n    self . register_options_provider ( checker ) \n    if hasattr ( checker , \"msgs\" ) : \n        self . msgs_store . register_messages_from_checker ( checker ) \n    checker . load_defaults ( ) \n    if not getattr ( checker , \"enabled\" , 1 ) : \n        self . disable ( checker . name ) "}
{"3698": "\ndef python3_porting_mode ( self ) : \n    self . disable ( \"all\" ) \n    self . enable ( \"python3\" ) \n    if self . _error_mode : \n        for msg_id in self . _checker_messages ( \"python3\" ) : \n            if msg_id . startswith ( \"E\" ) : \n                self . enable ( msg_id ) \n            else : \n                self . disable ( msg_id ) \n    config_parser = self . cfgfile_parser \n    if config_parser . has_option ( \"MESSAGES CONTROL\" , \"disable\" ) : \n        value = config_parser . get ( \"MESSAGES CONTROL\" , \"disable\" ) \n        self . global_set_option ( \"disable\" , value ) \n    self . _python3_porting_mode = 1 "}
{"3701": "\ndef prepare_checkers ( self ) : \n    if not self . config . reports : \n        self . disable_reporters ( ) \n    neededcheckers = [ self ] \n    for checker in self . get_checkers ( ) [ 1 : ] : \n        messages = { msg for msg in checker . msgs if self . is_message_enabled ( msg ) } \n        if messages or any ( self . report_is_enabled ( r [ 0 ] ) for r in checker . reports ) : \n            neededcheckers . append ( checker ) \n    neededcheckers = sorted ( neededcheckers , key = operator . attrgetter ( \"priority\" ) , reverse = 1 ) \n    return neededcheckers "}
{"3704": "\ndef check_astroid_module ( self , ast_node , walker , rawcheckers , tokencheckers ) : \n    try : \n        tokens = utils . tokenize_module ( ast_node ) \n    except tokenize . TokenError as ex : \n        self . add_message ( \"syntax-error\" , line = ex . args [ 1 ] [ 0 ] , args = ex . args [ 0 ] ) \n        return None \n    if not ast_node . pure_python : \n        self . add_message ( \"raw-checker-failed\" , args = ast_node . name ) \n    else : \n        self . process_tokens ( tokens ) \n        if self . _ignore_file : \n            return 0 \n        self . file_state . collect_block_lines ( self . msgs_store , ast_node ) \n        for checker in rawcheckers : \n            checker . process_module ( ast_node ) \n        for checker in tokencheckers : \n            checker . process_tokens ( tokens ) \n    walker . walk ( ast_node ) \n    return 1 "}
{"3713": "\ndef _basename_in_blacklist_re ( base_name , black_list_re ) : \n    for file_pattern in black_list_re : \n        if file_pattern . match ( base_name ) : \n            return 1 \n    return 0 "}
{"3732": "\ndef returns_something ( return_node ) : \n    returns = return_node . value \n    if returns is None : \n        return 0 \n    return not ( isinstance ( returns , astroid . Const ) and returns . value is None ) "}
{"3737": "\ndef _is_from_future_import ( stmt , name ) : \n    try : \n        module = stmt . do_import_module ( stmt . modname ) \n    except astroid . AstroidBuildingException : \n        return None \n    for local_node in module . locals . get ( name , [ ] ) : \n        if isinstance ( local_node , astroid . ImportFrom ) and local_node . modname == FUTURE : \n            return 1 \n    return None "}
{"3741": "\ndef _detect_global_scope ( node , frame , defframe ) : \n    def_scope = scope = None \n    if frame and frame . parent : \n        scope = frame . parent . scope ( ) \n    if defframe and defframe . parent : \n        def_scope = defframe . parent . scope ( ) \n    if isinstance ( frame , astroid . FunctionDef ) : \n        if not isinstance ( node . parent , ( astroid . FunctionDef , astroid . Arguments ) ) : \n            return 0 \n    elif any ( not isinstance ( f , ( astroid . ClassDef , astroid . Module ) ) for f in ( frame , defframe ) ) : \n        return 0 \n    break_scopes = [ ] \n    for s in ( scope , def_scope ) : \n        parent_scope = s \n        while parent_scope : \n            if not isinstance ( parent_scope , ( astroid . ClassDef , astroid . Module ) ) : \n                break_scopes . append ( parent_scope ) \n                break \n            if parent_scope . parent : \n                parent_scope = parent_scope . parent . scope ( ) \n            else : \n                break \n    if break_scopes and len ( set ( break_scopes ) ) != 1 : \n        return 0 \n    return frame . lineno < defframe . lineno "}
{"3744": "\ndef visit_global ( self , node ) : \n    frame = node . frame ( ) \n    if isinstance ( frame , astroid . Module ) : \n        self . add_message ( \"global-at-module-level\" , node = node ) \n        return \n    module = frame . root ( ) \n    default_message = 1 \n    locals_ = node . scope ( ) . locals \n    for name in node . names : \n        try : \n            assign_nodes = module . getattr ( name ) \n        except astroid . NotFoundError : \n            assign_nodes = [ ] \n        not_defined_locally_by_import = not any ( isinstance ( local , astroid . node_classes . Import ) for local in locals_ . get ( name , ( ) ) ) \n        if not assign_nodes and not_defined_locally_by_import : \n            self . add_message ( \"global-variable-not-assigned\" , args = name , node = node ) \n            default_message = 0 \n            continue \n        for anode in assign_nodes : \n            if ( isinstance ( anode , astroid . AssignName ) and anode . name in module . special_attributes ) : \n                self . add_message ( \"redefined-builtin\" , args = name , node = node ) \n                break \n            if anode . frame ( ) is module : \n                break \n        else : \n            if not_defined_locally_by_import : \n                self . add_message ( \"global-variable-undefined\" , args = name , node = node ) \n                default_message = 0 \n    if default_message : \n        self . add_message ( \"global-statement\" , node = node ) "}
{"3746": "\ndef _has_homonym_in_upper_function_scope ( self , node , index ) : \n    for _consumer in self . _to_consume [ index - 1 : : - 1 ] : \n        if _consumer . scope_type == \"function\" and node . name in _consumer . to_consume : \n            return 1 \n    return 0 "}
{"3751": "\ndef run ( self ) : \n    install_lib . install_lib . run ( self ) \n    if include_dirs : \n        for directory in include_dirs : \n            dest = join ( self . install_dir , directory ) \n            if sys . version_info >= ( 3 , 0 ) : \n                exclude = { \"invalid_encoded_data*\" , \"unknown_encoding*\" } \n            else : \n                exclude = set ( ) \n            shutil . rmtree ( dest , ignore_errors = 1 ) \n            shutil . copytree ( directory , dest , ignore = shutil . ignore_patterns ( * exclude ) ) "}
{"3753": "\ndef Run ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    from getopt import getopt \n    s_opts = \"hdi\" \n    l_opts = ( \"help\" , \"duplicates=\" , \"ignore-comments\" , \"ignore-imports\" , \"ignore-docstrings\" , ) \n    min_lines = 4 \n    ignore_comments = 0 \n    ignore_docstrings = 0 \n    ignore_imports = 0 \n    opts , args = getopt ( argv , s_opts , l_opts ) \n    for opt , val in opts : \n        if opt in ( \"-d\" , \"--duplicates\" ) : \n            min_lines = int ( val ) \n        elif opt in ( \"-h\" , \"--help\" ) : \n            usage ( ) \n        elif opt in ( \"-i\" , \"--ignore-comments\" ) : \n            ignore_comments = 1 \n        elif opt in ( \"--ignore-docstrings\" , ) : \n            ignore_docstrings = 1 \n        elif opt in ( \"--ignore-imports\" , ) : \n            ignore_imports = 1 \n    if not args : \n        usage ( 1 ) \n    sim = Similar ( min_lines , ignore_comments , ignore_docstrings , ignore_imports ) \n    for filename in args : \n        with open ( filename ) as stream : \n            sim . append_stream ( filename , stream ) \n    sim . run ( ) \n    sys . exit ( 0 ) "}
{"3761": "\ndef _definition_equivalent_to_call ( definition , call ) : \n    if definition . kwargs : \n        same_kw_variadics = definition . kwargs in call . starred_kws \n    else : \n        same_kw_variadics = not call . starred_kws \n    if definition . varargs : \n        same_args_variadics = definition . varargs in call . starred_args \n    else : \n        same_args_variadics = not call . starred_args \n    same_kwonlyargs = all ( kw in call . kws for kw in definition . kwonlyargs ) \n    same_args = definition . args == call . args \n    no_additional_kwarg_arguments = 1 \n    if call . kws : \n        for keyword in call . kws : \n            is_arg = keyword in call . args \n            is_kwonly = keyword in definition . kwonlyargs \n            if not is_arg and not is_kwonly : \n                no_additional_kwarg_arguments = 0 \n                break \n    return all ( ( same_args , same_kwonlyargs , same_args_variadics , same_kw_variadics , no_additional_kwarg_arguments , ) ) "}
{"3763": "\ndef _has_different_parameters_default_value ( original , overridden ) : \n    if original . args is None or overridden . args is None : \n        return 0 \n    all_args = chain ( original . args , original . kwonlyargs ) \n    original_param_names = [ param . name for param in all_args ] \n    default_missing = object ( ) \n    for param_name in original_param_names : \n        try : \n            original_default = original . default_value ( param_name ) \n        except astroid . exceptions . NoDefault : \n            original_default = default_missing \n        try : \n            overridden_default = overridden . default_value ( param_name ) \n        except astroid . exceptions . NoDefault : \n            overridden_default = default_missing \n        default_list = [ arg == default_missing for arg in ( original_default , overridden_default ) ] \n        if any ( default_list ) and not all ( default_list ) : \n            return 1 \n        astroid_type_compared_attr = { astroid . Const : \"value\" , astroid . ClassDef : \"name\" , astroid . Tuple : \"elts\" , astroid . List : \"elts\" , } \n        handled_types = tuple ( astroid_type for astroid_type in astroid_type_compared_attr ) \n        original_type = _get_node_type ( original_default , handled_types ) \n        if original_type : \n            if not isinstance ( overridden_default , original_type ) : \n                return 1 \n            if not _check_arg_equality ( original_default , overridden_default , astroid_type_compared_attr [ original_type ] , ) : \n                return 1 \n    return 0 "}
{"3764": "\ndef _different_parameters ( original , overridden , dummy_parameter_regex ) : \n    original_parameters = _positional_parameters ( original ) \n    overridden_parameters = _positional_parameters ( overridden ) \n    different_positional = _has_different_parameters ( original_parameters , overridden_parameters , dummy_parameter_regex ) \n    different_kwonly = _has_different_parameters ( original . args . kwonlyargs , overridden . args . kwonlyargs , dummy_parameter_regex ) \n    if original . name in PYMETHODS : \n        different_positional = different_kwonly = 0 \n    different_kwarg = ( sum ( 1 for param in ( original . args . kwarg , overridden . args . kwarg ) if not param ) == 1 ) \n    different_vararg = ( sum ( 1 for param in ( original . args . vararg , overridden . args . vararg ) if not param ) == 1 ) \n    return any ( ( different_positional , different_kwarg , different_vararg , different_kwonly ) ) "}
{"3770": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    self . _check_useless_super_delegation ( node ) \n    klass = node . parent . frame ( ) \n    self . _meth_could_be_func = 1 \n    self . _check_first_arg_for_type ( node , klass . type == \"metaclass\" ) \n    if node . name == \"__init__\" : \n        self . _check_init ( node ) \n        return \n    for overridden in klass . local_attr_ancestors ( node . name ) : \n        try : \n            meth_node = overridden [ node . name ] \n        except KeyError : \n            continue \n        if not isinstance ( meth_node , astroid . FunctionDef ) : \n            continue \n        self . _check_signature ( node , meth_node , \"overridden\" , klass ) \n        break \n    if node . decorators : \n        for decorator in node . decorators . nodes : \n            if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"getter\" , \"setter\" , \"deleter\" , ) : \n                return \n            if isinstance ( decorator , astroid . Name ) : \n                if decorator . name == \"property\" : \n                    return \n            inferred = safe_infer ( decorator ) \n            if not inferred : \n                return \n            if isinstance ( inferred , astroid . FunctionDef ) : \n                try : \n                    inferred = next ( inferred . infer_call_result ( inferred ) ) \n                except astroid . InferenceError : \n                    return \n            try : \n                if ( isinstance ( inferred , ( astroid . Instance , astroid . ClassDef ) ) and inferred . getattr ( \"__get__\" ) and inferred . getattr ( \"__set__\" ) ) : \n                    return \n            except astroid . AttributeInferenceError : \n                pass \n    try : \n        overridden = klass . instance_attr ( node . name ) [ 0 ] \n        overridden_frame = overridden . frame ( ) \n        if ( isinstance ( overridden_frame , astroid . FunctionDef ) and overridden_frame . type == \"method\" ) : \n            overridden_frame = overridden_frame . parent . frame ( ) \n        if isinstance ( overridden_frame , astroid . ClassDef ) and klass . is_subtype_of ( overridden_frame . qname ( ) ) : \n            args = ( overridden . root ( ) . name , overridden . fromlineno ) \n            self . add_message ( \"method-hidden\" , args = args , node = node ) \n    except astroid . NotFoundError : \n        pass "}
{"3774": "\ndef visit_name ( self , node ) : \n    if self . _first_attrs and ( node . name == self . _first_attrs [ - 1 ] or not self . _first_attrs [ - 1 ] ) : \n        self . _meth_could_be_func = 0 "}
{"3776": "\ndef _check_bases_classes ( self , node ) : \n    def is_abstract ( method ) : \n        return method . is_abstract ( pass_is_abstract = 0 ) \n    if class_is_abstract ( node ) : \n        return \n    methods = sorted ( unimplemented_abstract_methods ( node , is_abstract ) . items ( ) , key = lambda item : item [ 0 ] , ) \n    for name , method in methods : \n        owner = method . parent . frame ( ) \n        if owner is node : \n            continue \n        if name in node . locals : \n            continue \n        self . add_message ( \"abstract-method\" , node = node , args = ( name , owner . name ) ) "}
{"3779": "\ndef _is_raising ( body : typing . List ) -> bool : \n    for node in body : \n        if isinstance ( node , astroid . Raise ) : \n            return 1 \n    return 0 "}
{"3783": "\ndef _is_typing_namedtuple ( node : astroid . ClassDef ) -> bool : \n    for base in node . ancestors ( ) : \n        if base . qname ( ) == TYPING_NAMEDTUPLE : \n            return 1 \n    return 0 "}
{"3784": "\ndef _is_enum_class ( node : astroid . ClassDef ) -> bool : \n    for base in node . bases : \n        try : \n            inferred_bases = base . inferred ( ) \n        except astroid . InferenceError : \n            continue \n        for ancestor in inferred_bases : \n            if not isinstance ( ancestor , astroid . ClassDef ) : \n                continue \n            if ancestor . name == \"Enum\" and ancestor . root ( ) . name == \"enum\" : \n                return 1 \n    return 0 "}
{"3785": "\ndef _is_dataclass ( node : astroid . ClassDef ) -> bool : \n    if not node . decorators : \n        return 0 \n    root_locals = node . root ( ) . locals \n    for decorator in node . decorators . nodes : \n        if isinstance ( decorator , astroid . Call ) : \n            decorator = decorator . func \n        if not isinstance ( decorator , ( astroid . Name , astroid . Attribute ) ) : \n            continue \n        if isinstance ( decorator , astroid . Name ) : \n            name = decorator . name \n        else : \n            name = decorator . attrname \n        if name == DATACLASS_DECORATOR and DATACLASS_DECORATOR in root_locals : \n            return 1 \n    return 0 "}
{"3793": "\ndef _is_trailing_comma ( tokens , index ) : \n    token = tokens [ index ] \n    if token . exact_type != tokenize . COMMA : \n        return 0 \n    left_tokens = itertools . islice ( tokens , index + 1 , None ) \n    same_line_remaining_tokens = list ( itertools . takewhile ( lambda other_token , _token = token : other_token . start [ 0 ] == _token . start [ 0 ] , left_tokens , ) ) \n    is_last_element = all ( other_token . type in ( tokenize . NEWLINE , tokenize . COMMENT ) for other_token in same_line_remaining_tokens ) \n    if not same_line_remaining_tokens or not is_last_element : \n        return 0 \n    def get_curline_index_start ( ) : \n        for subindex , token in enumerate ( reversed ( tokens [ : index ] ) ) : \n            if token . type in ( tokenize . NEWLINE , tokenize . NL ) : \n                return index - subindex \n        return 0 \n    curline_start = get_curline_index_start ( ) \n    expected_tokens = { \"return\" , \"yield\" } \n    for prevtoken in tokens [ curline_start : index ] : \n        if \"=\" in prevtoken . string or prevtoken . string in expected_tokens : \n            return 1 \n    return 0 "}
{"3794": "\ndef _is_actual_elif ( self , node ) : \n    if isinstance ( node . parent , astroid . If ) : \n        orelse = node . parent . orelse \n        if orelse and orelse == [ node ] : \n            if ( node . lineno , node . col_offset ) in self . _elifs : \n                return 1 \n    return 0 "}
{"3798": "\ndef _check_raising_stopiteration_in_generator_next_call ( self , node ) : \n    def _looks_like_infinite_iterator ( param ) : \n        inferred = utils . safe_infer ( param ) \n        if inferred : \n            return inferred . qname ( ) in KNOWN_INFINITE_ITERATORS \n        return 0 \n    if isinstance ( node . func , astroid . Attribute ) : \n        return \n    inferred = utils . safe_infer ( node . func ) \n    if getattr ( inferred , \"name\" , \"\" ) == \"next\" : \n        frame = node . frame ( ) \n        has_sentinel_value = len ( node . args ) > 1 \n        if ( isinstance ( frame , astroid . FunctionDef ) and frame . is_generator ( ) and not has_sentinel_value and not utils . node_ignores_exception ( node , StopIteration ) and not _looks_like_infinite_iterator ( node . args [ 0 ] ) ) : \n            self . add_message ( \"stop-iteration-return\" , node = node ) "}
{"3805": "\ndef _is_node_return_ended ( self , node ) : \n    if isinstance ( node , astroid . Return ) : \n        return 1 \n    if isinstance ( node , astroid . Call ) : \n        try : \n            funcdef_node = node . func . inferred ( ) [ 0 ] \n            if self . _is_function_def_never_returning ( funcdef_node ) : \n                return 1 \n        except astroid . InferenceError : \n            pass \n    if isinstance ( node , astroid . While ) : \n        return 1 \n    if isinstance ( node , astroid . Raise ) : \n        if not node . exc : \n            return 1 \n        if not utils . is_node_inside_try_except ( node ) : \n            return 1 \n        exc = utils . safe_infer ( node . exc ) \n        if exc is None or exc is astroid . Uninferable : \n            return 0 \n        exc_name = exc . pytype ( ) . split ( \".\" ) [ - 1 ] \n        handlers = utils . get_exception_handlers ( node , exc_name ) \n        handlers = list ( handlers ) if handlers is not None else [ ] \n        if handlers : \n            return any ( self . _is_node_return_ended ( _handler ) for _handler in handlers ) \n        return 1 \n    if isinstance ( node , astroid . If ) : \n        is_orelse_returning = any ( self . _is_node_return_ended ( _ore ) for _ore in node . orelse if not isinstance ( _ore , astroid . FunctionDef ) ) \n        is_if_returning = any ( self . _is_node_return_ended ( _ifn ) for _ifn in node . body if not isinstance ( _ifn , astroid . FunctionDef ) ) \n        return is_if_returning and is_orelse_returning \n    return any ( self . _is_node_return_ended ( _child ) for _child in node . get_children ( ) if not isinstance ( _child , astroid . ExceptHandler ) ) "}
{"3808": "\ndef run ( self , args ) : \n    if not args : \n        print ( self . help ( ) ) \n        return 1 \n    sys . path . insert ( 0 , os . getcwd ( ) ) \n    try : \n        project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) \n        linker = Linker ( project , tag = 1 ) \n        handler = DiadefsHandler ( self . config ) \n        diadefs = handler . get_diadefs ( project , linker ) \n    finally : \n        sys . path . pop ( 0 ) \n    if self . config . output_format == \"vcg\" : \n        writer . VCGWriter ( self . config ) . write ( diadefs ) \n    else : \n        writer . DotWriter ( self . config ) . write ( diadefs ) \n    return 0 "}
{"3813": "\ndef may_be_emitted ( self ) : \n    if self . minversion is not None and self . minversion > sys . version_info : \n        return 0 \n    if self . maxversion is not None and self . maxversion <= sys . version_info : \n        return 0 \n    return 1 "}
{"3814": "\ndef format_help ( self , checkerref = 0 ) : \n    desc = self . descr \n    if checkerref : \n        desc += \" This message belongs to the %s checker.\" % self . checker . name \n    title = self . msg \n    if self . symbol : \n        msgid = \"%s (%s)\" % ( self . symbol , self . msgid ) \n    else : \n        msgid = self . msgid \n    if self . minversion or self . maxversion : \n        restr = [ ] \n        if self . minversion : \n            restr . append ( \"< %s\" % \".\" . join ( [ str ( n ) for n in self . minversion ] ) ) \n        if self . maxversion : \n            restr . append ( \">= %s\" % \".\" . join ( [ str ( n ) for n in self . maxversion ] ) ) \n        restr = \" or \" . join ( restr ) \n        if checkerref : \n            desc += \" It can't be emitted when using Python %s.\" % restr \n        else : \n            desc += \" This message can't be emitted when using Python %s.\" % restr \n    desc = normalize_text ( \" \" . join ( desc . split ( ) ) , indent = \"  \" ) \n    if title != \"%s\" : \n        title = title . splitlines ( ) [ 0 ] \n        return \":%s: *%s*\\n%s\" % ( msgid , title . rstrip ( \" \" ) , desc ) \n    return \":%s:\\n%s\" % ( msgid , desc ) "}
{"3816": "\ndef lint ( filename , options = ( ) ) : \n    full_path = osp . abspath ( filename ) \n    parent_path = osp . dirname ( full_path ) \n    child_path = osp . basename ( full_path ) \n    while parent_path != \"/\" and osp . exists ( osp . join ( parent_path , \"__init__.py\" ) ) : \n        child_path = osp . join ( osp . basename ( parent_path ) , child_path ) \n        parent_path = osp . dirname ( parent_path ) \n    run_cmd = \"import sys; from pylint.lint import Run; Run(sys.argv[1:])\" \n    cmd = ( [ sys . executable , \"-c\" , run_cmd ] + [ \"--msg-template\" , \"{path}:{line}: {category} ({msg_id}, {symbol}, {obj}) {msg}\" , \"-r\" , \"n\" , child_path , ] + list ( options ) ) \n    process = Popen ( cmd , stdout = PIPE , cwd = parent_path , env = _get_env ( ) , universal_newlines = 1 ) \n    for line in process . stdout : \n        if line . startswith ( \"No config file found\" ) : \n            continue \n        parts = line . split ( \":\" ) \n        if parts and parts [ 0 ] == child_path : \n            line = \":\" . join ( [ filename ] + parts [ 1 : ] ) \n        print ( line , end = \" \" ) \n    process . wait ( ) \n    return process . returncode "}
{"3817": "\ndef py_run ( command_options = \"\" , return_std = 0 , stdout = None , stderr = None ) : \n    executable = sys . executable if \"python\" in sys . executable else \"python\" \n    epylint_part = [ executable , \"-c\" , \"from pylint import epylint;epylint.Run()\" ] \n    options = shlex . split ( command_options , posix = not sys . platform . startswith ( \"win\" ) ) \n    cli = epylint_part + options \n    if stdout is None : \n        if return_std : \n            stdout = PIPE \n        else : \n            stdout = sys . stdout \n    if stderr is None : \n        if return_std : \n            stderr = PIPE \n        else : \n            stderr = sys . stderr \n    process = Popen ( cli , shell = 0 , stdout = stdout , stderr = stderr , env = _get_env ( ) , universal_newlines = 1 , ) \n    proc_stdout , proc_stderr = process . communicate ( ) \n    if return_std : \n        return StringIO ( proc_stdout ) , StringIO ( proc_stderr ) \n    return None "}
{"3822": "\ndef _register_by_id_managed_msg ( self , msgid , line , is_disabled = 1 ) : \n    try : \n        message_definitions = self . msgs_store . get_message_definitions ( msgid ) \n        for message_definition in message_definitions : \n            if msgid == message_definition . msgid : \n                MessagesHandlerMixIn . __by_id_managed_msgs . append ( ( self . current_name , message_definition . msgid , message_definition . symbol , line , is_disabled , ) ) \n    except UnknownMessageError : \n        pass "}
{"3823": "\ndef disable ( self , msgid , scope = \"package\" , line = None , ignore_unknown = 0 ) : \n    self . _set_msg_status ( msgid , enable = 0 , scope = scope , line = line , ignore_unknown = ignore_unknown ) \n    self . _register_by_id_managed_msg ( msgid , line ) "}
{"3824": "\ndef enable ( self , msgid , scope = \"package\" , line = None , ignore_unknown = 0 ) : \n    self . _set_msg_status ( msgid , enable = 1 , scope = scope , line = line , ignore_unknown = ignore_unknown ) \n    self . _register_by_id_managed_msg ( msgid , line , is_disabled = 0 ) "}
{"3826": "\ndef is_message_enabled ( self , msg_descr , line = None , confidence = None ) : \n    if self . config . confidence and confidence : \n        if confidence . name not in self . config . confidence : \n            return 0 \n    try : \n        message_definitions = self . msgs_store . get_message_definitions ( msg_descr ) \n        msgids = [ md . msgid for md in message_definitions ] \n    except UnknownMessageError : \n        msgids = [ msg_descr ] \n    for msgid in msgids : \n        if self . is_one_message_enabled ( msgid , line ) : \n            return 1 \n    return 0 "}
{"3829": "\ndef _print_checker_doc ( checker_name , info , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    doc = info . get ( \"doc\" ) \n    module = info . get ( \"module\" ) \n    msgs = info . get ( \"msgs\" ) \n    options = info . get ( \"options\" ) \n    reports = info . get ( \"reports\" ) \n    checker_title = \"%s checker\" % ( checker_name . replace ( \"_\" , \" \" ) . title ( ) ) \n    if module : \n        print ( \".. _%s:\\n\" % module , file = stream ) \n    print ( checker_title , file = stream ) \n    print ( \"~\" * len ( checker_title ) , file = stream ) \n    print ( \"\" , file = stream ) \n    if module : \n        print ( \"This checker is provided by ``%s``.\" % module , file = stream ) \n    print ( \"Verbatim name of the checker is ``%s``.\" % checker_name , file = stream ) \n    print ( \"\" , file = stream ) \n    if doc : \n        title = \"{} Documentation\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        print ( cleandoc ( doc ) , file = stream ) \n        print ( \"\" , file = stream ) \n    if options : \n        title = \"{} Options\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        _rest_format_section ( stream , None , options ) \n        print ( \"\" , file = stream ) \n    if msgs : \n        title = \"{} Messages\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        for msgid , msg in sorted ( msgs . items ( ) , key = lambda kv : ( _MSG_ORDER . index ( kv [ 0 ] [ 0 ] ) , kv [ 1 ] ) ) : \n            msg = build_message_definition ( checker_name , msgid , msg ) \n            print ( msg . format_help ( checkerref = 0 ) , file = stream ) \n        print ( \"\" , file = stream ) \n    if reports : \n        title = \"{} Reports\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        for report in reports : \n            print ( \":%s: %s\" % report [ : 2 ] , file = stream ) \n        print ( \"\" , file = stream ) \n    print ( \"\" , file = stream ) "}
{"3835": "\ndef _hanging_indent_after_bracket ( self , bracket , position ) : \n    indentation = self . _tokens . line_indent ( position ) \n    if ( self . _is_block_opener and self . _continuation_string == self . _block_indent_string ) : \n        return _ContinuedIndent ( HANGING_BLOCK , bracket , position , _Indentations ( indentation + self . _continuation_string , indentation ) , _BeforeBlockIndentations ( indentation + self . _continuation_string , indentation + self . _continuation_string * 2 , ) , ) \n    if bracket == \":\" : \n        paren_align = self . _cont_stack [ - 1 ] . valid_outdent_strings \n        next_align = self . _cont_stack [ - 1 ] . valid_continuation_strings . copy ( ) \n        next_align_keys = list ( next_align . keys ( ) ) \n        next_align [ next_align_keys [ 0 ] + self . _continuation_string ] = 1 \n        return _ContinuedIndent ( HANGING_DICT_VALUE , bracket , position , paren_align , next_align ) \n    return _ContinuedIndent ( HANGING , bracket , position , _Indentations ( indentation , indentation + self . _continuation_string ) , _Indentations ( indentation + self . _continuation_string ) , ) "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and tokens [ start ] [ 1 ] == \"for\" : \n        self . _pop_token ( ) \n    if tokens [ start + 1 ] [ 1 ] != \"(\" : \n        return \n    found_and_or = 0 \n    depth = 0 \n    keyword_token = str ( tokens [ start ] [ 1 ] ) \n    line_num = tokens [ start ] [ 2 ] [ 0 ] \n    for i in range ( start , len ( tokens ) - 1 ) : \n        token = tokens [ i ] \n        if token [ 0 ] == tokenize . NL : \n            return \n        if token [ 1 ] == \"(\" : \n            depth += 1 \n        elif token [ 1 ] == \")\" : \n            depth -= 1 \n            if depth : \n                continue \n            if tokens [ i + 1 ] [ 1 ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + 1 ] [ 0 ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                if i == start + 2 : \n                    return \n                if keyword_token == \"not\" : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token in ( \"return\" , \"yield\" ) : \n                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token not in self . _keywords_with_parens : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n            return \n        elif depth == 1 : \n            if token [ 1 ] == \",\" : \n                return \n            if token [ 1 ] in ( \"and\" , \"or\" ) : \n                found_and_or = 1 \n            elif token [ 1 ] == \"yield\" : \n                return \n            elif token [ 1 ] == \"for\" : \n                return "}
{"3840": "\ndef _has_valid_type_annotation ( self , tokens , i ) : \n    if not self . _inside_brackets ( \"(\" ) : \n        return 0 \n    bracket_level = 0 \n    for token in tokens [ i - 1 : : - 1 ] : \n        if token [ 1 ] == \":\" : \n            return 1 \n        if token [ 1 ] == \"(\" : \n            return 0 \n        if token [ 1 ] == \"]\" : \n            bracket_level += 1 \n        elif token [ 1 ] == \"[\" : \n            bracket_level -= 1 \n        elif token [ 1 ] == \",\" : \n            if not bracket_level : \n                return 0 \n        elif token [ 1 ] in ( \".\" , \"...\" ) : \n            continue \n        elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : \n            return 0 \n    return 0 "}
{"3845": "\ndef check_lines ( self , lines , i ) : \n    max_chars = self . config . max_line_length \n    ignore_long_line = self . config . ignore_long_lines \n    def check_line ( line , i ) : \n        if not line . endswith ( \"\\n\" ) : \n            self . add_message ( \"missing-final-newline\" , line = i ) \n        else : \n            stripped_line = line . rstrip ( \"\\t\\n\\r\\v \" ) \n            if not stripped_line and _EMPTY_LINE in self . config . no_space_check : \n                pass \n            elif line [ len ( stripped_line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : \n                self . add_message ( \"trailing-whitespace\" , line = i , col_offset = len ( stripped_line ) ) \n            line = stripped_line \n        mobj = OPTION_RGX . search ( line ) \n        if mobj and \"=\" in line : \n            front_of_equal , _ , back_of_equal = mobj . group ( 1 ) . partition ( \"=\" ) \n            if front_of_equal . strip ( ) == \"disable\" : \n                if \"line-too-long\" in { _msg_id . strip ( ) for _msg_id in back_of_equal . split ( \",\" ) } : \n                    return None \n                line = line . rsplit ( \"#\" , 1 ) [ 0 ] . rstrip ( ) \n        if len ( line ) > max_chars and not ignore_long_line . search ( line ) : \n            self . add_message ( \"line-too-long\" , line = i , args = ( len ( line ) , max_chars ) ) \n        return i + 1 \n    unsplit_ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } \n    unsplit = [ ] \n    for line in lines . splitlines ( 1 ) : \n        if line [ - 1 ] in unsplit_ends : \n            unsplit . append ( line ) \n            continue \n        if unsplit : \n            unsplit . append ( line ) \n            line = \"\" . join ( unsplit ) \n            unsplit = [ ] \n        i = check_line ( line , i ) \n        if i is None : \n            break \n    if unsplit : \n        check_line ( \"\" . join ( unsplit ) , i ) "}
{"3847": "\ndef _in_iterating_context ( node ) : \n    parent = node . parent \n    if isinstance ( parent , astroid . For ) : \n        return 1 \n    if isinstance ( parent , astroid . Comprehension ) : \n        if parent . iter == node : \n            return 1 \n    elif isinstance ( parent , astroid . Call ) : \n        if isinstance ( parent . func , astroid . Name ) : \n            parent_scope = parent . func . lookup ( parent . func . name ) [ 0 ] \n            if _is_builtin ( parent_scope ) and parent . func . name in _ACCEPTS_ITERATOR : \n                return 1 \n        elif isinstance ( parent . func , astroid . Attribute ) : \n            if parent . func . attrname in ATTRIBUTES_ACCEPTS_ITERATOR : \n                return 1 \n        inferred = utils . safe_infer ( parent . func ) \n        if inferred : \n            if inferred . qname ( ) in _BUILTIN_METHOD_ACCEPTS_ITERATOR : \n                return 1 \n            root = inferred . root ( ) \n            if root and root . name == \"itertools\" : \n                return 1 \n    elif isinstance ( parent , astroid . Assign ) and isinstance ( parent . targets [ 0 ] , ( astroid . List , astroid . Tuple ) ) : \n        if len ( parent . targets [ 0 ] . elts ) > 1 : \n            return 1 \n    elif ( isinstance ( parent , astroid . Compare ) and len ( parent . ops ) == 1 and parent . ops [ 0 ] [ 0 ] == \"in\" ) : \n        return 1 \n    elif isinstance ( parent , astroid . YieldFrom ) : \n        return 1 \n    if isinstance ( parent , astroid . Starred ) : \n        return 1 \n    return 0 "}
{"3851": "\ndef visit_attribute ( self , node ) : \n    if node . attrname == \"xreadlines\" : \n        self . add_message ( \"xreadlines-attribute\" , node = node ) \n        return \n    exception_message = \"message\" \n    try : \n        for inferred in node . expr . infer ( ) : \n            if isinstance ( inferred , astroid . Instance ) and utils . inherit_from_std_ex ( inferred ) : \n                if node . attrname == exception_message : \n                    if exception_message in inferred . instance_attrs : \n                        continue \n                    self . add_message ( \"exception-message-attribute\" , node = node ) \n            if isinstance ( inferred , astroid . Module ) : \n                self . _warn_if_deprecated ( node , inferred . name , { node . attrname } , report_on_modules = 0 ) \n    except astroid . InferenceError : \n        return "}
{"3858": "\ndef register_options_provider ( self , provider , own_group = 1 ) : \n    assert provider . priority <= 0 , \"provider's priority can't be >= 0\" \n    for i in range ( len ( self . options_providers ) ) : \n        if provider . priority > self . options_providers [ i ] . priority : \n            self . options_providers . insert ( i , provider ) \n            break \n    else : \n        self . options_providers . append ( provider ) \n    non_group_spec_options = [ option for option in provider . options if \"group\" not in option [ 1 ] ] \n    groups = getattr ( provider , \"option_groups\" , ( ) ) \n    if own_group and non_group_spec_options : \n        self . add_option_group ( provider . name . upper ( ) , provider . __doc__ , non_group_spec_options , provider , ) \n    else : \n        for opt , optdict in non_group_spec_options : \n            self . add_optik_option ( provider , self . cmdline_parser , opt , optdict ) \n    for gname , gdoc in groups : \n        gname = gname . upper ( ) \n        goptions = [ option for option in provider . options if option [ 1 ] . get ( \"group\" , \"\" ) . upper ( ) == gname ] \n        self . add_option_group ( gname , gdoc , goptions , provider ) "}
{"3861": "\ndef generate_config ( self , stream = None , skipsections = ( ) , encoding = None ) : \n    options_by_section = { } \n    sections = [ ] \n    for provider in self . options_providers : \n        for section , options in provider . options_by_section ( ) : \n            if section is None : \n                section = provider . name \n            if section in skipsections : \n                continue \n            options = [ ( n , d , v ) for ( n , d , v ) in options if d . get ( \"type\" ) is not None and not d . get ( \"deprecated\" ) ] \n            if not options : \n                continue \n            if section not in sections : \n                sections . append ( section ) \n            alloptions = options_by_section . setdefault ( section , [ ] ) \n            alloptions += options \n    stream = stream or sys . stdout \n    printed = 0 \n    for section in sections : \n        if printed : \n            print ( \"\\n\" , file = stream ) \n        utils . format_section ( stream , section . upper ( ) , sorted ( options_by_section [ section ] ) ) \n        printed = 1 "}
{"3870": "\ndef is_method_call ( func , types = ( ) , methods = ( ) ) : \n    return ( isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) and ( func . bound . name in types if types else 1 ) and ( func . name in methods if methods else 1 ) ) "}
{"3871": "\ndef is_complex_format_str ( node ) : \n    inferred = utils . safe_infer ( node ) \n    if inferred is None or not isinstance ( inferred . value , str ) : \n        return 1 \n    try : \n        parsed = list ( string . Formatter ( ) . parse ( inferred . value ) ) \n    except ValueError : \n        return 0 \n    for _ , _ , format_spec , _ in parsed : \n        if format_spec : \n            return 1 \n    return 0 "}
{"3875": "\ndef visit_call ( self , node ) : \n    def is_logging_name ( ) : \n        return ( isinstance ( node . func , astroid . Attribute ) and isinstance ( node . func . expr , astroid . Name ) and node . func . expr . name in self . _logging_names ) \n    def is_logger_class ( ) : \n        try : \n            for inferred in node . func . infer ( ) : \n                if isinstance ( inferred , astroid . BoundMethod ) : \n                    parent = inferred . _proxied . parent \n                    if isinstance ( parent , astroid . ClassDef ) and ( parent . qname ( ) == \"logging.Logger\" or any ( ancestor . qname ( ) == \"logging.Logger\" for ancestor in parent . ancestors ( ) ) ) : \n                        return 1 , inferred . _proxied . name \n        except astroid . exceptions . InferenceError : \n            pass \n        return 0 , None \n    if is_logging_name ( ) : \n        name = node . func . attrname \n    else : \n        result , name = is_logger_class ( ) \n        if not result : \n            return \n    self . _check_log_method ( node , name ) "}
{"3877": "\ndef in_loop ( node ) : \n    parent = node . parent \n    while parent is not None : \n        if isinstance ( parent , ( astroid . For , astroid . ListComp , astroid . SetComp , astroid . DictComp , astroid . GeneratorExp , ) , ) : \n            return 1 \n        parent = parent . parent \n    return 0 "}
{"3883": "\ndef redefined_by_decorator ( node ) : \n    if node . decorators : \n        for decorator in node . decorators . nodes : \n            if ( isinstance ( decorator , astroid . Attribute ) and getattr ( decorator . expr , \"name\" , None ) == node . name ) : \n                return 1 \n    return 0 "}
{"3899": "\ndef _check_name ( self , node_type , name , node , confidence = interfaces . HIGH ) : \n    def _should_exempt_from_invalid_name ( node ) : \n        if node_type == \"variable\" : \n            inferred = utils . safe_infer ( node ) \n            if isinstance ( inferred , astroid . ClassDef ) : \n                return 1 \n        return 0 \n    if utils . is_inside_except ( node ) : \n        clobbering , _ = utils . clobber_in_except ( node ) \n        if clobbering : \n            return \n    if name in self . config . good_names : \n        return \n    if name in self . config . bad_names : \n        self . stats [ \"badname_\" + node_type ] += 1 \n        self . add_message ( \"blacklisted-name\" , node = node , args = name ) \n        return \n    regexp = self . _name_regexps [ node_type ] \n    match = regexp . match ( name ) \n    if _is_multi_naming_match ( match , node_type , confidence ) : \n        name_group = self . _find_name_group ( node_type ) \n        bad_name_group = self . _bad_names . setdefault ( name_group , { } ) \n        warnings = bad_name_group . setdefault ( match . lastgroup , [ ] ) \n        warnings . append ( ( node , node_type , name , confidence ) ) \n    if match is None and not _should_exempt_from_invalid_name ( node ) : \n        self . _raise_name_warning ( node , node_type , name , confidence ) "}
{"3900": "\ndef _check_docstring ( self , node_type , node , report_missing = 1 , confidence = interfaces . HIGH ) : \n    docstring = node . doc \n    if docstring is None : \n        if not report_missing : \n            return \n        lines = utils . get_node_last_lineno ( node ) - node . lineno \n        if node_type == \"module\" and not lines : \n            return \n        max_lines = self . config . docstring_min_length \n        if node_type != \"module\" and max_lines > - 1 and lines < max_lines : \n            return \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : \n            func = utils . safe_infer ( node . body [ 0 ] . value . func ) \n            if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : \n                if PY3K and func . bound . name == \"str\" : \n                    return \n                if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : \n                    return \n        self . add_message ( \"missing-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) \n    elif not docstring . strip ( ) : \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        self . add_message ( \"empty-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) "}
{"3901": "\ndef _check_literal_comparison ( self , literal , node ) : \n    nodes = ( astroid . List , astroid . Tuple , astroid . Dict , astroid . Set ) \n    is_other_literal = isinstance ( literal , nodes ) \n    is_const = 0 \n    if isinstance ( literal , astroid . Const ) : \n        if isinstance ( literal . value , bool ) or literal . value is None : \n            return \n        is_const = isinstance ( literal . value , ( bytes , str , int , float ) ) \n    if is_const or is_other_literal : \n        self . add_message ( \"literal-comparison\" , node = node ) "}
{"3928": "\ndef remote_app ( self , name , register = 1 , ** kwargs ) : \n    remote = OAuthRemoteApp ( self , name , ** kwargs ) \n    if register : \n        assert name not in self . remote_apps \n        self . remote_apps [ name ] = remote \n    return remote "}
{"3948": "\ndef validate_client_key ( self , client_key , request ) : \n    log . debug ( 'Validate client key for %r' , client_key ) \n    if not request . client : \n        request . client = self . _clientgetter ( client_key = client_key ) \n    if request . client : \n        return 1 \n    return 0 "}
{"3949": "\ndef validate_request_token ( self , client_key , token , request ) : \n    log . debug ( 'Validate request token %r for %r' , token , client_key ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if tok and tok . client_key == client_key : \n        request . request_token = tok \n        return 1 \n    return 0 "}
{"3950": "\ndef validate_access_token ( self , client_key , token , request ) : \n    log . debug ( 'Validate access token %r for %r' , token , client_key ) \n    tok = request . access_token or self . _tokengetter ( client_key = client_key , token = token , ) \n    if tok : \n        request . access_token = tok \n        return 1 \n    return 0 "}
{"3951": "\ndef validate_timestamp_and_nonce ( self , client_key , timestamp , nonce , request , request_token = None , access_token = None ) : \n    log . debug ( 'Validate timestamp and nonce %r' , client_key ) \n    nonce_exists = self . _noncegetter ( client_key = client_key , timestamp = timestamp , nonce = nonce , request_token = request_token , access_token = access_token ) \n    if nonce_exists : \n        return 0 \n    self . _noncesetter ( client_key = client_key , timestamp = timestamp , nonce = nonce , request_token = request_token , access_token = access_token ) \n    return 1 "}
{"3952": "\ndef validate_redirect_uri ( self , client_key , redirect_uri , request ) : \n    log . debug ( 'Validate redirect_uri %r for %r' , redirect_uri , client_key ) \n    if not request . client : \n        request . client = self . _clientgetter ( client_key = client_key ) \n    if not request . client : \n        return 0 \n    if not request . client . redirect_uris and redirect_uri is None : \n        return 1 \n    request . redirect_uri = redirect_uri \n    return redirect_uri in request . client . redirect_uris "}
{"3953": "\ndef validate_realms ( self , client_key , token , request , uri = None , realms = None ) : \n    log . debug ( 'Validate realms %r for %r' , realms , client_key ) \n    if request . access_token : \n        tok = request . access_token \n    else : \n        tok = self . _tokengetter ( client_key = client_key , token = token ) \n        request . access_token = tok \n    if not tok : \n        return 0 \n    return set ( tok . realms ) . issuperset ( set ( realms ) ) "}
{"3954": "\ndef validate_verifier ( self , client_key , token , verifier , request ) : \n    log . debug ( 'Validate verifier %r for %r' , verifier , client_key ) \n    data = self . _verifiergetter ( verifier = verifier , token = token ) \n    if not data : \n        return 0 \n    if not hasattr ( data , 'user' ) : \n        log . debug ( 'Verifier should has user attribute' ) \n        return 0 \n    request . user = data . user \n    if hasattr ( data , 'client_key' ) : \n        return data . client_key == client_key \n    return 1 "}
{"3955": "\ndef verify_request_token ( self , token , request ) : \n    log . debug ( 'Verify request token %r' , token ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if tok : \n        request . request_token = tok \n        return 1 \n    return 0 "}
{"3956": "\ndef verify_realms ( self , token , realms , request ) : \n    log . debug ( 'Verify realms %r' , realms ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if not tok : \n        return 0 \n    request . request_token = tok \n    if not hasattr ( tok , 'realms' ) : \n        return 1 \n    return set ( tok . realms ) == set ( realms ) "}
{"3961": "\ndef confirm_authorization_request ( self ) : \n    server = self . server \n    scope = request . values . get ( 'scope' ) or '' \n    scopes = scope . split ( ) \n    credentials = dict ( client_id = request . values . get ( 'client_id' ) , redirect_uri = request . values . get ( 'redirect_uri' , None ) , response_type = request . values . get ( 'response_type' , None ) , state = request . values . get ( 'state' , None ) ) \n    log . debug ( 'Fetched credentials from request %r.' , credentials ) \n    redirect_uri = credentials . get ( 'redirect_uri' ) \n    log . debug ( 'Found redirect_uri %s.' , redirect_uri ) \n    uri , http_method , body , headers = extract_params ( ) \n    try : \n        ret = server . create_authorization_response ( uri , http_method , body , headers , scopes , credentials ) \n        log . debug ( 'Authorization successful.' ) \n        return create_response ( * ret ) \n    except oauth2 . FatalClientError as e : \n        log . debug ( 'Fatal client error %r' , e , exc_info = 1 ) \n        return self . _on_exception ( e , e . in_uri ( self . error_uri ) ) \n    except oauth2 . OAuth2Error as e : \n        log . debug ( 'OAuth2Error: %r' , e , exc_info = 1 ) \n        state = request . values . get ( 'state' ) \n        if state and not e . state : \n            e . state = state \n        return self . _on_exception ( e , e . in_uri ( redirect_uri or self . error_uri ) ) \n    except Exception as e : \n        log . exception ( e ) \n        return self . _on_exception ( e , add_params_to_uri ( self . error_uri , { 'error' : str ( e ) } ) ) "}
{"3964": "\ndef client_authentication_required ( self , request , * args , ** kwargs ) : \n    def is_confidential ( client ) : \n        if hasattr ( client , 'is_confidential' ) : \n            return client . is_confidential \n        client_type = getattr ( client , 'client_type' , None ) \n        if client_type : \n            return client_type == 'confidential' \n        return 1 \n    grant_types = ( 'password' , 'authorization_code' , 'refresh_token' ) \n    client_id , _ = self . _get_client_creds_from_request ( request ) \n    if client_id and request . grant_type in grant_types : \n        client = self . _clientgetter ( client_id ) \n        if client : \n            return is_confidential ( client ) \n    return 0 "}
{"3965": "\ndef authenticate_client ( self , request , * args , ** kwargs ) : \n    client_id , client_secret = self . _get_client_creds_from_request ( request ) \n    log . debug ( 'Authenticate client %r' , client_id ) \n    client = self . _clientgetter ( client_id ) \n    if not client : \n        log . debug ( 'Authenticate client failed, client not found.' ) \n        return 0 \n    request . client = client \n    if hasattr ( client , 'client_secret' ) and client . client_secret != client_secret : \n        log . debug ( 'Authenticate client failed, secret not match.' ) \n        return 0 \n    log . debug ( 'Authenticate client success.' ) \n    return 1 "}
{"3966": "\ndef authenticate_client_id ( self , client_id , request , * args , ** kwargs ) : \n    if client_id is None : \n        client_id , _ = self . _get_client_creds_from_request ( request ) \n    log . debug ( 'Authenticate client %r.' , client_id ) \n    client = request . client or self . _clientgetter ( client_id ) \n    if not client : \n        log . debug ( 'Authenticate failed, client not found.' ) \n        return 0 \n    request . client = client \n    return 1 "}
{"3968": "\ndef confirm_scopes ( self , refresh_token , scopes , request , * args , ** kwargs ) : \n    if not scopes : \n        log . debug ( 'Scope omitted for refresh token %r' , refresh_token ) \n        return 1 \n    log . debug ( 'Confirm scopes %r for refresh token %r' , scopes , refresh_token ) \n    tok = self . _tokengetter ( refresh_token = refresh_token ) \n    return set ( tok . scopes ) == set ( scopes ) "}
{"3974": "\ndef validate_bearer_token ( self , token , scopes , request ) : \n    log . debug ( 'Validate bearer token %r' , token ) \n    tok = self . _tokengetter ( access_token = token ) \n    if not tok : \n        msg = 'Bearer token not found.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return 0 \n    if tok . expires is not None and datetime . datetime . utcnow ( ) > tok . expires : \n        msg = 'Bearer token is expired.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return 0 \n    if scopes and not set ( tok . scopes ) & set ( scopes ) : \n        msg = 'Bearer token scope not valid.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return 0 \n    request . access_token = tok \n    request . user = tok . user \n    request . scopes = scopes \n    if hasattr ( tok , 'client' ) : \n        request . client = tok . client \n    elif hasattr ( tok , 'client_id' ) : \n        request . client = self . _clientgetter ( tok . client_id ) \n    return 1 "}
{"3975": "\ndef validate_client_id ( self , client_id , request , * args , ** kwargs ) : \n    log . debug ( 'Validate client %r' , client_id ) \n    client = request . client or self . _clientgetter ( client_id ) \n    if client : \n        request . client = client \n        return 1 \n    return 0 "}
{"3976": "\ndef validate_code ( self , client_id , code , client , request , * args , ** kwargs ) : \n    client = client or self . _clientgetter ( client_id ) \n    log . debug ( 'Validate code for client %r and code %r' , client . client_id , code ) \n    grant = self . _grantgetter ( client_id = client . client_id , code = code ) \n    if not grant : \n        log . debug ( 'Grant not found.' ) \n        return 0 \n    if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : \n        log . debug ( 'Grant is expired.' ) \n        return 0 \n    request . state = kwargs . get ( 'state' ) \n    request . user = grant . user \n    request . scopes = grant . scopes \n    return 1 "}
{"3977": "\ndef validate_grant_type ( self , client_id , grant_type , client , request , * args , ** kwargs ) : \n    if self . _usergetter is None and grant_type == 'password' : \n        log . debug ( 'Password credential authorization is disabled.' ) \n        return 0 \n    default_grant_types = ( 'authorization_code' , 'password' , 'client_credentials' , 'refresh_token' , ) \n    if hasattr ( client , 'allowed_grant_types' ) : \n        if grant_type not in client . allowed_grant_types : \n            return 0 \n    else : \n        if grant_type not in default_grant_types : \n            return 0 \n    if grant_type == 'client_credentials' : \n        if not hasattr ( client , 'user' ) : \n            log . debug ( 'Client should have a user property' ) \n            return 0 \n        request . user = client . user \n    return 1 "}
{"3978": "\ndef validate_refresh_token ( self , refresh_token , client , request , * args , ** kwargs ) : \n    token = self . _tokengetter ( refresh_token = refresh_token ) \n    if token and token . client_id == client . client_id : \n        request . client_id = token . client_id \n        request . user = token . user \n        return 1 \n    return 0 "}
{"3979": "\ndef validate_response_type ( self , client_id , response_type , client , request , * args , ** kwargs ) : \n    if response_type not in ( 'code' , 'token' ) : \n        return 0 \n    if hasattr ( client , 'allowed_response_types' ) : \n        return response_type in client . allowed_response_types \n    return 1 "}
{"3981": "\ndef validate_user ( self , username , password , client , request , * args , ** kwargs ) : \n    log . debug ( 'Validating username %r and its password' , username ) \n    if self . _usergetter is not None : \n        user = self . _usergetter ( username , password , client , request , * args , ** kwargs ) \n        if user : \n            request . user = user \n            return 1 \n        return 0 \n    log . debug ( 'Password credential authorization is disabled.' ) \n    return 0 "}
{"3982": "\ndef revoke_token ( self , token , token_type_hint , request , * args , ** kwargs ) : \n    if token_type_hint : \n        tok = self . _tokengetter ( ** { token_type_hint : token } ) \n    else : \n        tok = self . _tokengetter ( access_token = token ) \n        if not tok : \n            tok = self . _tokengetter ( refresh_token = token ) \n    if tok : \n        request . client_id = tok . client_id \n        request . user = tok . user \n        tok . delete ( ) \n        return 1 \n    msg = 'Invalid token supplied.' \n    log . debug ( msg ) \n    request . error_message = msg \n    return 0 "}
{"3987": "\ndef create ( self , oauth , ** kwargs ) : \n    kwargs = self . _process_kwargs ( name = self . default_name , register = 0 , ** kwargs ) \n    return oauth . remote_app ( ** kwargs ) "}
{"4033": "\ndef renegotiate ( self ) : \n    if not self . renegotiate_pending ( ) : \n        _openssl_assert ( _lib . SSL_renegotiate ( self . _ssl ) == 1 ) \n        return 1 \n    return 0 "}
{"4034": "\ndef shutdown ( self ) : \n    result = _lib . SSL_shutdown ( self . _ssl ) \n    if result < 0 : \n        self . _raise_ssl_error ( self . _ssl , result ) \n    elif result > 0 : \n        return 1 \n    else : \n        return 0 "}
{"4056": "\ndef load_publickey ( type , buffer ) : \n    if isinstance ( buffer , _text_type ) : \n        buffer = buffer . encode ( \"ascii\" ) \n    bio = _new_mem_buf ( buffer ) \n    if type == FILETYPE_PEM : \n        evp_pkey = _lib . PEM_read_bio_PUBKEY ( bio , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n    elif type == FILETYPE_ASN1 : \n        evp_pkey = _lib . d2i_PUBKEY_bio ( bio , _ffi . NULL ) \n    else : \n        raise ValueError ( \"type argument must be FILETYPE_PEM or FILETYPE_ASN1\" ) \n    if evp_pkey == _ffi . NULL : \n        _raise_current_error ( ) \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _ffi . gc ( evp_pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = 1 \n    return pkey "}
{"4061": "\ndef generate_key ( self , type , bits ) : \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if not isinstance ( bits , int ) : \n        raise TypeError ( \"bits must be an integer\" ) \n    if type == TYPE_RSA : \n        if bits <= 0 : \n            raise ValueError ( \"Invalid number of bits\" ) \n        exponent = _lib . BN_new ( ) \n        exponent = _ffi . gc ( exponent , _lib . BN_free ) \n        _lib . BN_set_word ( exponent , _lib . RSA_F4 ) \n        rsa = _lib . RSA_new ( ) \n        result = _lib . RSA_generate_key_ex ( rsa , bits , exponent , _ffi . NULL ) \n        _openssl_assert ( result == 1 ) \n        result = _lib . EVP_PKEY_assign_RSA ( self . _pkey , rsa ) \n        _openssl_assert ( result == 1 ) \n    elif type == TYPE_DSA : \n        dsa = _lib . DSA_new ( ) \n        _openssl_assert ( dsa != _ffi . NULL ) \n        dsa = _ffi . gc ( dsa , _lib . DSA_free ) \n        res = _lib . DSA_generate_parameters_ex ( dsa , bits , _ffi . NULL , 0 , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n        _openssl_assert ( res == 1 ) \n        _openssl_assert ( _lib . DSA_generate_key ( dsa ) == 1 ) \n        _openssl_assert ( _lib . EVP_PKEY_set1_DSA ( self . _pkey , dsa ) == 1 ) \n    else : \n        raise Error ( \"No such key type\" ) \n    self . _initialized = 1 "}
{"4062": "\ndef check ( self ) : \n    if self . _only_public : \n        raise TypeError ( \"public key only\" ) \n    if _lib . EVP_PKEY_type ( self . type ( ) ) != _lib . EVP_PKEY_RSA : \n        raise TypeError ( \"key type unsupported\" ) \n    rsa = _lib . EVP_PKEY_get1_RSA ( self . _pkey ) \n    rsa = _ffi . gc ( rsa , _lib . RSA_free ) \n    result = _lib . RSA_check_key ( rsa ) \n    if result : \n        return 1 \n    _raise_current_error ( ) "}
{"4072": "\ndef get_pubkey ( self ) : \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _lib . X509_REQ_get_pubkey ( self . _req ) \n    _openssl_assert ( pkey . _pkey != _ffi . NULL ) \n    pkey . _pkey = _ffi . gc ( pkey . _pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = 1 \n    return pkey "}
{"4079": "\ndef get_pubkey ( self ) : \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _lib . X509_get_pubkey ( self . _x509 ) \n    if pkey . _pkey == _ffi . NULL : \n        _raise_current_error ( ) \n    pkey . _pkey = _ffi . gc ( pkey . _pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = 1 \n    return pkey "}
{"4115": "\ndef verify ( self , key ) : \n    answer = _lib . NETSCAPE_SPKI_verify ( self . _spki , key . _pkey ) \n    if answer <= 0 : \n        _raise_current_error ( ) \n    return 1 "}
{"4117": "\ndef get_pubkey ( self ) : \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _lib . NETSCAPE_SPKI_get_pubkey ( self . _spki ) \n    _openssl_assert ( pkey . _pkey != _ffi . NULL ) \n    pkey . _pkey = _ffi . gc ( pkey . _pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = 1 \n    return pkey "}
{"4119": "\ndef exception_from_error_queue ( exception_type ) : \n    errors = [ ] \n    while 1 : \n        error = lib . ERR_get_error ( ) \n        if error == 0 : \n            break \n        errors . append ( ( text ( lib . ERR_lib_error_string ( error ) ) , text ( lib . ERR_func_error_string ( error ) ) , text ( lib . ERR_reason_error_string ( error ) ) ) ) \n    raise exception_type ( errors ) "}
{"4122": "\ndef get_service_metadata ( self ) : \n    return { 'import_labels_as_tags' : self . config . get ( 'import_labels_as_tags' , 0 , asbool ) , 'label_template' : self . config . get ( 'label_template' , DEFAULT_LABEL_TEMPLATE ) , } "}
{"4127": "\ndef get_cards ( self , list_id ) : \n    params = { 'fields' : 'name,idShort,shortLink,shortUrl,url,labels,due' } \n    member = self . config . get ( 'only_if_assigned' , None ) \n    unassigned = self . config . get ( 'also_unassigned' , 0 , asbool ) \n    if member is not None : \n        params [ 'members' ] = 'true' \n        params [ 'member_fields' ] = 'username' \n    cards = self . api_request ( \"/1/lists/{list_id}/cards/open\" . format ( list_id = list_id ) , ** params ) \n    for card in cards : \n        if ( member is None or member in [ m [ 'username' ] for m in card [ 'members' ] ] or ( unassigned and not card [ 'members' ] ) ) : \n            yield card "}
{"4134": "\ndef aggregate_issues ( conf , main_section , debug ) : \n    log . info ( \"Starting to aggregate remote issues.\" ) \n    targets = aslist ( conf . get ( main_section , 'targets' ) ) \n    queue = multiprocessing . Queue ( ) \n    log . info ( \"Spawning %i workers.\" % len ( targets ) ) \n    processes = [ ] \n    if debug : \n        for target in targets : \n            _aggregate_issues ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) \n    else : \n        for target in targets : \n            proc = multiprocessing . Process ( target = _aggregate_issues , args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) ) \n            proc . start ( ) \n            processes . append ( proc ) \n            time . sleep ( 1 ) \n    currently_running = len ( targets ) \n    while currently_running > 0 : \n        issue = queue . get ( 1 ) \n        if isinstance ( issue , tuple ) : \n            completion_type , args = issue \n            if completion_type == SERVICE_FINISHED_ERROR : \n                target , e = args \n                log . info ( \"Terminating workers\" ) \n                for process in processes : \n                    process . terminate ( ) \n                raise RuntimeError ( \"critical error in target '{}'\" . format ( target ) ) \n            currently_running -= 1 \n            continue \n        yield issue \n    log . info ( \"Done aggregating remote issues.\" ) "}
{"4138": "\ndef include ( self , issue ) : \n    only_if_assigned = self . config . get ( 'only_if_assigned' , None ) \n    if only_if_assigned : \n        owner = self . get_owner ( issue ) \n        include_owners = [ only_if_assigned ] \n        if self . config . get ( 'also_unassigned' , None , asbool ) : \n            include_owners . append ( None ) \n        return owner in include_owners \n    only_if_author = self . config . get ( 'only_if_author' , None ) \n    if only_if_author : \n        return self . get_author ( issue ) == only_if_author \n    return 1 "}
{"4140": "\ndef oracle_eval ( command ) : \n    p = subprocess . Popen ( command , shell = 1 , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    p . wait ( ) \n    if p . returncode == 0 : \n        return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) \n    else : \n        die ( \"Error retrieving password: `{command}` returned '{error}'\" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) ) "}
{"4145": "\ndef find_local_uuid ( tw , keys , issue , legacy_matching = 0 ) : \n    if not issue [ 'description' ] : \n        raise ValueError ( 'Issue %s has no description.' % issue ) \n    possibilities = set ( [ ] ) \n    if legacy_matching : \n        legacy_description = issue . get_default_description ( ) . rsplit ( '..' , 1 ) [ 0 ] \n        legacy_description = legacy_description . split ( \"'\" ) [ 0 ] \n        results = tw . filter_tasks ( { 'description.startswith' : legacy_description , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n        possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    for service , key_list in six . iteritems ( keys ) : \n        if any ( [ key in issue for key in key_list ] ) : \n            results = tw . filter_tasks ( { 'and' : [ ( \"%s.is\" % key , issue [ key ] ) for key in key_list ] , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n            possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    if len ( possibilities ) == 1 : \n        return possibilities . pop ( ) \n    if len ( possibilities ) > 1 : \n        raise MultipleMatches ( \"Issue %s matched multiple IDs: %s\" % ( issue [ 'description' ] , possibilities ) ) \n    raise NotFound ( \"No issue was found matching %s\" % issue ) "}
{"4146": "\ndef merge_left ( field , local_task , remote_issue , hamming = 0 ) : \n    local_field = local_task . get ( field , [ ] ) \n    remote_field = remote_issue . get ( field , [ ] ) \n    if field not in local_task : \n        local_task [ field ] = [ ] \n    new_count = 0 \n    for remote in remote_field : \n        for local in local_field : \n            if ( ( hamming and get_annotation_hamming_distance ( remote , local ) == 0 ) or ( remote == local ) ) : \n                break \n        else : \n            log . debug ( \"%s not found in %r\" % ( remote , local_field ) ) \n            local_task [ field ] . append ( remote ) \n            new_count += 1 \n    if new_count > 0 : \n        log . debug ( 'Added %s new values to %s (total: %s)' % ( new_count , field , len ( local_task [ field ] ) , ) ) "}
{"4152": "\ndef fdrcorrection ( pvals , alpha = 0.05 ) : \n    pvals = np . asarray ( pvals ) \n    pvals_sortind = np . argsort ( pvals ) \n    pvals_sorted = np . take ( pvals , pvals_sortind ) \n    ecdffactor = _ecdf ( pvals_sorted ) \n    reject = pvals_sorted <= ecdffactor * alpha \n    if reject . any ( ) : \n        rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) \n        reject [ : rejectmax ] = 1 \n    pvals_corrected_raw = pvals_sorted / ecdffactor \n    pvals_corrected = np . minimum . accumulate ( pvals_corrected_raw [ : : - 1 ] ) [ : : - 1 ] \n    del pvals_corrected_raw \n    pvals_corrected [ pvals_corrected > 1 ] = 1 \n    pvals_corrected_ = np . empty_like ( pvals_corrected ) \n    pvals_corrected_ [ pvals_sortind ] = pvals_corrected \n    del pvals_corrected \n    reject_ = np . empty_like ( reject ) \n    reject_ [ pvals_sortind ] = reject \n    return reject_ , pvals_corrected_ "}
{"4154": "\ndef heatmap ( df , z_score = None , title = '' , figsize = ( 5 , 5 ) , cmap = 'RdBu_r' , xticklabels = 1 , yticklabels = 1 , ofname = None , ** kwargs ) : \n    df = zscore ( df , axis = z_score ) \n    df = df . iloc [ : : - 1 ] \n    ny , nx = df . shape \n    xticks = np . arange ( 0 , nx , 1 ) + .5 \n    yticks = np . arange ( 0 , ny , 1 ) + .5 \n    if hasattr ( sys , 'ps1' ) and ( ofname is None ) : \n        fig = plt . figure ( figsize = figsize ) \n    else : \n        fig = Figure ( figsize = figsize ) \n        canvas = FigureCanvas ( fig ) \n    ax = fig . add_subplot ( 111 ) \n    vmin = np . percentile ( df . min ( ) , 2 ) \n    vmax = np . percentile ( df . max ( ) , 98 ) \n    matrix = ax . pcolormesh ( df . values , cmap = cmap , vmin = vmin , vmax = vmax ) \n    ax . set_ylim ( [ 0 , len ( df ) ] ) \n    ax . set ( xticks = xticks , yticks = yticks ) \n    ax . set_xticklabels ( df . columns . values if xticklabels else '' , fontsize = 14 , rotation = 90 ) \n    ax . set_yticklabels ( df . index . values if yticklabels else '' , fontsize = 14 ) \n    ax . set_title ( \"%s\\nHeatmap of the Analyzed Geneset\" % title , fontsize = 20 ) \n    ax . tick_params ( axis = 'both' , which = 'both' , bottom = 0 , top = 0 , right = 0 , left = 0 ) \n    cbar = colorbar ( matrix ) \n    cbar . ax . tick_params ( axis = 'both' , which = 'both' , bottom = 0 , top = 0 , right = 0 , left = 0 ) \n    for side in [ \"top\" , \"right\" , \"left\" , \"bottom\" ] : \n        ax . spines [ side ] . set_visible ( 0 ) \n        cbar . ax . spines [ side ] . set_visible ( 0 ) \n    if ofname is not None : \n        fig . savefig ( ofname , bbox_inches = 'tight' , dpi = 300 ) \n    return "}
{"4157": "\ndef add_prerank_parser ( subparsers ) : \n    argparser_prerank = subparsers . add_parser ( \"prerank\" , help = \"Run GSEApy Prerank tool on preranked gene list.\" ) \n    prerank_input = argparser_prerank . add_argument_group ( \"Input files arguments\" ) \n    prerank_input . add_argument ( \"-r\" , \"--rnk\" , dest = \"rnk\" , action = \"store\" , type = str , required = 1 , help = \"Ranking metric file in .rnk format. Same with GSEA.\" ) \n    prerank_input . add_argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = 1 , help = \"Gene set database in GMT format. Same with GSEA.\" ) \n    prerank_input . add_argument ( \"-l\" , \"--label\" , action = 'store' , nargs = 2 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = \"The phenotype label argument need two parameters to define. Default: ('Pos','Neg')\" ) \n    prerank_output = argparser_prerank . add_argument_group ( \"Output arguments\" ) \n    add_output_option ( prerank_output ) \n    prerank_opt = argparser_prerank . add_argument_group ( \"GSEA advanced arguments\" ) \n    prerank_opt . add_argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) \n    prerank_opt . add_argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) \n    prerank_opt . add_argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) \n    prerank_opt . add_argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) \n    prerank_opt . add_argument ( \"-a\" , \"--ascending\" , action = 'store_true' , dest = 'ascending' , default = 0 , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) \n    prerank_opt . add_argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) \n    prerank_opt . add_argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = 1 , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) \n    return "}
{"4158": "\ndef add_plot_parser ( subparsers ) : \n    argparser_replot = subparsers . add_parser ( \"replot\" , help = \"Reproduce GSEA desktop output figures.\" ) \n    group_replot = argparser_replot . add_argument_group ( \"Input arguments\" ) \n    group_replot . add_argument ( \"-i\" , \"--indir\" , action = \"store\" , dest = \"indir\" , required = 1 , metavar = 'GSEA_dir' , help = \"The GSEA desktop results directroy that you want to reproduce the figure \" ) \n    add_output_option ( group_replot ) \n    group_replot . add_argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. Please Use the same value in GSEA. Choose from (0, 1, 1.5, 2),default: 1' , ) \n    return "}
{"4159": "\ndef add_enrichr_parser ( subparsers ) : \n    argparser_enrichr = subparsers . add_parser ( \"enrichr\" , help = \"Using Enrichr API to perform GO analysis.\" ) \n    enrichr_opt = argparser_enrichr . add_argument_group ( \"Input arguments\" ) \n    enrichr_opt . add_argument ( \"-i\" , \"--input-list\" , action = \"store\" , dest = \"gene_list\" , type = str , required = 1 , metavar = 'IDs' , help = \"Enrichr uses a list of gene names as input.\" ) \n    enrichr_opt . add_argument ( \"-g\" , \"--gene-sets\" , action = \"store\" , dest = \"library\" , type = str , required = 1 , metavar = 'GMT' , help = \"Enrichr library name(s) required. Separate each name by comma.\" ) \n    enrichr_opt . add_argument ( \"--org\" , \"--organism\" , action = \"store\" , dest = \"organism\" , type = str , default = '' , help = \"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/modEnrichr.\" ) \n    enrichr_opt . add_argument ( \"--ds\" , \"--description\" , action = \"store\" , dest = \"descrip\" , type = str , default = 'enrichr' , metavar = 'STRING' , help = \"It is recommended to enter a short description for your list so that multiple lists \\                              can be differentiated from each other if you choose to save or share your list.\" ) \n    enrichr_opt . add_argument ( \"--cut\" , \"--cut-off\" , action = \"store\" , dest = \"thresh\" , metavar = 'float' , type = float , default = 0.05 , help = \"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\" ) \n    enrichr_opt . add_argument ( \"--bg\" , \"--background\" , action = \"store\" , dest = \"bg\" , default = 'hsapiens_gene_ensembl' , metavar = 'BGNUM' , help = \"BioMart Dataset name or Background total genes number. Default: None\" ) \n    enrichr_opt . add_argument ( \"-t\" , \"--top-term\" , dest = \"term\" , action = \"store\" , type = int , default = 10 , metavar = 'int' , help = \"Numbers of top terms shown in the plot. Default: 10\" ) \n    enrichr_output = argparser_enrichr . add_argument_group ( \"Output figure arguments\" ) \n    add_output_option ( enrichr_output ) \n    return "}
{"4160": "\ndef enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = 1 , nperm = 1000 , rs = np . random . RandomState ( ) , single = 0 , scale = 0 ) : \n    N = len ( gene_list ) \n    tag_indicator = np . in1d ( gene_list , gene_set , assume_unique = 1 ) . astype ( int ) \n    if weighted_score_type == 0 : \n        correl_vector = np . repeat ( 1 , N ) \n    else : \n        correl_vector = np . abs ( correl_vector ) ** weighted_score_type \n    hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) \n    axis = 1 \n    tag_indicator = np . tile ( tag_indicator , ( nperm + 1 , 1 ) ) \n    correl_vector = np . tile ( correl_vector , ( nperm + 1 , 1 ) ) \n    for i in range ( nperm ) : \n        rs . shuffle ( tag_indicator [ i ] ) \n    Nhint = tag_indicator . sum ( axis = axis , keepdims = 1 ) \n    sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = 1 ) \n    no_tag_indicator = 1 - tag_indicator \n    Nmiss = N - Nhint \n    norm_tag = 1.0 / sum_correl_tag \n    norm_no_tag = 1.0 / Nmiss \n    RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) \n    if scale : \n        RES = RES / N \n    if single : \n        es_vec = RES . sum ( axis = axis ) \n    else : \n        max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) \n        es_vec = np . where ( np . abs ( max_ES ) > np . abs ( min_ES ) , max_ES , min_ES ) \n    es , esnull , RES = es_vec [ - 1 ] , es_vec [ : - 1 ] , RES [ - 1 , : ] \n    return es , esnull , hit_ind , RES "}
{"4166": "\ndef get_datasets ( self , mart = 'ENSEMBL_MART_ENSEMBL' ) : \n    datasets = self . datasets ( mart , raw = 1 ) \n    return pd . read_csv ( StringIO ( datasets ) , header = None , usecols = [ 1 , 2 ] , names = [ \"Name\" , \"Description\" ] , sep = \"\\t\" ) "}
{"4169": "\ndef query ( self , dataset = 'hsapiens_gene_ensembl' , attributes = [ ] , filters = { } , filename = None ) : \n    if not attributes : \n        attributes = [ 'ensembl_gene_id' , 'external_gene_name' , 'entrezgene' , 'go_id' ] \n    self . new_query ( ) \n    self . add_dataset_to_xml ( dataset ) \n    for at in attributes : \n        self . add_attribute_to_xml ( at ) \n    if filters : \n        for k , v in filters . items ( ) : \n            if isinstance ( v , list ) : \n                v = \",\" . join ( v ) \n            self . add_filter_to_xml ( k , v ) \n    xml_query = self . get_xml ( ) \n    results = super ( Biomart , self ) . query ( xml_query ) \n    df = pd . read_csv ( StringIO ( results ) , header = None , sep = \"\\t\" , names = attributes , index_col = None ) \n    if filename is None : \n        mkdirs ( DEFAULT_CACHE_PATH ) \n        filename = os . path . join ( DEFAULT_CACHE_PATH , \"{}.background.genes.txt\" . format ( dataset ) ) \n    df . to_csv ( filename , sep = \"\\t\" , index = 0 ) \n    return df "}
{"4170": "\ndef gsea ( data , gene_sets , cls , outdir = 'GSEA_' , min_size = 15 , max_size = 500 , permutation_num = 1000 , weighted_score_type = 1 , permutation_type = 'gene_set' , method = 'log2_ratio_of_classes' , ascending = 0 , processes = 1 , figsize = ( 6.5 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = 0 , seed = None , verbose = 0 ) : \n    gs = GSEA ( data , gene_sets , cls , outdir , min_size , max_size , permutation_num , weighted_score_type , permutation_type , method , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    gs . run ( ) \n    return gs "}
{"4171": "\ndef ssgsea ( data , gene_sets , outdir = \"ssGSEA_\" , sample_norm_method = 'rank' , min_size = 15 , max_size = 2000 , permutation_num = 0 , weighted_score_type = 0.25 , scale = 1 , ascending = 0 , processes = 1 , figsize = ( 7 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = 0 , seed = None , verbose = 0 ) : \n    ss = SingleSampleGSEA ( data , gene_sets , outdir , sample_norm_method , min_size , max_size , permutation_num , weighted_score_type , scale , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    ss . run ( ) \n    return ss "}
{"4172": "\ndef prerank ( rnk , gene_sets , outdir = 'GSEA_Prerank' , pheno_pos = 'Pos' , pheno_neg = 'Neg' , min_size = 15 , max_size = 500 , permutation_num = 1000 , weighted_score_type = 1 , ascending = 0 , processes = 1 , figsize = ( 6.5 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = 0 , seed = None , verbose = 0 ) : \n    pre = Prerank ( rnk , gene_sets , outdir , pheno_pos , pheno_neg , min_size , max_size , permutation_num , weighted_score_type , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    pre . run ( ) \n    return pre "}
{"4173": "\ndef replot ( indir , outdir = 'GSEA_Replot' , weighted_score_type = 1 , min_size = 3 , max_size = 1000 , figsize = ( 6.5 , 6 ) , graph_num = 20 , format = 'pdf' , verbose = 0 ) : \n    rep = Replot ( indir , outdir , weighted_score_type , min_size , max_size , figsize , graph_num , format , verbose ) \n    rep . run ( ) \n    return "}
{"4175": "\ndef load_gmt ( self , gene_list , gmt ) : \n    if isinstance ( gmt , dict ) : \n        genesets_dict = gmt \n    elif isinstance ( gmt , str ) : \n        genesets_dict = self . parse_gmt ( gmt ) \n    else : \n        raise Exception ( \"Error parsing gmt parameter for gene sets\" ) \n    subsets = list ( genesets_dict . keys ( ) ) \n    self . n_genesets = len ( subsets ) \n    for subset in subsets : \n        subset_list = genesets_dict . get ( subset ) \n        if isinstance ( subset_list , set ) : \n            subset_list = list ( subset_list ) \n            genesets_dict [ subset ] = subset_list \n        tag_indicator = np . in1d ( gene_list , subset_list , assume_unique = 1 ) \n        tag_len = tag_indicator . sum ( ) \n        if self . min_size <= tag_len <= self . max_size : \n            continue \n        del genesets_dict [ subset ] \n    filsets_num = len ( subsets ) - len ( genesets_dict ) \n    self . _logger . info ( \"%04d gene_sets have been filtered out when max_size=%s and min_size=%s\" % ( filsets_num , self . max_size , self . min_size ) ) \n    if filsets_num == len ( subsets ) : \n        self . _logger . error ( \"No gene sets passed through filtering condition!!!, try new parameters again!\\n\" + \"Note: check gene name, gmt file format, or filtering size.\" ) \n        sys . exit ( 0 ) \n    self . _gmtdct = genesets_dict \n    return genesets_dict "}
{"4178": "\ndef _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : \n    width = len ( classes ) if len ( classes ) >= 6 else 5 \n    cls_booA = list ( map ( lambda x : 1 if x == pheno_pos else 0 , classes ) ) \n    cls_booB = list ( map ( lambda x : 1 if x == pheno_neg else 0 , classes ) ) \n    datA = df . loc [ : , cls_booA ] \n    datB = df . loc [ : , cls_booB ] \n    datAB = pd . concat ( [ datA , datB ] , axis = 1 ) \n    self . _width = width \n    self . heatmat = datAB \n    return "}
{"4179": "\ndef _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : \n    res = OrderedDict ( ) \n    for gs , gseale , ind , RES in zipdata : \n        rdict = OrderedDict ( ) \n        rdict [ 'es' ] = gseale [ 0 ] \n        rdict [ 'nes' ] = gseale [ 1 ] \n        rdict [ 'pval' ] = gseale [ 2 ] \n        rdict [ 'fdr' ] = gseale [ 3 ] \n        rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) \n        rdict [ 'matched_size' ] = len ( ind ) \n        _genes = rank_metric . index . values [ ind ] \n        rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) \n        if self . module != 'ssgsea' : \n            if rdict [ 'es' ] > 0 : \n                idx = RES . argmax ( ) \n                ldg_pos = list ( filter ( lambda x : x <= idx , ind ) ) \n            elif rdict [ 'es' ] < 0 : \n                idx = RES . argmin ( ) \n                ldg_pos = list ( filter ( lambda x : x >= idx , ind ) ) \n            else : \n                ldg_pos = ind \n            rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) \n        rdict [ 'RES' ] = RES \n        rdict [ 'hits_indices' ] = ind \n        res [ gs ] = rdict \n    self . results = res \n    res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) \n    res_df . index . name = 'Term' \n    res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = 1 ) \n    res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = 1 ) \n    self . res2d = res_df \n    if self . _outdir is None : \n        return \n    out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) \n    if self . module == 'ssgsea' : \n        out = out . replace ( \".csv\" , \".txt\" ) \n        with open ( out , 'a' ) as f : \n            f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\\n' ) \n            f . write ( \"# might not proper for publication\\n\" ) \n            res_df . to_csv ( f , sep = '\\t' ) \n    else : \n        res_df . to_csv ( out ) \n    return "}
{"4180": "\ndef load_data ( self , cls_vec ) : \n    if isinstance ( self . data , pd . DataFrame ) : \n        exprs = self . data . copy ( ) \n        if exprs . index . dtype == 'O' : \n            exprs = exprs . reset_index ( ) \n    elif os . path . isfile ( self . data ) : \n        if self . data . endswith ( \"gct\" ) : \n            exprs = pd . read_csv ( self . data , skiprows = 1 , comment = '#' , sep = \"\\t\" ) \n        else : \n            exprs = pd . read_csv ( self . data , comment = '#' , sep = \"\\t\" ) \n    else : \n        raise Exception ( 'Error parsing gene expression DataFrame!' ) \n    if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : \n        self . _logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) \n        exprs . drop_duplicates ( subset = exprs . columns [ 0 ] , inplace = 1 ) \n    if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : \n        self . _logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) \n        exprs . dropna ( how = 'all' , inplace = 1 ) \n        exprs = exprs . fillna ( 0 ) \n    exprs . set_index ( keys = exprs . columns [ 0 ] , inplace = 1 ) \n    df = exprs . select_dtypes ( include = [ np . number ] ) \n    df_std = df . groupby ( by = cls_vec , axis = 1 ) . std ( ) \n    df = df [ ~ df_std . isin ( [ 0 ] ) . any ( axis = 1 ) ] \n    df = df + 0.00001 \n    return df "}
{"4183": "\ndef runSamplesPermu ( self , df , gmt = None ) : \n    assert self . min_size <= self . max_size \n    mkdirs ( self . outdir ) \n    self . resultsOnSamples = OrderedDict ( ) \n    outdir = self . outdir \n    for name , ser in df . iteritems ( ) : \n        self . outdir = os . path . join ( outdir , str ( name ) ) \n        self . _logger . info ( \"Run Sample: %s \" % name ) \n        mkdirs ( self . outdir ) \n        dat2 = ser . sort_values ( ascending = self . ascending ) \n        gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = '' , pheno_neg = '' , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed , single = 1 , scale = self . scale ) \n        res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n        self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = \"gene_sets\" ) \n        self . resultsOnSamples [ name ] = self . res2d . es \n        if self . _noplot : \n            continue \n        self . _logger . info ( \"Plotting Sample: %s \\n\" % name ) \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format ) \n    self . _save ( outdir ) \n    return "}
{"4184": "\ndef runSamples ( self , df , gmt = None ) : \n    self . resultsOnSamples = OrderedDict ( ) \n    outdir = self . outdir \n    subsets = sorted ( gmt . keys ( ) ) \n    tempes = [ ] \n    names = [ ] \n    rankings = [ ] \n    pool = Pool ( processes = self . _processes ) \n    for name , ser in df . iteritems ( ) : \n        dat = ser . sort_values ( ascending = self . ascending ) \n        rankings . append ( dat ) \n        names . append ( name ) \n        genes_sorted , cor_vec = dat . index . values , dat . values \n        rs = np . random . RandomState ( self . seed ) \n        tempes . append ( pool . apply_async ( enrichment_score_tensor , args = ( genes_sorted , cor_vec , gmt , self . weighted_score_type , self . permutation_num , rs , 1 , self . scale ) ) ) \n    pool . close ( ) \n    pool . join ( ) \n    for i , temp in enumerate ( tempes ) : \n        name , rnk = names [ i ] , rankings [ i ] \n        self . _logger . info ( \"Calculate Enrichment Score for Sample: %s \" % name ) \n        es , esnull , hit_ind , RES = temp . get ( ) \n        self . outdir = os . path . join ( outdir , str ( name ) ) \n        mkdirs ( self . outdir ) \n        self . resultsOnSamples [ name ] = pd . Series ( data = es , index = subsets , name = name ) \n        if self . _noplot : \n            continue \n        self . _logger . info ( \"Plotting Sample: %s \\n\" % name ) \n        for i , term in enumerate ( subsets ) : \n            term = term . replace ( '/' , '_' ) . replace ( \":\" , \"_\" ) \n            outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) \n            gseaplot ( rank_metric = rnk , term = term , hits_indices = hit_ind [ i ] , nes = es [ i ] , pval = 1 , fdr = 1 , RES = RES [ i ] , pheno_pos = '' , pheno_neg = '' , figsize = self . figsize , ofname = outfile ) \n    self . _save ( outdir ) \n    return "}
{"4185": "\ndef _save ( self , outdir ) : \n    samplesRawES = pd . DataFrame ( self . resultsOnSamples ) \n    samplesRawES . index . name = 'Term|ES' \n    samplesNES = samplesRawES / ( samplesRawES . values . max ( ) - samplesRawES . values . min ( ) ) \n    samplesNES = samplesNES . copy ( ) \n    samplesNES . index . rename ( 'Term|NES' , inplace = 1 ) \n    self . res2d = samplesNES \n    self . _logger . info ( \"Congratulations. GSEApy runs successfully................\\n\" ) \n    if self . _outdir is None : \n        return \n    outESfile = os . path . join ( outdir , \"gseapy.samples.raw.es.txt\" ) \n    with open ( outESfile , 'a' ) as f : \n        if self . scale : \n            f . write ( '# scale the enrichment scores by number of genes in the gene sets\\n' ) \n            f . write ( '# this normalization has not effects on the final NES, ' + 'as indicated by Barbie et al., 2009, online methods, pg. 2\\n' ) \n        else : \n            f . write ( '# raw enrichment scores of all data\\n' ) \n            f . write ( '# no scale es by numbers of genes in the gene sets\\n' ) \n        samplesRawES . to_csv ( f , sep = '\\t' ) \n    outNESfile = os . path . join ( outdir , \"gseapy.samples.normalized.es.txt\" ) \n    with open ( outNESfile , 'a' ) as f : \n        f . write ( '# normalize enrichment scores by using the entire data set\\n' ) \n        f . write ( '# as indicated by Barbie et al., 2009, online methods, pg. 2\\n' ) \n        samplesNES . to_csv ( f , sep = '\\t' ) \n    return "}
{"4187": "\ndef enrichr ( gene_list , gene_sets , organism = 'human' , description = '' , outdir = 'Enrichr' , background = 'hsapiens_gene_ensembl' , cutoff = 0.05 , format = 'pdf' , figsize = ( 8 , 6 ) , top_term = 10 , no_plot = 0 , verbose = 0 ) : \n    enr = Enrichr ( gene_list , gene_sets , organism , description , outdir , cutoff , background , format , figsize , top_term , no_plot , verbose ) \n    enr . run ( ) \n    return enr "}
{"4192": "\ndef get_background ( self ) : \n    if os . path . isfile ( self . background ) : \n        with open ( self . background ) as b : \n            bg2 = b . readlines ( ) \n        bg = [ g . strip ( ) for g in bg2 ] \n        return set ( bg ) \n    DB_FILE = resource_filename ( \"gseapy\" , \"data/{}.background.genes.txt\" . format ( self . background ) ) \n    filename = os . path . join ( DEFAULT_CACHE_PATH , \"{}.background.genes.txt\" . format ( self . background ) ) \n    if os . path . exists ( filename ) : \n        df = pd . read_csv ( filename , sep = \"\\t\" ) \n    elif os . path . exists ( DB_FILE ) : \n        df = pd . read_csv ( DB_FILE , sep = \"\\t\" ) \n    else : \n        self . _logger . warning ( \"Downloading %s for the first time. It might take a couple of miniutes.\" % self . background ) \n        bm = Biomart ( ) \n        df = bm . query ( dataset = self . background ) \n        df . dropna ( subset = [ 'go_id' ] , inplace = 1 ) \n    self . _logger . info ( \"using all annotated genes with GO_ID as background genes\" ) \n    df . dropna ( subset = [ 'entrezgene' ] , inplace = 1 ) \n    if self . _isezid : \n        bg = df [ 'entrezgene' ] . astype ( int ) \n    else : \n        bg = df [ 'external_gene_name' ] \n    return set ( bg ) "}
{"4193": "\ndef run ( self ) : \n    self . get_organism ( ) \n    genes_list = self . parse_genelists ( ) \n    gss = self . parse_genesets ( ) \n    self . _logger . info ( \"Connecting to Enrichr Server to get latest library names\" ) \n    if len ( gss ) < 1 : \n        sys . stderr . write ( \"Not validated Enrichr library name provided\\n\" ) \n        sys . stdout . write ( \"Hint: use get_library_name() to view full list of supported names\" ) \n        sys . exit ( 1 ) \n    self . results = pd . DataFrame ( ) \n    for g in gss : \n        if isinstance ( g , dict ) : \n            res = self . enrich ( g ) \n            shortID , self . _gs = str ( id ( g ) ) , \"CUSTOM%s\" % id ( g ) \n            if res is None : \n                self . _logger . info ( \"No hits return, for gene set: Custom%s\" % shortID ) \n                continue \n        else : \n            self . _gs = str ( g ) \n            self . _logger . debug ( \"Start Enrichr using library: %s\" % ( self . _gs ) ) \n            self . _logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . _gs ) ) \n            shortID , res = self . get_results ( genes_list ) \n        res . insert ( 0 , \"Gene_set\" , self . _gs ) \n        self . results = self . results . append ( res , ignore_index = 1 , sort = 1 ) \n        self . res2d = res \n        if self . _outdir is None : \n            continue \n        self . _logger . info ( 'Save file of enrichment results: Job Id:' + str ( shortID ) ) \n        outfile = \"%s/%s.%s.%s.reports.txt\" % ( self . outdir , self . _gs , self . descriptions , self . module ) \n        self . res2d . to_csv ( outfile , index = 0 , encoding = 'utf-8' , sep = \"\\t\" ) \n        if not self . __no_plot : \n            msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top_term = self . __top_term , color = 'salmon' , title = self . _gs , ofname = outfile . replace ( \"txt\" , self . format ) ) \n            if msg is not None : \n                self . _logger . warning ( msg ) \n        self . _logger . info ( 'Done.\\n' ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4194": "\ndef cube ( script , size = 1.0 , center = 0 , color = None ) : \n    size = util . make_list ( size , 3 ) \n    if script . ml_version == '1.3.4BETA' : \n        filter_name = 'Box' \n    else : \n        filter_name = 'Box/Cube' \n    filter_xml = '' . join ( [ '  <filter name=\"{}\">\\n' . format ( filter_name ) , '    <Param name=\"size\" ' , 'value=\"1.0\" ' , 'description=\"Scale factor\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cube' , change_layer = 1 ) \n    transform . scale ( script , value = size ) \n    if not center : \n        transform . translate ( script , value = [ size [ 0 ] / 2 , size [ 1 ] / 2 , size [ 2 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4195": "\ndef icosphere ( script , radius = 1.0 , diameter = None , subdivisions = 3 , color = None ) : \n    if diameter is not None : \n        radius = diameter / 2 \n    filter_xml = '' . join ( [ '  <filter name=\"Sphere\">\\n' , '    <Param name=\"radius\" ' , 'value=\"%s\" ' % radius , 'description=\"Radius\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"subdiv\" ' , 'value=\"%d\" ' % subdivisions , 'description=\"Subdiv. Level\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Sphere' , change_layer = 1 ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4196": "\ndef torus ( script , major_radius = 3.0 , minor_radius = 1.0 , inner_diameter = None , outer_diameter = None , major_segments = 48 , minor_segments = 12 , color = None ) : \n    if inner_diameter is not None and outer_diameter is not None : \n        major_radius = ( inner_diameter + outer_diameter ) / 4 \n        minor_radius = major_radius - inner_diameter / 2 \n    filter_xml = '' . join ( [ '  <filter name=\"Torus\">\\n' , '    <Param name=\"hRadius\" ' , 'value=\"%s\" ' % major_radius , 'description=\"Horizontal Radius\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"vRadius\" ' , 'value=\"%s\" ' % minor_radius , 'description=\"Vertical Radius\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"hSubdiv\" ' , 'value=\"%d\" ' % major_segments , 'description=\"Horizontal Subdivision\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"vSubdiv\" ' , 'value=\"%d\" ' % minor_segments , 'description=\"Vertical Subdivision\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Torus' , change_layer = 1 ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4197": "\ndef plane_hires_edges ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , center = 0 , color = None ) : \n    size = util . make_list ( size , 2 ) \n    grid ( script , size = [ x_segments + y_segments - 1 , 1 ] , x_segments = ( x_segments + y_segments - 1 ) , y_segments = 1 ) \n    if ml_script1 . ml_version == '1.3.4BETA' : \n        and_val = 'and' \n    else : \n        and_val = '&&' \n    if script . ml_version == '1.3.4BETA' : \n        transform . vert_function ( script , x_func = 'if((y>0) and (x<%s),0,x)' % ( y_segments ) , y_func = 'if((y>0) and (x<%s),(x+1)*%s,y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = 'if((y>0) and (x>=%s),%s,y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x>%s),%s,x)' % ( x_segments , size [ 0 ] ) , y_func = 'if((y<.00001) and (x>%s),(x-%s)*%s,y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = 'if((y<.00001) and (x<=%s) and (x>0),0,y)' % ( x_segments ) ) \n    else : \n        transform . vert_function ( script , x_func = '((y>0) && (x<{yseg}) ? 0 : x)' . format ( yseg = y_segments ) , y_func = '((y>0) && (x<%s) ? (x+1)*%s : y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = '((y>0) && (x>=%s) ? %s : y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x>%s) ? %s : x)' % ( x_segments , size [ 0 ] ) , y_func = '((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = '((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % ( x_segments ) ) \n    if center : \n        transform . translate ( script , [ - size [ 0 ] / 2 , - size [ 1 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4198": "\ndef cube_hires ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , z_segments = 1 , simple_bottom = 1 , center = 0 , color = None ) : \n    size = util . make_list ( size , 3 ) \n    grid ( script , size , x_segments , y_segments ) \n    transform . translate ( script , [ 0 , 0 , size [ 2 ] ] ) \n    if simple_bottom : \n        plane_hires_edges ( script , size , x_segments , y_segments ) \n    else : \n        layers . duplicate ( script ) \n        transform . translate ( script , [ 0 , 0 , - size [ 2 ] ] ) \n    transform . rotate ( script , 'x' , 180 ) \n    transform . translate ( script , [ 0 , size [ 1 ] , 0 ] ) \n    cube_open_hires ( script = script , size = size , x_segments = x_segments , y_segments = y_segments , z_segments = z_segments ) \n    layers . join ( script ) \n    clean . merge_vert ( script , threshold = 0.00002 ) \n    if center : \n        transform . translate ( script , [ - size [ 0 ] / 2 , - size [ 1 ] / 2 , - size [ 2 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4199": "\ndef color_values ( color ) : \n    this_dir = os . path . dirname ( os . path . realpath ( inspect . getsourcefile ( lambda : 0 ) ) ) \n    color_name_file = os . path . join ( this_dir , 'color_names.txt' ) \n    found = 0 \n    for line in open ( color_name_file , 'r' ) : \n        line = line . rstrip ( ) \n        if color . lower ( ) == line . split ( ) [ 0 ] : \n            red = line . split ( ) [ 2 ] \n            green = line . split ( ) [ 3 ] \n            blue = line . split ( ) [ 4 ] \n            found = 1 \n            break \n    if not found : \n        print ( 'Color name \"%s\" not found, using default (white)' % color ) \n        red = 255 \n        green = 255 \n        blue = 255 \n    return red , green , blue "}
{"4203": "\ndef ls3loop ( script , iterations = 1 , loop_weight = 0 , edge_threshold = 0 , selected = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Subdivision Surfaces: LS3 Loop\">\\n' , '    <Param name=\"LoopWeight\" ' , 'value=\"{:d}\" ' . format ( loop_weight ) , 'description=\"Weighting scheme\" ' , 'enum_val0=\"Loop\" ' , 'enum_val1=\"Enhance regularity\" ' , 'enum_val2=\"Enhance continuity\" ' , 'enum_cardinality=\"3\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"Iterations\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Iterations\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Threshold\" ' , 'value=\"{}\" ' . format ( edge_threshold ) , 'description=\"Edge Threshold\" ' , 'min=\"0\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4205": "\ndef close_holes ( script , hole_max_edge = 30 , selected = 0 , sel_new_face = 1 , self_intersection = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Close Holes\">\\n' , '    <Param name=\"maxholesize\" ' , 'value=\"{:d}\" ' . format ( hole_max_edge ) , 'description=\"Max size to be closed\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Close holes with selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"NewFaceSelected\" ' , 'value=\"{}\" ' . format ( str ( sel_new_face ) . lower ( ) ) , 'description=\"Select the newly created faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SelfIntersection\" ' , 'value=\"{}\" ' . format ( str ( self_intersection ) . lower ( ) ) , 'description=\"Prevent creation of selfIntersecting faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4207": "\ndef snap_mismatched_borders ( script , edge_dist_ratio = 0.01 , unify_vert = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Snap Mismatched Borders\">\\n' , '    <Param name=\"EdgeDistRatio\" ' , 'value=\"{}\" ' . format ( edge_dist_ratio ) , 'description=\"Edge Distance Ratio\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"UnifyVertices\" ' , 'value=\"{}\" ' . format ( str ( unify_vert ) . lower ( ) ) , 'description=\"UnifyVertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4213": "\ndef bend ( script , radius = 1 , pitch = 0 , taper = 0 , angle = 0 , straght_start = 1 , straght_end = 0 , radius_limit = None , outside_limit_end = 1 ) : \n    if radius_limit is None : \n        radius_limit = 2 * radius \n    angle = math . radians ( angle ) \n    segment = radius * angle \n    pitch_func = '-(pitch)*x/(2*pi*(radius))' . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) . replace ( 'radius' , str ( radius ) ) \n    taper_func = '(taper)*(pitch_func)' . replace ( 'taper' , str ( taper ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pi' , str ( math . pi ) ) \n    if outside_limit_end : \n        x_func = 'if(x<(segment) and y<(radius_limit), if(x>0, (y+(radius)+(taper_func))*sin(x/(radius)), x), (y+(radius)+(taper_func))*sin(angle)+(x-(segment))*cos(angle))' \n    else : \n        x_func = 'if(x<(segment), if(x>0 and y<(radius_limit), (y+(radius)+(taper_func))*sin(x/(radius)), x), if(y<(radius_limit), (y+(radius)+(taper_func))*sin(angle)+(x-(segment))*cos(angle), x))' \n    x_func = x_func . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) . replace ( 'angle' , str ( angle ) ) \n    if outside_limit_end : \n        y_func = 'if(x<(segment) and y<(radius_limit), if(x>0, (y+(radius)+(taper_func))*cos(x/(radius))-(radius), y), (y+(radius)+(taper_func))*cos(angle)-(x-(segment))*sin(angle)-(radius))' \n    else : \n        y_func = 'if(x<(segment), if(x>0 and y<(radius_limit), (y+(radius)+(taper_func))*cos(x/(radius))-(radius), y), if(y<(radius_limit), (y+(radius)+(taper_func))*cos(angle)-(x-(segment))*sin(angle)-(radius), y))' \n    y_func = y_func . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) . replace ( 'angle' , str ( angle ) ) \n    if straght_start : \n        start = 'z' \n    else : \n        start = 'z+(pitch_func)' \n    if straght_end : \n        end = 'z-(pitch)*(angle)/(2*pi)' \n    else : \n        end = 'z+(pitch_func)' \n    if outside_limit_end : \n        z_func = 'if(x<(segment) and y<(radius_limit), if(x>0, z+(pitch_func), (start)), (end))' \n    else : \n        z_func = 'if(x<(segment), if(x>0 and y<(radius_limit), z+(pitch_func), (start)), if(y<(radius_limit), (end), z))' \n    z_func = z_func . replace ( 'start' , str ( start ) ) . replace ( 'end' , str ( end ) ) . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'angle' , str ( angle ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) \n    vert_function ( script , x_func = x_func , y_func = y_func , z_func = z_func ) \n    return None "}
{"4215": "\ndef vc2tex ( script , tex_name = 'TEMP3D_texture.png' , tex_width = 1024 , tex_height = 1024 , overwrite_tex = 0 , assign_tex = 0 , fill_tex = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Vertex Color to Texture\">\\n' , '    <Param name=\"textName\" ' , 'value=\"%s\" ' % tex_name , 'description=\"Texture file\" ' , 'type=\"RichString\" ' , '/>\\n' , '    <Param name=\"textW\" ' , 'value=\"%d\" ' % tex_width , 'description=\"Texture width (px)\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"textH\" ' , 'value=\"%d\" ' % tex_height , 'description=\"Texture height (px)\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"overwrite\" ' , 'value=\"%s\" ' % str ( overwrite_tex ) . lower ( ) , 'description=\"Overwrite texture\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"assign\" ' , 'value=\"%s\" ' % str ( assign_tex ) . lower ( ) , 'description=\"Assign Texture\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"pullpush\" ' , 'value=\"%s\" ' % str ( fill_tex ) . lower ( ) , 'description=\"Fill texture\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4216": "\ndef mesh2fc ( script , all_visible_layers = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Transfer Color: Mesh to Face\">\\n' , '    <Param name=\"allVisibleMesh\" ' , 'value=\"%s\" ' % str ( all_visible_layers ) . lower ( ) , 'description=\"Apply to all Meshes\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4217": "\ndef uniform_resampling ( script , voxel = 1.0 , offset = 0.0 , merge_vert = 1 , discretize = 0 , multisample = 0 , thicken = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Uniform Mesh Resampling\">\\n' , '    <Param name=\"CellSize\" ' , 'value=\"{}\" ' . format ( voxel ) , 'description=\"Precision\" ' , 'min=\"0\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Offset\" ' , 'value=\"{}\" ' . format ( offset ) , 'description=\"Offset\" ' , 'min=\"-100\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"mergeCloseVert\" ' , 'value=\"{}\" ' . format ( str ( merge_vert ) . lower ( ) ) , 'description=\"Clean Vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"discretize\" ' , 'value=\"{}\" ' . format ( str ( discretize ) . lower ( ) ) , 'description=\"Discretize\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"multisample\" ' , 'value=\"{}\" ' . format ( str ( multisample ) . lower ( ) ) , 'description=\"Multisample\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"absDist\" ' , 'value=\"{}\" ' . format ( str ( thicken ) . lower ( ) ) , 'description=\"Absolute Distance\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Offset mesh' ) \n    return None "}
{"4218": "\ndef surface_poisson_screened ( script , visible_layer = 0 , depth = 8 , full_depth = 5 , cg_depth = 0 , scale = 1.1 , samples_per_node = 1.5 , point_weight = 4.0 , iterations = 8 , confidence = 0 , pre_clean = 0 ) : \n    filter_xml = '' . join ( [ '  <xmlfilter name=\"Screened Poisson Surface Reconstruction\">\\n' , '    <xmlparam name=\"cgDepth\" value=\"{:d}\"/>\\n' . format ( cg_depth ) , '    <xmlparam name=\"confidence\" value=\"{}\"/>\\n' . format ( str ( confidence ) . lower ( ) ) , '    <xmlparam name=\"depth\" value=\"{:d}\"/>\\n' . format ( depth ) , '    <xmlparam name=\"fullDepth\" value=\"{:d}\"/>\\n' . format ( full_depth ) , '    <xmlparam name=\"iters\" value=\"{:d}\"/>\\n' . format ( iterations ) , '    <xmlparam name=\"pointWeight\" value=\"{}\"/>\\n' . format ( point_weight ) , '    <xmlparam name=\"preClean\" value=\"{}\"/>\\n' . format ( str ( pre_clean ) . lower ( ) ) , '    <xmlparam name=\"samplesPerNode\" value=\"{}\"/>\\n' . format ( samples_per_node ) , '    <xmlparam name=\"scale\" value=\"{}\"/>\\n' . format ( scale ) , '    <xmlparam name=\"visibleLayer\" value=\"{}\"/>\\n' . format ( str ( visible_layer ) . lower ( ) ) , '  </xmlfilter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Poisson mesh' , change_layer = 0 ) \n    return None "}
{"4219": "\ndef voronoi ( script , hole_num = 50 , target_layer = None , sample_layer = None , thickness = 0.5 , backward = 1 ) : \n    if target_layer is None : \n        target_layer = script . current_layer ( ) \n    if sample_layer is None : \n        sampling . poisson_disk ( script , sample_num = hole_num ) \n        sample_layer = script . last_layer ( ) \n    vert_color . voronoi ( script , target_layer = target_layer , source_layer = sample_layer , backward = backward ) \n    select . vert_quality ( script , min_quality = 0.0 , max_quality = thickness ) \n    if backward : \n        select . invert ( script ) \n    delete . selected ( script ) \n    smooth . laplacian ( script , iterations = 3 ) \n    return None "}
{"4220": "\ndef all ( script , face = 1 , vert = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Select All\">\\n' , '    <Param name=\"allFaces\" ' , 'value=\"{}\" ' . format ( str ( face ) . lower ( ) ) , 'description=\"DSelect all Faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"allVerts\" ' , 'value=\"{}\" ' . format ( str ( vert ) . lower ( ) ) , 'description=\"Select all Vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4221": "\ndef vert_quality ( script , min_quality = 0.0 , max_quality = 0.05 , inclusive = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Select by Vertex Quality\">\\n' , '    <Param name=\"minQ\" ' , 'value=\"{}\" ' . format ( min_quality ) , 'description=\"Min Quality\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( 2 * max_quality ) , 'type=\"RichDynamicFloat\" ' , '/>\\n' , '    <Param name=\"maxQ\" ' , 'value=\"{}\" ' . format ( max_quality ) , 'description=\"Max Quality\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( 2 * max_quality ) , 'type=\"RichDynamicFloat\" ' , '/>\\n' , '    <Param name=\"Inclusive\" ' , 'value=\"{}\" ' . format ( str ( inclusive ) . lower ( ) ) , 'description=\"Inclusive Sel.\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4223": "\ndef vert_function ( script , function = '(q < 0)' , strict_face_select = 1 ) : \n    if script . ml_version == '1.3.4BETA' : \n        strict_select = '' . join ( [ '    <Param name=\"strictSelect\" ' , 'value=\"{}\" ' . format ( str ( strict_face_select ) . lower ( ) ) , 'description=\"Strict face selection\" ' , 'type=\"RichBool\" ' , '/>\\n' , ] ) \n    else : \n        strict_select = '' \n    filter_xml = '' . join ( [ '  <filter name=\"Conditional Vertex Selection\">\\n' , '    <Param name=\"condSelect\" ' , 'value=\"{}\" ' . format ( str ( function ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 'description=\"boolean function\" ' , 'type=\"RichString\" ' , '/>\\n' , strict_select , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4224": "\ndef cylindrical_vert ( script , radius = 1.0 , inside = 1 ) : \n    if inside : \n        function = 'sqrt(x^2+y^2)<={}' . format ( radius ) \n    else : \n        function = 'sqrt(x^2+y^2)>={}' . format ( radius ) \n    vert_function ( script , function = function ) \n    return None "}
{"4226": "\ndef join ( script , merge_visible = 1 , merge_vert = 0 , delete_layer = 1 , keep_unreferenced_vert = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Flatten Visible Layers\">\\n' , '    <Param name=\"MergeVisible\" ' , 'value=\"{}\" ' . format ( str ( merge_visible ) . lower ( ) ) , 'description=\"Merge Only Visible Layers\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"MergeVertices\" ' , 'value=\"{}\" ' . format ( str ( merge_vert ) . lower ( ) ) , 'description=\"Merge duplicate vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"DeleteLayer\" ' , 'value=\"{}\" ' . format ( str ( delete_layer ) . lower ( ) ) , 'description=\"Delete Layers\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"AlsoUnreferenced\" ' , 'value=\"{}\" ' . format ( str ( keep_unreferenced_vert ) . lower ( ) ) , 'description=\"Keep unreferenced vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , mlx . FilterScript ) : \n        script . add_layer ( 'Merged Mesh' ) \n        if delete_layer : \n            for i in range ( script . last_layer ( ) ) : \n                script . del_layer ( 0 ) \n    return None "}
{"4229": "\ndef duplicate ( script , layer_num = None ) : \n    filter_xml = '  <filter name=\"Duplicate Current layer\"/>\\n' \n    if isinstance ( script , mlx . FilterScript ) : \n        if ( layer_num is None ) or ( layer_num == script . current_layer ( ) ) : \n            util . write_filter ( script , filter_xml ) \n            script . add_layer ( '{}_copy' . format ( script . layer_stack [ script . current_layer ( ) ] ) , 1 ) \n        else : \n            change ( script , layer_num ) \n            util . write_filter ( script , filter_xml ) \n            script . add_layer ( '{}_copy' . format ( script . layer_stack [ layer_num ] ) , 1 ) \n    else : \n        util . write_filter ( script , filter_xml ) \n    return None "}
{"4231": "\ndef handle_error ( program_name , cmd , log = None ) : \n    print ( '\\nHouston, we have a problem.' , '\\n%s did not finish successfully. Review the log' % program_name , 'file and the input file(s) to see what went wrong.' ) \n    print ( '%s command: \"%s\"' % ( program_name , cmd ) ) \n    if log is not None : \n        print ( 'log: \"%s\"' % log ) \n    print ( 'Where do we go from here?' ) \n    print ( ' r  - retry running %s (probably after' % program_name , 'you\\'ve fixed any problems with the input files)' ) \n    print ( ' c  - continue on with the script (probably after' , 'you\\'ve manually re-run and generated the desired' , 'output file(s)' ) \n    print ( ' x  - exit, keeping the TEMP3D files and log' ) \n    print ( ' xd - exit, deleting the TEMP3D files and log' ) \n    while 1 : \n        choice = input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if choice == 'x' : \n        print ( 'Exiting ...' ) \n        sys . exit ( 1 ) \n    elif choice == 'xd' : \n        print ( 'Deleting TEMP3D* and log files and exiting ...' ) \n        util . delete_all ( 'TEMP3D*' ) \n        if log is not None : \n            os . remove ( log ) \n        sys . exit ( 1 ) \n    elif choice == 'c' : \n        print ( 'Continuing on ...' ) \n        break_now = 1 \n    elif choice == 'r' : \n        print ( 'Retrying %s cmd ...' % program_name ) \n        break_now = 0 \n    return break_now "}
{"4232": "\ndef begin ( script = 'TEMP3D_default.mlx' , file_in = None , mlp_in = None ) : \n    script_file = open ( script , 'w' ) \n    script_file . write ( '' . join ( [ '<!DOCTYPE FilterScript>\\n' , '<FilterScript>\\n' ] ) ) \n    script_file . close ( ) \n    current_layer = - 1 \n    last_layer = - 1 \n    stl = 0 \n    if mlp_in is not None : \n        if not isinstance ( mlp_in , list ) : \n            mlp_in = [ mlp_in ] \n        for val in mlp_in : \n            tree = ET . parse ( val ) \n            for elem in tree . iter ( tag = 'MLMesh' ) : \n                filename = ( elem . attrib [ 'filename' ] ) \n                current_layer += 1 \n                last_layer += 1 \n                if os . path . splitext ( filename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : \n                    layers . change ( script , current_layer ) \n                    clean . merge_vert ( script ) \n                    stl = 1 \n    if file_in is not None : \n        if not isinstance ( file_in , list ) : \n            file_in = [ file_in ] \n        for val in file_in : \n            current_layer += 1 \n            last_layer += 1 \n            if os . path . splitext ( val ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : \n                layers . change ( script , current_layer ) \n                clean . merge_vert ( script ) \n                stl = 1 \n    if stl : \n        layers . change ( script , last_layer ) \n    elif last_layer == - 1 : \n        file_in = [ 'TEMP3D.xyz' ] \n        file_in_descriptor = open ( file_in [ 0 ] , 'w' ) \n        file_in_descriptor . write ( '0 0 0' ) \n        file_in_descriptor . close ( ) \n        layers . delete ( script ) \n    return current_layer , last_layer "}
{"4233": "\ndef add_layer ( self , label , change_layer = 1 ) : \n    self . layer_stack . insert ( self . last_layer ( ) + 1 , label ) \n    if change_layer : \n        self . set_current_layer ( self . last_layer ( ) ) \n    return None "}
{"4236": "\ndef run_script ( self , log = None , ml_log = None , mlp_out = None , overwrite = 0 , file_out = None , output_mask = None , script_file = None , print_meshlabserver_output = 1 ) : \n    temp_script = 0 \n    temp_ml_log = 0 \n    if self . __no_file_in : \n        temp_file_in_file = tempfile . NamedTemporaryFile ( delete = 0 , suffix = '.xyz' , dir = os . getcwd ( ) ) \n        temp_file_in_file . write ( b'0 0 0' ) \n        temp_file_in_file . close ( ) \n        self . file_in = [ temp_file_in_file . name ] \n    if not self . filters : \n        script_file = None \n    elif script_file is None : \n        temp_script = 1 \n        temp_script_file = tempfile . NamedTemporaryFile ( delete = 0 , suffix = '.mlx' ) \n        temp_script_file . close ( ) \n        self . save_to_file ( temp_script_file . name ) \n        script_file = temp_script_file . name \n    if ( self . parse_geometry or self . parse_topology or self . parse_hausdorff ) and ( ml_log is None ) : \n        temp_ml_log = 1 \n        ml_log_file = tempfile . NamedTemporaryFile ( delete = 0 , suffix = '.txt' ) \n        ml_log_file . close ( ) \n        ml_log = ml_log_file . name \n    if file_out is None : \n        file_out = self . file_out \n    run ( script = script_file , log = log , ml_log = ml_log , mlp_in = self . mlp_in , mlp_out = mlp_out , overwrite = overwrite , file_in = self . file_in , file_out = file_out , output_mask = output_mask , ml_version = self . ml_version , print_meshlabserver_output = print_meshlabserver_output ) \n    if self . parse_geometry : \n        self . geometry = compute . parse_geometry ( ml_log , log , print_output = print_meshlabserver_output ) \n    if self . parse_topology : \n        self . topology = compute . parse_topology ( ml_log , log , print_output = print_meshlabserver_output ) \n    if self . parse_hausdorff : \n        self . hausdorff_distance = compute . parse_hausdorff ( ml_log , log , print_output = print_meshlabserver_output ) \n    if self . __no_file_in : \n        os . remove ( temp_file_in_file . name ) \n    if temp_script : \n        os . remove ( temp_script_file . name ) \n    if temp_ml_log : \n        os . remove ( ml_log_file . name ) "}
{"4237": "\ndef main ( ) : \n    segments = 50 \n    star_points = 5 \n    star_radius = 2 \n    ring_thickness = 1 \n    sphere_radius = 2 * ( star_radius + 3 * ring_thickness ) \n    polygon_radius = star_radius / ( 1 + math . tan ( math . radians ( 180 / star_points ) ) / math . tan ( math . radians ( 90 / star_points ) ) ) \n    width = polygon_radius * math . tan ( math . radians ( 180 / star_points ) ) \n    height = width / math . tan ( math . radians ( 90 / star_points ) ) \n    shield = mlx . FilterScript ( file_out = \"shield.ply\" ) \n    mlx . create . annulus ( shield , radius = star_radius , cir_segments = segments , color = 'blue' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + ring_thickness , radius2 = star_radius , cir_segments = segments , color = 'red' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 2 * ring_thickness , radius2 = star_radius + ring_thickness , cir_segments = segments , color = 'white' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3 * ring_thickness , radius2 = star_radius + 2 * ring_thickness , cir_segments = segments , color = 'red' ) \n    mlx . layers . join ( shield ) \n    mlx . subdivide . midpoint ( shield , iterations = 2 ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3 * ring_thickness , cir_segments = segments , color = 'silver' ) \n    mlx . transform . rotate ( shield , axis = 'y' , angle = 180 ) \n    mlx . transform . translate ( shield , value = [ 0 , 0 , - 0.005 ] ) \n    mlx . subdivide . midpoint ( shield , iterations = 4 ) \n    mlx . create . grid ( shield , size = math . sqrt ( 2 ) , x_segments = 10 , y_segments = 10 , center = 1 , color = 'white' ) \n    mlx . transform . rotate ( shield , axis = 'z' , angle = 45 ) \n    mlx . transform . scale ( shield , value = [ width , height , 1 ] ) \n    mlx . transform . translate ( shield , value = [ 0 , polygon_radius , 0.001 ] ) \n    for _ in range ( 1 , star_points ) : \n        mlx . layers . duplicate ( shield ) \n        mlx . transform . rotate ( shield , axis = 'z' , angle = 360 / star_points ) \n    mlx . layers . join ( shield ) \n    mlx . transform . vert_function ( shield , z_func = 'sqrt(%s-x^2-y^2)-%s+z' % ( sphere_radius ** 2 , sphere_radius ) ) \n    shield . run_script ( ) \n    return None "}
{"4238": "\ndef hausdorff_distance ( script , sampled_layer = 1 , target_layer = 0 , save_sample = 0 , sample_vert = 1 , sample_edge = 1 , sample_faux_edge = 0 , sample_face = 1 , sample_num = 1000 , maxdist = 10 ) : \n    maxdist_max = 2 * maxdist \n    filter_xml = '' . join ( [ '  <filter name=\"Hausdorff Distance\">\\n' , '    <Param name=\"SampledMesh\" ' , 'value=\"{:d}\" ' . format ( sampled_layer ) , 'description=\"Sampled Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"TargetMesh\" ' , 'value=\"{:d}\" ' . format ( target_layer ) , 'description=\"Target Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"SaveSample\" ' , 'value=\"{}\" ' . format ( str ( save_sample ) . lower ( ) ) , 'description=\"Save Samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleVert\" ' , 'value=\"{}\" ' . format ( str ( sample_vert ) . lower ( ) ) , 'description=\"Sample Vertexes\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleEdge\" ' , 'value=\"{}\" ' . format ( str ( sample_edge ) . lower ( ) ) , 'description=\"Sample Edges\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleFauxEdge\" ' , 'value=\"{}\" ' . format ( str ( sample_faux_edge ) . lower ( ) ) , 'description=\"Sample FauxEdge\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleFace\" ' , 'value=\"{}\" ' . format ( str ( sample_face ) . lower ( ) ) , 'value=\"%s\" ' % str ( sample_face ) . lower ( ) + 'description=\"Sample Faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"MaxDist\" ' , 'value=\"{}\" ' . format ( maxdist ) , 'value=\"%s\" ' % maxdist + 'description=\"Max Distance\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( maxdist_max ) , 'type=\"RichAbsPerc\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . parse_hausdorff = 1 \n    if isinstance ( script , FilterScript ) and save_sample : \n        script . add_layer ( 'Hausdorff Closest Points' ) \n        script . add_layer ( 'Hausdorff Sample Point' ) \n    return None "}
{"4239": "\ndef poisson_disk ( script , sample_num = 1000 , radius = 0.0 , montecarlo_rate = 20 , save_montecarlo = 0 , approx_geodesic_dist = 0 , subsample = 0 , refine = 0 , refine_layer = 0 , best_sample = 1 , best_sample_pool = 10 , exact_num = 0 , radius_variance = 1.0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Poisson-disk Sampling\">\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Radius\" ' , 'value=\"{}\" ' . format ( radius ) , 'description=\"Explicit Radius\" ' , 'min=\"0\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"MontecarloRate\" ' , 'value=\"{:d}\" ' . format ( montecarlo_rate ) , 'description=\"MonterCarlo OverSampling\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"SaveMontecarlo\" ' , 'value=\"{}\" ' . format ( str ( save_montecarlo ) . lower ( ) ) , 'description=\"Save Montecarlo\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"ApproximateGeodesicDistance\" ' , 'value=\"{}\" ' . format ( str ( approx_geodesic_dist ) . lower ( ) ) , 'description=\"Approximate Geodesic Distance\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"Subsample\" ' , 'value=\"{}\" ' . format ( str ( subsample ) . lower ( ) ) , 'description=\"Base Mesh Subsampling\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RefineFlag\" ' , 'value=\"{}\" ' . format ( str ( refine ) . lower ( ) ) , 'description=\"Refine Existing Samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RefineMesh\" ' , 'value=\"{:d}\" ' . format ( refine_layer ) , 'description=\"Samples to be refined\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"BestSampleFlag\" ' , 'value=\"{}\" ' . format ( str ( best_sample ) . lower ( ) ) , 'description=\"Best Sample Heuristic\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"BestSamplePool\" ' , 'value=\"{:d}\" ' . format ( best_sample_pool ) , 'description=\"Best Sample Pool Size\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"ExactNumFlag\" ' , 'value=\"{}\" ' . format ( str ( exact_num ) . lower ( ) ) , 'description=\"Exact number of samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RadiusVariance\" ' , 'value=\"{}\" ' . format ( radius_variance ) , 'description=\"Radius Variance\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Poisson-disk Samples' ) \n        if save_montecarlo : \n            script . add_layer ( 'Montecarlo Samples' ) \n    return None "}
{"4241": "\ndef clustered_vert ( script , cell_size = 1.0 , strategy = 'AVERAGE' , selected = 0 ) : \n    if strategy . lower ( ) == 'average' : \n        strategy_num = 0 \n    elif strategy . lower ( ) == 'center' : \n        strategy_num = 1 \n    filter_xml = '' . join ( [ '  <filter name=\"Clustered Vertex Subsampling\">\\n' , '    <Param name=\"Threshold\" ' , 'value=\"{}\" ' . format ( cell_size ) , 'description=\"Cell Size\" ' , 'min=\"0\" ' , 'max=\"1000\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( strategy_num ) , 'description=\"Representative Strategy:\" ' , 'enum_val0=\"Average\" ' , 'enum_val1=\"Closest to center\" ' , 'enum_cardinality=\"2\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Selected\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cluster Samples' ) \n    return None "}
{"4242": "\ndef flat_plane ( script , plane = 0 , aspect_ratio = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Parametrization: Flat Plane \">\\n' , '    <Param name=\"projectionPlane\"' , 'value=\"%d\"' % plane , 'description=\"Projection plane\"' , 'enum_val0=\"XY\"' , 'enum_val1=\"XZ\"' , 'enum_val2=\"YZ\"' , 'enum_cardinality=\"3\"' , 'type=\"RichEnum\"' , 'tooltip=\"Choose the projection plane\"' , '/>\\n' , '    <Param name=\"aspectRatio\"' , 'value=\"%s\"' % str ( aspect_ratio ) . lower ( ) , 'description=\"Preserve Ratio\"' , 'type=\"RichBool\"' , 'tooltip=\"If checked the resulting parametrization will preserve the original apsect ratio of the model otherwise it will fill up the whole 0..1 uv space\"' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4244": "\ndef voronoi ( script , region_num = 10 , overlap = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Parametrization: Voronoi Atlas\">\\n' , '    <Param name=\"regionNum\"' , 'value=\"%d\"' % region_num , 'description=\"Approx. Region Num\"' , 'type=\"RichInt\"' , 'tooltip=\"An estimation of the number of regions that must be generated. Smaller regions could lead to parametrizations with smaller distortion.\"' , '/>\\n' , '    <Param name=\"overlapFlag\"' , 'value=\"%s\"' % str ( overlap ) . lower ( ) , 'description=\"Overlap\"' , 'type=\"RichBool\"' , 'tooltip=\"If checked the resulting parametrization will be composed by overlapping regions, e.g. the resulting mesh will have duplicated faces: each region will have a ring of ovelapping duplicate faces that will ensure that border regions will be parametrized in the atlas twice. This is quite useful for building mipmap robust atlases\"' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4245": "\ndef measure_topology ( script ) : \n    filter_xml = '  <xmlfilter name=\"Compute Topological Measures\"/>\\n' \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , mlx . FilterScript ) : \n        script . parse_topology = 1 \n    return None "}
{"4246": "\ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = 0 ) : \n    topology = { 'manifold' : 1 , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = 0 \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2 ] \n                if topology [ 'genus' ] != 'undefined' : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2 ] \n                if topology [ 'hole_num' ] == 'a' : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        elif print_output : \n            print ( '{:16} = {}' . format ( key , value ) ) \n    return topology "}
{"4247": "\ndef parse_hausdorff ( ml_log , log = None , print_output = 0 ) : \n    hausdorff_distance = { \"min_distance\" : 0.0 , \"max_distance\" : 0.0 , \"mean_distance\" : 0.0 , \"rms_distance\" : 0.0 , \"number_points\" : 0 } \n    with open ( ml_log ) as fread : \n        result = fread . readlines ( ) \n        data = \"\" \n        for idx , line in enumerate ( result ) : \n            m = re . match ( r\"\\s*Sampled (\\d+) pts.*\" , line ) \n            if m is not None : \n                hausdorff_distance [ \"number_points\" ] = int ( m . group ( 1 ) ) \n            if 'Hausdorff Distance computed' in line : \n                data = result [ idx + 2 ] \n        m = re . match ( r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\" , data ) \n        hausdorff_distance [ \"min_distance\" ] = float ( m . group ( 1 ) ) \n        hausdorff_distance [ \"max_distance\" ] = float ( m . group ( 2 ) ) \n        hausdorff_distance [ \"mean_distance\" ] = float ( m . group ( 3 ) ) \n        hausdorff_distance [ \"rms_distance\" ] = float ( m . group ( 4 ) ) \n        for key , value in hausdorff_distance . items ( ) : \n            if log is not None : \n                log_file = open ( log , 'a' ) \n                log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n                log_file . close ( ) \n            elif print_output : \n                print ( '{:16} = {}' . format ( key , value ) ) \n        return hausdorff_distance "}
{"4249": "\ndef voronoi ( script , target_layer = 0 , source_layer = 1 , backward = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Voronoi Vertex Coloring\">\\n' , '    <Param name=\"ColoredMesh\" ' , 'value=\"{:d}\" ' . format ( target_layer ) , 'description=\"To be Colored Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"VertexMesh\" ' , 'value=\"{:d}\" ' . format ( source_layer ) , 'description=\"Vertex Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"backward\" ' , 'value=\"{}\" ' . format ( str ( backward ) . lower ( ) ) , 'description=\"BackDistance\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4250": "\ndef cyclic_rainbow ( script , direction = 'sphere' , start_pt = ( 0 , 0 , 0 ) , amplitude = 255 / 2 , center = 255 / 2 , freq = 0.8 , phase = ( 0 , 120 , 240 , 0 ) , alpha = 0 ) : \n    start_pt = util . make_list ( start_pt , 3 ) \n    amplitude = util . make_list ( amplitude , 4 ) \n    center = util . make_list ( center , 4 ) \n    freq = util . make_list ( freq , 4 ) \n    phase = util . make_list ( phase , 4 ) \n    if direction . lower ( ) == 'sphere' : \n        increment = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)' . format ( start_pt [ 0 ] , start_pt [ 1 ] , start_pt [ 2 ] ) \n    elif direction . lower ( ) == 'x' : \n        increment = 'x - {}' . format ( start_pt [ 0 ] ) \n    elif direction . lower ( ) == 'y' : \n        increment = 'y - {}' . format ( start_pt [ 1 ] ) \n    elif direction . lower ( ) == 'z' : \n        increment = 'z - {}' . format ( start_pt [ 2 ] ) \n    else : \n        increment = direction \n    red_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 0 ] , i = increment , p = math . radians ( phase [ 0 ] ) , a = amplitude [ 0 ] , c = center [ 0 ] ) \n    green_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 1 ] , i = increment , p = math . radians ( phase [ 1 ] ) , a = amplitude [ 1 ] , c = center [ 1 ] ) \n    blue_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 2 ] , i = increment , p = math . radians ( phase [ 2 ] ) , a = amplitude [ 2 ] , c = center [ 2 ] ) \n    if alpha : \n        alpha_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 3 ] , i = increment , p = math . radians ( phase [ 3 ] ) , a = amplitude [ 3 ] , c = center [ 3 ] ) \n    else : \n        alpha_func = 255 \n    function ( script , red = red_func , green = green_func , blue = blue_func , alpha = alpha_func ) \n    return None "}
{"4255": "\ndef flip ( script , force_flip = 0 , selected = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Invert Faces Orientation\">\\n' , '    <Param name=\"forceFlip\" ' , 'value=\"{}\" ' . format ( str ( force_flip ) . lower ( ) ) , 'description=\"Force Flip\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"onlySelected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Flip only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4256": "\ndef point_sets ( script , neighbors = 10 , smooth_iteration = 0 , flip = 0 , viewpoint_pos = ( 0.0 , 0.0 , 0.0 ) ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Compute normals for point sets\">\\n' , '    <Param name=\"K\" ' , 'value=\"{:d}\" ' . format ( neighbors ) , 'description=\"Neighbour num\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"smoothIter\" ' , 'value=\"{:d}\" ' . format ( smooth_iteration ) , 'description=\"Smooth Iteration\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"flipFlag\" ' , 'value=\"{}\" ' . format ( str ( flip ) . lower ( ) ) , 'description=\"Flip normals w.r.t. viewpoint\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"viewPos\" ' , 'x=\"{}\" y=\"{}\" z=\"{}\" ' . format ( viewpoint_pos [ 0 ] , viewpoint_pos [ 1 ] , viewpoint_pos [ 2 ] , ) , 'description=\"Viewpoint Pos.\" ' , 'type=\"RichPoint3f\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4257": "\ndef taubin ( script , iterations = 10 , t_lambda = 0.5 , t_mu = - 0.53 , selected = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Taubin Smooth\">\\n' , '    <Param name=\"lambda\" ' , 'value=\"{}\" ' . format ( t_lambda ) , 'description=\"Lambda\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"mu\" ' , 'value=\"{}\" ' . format ( t_mu ) , 'description=\"mu\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"stepSmoothNum\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Smoothing steps\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4258": "\ndef depth ( script , iterations = 3 , viewpoint = ( 0 , 0 , 0 ) , selected = 0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Depth Smooth\">\\n' , '    <Param name=\"stepSmoothNum\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Smoothing steps\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"viewPoint\" ' , 'x=\"{}\" ' . format ( viewpoint [ 0 ] ) , 'y=\"{}\" ' . format ( viewpoint [ 1 ] ) , 'z=\"{}\" ' . format ( viewpoint [ 2 ] ) , 'description=\"Smoothing steps\" ' , 'type=\"RichPoint3f\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4259": "\ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) \n    if fext != 'obj' : \n        print ( 'Input file must be obj. Exiting ...' ) \n        sys . exit ( 1 ) \n    fread = open ( fbasename , 'r' ) \n    first = 1 \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if element == 'v' : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        elif element == 'l' : \n            p1 = x_co \n            p2 = y_co \n            line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4262": "\ndef measure_dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml_version = ml_version ) : \n    axis1 = axis1 . lower ( ) \n    axis2 = axis2 . lower ( ) \n    ml_script1_file = 'TEMP3D_measure_dimension.mlx' \n    file_out = 'TEMP3D_measure_dimension.xyz' \n    ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) \n    compute . section ( ml_script1 , axis1 , offset1 , surface = 1 ) \n    compute . section ( ml_script1 , axis2 , offset2 , surface = 0 ) \n    layers . delete_lower ( ml_script1 ) \n    ml_script1 . save_to_file ( ml_script1_file ) \n    ml_script1 . run_script ( log = log , script_file = ml_script1_file ) \n    for val in ( 'x' , 'y' , 'z' ) : \n        if val not in ( axis1 , axis2 ) : \n            axis = val \n    axis_num = ord ( axis ) - ord ( 'x' ) \n    aabb = measure_aabb ( file_out , log ) \n    dimension = { 'min' : aabb [ 'min' ] [ axis_num ] , 'max' : aabb [ 'max' ] [ axis_num ] , 'length' : aabb [ 'size' ] [ axis_num ] , 'axis' : axis } \n    if log is None : \n        print ( '\\nFor file \"%s\"' % fbasename ) \n        print ( 'Dimension parallel to %s with %s=%s & %s=%s:' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) \n        print ( '  Min = %s, Max = %s, Total length = %s' % ( dimension [ 'min' ] , dimension [ 'max' ] , dimension [ 'length' ] ) ) \n    else : \n        log_file = open ( log , 'a' ) \n        log_file . write ( '\\nFor file \"%s\"\\n' % fbasename ) \n        log_file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) \n        log_file . write ( 'min = %s\\n' % dimension [ 'min' ] ) \n        log_file . write ( 'max = %s\\n' % dimension [ 'max' ] ) \n        log_file . write ( 'Total length = %s\\n' % dimension [ 'length' ] ) \n        log_file . close ( ) \n    return dimension "}
{"4265": "\ndef config_for_set ( uset , app , defaults = None ) : \n    config = app . config \n    prefix = 'UPLOADED_%s_' % uset . name . upper ( ) \n    using_defaults = 0 \n    if defaults is None : \n        defaults = dict ( dest = None , url = None ) \n    allow_extns = tuple ( config . get ( prefix + 'ALLOW' , ( ) ) ) \n    deny_extns = tuple ( config . get ( prefix + 'DENY' , ( ) ) ) \n    destination = config . get ( prefix + 'DEST' ) \n    base_url = config . get ( prefix + 'URL' ) \n    if destination is None : \n        if uset . default_dest : \n            destination = uset . default_dest ( app ) \n        if destination is None : \n            if defaults [ 'dest' ] is not None : \n                using_defaults = 1 \n                destination = os . path . join ( defaults [ 'dest' ] , uset . name ) \n            else : \n                raise RuntimeError ( \"no destination for set %s\" % uset . name ) \n    if base_url is None and using_defaults and defaults [ 'url' ] : \n        base_url = addslash ( defaults [ 'url' ] ) + uset . name + '/' \n    return UploadConfiguration ( destination , base_url , allow_extns , deny_extns ) "}
{"4268": "\ndef url ( self , filename ) : \n    base = self . config . base_url \n    if base is None : \n        return url_for ( '_uploads.uploaded_file' , setname = self . name , filename = filename , _external = 1 ) \n    else : \n        return base + filename "}
{"4271": "\ndef resolve_conflict ( self , target_folder , basename ) : \n    name , ext = os . path . splitext ( basename ) \n    count = 0 \n    while 1 : \n        count = count + 1 \n        newname = '%s_%d%s' % ( name , count , ext ) \n        if not os . path . exists ( os . path . join ( target_folder , newname ) ) : \n            return newname "}
{"4275": "\ndef _format_obj_count ( objects ) : \n    result = [ ] \n    regex = re . compile ( r'<(?P<type>\\w+) \\'(?P<name>\\S+)\\'>' ) \n    for obj_type , obj_count in objects . items ( ) : \n        if obj_count != 0 : \n            match = re . findall ( regex , repr ( obj_type ) ) \n            if match : \n                obj_type , obj_name = match [ 0 ] \n                result . append ( ( \"%s %s\" % ( obj_type , obj_name ) , obj_count ) ) \n    return sorted ( result , key = operator . itemgetter ( 1 ) , reverse = 1 ) "}
{"4299": "\ndef _transform_stats ( prof ) : \n    records = [ ] \n    for info , params in prof . stats . items ( ) : \n        filename , lineno , funcname = info \n        cum_calls , num_calls , time_per_call , cum_time , _ = params \n        if prof . total_tt == 0 : \n            percentage = 0 \n        else : \n            percentage = round ( 100 * ( cum_time / prof . total_tt ) , 4 ) \n        cum_time = round ( cum_time , 4 ) \n        func_name = '%s @ %s' % ( funcname , filename ) \n        color_hash = base_profiler . hash_name ( func_name ) \n        records . append ( ( filename , lineno , funcname , cum_time , percentage , num_calls , cum_calls , time_per_call , filename , color_hash ) ) \n    return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = 1 ) "}
{"4313": "\ndef check_standard_dir ( module_path ) : \n    if 'site-packages' in module_path : \n        return 1 \n    for stdlib_path in _STDLIB_PATHS : \n        if fnmatch . fnmatchcase ( module_path , stdlib_path + '*' ) : \n            return 1 \n    return 0 "}
{"4322": "\ndef run_profilers ( run_object , prof_config , verbose = 0 ) : \n    if len ( prof_config ) > len ( set ( prof_config ) ) : \n        raise AmbiguousConfigurationError ( 'Profiler configuration %s is ambiguous' % prof_config ) \n    available_profilers = { opt for opt , _ in _PROFILERS } \n    for option in prof_config : \n        if option not in available_profilers : \n            raise BadOptionError ( 'Unknown option: %s' % option ) \n    run_stats = OrderedDict ( ) \n    present_profilers = ( ( o , p ) for o , p in _PROFILERS if o in prof_config ) \n    for option , prof in present_profilers : \n        curr_profiler = prof ( run_object ) \n        if verbose : \n            print ( 'Running %s...' % curr_profiler . __class__ . __name__ ) \n        run_stats [ option ] = curr_profiler . run ( ) \n    return run_stats "}
{"4329": "\ndef _limit_features ( self , X , vocabulary , high = None , low = None , limit = None ) : \n    if high is None and low is None and limit is None : \n        return X , set ( ) \n    dfs = X . map ( _document_frequency ) . sum ( ) \n    tfs = X . map ( lambda x : np . asarray ( x . sum ( axis = 0 ) ) ) . sum ( ) . ravel ( ) \n    mask = np . ones ( len ( dfs ) , dtype = bool ) \n    if high is not None : \n        mask &= dfs <= high \n    if low is not None : \n        mask &= dfs >= low \n    if limit is not None and mask . sum ( ) > limit : \n        mask_inds = ( - tfs [ mask ] ) . argsort ( ) [ : limit ] \n        new_mask = np . zeros ( len ( dfs ) , dtype = bool ) \n        new_mask [ np . where ( mask ) [ 0 ] [ mask_inds ] ] = 1 \n        mask = new_mask \n    new_indices = np . cumsum ( mask ) - 1 \n    removed_terms = set ( ) \n    for term , old_index in list ( six . iteritems ( vocabulary ) ) : \n        if mask [ old_index ] : \n            vocabulary [ term ] = new_indices [ old_index ] \n        else : \n            del vocabulary [ term ] \n            removed_terms . add ( term ) \n    kept_indices = np . where ( mask ) [ 0 ] \n    if len ( kept_indices ) == 0 : \n        raise ValueError ( \"After pruning, no terms remain. Try a lower\" \" min_df or a higher max_df.\" ) \n    return kept_indices , removed_terms "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if self . verbose > 0 : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = 1 , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( 0 , n_fits , n_folds ) : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = 1 ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4344": "\ndef check_rdd_dtype ( rdd , expected_dtype ) : \n    if not isinstance ( rdd , BlockRDD ) : \n        raise TypeError ( \"Expected {0} for parameter rdd, got {1}.\" . format ( BlockRDD , type ( rdd ) ) ) \n    if isinstance ( rdd , DictRDD ) : \n        if not isinstance ( expected_dtype , dict ) : \n            raise TypeError ( 'Expected {0} for parameter ' 'expected_dtype, got {1}.' . format ( dict , type ( expected_dtype ) ) ) \n        accept = 1 \n        types = dict ( list ( zip ( rdd . columns , rdd . dtype ) ) ) \n        for key , values in expected_dtype . items ( ) : \n            if not isinstance ( values , ( tuple , list ) ) : \n                values = [ values ] \n            accept = accept and types [ key ] in values \n        return accept \n    if not isinstance ( expected_dtype , ( tuple , list ) ) : \n        expected_dtype = [ expected_dtype ] \n    return rdd . dtype in expected_dtype "}
{"4347": "\ndef fit_transform ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( sp . spmatrix , np . ndarray ) ) \n    if self . algorithm == \"em\" : \n        X = X . persist ( ) \n        Sigma , V = svd_em ( X , k = self . n_components , maxiter = self . n_iter , tol = self . tol , compute_u = 0 , seed = self . random_state ) \n        self . components_ = V \n        X . unpersist ( ) \n        return self . transform ( Z ) \n    else : \n        return super ( SparkTruncatedSVD , self ) . fit_transform ( X . tosparse ( ) ) "}
{"4352": "\ndef transform ( self , fn , dtype = None , * args , ** kwargs ) : \n    rdd = self . _rdd . map ( fn ) \n    if dtype is None : \n        return self . __class__ ( rdd , noblock = 1 , ** self . get_params ( ) ) \n    if dtype is np . ndarray : \n        return ArrayRDD ( rdd , bsize = self . bsize , noblock = 1 ) \n    elif dtype is sp . spmatrix : \n        return SparseRDD ( rdd , bsize = self . bsize , noblock = 1 ) \n    else : \n        return BlockRDD ( rdd , bsize = self . bsize , dtype = dtype , noblock = 1 ) "}
{"4355": "\ndef transform ( self , fn , column = None , dtype = None ) : \n    dtypes = self . dtype \n    if column is None : \n        indices = list ( range ( len ( self . columns ) ) ) \n    else : \n        if not type ( column ) in ( list , tuple ) : \n            column = [ column ] \n        indices = [ self . columns . index ( c ) for c in column ] \n    if dtype is not None : \n        if not type ( dtype ) in ( list , tuple ) : \n            dtype = [ dtype ] \n        dtypes = [ dtype [ indices . index ( i ) ] if i in indices else t for i , t in enumerate ( self . dtype ) ] \n    def mapper ( values ) : \n        result = fn ( * [ values [ i ] for i in indices ] ) \n        if len ( indices ) == 1 : \n            result = ( result , ) \n        elif not isinstance ( result , ( tuple , list ) ) : \n            raise ValueError ( \"Transformer function must return an\" \" iterable!\" ) \n        elif len ( result ) != len ( indices ) : \n            raise ValueError ( \"Transformer result's length must be\" \" equal to the given columns length!\" ) \n        return tuple ( result [ indices . index ( i ) ] if i in indices else v for i , v in enumerate ( values ) ) \n    return DictRDD ( self . _rdd . map ( mapper ) , columns = self . columns , dtype = dtypes , bsize = self . bsize , noblock = 1 ) "}
{"4357": "\ndef only_root_write ( path ) : \n    s = os . stat ( path ) \n    for ug , bp in [ ( s . st_uid , bitperm ( s , 'w' , 'usr' ) ) , ( s . st_gid , bitperm ( s , 'w' , 'grp' ) ) ] : \n        if ug and bp : \n            return 0 \n    if bitperm ( s , 'w' , 'oth' ) : \n        return 0 \n    return 1 "}
{"4371": "\ndef execute ( self , root_allowed = 0 ) : \n    logger . debug ( '%s device executed (mac %s)' , self . name , self . src ) \n    if not self . execute_instance : \n        msg = '%s: There is not execution method in device conf.' \n        logger . warning ( msg , self . name ) \n        self . send_confirmation ( msg % self . name , 0 ) \n        return \n    try : \n        result = self . execute_instance . execute ( root_allowed ) \n    except Exception as e : \n        self . send_confirmation ( 'Error executing the device {}: {}' . format ( self . name , e ) , 0 ) \n        raise \n    else : \n        result = 'The {} device has been started and is running right now' . format ( self . name ) if result is None else result \n        result = result or 'The {} device has been executed successfully' . format ( self . name ) \n        self . send_confirmation ( result ) \n    return result "}
{"4372": "\ndef send_confirmation ( self , message , success = 1 ) : \n    message = message . strip ( ) \n    if not self . confirmation : \n        return \n    try : \n        self . confirmation . send ( message , success ) \n    except Exception as e : \n        logger . warning ( 'Error sending confirmation on device {}: {}' . format ( self . name , e ) ) "}
{"4375": "\ndef run ( self , root_allowed = 0 ) : \n    self . root_allowed = root_allowed \n    scan_devices ( self . on_push , lambda d : d . src . lower ( ) in self . devices , self . settings . get ( 'interface' ) ) "}
{"4376": "\ndef convert ( self , txn ) : \n    ofxid = self . mk_ofxid ( txn . id ) \n    metadata = { } \n    posting_metadata = { \"ofxid\" : ofxid } \n    if isinstance ( txn , OfxTransaction ) : \n        posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) \n        return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) \n    elif isinstance ( txn , InvestmentTransaction ) : \n        acct1 = self . name \n        acct2 = self . name \n        posting1 = None \n        posting2 = None \n        security = self . maybe_get_ticker ( txn . security ) \n        if isinstance ( txn . type , str ) : \n            if re . match ( '^(buy|sell)' , txn . type ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif txn . type == 'transfer' : \n                acct2 = 'Transfer' \n            elif txn . type == 'reinvest' : \n                acct2 = 'Income:Interest' \n            elif txn . type == 'income' and txn . income_type == 'DIV' : \n                metadata [ 'dividend_from' ] = security \n                acct2 = 'Income:Dividends' \n                posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) \n                posting2 = posting1 . clone_inverted ( acct2 ) \n            else : \n                pass \n        else : \n            if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif ( txn . type == 2 ) : \n                acct2 = 'Income:Interest' \n            else : \n                pass \n        aux_date = None \n        if txn . settleDate is not None and txn . settleDate != txn . tradeDate : \n            aux_date = txn . settleDate \n        if posting1 is None and posting2 is None : \n            posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = 1 ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = 1 ) , metadata = posting_metadata ) \n            posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = 1 ) ) \n        else : \n            pass \n        return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] ) "}
{"4382": "\ndef badge ( left_text : str , right_text : str , left_link : Optional [ str ] = None , right_link : Optional [ str ] = None , whole_link : Optional [ str ] = None , logo : Optional [ str ] = None , left_color : str = '#555' , right_color : str = '#007ec6' , measurer : Optional [ text_measurer . TextMeasurer ] = None , embed_logo : bool = 0 ) -> str : \n    if measurer is None : \n        measurer = ( precalculated_text_measurer . PrecalculatedTextMeasurer . default ( ) ) \n    if ( left_link or right_link ) and whole_link : \n        raise ValueError ( 'whole_link may not bet set with left_link or right_link' ) \n    template = _JINJA2_ENVIRONMENT . get_template ( 'badge-template-full.svg' ) \n    if logo and embed_logo : \n        logo = _embed_image ( logo ) \n    svg = template . render ( left_text = left_text , right_text = right_text , left_text_width = measurer . text_width ( left_text ) / 10.0 , right_text_width = measurer . text_width ( right_text ) / 10.0 , left_link = left_link , right_link = right_link , whole_link = whole_link , logo = logo , left_color = _NAME_TO_COLOR . get ( left_color , left_color ) , right_color = _NAME_TO_COLOR . get ( right_color , right_color ) , ) \n    xml = minidom . parseString ( svg ) \n    _remove_blanks ( xml ) \n    xml . normalize ( ) \n    return xml . documentElement . toxml ( ) "}
{"4386": "\ndef write_json ( f : TextIO , deja_vu_sans_path : str , measurer : text_measurer . TextMeasurer , encodings : Iterable [ str ] ) -> None : \n    supported_characters = list ( generate_supported_characters ( deja_vu_sans_path ) ) \n    kerning_characters = '' . join ( generate_encodeable_characters ( supported_characters , encodings ) ) \n    char_to_length = calculate_character_to_length_mapping ( measurer , supported_characters ) \n    pair_to_kerning = calculate_pair_to_kern_mapping ( measurer , char_to_length , kerning_characters ) \n    json . dump ( { 'mean-character-length' : statistics . mean ( char_to_length . values ( ) ) , 'character-lengths' : char_to_length , 'kerning-characters' : kerning_characters , 'kerning-pairs' : pair_to_kerning } , f , sort_keys = 1 , indent = 1 ) "}
{"4393": "\ndef destroy ( self ) : \n    if self . __conf . autoTick : \n        self . __destroying = 1 \n    else : \n        self . _doDestroy ( ) "}
{"4398": "\ndef _maybeBind ( self ) : \n    if self . _ready or self . _selfIsReadonlyNode or time . time ( ) < self . _lastBindAttemptTime + self . _syncObj . conf . bindRetryTime : \n        return \n    self . _lastBindAttemptTime = time . time ( ) \n    try : \n        self . _server . bind ( ) \n    except Exception as e : \n        self . _bindAttempts += 1 \n        if self . _syncObj . conf . maxBindRetries and self . _bindAttempts >= self . _syncObj . conf . maxBindRetries : \n            self . _bindOverEvent . set ( ) \n            raise TransportNotReadyError \n    else : \n        self . _ready = 1 \n        self . _bindOverEvent . set ( ) "}
{"4400": "\ndef _onIncomingMessageReceived ( self , conn , message ) : \n    if self . _syncObj . encryptor and not conn . sendRandKey : \n        conn . sendRandKey = message \n        conn . recvRandKey = os . urandom ( 32 ) \n        conn . send ( conn . recvRandKey ) \n        return \n    if isinstance ( message , list ) : \n        done = 0 \n        try : \n            if message [ 0 ] == 'status' : \n                conn . send ( self . _syncObj . getStatus ( ) ) \n                done = 1 \n            elif message [ 0 ] == 'add' : \n                self . _syncObj . addNodeToCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'ADD' , arg = message [ 1 ] ) ) \n                done = 1 \n            elif message [ 0 ] == 'remove' : \n                if message [ 1 ] == self . _selfNode . address : \n                    conn . send ( 'FAIL REMOVE ' + message [ 1 ] ) \n                else : \n                    self . _syncObj . removeNodeFromCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'REMOVE' , arg = message [ 1 ] ) ) \n                done = 1 \n            elif message [ 0 ] == 'set_version' : \n                self . _syncObj . setCodeVersion ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'SET_VERSION' , arg = str ( message [ 1 ] ) ) ) \n                done = 1 \n        except Exception as e : \n            conn . send ( str ( e ) ) \n            done = 1 \n        if done : \n            return \n    node = self . _nodeAddrToNode [ message ] if message in self . _nodeAddrToNode else None \n    if node is None and message != 'readonly' : \n        conn . disconnect ( ) \n        self . _unknownConnections . discard ( conn ) \n        return \n    readonly = node is None \n    if readonly : \n        nodeId = str ( self . _readonlyNodesCounter ) \n        node = Node ( nodeId ) \n        self . _readonlyNodes . add ( node ) \n        self . _readonlyNodesCounter += 1 \n    self . _unknownConnections . discard ( conn ) \n    self . _connections [ node ] = conn \n    conn . setOnMessageReceivedCallback ( functools . partial ( self . _onMessageReceived , node ) ) \n    if not readonly : \n        self . _onNodeConnected ( node ) \n    else : \n        self . _onReadonlyNodeConnected ( node ) "}
{"4403": "\ndef _connectIfNecessarySingle ( self , node ) : \n    if node in self . _connections and self . _connections [ node ] . state != CONNECTION_STATE . DISCONNECTED : \n        return 1 \n    if not self . _shouldConnect ( node ) : \n        return 0 \n    assert node in self . _connections \n    if node in self . _lastConnectAttempt and time . time ( ) - self . _lastConnectAttempt [ node ] < self . _syncObj . conf . connectionRetryTime : \n        return 0 \n    self . _lastConnectAttempt [ node ] = time . time ( ) \n    return self . _connections [ node ] . connect ( node . ip , node . port ) "}
{"4409": "\ndef send ( self , node , message ) : \n    if node not in self . _connections or self . _connections [ node ] . state != CONNECTION_STATE . CONNECTED : \n        return 0 \n    self . _connections [ node ] . send ( message ) \n    if self . _connections [ node ] . state != CONNECTION_STATE . CONNECTED : \n        return 0 \n    return 1 "}
{"4411": "\ndef put ( self , item ) : \n    if self . __maxsize and len ( self . __data ) >= self . __maxsize : \n        return 0 \n    self . __data . append ( item ) \n    return 1 "}
{"4412": "\ndef put ( self , item ) : \n    if self . __maxsize and len ( self . __data ) >= self . __maxsize : \n        return 0 \n    heapq . heappush ( self . __data , item ) \n    return 1 "}
{"4414": "\ndef tryAcquire ( self , lockID , callback = None , sync = 0 , timeout = None ) : \n    return self . __lockImpl . acquire ( lockID , self . __selfID , time . time ( ) , callback = callback , sync = sync , timeout = timeout ) "}
{"4416": "\ndef release ( self , lockID , callback = None , sync = 0 , timeout = None ) : \n    self . __lockImpl . release ( lockID , self . __selfID , callback = callback , sync = sync , timeout = timeout ) "}
{"4417": "\ndef check ( func ) : \n    def wrapped ( * args , ** kwargs ) : \n        check_name = func . __name__ \n        arg_name = None \n        if args : \n            arg_name = args [ 0 ] \n        try : \n            if arg_name : \n                logger . debug ( \"Checking '%s' for '%s'\" , check_name , arg_name ) \n            else : \n                logger . debug ( \"Checking '%s'\" , check_name ) \n            response = func ( * args , ** kwargs ) \n        except Exception as e : \n            message = str ( e ) \n            response = { \"ok\" : 0 , \"error\" : message , \"stacktrace\" : traceback . format_exc ( ) , } \n            if arg_name : \n                response = { arg_name : response } \n                logger . exception ( \"Error calling '%s' for '%s': %s\" , check_name , arg_name , message ) \n            else : \n                logger . exception ( \"Error calling '%s': %s\" , check_name , message ) \n        return response \n    return wrapped "}
{"4418": "\ndef token_required ( view_func ) : \n    def _parse_auth_header ( auth_header ) : \n        reg = re . compile ( '(\\w+)[=] ?\"?([\\w-]+)\"?' ) \n        header_dict = dict ( reg . findall ( auth_header ) ) \n        return header_dict [ 'Token' ] \n    def _get_passed_token ( request ) : \n        try : \n            auth_header = request . META [ 'HTTP_AUTHORIZATION' ] \n            token = _parse_auth_header ( auth_header ) \n        except KeyError : \n            token = request . GET . get ( settings . WATCHMAN_TOKEN_NAME ) \n        return token \n    def _validate_token ( request ) : \n        if settings . WATCHMAN_TOKENS : \n            watchman_tokens = settings . WATCHMAN_TOKENS . split ( ',' ) \n        elif settings . WATCHMAN_TOKEN : \n            watchman_tokens = [ settings . WATCHMAN_TOKEN , ] \n        else : \n            return 1 \n        return _get_passed_token ( request ) in watchman_tokens \n    \n    @ csrf_exempt \n    @ wraps ( view_func ) \n    def _wrapped_view ( request , * args , ** kwargs ) : \n        if _validate_token ( request ) : \n            return view_func ( request , * args , ** kwargs ) \n        return HttpResponseForbidden ( ) \n    return _wrapped_view "}
{"4419": "\ndef set_hosts ( hosts , use_ssl = 0 , ssl_cert_path = None ) : \n    if type ( hosts ) != list : \n        hosts = [ hosts ] \n    conn_params = { \"hosts\" : hosts , \"timeout\" : 20 } \n    if use_ssl : \n        conn_params [ 'use_ssl' ] = 1 \n        if ssl_cert_path : \n            conn_params [ 'verify_certs' ] = 1 \n            conn_params [ 'ca_certs' ] = ssl_cert_path \n        else : \n            conn_params [ 'verify_certs' ] = 0 \n    connections . create_connection ( ** conn_params ) "}
{"4425": "\ndef parse_aggregate_report_file ( _input , nameservers = None , dns_timeout = 2.0 , parallel = 0 ) : \n    xml = extract_xml ( _input ) \n    return parse_aggregate_report_xml ( xml , nameservers = nameservers , timeout = dns_timeout , parallel = parallel ) "}
{"4427": "\ndef parse_report_file ( input_ , nameservers = None , dns_timeout = 2.0 , strip_attachment_payloads = 0 , parallel = 0 ) : \n    if type ( input_ ) == str : \n        file_object = open ( input_ , \"rb\" ) \n    elif type ( input_ ) == bytes : \n        file_object = BytesIO ( input_ ) \n    else : \n        file_object = input_ \n    content = file_object . read ( ) \n    try : \n        report = parse_aggregate_report_file ( content , nameservers = nameservers , dns_timeout = dns_timeout , parallel = parallel ) \n        results = OrderedDict ( [ ( \"report_type\" , \"aggregate\" ) , ( \"report\" , report ) ] ) \n    except InvalidAggregateReport : \n        try : \n            sa = strip_attachment_payloads \n            results = parse_report_email ( content , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) \n        except InvalidDMARCReport : \n            raise InvalidDMARCReport ( \"Not a valid aggregate or forensic \" \"report\" ) \n    return results "}
{"4429": "\ndef save_output ( results , output_directory = \"output\" ) : \n    aggregate_reports = results [ \"aggregate_reports\" ] \n    forensic_reports = results [ \"forensic_reports\" ] \n    if os . path . exists ( output_directory ) : \n        if not os . path . isdir ( output_directory ) : \n            raise ValueError ( \"{0} is not a directory\" . format ( output_directory ) ) \n    else : \n        os . makedirs ( output_directory ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_json : \n        agg_json . write ( json . dumps ( aggregate_reports , ensure_ascii = 0 , indent = 2 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_csv : \n        csv = parsed_aggregate_reports_to_csv ( aggregate_reports ) \n        agg_csv . write ( csv ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_json : \n        for_json . write ( json . dumps ( forensic_reports , ensure_ascii = 0 , indent = 2 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_csv : \n        csv = parsed_forensic_reports_to_csv ( forensic_reports ) \n        for_csv . write ( csv ) \n    samples_directory = os . path . join ( output_directory , \"samples\" ) \n    if not os . path . exists ( samples_directory ) : \n        os . makedirs ( samples_directory ) \n    sample_filenames = [ ] \n    for forensic_report in forensic_reports : \n        sample = forensic_report [ \"sample\" ] \n        message_count = 0 \n        parsed_sample = forensic_report [ \"parsed_sample\" ] \n        subject = parsed_sample [ \"filename_safe_subject\" ] \n        filename = subject \n        while filename in sample_filenames : \n            message_count += 1 \n            filename = \"{0} ({1})\" . format ( subject , message_count ) \n        sample_filenames . append ( filename ) \n        filename = \"{0}.eml\" . format ( filename ) \n        path = os . path . join ( samples_directory , filename ) \n        with open ( path , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as sample_file : \n            sample_file . write ( sample ) "}
{"4431": "\ndef email_results ( results , host , mail_from , mail_to , port = 0 , ssl = 0 , user = None , password = None , subject = None , attachment_filename = None , message = None , ssl_context = None ) : \n    logging . debug ( \"Emailing report to: {0}\" . format ( \",\" . join ( mail_to ) ) ) \n    date_string = datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) \n    if attachment_filename : \n        if not attachment_filename . lower ( ) . endswith ( \".zip\" ) : \n            attachment_filename += \".zip\" \n        filename = attachment_filename \n    else : \n        filename = \"DMARC-{0}.zip\" . format ( date_string ) \n    assert isinstance ( mail_to , list ) \n    msg = MIMEMultipart ( ) \n    msg [ 'From' ] = mail_from \n    msg [ 'To' ] = \", \" . join ( mail_to ) \n    msg [ 'Date' ] = email . utils . formatdate ( localtime = 1 ) \n    msg [ 'Subject' ] = subject or \"DMARC results for {0}\" . format ( date_string ) \n    text = message or \"Please see the attached zip file\\n\" \n    msg . attach ( MIMEText ( text ) ) \n    zip_bytes = get_report_zip ( results ) \n    part = MIMEApplication ( zip_bytes , Name = filename ) \n    part [ 'Content-Disposition' ] = 'attachment; filename=\"{0}\"' . format ( filename ) \n    msg . attach ( part ) \n    try : \n        if ssl_context is None : \n            ssl_context = create_default_context ( ) \n        if ssl : \n            server = smtplib . SMTP_SSL ( host , port = port , context = ssl_context ) \n            server . connect ( host , port ) \n            server . ehlo_or_helo_if_needed ( ) \n        else : \n            server = smtplib . SMTP ( host , port = port ) \n            server . connect ( host , port ) \n            server . ehlo_or_helo_if_needed ( ) \n            if server . has_extn ( \"starttls\" ) : \n                server . starttls ( context = ssl_context ) \n                server . ehlo ( ) \n            else : \n                logger . warning ( \"SMTP server does not support STARTTLS. \" \"Proceeding in plain text!\" ) \n        if user and password : \n            server . login ( user , password ) \n        server . sendmail ( mail_from , mail_to , msg . as_string ( ) ) \n    except smtplib . SMTPException as error : \n        error = error . __str__ ( ) . lstrip ( \"b'\" ) . rstrip ( \"'\" ) . rstrip ( \".\" ) \n        raise SMTPError ( error ) \n    except socket . gaierror : \n        raise SMTPError ( \"DNS resolution failed\" ) \n    except ConnectionRefusedError : \n        raise SMTPError ( \"Connection refused\" ) \n    except ConnectionResetError : \n        raise SMTPError ( \"Connection reset\" ) \n    except ConnectionAbortedError : \n        raise SMTPError ( \"Connection aborted\" ) \n    except TimeoutError : \n        raise SMTPError ( \"Connection timed out\" ) \n    except SSLError as error : \n        raise SMTPError ( \"SSL error: {0}\" . format ( error . __str__ ( ) ) ) \n    except CertificateError as error : \n        raise SMTPError ( \"Certificate error: {0}\" . format ( error . __str__ ( ) ) ) "}
{"4435": "\ndef get_base_domain ( domain , use_fresh_psl = 0 ) : \n    psl_path = os . path . join ( tempdir , \"public_suffix_list.dat\" ) \n    def download_psl ( ) : \n        url = \"https://publicsuffix.org/list/public_suffix_list.dat\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        fresh_psl = requests . get ( url , headers = headers ) . text \n        with open ( psl_path , \"w\" , encoding = \"utf-8\" ) as fresh_psl_file : \n            fresh_psl_file . write ( fresh_psl ) \n    if use_fresh_psl : \n        if not os . path . exists ( psl_path ) : \n            download_psl ( ) \n        else : \n            psl_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( psl_path ) . st_mtime ) \n            if psl_age > timedelta ( hours = 24 ) : \n                try : \n                    download_psl ( ) \n                except Exception as error : \n                    logger . warning ( \"Failed to download an updated PSL {0}\" . format ( error ) ) \n        with open ( psl_path , encoding = \"utf-8\" ) as psl_file : \n            psl = publicsuffix2 . PublicSuffixList ( psl_file ) \n        return psl . get_public_suffix ( domain ) \n    else : \n        return publicsuffix2 . get_public_suffix ( domain ) "}
{"4437": "\ndef human_timestamp_to_datetime ( human_timestamp , to_utc = 0 ) : \n    settings = { } \n    if to_utc : \n        settings = { \"TO_TIMEZONE\" : \"UTC\" } \n    return dateparser . parse ( human_timestamp , settings = settings ) "}
{"4438": "\ndef get_ip_address_country ( ip_address , parallel = 0 ) : \n    def download_country_database ( location = \"GeoLite2-Country.mmdb\" ) : \n        if parallel : \n            logging . warning ( \"Cannot download GeoIP database in parallel mode\" ) \n            return \n        url = \"https://geolite.maxmind.com/download/geoip/database/\" \"GeoLite2-Country.tar.gz\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        original_filename = \"GeoLite2-Country.mmdb\" \n        try : \n            response = requests . get ( url , headers = headers ) \n            response . raise_for_status ( ) \n            tar_bytes = response . content \n            tar_file = tarfile . open ( fileobj = BytesIO ( tar_bytes ) , mode = \"r:gz\" ) \n            tar_dir = tar_file . getnames ( ) [ 0 ] \n            tar_path = \"{0}/{1}\" . format ( tar_dir , original_filename ) \n            tar_file . extract ( tar_path ) \n            shutil . move ( tar_path , location ) \n            shutil . rmtree ( tar_dir ) \n        except Exception as e : \n            logger . warning ( \"Error downloading {0}: {1}\" . format ( url , e . __str__ ( ) ) ) \n    system_paths = [ \"GeoLite2-Country.mmdb\" , \"/usr/local/share/GeoIP/GeoLite2-Country.mmdb\" , \"/usr/share/GeoIP/GeoLite2-Country.mmdb\" , \"/var/lib/GeoIP/GeoLite2-Country.mmdb\" , \"/var/local/lib/GeoIP/GeoLite2-Country.mmdb\" , \"C:\\\\GeoIP\\\\GeoLite2-Country.mmdb\" ] \n    db_path = None \n    for system_path in system_paths : \n        if os . path . exists ( system_path ) : \n            db_path = system_path \n            break \n    if db_path is None : \n        db_path = os . path . join ( tempdir , \"GeoLite2-Country.mmdb\" ) \n        if not os . path . exists ( db_path ) : \n            download_country_database ( db_path ) \n            if not os . path . exists ( db_path ) : \n                return None \n        else : \n            db_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( db_path ) . st_mtime ) \n            if db_age > timedelta ( days = 7 ) : \n                download_country_database ( ) \n        db_path = db_path \n    db_reader = geoip2 . database . Reader ( db_path ) \n    country = None \n    try : \n        country = db_reader . country ( ip_address ) . country . iso_code \n    except geoip2 . errors . AddressNotFoundError : \n        pass \n    return country "}
{"4439": "\ndef get_ip_address_info ( ip_address , cache = None , nameservers = None , timeout = 2.0 , parallel = 0 ) : \n    ip_address = ip_address . lower ( ) \n    if cache : \n        info = cache . get ( ip_address , None ) \n        if info : \n            return info \n    info = OrderedDict ( ) \n    info [ \"ip_address\" ] = ip_address \n    reverse_dns = get_reverse_dns ( ip_address , nameservers = nameservers , timeout = timeout ) \n    country = get_ip_address_country ( ip_address , parallel = parallel ) \n    info [ \"country\" ] = country \n    info [ \"reverse_dns\" ] = reverse_dns \n    info [ \"base_domain\" ] = None \n    if reverse_dns is not None : \n        base_domain = get_base_domain ( reverse_dns ) \n        info [ \"base_domain\" ] = base_domain \n    return info "}
{"4442": "\ndef cli_parse ( file_path , sa , nameservers , dns_timeout , parallel = 0 ) : \n    try : \n        file_results = parse_report_file ( file_path , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) \n    except ParserError as error : \n        return error , file_path \n    finally : \n        global counter \n        with counter . get_lock ( ) : \n            counter . value += 1 \n    return file_results , file_path "}
{"4447": "\ndef subscribe_async ( self , subject , ** kwargs ) : \n    kwargs [ \"is_async\" ] = 1 \n    sid = yield from self . subscribe ( subject , ** kwargs ) \n    return sid "}
{"4450": "\ndef _select_next_server ( self ) : \n    while 1 : \n        if len ( self . _server_pool ) == 0 : \n            self . _current_server = None \n            raise ErrNoServers \n        now = time . monotonic ( ) \n        s = self . _server_pool . pop ( 0 ) \n        if self . options [ \"max_reconnect_attempts\" ] > 0 : \n            if s . reconnects > self . options [ \"max_reconnect_attempts\" ] : \n                continue \n        self . _server_pool . append ( s ) \n        if s . last_attempt is not None and now < s . last_attempt + self . options [ \"reconnect_time_wait\" ] : \n            yield from asyncio . sleep ( self . options [ \"reconnect_time_wait\" ] , loop = self . _loop ) \n        try : \n            s . last_attempt = time . monotonic ( ) \n            r , w = yield from asyncio . open_connection ( s . uri . hostname , s . uri . port , loop = self . _loop , limit = DEFAULT_BUFFER_SIZE ) \n            self . _current_server = s \n            self . _bare_io_reader = self . _io_reader = r \n            self . _bare_io_writer = self . _io_writer = w \n            break \n        except Exception as e : \n            s . last_attempt = time . monotonic ( ) \n            s . reconnects += 1 \n            self . _err = e \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            continue "}
{"4451": "\ndef _process_err ( self , err_msg ) : \n    if STALE_CONNECTION in err_msg : \n        yield from self . _process_op_err ( ErrStaleConnection ) \n        return \n    if AUTHORIZATION_VIOLATION in err_msg : \n        self . _err = ErrAuthorization \n    else : \n        m = b'nats: ' + err_msg [ 0 ] \n        self . _err = NatsError ( m . decode ( ) ) \n    do_cbs = 0 \n    if not self . is_connecting : \n        do_cbs = 1 \n    self . _loop . create_task ( self . _close ( Client . CLOSED , do_cbs ) ) "}
{"4452": "\ndef _process_op_err ( self , e ) : \n    if self . is_connecting or self . is_closed or self . is_reconnecting : \n        return \n    if self . options [ \"allow_reconnect\" ] and self . is_connected : \n        self . _status = Client . RECONNECTING \n        self . _ps . reset ( ) \n        if self . _reconnection_task is not None and not self . _reconnection_task . cancelled ( ) : \n            self . _reconnection_task . cancel ( ) \n        self . _reconnection_task = self . _loop . create_task ( self . _attempt_reconnect ( ) ) \n    else : \n        self . _process_disconnect ( ) \n        self . _err = e \n        yield from self . _close ( Client . CLOSED , 1 ) "}
{"4453": "\ndef _connect_command ( self ) : \n    options = { \"verbose\" : self . options [ \"verbose\" ] , \"pedantic\" : self . options [ \"pedantic\" ] , \"lang\" : __lang__ , \"version\" : __version__ , \"protocol\" : PROTOCOL } \n    if \"auth_required\" in self . _server_info : \n        if self . _server_info [ \"auth_required\" ] : \n            if self . options [ \"user\" ] is not None and self . options [ \"password\" ] is not None : \n                options [ \"user\" ] = self . options [ \"user\" ] \n                options [ \"pass\" ] = self . options [ \"password\" ] \n            elif self . options [ \"token\" ] is not None : \n                options [ \"auth_token\" ] = self . options [ \"token\" ] \n            elif self . _current_server . uri . password is None : \n                options [ \"auth_token\" ] = self . _current_server . uri . username \n            else : \n                options [ \"user\" ] = self . _current_server . uri . username \n                options [ \"pass\" ] = self . _current_server . uri . password \n    if self . options [ \"name\" ] is not None : \n        options [ \"name\" ] = self . options [ \"name\" ] \n    if self . options [ \"no_echo\" ] is not None : \n        options [ \"echo\" ] = not self . options [ \"no_echo\" ] \n    connect_opts = json . dumps ( options , sort_keys = 1 ) \n    return b'' . join ( [ CONNECT_OP + _SPC_ + connect_opts . encode ( ) + _CRLF_ ] ) "}
{"4454": "\ndef _process_pong ( self ) : \n    if len ( self . _pongs ) > 0 : \n        future = self . _pongs . pop ( 0 ) \n        future . set_result ( 1 ) \n        self . _pongs_received += 1 \n        self . _pings_outstanding -= 1 "}
{"4456": "\ndef _process_info ( self , info ) : \n    if 'connect_urls' in info : \n        if info [ 'connect_urls' ] : \n            connect_urls = [ ] \n            for connect_url in info [ 'connect_urls' ] : \n                uri = urlparse ( \"nats://%s\" % connect_url ) \n                srv = Srv ( uri ) \n                srv . discovered = 1 \n                should_add = 1 \n                for s in self . _server_pool : \n                    if uri . netloc == s . uri . netloc : \n                        should_add = 0 \n                if should_add : \n                    connect_urls . append ( srv ) \n            if self . options [ \"dont_randomize\" ] is not 1 : \n                shuffle ( connect_urls ) \n            for srv in connect_urls : \n                self . _server_pool . append ( srv ) "}
{"4458": "\ndef _flusher ( self ) : \n    while 1 : \n        if not self . is_connected or self . is_connecting : \n            break \n        try : \n            yield from self . _flush_queue . get ( ) \n            if self . _pending_data_size > 0 : \n                self . _io_writer . writelines ( self . _pending [ : ] ) \n                self . _pending = [ ] \n                self . _pending_data_size = 0 \n                yield from self . _io_writer . drain ( ) \n        except OSError as e : \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            yield from self . _process_op_err ( e ) \n            break \n        except asyncio . CancelledError : \n            break "}
{"4459": "\ndef _read_loop ( self ) : \n    while 1 : \n        try : \n            should_bail = self . is_closed or self . is_reconnecting \n            if should_bail or self . _io_reader is None : \n                break \n            if self . is_connected and self . _io_reader . at_eof ( ) : \n                if self . _error_cb is not None : \n                    yield from self . _error_cb ( ErrStaleConnection ) \n                yield from self . _process_op_err ( ErrStaleConnection ) \n                break \n            b = yield from self . _io_reader . read ( DEFAULT_BUFFER_SIZE ) \n            yield from self . _ps . parse ( b ) \n        except ErrProtocol : \n            yield from self . _process_op_err ( ErrProtocol ) \n            break \n        except OSError as e : \n            yield from self . _process_op_err ( e ) \n            break \n        except asyncio . CancelledError : \n            break "}
{"4466": "\ndef feature_selection ( feat_select , X , y ) : \n    if re . match ( '.*-best' , feat_select ) is not None : \n        n = int ( feat_select . split ( '-' ) [ 0 ] ) \n        selector = SelectKBest ( k = n ) \n        import warnings \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( 'ignore' , category = UserWarning ) \n            features_selected = np . where ( selector . fit ( X , y ) . get_support ( ) is 1 ) [ 0 ] \n    elif re . match ( '.*-randombest' , feat_select ) is not None : \n        n = int ( feat_select . split ( '-' ) [ 0 ] ) \n        from random import shuffle \n        features = range ( 0 , X . shape [ 1 ] ) \n        shuffle ( features ) \n        features_selected = features [ : n ] \n    return features_selected "}
{"4467": "\ndef get_studies_by_regions ( dataset , masks , threshold = 0.08 , remove_overlap = 1 , studies = None , features = None , regularization = \"scale\" ) : \n    import nibabel as nib \n    import os \n    try : \n        loaded_masks = [ nib . load ( os . path . relpath ( m ) ) for m in masks ] \n    except OSError : \n        print ( 'Error loading masks. Check the path' ) \n    grouped_ids = [ dataset . get_studies ( mask = m , activation_threshold = threshold ) for m in loaded_masks ] \n    flat_ids = reduce ( lambda a , b : a + b , grouped_ids ) \n    if remove_overlap : \n        import collections \n        flat_ids = [ id for ( id , count ) in collections . Counter ( flat_ids ) . items ( ) if count == 1 ] \n        grouped_ids = [ [ x for x in m if x in flat_ids ] for m in grouped_ids ] \n    y = [ [ idx ] * len ( ids ) for ( idx , ids ) in enumerate ( grouped_ids ) ] \n    y = reduce ( lambda a , b : a + b , y ) \n    y = np . array ( y ) \n    X = [ dataset . get_feature_data ( ids = group_ids , features = features ) for group_ids in grouped_ids ] \n    X = np . vstack ( tuple ( X ) ) \n    if regularization : \n        X = regularize ( X , method = regularization ) \n    return ( X , y ) "}
{"4469": "\ndef classify_regions ( dataset , masks , method = 'ERF' , threshold = 0.08 , remove_overlap = 1 , regularization = 'scale' , output = 'summary' , studies = None , features = None , class_weight = 'auto' , classifier = None , cross_val = '4-Fold' , param_grid = None , scoring = 'accuracy' ) : \n    ( X , y ) = get_studies_by_regions ( dataset , masks , threshold , remove_overlap , studies , features , regularization = regularization ) \n    return classify ( X , y , method , classifier , output , cross_val , class_weight , scoring = scoring , param_grid = param_grid ) "}
{"4470": "\ndef classify ( X , y , clf_method = 'ERF' , classifier = None , output = 'summary_clf' , cross_val = None , class_weight = None , regularization = None , param_grid = None , scoring = 'accuracy' , refit_all = 1 , feat_select = None ) : \n    clf = Classifier ( clf_method , classifier , param_grid ) \n    if cross_val is not None : \n        score = clf . cross_val_fit ( X , y , cross_val , scoring = scoring , feat_select = feat_select , class_weight = class_weight ) \n    else : \n        score = clf . fit ( X , y , class_weight = class_weight ) . score ( X , y ) \n    from collections import Counter \n    if output == 'clf' : \n        return clf \n    else : \n        if output == 'summary' : \n            output = { 'score' : score , 'n' : dict ( Counter ( y ) ) } \n        elif output == 'summary_clf' : \n            output = { 'score' : score , 'n' : dict ( Counter ( y ) ) , 'clf' : clf , 'features_selected' : clf . features_selected , 'predictions' : clf . predictions } \n        return output "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = 1 ) : \n    if masker is not None : \n        masker = masker \n    else : \n        if isinstance ( dataset , Dataset ) : \n            masker = dataset . masker \n        else : \n            if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n                raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = 0 ) \n    if regions . ndim == 2 : \n        m = regions \n        for i in range ( regions . shape [ 1 ] ) : \n            _nz = np . nonzero ( m [ : , i ] ) [ 0 ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        for i in range ( n_regions ) : \n            if isinstance ( threshold , int ) : \n                m [ regions == labels [ i ] , i ] = 1.0 \n            else : \n                m [ regions == labels [ i ] , i ] = 1.0 / np . sum ( regions == labels [ i ] ) \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ result < threshold ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = 1 ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = 1 ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( prop_mask_active > activation_threshold ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( dists > 5.5 ) & ( dists < 6.5 ) ) [ 0 ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ dists <= r ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    elif return_type == 'data' : \n        return self . get_image_data ( ids ) "}
{"4483": "\ndef add_features ( self , features , append = 1 , merge = 'outer' , duplicates = 'ignore' , min_studies = 0.0 , threshold = 0.001 ) : \n    if ( not append ) or not hasattr ( self , 'feature_table' ) : \n        self . feature_table = FeatureTable ( self ) \n    self . feature_table . add_features ( features , merge = merge , duplicates = duplicates , min_studies = min_studies , threshold = threshold ) "}
{"4488": "\ndef get_image_data ( self , ids = None , voxels = None , dense = 1 ) : \n    if dense and ids is None and voxels is None : \n        logger . warning ( \"Warning: get_image_data() is being called without specifying \" \"a subset of studies or voxels to retrieve. This may result in\" \" a very large amount of data (several GB) being read into \" \"memory. If you experience any problems, consider returning a \" \"sparse matrix by passing dense=False, or pass in a list of \" \"ids of voxels to retrieve only a portion of the data.\" ) \n    result = self . data \n    if ids is not None : \n        idxs = np . where ( np . in1d ( np . array ( self . ids ) , np . array ( ids ) ) ) [ 0 ] \n        result = result [ : , idxs ] \n    if voxels is not None : \n        result = result [ voxels , : ] \n    return result . toarray ( ) if dense else result "}
{"4489": "\ndef get_feature_data ( self , ids = None , features = None , dense = 1 ) : \n    result = self . data \n    if ids is not None : \n        result = result . ix [ ids ] \n    if features is not None : \n        result = result . ix [ : , features ] \n    return result . to_dense ( ) if dense else result "}
{"4491": "\ndef get_ids ( self , features , threshold = 0.0 , func = np . sum , get_weights = 0 ) : \n    if isinstance ( features , str ) : \n        features = [ features ] \n    features = self . search_features ( features ) \n    feature_weights = self . data . ix [ : , features ] \n    weights = feature_weights . apply ( func , 1 ) \n    above_thresh = weights [ weights >= threshold ] \n    return above_thresh if get_weights else list ( above_thresh . index ) "}
{"4499": "\ndef mask ( self , image , nan_to_num = 1 , layers = None , in_global_mask = 0 ) : \n    self . set_mask ( layers ) \n    image = self . get_image ( image , output = 'vector' ) \n    if in_global_mask : \n        masked_data = image [ self . global_mask ] \n        masked_data [ ~ self . get_mask ( in_global_mask = 1 ) ] = 0 \n    else : \n        masked_data = image [ self . current_mask ] \n    if nan_to_num : \n        masked_data = np . nan_to_num ( masked_data ) \n    return masked_data "}
{"4500": "\ndef get_mask ( self , layers = None , output = 'vector' , in_global_mask = 1 ) : \n    if in_global_mask : \n        output = 'vector' \n    if layers is None : \n        layers = self . layers . keys ( ) \n    elif not isinstance ( layers , list ) : \n        layers = [ layers ] \n    layers = map ( lambda x : x if isinstance ( x , string_types ) else self . stack [ x ] , layers ) \n    layers = [ self . layers [ l ] for l in layers if l in self . layers ] \n    layers . append ( self . full ) \n    layers = np . vstack ( layers ) . T . astype ( bool ) \n    mask = layers . all ( axis = 1 ) \n    mask = self . get_image ( mask , output ) \n    return mask [ self . global_mask ] if in_global_mask else mask "}
{"4501": "\ndef load_imgs ( filenames , masker , nan_to_num = 1 ) : \n    if isinstance ( filenames , string_types ) : \n        filenames = [ filenames ] \n    data = np . zeros ( ( masker . n_vox_in_mask , len ( filenames ) ) ) \n    for i , f in enumerate ( filenames ) : \n        data [ : , i ] = masker . mask ( f , nan_to_num ) \n    return data "}
{"4505": "\ndef normalized_tokens ( s , string_options = DEFAULT_STRING_OPTIONS , token_options = DEFAULT_TOKEN_OPTIONS , strip_parentheticals = 1 , whitespace = 0 , languages = None ) : \n    s = safe_decode ( s ) \n    normalized_tokens = _normalize . normalized_tokens ( s , string_options , token_options , whitespace , languages = languages ) \n    if strip_parentheticals : \n        normalized_tokens = remove_parens ( normalized_tokens ) \n    return [ ( s , token_types . from_id ( token_type ) ) for s , token_type in normalized_tokens ] "}
{"4510": "\ndef get_dataframe ( self , tickers , startDate = None , endDate = None , metric_name = None , frequency = 'daily' ) : \n    valid_columns = [ 'open' , 'high' , 'low' , 'close' , 'volume' , 'adjOpen' , 'adjHigh' , 'adjLow' , 'adjClose' , 'adjVolume' , 'divCash' , 'splitFactor' ] \n    if metric_name is not None and metric_name not in valid_columns : \n        raise APIColumnNameError ( 'Valid data items are: ' + str ( valid_columns ) ) \n    params = { 'format' : 'json' , 'resampleFreq' : frequency } \n    if startDate : \n        params [ 'startDate' ] = startDate \n    if endDate : \n        params [ 'endDate' ] = endDate \n    if pandas_is_installed : \n        if type ( tickers ) is str : \n            stock = tickers \n            url = self . _get_url ( stock , frequency ) \n            response = self . _request ( 'GET' , url , params = params ) \n            df = pd . DataFrame ( response . json ( ) ) \n            if metric_name is not None : \n                prices = df [ metric_name ] \n                prices . index = df [ 'date' ] \n            else : \n                prices = df \n                prices . index = df [ 'date' ] \n                del ( prices [ 'date' ] ) \n        else : \n            prices = pd . DataFrame ( ) \n            for stock in tickers : \n                url = self . _get_url ( stock , frequency ) \n                response = self . _request ( 'GET' , url , params = params ) \n                df = pd . DataFrame ( response . json ( ) ) \n                df . index = df [ 'date' ] \n                df . rename ( index = str , columns = { metric_name : stock } , inplace = 1 ) \n                prices = pd . concat ( [ prices , df [ stock ] ] , axis = 1 ) \n        prices . index = pd . to_datetime ( prices . index ) \n        return prices \n    else : \n        error_message = ( \"Pandas is not installed, but .get_ticker_price() was \" \"called with fmt=pandas.  In order to install tiingo with \" \"pandas, reinstall with pandas as an optional dependency. \\n\" \"Install tiingo with pandas dependency: \\'pip install tiingo[pandas]\\'\\n\" \"Alternatively, just install pandas: pip install pandas.\" ) \n        raise InstallPandasException ( error_message ) "}
{"4538": "\nasync def create_playlist ( self , name , * , public = 1 , collaborative = 0 , description = None ) : \n    data = { 'name' : name , 'public' : public , 'collaborative' : collaborative } \n    if description : \n        data [ 'description' ] = description \n    playlist_data = await self . http . create_playlist ( self . id , data ) \n    return Playlist ( self . __client , playlist_data ) "}
{"4541": "\nasync def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : \n    tracks = [ ] \n    offset = 0 \n    total = self . total_tracks or None \n    while 1 : \n        data = await self . __client . http . album_tracks ( self . id , limit = 50 , offset = offset , market = market ) \n        if total is None : \n            total = data [ 'total' ] \n        offset += 50 \n        tracks += list ( Track ( self . __client , item ) for item in data [ 'items' ] ) \n        if len ( tracks ) >= total : \n            break \n    return tracks "}
{"4553": "\ndef url_ ( client_id : str , redirect_uri : str , * , scope : str = None , state : str = None , secure : bool = 1 ) -> str : \n    attrs = { 'client_id' : client_id , 'redirect_uri' : quote ( redirect_uri ) } \n    if scope is not None : \n        attrs [ 'scope' ] = quote ( scope ) \n    if state is not None : \n        attrs [ 'state' ] = state \n    parameters = '&' . join ( '{0}={1}' . format ( * item ) for item in attrs . items ( ) ) \n    return OAuth2 . _BASE . format ( parameters = parameters ) "}
{"4559": "\nasync def transfer ( self , device : SomeDevice , ensure_playback : bool = 0 ) : \n    await self . _user . http . transfer_player ( str ( device ) , play = ensure_playback ) "}
{"4561": "\ndef get ( self ) : \n    domain_validation = self . checker . is_domain_valid ( ) \n    ip_validation = self . checker . is_ip_valid ( ) \n    if \"current_test_data\" in PyFunceble . INTERN : \n        PyFunceble . INTERN [ \"current_test_data\" ] . update ( { \"domain_syntax_validation\" : domain_validation , \"ip4_syntax_validation\" : ip_validation , } ) \n    if ( domain_validation and not ip_validation or domain_validation or PyFunceble . CONFIGURATION [ \"local\" ] ) : \n        PyFunceble . INTERN . update ( { \"http_code\" : HTTPCode ( ) . get ( ) , \"referer\" : Referer ( ) . get ( ) } ) \n        if not PyFunceble . INTERN [ \"referer\" ] : \n            return PyFunceble . INTERN [ \"referer\" ] \n        if PyFunceble . INTERN [ \"referer\" ] and not self . checker . is_subdomain ( ) : \n            return self . _extract ( ) \n        Logs ( ) . whois ( self . whois_record ) \n        return None \n    if ( ip_validation and not domain_validation or ip_validation or PyFunceble . CONFIGURATION [ \"local\" ] ) : \n        PyFunceble . INTERN [ \"http_code\" ] = HTTPCode ( ) . get ( ) \n        Logs ( ) . whois ( self . whois_record ) \n        return None \n    Logs ( ) . whois ( self . whois_record ) \n    return 0 "}
{"4564": "\ndef _is_version_greater ( self ) : \n    checked = Version ( 1 ) . check_versions ( self . current_version [ 0 ] , self . version_yaml ) \n    if checked is not None and not checked : \n        return 1 \n    return 0 "}
{"4565": "\ndef is_dev_version ( cls ) : \n    command = \"git branch\" \n    command_result = Command ( command ) . execute ( ) \n    for branch in command_result . split ( \"\\n\" ) : \n        if branch . startswith ( \"*\" ) and \"dev\" in branch : \n            return 1 \n    return 0 "}
{"4566": "\ndef _does_require_deprecation ( self ) : \n    for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2 ] ) : \n        if version_number > self . version_yaml [ index ] : \n            return 1 \n    return 0 "}
{"4569": "\ndef _is_to_ignore ( cls , line ) : \n    to_ignore = [ r\"(^!|^@@|^\\/|^\\[|^\\.|^-|^_|^\\?|^&)\" ] \n    for element in to_ignore : \n        if Regex ( line , element , return_data = 0 ) . match ( ) : \n            return 1 \n    return 0 "}
{"4570": "\ndef _handle_options ( self , options ) : \n    result = [ ] \n    regex_domain_option = r\"domain=(.*)\" \n    for option in options : \n        try : \n            domains = Regex ( option , regex_domain_option , return_data = 1 , rematch = 1 , group = 0 ) . match ( ) [ - 1 ] \n            if domains : \n                if self . aggressive : \n                    result . extend ( [ x for x in domains . split ( \"|\" ) if x and not x . startswith ( \"~\" ) ] ) \n                else : \n                    return 1 \n        except TypeError : \n            pass \n    return result "}
{"4571": "\ndef _extract_base ( self , element ) : \n    if isinstance ( element , list ) : \n        return [ self . _extract_base ( x ) for x in element ] \n    base = self . checker . is_url_valid ( url = element , return_base = 1 ) \n    if base : \n        return base \n    if \"/\" in element : \n        return element . split ( \"/\" ) [ 0 ] \n    return element "}
{"4572": "\ndef _format_decoded ( self , to_format , result = None ) : \n    if not result : \n        result = [ ] \n    for data in List ( to_format ) . format ( ) : \n        if data : \n            if \"^\" in data : \n                return self . _format_decoded ( data . split ( \"^\" ) , result ) \n            if \"#\" in data : \n                return self . _format_decoded ( data . split ( \"#\" ) , result ) \n            if \",\" in data : \n                return self . _format_decoded ( data . split ( \",\" ) , result ) \n            if \"!\" in data : \n                return self . _format_decoded ( data . split ( \"!\" ) , result ) \n            if \"|\" in data : \n                return self . _format_decoded ( data . split ( \"|\" ) , result ) \n            if data : \n                data = self . _extract_base ( data ) \n                if data and ( self . checker . is_domain_valid ( data ) or self . checker . is_ip_valid ( data ) ) : \n                    result . append ( data ) \n                elif data : \n                    url_base = self . checker . is_url_valid ( data , return_base = 1 ) \n                    if url_base : \n                        result . append ( url_base ) \n    return result "}
{"4575": "\ndef syntax_check ( domain ) : \n    if domain and isinstance ( domain , str ) : \n        load_config ( 1 ) \n        return Check ( domain ) . is_domain_valid ( ) \n    return None "}
{"4576": "\ndef is_subdomain ( domain ) : \n    if domain and isinstance ( domain , str ) : \n        load_config ( 1 ) \n        return Check ( domain ) . is_subdomain ( ) \n    return None "}
{"4577": "\ndef ipv4_syntax_check ( ip ) : \n    if ip and isinstance ( ip , str ) : \n        load_config ( 1 ) \n        return Check ( ip ) . is_ip_valid ( ) \n    return None "}
{"4578": "\ndef is_ipv4_range ( ip ) : \n    if ip and isinstance ( ip , str ) : \n        load_config ( 1 ) \n        return Check ( ip ) . is_ip_range ( ) \n    return None "}
{"4579": "\ndef url_syntax_check ( url ) : \n    if url and isinstance ( url , str ) : \n        load_config ( 1 ) \n        return Check ( url ) . is_url_valid ( ) \n    return None "}
{"4580": "\ndef load_config ( under_test = 0 , custom = None ) : \n    if \"config_loaded\" not in INTERN : \n        Load ( CURRENT_DIRECTORY ) \n        if not under_test : \n            DirectoryStructure ( ) \n        INTERN . update ( { \"config_loaded\" : 1 } ) \n        if custom and isinstance ( custom , dict ) : \n            CONFIGURATION . update ( custom ) "}
{"4582": "\ndef _entry_management_url_download ( self , passed ) : \n    if passed and self . checker . is_url_valid ( passed ) : \n        file_to_test = passed . split ( \"/\" ) [ - 1 ] \n        if ( not PyFunceble . path . isfile ( file_to_test ) or PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] == 0 ) : \n            Download ( passed , file_to_test ) . text ( ) \n        PyFunceble . INTERN [ \"file_to_test\" ] = file_to_test \n        return 1 \n    return 0 "}
{"4584": "\ndef _print_header ( cls ) : \n    if ( not PyFunceble . CONFIGURATION [ \"quiet\" ] and not PyFunceble . CONFIGURATION [ \"header_printed\" ] ) : \n        print ( \"\\n\" ) \n        if PyFunceble . CONFIGURATION [ \"less\" ] : \n            Prints ( None , \"Less\" ) . header ( ) \n        else : \n            Prints ( None , \"Generic\" ) . header ( ) \n        PyFunceble . CONFIGURATION [ \"header_printed\" ] = 1 "}
{"4585": "\ndef _file_decision ( self , current , last , status = None ) : \n    if ( status and not PyFunceble . CONFIGURATION [ \"simple\" ] and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . mining . process ( ) \n        self . mining . remove ( ) \n        if ( status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            if self . inactive_database . is_present ( ) : \n                Generate ( PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ) . analytic_file ( \"suspicious\" ) \n                self . inactive_database . remove ( ) \n        else : \n            self . inactive_database . add ( ) \n        self . auto_continue . backup ( ) \n        if current != last : \n            AutoSave ( ) \n        else : \n            ExecutionTime ( \"stop\" , last = 1 ) \n            self . percentage . log ( ) \n            self . reset_counters ( ) \n            self . auto_continue . backup ( ) \n            self . colorify_logo ( ) \n            AutoSave ( 1 ) \n    for index in [ \"http_code\" , \"referer\" ] : \n        if index in PyFunceble . INTERN : \n            PyFunceble . INTERN [ index ] = \"\" "}
{"4588": "\ndef colorify_logo ( cls , home = 0 ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        elif PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] >= 50 : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n        else : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        print ( \"\\n\" . join ( to_print ) ) "}
{"4593": "\ndef switch ( cls , variable , custom = 0 ) : \n    if not custom : \n        current_state = dict . get ( PyFunceble . CONFIGURATION , variable ) \n    else : \n        current_state = variable \n    if isinstance ( current_state , bool ) : \n        if current_state : \n            return 0 \n        return 1 \n    to_print = \"Impossible to switch %s. Please post an issue to %s\" \n    raise Exception ( to_print % ( repr ( variable ) , PyFunceble . LINKS [ \"repo\" ] + \"/issues.\" ) ) "}
{"4594": "\ndef get ( cls ) : \n    if \"to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"to_test\" ] : \n        expiration_date = ExpirationDate ( ) . get ( ) \n        if expiration_date is 0 : \n            return cls . handle ( status = \"invalid\" ) \n        if expiration_date == PyFunceble . STATUS [ \"official\" ] [ \"up\" ] : \n            return expiration_date , \"WHOIS\" \n        return cls . handle ( status = \"inactive\" ) \n    raise NotImplementedError ( \"We expect `INTERN['to_test']` to be set.\" ) "}
{"4597": "\ndef _create_directory ( cls , directory , loop = 0 ) : \n    if not loop and PyFunceble . directory_separator in directory : \n        splited_directory = directory . split ( PyFunceble . directory_separator ) \n        full_path_to_create = \"\" \n        for single_directory in splited_directory : \n            full_path_to_create += single_directory + PyFunceble . directory_separator \n            cls . _create_directory ( full_path_to_create , 1 ) \n    if not PyFunceble . path . isdir ( directory ) : \n        AutoSave . travis_permissions ( ) \n        PyFunceble . mkdir ( directory ) \n        AutoSave . travis_permissions ( ) "}
{"4601": "\ndef _install_production_config ( self ) : \n    production_config_link = \"https://raw.githubusercontent.com/funilrys/PyFunceble/master/.PyFunceble_production.yaml\" \n    production_config_link = Version ( 1 ) . right_url_from_version ( production_config_link ) \n    if not Version ( 1 ) . is_cloned ( ) : \n        Download ( production_config_link , self . path_to_default_config ) . text ( ) \n    return Download ( production_config_link , self . path_to_config ) . text ( ) "}
{"4602": "\ndef _install_iana_config ( cls ) : \n    iana_link = PyFunceble . CONFIGURATION [ \"links\" ] [ \"iana\" ] \n    iana_link = Version ( 1 ) . right_url_from_version ( iana_link ) \n    destination = PyFunceble . CURRENT_DIRECTORY + \"iana-domains-db.json\" \n    if not Version ( 1 ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : \n        return Download ( iana_link , destination ) . text ( ) \n    return None "}
{"4603": "\ndef _install_psl_config ( cls ) : \n    psl_link = PyFunceble . CONFIGURATION [ \"links\" ] [ \"psl\" ] \n    psl_link = Version ( 1 ) . right_url_from_version ( psl_link ) \n    destination = ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . CONFIGURATION [ \"outputs\" ] [ \"default_files\" ] [ \"public_suffix\" ] ) \n    if not Version ( 1 ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : \n        return Download ( psl_link , destination ) . text ( ) \n    return None "}
{"4604": "\ndef _install_directory_structure_file ( cls ) : \n    dir_structure_link = PyFunceble . CONFIGURATION [ \"links\" ] [ \"dir_structure\" ] \n    dir_structure_link = Version ( 1 ) . right_url_from_version ( dir_structure_link ) \n    destination = ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . CONFIGURATION [ \"outputs\" ] [ \"default_files\" ] [ \"dir_structure\" ] ) \n    if not Version ( 1 ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : \n        data = Download ( dir_structure_link , destination , return_data = 1 ) . text ( ) \n        File ( destination ) . write ( data , overwrite = 1 ) \n        return 1 \n    return None "}
{"4606": "\ndef _load ( self ) : \n    if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in PyFunceble . environ : \n        while 1 : \n            response = input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + \"A configuration key is missing.\\n\" + PyFunceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) \n            if isinstance ( response , str ) : \n                if response . lower ( ) == \"y\" : \n                    self . _merge_values ( ) \n                    self . _save ( ) \n                    print ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) \n                    break \n                elif response . lower ( ) == \"n\" : \n                    raise Exception ( \"Configuration key still missing.\" ) \n    else : \n        self . _merge_values ( ) \n        self . _save ( ) "}
{"4607": "\ndef split_versions ( cls , version , return_non_digits = 0 ) : \n    splited_version = version . split ( \".\" ) \n    digits = [ x for x in splited_version if x . isdigit ( ) ] \n    if not return_non_digits : \n        return digits \n    non_digits = [ x for x in splited_version if not x . isdigit ( ) ] \n    return ( digits , non_digits [ 0 ] ) "}
{"4608": "\ndef check_versions ( cls , local , upstream ) : \n    status = [ None , None , None ] \n    for index , version_number in enumerate ( local ) : \n        if int ( version_number ) < int ( upstream [ index ] ) : \n            status [ index ] = 1 \n        elif int ( version_number ) > int ( upstream [ index ] ) : \n            status [ index ] = 0 \n    if 0 in status : \n        return 0 \n    if 1 in status : \n        return 1 \n    return None "}
{"4609": "\ndef is_cloned ( cls ) : \n    if not PyFunceble . path . isdir ( \".git\" ) : \n        return 0 \n    list_of_file = [ \".coveragerc\" , \".coveralls.yml\" , \".gitignore\" , \".PyFunceble_production.yaml\" , \".travis.yml\" , \"CODE_OF_CONDUCT.md\" , \"CONTRIBUTING.md\" , \"dir_structure_production.json\" , \"MANIFEST.in\" , \"README.rst\" , \"requirements.txt\" , \"setup.py\" , \"version.yaml\" , ] \n    list_of_dir = [ \"docs\" , \"PyFunceble\" , \"tests\" ] \n    for file in list_of_file : \n        if not PyFunceble . path . isfile ( file ) : \n            return 0 \n    for directory in list_of_dir : \n        if not PyFunceble . path . isdir ( directory ) : \n            return 0 \n    return 1 "}
{"4612": "\ndef unified_file ( self ) : \n    if ( \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] and PyFunceble . CONFIGURATION [ \"unified\" ] ) : \n        output = ( self . output_parent_dir + PyFunceble . OUTPUTS [ \"default_files\" ] [ \"results\" ] ) \n        if PyFunceble . CONFIGURATION [ \"less\" ] : \n            if PyFunceble . HTTP_CODE [ \"active\" ] : \n                to_print = [ self . tested , self . domain_status , PyFunceble . INTERN [ \"http_code\" ] , ] \n            else : \n                to_print = [ self . tested , self . domain_status , self . source ] \n            Prints ( to_print , \"Less\" , output , 1 ) . data ( ) \n        else : \n            to_print = [ self . tested , self . domain_status , self . expiration_date , self . source , PyFunceble . INTERN [ \"http_code\" ] , PyFunceble . CURRENT_TIME , ] \n            Prints ( to_print , \"Generic_File\" , output , 1 ) . data ( ) "}
{"4614": "\ndef _do_not_produce_file ( self ) : \n    if ( Inactive ( ) . is_present ( ) and self . domain_status in [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , ] and PyFunceble . INTERN [ \"to_test\" ] not in PyFunceble . INTERN [ \"extracted_list_to_test\" ] ) : \n        return 1 \n    return 0 "}
{"4618": "\ndef hierarchical ( cls , element ) : \n    to_sort = \"\" \n    full_extension = \"\" \n    element = element . lower ( ) \n    url_base = Check ( ) . is_url_valid ( element , return_base = 1 ) \n    if not isinstance ( url_base , str ) : \n        if \".\" in element : \n            extension_index = element . rindex ( \".\" ) + 1 \n            extension = element [ extension_index : ] \n            if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n                for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                    formatted_suffix = \".\" + suffix \n                    if element . endswith ( formatted_suffix ) : \n                        suffix_index = element . rindex ( formatted_suffix ) \n                        to_sort = element [ : suffix_index ] \n                        full_extension = suffix \n                        break \n            if not full_extension : \n                full_extension = element [ extension_index : ] \n                to_sort = element [ : extension_index - 1 ] \n            full_extension += \".\" \n            tros_ot = to_sort [ : : - 1 ] \n            if \".\" in tros_ot : \n                full_extension = ( tros_ot [ : tros_ot . index ( \".\" ) ] [ : : - 1 ] + \".\" + full_extension ) \n                tros_ot = tros_ot [ tros_ot . index ( \".\" ) + 1 : ] \n                reversion = full_extension + \".\" . join ( [ x [ : : - 1 ] for x in tros_ot . split ( \".\" ) ] ) \n                return ( Regex ( reversion , cls . regex_replace , replace_with = \"@funilrys\" ) . replace ( ) . replace ( \"@funilrys\" , \"\" ) ) \n            return ( Regex ( to_sort + full_extension , cls . regex_replace , replace_with = \"@funilrys\" , ) . replace ( ) . replace ( \"@funilrys\" , \"\" ) ) \n        return element \n    protocol_position = element . rindex ( url_base ) \n    protocol = element [ : protocol_position ] \n    return protocol + cls . hierarchical ( url_base ) "}
{"4620": "\ndef _referer ( self , extension ) : \n    iana_record = self . lookup . whois ( PyFunceble . CONFIGURATION [ \"iana_whois_server\" ] , \"hello.%s\" % extension ) \n    if iana_record and \"refer\" in iana_record : \n        regex_referer = r\"(?s)refer\\:\\s+([a-zA-Z0-9._-]+)\\n\" \n        matched = Regex ( iana_record , regex_referer , return_data = 1 , group = 1 ) . match ( ) \n        if matched : \n            return matched \n    if extension in self . manual_server : \n        return self . manual_server [ extension ] \n    return None "}
{"4621": "\ndef _extensions ( self ) : \n    upstream_lines = ( Download ( self . iana_url , return_data = 1 ) . text ( ) . split ( '<span class=\"domain tld\">' ) ) \n    regex_valid_extension = r\"(/domains/root/db/)(.*)(\\.html)\" \n    for block in upstream_lines : \n        if \"/domains/root/db/\" in block : \n            matched = Regex ( block , regex_valid_extension , return_data = 1 , rematch = 1 ) . match ( ) [ 1 ] \n            if matched : \n                referer = self . _referer ( matched ) \n                yield ( matched , referer ) "}
{"4623": "\ndef mine ( self ) : \n    if PyFunceble . CONFIGURATION [ \"mining\" ] : \n        try : \n            history = PyFunceble . requests . get ( self . to_get , timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] , headers = self . headers , ) . history \n            mined = { self . to_get_bare : [ ] } \n            for element in history : \n                element = element . url \n                if PyFunceble . INTERN [ \"to_test_type\" ] == \"url\" : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = 0 ) \n                elif PyFunceble . INTERN [ \"to_test_type\" ] == \"domain\" : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = 1 ) \n                else : \n                    raise Exception ( \"Unknown tested.\" ) \n                if to_append : \n                    if to_append . endswith ( \":80\" ) : \n                        to_append = to_append [ : - 3 ] \n                    if to_append != self . to_get_bare : \n                        mined [ self . to_get_bare ] . append ( to_append ) \n            if mined [ self . to_get_bare ] : \n                return mined \n            return None \n        except ( PyFunceble . requests . ConnectionError , PyFunceble . requests . exceptions . Timeout , PyFunceble . requests . exceptions . InvalidURL , PyFunceble . socket . timeout , urllib3_exceptions . InvalidHeader , UnicodeDecodeError , ) : \n            return None \n    return None "}
{"4637": "\ndef header ( self , do_not_print = 0 ) : \n    if ( not PyFunceble . CONFIGURATION [ \"header_printed\" ] or self . template == \"Percentage\" or do_not_print ) : \n        if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template == \"Generic_File\" ) : \n            to_print = self . headers [ \"Generic\" ] \n            if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] and PyFunceble . HTTP_CODE [ \"active\" ] ) : \n                to_print = Dict ( to_print ) . remove_key ( \"Analyze Date\" ) \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"invalid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ] \n        elif ( self . template == \"Less\" or self . template == \"Percentage\" or self . template == \"HTTP\" ) : \n            to_print = self . headers [ self . template ] \n            if self . template == \"Less\" and not PyFunceble . HTTP_CODE [ \"active\" ] : \n                to_print [ \"Source\" ] = 10 \n        if not PyFunceble . HTTP_CODE [ \"active\" ] : \n            to_print = Dict ( to_print ) . remove_key ( \"HTTP Code\" ) \n        self . currently_used_header = to_print \n        if not do_not_print : \n            self . _before_header ( ) \n            for formatted_template in self . _header_constructor ( to_print ) : \n                if not self . only_on_file : \n                    print ( formatted_template ) \n                if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                    File ( self . output ) . write ( formatted_template + \"\\n\" ) "}
{"4642": "\ndef data ( self ) : \n    if isinstance ( self . data_to_print , list ) : \n        to_print = { } \n        to_print_size = [ ] \n        alone_cases = [ \"Percentage\" , \"HTTP\" ] \n        without_header = [ \"FullHosts\" , \"PlainDomain\" ] \n        if self . template . lower ( ) == \"json\" : \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                return self . _json_print ( ) \n            return None \n        if self . template not in alone_cases and self . template not in without_header : \n            self . header ( 1 ) \n            to_print_size = self . _size_from_header ( self . currently_used_header ) \n        elif self . template in without_header : \n            for data in self . data_to_print : \n                to_print_size . append ( str ( len ( data ) ) ) \n        else : \n            to_print_size = self . _size_from_header ( self . headers [ self . template ] ) \n        to_print = self . _data_constructor ( to_print_size ) \n        self . _before_header ( ) \n        for data in self . _header_constructor ( to_print , 0 ) : \n            if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template in [ \"Less\" , \"Percentage\" ] : \n                if not self . only_on_file : \n                    colorified_data = self . _colorify ( data ) \n                    print ( colorified_data ) \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                File ( self . output ) . write ( data + \"\\n\" ) \n    else : \n        raise Exception ( \"Please review Prints().data()\" ) "}
{"4643": "\ndef _save ( self , last = 0 ) : \n    if ( self . _authorization ( ) and PyFunceble . CONFIGURATION [ \"logs\" ] and \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . file = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"execution_time\" ] ) \n        if PyFunceble . path . isfile ( self . file ) : \n            content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) \n        else : \n            content = { } \n        if self . action == \"start\" : \n            if \"final_total\" in content and content [ \"final_total\" ] : \n                del content [ \"final_total\" ] \n            if \"data\" in content : \n                content [ \"data\" ] . append ( [ PyFunceble . INTERN [ \"start\" ] ] ) \n            else : \n                content [ \"data\" ] = [ [ PyFunceble . INTERN [ \"start\" ] ] ] \n        elif self . action == \"stop\" : \n            try : \n                content [ \"data\" ] [ - 1 ] . append ( PyFunceble . INTERN [ \"end\" ] ) \n                start = content [ \"data\" ] [ 0 ] [ 0 ] \n                end = content [ \"data\" ] [ - 1 ] [ - 1 ] \n                content [ \"current_total\" ] = self . format_execution_time ( start , end ) \n                if last : \n                    content [ \"final_total\" ] = content [ \"current_total\" ] \n                    print ( PyFunceble . Fore . MAGENTA + PyFunceble . Style . BRIGHT + \"Global execution time: \" + content [ \"final_total\" ] ) \n            except KeyError : \n                pass \n        try : \n            Dict ( content ) . to_json ( self . file ) \n        except FileNotFoundError : \n            DirectoryStructure ( ) \n            Dict ( content ) . to_json ( self . file ) "}
{"4648": "\ndef almost_everything ( self , clean_all = 0 ) : \n    to_delete = self . file_to_delete ( ) \n    if clean_all : \n        to_delete . extend ( self . databases_to_delete ( ) ) \n    for file in to_delete : \n        File ( file ) . delete ( ) \n    if clean_all : \n        Load ( PyFunceble . CURRENT_DIRECTORY ) "}
{"4652": "\ndef execute ( self ) : \n    process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = 1 ) \n    ( output , error ) = process . communicate ( ) \n    if process . returncode != 0 : \n        return self . _decode_output ( error ) \n    return self . _decode_output ( output ) "}
{"4654": "\ndef rename_key ( self , key_to_rename , strict = 1 ) : \n    if isinstance ( self . main_dictionnary , dict ) and isinstance ( key_to_rename , dict ) : \n        for old , new in key_to_rename . items ( ) : \n            if strict : \n                if old in self . main_dictionnary : \n                    self . main_dictionnary [ new ] = self . main_dictionnary . pop ( old ) \n            else : \n                to_rename = { } \n                for index in self . main_dictionnary : \n                    if old in index : \n                        to_rename . update ( { index : new [ : - 1 ] + index . split ( old ) [ - 1 ] } ) \n                self . main_dictionnary = Dict ( self . main_dictionnary ) . rename_key ( to_rename , 1 ) \n        return self . main_dictionnary \n    return None "}
{"4655": "\ndef merge ( self , to_merge , strict = 1 ) : \n    result = { } \n    for element in to_merge : \n        if element in self . main_dictionnary : \n            if isinstance ( to_merge [ element ] , dict ) and isinstance ( self . main_dictionnary [ element ] , dict ) : \n                result [ element ] = Dict ( self . main_dictionnary [ element ] ) . merge ( to_merge [ element ] ) \n            elif isinstance ( to_merge [ element ] , list ) and isinstance ( self . main_dictionnary [ element ] , list ) : \n                result [ element ] = List ( self . main_dictionnary [ element ] ) . merge ( to_merge [ element ] , strict ) \n            else : \n                result . update ( { element : to_merge [ element ] } ) \n        else : \n            result . update ( { element : to_merge [ element ] } ) \n    for element in self . main_dictionnary : \n        if element not in result : \n            result [ element ] = self . main_dictionnary [ element ] \n    return result "}
{"4656": "\ndef to_json ( self , destination ) : \n    try : \n        with open ( destination , \"w\" ) as file : \n            dump ( self . main_dictionnary , file , ensure_ascii = 0 , indent = 4 , sort_keys = 1 , ) \n    except UnicodeEncodeError : \n        with open ( destination , \"w\" , encoding = \"utf-8\" ) as file : \n            dump ( self . main_dictionnary , file , ensure_ascii = 0 , indent = 4 , sort_keys = 1 , ) "}
{"4657": "\ndef to_yaml ( self , destination , flow_style = 0 ) : \n    with open ( destination , \"w\" ) as file : \n        dump_yaml ( self . main_dictionnary , file , encoding = \"utf-8\" , allow_unicode = 1 , indent = 4 , default_flow_style = flow_style , ) "}
{"4659": "\ndef write ( self , data_to_write , overwrite = 0 ) : \n    if overwrite or not path . isfile ( self . file ) : \n        with open ( self . file , \"w\" , encoding = \"utf-8\" , newline = \"\\n\" ) as file : \n            if data_to_write and isinstance ( data_to_write , str ) : \n                file . write ( data_to_write ) \n    else : \n        with open ( self . file , \"a\" , encoding = \"utf-8\" , newline = \"\\n\" ) as file : \n            if data_to_write and isinstance ( data_to_write , str ) : \n                file . write ( data_to_write ) "}
{"4662": "\ndef merge ( self , to_merge , strict = 1 ) : \n    result = [ ] \n    if strict : \n        for index , element in enumerate ( to_merge ) : \n            try : \n                if isinstance ( element , dict ) and isinstance ( self . main_list [ index ] , dict ) : \n                    result . append ( Dict ( self . main_list [ index ] ) . merge ( element ) ) \n                elif isinstance ( element , list ) and isinstance ( self . main_list [ index ] , list ) : \n                    result . append ( List ( self . main_list [ index ] ) . merge ( element ) ) \n                else : \n                    result . append ( element ) \n            except IndexError : \n                result . append ( element ) \n    else : \n        result = self . main_list \n        for element in to_merge : \n            if element not in result : \n                result . append ( element ) \n    return result "}
{"4664": "\ndef match ( self ) : \n    result = [ ] \n    to_match = comp ( self . regex ) \n    if self . rematch : \n        pre_result = to_match . findall ( self . data ) \n    else : \n        pre_result = to_match . search ( self . data ) \n    if self . return_data and pre_result : \n        if self . rematch : \n            for data in pre_result : \n                if isinstance ( data , tuple ) : \n                    result . extend ( list ( data ) ) \n                else : \n                    result . append ( data ) \n            if self . group != 0 : \n                return result [ self . group ] \n        else : \n            result = pre_result . group ( self . group ) . strip ( ) \n        return result \n    if not self . return_data and pre_result : \n        return 1 \n    return 0 "}
{"4669": "\ndef is_url_valid ( self , url = None , return_base = 0 , return_formatted = 0 ) : \n    initial_base = None \n    if url : \n        to_test = url \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    if to_test . startswith ( \"http\" ) : \n        try : \n            regex = r\"(^(http:\\/\\/|https:\\/\\/)(.+?(?=\\/)|.+?$))\" \n            initial_base = base = Regex ( to_test , regex , return_data = 1 , rematch = 1 ) . match ( ) [ 2 ] \n            if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n                base = domain2idna ( base ) \n            domain_status = self . is_domain_valid ( base ) \n            ip_status = self . is_ip_valid ( base ) \n            if domain_status or ip_status : \n                if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] and return_formatted : \n                    return Regex ( to_test , initial_base , escape = 1 , return_data = 1 , replace_with = base , occurences = 1 , ) . replace ( ) \n                if return_formatted : \n                    return to_test \n                if return_base : \n                    return base \n                return 1 \n        except TypeError : \n            pass \n    if return_formatted : \n        return to_test \n    return 0 "}
{"4670": "\ndef is_domain_valid ( self , domain = None , subdomain_check = 0 ) : \n    regex_valid_domains = r\"^(?=.{0,253}$)(([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9](?:\\.)?|[a-z0-9](?:\\.)?))$\" \n    regex_valid_subdomains = r\"^(?=.{0,253}$)(([a-z0-9_][a-z0-9-_]{0,61}[a-z0-9_-]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9]))$\" \n    if domain : \n        to_test = domain \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    try : \n        last_point_index = to_test . rindex ( \".\" ) \n        extension = to_test [ last_point_index + 1 : ] \n        if not extension and to_test . endswith ( \".\" ) : \n            try : \n                extension = [ x for x in to_test . split ( \".\" ) if x ] [ - 1 ] \n            except IndexError : \n                pass \n        if not extension or extension not in PyFunceble . INTERN [ \"iana_db\" ] : \n            return 0 \n        if ( Regex ( to_test , regex_valid_domains , return_data = 0 ) . match ( ) and not subdomain_check ) : \n            return 1 \n        if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n            for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                try : \n                    suffix_index = to_test . rindex ( \".\" + suffix ) \n                    to_check = to_test [ : suffix_index ] \n                    if \".\" not in to_check and subdomain_check : \n                        return 0 \n                    if \".\" in to_check and subdomain_check : \n                        return 1 \n                    if \".\" in to_check : \n                        return Regex ( to_check , regex_valid_subdomains , return_data = 0 ) . match ( ) \n                except ValueError : \n                    pass \n        to_check = to_test [ : last_point_index ] \n        if \".\" in to_check and subdomain_check : \n            return 1 \n        if \".\" in to_check : \n            return Regex ( to_check , regex_valid_subdomains , return_data = 0 ) . match ( ) \n    except ( ValueError , AttributeError ) : \n        pass \n    return 0 "}
{"4671": "\ndef is_subdomain ( self , domain = None ) : \n    if domain : \n        to_test = domain \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    return self . is_domain_valid ( to_test , subdomain_check = 1 ) "}
{"4678": "\ndef is_present ( cls ) : \n    if PyFunceble . CONFIGURATION [ \"inactive_database\" ] : \n        if PyFunceble . INTERN [ \"to_test\" ] in PyFunceble . INTERN [ \"flatten_inactive_db\" ] or ( PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"inactive_db\" ] and PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] and \"to_test\" in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] and PyFunceble . INTERN [ \"to_test\" ] in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ \"to_test\" ] ) : \n            return 1 \n    return 0 "}
{"4681": "\ndef is_in_database ( self ) : \n    if ( self . _authorization ( ) and PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] and PyFunceble . INTERN [ \"to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] ) : \n        return 1 \n    return 0 "}
{"4682": "\ndef is_time_older ( self ) : \n    if ( self . _authorization ( ) and self . is_in_database ( ) and int ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) < int ( PyFunceble . time ( ) ) ) : \n        return 1 \n    return 0 "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = 0 \n            try : \n                time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                print ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"4687": "\ndef nslookup ( cls ) : \n    try : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            if not Check ( ) . is_ip_valid ( ) : \n                request = PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n                for sequence in request : \n                    PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] . append ( sequence [ - 1 ] [ 0 ] ) \n            else : \n                request = PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ 0 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ 1 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2 ] \n        else : \n            if not Check ( ) . is_ip_valid ( ) : \n                PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n            else : \n                PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n        return 1 \n    except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : \n        return 0 "}
{"4688": "\ndef whois ( cls , whois_server , domain = None , timeout = None ) : \n    if domain is None : \n        domain = PyFunceble . INTERN [ \"to_test\" ] \n    if timeout is None : \n        timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] \n    if whois_server : \n        req = PyFunceble . socket . socket ( PyFunceble . socket . AF_INET , PyFunceble . socket . SOCK_STREAM ) \n        if timeout % 3 == 0 : \n            req . settimeout ( timeout ) \n        else : \n            req . settimeout ( 3 ) \n        try : \n            req . connect ( ( whois_server , 43 ) ) \n        except PyFunceble . socket . error : \n            return None \n        req . send ( ( domain + \"\\r\\n\" ) . encode ( ) ) \n        response = b\"\" \n        while 1 : \n            try : \n                data = req . recv ( 4096 ) \n            except ( PyFunceble . socket . timeout , ConnectionResetError ) : \n                req . close ( ) \n                return None \n            response += data \n            if not data : \n                break \n        req . close ( ) \n        try : \n            return response . decode ( ) \n        except UnicodeDecodeError : \n            return response . decode ( \"utf-8\" , \"replace\" ) \n    return None "}
{"4689": "\ndef get ( cls ) : \n    if Check ( ) . is_url_valid ( ) or PyFunceble . CONFIGURATION [ \"local\" ] : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            PyFunceble . INTERN [ \"current_test_data\" ] [ \"url_syntax_validation\" ] = 1 \n        PyFunceble . INTERN . update ( { \"http_code\" : HTTPCode ( ) . get ( ) } ) \n        active_list = [ ] \n        active_list . extend ( PyFunceble . HTTP_CODE [ \"list\" ] [ \"potentially_up\" ] ) \n        active_list . extend ( PyFunceble . HTTP_CODE [ \"list\" ] [ \"up\" ] ) \n        inactive_list = [ ] \n        inactive_list . extend ( PyFunceble . HTTP_CODE [ \"list\" ] [ \"potentially_down\" ] ) \n        inactive_list . append ( \"*\" * 3 ) \n        if PyFunceble . INTERN [ \"http_code\" ] in active_list : \n            return URLStatus ( PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ) . handle ( ) \n        if PyFunceble . INTERN [ \"http_code\" ] in inactive_list : \n            return URLStatus ( PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ) . handle ( ) \n    if \"current_test_data\" in PyFunceble . INTERN : \n        PyFunceble . INTERN [ \"current_test_data\" ] [ \"url_syntax_validation\" ] = 0 \n    return URLStatus ( PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ) . handle ( ) "}
{"4690": "\ndef get ( self ) : \n    if not PyFunceble . CONFIGURATION [ \"local\" ] : \n        if self . domain_extension not in self . ignored_extension : \n            referer = None \n            if self . domain_extension in PyFunceble . INTERN [ \"iana_db\" ] : \n                if not PyFunceble . CONFIGURATION [ \"no_whois\" ] : \n                    referer = PyFunceble . INTERN [ \"iana_db\" ] [ self . domain_extension ] \n                    if not referer : \n                        Logs ( ) . referer_not_found ( self . domain_extension ) \n                        return None \n                    return referer \n                return None \n            return 0 \n        return None \n    return None "}
{"4692": "\ndef standard_paths ( ) : \n    for is_plat_spec in [ 1 , 0 ] : \n        path = distutils . sysconfig . get_python_lib ( standard_lib = 1 , plat_specific = is_plat_spec ) \n        for name in os . listdir ( path ) : \n            yield name \n        try : \n            for name in os . listdir ( os . path . join ( path , 'lib-dynload' ) ) : \n                yield name \n        except OSError : \n            pass "}
{"4699": "\ndef duplicate_key_line_numbers ( messages , source ) : \n    messages = [ message for message in messages if isinstance ( message , pyflakes . messages . MultiValueRepeatedKeyLiteral ) ] \n    if messages : \n        key_to_messages = create_key_to_messages_dict ( messages ) \n        lines = source . split ( '\\n' ) \n        for ( key , messages ) in key_to_messages . items ( ) : \n            good = 1 \n            for message in messages : \n                line = lines [ message . lineno - 1 ] \n                key = message . message_args [ 0 ] \n                if not dict_entry_has_key ( line , key ) : \n                    good = 0 \n            if good : \n                for message in messages : \n                    yield message . lineno "}
{"4703": "\ndef multiline_import ( line , previous_line = '' ) : \n    for symbol in '()' : \n        if symbol in line : \n            return 1 \n    if line . lstrip ( ) . startswith ( '>' ) : \n        return 1 \n    return multiline_statement ( line , previous_line ) "}
{"4704": "\ndef multiline_statement ( line , previous_line = '' ) : \n    for symbol in '\\\\:;' : \n        if symbol in line : \n            return 1 \n    sio = io . StringIO ( line ) \n    try : \n        list ( tokenize . generate_tokens ( sio . readline ) ) \n        return previous_line . rstrip ( ) . endswith ( '\\\\' ) \n    except ( SyntaxError , tokenize . TokenError ) : \n        return 1 "}
{"4707": "\ndef filter_code ( source , additional_imports = None , expand_star_imports = 0 , remove_all_unused_imports = 0 , remove_duplicate_keys = 0 , remove_unused_variables = 0 , ignore_init_module_imports = 0 , ) : \n    imports = SAFE_IMPORTS \n    if additional_imports : \n        imports |= frozenset ( additional_imports ) \n    del additional_imports \n    messages = check ( source ) \n    if ignore_init_module_imports : \n        marked_import_line_numbers = frozenset ( ) \n    else : \n        marked_import_line_numbers = frozenset ( unused_import_line_numbers ( messages ) ) \n    marked_unused_module = collections . defaultdict ( lambda : [ ] ) \n    for line_number , module_name in unused_import_module_name ( messages ) : \n        marked_unused_module [ line_number ] . append ( module_name ) \n    if expand_star_imports and not ( re . search ( r'\\b__all__\\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : \n        marked_star_import_line_numbers = frozenset ( star_import_used_line_numbers ( messages ) ) \n        if len ( marked_star_import_line_numbers ) > 1 : \n            marked_star_import_line_numbers = frozenset ( ) \n        else : \n            undefined_names = [ ] \n            for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : \n                undefined_names . append ( undefined_name ) \n            if not undefined_names : \n                marked_star_import_line_numbers = frozenset ( ) \n    else : \n        marked_star_import_line_numbers = frozenset ( ) \n    if remove_unused_variables : \n        marked_variable_line_numbers = frozenset ( unused_variable_line_numbers ( messages ) ) \n    else : \n        marked_variable_line_numbers = frozenset ( ) \n    if remove_duplicate_keys : \n        marked_key_line_numbers = frozenset ( duplicate_key_line_numbers ( messages , source ) ) \n    else : \n        marked_key_line_numbers = frozenset ( ) \n    line_messages = get_messages_by_line ( messages ) \n    sio = io . StringIO ( source ) \n    previous_line = '' \n    for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : \n        if '#' in line : \n            yield line \n        elif line_number in marked_import_line_numbers : \n            yield filter_unused_import ( line , unused_module = marked_unused_module [ line_number ] , remove_all_unused_imports = remove_all_unused_imports , imports = imports , previous_line = previous_line ) \n        elif line_number in marked_variable_line_numbers : \n            yield filter_unused_variable ( line ) \n        elif line_number in marked_key_line_numbers : \n            yield filter_duplicate_key ( line , line_messages [ line_number ] , line_number , marked_key_line_numbers , source ) \n        elif line_number in marked_star_import_line_numbers : \n            yield filter_star_import ( line , undefined_names ) \n        else : \n            yield line \n        previous_line = line "}
{"4711": "\ndef dict_entry_has_key ( line , key ) : \n    if '#' in line : \n        return 0 \n    result = re . match ( r'\\s*(.*)\\s*:\\s*(.*),\\s*$' , line ) \n    if not result : \n        return 0 \n    try : \n        candidate_key = ast . literal_eval ( result . group ( 1 ) ) \n    except ( SyntaxError , ValueError ) : \n        return 0 \n    if multiline_statement ( result . group ( 2 ) ) : \n        return 0 \n    return candidate_key == key "}
{"4712": "\ndef is_literal_or_name ( value ) : \n    try : \n        ast . literal_eval ( value ) \n        return 1 \n    except ( SyntaxError , ValueError ) : \n        pass \n    if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : \n        return 1 \n    return re . match ( r'^\\w+\\s*$' , value ) "}
{"4717": "\ndef fix_code ( source , additional_imports = None , expand_star_imports = 0 , remove_all_unused_imports = 0 , remove_duplicate_keys = 0 , remove_unused_variables = 0 , ignore_init_module_imports = 0 ) : \n    if not source : \n        return source \n    if 'nonlocal' in source : \n        remove_unused_variables = 0 \n    filtered_source = None \n    while 1 : \n        filtered_source = '' . join ( filter_useless_pass ( '' . join ( filter_code ( source , additional_imports = additional_imports , expand_star_imports = expand_star_imports , remove_all_unused_imports = remove_all_unused_imports , remove_duplicate_keys = remove_duplicate_keys , remove_unused_variables = remove_unused_variables , ignore_init_module_imports = ignore_init_module_imports , ) ) ) ) \n        if filtered_source == source : \n            break \n        source = filtered_source \n    return filtered_source "}
{"4719": "\ndef is_python_file ( filename ) : \n    if filename . endswith ( '.py' ) : \n        return 1 \n    try : \n        with open_with_encoding ( filename , None , limit_byte_check = MAX_PYTHON_FILE_DETECTION_BYTES ) as f : \n            text = f . read ( MAX_PYTHON_FILE_DETECTION_BYTES ) \n            if not text : \n                return 0 \n            first_line = text . splitlines ( ) [ 0 ] \n    except ( IOError , IndexError ) : \n        return 0 \n    if not PYTHON_SHEBANG_REGEX . match ( first_line ) : \n        return 0 \n    return 1 "}
{"4720": "\ndef is_exclude_file ( filename , exclude ) : \n    base_name = os . path . basename ( filename ) \n    if base_name . startswith ( '.' ) : \n        return 1 \n    for pattern in exclude : \n        if fnmatch . fnmatch ( base_name , pattern ) : \n            return 1 \n        if fnmatch . fnmatch ( filename , pattern ) : \n            return 1 \n    return 0 "}
{"4722": "\ndef _main ( argv , standard_out , standard_error ) : \n    import argparse \n    parser = argparse . ArgumentParser ( description = __doc__ , prog = 'autoflake' ) \n    parser . add_argument ( '-c' , '--check' , action = 'store_true' , help = 'return error code if changes are needed' ) \n    parser . add_argument ( '-i' , '--in-place' , action = 'store_true' , help = 'make changes to files instead of printing diffs' ) \n    parser . add_argument ( '-r' , '--recursive' , action = 'store_true' , help = 'drill down directories recursively' ) \n    parser . add_argument ( '--exclude' , metavar = 'globs' , help = 'exclude file/directory names that match these ' 'comma-separated globs' ) \n    parser . add_argument ( '--imports' , help = 'by default, only unused standard library ' 'imports are removed; specify a comma-separated ' 'list of additional modules/packages' ) \n    parser . add_argument ( '--expand-star-imports' , action = 'store_true' , help = 'expand wildcard star imports with undefined ' 'names; this only triggers if there is only ' 'one star import in the file; this is skipped if ' 'there are any uses of `__all__` or `del` in the ' 'file' ) \n    parser . add_argument ( '--remove-all-unused-imports' , action = 'store_true' , help = 'remove all unused imports (not just those from ' 'the standard library)' ) \n    parser . add_argument ( '--ignore-init-module-imports' , action = 'store_true' , help = 'exclude __init__.py when removing unused ' 'imports' ) \n    parser . add_argument ( '--remove-duplicate-keys' , action = 'store_true' , help = 'remove all duplicate keys in objects' ) \n    parser . add_argument ( '--remove-unused-variables' , action = 'store_true' , help = 'remove unused variables' ) \n    parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s ' + __version__ ) \n    parser . add_argument ( 'files' , nargs = '+' , help = 'files to format' ) \n    args = parser . parse_args ( argv [ 1 : ] ) \n    if args . remove_all_unused_imports and args . imports : \n        print ( 'Using both --remove-all and --imports is redundant' , file = standard_error ) \n        return 1 \n    if args . exclude : \n        args . exclude = _split_comma_separated ( args . exclude ) \n    else : \n        args . exclude = set ( [ ] ) \n    filenames = list ( set ( args . files ) ) \n    failure = 0 \n    for name in find_files ( filenames , args . recursive , args . exclude ) : \n        try : \n            fix_file ( name , args = args , standard_out = standard_out ) \n        except IOError as exception : \n            print ( unicode ( exception ) , file = standard_error ) \n            failure = 1 \n    return 1 if failure else 0 "}
{"4743": "\ndef process_request ( self , request , credential = None ) : \n    self . _client_identity = [ None , None ] \n    header = request . request_header \n    self . _set_protocol_version ( header . protocol_version ) \n    max_response_size = None \n    if header . maximum_response_size : \n        max_response_size = header . maximum_response_size . value \n    now = int ( time . time ( ) ) \n    if header . time_stamp : \n        then = header . time_stamp . value \n        if ( now >= then ) and ( ( now - then ) < 60 ) : \n            self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( then ) ) ) ) \n        else : \n            if now < then : \n                self . _logger . warning ( \"Received request with future timestamp. Received \" \"timestamp: {0}, Current timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Future request rejected by server.\" ) \n            else : \n                self . _logger . warning ( \"Received request with old timestamp. Possible \" \"replay attack. Received timestamp: {0}, Current \" \"timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Stale request rejected by server.\" ) \n    else : \n        self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( now ) ) ) ) \n    self . is_asynchronous = 0 \n    if header . asynchronous_indicator is not None : \n        self . is_asynchronous = header . asynchronous_indicator . value \n    if self . is_asynchronous : \n        raise exceptions . InvalidMessage ( \"Asynchronous operations are not supported.\" ) \n    if header . authentication : \n        if header . authentication . credentials : \n            auth_credentials = header . authentication . credentials [ 0 ] \n        else : \n            auth_credentials = None \n    else : \n        auth_credentials = None \n    self . _verify_credential ( auth_credentials , credential ) \n    batch_error_option = enums . BatchErrorContinuationOption . STOP \n    if header . batch_error_cont_option is not None : \n        batch_error_option = header . batch_error_cont_option . value \n    if batch_error_option == enums . BatchErrorContinuationOption . UNDO : \n        raise exceptions . InvalidMessage ( \"Undo option for batch handling is not supported.\" ) \n    batch_order_option = 0 \n    if header . batch_order_option : \n        batch_order_option = header . batch_order_option . value \n    response_batch = self . _process_batch ( request . batch_items , batch_error_option , batch_order_option ) \n    response = self . _build_response ( header . protocol_version , response_batch ) \n    return response , max_response_size , header . protocol_version "}
{"4750": "\ndef is_allowed ( self , policy_name , session_user , session_group , object_owner , object_type , operation ) : \n    policy_section = self . get_relevant_policy_section ( policy_name , session_group ) \n    if policy_section is None : \n        return 0 \n    object_policy = policy_section . get ( object_type ) \n    if not object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} objects.\" . format ( policy_name , self . _get_enum_string ( object_type ) ) ) \n        return 0 \n    operation_object_policy = object_policy . get ( operation ) \n    if not operation_object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} operations on {2} \" \"objects.\" . format ( policy_name , self . _get_enum_string ( operation ) , self . _get_enum_string ( object_type ) ) ) \n        return 0 \n    if operation_object_policy == enums . Policy . ALLOW_ALL : \n        return 1 \n    elif operation_object_policy == enums . Policy . ALLOW_OWNER : \n        if session_user == object_owner : \n            return 1 \n        else : \n            return 0 \n    elif operation_object_policy == enums . Policy . DISALLOW_ALL : \n        return 0 \n    else : \n        return 0 "}
{"4764": "\ndef read_value ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    try : \n        value = unpack ( '!Q' , istream . read ( self . LENGTH ) ) [ 0 ] \n    except Exception : \n        self . logger . error ( \"Error reading boolean value from buffer\" ) \n        raise \n    if value == 1 : \n        self . value = 1 \n    elif value == 0 : \n        self . value = 0 \n    else : \n        raise ValueError ( \"expected: 0 or 1, observed: {0}\" . format ( value ) ) \n    self . validate ( ) "}
{"4779": "\ndef is_bit_mask ( enumeration , potential_mask ) : \n    if not isinstance ( potential_mask , six . integer_types ) : \n        return 0 \n    mask_enumerations = ( CryptographicUsageMask , ProtectionStorageMask , StorageStatusMask ) \n    if enumeration not in mask_enumerations : \n        return 0 \n    mask = 0 \n    for value in [ e . value for e in enumeration ] : \n        if ( value & potential_mask ) == value : \n            mask |= value \n    if mask != potential_mask : \n        return 0 \n    return 1 "}
{"4789": "\ndef scan_policies ( self ) : \n    policy_files = get_json_files ( self . policy_directory ) \n    for f in set ( policy_files ) - set ( self . policy_files ) : \n        self . file_timestamps [ f ] = 0 \n    for f in set ( self . policy_files ) - set ( policy_files ) : \n        self . logger . info ( \"Removing policies for file: {}\" . format ( f ) ) \n        self . file_timestamps . pop ( f , None ) \n        for p in self . policy_cache . keys ( ) : \n            self . disassociate_policy_and_file ( p , f ) \n        for p in [ k for k , v in self . policy_map . items ( ) if v == f ] : \n            self . restore_or_delete_policy ( p ) \n    self . policy_files = policy_files \n    for f in sorted ( self . file_timestamps . keys ( ) ) : \n        t = os . path . getmtime ( f ) \n        if t > self . file_timestamps [ f ] : \n            self . logger . info ( \"Loading policies for file: {}\" . format ( f ) ) \n            self . file_timestamps [ f ] = t \n            old_p = [ k for k , v in self . policy_map . items ( ) if v == f ] \n            try : \n                new_p = operation_policy . read_policy_from_file ( f ) \n            except ValueError : \n                self . logger . error ( \"Failure loading file: {}\" . format ( f ) ) \n                self . logger . debug ( \"\" , exc_info = 1 ) \n                continue \n            for p in new_p . keys ( ) : \n                self . logger . info ( \"Loading policy: {}\" . format ( p ) ) \n                if p in self . reserved_policies : \n                    self . logger . warning ( \"Policy '{}' overwrites a reserved policy and \" \"will be thrown out.\" . format ( p ) ) \n                    continue \n                if p in sorted ( self . policy_store . keys ( ) ) : \n                    self . logger . debug ( \"Policy '{}' overwrites an existing \" \"policy.\" . format ( p ) ) \n                    if f != self . policy_map . get ( p ) : \n                        self . policy_cache . get ( p ) . append ( ( time . time ( ) , self . policy_map . get ( p ) , self . policy_store . get ( p ) ) ) \n                else : \n                    self . policy_cache [ p ] = [ ] \n                self . policy_store [ p ] = new_p . get ( p ) \n                self . policy_map [ p ] = f \n            for p in set ( old_p ) - set ( new_p . keys ( ) ) : \n                self . disassociate_policy_and_file ( p , f ) \n                self . restore_or_delete_policy ( p ) "}
{"4791": "\ndef get_certificate_from_connection ( connection ) : \n    certificate = connection . getpeercert ( binary_form = 1 ) \n    if certificate : \n        return x509 . load_der_x509_certificate ( certificate , backends . default_backend ( ) ) \n    return None "}
{"4803": "\ndef is_attribute_supported ( self , attribute ) : \n    if attribute not in self . _attribute_rule_sets . keys ( ) : \n        return 0 \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if self . _version >= rule_set . version_added : \n        return 1 \n    else : \n        return 0 "}
{"4804": "\ndef is_attribute_deprecated ( self , attribute ) : \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if rule_set . version_deprecated : \n        if self . _version >= rule_set . version_deprecated : \n            return 1 \n        else : \n            return 0 \n    else : \n        return 0 "}
{"4805": "\ndef is_attribute_applicable_to_object_type ( self , attribute , object_type ) : \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if object_type in rule_set . applies_to_object_types : \n        return 1 \n    else : \n        return 0 "}
{"4812": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    super ( Attributes , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    while 1 : \n        if len ( local_stream ) < 3 : \n            break \n        tag = struct . unpack ( '!I' , b'\\x00' + local_stream . peek ( 3 ) ) [ 0 ] \n        if enums . is_enum_value ( enums . Tags , tag ) : \n            tag = enums . Tags ( tag ) \n            if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n                raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n            value = self . _factory . create_attribute_value_by_enum ( tag , None ) \n            value . read ( local_stream , kmip_version = kmip_version ) \n            self . _attributes . append ( value ) \n        else : \n            break \n    self . is_oversized ( local_stream ) "}
{"4846": "\ndef serve ( self ) : \n    self . _socket . listen ( 5 ) \n    def _signal_handler ( signal_number , stack_frame ) : \n        self . _is_serving = 0 \n        if signal_number == signal . SIGINT : \n            raise KeyboardInterrupt ( \"SIGINT received\" ) \n    signal . signal ( signal . SIGINT , _signal_handler ) \n    signal . signal ( signal . SIGTERM , _signal_handler ) \n    self . _logger . info ( \"Starting connection service...\" ) \n    while self . _is_serving : \n        try : \n            connection , address = self . _socket . accept ( ) \n        except socket . timeout : \n            pass \n        except socket . error as e : \n            self . _logger . warning ( \"Error detected while establishing new connection.\" ) \n            self . _logger . exception ( e ) \n        except KeyboardInterrupt : \n            self . _logger . warning ( \"Interrupting connection service.\" ) \n            self . _is_serving = 0 \n            break \n        except Exception as e : \n            self . _logger . warning ( \"Error detected while establishing new connection.\" ) \n            self . _logger . exception ( e ) \n        else : \n            self . _setup_connection_handler ( connection , address ) \n    self . _logger . info ( \"Stopping connection service.\" ) "}
{"4855": "\ndef _encrypt_symmetric ( self , encryption_algorithm , encryption_key , plain_text , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    algorithm = self . _symmetric_key_algorithms . get ( encryption_algorithm , None ) \n    if algorithm is None : \n        raise exceptions . InvalidField ( \"Encryption algorithm '{0}' is not a supported symmetric \" \"encryption algorithm.\" . format ( encryption_algorithm ) ) \n    try : \n        algorithm = algorithm ( encryption_key ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"Invalid key bytes for the specified encryption algorithm.\" ) \n    return_iv_nonce = 0 \n    if encryption_algorithm == enums . CryptographicAlgorithm . RC4 : \n        mode = None \n    else : \n        if cipher_mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode is required.\" ) \n        mode = self . _modes . get ( cipher_mode , None ) \n        if mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode '{0}' is not a supported mode.\" . format ( cipher_mode ) ) \n        if hasattr ( mode , 'initialization_vector' ) or hasattr ( mode , 'nonce' ) : \n            if iv_nonce is None : \n                iv_nonce = os . urandom ( algorithm . block_size // 8 ) \n                return_iv_nonce = 1 \n            mode = mode ( iv_nonce ) \n        else : \n            mode = mode ( ) \n    if cipher_mode in [ enums . BlockCipherMode . CBC , enums . BlockCipherMode . ECB ] : \n        plain_text = self . _handle_symmetric_padding ( self . _symmetric_key_algorithms . get ( encryption_algorithm ) , plain_text , padding_method ) \n    cipher = ciphers . Cipher ( algorithm , mode , backend = default_backend ( ) ) \n    encryptor = cipher . encryptor ( ) \n    cipher_text = encryptor . update ( plain_text ) + encryptor . finalize ( ) \n    if return_iv_nonce : \n        return { 'cipher_text' : cipher_text , 'iv_nonce' : iv_nonce } \n    else : \n        return { 'cipher_text' : cipher_text } "}
{"4861": "\ndef verify_signature ( self , signing_key , message , signature , padding_method , signing_algorithm = None , hashing_algorithm = None , digital_signature_algorithm = None ) : \n    backend = default_backend ( ) \n    hash_algorithm = None \n    dsa_hash_algorithm = None \n    dsa_signing_algorithm = None \n    if hashing_algorithm : \n        hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n    if digital_signature_algorithm : \n        algorithm_pair = self . _digital_signature_algorithms . get ( digital_signature_algorithm ) \n        if algorithm_pair : \n            dsa_hash_algorithm = algorithm_pair [ 0 ] \n            dsa_signing_algorithm = algorithm_pair [ 1 ] \n    if dsa_hash_algorithm and dsa_signing_algorithm : \n        if hash_algorithm and ( hash_algorithm != dsa_hash_algorithm ) : \n            raise exceptions . InvalidField ( \"The hashing algorithm does not match the digital \" \"signature algorithm.\" ) \n        if ( signing_algorithm and ( signing_algorithm != dsa_signing_algorithm ) ) : \n            raise exceptions . InvalidField ( \"The signing algorithm does not match the digital \" \"signature algorithm.\" ) \n        signing_algorithm = dsa_signing_algorithm \n        hash_algorithm = dsa_hash_algorithm \n    if signing_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . PSS : \n            if hash_algorithm : \n                padding = asymmetric_padding . PSS ( mgf = asymmetric_padding . MGF1 ( hash_algorithm ( ) ) , salt_length = asymmetric_padding . PSS . MAX_LENGTH ) \n            else : \n                raise exceptions . InvalidField ( \"A hashing algorithm must be specified for PSS \" \"padding.\" ) \n        elif padding_method == enums . PaddingMethod . PKCS1v15 : \n            padding = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for signature \" \"verification.\" . format ( padding_method ) ) \n        try : \n            public_key = backend . load_der_public_key ( signing_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( signing_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The signing key bytes could not be loaded.\" ) \n        try : \n            public_key . verify ( signature , message , padding , hash_algorithm ( ) ) \n            return 1 \n        except errors . InvalidSignature : \n            return 0 \n        except Exception : \n            raise exceptions . CryptographicFailure ( \"The signature verification process failed.\" ) \n    else : \n        raise exceptions . InvalidField ( \"The signing algorithm '{0}' is not supported for \" \"signature verification.\" . format ( signing_algorithm ) ) "}
{"4876": "\ndef run ( self ) : \n    self . _logger . info ( \"Starting session: {0}\" . format ( self . name ) ) \n    try : \n        self . _connection . do_handshake ( ) \n    except Exception as e : \n        self . _logger . info ( \"Failure running TLS handshake\" ) \n        self . _logger . exception ( e ) \n    else : \n        while 1 : \n            try : \n                self . _handle_message_loop ( ) \n            except exceptions . ConnectionClosed as e : \n                break \n            except Exception as e : \n                self . _logger . info ( \"Failure handling message loop\" ) \n                self . _logger . exception ( e ) \n    self . _connection . shutdown ( socket . SHUT_RDWR ) \n    self . _connection . close ( ) \n    self . _logger . info ( \"Stopping session: {0}\" . format ( self . name ) ) "}
{"4882": "\ndef query ( self , batch = 0 , query_functions = None , credential = None ) : \n    batch_item = self . _build_query_batch_item ( query_functions ) \n    if batch : \n        self . batch_items . append ( batch_item ) \n    else : \n        request = self . _build_request_message ( credential , [ batch_item ] ) \n        response = self . _send_and_receive_message ( request ) \n        results = self . _process_batch_items ( response ) \n        return results [ 0 ] "}
{"4884": "\ndef open ( self ) : \n    if self . _is_open : \n        raise exceptions . ClientConnectionFailure ( \"client connection already open\" ) \n    else : \n        try : \n            self . proxy . open ( ) \n            self . _is_open = 1 \n        except Exception as e : \n            self . logger . error ( \"could not open client connection: %s\" , e ) \n            raise "}
{"4885": "\ndef close ( self ) : \n    if not self . _is_open : \n        return \n    else : \n        try : \n            self . proxy . close ( ) \n            self . _is_open = 0 \n        except Exception as e : \n            self . logger . error ( \"could not close client connection: %s\" , e ) \n            raise "}
{"4886": "\ndef create ( self , algorithm , length , operation_policy_name = None , name = None , cryptographic_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or length <= 0 : \n        raise TypeError ( \"length must be a positive integer\" ) \n    if cryptographic_usage_mask is not None : \n        if not isinstance ( cryptographic_usage_mask , list ) or all ( isinstance ( item , enums . CryptographicUsageMask ) for item in cryptographic_usage_mask ) is 0 : \n            raise TypeError ( \"cryptographic_usage_mask must be a list of \" \"CryptographicUsageMask enumerations\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    key_attributes = self . _build_key_attributes ( algorithm , length , cryptographic_usage_mask ) \n    key_attributes . extend ( common_attributes ) \n    if name : \n        key_attributes . extend ( self . _build_name_attribute ( name ) ) \n    template = cobjects . TemplateAttribute ( attributes = key_attributes ) \n    result = self . proxy . create ( enums . ObjectType . SYMMETRIC_KEY , template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        return result . uuid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4891": "\ndef locate ( self , maximum_items = None , storage_status_mask = None , object_group_member = None , attributes = None ) : \n    if maximum_items is not None : \n        if not isinstance ( maximum_items , six . integer_types ) : \n            raise TypeError ( \"maximum_items must be an integer\" ) \n    if storage_status_mask is not None : \n        if not isinstance ( storage_status_mask , six . integer_types ) : \n            raise TypeError ( \"storage_status_mask must be an integer\" ) \n    if object_group_member is not None : \n        if not isinstance ( object_group_member , enums . ObjectGroupMember ) : \n            raise TypeError ( \"object_group_member must be a ObjectGroupMember\" \"enumeration\" ) \n    if attributes is not None : \n        if not isinstance ( attributes , list ) or all ( isinstance ( item , cobjects . Attribute ) for item in attributes ) is 0 : \n            raise TypeError ( \"attributes must be a list of attributes\" ) \n    result = self . proxy . locate ( maximum_items , storage_status_mask , object_group_member , attributes ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        return result . uuids \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4930": "\ndef signin_user ( user , permenent = 1 ) : \n    session . permanent = permenent \n    session [ 'user_id' ] = user . id "}
{"4937": "\ndef _dataframe_to_csv ( writer , dataframe , delimiter , with_header ) : \n    encoding_writer = codecs . getwriter ( 'utf-8' ) ( writer ) \n    dataframe . to_csv ( path_or_buf = encoding_writer , sep = delimiter , header = with_header , index = 0 ) "}
{"4962": "\ndef draw ( self , cr , highlight = 0 , bounding = None ) : \n    if bounding is None or self . _intersects ( bounding ) : \n        self . _draw ( cr , highlight , bounding ) "}
{"4969": "\ndef init ( self ) : \n    cache . get ( 'sitetrees_reset' ) and self . empty ( init = 0 ) \n    self . cache = cache . get ( 'sitetrees' , { 'sitetrees' : { } , 'parents' : { } , 'items_by_ids' : { } , 'tree_aliases' : { } } ) "}
{"4970": "\ndef empty ( self , ** kwargs ) : \n    cache . delete ( 'sitetrees' ) \n    cache . delete ( 'sitetrees_reset' ) \n    kwargs . get ( 'init' , 1 ) and self . init ( ) "}
{"4971": "\ndef get_entry ( self , entry_name , key ) : \n    return self . cache [ entry_name ] . get ( key , 0 ) "}
{"4975": "\ndef resolve_tree_i18n_alias ( self , alias ) : \n    if alias not in _I18N_TREES : \n        return alias \n    current_language_code = self . current_lang \n    i18n_tree_alias = '%s_%s' % ( alias , current_language_code ) \n    trees_count = self . cache . get_entry ( 'tree_aliases' , i18n_tree_alias ) \n    if trees_count is 0 : \n        trees_count = MODEL_TREE_CLASS . objects . filter ( alias = i18n_tree_alias ) . count ( ) \n        self . cache . set_entry ( 'tree_aliases' , i18n_tree_alias , trees_count ) \n    if trees_count : \n        alias = i18n_tree_alias \n    return alias "}
{"4978": "\ndef get_tree_current_item ( self , tree_alias ) : \n    current_item = self . _current_items . get ( tree_alias , _UNSET ) \n    if current_item is not _UNSET : \n        if current_item is not None : \n            current_item . is_current = 1 \n        return current_item \n    current_item = None \n    if self . current_app_is_admin ( ) : \n        self . _current_items [ tree_alias ] = current_item \n        return None \n    current_url = self . current_request . path \n    if isinstance ( current_url , str ) : \n        current_url = current_url . encode ( 'UTF-8' ) \n    if current_url : \n        current_url = urlquote ( current_url ) \n    for url_item , url in self . _items_urls . items ( ) : \n        if url != current_url : \n            continue \n        url_item . is_current = 1 \n        if url_item . tree . alias == tree_alias : \n            current_item = url_item \n    if current_item is not None : \n        self . _current_items [ tree_alias ] = current_item \n    return current_item "}
{"4983": "\ndef menu ( self , tree_alias , tree_branches , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return '' \n    tree_branches = self . resolve_var ( tree_branches ) \n    parent_isnull = 0 \n    parent_ids = [ ] \n    parent_aliases = [ ] \n    current_item = self . get_tree_current_item ( tree_alias ) \n    self . tree_climber ( tree_alias , current_item ) \n    for branch_id in tree_branches . split ( ',' ) : \n        branch_id = branch_id . strip ( ) \n        if branch_id == ALIAS_TRUNK : \n            parent_isnull = 1 \n        elif branch_id == ALIAS_THIS_CHILDREN and current_item is not None : \n            branch_id = current_item . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_ANCESTOR_CHILDREN and current_item is not None : \n            branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_SIBLINGS and current_item is not None and current_item . parent is not None : \n            branch_id = current_item . parent . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_PARENT_SIBLINGS and current_item is not None : \n            branch_id = self . get_ancestor_level ( current_item , depth = 2 ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id . isdigit ( ) : \n            parent_ids . append ( int ( branch_id ) ) \n        else : \n            parent_aliases . append ( branch_id ) \n    check_access = self . check_access \n    menu_items = [ ] \n    for item in sitetree_items : \n        if not item . hidden and item . inmenu and check_access ( item , context ) : \n            if item . parent is None : \n                if parent_isnull : \n                    menu_items . append ( item ) \n            else : \n                if item . parent . id in parent_ids or item . parent . alias in parent_aliases : \n                    menu_items . append ( item ) \n    menu_items = self . apply_hook ( menu_items , 'menu' ) \n    self . update_has_children ( tree_alias , menu_items , 'menu' ) \n    return menu_items "}
{"4984": "\ndef check_access ( self , item , context ) : \n    if hasattr ( self . current_request . user . is_authenticated , '__call__' ) : \n        authenticated = self . current_request . user . is_authenticated ( ) \n    else : \n        authenticated = self . current_request . user . is_authenticated \n    if item . access_loggedin and not authenticated : \n        return 0 \n    if item . access_guest and authenticated : \n        return 0 \n    if item . access_restricted : \n        user_perms = self . _current_user_permissions \n        if user_perms is _UNSET : \n            user_perms = set ( context [ 'user' ] . get_all_permissions ( ) ) \n            self . _current_user_permissions = user_perms \n        if item . access_perm_type == MODEL_TREE_ITEM_CLASS . PERM_TYPE_ALL : \n            if len ( item . perms ) != len ( item . perms . intersection ( user_perms ) ) : \n                return 0 \n        else : \n            if not len ( item . perms . intersection ( user_perms ) ) : \n                return 0 \n    return 1 "}
{"4990": "\ndef filter_items ( self , items , navigation_type = None ) : \n    if self . current_app_is_admin ( ) : \n        return items \n    items_filtered = [ ] \n    context = self . current_page_context \n    check_access = self . check_access \n    for item in items : \n        if item . hidden : \n            continue \n        if not check_access ( item , context ) : \n            continue \n        if not getattr ( item , 'in%s' % navigation_type , 1 ) : \n            continue \n        items_filtered . append ( item ) \n    return items_filtered "}
{"4992": "\ndef tree_climber ( self , tree_alias , base_item ) : \n    if base_item is not None : \n        base_item . in_current_branch = 1 \n        if hasattr ( base_item , 'parent' ) and base_item . parent is not None : \n            self . tree_climber ( tree_alias , self . get_item_by_id ( tree_alias , base_item . parent . id ) ) "}
{"5000": "\ndef get_model_url_name ( model_nfo , page , with_namespace = 0 ) : \n    prefix = '' \n    if with_namespace : \n        prefix = 'admin:' \n    return ( '%s%s_%s' % ( prefix , '%s_%s' % model_nfo , page ) ) . lower ( ) "}
{"5006": "\ndef get_form ( self , request , obj = None , ** kwargs ) : \n    if obj is not None and obj . parent is not None : \n        self . previous_parent = obj . parent \n        previous_parent_id = self . previous_parent . id \n    else : \n        previous_parent_id = None \n    my_choice_field = TreeItemChoiceField ( self . tree , initial = previous_parent_id ) \n    form = super ( TreeItemAdmin , self ) . get_form ( request , obj , ** kwargs ) \n    my_choice_field . label = form . base_fields [ 'parent' ] . label \n    my_choice_field . help_text = form . base_fields [ 'parent' ] . help_text \n    my_choice_field . widget = form . base_fields [ 'parent' ] . widget \n    form . base_fields [ 'parent' ] = my_choice_field \n    if not getattr ( self , 'known_url_names' , 0 ) : \n        self . known_url_names = [ ] \n        self . known_url_rules = [ ] \n        resolver = get_resolver ( get_urlconf ( ) ) \n        for ns , ( url_prefix , ns_resolver ) in resolver . namespace_dict . items ( ) : \n            if ns != 'admin' : \n                self . _stack_known_urls ( ns_resolver . reverse_dict , ns ) \n        self . _stack_known_urls ( resolver . reverse_dict ) \n        self . known_url_rules = sorted ( self . known_url_rules ) \n    form . known_url_names_hint = _ ( 'You are seeing this warning because \"URL as Pattern\" option is active and pattern entered above ' 'seems to be invalid. Currently registered URL pattern names and parameters: ' ) \n    form . known_url_names = self . known_url_names \n    form . known_url_rules = self . known_url_rules \n    return form "}
{"5009": "\ndef save_model ( self , request , obj , form , change ) : \n    if change : \n        if obj . parent is not None and obj . parent . id == obj . id : \n            obj . parent = self . previous_parent \n            messages . warning ( request , _ ( \"Item's parent left unchanged. Item couldn't be parent to itself.\" ) , '' , 1 ) \n    obj . tree = self . tree \n    obj . save ( ) "}
{"5012": "\ndef tree ( alias , title = '' , items = None , ** kwargs ) : \n    tree_obj = get_tree_model ( ) ( alias = alias , title = title , ** kwargs ) \n    tree_obj . id = generate_id_for ( tree_obj ) \n    tree_obj . is_dynamic = 1 \n    if items is not None : \n        tree_obj . dynamic_items = [ ] \n        def traverse ( items ) : \n            for item in items : \n                item . tree = tree_obj \n                tree_obj . dynamic_items . append ( item ) \n                if hasattr ( item , 'dynamic_children' ) : \n                    traverse ( item . dynamic_children ) \n        traverse ( items ) \n    return tree_obj "}
{"5013": "\ndef item ( title , url , children = None , url_as_pattern = 1 , hint = '' , alias = '' , description = '' , in_menu = 1 , in_breadcrumbs = 1 , in_sitetree = 1 , access_loggedin = 0 , access_guest = 0 , access_by_perms = None , perms_mode_all = 1 , ** kwargs ) : \n    item_obj = get_tree_item_model ( ) ( title = title , url = url , urlaspattern = url_as_pattern , hint = hint , alias = alias , description = description , inmenu = in_menu , insitetree = in_sitetree , inbreadcrumbs = in_breadcrumbs , access_loggedin = access_loggedin , access_guest = access_guest , ** kwargs ) \n    item_obj . id = generate_id_for ( item_obj ) \n    item_obj . is_dynamic = 1 \n    item_obj . dynamic_children = [ ] \n    cleaned_permissions = [ ] \n    if access_by_perms : \n        if not isinstance ( access_by_perms , list ) : \n            access_by_perms = [ access_by_perms ] \n        for perm in access_by_perms : \n            if isinstance ( perm , six . string_types ) : \n                try : \n                    app , codename = perm . split ( '.' ) \n                except ValueError : \n                    raise ValueError ( 'Wrong permission string format: supplied - `%s`; ' 'expected - `<app_name>.<permission_name>`.' % perm ) \n                try : \n                    perm = Permission . objects . get ( codename = codename , content_type__app_label = app ) \n                except Permission . DoesNotExist : \n                    raise ValueError ( 'Permission `%s.%s` does not exist.' % ( app , codename ) ) \n            elif not isinstance ( perm , ( int , Permission ) ) : \n                raise ValueError ( 'Permissions must be given as strings, ints, or `Permission` instances.' ) \n            cleaned_permissions . append ( perm ) \n    item_obj . permissions = cleaned_permissions or [ ] \n    item_obj . access_perm_type = item_obj . PERM_TYPE_ALL if perms_mode_all else item_obj . PERM_TYPE_ANY \n    if item_obj . permissions : \n        item_obj . access_restricted = 1 \n    if children is not None : \n        for child in children : \n            child . parent = item_obj \n            item_obj . dynamic_children . append ( child ) \n    return item_obj "}
{"5020": "\ndef create_attrs_for_span ( sample_rate = 100.0 , trace_id = None , span_id = None , use_128bit_trace_id = 0 , ) : \n    if trace_id is None : \n        if use_128bit_trace_id : \n            trace_id = generate_random_128bit_string ( ) \n        else : \n            trace_id = generate_random_64bit_string ( ) \n    if span_id is None : \n        span_id = generate_random_64bit_string ( ) \n    if sample_rate == 0.0 : \n        is_sampled = 0 \n    else : \n        is_sampled = ( random . random ( ) * 100 ) < sample_rate \n    return ZipkinAttrs ( trace_id = trace_id , span_id = span_id , parent_span_id = None , flags = '0' , is_sampled = is_sampled , ) "}
{"5022": "\ndef _get_current_context ( self ) : \n    if self . _is_local_root_span : \n        if self . sample_rate is not None : \n            if self . zipkin_attrs_override and not self . zipkin_attrs_override . is_sampled : \n                return 1 , create_attrs_for_span ( sample_rate = self . sample_rate , trace_id = self . zipkin_attrs_override . trace_id , ) \n            elif not self . zipkin_attrs_override : \n                return 1 , create_attrs_for_span ( sample_rate = self . sample_rate , use_128bit_trace_id = self . use_128bit_trace_id , ) \n        if self . firehose_handler and not self . zipkin_attrs_override : \n            return 1 , create_attrs_for_span ( sample_rate = 0.0 , use_128bit_trace_id = self . use_128bit_trace_id , ) \n        return 0 , self . zipkin_attrs_override \n    else : \n        existing_zipkin_attrs = self . get_tracer ( ) . get_zipkin_attrs ( ) \n        if existing_zipkin_attrs : \n            return 0 , ZipkinAttrs ( trace_id = existing_zipkin_attrs . trace_id , span_id = generate_random_64bit_string ( ) , parent_span_id = existing_zipkin_attrs . span_id , flags = existing_zipkin_attrs . flags , is_sampled = existing_zipkin_attrs . is_sampled , ) \n    return 0 , None "}
{"5023": "\ndef start ( self ) : \n    self . do_pop_attrs = 0 \n    report_root_timestamp , self . zipkin_attrs = self . _get_current_context ( ) \n    if not self . zipkin_attrs : \n        return self \n    self . get_tracer ( ) . push_zipkin_attrs ( self . zipkin_attrs ) \n    self . do_pop_attrs = 1 \n    self . start_timestamp = time . time ( ) \n    if self . _is_local_root_span : \n        if not self . zipkin_attrs . is_sampled and not self . firehose_handler : \n            return self \n        if self . get_tracer ( ) . is_transport_configured ( ) : \n            log . info ( 'Transport was already configured, ignoring override' 'from span {}' . format ( self . span_name ) ) \n            return self \n        endpoint = create_endpoint ( self . port , self . service_name , self . host ) \n        self . logging_context = ZipkinLoggingContext ( self . zipkin_attrs , endpoint , self . span_name , self . transport_handler , report_root_timestamp or self . report_root_timestamp_override , self . get_tracer , self . service_name , binary_annotations = self . binary_annotations , add_logging_annotation = self . add_logging_annotation , client_context = self . kind == Kind . CLIENT , max_span_batch_size = self . max_span_batch_size , firehose_handler = self . firehose_handler , encoding = self . encoding , ) \n        self . logging_context . start ( ) \n        self . get_tracer ( ) . set_transport_configured ( configured = 1 ) \n    return self "}
{"5024": "\ndef stop ( self , _exc_type = None , _exc_value = None , _exc_traceback = None ) : \n    if self . do_pop_attrs : \n        self . get_tracer ( ) . pop_zipkin_attrs ( ) \n    if not self . get_tracer ( ) . is_transport_configured ( ) : \n        return \n    if any ( ( _exc_type , _exc_value , _exc_traceback ) ) : \n        error_msg = u'{0}: {1}' . format ( _exc_type . __name__ , _exc_value ) \n        self . update_binary_annotations ( { ERROR_KEY : error_msg , } ) \n    if self . logging_context : \n        try : \n            self . logging_context . stop ( ) \n        except Exception as ex : \n            err_msg = 'Error emitting zipkin trace. {}' . format ( repr ( ex ) , ) \n            log . error ( err_msg ) \n        finally : \n            self . logging_context = None \n            self . get_tracer ( ) . clear ( ) \n            self . get_tracer ( ) . set_transport_configured ( configured = 0 ) \n            return \n    end_timestamp = time . time ( ) \n    if self . duration : \n        duration = self . duration \n    else : \n        duration = end_timestamp - self . start_timestamp \n    endpoint = create_endpoint ( self . port , self . service_name , self . host ) \n    self . get_tracer ( ) . add_span ( Span ( trace_id = self . zipkin_attrs . trace_id , name = self . span_name , parent_id = self . zipkin_attrs . parent_span_id , span_id = self . zipkin_attrs . span_id , kind = self . kind , timestamp = self . timestamp if self . timestamp else self . start_timestamp , duration = duration , annotations = self . annotations , local_endpoint = endpoint , remote_endpoint = self . remote_endpoint , tags = self . binary_annotations , ) ) "}
{"5028": "\ndef create_endpoint ( port = None , service_name = None , host = None , use_defaults = 1 ) : \n    if use_defaults : \n        if port is None : \n            port = 0 \n        if service_name is None : \n            service_name = 'unknown' \n        if host is None : \n            try : \n                host = socket . gethostbyname ( socket . gethostname ( ) ) \n            except socket . gaierror : \n                host = '127.0.0.1' \n    ipv4 = None \n    ipv6 = None \n    if host : \n        try : \n            socket . inet_pton ( socket . AF_INET , host ) \n            ipv4 = host \n        except socket . error : \n            try : \n                socket . inet_pton ( socket . AF_INET6 , host ) \n                ipv6 = host \n            except socket . error : \n                pass \n    return Endpoint ( ipv4 = ipv4 , ipv6 = ipv6 , port = port , service_name = service_name , ) "}
{"5030": "\ndef build_v1_span ( self ) : \n    full_annotations = OrderedDict ( [ ( 'cs' , self . timestamp ) , ( 'sr' , self . timestamp ) , ( 'ss' , self . timestamp + self . duration ) , ( 'cr' , self . timestamp + self . duration ) , ] ) \n    if self . kind != Kind . LOCAL : \n        for ann in _DROP_ANNOTATIONS_BY_KIND [ self . kind ] : \n            del full_annotations [ ann ] \n    full_annotations . update ( self . annotations ) \n    return _V1Span ( trace_id = self . trace_id , name = self . name , parent_id = self . parent_id , id = self . span_id , timestamp = self . timestamp if self . shared is 0 else None , duration = self . duration if self . shared is 0 else None , endpoint = self . local_endpoint , annotations = full_annotations , binary_annotations = self . tags , remote_endpoint = self . remote_endpoint , ) "}
{"5065": "\ndef parse ( src , encoding = None ) : \n    def safe_is_file ( filename ) : \n        try : \n            return os . path . isfile ( src ) \n        except ValueError : \n            return 0 \n    if hasattr ( src , 'read' ) : \n        data = src . read ( ) \n    elif safe_is_file ( src ) : \n        with open ( src , 'rb' ) as fh : \n            data = fh . read ( ) \n    else : \n        data = src \n    if hasattr ( data , 'decode' ) : \n        exception = None \n        encodings = [ encoding , 'utf-8' , 'cp852' , 'iso8859-15' , 'latin1' ] \n        for encoding in encodings : \n            if not encoding : \n                continue \n            try : \n                data = data . decode ( encoding ) \n                break \n            except UnicodeDecodeError as e : \n                exception = e \n            except UnicodeEncodeError : \n                break \n        else : \n            raise exception \n    transactions = mt940 . models . Transactions ( ) \n    transactions . parse ( data ) \n    return transactions "}
{"5069": "\nasync def request ( self , method , url , ** kwargs ) : \n    rate_limiter = RateLimiter ( max_calls = 59 , period = 60 , callback = limited ) \n    async with rate_limiter : \n        if not self . token : \n            raise UnauthorizedDetected ( 'UnauthorizedDetected (status code: 401): No TOKEN provided' ) \n        headers = { 'User-Agent' : self . user_agent , 'Content-Type' : 'application/json' } \n        if 'json' in kwargs : \n            kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) \n        kwargs [ 'headers' ] = headers \n        headers [ 'Authorization' ] = self . token \n        for tries in range ( 5 ) : \n            async with self . session . request ( method , url , ** kwargs ) as resp : \n                log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) \n                data = await json_or_text ( resp ) \n                if 300 > resp . status >= 200 : \n                    return data \n                if resp . status == 429 : \n                    fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' \n                    retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) \n                    mins = retry_after / 60 \n                    log . warning ( fmt , retry_after , mins ) \n                    is_global = 1 \n                    if is_global : \n                        self . _global_over . clear ( ) \n                    await asyncio . sleep ( retry_after , loop = self . loop ) \n                    log . debug ( 'Done sleeping for the rate limit. Retrying...' ) \n                    if is_global : \n                        self . _global_over . set ( ) \n                        log . debug ( 'Global rate limit is now over.' ) \n                    continue \n                if resp . status == 400 : \n                    raise HTTPException ( resp , data ) \n                elif resp . status == 401 : \n                    raise Unauthorized ( resp , data ) \n                elif resp . status == 403 : \n                    raise Forbidden ( resp , data ) \n                elif resp . status == 404 : \n                    raise NotFound ( resp , data ) \n                else : \n                    raise HTTPException ( resp , data ) \n        raise HTTPException ( resp , data ) "}
{"5076": "\ndef encode ( term , compressed = 0 ) : \n    encoded_term = encode_term ( term ) \n    if compressed : \n        if compressed is 1 : \n            compressed = 6 \n        elif compressed < 0 or compressed > 9 : \n            raise ValueError ( \"invalid compression level: %r\" % ( compressed , ) ) \n        zlib_term = compress ( encoded_term , compressed ) \n        ln = len ( encoded_term ) \n        if len ( zlib_term ) + 5 <= ln : \n            return b\"\\x83P\" + _int4_pack ( ln ) + zlib_term \n    return b\"\\x83\" + encoded_term "}
{"5080": "\ndef stop ( self ) : \n    self . clearRemoteServices ( ) \n    self . clearLocalServices ( ) \n    self . _stopThreads ( ) \n    self . _serverStarted = 0 "}
{"5089": "\ndef from_serializable_data ( cls , data , check_fks = 1 , strict_fks = 0 ) : \n    obj = model_from_serializable_data ( cls , data , check_fks = check_fks , strict_fks = strict_fks ) \n    if obj is None : \n        return None \n    child_relations = get_all_child_relations ( cls ) \n    for rel in child_relations : \n        rel_name = rel . get_accessor_name ( ) \n        try : \n            child_data_list = data [ rel_name ] \n        except KeyError : \n            continue \n        related_model = rel . related_model \n        if hasattr ( related_model , 'from_serializable_data' ) : \n            children = [ related_model . from_serializable_data ( child_data , check_fks = check_fks , strict_fks = 1 ) for child_data in child_data_list ] \n        else : \n            children = [ model_from_serializable_data ( related_model , child_data , check_fks = check_fks , strict_fks = 1 ) for child_data in child_data_list ] \n        children = filter ( lambda child : child is not None , children ) \n        setattr ( obj , rel_name , children ) \n    return obj "}
{"5091": "\ndef has_changed ( self ) : \n    if self . formsets : \n        for formset in self . formsets . values ( ) : \n            for form in formset . forms : \n                if form . has_changed ( ) : \n                    return 1 \n    return bool ( self . changed_data ) "}
{"5095": "\ndef create_argument_parser ( self ) : \n    parser = ArgumentParser ( description = self . __doc__ , epilog = 'PyOTA v{version}' . format ( version = __version__ ) , ) \n    parser . add_argument ( '--uri' , type = text_type , default = 'http://localhost:14265/' , help = ( 'URI of the node to connect to ' '(defaults to http://localhost:14265/).' ) , ) \n    if self . requires_seed : \n        parser . add_argument ( '--seed-file' , type = text_type , dest = 'seed_file' , help = ( 'Path to a file containing your seed in cleartext. ' 'If not provided, you will be prompted to enter ' 'your seed via stdin.' ) , ) \n    parser . add_argument ( '--testnet' , action = 'store_true' , default = 0 , help = 'If set, use testnet settings (e.g., for PoW).' , ) \n    return parser "}
{"5115": "\ndef get_new_addresses ( self , index = 0 , count = 1 , security_level = AddressGenerator . DEFAULT_SECURITY_LEVEL , checksum = 0 , ) : \n    return extended . GetNewAddressesCommand ( self . adapter ) ( count = count , index = index , securityLevel = security_level , checksum = checksum , seed = self . seed , ) "}
{"5116": "\ndef get_transfers ( self , start = 0 , stop = None , inclusion_states = 0 ) : \n    return extended . GetTransfersCommand ( self . adapter ) ( seed = self . seed , start = start , stop = stop , inclusionStates = inclusion_states , ) "}
{"5129": "\ndef create_iterator ( self , start = 0 , step = 1 ) : \n    key_iterator = ( KeyGenerator ( self . seed ) . create_iterator ( start , step , self . security_level , ) ) \n    while 1 : \n        yield self . _generate_address ( key_iterator ) "}
{"5136": "\ndef get_codec_info ( cls ) : \n    codec = cls ( ) \n    codec_info = { 'encode' : codec . encode , 'decode' : codec . decode , } \n    if PY3 : \n        codec_info [ '_is_text_encoding' ] = 0 \n    return CodecInfo ( ** codec_info ) "}
{"5146": "\ndef as_tryte_strings ( self , head_to_tail = 0 ) : \n    transactions = self if head_to_tail else reversed ( self ) \n    return [ t . as_tryte_string ( ) for t in transactions ] "}
{"5148": "\ndef discover_commands ( package , recursively = 1 ) : \n    if isinstance ( package , string_types ) : \n        package = import_module ( package ) \n    commands = { } \n    for _ , name , is_package in walk_packages ( package . __path__ , package . __name__ + '.' ) : \n        sub_package = import_module ( name ) \n        for ( _ , obj ) in get_members ( sub_package ) : \n            if is_class ( obj ) and isinstance ( obj , CommandMeta ) : \n                command_name = getattr ( obj , 'command' ) \n                if command_name : \n                    commands [ command_name ] = obj \n        if recursively and is_package : \n            commands . update ( discover_commands ( sub_package ) ) \n    return commands "}
{"5150": "\ndef _apply_filter ( value , filter_ , failure_message ) : \n    if filter_ : \n        runner = f . FilterRunner ( filter_ , value ) \n        if runner . is_valid ( ) : \n            return runner . cleaned_data \n        else : \n            raise with_context ( exc = ValueError ( '{message} ({error_codes}) ' '(`exc.context[\"filter_errors\"]` ' 'contains more information).' . format ( message = failure_message , error_codes = runner . error_codes , ) , ) , context = { 'filter_errors' : runner . get_errors ( with_context = 1 ) , } , ) \n    return value "}
{"5154": "\ndef _create_validator ( self ) : \n    grouped_transactions = self . bundle . group_transactions ( ) \n    bundle_hash = self . bundle . hash \n    last_index = len ( self . bundle ) - 1 \n    balance = 0 \n    counter = 0 \n    for group in grouped_transactions : \n        for txn in group : \n            balance += txn . value \n            if txn . bundle_hash != bundle_hash : \n                yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) \n            if txn . current_index != counter : \n                yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current_index , i = counter , ) ) \n            if txn . last_index != last_index : \n                yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last_index , expected = last_index , i = counter , ) ) \n            counter += 1 \n    if balance != 0 : \n        yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) \n    if not self . _errors : \n        signature_validation_queue = [ ] \n        for group in grouped_transactions : \n            if group [ 0 ] . value >= 0 : \n                continue \n            validate_group_signature = 1 \n            for j , txn in enumerate ( group ) : \n                if ( j > 0 ) and ( txn . value != 0 ) : \n                    yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current_index , ) ) \n                    validate_group_signature = 0 \n                    continue \n            if validate_group_signature : \n                signature_validation_queue . append ( group ) \n        if signature_validation_queue : \n            for error in self . _get_bundle_signature_errors ( signature_validation_queue ) : \n                yield error "}
{"5170": "\ndef finalize ( self ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not self : \n        raise ValueError ( 'Bundle has no transactions.' ) \n    balance = self . balance \n    if balance < 0 : \n        if self . change_address : \n            self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) \n        else : \n            raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) \n    elif balance > 0 : \n        raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) \n    while 1 : \n        sponge = Kerl ( ) \n        last_index = len ( self ) - 1 \n        for i , txn in enumerate ( self ) : \n            txn . current_index = i \n            txn . last_index = last_index \n            sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) \n        bundle_hash_trits = [ 0 ] * HASH_LENGTH \n        sponge . squeeze ( bundle_hash_trits ) \n        bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) \n        if any ( 13 in part for part in normalize ( bundle_hash ) ) : \n            tail_transaction = ( self . tail_transaction ) \n            tail_transaction . increment_legacy_tag ( ) \n        else : \n            break \n    for txn in self : \n        txn . bundle_hash = bundle_hash \n        txn . signature_message_fragment = Fragment ( txn . message or b'' ) "}
{"5178": "\ndef find_word_groups ( self , text , category , proximity = 2 ) : \n    f = re . IGNORECASE \n    words = getattr ( self , category ) \n    regex = re . compile ( r'(\\b' + r'\\b|\\b' . join ( words ) + r'\\b)' , flags = f ) \n    candidates = regex . finditer ( text ) \n    starts , ends = [ ] , [ ] \n    groups = [ ] \n    for item in candidates : \n        starts . append ( item . span ( ) [ 0 ] ) \n        ends . append ( item . span ( ) [ 1 ] ) \n        groups . append ( item . group ( ) . lower ( ) ) \n    new_starts = [ ] \n    new_groups = [ ] \n    skip = 0 \n    for i , g in enumerate ( groups ) : \n        if skip : \n            skip = 0 \n            continue \n        if ( i < len ( groups ) - 1 ) and ( starts [ i + 1 ] - ends [ i ] <= proximity ) : \n            if g [ - 1 ] == '-' : \n                sep = '' \n            else : \n                sep = ' ' \n            new_groups . append ( g + sep + groups [ i + 1 ] ) \n            new_starts . append ( starts [ i ] ) \n            skip = 1 \n        else : \n            if g not in new_groups : \n                new_groups . append ( g ) \n                new_starts . append ( starts [ i ] ) \n            skip = 0 \n    return new_groups "}
{"5184": "\ndef plot ( self , fmt = None , fig = None , ax = None ) : \n    u = 4 \n    v = 0.25 \n    r = None \n    if ( fig is None ) and ( ax is None ) : \n        fig = plt . figure ( figsize = ( u , 1 ) ) \n    else : \n        r = fig \n    if ax is None : \n        ax = fig . add_axes ( [ 0.1 * v , 0.1 , 0.8 * v , 0.8 ] ) \n    else : \n        r = ax \n    rect1 = patches . Rectangle ( ( 0 , 0 ) , u * v , u * v , color = self . colour , lw = 1 , hatch = self . hatch , ec = 'k' ) \n    ax . add_patch ( rect1 ) \n    ax . text ( 1.0 + 0.1 * v * u , u * v * 0.5 , self . component . summary ( fmt = fmt ) , fontsize = max ( u , 15 ) , verticalalignment = 'center' , horizontalalignment = 'left' ) \n    ax . set_xlim ( [ 0 , u * v ] ) \n    ax . set_ylim ( [ 0 , u * v ] ) \n    ax . get_xaxis ( ) . set_visible ( 0 ) \n    ax . get_yaxis ( ) . set_visible ( 0 ) \n    ax . invert_yaxis ( ) \n    return r "}
{"5187": "\ndef random ( cls , components , width = 0 , colour = None ) : \n    try : \n        list_of_Decors = [ Decor . random ( c ) for c in [ i [ 0 ] for i in components . unique if i [ 0 ] ] ] \n    except : \n        try : \n            list_of_Decors = [ Decor . random ( c ) for c in components . copy ( ) ] \n        except : \n            list_of_Decors = [ Decor . random ( components ) ] \n    if colour is not None : \n        for d in list_of_Decors : \n            d . colour = colour \n    if width : \n        for i , d in enumerate ( list_of_Decors ) : \n            d . width = i + 1 \n    return cls ( list_of_Decors ) "}
{"5189": "\ndef from_csv ( cls , filename = None , text = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise LegendError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    r = csv . DictReader ( f , skipinitialspace = 1 ) \n    list_of_Decors , components = [ ] , [ ] \n    kind = 'component' \n    for row in r : \n        d , component = { } , { } \n        for ( k , v ) in row . items ( ) : \n            if ( k in [ None , '' ] ) : \n                continue \n            if ( v in [ None , '' ] ) : \n                if k . lower ( ) not in [ 'color' , 'colour' ] : \n                    continue \n            if k [ : 4 ] . lower ( ) == 'comp' : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                if v . lower ( ) == 'true' : \n                    component [ prop ] = 1 \n                elif v . lower ( ) == 'false' : \n                    component [ prop ] = 0 \n                else : \n                    try : \n                        component [ prop ] = float ( v ) \n                    except ValueError : \n                        component [ prop ] = v . lower ( ) \n            elif k [ : 5 ] . lower ( ) == 'curve' : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                component [ prop ] = v . lower ( ) \n                kind = 'curve' \n            else : \n                try : \n                    d [ k ] = float ( v ) \n                except ValueError : \n                    d [ k ] = v . lower ( ) \n        this_component = Component ( component ) \n        d [ kind ] = this_component \n        if this_component in components : \n            with warnings . catch_warnings ( ) : \n                warnings . simplefilter ( \"always\" ) \n                w = \"This legend contains duplicate components.\" \n                warnings . warn ( w ) \n        components . append ( this_component ) \n        list_of_Decors . append ( Decor ( d ) ) \n    return cls ( list_of_Decors ) "}
{"5190": "\ndef to_csv ( self ) : \n    header = [ ] \n    component_header = [ ] \n    for row in self : \n        for j in row . __dict__ . keys ( ) : \n            if j == '_colour' : \n                j = 'colour' \n            header . append ( j ) \n        for k in row . component . __dict__ . keys ( ) : \n            component_header . append ( k ) \n    header = set ( header ) \n    component_header = set ( component_header ) \n    header . remove ( 'component' ) \n    header_row = '' \n    if 'colour' in header : \n        header_row += 'colour,' \n        header . remove ( 'colour' ) \n        has_colour = 1 \n    for item in header : \n        header_row += item + ',' \n    for item in component_header : \n        header_row += 'component ' + item + ',' \n    result = header_row . strip ( ',' ) + '\\n' \n    for row in self : \n        if has_colour : \n            result += row . __dict__ . get ( '_colour' , '' ) + ',' \n        for item in header : \n            result += str ( row . __dict__ . get ( item , '' ) ) + ',' \n        for item in component_header : \n            result += str ( row . component . __dict__ . get ( item , '' ) ) + ',' \n        result += '\\n' \n    return result "}
{"5196": "\ndef from_text ( cls , text , lexicon , required = None , first_only = 1 ) : \n    component = lexicon . get_component ( text , first_only = first_only ) \n    if required and ( required not in component ) : \n        return None \n    else : \n        return cls ( component ) "}
{"5197": "\ndef summary ( self , fmt = None , initial = 1 , default = '' ) : \n    if default and not self . __dict__ : \n        return default \n    if fmt == '' : \n        return default \n    keys = [ k for k , v in self . __dict__ . items ( ) if v is not '' ] \n    f = fmt or '{' + '}, {' . join ( keys ) + '}' \n    try : \n        summary = CustomFormatter ( ) . format ( f , ** self . __dict__ ) \n    except KeyError as e : \n        raise ComponentError ( \"Error building summary, \" + str ( e ) ) \n    if summary and initial and not fmt : \n        summary = summary [ 0 ] . upper ( ) + summary [ 1 : ] \n    return summary "}
{"5202": "\ndef unique ( self ) : \n    all_rx = set ( [ iv . primary for iv in self ] ) \n    table = { r : 0 for r in all_rx } \n    for iv in self : \n        table [ iv . primary ] += iv . thickness \n    return sorted ( table . items ( ) , key = operator . itemgetter ( 1 ) , reverse = 1 ) "}
{"5203": "\ndef __intervals_from_tops ( self , tops , values , basis , components , field = None , ignore_nan = 1 ) : \n    length = float ( basis . size ) \n    start , stop = basis [ 0 ] , basis [ - 1 ] \n    tops = [ start + ( p / ( length - 1 ) ) * ( stop - start ) for p in tops ] \n    bases = tops [ 1 : ] + [ stop ] \n    list_of_Intervals = [ ] \n    for i , t in enumerate ( tops ) : \n        v , c , d = values [ i ] , [ ] , { } \n        if ignore_nan and np . isnan ( v ) : \n            continue \n        if ( field is not None ) : \n            d = { field : v } \n        if components is not None : \n            try : \n                c = [ deepcopy ( components [ int ( v ) ] ) ] \n            except IndexError : \n                c = [ ] \n            if c and ( c [ 0 ] is None ) : \n                c = [ ] \n        interval = Interval ( t , bases [ i ] , data = d , components = c ) \n        list_of_Intervals . append ( interval ) \n    return list_of_Intervals "}
{"5205": "\ndef from_petrel ( cls , filename , stop = None , points = 0 , null = None , function = None , include = None , exclude = None , remap = None , ignore = None ) : \n    result = utils . read_petrel ( filename , function = function , remap = remap , ) \n    data = cls . _clean_longitudinal_data ( result , null = null ) \n    list_of_Intervals = cls . _build_list_of_Intervals ( data , stop = stop , points = points , include = include , exclude = exclude , ignore = ignore ) \n    if list_of_Intervals : \n        return cls ( list_of_Intervals ) \n    return None "}
{"5206": "\ndef _build_list_of_Intervals ( cls , data_dict , stop = None , points = 0 , include = None , exclude = None , ignore = None , lexicon = None ) : \n    include = include or { } \n    exclude = exclude or { } \n    ignore = ignore or [ ] \n    all_data = [ ] \n    for data in zip ( * data_dict . values ( ) ) : \n        all_data . append ( { k : v for k , v in zip ( data_dict . keys ( ) , data ) } ) \n    all_data = sorted ( all_data , key = lambda x : x [ 'top' ] ) \n    wanted_data = [ ] \n    for dictionary in all_data : \n        keep = 1 \n        delete = [ ] \n        for k , v in dictionary . items ( ) : \n            incl = include . get ( k , utils . null_default ( 1 ) ) \n            excl = exclude . get ( k , utils . null_default ( 0 ) ) \n            if k in ignore : \n                delete . append ( k ) \n            if not incl ( v ) : \n                keep = 0 \n            if excl ( v ) : \n                keep = 0 \n        if delete : \n            for key in delete : \n                _ = dictionary . pop ( key , None ) \n        if keep : \n            wanted_data . append ( dictionary ) \n    if not points : \n        for i , iv in enumerate ( wanted_data ) : \n            if iv . get ( 'base' , None ) is None : \n                try : \n                    iv [ 'base' ] = wanted_data [ i + 1 ] [ 'top' ] \n                except ( IndexError , KeyError ) : \n                    if stop is not None : \n                        thick = stop - iv [ 'top' ] \n                    else : \n                        thick = 1 \n                    iv [ 'base' ] = iv [ 'top' ] + thick \n    list_of_Intervals = [ ] \n    for iv in wanted_data : \n        top = iv . pop ( 'top' ) \n        base = iv . pop ( 'base' , None ) \n        descr = iv . pop ( 'description' , '' ) \n        if iv : \n            c , d = { } , { } \n            for k , v in iv . items ( ) : \n                if ( k [ : 5 ] . lower ( ) == 'comp ' ) or ( k [ : 9 ] . lower ( ) == 'component' ) : \n                    k = re . sub ( r'comp(?:onent)? ' , '' , k , flags = re . I ) \n                    c [ k ] = v \n                else : \n                    if v is not None : \n                        d [ k ] = v \n            comp = [ Component ( c ) ] if c else None \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'data' : d , 'components' : comp } ) \n        else : \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'lexicon' : lexicon } ) \n        list_of_Intervals . append ( this ) \n    return list_of_Intervals "}
{"5207": "\ndef from_csv ( cls , filename = None , text = None , dlm = ',' , lexicon = None , points = 0 , include = None , exclude = None , remap = None , function = None , null = None , ignore = None , source = None , stop = None , fieldnames = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise StriplogError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        if source is None : \n            source = filename \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    source = source or 'CSV' \n    if dlm == ' ' : \n        text = re . sub ( r'[ \\t]+' , ' ' , text ) \n    if fieldnames is not None : \n        text = dlm . join ( fieldnames ) + '\\n' + text \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    reader = csv . DictReader ( f , delimiter = dlm ) \n    reorg = { k . strip ( ) . lower ( ) : [ ] for k in reader . fieldnames if k is not None } \n    t = f . tell ( ) \n    for key in reorg : \n        f . seek ( t ) \n        for r in reader : \n            s = { k . strip ( ) . lower ( ) : v . strip ( ) for k , v in r . items ( ) } \n            try : \n                reorg [ key ] . append ( float ( s [ key ] ) ) \n            except ValueError : \n                reorg [ key ] . append ( s [ key ] ) \n    f . close ( ) \n    remap = remap or { } \n    for k , v in remap . items ( ) : \n        reorg [ v ] = reorg . pop ( k ) \n    data = cls . _clean_longitudinal_data ( reorg , null = null ) \n    list_of_Intervals = cls . _build_list_of_Intervals ( data , points = points , lexicon = lexicon , include = include , exclude = exclude , ignore = ignore , stop = stop ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5209": "\ndef from_log ( cls , log , cutoff = None , components = None , legend = None , legend_field = None , field = None , right = 0 , basis = None , source = 'Log' ) : \n    if ( components is None ) and ( legend is None ) and ( field is None ) : \n        m = 'You must provide a list of components, and legend, or a field.' \n        raise StriplogError ( m ) \n    if ( legend is not None ) and ( legend_field is None ) : \n        try : \n            components = [ deepcopy ( decor . component ) for decor in legend ] \n        except AttributeError : \n            pass \n    if legend_field is not None : \n        field_values = [ getattr ( d , legend_field , 0 ) for d in legend ] \n        components = [ Component ( ) for i in range ( int ( max ( field_values ) + 1 ) ) ] \n        for i , decor in enumerate ( legend ) : \n            components [ i ] = deepcopy ( decor . component ) \n    if cutoff is not None : \n        try : \n            n = len ( cutoff ) \n        except TypeError : \n            n = 1 \n        if len ( components ) < n + 1 : \n            m = 'For n cutoffs, you need to provide at least' \n            m += 'n+1 components.' \n            raise StriplogError ( m ) \n        try : \n            a = np . digitize ( log , cutoff , right ) \n        except ValueError : \n            a = np . digitize ( log , [ cutoff ] , right ) \n    else : \n        a = np . copy ( log ) \n    tops , values = utils . tops_from_loglike ( a ) \n    if basis is None : \n        m = 'You must provide a depth or elevation basis.' \n        raise StriplogError ( m ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components , field = field ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5210": "\ndef from_las3 ( cls , string , lexicon = None , source = \"LAS\" , dlm = ',' , abbreviations = 0 ) : \n    f = re . DOTALL | re . IGNORECASE \n    regex = r'\\~\\w+?_Data.+?\\n(.+?)(?:\\n\\n+|\\n*\\~|\\n*$)' \n    pattern = re . compile ( regex , flags = f ) \n    text = pattern . search ( string ) . group ( 1 ) \n    s = re . search ( r'\\.(.+?)\\: ?.+?source' , string ) \n    if s : \n        source = s . group ( 1 ) . strip ( ) \n    return cls . from_descriptions ( text , lexicon , source = source , dlm = dlm , abbreviations = abbreviations ) "}
{"5213": "\ndef to_csv ( self , filename = None , as_text = 1 , use_descriptions = 0 , dlm = \",\" , header = 1 ) : \n    if ( filename is None ) : \n        if ( not as_text ) : \n            raise StriplogError ( \"You must provide a filename or set as_text to True.\" ) \n    else : \n        as_text = 0 \n    if as_text : \n        output = StringIO ( ) \n    else : \n        output = open ( filename , 'w' ) \n    fieldnames = [ 'Top' , 'Base' , 'Component' ] \n    writer = csv . DictWriter ( output , delimiter = dlm , fieldnames = fieldnames , quoting = csv . QUOTE_MINIMAL ) \n    if header : \n        writer . writeheader ( ) \n    for i in self . __list : \n        if use_descriptions and i . description : \n            text = i . description \n        elif i . primary : \n            text = i . primary . summary ( ) \n        else : \n            text = '' \n        data = { j : k for j , k in zip ( fieldnames , [ i . top . z , i . base . z , text ] ) } \n        writer . writerow ( data ) \n    if as_text : \n        return output . getvalue ( ) \n    else : \n        output . close \n        return None "}
{"5214": "\ndef to_las3 ( self , use_descriptions = 0 , dlm = \",\" , source = \"Striplog\" ) : \n    data = self . to_csv ( use_descriptions = use_descriptions , dlm = dlm , header = 0 ) \n    return templates . section . format ( name = 'Lithology' , short = \"LITH\" , source = source , data = data ) "}
{"5215": "\ndef plot_axis ( self , ax , legend , ladder = 0 , default_width = 1 , match_only = None , colour = None , colour_function = None , cmap = None , default = None , width_field = None , ** kwargs ) : \n    default_c = None \n    patches = [ ] \n    for iv in self . __list : \n        origin = ( 0 , iv . top . z ) \n        d = legend . get_decor ( iv . primary , match_only = match_only ) \n        thick = iv . base . z - iv . top . z \n        if ladder : \n            if width_field is not None : \n                w = iv . data . get ( width_field , 1 ) \n                w = default_width * w / self . max_field ( width_field ) \n                default_c = 'gray' \n            elif legend is not None : \n                w = d . width or default_width \n                try : \n                    w = default_width * w / legend . max_width \n                except : \n                    w = default_width \n        else : \n            w = default_width \n        this_patch_kwargs = kwargs . copy ( ) \n        lw = this_patch_kwargs . pop ( 'lw' , 0 ) \n        ec = this_patch_kwargs . pop ( 'ec' , 'k' ) \n        fc = this_patch_kwargs . pop ( 'fc' , None ) or default_c or d . colour \n        if colour is None : \n            rect = mpl . patches . Rectangle ( origin , w , thick , fc = fc , lw = lw , hatch = d . hatch , ec = ec , ** this_patch_kwargs ) \n            ax . add_patch ( rect ) \n        else : \n            rect = mpl . patches . Rectangle ( origin , w , thick , lw = lw , ec = ec , ** this_patch_kwargs ) \n            patches . append ( rect ) \n    if colour is not None : \n        cmap = cmap or 'viridis' \n        p = mpl . collections . PatchCollection ( patches , cmap = cmap , lw = lw ) \n        p . set_array ( self . get_data ( colour , colour_function , default = default ) ) \n        ax . add_collection ( p ) \n        cb = plt . colorbar ( p ) \n        cb . outline . set_linewidth ( 0 ) \n    return ax "}
{"5217": "\ndef extract ( self , log , basis , name , function = None ) : \n    intervals = { } \n    previous_ix = - 1 \n    for i , z in enumerate ( basis ) : \n        ix = self . read_at ( z , index = 1 ) \n        if ix is None : \n            continue \n        if ix == previous_ix : \n            intervals [ ix ] . append ( log [ i ] ) \n        else : \n            intervals [ ix ] = [ log [ i ] ] \n        previous_ix = ix \n    for ix , data in intervals . items ( ) : \n        f = function or utils . null \n        d = f ( np . array ( data ) ) \n        self [ ix ] . data [ name ] = d \n    return None "}
{"5218": "\ndef find ( self , search_term , index = 0 ) : \n    hits = [ ] \n    for i , iv in enumerate ( self ) : \n        try : \n            search_text = iv . description or iv . primary . summary ( ) \n            pattern = re . compile ( search_term , flags = re . IGNORECASE ) \n            if pattern . search ( search_text ) : \n                hits . append ( i ) \n        except TypeError : \n            if search_term in iv . components : \n                hits . append ( i ) \n    if hits and index : \n        return hits \n    elif hits : \n        return self [ hits ] \n    else : \n        return "}
{"5219": "\ndef find_overlaps ( self , index = 0 ) : \n    return self . __find_incongruities ( op = operator . gt , index = index ) "}
{"5220": "\ndef find_gaps ( self , index = 0 ) : \n    return self . __find_incongruities ( op = operator . lt , index = index ) "}
{"5221": "\ndef prune ( self , limit = None , n = None , percentile = None , keep_ends = 0 ) : \n    strip = self . copy ( ) \n    if not ( limit or n or percentile ) : \n        m = \"You must provide a limit or n or percentile for pruning.\" \n        raise StriplogError ( m ) \n    if limit : \n        prune = [ i for i , iv in enumerate ( strip ) if iv . thickness < limit ] \n    if n : \n        prune = strip . thinnest ( n = n , index = 1 ) \n    if percentile : \n        n = np . floor ( len ( strip ) * percentile / 100 ) \n        prune = strip . thinnest ( n = n , index = 1 ) \n    if keep_ends : \n        first , last = 0 , len ( strip ) - 1 \n        if first in prune : \n            prune . remove ( first ) \n        if last in prune : \n            prune . remove ( last ) \n    del strip [ prune ] \n    return strip "}
{"5222": "\ndef anneal ( self ) : \n    strip = self . copy ( ) \n    gaps = strip . find_gaps ( index = 1 ) \n    if not gaps : \n        return \n    for gap in gaps : \n        before = strip [ gap ] \n        after = strip [ gap + 1 ] \n        if strip . order == 'depth' : \n            t = ( after . top . z - before . base . z ) / 2 \n            before . base = before . base . z + t \n            after . top = after . top . z - t \n        else : \n            t = ( after . base - before . top ) / 2 \n            before . top = before . top . z + t \n            after . base = after . base . z - t \n    return strip "}
{"5226": "\ndef merge_overlaps ( self ) : \n    overlaps = np . array ( self . find_overlaps ( index = 1 ) ) \n    if not overlaps . any ( ) : \n        return \n    for overlap in overlaps : \n        before = self [ overlap ] . copy ( ) \n        after = self [ overlap + 1 ] . copy ( ) \n        del self [ overlap ] \n        del self [ overlap ] \n        new_segment = before . merge ( after ) \n        self . __insert ( overlap , new_segment ) \n        overlaps += 1 \n    return "}
{"5227": "\ndef hist ( self , lumping = None , summary = 0 , sort = 1 , plot = 1 , legend = None , ax = None ) : \n    comps = [ ] \n    labels = [ ] \n    entries = defaultdict ( int ) \n    for i in self : \n        if lumping : \n            k = i . primary [ lumping ] \n        else : \n            if summary : \n                k = i . primary . summary ( ) \n            else : \n                k = i . primary \n        comps . append ( i . primary ) \n        labels . append ( i . primary . summary ( ) ) \n        entries [ k ] += i . thickness \n    if sort : \n        allitems = sorted ( entries . items ( ) , key = lambda i : i [ 1 ] , reverse = 1 ) \n        ents , counts = zip ( * allitems ) \n    else : \n        ents , counts = tuple ( entries . keys ( ) ) , tuple ( entries . values ( ) ) \n    if plot : \n        if ax is None : \n            fig , ax = plt . subplots ( ) \n            return_ax = 0 \n        else : \n            return_ax = 1 \n        ind = np . arange ( len ( ents ) ) \n        bars = ax . bar ( ind , counts , align = 'center' ) \n        ax . set_xticks ( ind ) \n        ax . set_xticklabels ( labels ) \n        if legend : \n            colours = [ legend . get_colour ( c ) for c in comps ] \n            for b , c in zip ( bars , colours ) : \n                b . set_color ( c ) \n        ax . set_ylabel ( 'Thickness [m]' ) \n    else : \n        bars = [ ] \n    if plot and return_ax : \n        return counts , ents , ax \n    return counts , ents , bars "}
{"5228": "\ndef invert ( self , copy = 0 ) : \n    if copy : \n        return Striplog ( [ i . invert ( copy = 1 ) for i in self ] ) \n    else : \n        for i in self : \n            i . invert ( ) \n        self . __sort ( ) \n        o = self . order \n        self . order = { 'depth' : 'elevation' , 'elevation' : 'depth' } [ o ] \n        return "}
{"5229": "\ndef crop ( self , extent , copy = 0 ) : \n    try : \n        if extent [ 0 ] is None : \n            extent = ( self . start . z , extent [ 1 ] ) \n        if extent [ 1 ] is None : \n            extent = ( extent [ 0 ] , self . stop . z ) \n    except : \n        m = \"You must provide a 2-tuple for the new extents. Use None for\" \n        m += \" the existing start or stop.\" \n        raise StriplogError ( m ) \n    first_ix = self . read_at ( extent [ 0 ] , index = 1 ) \n    last_ix = self . read_at ( extent [ 1 ] , index = 1 ) \n    first = self [ first_ix ] . split_at ( extent [ 0 ] ) [ 1 ] \n    last = self [ last_ix ] . split_at ( extent [ 1 ] ) [ 0 ] \n    new_list = self . __list [ first_ix : last_ix + 1 ] . copy ( ) \n    new_list [ 0 ] = first \n    new_list [ - 1 ] = last \n    if copy : \n        return Striplog ( new_list ) \n    else : \n        self . __list = new_list \n        return "}
{"5235": "\ndef parse ( self , hcl , canonicalize = 0 ) : \n    return self . request ( \"parse\" , json = { \"JobHCL\" : hcl , \"Canonicalize\" : canonicalize } , method = \"post\" , allow_redirects = 1 ) . json ( ) "}
{"5241": "\ndef drain_node ( self , id , enable = 0 ) : \n    return self . request ( id , \"drain\" , params = { \"enable\" : enable } , method = \"post\" ) . json ( ) "}
{"5254": "\ndef plan_job ( self , id , job , diff = 0 , policy_override = 0 ) : \n    json_dict = { } \n    json_dict . update ( job ) \n    json_dict . setdefault ( 'Diff' , diff ) \n    json_dict . setdefault ( 'PolicyOverride' , policy_override ) \n    return self . request ( id , \"plan\" , json = json_dict , method = \"post\" ) . json ( ) "}
{"5259": "\ndef get_configuration ( self , stale = 0 ) : \n    params = { \"stale\" : stale } \n    return self . request ( \"raft\" , \"configuration\" , params = params , method = \"get\" ) . json ( ) "}
{"5260": "\ndef delete_peer ( self , peer_address , stale = 0 ) : \n    params = { \"address\" : peer_address , \"stale\" : stale } \n    return self . request ( \"raft\" , \"peer\" , params = params , method = \"delete\" ) . ok "}
{"5272": "\ndef spawn ( self , cmd , stdin_content = \"\" , stdin = 0 , shell = 0 , timeout = 2 ) : \n    try : \n        if type ( cmd ) != list : \n            raise PJFInvalidType ( type ( cmd ) , list ) \n        if type ( stdin_content ) != str : \n            raise PJFInvalidType ( type ( stdin_content ) , str ) \n        if type ( stdin ) != bool : \n            raise PJFInvalidType ( type ( stdin ) , bool ) \n        self . _in = stdin_content \n        try : \n            self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) \n            self . finish_read ( timeout , stdin_content , stdin ) \n            if self . process . poll ( ) is not None : \n                self . close ( ) \n        except KeyboardInterrupt : \n            return \n    except OSError : \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmd [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5274": "\ndef finish_read ( self , timeout = 2 , stdin_content = \"\" , stdin = 0 ) : \n    process = Thread ( target = self . get_output , args = ( stdin_content , stdin ) ) \n    process . start ( ) \n    if timeout > 0 : \n        process . join ( timeout ) \n    else : \n        process . join ( ) \n    if process . is_alive ( ) : \n        self . close ( ) \n        self . return_code = - signal . SIGHUP \n    else : \n        self . return_code = self . process . returncode "}
{"5276": "\ndef start ( self ) : \n    from . pjf_worker import PJFWorker \n    worker = PJFWorker ( self ) \n    if self . update_pjf : \n        worker . update_library ( ) \n    elif self . browser_auto : \n        worker . browser_autopwn ( ) \n    elif self . fuzz_web : \n        worker . web_fuzzer ( ) \n    elif self . json : \n        if not self . web_server and not self . ext_fuzz and not self . cmd_fuzz : \n            worker . fuzz ( ) \n        elif self . ext_fuzz : \n            if self . stdin : \n                worker . fuzz_stdin ( ) \n            else : \n                worker . fuzz_command_line ( ) \n        elif self . cmd_fuzz : \n            if self . stdin : \n                worker . fuzz_external ( 1 ) \n            else : \n                worker . fuzz_external ( ) \n        else : \n            worker . start_http_server ( ) \n    elif self . json_file : \n        worker . start_file_fuzz ( ) \n    elif self . process_to_monitor : \n        worker . start_process_monitor ( ) "}
{"5277": "\ndef execute ( self , obj ) : \n    try : \n        if self . config . stdin : \n            self . spawn ( self . config . command , stdin_content = obj , stdin = 1 , timeout = 1 ) \n        else : \n            if \"@@\" not in self . config . command : \n                raise PJFMissingArgument ( \"Missing @@ filename indicator while using non-stdin fuzzing method\" ) \n            for x in self . config . command : \n                if \"@@\" in x : \n                    self . config . command [ self . config . command . index ( x ) ] = x . replace ( \"@@\" , obj ) \n            self . spawn ( self . config . command , timeout = 2 ) \n        self . logger . debug ( \"[{0}] - PJFExternalFuzzer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) \n        return self . _out \n    except KeyboardInterrupt : \n        return \"\" \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = 1 ) : \n            if type ( d ) == dict : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = 0 ) \n                    elif type ( d [ k ] ) == str : \n                        if _decode : \n                            d [ k ] = decode ( d [ k ] ) \n                        else : \n                            d [ k ] = encode ( d [ k ] ) \n            elif type ( d ) == list : \n                arr = [ ] \n                for e in d : \n                    if type ( e ) == str : \n                        if _decode : \n                            arr . append ( decode ( e ) ) \n                        else : \n                            arr . append ( encode ( e ) ) \n                    elif type ( e ) in [ dict , list ] : \n                        if _decode : \n                            arr . append ( encode_decode_all ( e ) ) \n                        else : \n                            arr . append ( encode_decode_all ( e , _decode = 0 ) ) \n                    else : \n                        arr . append ( e ) \n                return arr \n            else : \n                if _decode : \n                    return decode ( d ) \n                else : \n                    return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if sys . version_info >= ( 3 , 0 ) : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if sys . version_info >= ( 3 , 0 ) : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5 ) ) , _decode = 0 ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = 0 ) \n    return func_wrapper "}
{"5279": "\ndef build ( self , pre = None , shortest = 0 ) : \n    if pre is None : \n        pre = [ ] \n    if self . value is not None and rand . maybe ( ) : \n        return utils . val ( self . value , pre , shortest = shortest ) \n    length = super ( String , self ) . build ( pre , shortest = shortest ) \n    res = rand . data ( length , self . charset ) \n    return res "}
{"5280": "\ndef build ( self , pre = None , shortest = 0 ) : \n    if pre is None : \n        pre = [ ] \n    res = deque ( ) \n    for x in self . values : \n        try : \n            res . append ( utils . val ( x , pre , shortest = shortest ) ) \n        except errors . OptGram as e : \n            continue \n        except errors . FlushGrams as e : \n            prev = \"\" . join ( res ) \n            res . clear ( ) \n            if len ( self . fuzzer . _scope_stack ) == 1 : \n                pre . append ( prev ) \n            else : \n                stmts = self . fuzzer . _curr_scope . setdefault ( \"prev_append\" , deque ( ) ) \n                stmts . extend ( pre ) \n                stmts . append ( prev ) \n                pre . clear ( ) \n            continue \n    return self . sep . join ( res ) "}
{"5281": "\ndef build ( self , pre = None , shortest = 0 ) : \n    res = super ( Q , self ) . build ( pre , shortest = shortest ) \n    if self . escape : \n        return repr ( res ) \n    elif self . html_js_escape : \n        return ( \"'\" + res . encode ( \"string_escape\" ) . replace ( \"<\" , \"\\\\x3c\" ) . replace ( \">\" , \"\\\\x3e\" ) + \"'\" ) \n    else : \n        return \"{q}{r}{q}\" . format ( q = self . quote , r = res ) "}
{"5282": "\ndef build ( self , pre = None , shortest = 0 ) : \n    if pre is None : \n        pre = [ ] \n    if shortest and self . shortest_vals is not None : \n        return utils . val ( rand . choice ( self . shortest_vals ) , pre , shortest = shortest ) \n    else : \n        return utils . val ( rand . choice ( self . values ) , pre , shortest = shortest ) "}
{"5283": "\ndef build ( self , pre = None , shortest = 0 ) : \n    if pre is None : \n        pre = [ ] \n    if shortest or rand . maybe ( self . prob ) : \n        raise errors . OptGram \n    return super ( Opt , self ) . build ( pre , shortest = shortest ) "}
{"5284": "\ndef build ( self , pre = None , shortest = 0 ) : \n    global REF_LEVEL \n    REF_LEVEL += 1 \n    try : \n        if pre is None : \n            pre = [ ] \n        definition = self . fuzzer . get_ref ( self . cat , self . refname ) \n        res = utils . val ( definition , pre , shortest = ( shortest or REF_LEVEL >= self . max_recursion ) ) \n        return res \n    finally : \n        REF_LEVEL -= 1 "}
{"5285": "\ndef build ( self , pre = None , shortest = 0 ) : \n    if pre is None : \n        pre = [ ] \n    if shortest : \n        raise errors . OptGram \n    elif rand . maybe ( ) : \n        return super ( STAR , self ) . build ( pre , shortest = shortest ) \n    else : \n        raise errors . OptGram "}
{"5286": "\ndef shutdown ( self , * args ) : \n    try : \n        self . _shutdown ( ) \n        if self . process : \n            self . process . wait ( ) \n            self . process . stdout . close ( ) \n            self . process . stdin . close ( ) \n            self . process . stderr . close ( ) \n        self . finished = 1 \n        self . send_testcase ( '' , '127.0.0.1' , self . config . ports [ \"servers\" ] [ \"TCASE_PORT\" ] ) \n        self . logger . debug ( \"[{0}] - PJFProcessMonitor successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5288": "\ndef start_monitor ( self , standalone = 1 ) : \n    try : \n        self . start ( ) \n        cmdline = shlex . split ( self . config . process_to_monitor ) \n        if standalone : \n            signal . signal ( signal . SIGINT , self . shutdown ) \n        self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        while self . process and not self . finished : \n            self . process . wait ( ) \n            if self . _is_sigsegv ( self . process . returncode ) : \n                if self . config . debug : \n                    print ( \"[\\033[92mINFO\\033[0m] Process crashed with \\033[91mSIGSEGV\\033[0m, waiting for testcase...\" ) \n                while not self . got_testcase ( ) : \n                    time . sleep ( 1 ) \n                self . save_testcase ( self . testcase [ - 10 : ] ) \n            if self . process : \n                self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n    except OSError : \n        self . shutdown ( ) \n        self . process = 0 \n        self . got_testcase = lambda : 1 \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmdline [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( \"Unknown error please send log to author\" ) "}
{"5290": "\ndef add_definition ( self , cat , def_name , def_val , no_prune = 0 , gram_file = \"default\" ) : \n    self . _rules_processed = 0 \n    self . add_to_cat_group ( cat , gram_file , def_name ) \n    if no_prune : \n        self . no_prunes . setdefault ( cat , { } ) . setdefault ( def_name , 1 ) \n    if self . _staged_defs is not None : \n        self . _staged_defs . append ( ( cat , def_name , def_val ) ) \n    else : \n        self . defs . setdefault ( cat , { } ) . setdefault ( def_name , deque ( ) ) . append ( def_val ) "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = 1 ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == 0 : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = 0 \n    while total_gend < num : \n        if len ( self . _last_pref_keys ) > 0 and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            print ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += 1 \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5293": "\ndef fuzz_elements ( self , element ) : \n    try : \n        if type ( element ) == dict : \n            tmp_element = { } \n            for key in element : \n                if len ( self . config . parameters ) > 0 : \n                    if self . config . exclude_parameters : \n                        fuzz = key not in self . config . parameters \n                    else : \n                        fuzz = key in self . config . parameters \n                else : \n                    fuzz = 1 \n                if fuzz : \n                    if type ( element [ key ] ) == dict : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    elif type ( element [ key ] ) == list : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    else : \n                        tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) \n                else : \n                    tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n            element = tmp_element \n            del tmp_element \n        elif type ( element ) == list : \n            arr = [ ] \n            for key in element : \n                if type ( key ) == dict : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                elif type ( key ) == list : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                else : \n                    if len ( self . config . parameters ) <= 0 : \n                        arr . append ( self . mutator . fuzz ( key ) ) \n                    else : \n                        arr . append ( key ) \n            element = arr \n            del arr \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) \n    return element "}
{"5295": "\ndef get_fuzzed ( self , indent = 0 , utf8 = 0 ) : \n    try : \n        if \"array\" in self . json : \n            return self . fuzz_elements ( dict ( self . json ) ) [ \"array\" ] \n        else : \n            return self . fuzz_elements ( dict ( self . json ) ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5334": "\ndef exhaust ( fn , transform = None , * args , ** kwargs ) : \n    while 1 : \n        iterRes = fn ( * args , ** kwargs ) \n        if iterRes : \n            for item in transform ( iterRes ) if transform else iterRes : \n                yield item \n        else : \n            break "}
{"5340": "\ndef export ( self , metadata , ** kwargs ) : \n    kwargs . setdefault ( 'Dumper' , SafeDumper ) \n    kwargs . setdefault ( 'default_flow_style' , 0 ) \n    kwargs . setdefault ( 'allow_unicode' , 1 ) \n    metadata = yaml . dump ( metadata , ** kwargs ) . strip ( ) \n    return u ( metadata ) "}
{"5349": "\ndef _pattern ( trie : dict ) -> str : \n    if '' in trie : \n        if len ( trie ) == 1 : \n            return '' \n        optional = 1 \n        del trie [ '' ] \n    else : \n        optional = 0 \n    subpattern_to_chars = _defaultdict ( list ) \n    for char , sub_trie in trie . items ( ) : \n        subpattern = _pattern ( sub_trie ) \n        subpattern_to_chars [ subpattern ] . append ( char ) \n    alts = [ ] \n    for subpattern , chars in subpattern_to_chars . items ( ) : \n        if len ( chars ) == 1 : \n            alts . append ( chars [ 0 ] + subpattern ) \n        else : \n            chars . sort ( reverse = 1 ) \n            alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) \n    if len ( alts ) == 1 : \n        result = alts [ 0 ] \n        if optional : \n            if len ( result ) == 1 : \n                result += '?+' \n            else : \n                result = '(?:' + result + ')?+' \n    else : \n        alts . sort ( reverse = 1 ) \n        result = '(?>' + '|' . join ( alts ) + ')' \n        if optional : \n            result += '?+' \n    return result "}
{"5354": "\ndef _shrink_update ( self , rmstart : int , rmstop : int ) -> None : \n    for spans in self . _type_to_spans . values ( ) : \n        i = len ( spans ) - 1 \n        while i >= 0 : \n            s , e = span = spans [ i ] \n            if rmstop <= s : \n                rmlength = rmstop - rmstart \n                span [ : ] = s - rmlength , e - rmlength \n                i -= 1 \n                continue \n            break \n        else : \n            continue \n        while 1 : \n            if rmstart <= s : \n                if rmstop < e : \n                    span [ : ] = rmstart , e + rmstart - rmstop \n                    i -= 1 \n                    if i < 0 : \n                        break \n                    s , e = span = spans [ i ] \n                    continue \n                spans . pop ( i ) [ : ] = - 1 , - 1 \n                i -= 1 \n                if i < 0 : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            break \n        while i >= 0 : \n            if e <= rmstart : \n                i -= 1 \n                if i < 0 : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            span [ 1 ] -= rmstop - rmstart \n            i -= 1 \n            if i < 0 : \n                break \n            s , e = span = spans [ i ] \n            continue "}
{"5360": "\ndef pprint ( self , indent : str = '    ' , remove_comments = 0 ) : \n    warn ( 'pprint method is deprecated, use pformat instead.' , DeprecationWarning , ) \n    return self . pformat ( indent , remove_comments ) "}
{"5368": "\ndef tables ( self ) -> List [ 'Table' ] : \n    tables = [ ] \n    tables_append = tables . append \n    type_to_spans = self . _type_to_spans \n    lststr = self . _lststr \n    shadow = self . _shadow [ : ] \n    ss , se = self . _span \n    spans = type_to_spans . setdefault ( 'Table' , [ ] ) \n    if not spans : \n        m = 1 \n        while m : \n            m = 0 \n            for m in TABLE_FINDITER ( shadow ) : \n                ms , me = m . span ( ) \n                span = [ ss + ms + len ( m [ 1 ] ) , ss + me ] \n                spans . append ( span ) \n                tables_append ( Table ( lststr , type_to_spans , span , 'Table' ) ) \n                shadow [ ms : me ] = b'_' * ( me - ms ) \n        return tables \n    span_tuple_to_span_get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get \n    m = 1 \n    while m : \n        m = 0 \n        for m in TABLE_FINDITER ( shadow ) : \n            ms , me = m . span ( ) \n            s , e = ss + ms + len ( m [ 1 ] ) , ss + me \n            old_span = span_tuple_to_span_get ( ( s , e ) ) \n            if old_span is None : \n                span = [ s , e ] \n                insort ( spans , span ) \n            else : \n                span = old_span \n            tables_append ( Table ( lststr , type_to_spans , span , 'Table' ) ) \n            shadow [ ms : me ] = b'_' * ( me - ms ) \n    return tables "}
{"5376": "\ndef normal_name ( self , rm_namespaces = ( 'Template' , ) , capital_links = 0 , _code : str = None , * , code : str = None , capitalize = 0 ) -> str : \n    if capital_links : \n        warn ( '`capital_links` argument is deprecated,' ' use `capitalize` instead' , DeprecationWarning ) \n        capitalize = capital_links \n    if _code : \n        warn ( '`positional_code` argument is deprecated,' ' use `code` instead' , DeprecationWarning ) \n        code = _code \n    name = COMMENT_SUB ( '' , self . name ) . strip ( WS ) \n    if code : \n        head , sep , tail = name . partition ( ':' ) \n        if not head and sep : \n            name = tail . strip ( ' ' ) \n            head , sep , tail = name . partition ( ':' ) \n        if code . lower ( ) == head . strip ( ' ' ) . lower ( ) : \n            name = tail . strip ( ' ' ) \n    head , sep , tail = name . partition ( ':' ) \n    if not head and sep : \n        name = tail . strip ( ' ' ) \n        head , sep , tail = name . partition ( ':' ) \n    if head : \n        ns = head . strip ( ' ' ) . lower ( ) \n        for namespace in rm_namespaces : \n            if namespace . lower ( ) == ns : \n                name = tail . strip ( ' ' ) \n                break \n    name = name . replace ( '_' , ' ' ) \n    if capitalize : \n        n0 = name [ 0 ] \n        if n0 . islower ( ) : \n            name = n0 . upper ( ) + name [ 1 : ] \n    name , sep , tail = name . partition ( '#' ) \n    return ' ' . join ( name . split ( ) ) "}
{"5379": "\ndef set_arg ( self , name : str , value : str , positional : bool = None , before : str = None , after : str = None , preserve_spacing : bool = 1 ) -> None : \n    args = list ( reversed ( self . arguments ) ) \n    arg = get_arg ( name , args ) \n    if arg : \n        if positional : \n            arg . positional = positional \n        if preserve_spacing : \n            val = arg . value \n            arg . value = val . replace ( val . strip ( WS ) , value ) \n        else : \n            arg . value = value \n        return \n    if not name and positional is None : \n        positional = 1 \n    if not positional and preserve_spacing and args : \n        before_names = [ ] \n        name_lengths = [ ] \n        before_values = [ ] \n        after_values = [ ] \n        for arg in args : \n            aname = arg . name \n            name_len = len ( aname ) \n            name_lengths . append ( name_len ) \n            before_names . append ( STARTING_WS_MATCH ( aname ) [ 0 ] ) \n            arg_value = arg . value \n            before_values . append ( STARTING_WS_MATCH ( arg_value ) [ 0 ] ) \n            after_values . append ( ENDING_WS_MATCH ( arg_value ) [ 0 ] ) \n        pre_name_ws_mode = mode ( before_names ) \n        name_length_mode = mode ( name_lengths ) \n        post_value_ws_mode = mode ( [ SPACE_AFTER_SEARCH ( self . string ) [ 0 ] ] + after_values [ 1 : ] ) \n        pre_value_ws_mode = mode ( before_values ) \n    else : \n        preserve_spacing = 0 \n    if positional : \n        addstring = '|' + value \n    else : \n        if preserve_spacing : \n            addstring = ( '|' + ( pre_name_ws_mode + name . strip ( WS ) ) . ljust ( name_length_mode ) + '=' + pre_value_ws_mode + value + post_value_ws_mode ) \n        else : \n            addstring = '|' + name + '=' + value \n    if before : \n        arg = get_arg ( before , args ) \n        arg . insert ( 0 , addstring ) \n    elif after : \n        arg = get_arg ( after , args ) \n        arg . insert ( len ( arg . string ) , addstring ) \n    else : \n        if args and not positional : \n            arg = args [ 0 ] \n            arg_string = arg . string \n            if preserve_spacing : \n                arg [ 0 : len ( arg_string ) ] = ( arg . string . rstrip ( WS ) + post_value_ws_mode + addstring . rstrip ( WS ) + after_values [ 0 ] ) \n            else : \n                arg . insert ( len ( arg_string ) , addstring ) \n        else : \n            self . insert ( - 2 , addstring ) "}
{"5381": "\ndef has_arg ( self , name : str , value : str = None ) -> bool : \n    for arg in reversed ( self . arguments ) : \n        if arg . name . strip ( WS ) == name . strip ( WS ) : \n            if value : \n                if arg . positional : \n                    if arg . value == value : \n                        return 1 \n                    return 0 \n                if arg . value . strip ( WS ) == value . strip ( WS ) : \n                    return 1 \n                return 0 \n            return 1 \n    return 0 "}
{"5384": "\ndef find ( ellipsname , crstype , strict = 0 ) : \n    if not strict : \n        ellipsname = ellipsname . lower ( ) . replace ( \" \" , \"_\" ) \n    for itemname , item in globals ( ) . items ( ) : \n        if itemname . startswith ( \"_\" ) or itemname == 'Ellipsoid' : \n            continue \n        try : \n            if hasattr ( item . name , crstype ) : \n                itemname = getattr ( item . name , crstype ) \n                if not strict : \n                    itemname = itemname . lower ( ) . replace ( \" \" , \"_\" ) \n                if ellipsname == itemname : \n                    return item \n        except : \n            pass \n    else : \n        return None "}
{"5385": "\ndef from_url ( url , format = None ) : \n    string = urllib2 . urlopen ( url ) . read ( ) \n    if PY3 is 1 : \n        string = string . decode ( 'utf-8' ) \n    if format : \n        format = format . lower ( ) . replace ( \" \" , \"_\" ) \n        func = parse . __getattr__ ( \"from_%s\" % format ) \n    else : \n        func = parse . from_unknown_text \n    crs = func ( string ) \n    return crs "}
{"5390": "\ndef from_unknown_text ( text , strict = 0 ) : \n    if text . startswith ( \"+\" ) : \n        crs = from_proj4 ( text , strict ) \n    elif text . startswith ( ( \"PROJCS[\" , \"GEOGCS[\" ) ) : \n        crs = from_unknown_wkt ( text , strict ) \n    elif text . startswith ( \"EPSG:\" ) : \n        crs = from_epsg_code ( text . split ( \":\" ) [ 1 ] ) \n    elif text . startswith ( \"ESRI:\" ) : \n        crs = from_esri_code ( text . split ( \":\" ) [ 1 ] ) \n    elif text . startswith ( \"SR-ORG:\" ) : \n        crs = from_sr_code ( text . split ( \":\" ) [ 1 ] ) \n    else : \n        raise FormatError ( \"Could not auto-detect the type of crs format, make sure it is one of the supported formats\" ) \n    return crs "}
{"5408": "\ndef write_to ( self , out_stream , do_compress = 0 ) : \n    self . update_header ( ) \n    if ( self . vlrs . get ( \"ExtraBytesVlr\" ) and not self . points_data . extra_dimensions_names ) : \n        logger . error ( \"Las contains an ExtraBytesVlr, but no extra bytes were found in the point_record, \" \"removing the vlr\" ) \n        self . vlrs . extract ( \"ExtraBytesVlr\" ) \n    if do_compress : \n        laz_vrl = create_laz_vlr ( self . points_data ) \n        self . vlrs . append ( known . LasZipVlr ( laz_vrl . data ( ) ) ) \n        raw_vlrs = vlrlist . RawVLRList . from_list ( self . vlrs ) \n        self . header . offset_to_point_data = ( self . header . size + raw_vlrs . total_size_in_bytes ( ) ) \n        self . header . point_format_id = uncompressed_id_to_compressed ( self . header . point_format_id ) \n        self . header . number_of_vlr = len ( raw_vlrs ) \n        points_bytes = compress_buffer ( np . frombuffer ( self . points_data . array , np . uint8 ) , laz_vrl . schema , self . header . offset_to_point_data , ) . tobytes ( ) \n    else : \n        raw_vlrs = vlrlist . RawVLRList . from_list ( self . vlrs ) \n        self . header . number_of_vlr = len ( raw_vlrs ) \n        self . header . offset_to_point_data = ( self . header . size + raw_vlrs . total_size_in_bytes ( ) ) \n        points_bytes = self . points_data . raw_bytes ( ) \n    self . header . write_to ( out_stream ) \n    self . _raise_if_not_expected_pos ( out_stream , self . header . size ) \n    raw_vlrs . write_to ( out_stream ) \n    self . _raise_if_not_expected_pos ( out_stream , self . header . offset_to_point_data ) \n    out_stream . write ( points_bytes ) "}
{"5409": "\ndef write_to_file ( self , filename , do_compress = None ) : \n    is_ext_laz = filename . split ( \".\" ) [ - 1 ] == \"laz\" \n    if is_ext_laz and do_compress is None : \n        do_compress = 1 \n    with open ( filename , mode = \"wb\" ) as out : \n        self . write_to ( out , do_compress = do_compress ) "}
{"5410": "\ndef write ( self , destination , do_compress = None ) : \n    if isinstance ( destination , str ) : \n        self . write_to_file ( destination ) \n    else : \n        if do_compress is None : \n            do_compress = 0 \n        self . write_to ( destination , do_compress = do_compress ) "}
{"5413": "\ndef np_dtype_to_point_format ( dtype , unpacked = 0 ) : \n    all_dtypes = ( ALL_POINT_FORMATS_DTYPE if not unpacked else UNPACKED_POINT_FORMATS_DTYPES ) \n    for format_id , fmt_dtype in all_dtypes . items ( ) : \n        if fmt_dtype == dtype : \n            return format_id \n    else : \n        raise errors . IncompatibleDataFormat ( \"Data type of array is not compatible with any point format (array dtype: {})\" . format ( dtype ) ) "}
{"5429": "\ndef open_las ( source , closefd = 1 ) : \n    if isinstance ( source , str ) : \n        stream = open ( source , mode = \"rb\" ) \n        if not closefd : \n            raise ValueError ( \"Cannot use closefd with filename\" ) \n    elif isinstance ( source , bytes ) : \n        stream = io . BytesIO ( source ) \n    else : \n        stream = source \n    return LasReader ( stream , closefd = closefd ) "}
{"5430": "\ndef read_las ( source , closefd = 1 ) : \n    with open_las ( source , closefd = closefd ) as reader : \n        return reader . read ( ) "}
{"5435": "\ndef write_then_read_again ( las , do_compress = 0 ) : \n    out = io . BytesIO ( ) \n    las . write ( out , do_compress = do_compress ) \n    out . seek ( 0 ) \n    return read_las ( out ) "}
{"5447": "\ndef pack ( array , sub_field_array , mask , inplace = 0 ) : \n    lsb = least_significant_bit ( mask ) \n    max_value = int ( mask >> lsb ) \n    if sub_field_array . max ( ) > max_value : \n        raise OverflowError ( \"value ({}) is greater than allowed (max: {})\" . format ( sub_field_array . max ( ) , max_value ) ) \n    if inplace : \n        array [ : ] = array & ~ mask \n        array [ : ] = array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) \n    else : \n        array = array & ~ mask \n        return array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) "}
{"5459": "\nasync def connect ( self ) : \n    _LOGGER . debug ( \"Connecting...\" ) \n    try : \n        self . _reader , self . _writer = await asyncio . open_connection ( self . _host , self . _port , loop = self . _loop ) \n        _LOGGER . debug ( \"sucess connecting...\" ) \n    except Exception as e : \n        _LOGGER . warning ( \"Exception during connecting: %s.\" , e ) \n        self . _writer = None \n        self . _reader = None \n        return 0 \n    return 1 "}
{"5464": "\nasync def keep_alive ( self ) : \n    while 1 : \n        await asyncio . sleep ( self . _keep_alive_timeout ) \n        if self . closed : \n            return \n        data = generate_query ( b'\\xEE\\x01\\x01' ) \n        await self . _send_data ( data ) "}
{"5465": "\nasync def monitor_status ( self , alarm_status_callback = None , zone_changed_callback = None , output_changed_callback = None ) : \n    self . _alarm_status_callback = alarm_status_callback \n    self . _zone_changed_callback = zone_changed_callback \n    self . _output_changed_callback = output_changed_callback \n    _LOGGER . info ( \"Starting monitor_status loop\" ) \n    while not self . closed : \n        _LOGGER . debug ( \"Iteration... \" ) \n        while not self . connected : \n            _LOGGER . info ( \"Not connected, re-connecting... \" ) \n            await self . connect ( ) \n            if not self . connected : \n                _LOGGER . warning ( \"Not connected, sleeping for 10s... \" ) \n                await asyncio . sleep ( self . _reconnection_timeout ) \n                continue \n        await self . start_monitoring ( ) \n        if not self . connected : \n            _LOGGER . warning ( \"Start monitoring failed, sleeping for 10s...\" ) \n            await asyncio . sleep ( self . _reconnection_timeout ) \n            continue \n        while 1 : \n            await self . _update_status ( ) \n            _LOGGER . debug ( \"Got status!\" ) \n            if not self . connected : \n                _LOGGER . info ( \"Got connection broken, reconnecting!\" ) \n                break \n    _LOGGER . info ( \"Closed, quit monitoring.\" ) "}
{"5466": "\ndef close ( self ) : \n    _LOGGER . debug ( \"Closing...\" ) \n    self . closed = 1 \n    if self . connected : \n        self . _writer . close ( ) "}
{"5468": "\ndef guess_type ( self , path , allow_directory = 1 ) : \n    if path . endswith ( '.ipynb' ) : \n        return 'notebook' \n    elif allow_directory and self . dir_exists ( path ) : \n        return 'directory' \n    else : \n        return 'file' "}
{"5473": "\ndef _convert_file_records ( self , file_records ) : \n    for record in file_records : \n        type_ = self . guess_type ( record [ 'name' ] , allow_directory = 0 ) \n        if type_ == 'notebook' : \n            yield self . _notebook_model_from_db ( record , 0 ) \n        elif type_ == 'file' : \n            yield self . _file_model_from_db ( record , 0 , None ) \n        else : \n            self . do_500 ( \"Unknown file type %s\" % type_ ) "}
{"5474": "\ndef _directory_model_from_db ( self , record , content ) : \n    model = base_directory_model ( to_api_path ( record [ 'name' ] ) ) \n    if content : \n        model [ 'format' ] = 'json' \n        model [ 'content' ] = list ( chain ( self . _convert_file_records ( record [ 'files' ] ) , ( self . _directory_model_from_db ( subdir , 0 ) for subdir in record [ 'subdirs' ] ) , ) ) \n    return model "}
{"5494": "\ndef file_exists ( db , user_id , path ) : \n    try : \n        get_file ( db , user_id , path , include_content = 0 , decrypt_func = unused_decrypt_func , ) \n        return 1 \n    except NoSuchFile : \n        return 0 "}
{"5524": "\ndef get ( self , path , content = 1 , type = None , format = None ) : \n    path = normalize_api_path ( path ) \n    if path : \n        return self . __get ( path , content = content , type = type , format = format ) \n    if not content : \n        return base_directory_model ( '' ) \n    extra_content = self . _extra_root_dirs ( ) \n    rm = self . root_manager \n    if rm is None : \n        root_model = base_directory_model ( '' ) \n        root_model . update ( format = 'json' , content = extra_content , ) \n    else : \n        root_model = rm . get ( path , content = content , type = type , format = format , ) \n        root_model [ 'content' ] . extend ( extra_content ) \n    return root_model "}
{"5533": "\ndef create_user ( db_url , user ) : \n    PostgresCheckpoints ( db_url = db_url , user_id = user , create_user_on_startup = 1 , ) "}
{"5535": "\ndef walk_dirs ( mgr , dirs ) : \n    for directory in dirs : \n        children = mgr . get ( directory , content = 1 , type = 'directory' , ) [ 'content' ] \n        dirs , files = map ( sorted , _separate_dirs_files ( children ) ) \n        yield directory , dirs , files \n        if dirs : \n            for entry in walk_dirs ( mgr , dirs ) : \n                yield entry "}
{"5537": "\ndef walk_files_with_content ( mgr ) : \n    for _ , _ , files in walk ( mgr ) : \n        for f in files : \n            yield mgr . get ( f , content = 1 ) "}
{"5554": "\ndef get ( self , * args , ** kwargs ) : \n    if 'pk' in kwargs : \n        kwargs [ 'parent' ] = kwargs [ 'pk' ] \n        kwargs [ 'head' ] = 1 \n        del kwargs [ 'pk' ] \n    if 'request' in kwargs : \n        request = kwargs [ 'request' ] \n        version = request . GET . get ( 'version' , None ) \n        preview_id = request . GET . get ( 'preview_id' , None ) \n        if ( version is not None ) and ( preview_id is not None ) : \n            kwargs [ 'revision_id' ] = version \n            kwargs [ 'preview_id' ] = preview_id \n            del kwargs [ 'is_published' ] \n        del kwargs [ 'request' ] \n    return super ( PublishableManager , self ) . get ( * args , ** kwargs ) "}
{"5557": "\ndef get_attribute ( self , instance ) : \n    attr = super ( NullBooleanField , self ) . get_attribute ( instance ) \n    return 1 if attr else 0 "}
{"5560": "\ndef is_valid_uuid ( id ) : \n    if not isinstance ( id , basestring ) : \n        return 0 \n    try : \n        val = UUID ( id , version = 4 ) \n    except ValueError : \n        return 0 \n    return 1 "}
{"5568": "\ndef get_settings ( cls , show_hidden = 0 ) : \n    settings = Integration . objects . get_settings ( cls . ID ) \n    if not show_hidden : \n        for field in cls . HIDDEN_FIELDS : \n            settings . pop ( field , None ) \n    return settings "}
{"5572": "\ndef signup ( request , uuid = None ) : \n    invite = get_object_or_404 ( Invite . objects . all ( ) , id = uuid ) \n    if invite . expiration_date < timezone . now ( ) : \n        invite . delete ( ) \n        raise Http404 ( 'This page does not exist.' ) \n    if request . method == 'POST' : \n        form = SignUpForm ( request . POST ) \n        if form . is_valid ( ) : \n            user = form . save ( commit = 0 ) \n            user . email = invite . email \n            user . person = invite . person \n            user . save ( ) \n            if invite . permissions == 'admin' : \n                group = Group . objects . get ( name = 'Admin' ) \n                user . groups . add ( group ) \n            invite . delete ( ) \n            return redirect ( 'dispatch-admin' ) \n        else : \n            return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) \n    else : \n        form = SignUpForm ( ) \n    return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) "}
{"5582": "\ndef get_bandwith_limited_stream ( self , fileobj , transfer_coordinator , enabled = 1 ) : \n    stream = BandwidthLimitedStream ( fileobj , self . _leaky_bucket , transfer_coordinator , self . _time_utils ) \n    if not enabled : \n        stream . disable_bandwidth_limiting ( ) \n    return stream "}
{"5594": "\ndef finalize ( self ) : \n    with self . _lock : \n        self . _is_finalized = 1 \n        if self . _count == 0 : \n            self . _callback ( ) "}
{"5595": "\ndef is_special_file ( cls , filename ) : \n    if not os . path . exists ( filename ) : \n        return 0 \n    mode = os . stat ( filename ) . st_mode \n    if stat . S_ISCHR ( mode ) : \n        return 1 \n    if stat . S_ISBLK ( mode ) : \n        return 1 \n    if stat . S_ISFIFO ( mode ) : \n        return 1 \n    if stat . S_ISSOCK ( mode ) : \n        return 1 \n    return 0 "}
{"5596": "\ndef acquire ( self , tag , blocking = 1 ) : \n    logger . debug ( \"Acquiring %s\" , tag ) \n    if not self . _semaphore . acquire ( blocking ) : \n        raise NoResourcesAvailable ( \"Cannot acquire tag '%s'\" % tag ) "}
{"5602": "\ndef _main ( self , client , bucket , key , fileobj , extra_args , callbacks , max_attempts , download_output_manager , io_chunksize , start_index = 0 , bandwidth_limiter = None ) : \n    last_exception = None \n    for i in range ( max_attempts ) : \n        try : \n            response = client . get_object ( Bucket = bucket , Key = key , ** extra_args ) \n            streaming_body = StreamReaderProgress ( response [ 'Body' ] , callbacks ) \n            if bandwidth_limiter : \n                streaming_body = bandwidth_limiter . get_bandwith_limited_stream ( streaming_body , self . _transfer_coordinator ) \n            current_index = start_index \n            chunks = DownloadChunkIterator ( streaming_body , io_chunksize ) \n            for chunk in chunks : \n                if not self . _transfer_coordinator . done ( ) : \n                    self . _handle_io ( download_output_manager , fileobj , chunk , current_index ) \n                    current_index += len ( chunk ) \n                else : \n                    return \n            return \n        except S3_RETRYABLE_DOWNLOAD_ERRORS as e : \n            logger . debug ( \"Retrying exception caught (%s), \" \"retrying request, (attempt %s / %s)\" , e , i , max_attempts , exc_info = 1 ) \n            last_exception = e \n            invoke_progress_callbacks ( callbacks , start_index - current_index ) \n            continue \n    raise RetriesExceededError ( last_exception ) "}
{"5605": "\ndef seekable ( fileobj ) : \n    if hasattr ( fileobj , 'seekable' ) : \n        return fileobj . seekable ( ) \n    elif hasattr ( fileobj , 'seek' ) and hasattr ( fileobj , 'tell' ) : \n        try : \n            fileobj . seek ( 0 , 1 ) \n            return 1 \n        except ( OSError , IOError ) : \n            return 0 \n    return 0 "}
{"5610": "\ndef shutdown ( self , cancel = 0 , cancel_msg = '' ) : \n    self . _shutdown ( cancel , cancel , cancel_msg ) "}
{"5613": "\ndef _read ( self , fileobj , amount , truncate = 1 ) : \n    if len ( self . _initial_data ) == 0 : \n        return fileobj . read ( amount ) \n    if amount <= len ( self . _initial_data ) : \n        data = self . _initial_data [ : amount ] \n        if truncate : \n            self . _initial_data = self . _initial_data [ amount : ] \n        return data \n    amount_to_read = amount - len ( self . _initial_data ) \n    data = self . _initial_data + fileobj . read ( amount_to_read ) \n    if truncate : \n        self . _initial_data = b'' \n    return data "}
{"5616": "\ndef set_exception ( self , exception ) : \n    if not self . done ( ) : \n        raise TransferNotDoneError ( 'set_exception can only be called once the transfer is ' 'complete.' ) \n    self . _coordinator . set_exception ( exception , override = 1 ) "}
{"5618": "\ndef set_exception ( self , exception , override = 0 ) : \n    with self . _lock : \n        if not self . done ( ) or override : \n            self . _exception = exception \n            self . _status = 'failed' "}
{"5620": "\ndef cancel ( self , msg = '' , exc_type = CancelledError ) : \n    with self . _lock : \n        if not self . done ( ) : \n            should_announce_done = 0 \n            logger . debug ( '%s cancel(%s) called' , self , msg ) \n            self . _exception = exc_type ( msg ) \n            if self . _status == 'not-started' : \n                should_announce_done = 1 \n            self . _status = 'cancelled' \n            if should_announce_done : \n                self . announce_done ( ) "}
{"5625": "\ndef submit ( self , task , tag = None , block = 1 ) : \n    semaphore = self . _semaphore \n    if tag : \n        semaphore = self . _tag_semaphores [ tag ] \n    acquire_token = semaphore . acquire ( task . transfer_id , block ) \n    release_callback = FunctionContainer ( semaphore . release , task . transfer_id , acquire_token ) \n    future = ExecutorFuture ( self . _executor . submit ( task ) ) \n    future . add_done_callback ( release_callback ) \n    return future "}
{"5628": "\ndef download_file ( self , bucket , key , filename , extra_args = None , callback = None ) : \n    if extra_args is None : \n        extra_args = { } \n    self . _validate_all_known_args ( extra_args , self . ALLOWED_DOWNLOAD_ARGS ) \n    object_size = self . _object_size ( bucket , key , extra_args ) \n    temp_filename = filename + os . extsep + random_file_extension ( ) \n    try : \n        self . _download_file ( bucket , key , temp_filename , object_size , extra_args , callback ) \n    except Exception : \n        logger . debug ( \"Exception caught in download_file, removing partial \" \"file: %s\" , temp_filename , exc_info = 1 ) \n        self . _osutil . remove_file ( temp_filename ) \n        raise \n    else : \n        self . _osutil . rename_file ( temp_filename , filename ) "}
{"5634": "\ndef refactor_step ( self , old_text , new_text , move_param_from_idx ) : \n    diffs = [ ] \n    step , func = self . _find_step_node ( old_text ) \n    if step is None : \n        return diffs \n    step_diff = self . _refactor_step_text ( step , old_text , new_text ) \n    diffs . append ( step_diff ) \n    moved_params = self . _move_params ( func . arguments , move_param_from_idx ) \n    if func . arguments is not moved_params : \n        params_span = self . _span_for_node ( func . arguments , 0 ) \n        func . arguments = moved_params \n        diffs . append ( ( params_span , func . arguments . dumps ( ) ) ) \n    return diffs "}
{"5636": "\ndef list ( self , teamId , max = None , ** request_parameters ) : \n    check_type ( teamId , basestring , may_be_none = 0 ) \n    check_type ( max , int ) \n    params = dict_from_items_with_values ( request_parameters , teamId = teamId , max = max , ) \n    items = self . _session . get_items ( API_ENDPOINT , params = params ) \n    for item in items : \n        yield self . _object_factory ( OBJECT_TYPE , item ) "}
{"5637": "\ndef create ( self , teamId , personId = None , personEmail = None , isModerator = 0 , ** request_parameters ) : \n    check_type ( teamId , basestring , may_be_none = 0 ) \n    check_type ( personId , basestring ) \n    check_type ( personEmail , basestring ) \n    check_type ( isModerator , bool ) \n    post_data = dict_from_items_with_values ( request_parameters , teamId = teamId , personId = personId , personEmail = personEmail , isModerator = isModerator , ) \n    json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5638": "\ndef update ( self , membershipId , isModerator = None , ** request_parameters ) : \n    check_type ( membershipId , basestring , may_be_none = 0 ) \n    check_type ( isModerator , bool ) \n    put_data = dict_from_items_with_values ( request_parameters , isModerator = isModerator , ) \n    json_data = self . _session . put ( API_ENDPOINT + '/' + membershipId , json = put_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5639": "\ndef delete ( self , membershipId ) : \n    check_type ( membershipId , basestring , may_be_none = 0 ) \n    self . _session . delete ( API_ENDPOINT + '/' + membershipId ) "}
{"5640": "\ndef get_catfact ( ) : \n    response = requests . get ( CAT_FACTS_URL , verify = 0 ) \n    response . raise_for_status ( ) \n    json_data = response . json ( ) \n    return json_data [ 'fact' ] "}
{"5647": "\ndef check_type ( o , acceptable_types , may_be_none = 1 ) : \n    if not isinstance ( acceptable_types , tuple ) : \n        acceptable_types = ( acceptable_types , ) \n    if may_be_none and o is None : \n        pass \n    elif isinstance ( o , acceptable_types ) : \n        pass \n    else : \n        error_message = ( \"We were expecting to receive an instance of one of the following \" \"types: {types}{none}; but instead we received {o} which is a \" \"{o_type}.\" . format ( types = \", \" . join ( [ repr ( t . __name__ ) for t in acceptable_types ] ) , none = \"or 'None'\" if may_be_none else \"\" , o = o , o_type = repr ( type ( o ) . __name__ ) ) ) \n        raise TypeError ( error_message ) "}
{"5654": "\ndef update ( self , roomId , title = None , ** request_parameters ) : \n    check_type ( roomId , basestring , may_be_none = 0 ) \n    check_type ( roomId , basestring ) \n    put_data = dict_from_items_with_values ( request_parameters , title = title , ) \n    json_data = self . _session . put ( API_ENDPOINT + '/' + roomId , json = put_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5655": "\ndef delete ( self , roomId ) : \n    check_type ( roomId , basestring , may_be_none = 0 ) \n    self . _session . delete ( API_ENDPOINT + '/' + roomId ) "}
{"5659": "\ndef create ( self , name , targetUrl , resource , event , filter = None , secret = None , ** request_parameters ) : \n    check_type ( name , basestring , may_be_none = 0 ) \n    check_type ( targetUrl , basestring , may_be_none = 0 ) \n    check_type ( resource , basestring , may_be_none = 0 ) \n    check_type ( event , basestring , may_be_none = 0 ) \n    check_type ( filter , basestring ) \n    check_type ( secret , basestring ) \n    post_data = dict_from_items_with_values ( request_parameters , name = name , targetUrl = targetUrl , resource = resource , event = event , filter = filter , secret = secret , ) \n    json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5660": "\ndef update ( self , webhookId , name = None , targetUrl = None , ** request_parameters ) : \n    check_type ( webhookId , basestring , may_be_none = 0 ) \n    check_type ( name , basestring ) \n    check_type ( targetUrl , basestring ) \n    put_data = dict_from_items_with_values ( request_parameters , name = name , targetUrl = targetUrl , ) \n    json_data = self . _session . put ( API_ENDPOINT + '/' + webhookId , json = put_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5661": "\ndef delete ( self , webhookId ) : \n    check_type ( webhookId , basestring , may_be_none = 0 ) \n    self . _session . delete ( API_ENDPOINT + '/' + webhookId ) "}
{"5663": "\ndef wait_on_rate_limit ( self , value ) : \n    check_type ( value , bool , may_be_none = 0 ) \n    self . _wait_on_rate_limit = value "}
{"5664": "\ndef update_headers ( self , headers ) : \n    check_type ( headers , dict , may_be_none = 0 ) \n    self . _req_session . headers . update ( headers ) "}
{"5666": "\ndef request ( self , method , url , erc , ** kwargs ) : \n    abs_url = self . abs_url ( url ) \n    kwargs . setdefault ( 'timeout' , self . single_request_timeout ) \n    while 1 : \n        response = self . _req_session . request ( method , abs_url , ** kwargs ) \n        try : \n            check_response_code ( response , erc ) \n        except RateLimitError as e : \n            if self . wait_on_rate_limit : \n                warnings . warn ( RateLimitWarning ( response ) ) \n                time . sleep ( e . retry_after ) \n                continue \n            else : \n                raise \n        else : \n            return response "}
{"5667": "\ndef get ( self , url , params = None , ** kwargs ) : \n    check_type ( url , basestring , may_be_none = 0 ) \n    check_type ( params , dict ) \n    erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'GET' ] ) \n    response = self . request ( 'GET' , url , erc , params = params , ** kwargs ) \n    return extract_and_parse_json ( response ) "}
{"5668": "\ndef get_pages ( self , url , params = None , ** kwargs ) : \n    check_type ( url , basestring , may_be_none = 0 ) \n    check_type ( params , dict ) \n    erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'GET' ] ) \n    response = self . request ( 'GET' , url , erc , params = params , ** kwargs ) \n    while 1 : \n        yield extract_and_parse_json ( response ) \n        if response . links . get ( 'next' ) : \n            next_url = response . links . get ( 'next' ) . get ( 'url' ) \n            next_url = _fix_next_url ( next_url ) \n            response = self . request ( 'GET' , next_url , erc , ** kwargs ) \n        else : \n            break "}
{"5670": "\ndef put ( self , url , json = None , data = None , ** kwargs ) : \n    check_type ( url , basestring , may_be_none = 0 ) \n    erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'PUT' ] ) \n    response = self . request ( 'PUT' , url , erc , json = json , data = data , ** kwargs ) \n    return extract_and_parse_json ( response ) "}
{"5671": "\ndef delete ( self , url , ** kwargs ) : \n    check_type ( url , basestring , may_be_none = 0 ) \n    erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'DELETE' ] ) \n    self . request ( 'DELETE' , url , erc , ** kwargs ) "}
{"5673": "\ndef list ( self , roomId , mentionedPeople = None , before = None , beforeMessage = None , max = None , ** request_parameters ) : \n    check_type ( roomId , basestring , may_be_none = 0 ) \n    check_type ( mentionedPeople , basestring ) \n    check_type ( before , basestring ) \n    check_type ( beforeMessage , basestring ) \n    check_type ( max , int ) \n    params = dict_from_items_with_values ( request_parameters , roomId = roomId , mentionedPeople = mentionedPeople , before = before , beforeMessage = beforeMessage , max = max , ) \n    items = self . _session . get_items ( API_ENDPOINT , params = params ) \n    for item in items : \n        yield self . _object_factory ( OBJECT_TYPE , item ) "}
{"5675": "\ndef delete ( self , messageId ) : \n    check_type ( messageId , basestring , may_be_none = 0 ) \n    self . _session . delete ( API_ENDPOINT + '/' + messageId ) "}
{"5676": "\ndef create ( self , emails , displayName = None , firstName = None , lastName = None , avatar = None , orgId = None , roles = None , licenses = None , ** request_parameters ) : \n    check_type ( emails , list , may_be_none = 0 ) \n    check_type ( displayName , basestring ) \n    check_type ( firstName , basestring ) \n    check_type ( lastName , basestring ) \n    check_type ( avatar , basestring ) \n    check_type ( orgId , basestring ) \n    check_type ( roles , list ) \n    check_type ( licenses , list ) \n    post_data = dict_from_items_with_values ( request_parameters , emails = emails , displayName = displayName , firstName = firstName , lastName = lastName , avatar = avatar , orgId = orgId , roles = roles , licenses = licenses , ) \n    json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5677": "\ndef get ( self , personId ) : \n    check_type ( personId , basestring , may_be_none = 0 ) \n    json_data = self . _session . get ( API_ENDPOINT + '/' + personId ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5679": "\ndef delete ( self , personId ) : \n    check_type ( personId , basestring , may_be_none = 0 ) \n    self . _session . delete ( API_ENDPOINT + '/' + personId ) "}
{"5683": "\ndef create ( self , name , ** request_parameters ) : \n    check_type ( name , basestring , may_be_none = 0 ) \n    post_data = dict_from_items_with_values ( request_parameters , name = name , ) \n    json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5684": "\ndef update ( self , teamId , name = None , ** request_parameters ) : \n    check_type ( teamId , basestring , may_be_none = 0 ) \n    check_type ( name , basestring ) \n    put_data = dict_from_items_with_values ( request_parameters , name = name , ) \n    json_data = self . _session . put ( API_ENDPOINT + '/' + teamId , json = put_data ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5685": "\ndef delete ( self , teamId ) : \n    check_type ( teamId , basestring , may_be_none = 0 ) \n    self . _session . delete ( API_ENDPOINT + '/' + teamId ) "}
{"5688": "\ndef get ( self , client_id , client_secret , code , redirect_uri ) : \n    check_type ( client_id , basestring , may_be_none = 0 ) \n    check_type ( client_secret , basestring , may_be_none = 0 ) \n    check_type ( code , basestring , may_be_none = 0 ) \n    check_type ( redirect_uri , basestring , may_be_none = 0 ) \n    post_data = dict_from_items_with_values ( grant_type = \"authorization_code\" , client_id = client_id , client_secret = client_secret , code = code , redirect_uri = redirect_uri , ) \n    response = requests . post ( self . _endpoint_url , data = post_data , ** self . _request_kwargs ) \n    check_response_code ( response , EXPECTED_RESPONSE_CODE [ 'POST' ] ) \n    json_data = extract_and_parse_json ( response ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5695": "\ndef console ( ) : \n    parser = argparse . ArgumentParser ( description = console . __doc__ ) \n    parser . add_argument ( '--device' , default = '/dev/ttyUSB0' , help = 'port to read DSMR data from' ) \n    parser . add_argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) \n    parser . add_argument ( '--port' , default = None , help = 'TCP port to use for connection' ) \n    parser . add_argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) \n    parser . add_argument ( '--verbose' , '-v' , action = 'count' ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        level = logging . DEBUG \n    else : \n        level = logging . ERROR \n    logging . basicConfig ( level = level ) \n    loop = asyncio . get_event_loop ( ) \n    def print_callback ( telegram ) : \n        for obiref , obj in telegram . items ( ) : \n            if obj : \n                print ( obj . value , obj . unit ) \n        print ( ) \n    if args . host and args . port : \n        create_connection = partial ( create_tcp_dsmr_reader , args . host , args . port , args . version , print_callback , loop = loop ) \n    else : \n        create_connection = partial ( create_dsmr_reader , args . device , args . version , print_callback , loop = loop ) \n    try : \n        while 1 : \n            conn = create_connection ( ) \n            transport , protocol = loop . run_until_complete ( conn ) \n            loop . run_until_complete ( protocol . wait_closed ( ) ) \n            loop . run_until_complete ( asyncio . sleep ( 5 ) ) \n    except KeyboardInterrupt : \n        transport . close ( ) \n        loop . run_until_complete ( asyncio . sleep ( 0 ) ) \n    finally : \n        loop . close ( ) "}
{"5696": "\ndef read ( self ) : \n    with serial . Serial ( ** self . serial_settings ) as serial_handle : \n        while 1 : \n            data = serial_handle . readline ( ) \n            self . telegram_buffer . append ( data . decode ( 'ascii' ) ) \n            for telegram in self . telegram_buffer . get_all ( ) : \n                try : \n                    yield self . telegram_parser . parse ( telegram ) \n                except InvalidChecksumError as e : \n                    logger . warning ( str ( e ) ) \n                except ParseError as e : \n                    logger . error ( 'Failed to parse telegram: %s' , e ) "}
{"5697": "\ndef read ( self , queue ) : \n    conn = serial_asyncio . open_serial_connection ( ** self . serial_settings ) \n    reader , _ = yield from conn \n    while 1 : \n        data = yield from reader . readline ( ) \n        self . telegram_buffer . append ( data . decode ( 'ascii' ) ) \n        for telegram in self . telegram_buffer . get_all ( ) : \n            try : \n                queue . put_nowait ( self . telegram_parser . parse ( telegram ) ) \n            except ParseError as e : \n                logger . warning ( 'Failed to parse telegram: %s' , e ) "}
{"5707": "\ndef find_packages ( top = HERE ) : \n    packages = [ ] \n    for d , dirs , _ in os . walk ( top , followlinks = 1 ) : \n        if os . path . exists ( pjoin ( d , '__init__.py' ) ) : \n            packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) \n        elif d != top : \n            dirs [ : ] = [ ] \n    return packages "}
{"5708": "\ndef create_cmdclass ( prerelease_cmd = None , package_data_spec = None , data_files_spec = None ) : \n    wrapped = [ prerelease_cmd ] if prerelease_cmd else [ ] \n    if package_data_spec or data_files_spec : \n        wrapped . append ( 'handle_files' ) \n    wrapper = functools . partial ( _wrap_command , wrapped ) \n    handle_files = _get_file_handler ( package_data_spec , data_files_spec ) \n    if 'bdist_egg' in sys . argv : \n        egg = wrapper ( bdist_egg , strict = 1 ) \n    else : \n        egg = bdist_egg_disabled \n    cmdclass = dict ( build_py = wrapper ( build_py , strict = is_repo ) , bdist_egg = egg , sdist = wrapper ( sdist , strict = 1 ) , handle_files = handle_files , ) \n    if bdist_wheel : \n        cmdclass [ 'bdist_wheel' ] = wrapper ( bdist_wheel , strict = 1 ) \n    cmdclass [ 'develop' ] = wrapper ( develop , strict = 1 ) \n    return cmdclass "}
{"5712": "\ndef _wrap_command ( cmds , cls , strict = 1 ) : \n    class WrappedCommand ( cls ) : \n        def run ( self ) : \n            if not getattr ( self , 'uninstall' , None ) : \n                try : \n                    [ self . run_command ( cmd ) for cmd in cmds ] \n                except Exception : \n                    if strict : \n                        raise \n                    else : \n                        pass \n            update_package_data ( self . distribution ) \n            result = cls . run ( self ) \n            return result \n    return WrappedCommand "}
{"5716": "\ndef _compile_pattern ( pat , ignore_case = 1 ) : \n    if isinstance ( pat , bytes ) : \n        pat_str = pat . decode ( 'ISO-8859-1' ) \n        res_str = _translate_glob ( pat_str ) \n        res = res_str . encode ( 'ISO-8859-1' ) \n    else : \n        res = _translate_glob ( pat ) \n    flags = re . IGNORECASE if ignore_case else 0 \n    return re . compile ( res , flags = flags ) . match "}
{"5733": "\ndef start ( self , block = 0 , timeout = None , retry_interval = 0.5 , extra_predicate = None ) : \n    start = time . time ( ) \n    while 1 : \n        task_handler = self . _dequeue_task ( extra_predicate ) \n        if task_handler is None and block : \n            if timeout is not None and ( time . time ( ) - start ) > timeout : \n                break \n            time . sleep ( retry_interval * ( random . random ( ) + 0.1 ) ) \n        else : \n            break \n    return task_handler "}
{"5754": "\ndef _connect ( self ) : \n    self . log . debug ( \"_connect(): Initializing Connection..\" ) \n    self . socket = websocket . WebSocketApp ( self . url , on_open = self . _on_open , on_message = self . _on_message , on_error = self . _on_error , on_close = self . _on_close ) \n    if 'ca_certs' not in self . sslopt . keys ( ) : \n        ssl_defaults = ssl . get_default_verify_paths ( ) \n        self . sslopt [ 'ca_certs' ] = ssl_defaults . cafile \n    self . log . debug ( \"_connect(): Starting Connection..\" ) \n    self . socket . run_forever ( sslopt = self . sslopt , http_proxy_host = self . http_proxy_host , http_proxy_port = self . http_proxy_port , http_proxy_auth = self . http_proxy_auth , http_no_proxy = self . http_no_proxy ) \n    self . _stop_timers ( ) \n    while self . reconnect_required . is_set ( ) : \n        if not self . disconnect_called . is_set ( ) : \n            self . log . info ( \"Attempting to connect again in %s seconds.\" % self . reconnect_interval ) \n            self . state = \"unavailable\" \n            time . sleep ( self . reconnect_interval ) \n            self . socket . keep_running = 1 \n            self . socket . sock = None \n            self . socket . run_forever ( sslopt = self . sslopt , http_proxy_host = self . http_proxy_host , http_proxy_port = self . http_proxy_port , http_proxy_auth = self . http_proxy_auth , http_no_proxy = self . http_no_proxy ) \n        else : \n            break "}
{"5758": "\ndef _check_pong ( self ) : \n    self . pong_timer . cancel ( ) \n    if self . pong_received : \n        self . log . debug ( \"_check_pong(): Pong received in time.\" ) \n        self . pong_received = 0 \n    else : \n        self . log . debug ( \"_check_pong(): Pong not received in time.\" \"Issuing reconnect..\" ) \n        self . reconnect ( ) "}
{"5759": "\ndef send ( self , api_key = None , secret = None , list_data = None , auth = 0 , ** kwargs ) : \n    if auth : \n        nonce = str ( int ( time . time ( ) * 10000000 ) ) \n        auth_string = 'AUTH' + nonce \n        auth_sig = hmac . new ( secret . encode ( ) , auth_string . encode ( ) , hashlib . sha384 ) . hexdigest ( ) \n        payload = { 'event' : 'auth' , 'apiKey' : api_key , 'authSig' : auth_sig , 'authPayload' : auth_string , 'authNonce' : nonce } \n        payload = json . dumps ( payload ) \n    elif list_data : \n        payload = json . dumps ( list_data ) \n    else : \n        payload = json . dumps ( kwargs ) \n    self . log . debug ( \"send(): Sending payload to API: %s\" , payload ) \n    try : \n        self . socket . send ( payload ) \n    except websocket . WebSocketConnectionClosedException : \n        self . log . error ( \"send(): Did not send out payload %s - client not connected. \" , kwargs ) "}
{"5760": "\ndef _unpause ( self ) : \n    self . log . debug ( \"_unpause(): Clearing paused() Flag!\" ) \n    self . paused . clear ( ) \n    self . log . debug ( \"_unpause(): Re-subscribing softly..\" ) \n    self . _resubscribe ( soft = 1 ) "}
{"5765": "\ndef _resubscribe ( self , soft = 0 ) : \n    if self . bitfinex_config : \n        self . send ( ** self . bitfinex_config ) \n    q_list = [ ] \n    while 1 : \n        try : \n            identifier , q = self . channel_configs . popitem ( last = 1 if soft else 0 ) \n        except KeyError : \n            break \n        q_list . append ( ( identifier , q . copy ( ) ) ) \n        if identifier == 'auth' : \n            self . send ( ** q , auth = 1 ) \n            continue \n        if soft : \n            q [ 'event' ] = 'unsubscribe' \n        self . send ( ** q ) \n    if soft : \n        for identifier , q in reversed ( q_list ) : \n            self . channel_configs [ identifier ] = q \n            self . send ( ** q ) \n    else : \n        for identifier , q in q_list : \n            self . channel_configs [ identifier ] = q "}
{"5771": "\ndef config ( self , decimals_as_strings = 1 , ts_as_dates = 0 , sequencing = 0 , ts = 0 , ** kwargs ) : \n    flags = 0 \n    if decimals_as_strings : \n        flags += 8 \n    if ts_as_dates : \n        flags += 32 \n    if ts : \n        flags += 32768 \n    if sequencing : \n        flags += 65536 \n    q = { 'event' : 'conf' , 'flags' : flags } \n    q . update ( kwargs ) \n    self . conn . bitfinex_config = q \n    self . conn . send ( ** q ) "}
{"5782": "\ndef authenticate ( self ) : \n    if not self . key and not self . secret : \n        raise ValueError ( \"Must supply both key and secret key for API!\" ) \n    self . channel_configs [ 'auth' ] = { 'api_key' : self . key , 'secret' : self . secret } \n    self . conn . send ( api_key = self . key , secret = self . secret , auth = 1 ) "}
{"5783": "\ndef cancel_order ( self , multi = 0 , ** order_identifiers ) : \n    if multi : \n        self . _send_auth_command ( 'oc_multi' , order_identifiers ) \n    else : \n        self . _send_auth_command ( 'oc' , order_identifiers ) "}
{"5799": "\ndef publishCommand ( self , typeId , deviceId , commandId , msgFormat , data = None , qos = 0 , on_publish = None ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support sending commands\" ) \n        return 0 \n    if not self . connectEvent . wait ( timeout = 10 ) : \n        return 0 \n    else : \n        topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n        if self . getMessageCodec ( msgFormat ) is None : \n            raise MissingMessageEncoderException ( msgFormat ) \n        payload = self . getMessageCodec ( msgFormat ) . encode ( data , datetime . now ( ) ) \n        result = self . client . publish ( topic , payload = payload , qos = qos , retain = 0 ) \n        if result [ 0 ] == paho . MQTT_ERR_SUCCESS : \n            with self . _messagesLock : \n                if result [ 1 ] in self . _onPublishCallbacks : \n                    del self . _onPublishCallbacks [ result [ 1 ] ] \n                    if on_publish is not None : \n                        on_publish ( ) \n                else : \n                    self . _onPublishCallbacks [ result [ 1 ] ] = on_publish \n            return 1 \n        else : \n            return 0 "}
{"5811": "\ndef nearest ( self , coordinates , num_results = 1 , objects = 0 ) : \n    if objects : \n        return self . _nearest_obj ( coordinates , num_results , objects ) \n    p_mins , p_maxs = self . get_coordinate_pointers ( coordinates ) \n    p_num_results = ctypes . pointer ( ctypes . c_uint64 ( num_results ) ) \n    it = ctypes . pointer ( ctypes . c_int64 ( ) ) \n    core . rt . Index_NearestNeighbors_id ( self . handle , p_mins , p_maxs , self . properties . dimension , ctypes . byref ( it ) , p_num_results ) \n    return self . _get_ids ( it , p_num_results . contents . value ) "}
{"5817": "\ndef check_return ( result , func , cargs ) : \n    if result != 0 : \n        s = rt . Error_GetLastErrorMsg ( ) . decode ( ) \n        msg = 'LASError in \"%s\": %s' % ( func . __name__ , s ) \n        rt . Error_Reset ( ) \n        raise RTreeError ( msg ) \n    return 1 "}
{"5830": "\ndef save_form_data ( self , instance , data ) : \n    to_assign = data \n    if data and isinstance ( data , tuple ) : \n        if data [ 0 ] is None : \n            current_field = getattr ( instance , self . name ) \n            if data [ 1 ] : \n                current_field . ppoi = data [ 1 ] \n            to_assign = current_field \n        elif data [ 0 ] is 0 : \n            to_assign = '' \n        else : \n            to_assign = data [ 0 ] \n    super ( VersatileImageField , self ) . save_form_data ( instance , to_assign ) "}
{"5858": "\ndef format_function ( func_body , func_type = None , indent = 2 , format_locals = 1 , ) : \n    if func_type is None : \n        yield 'func' \n    else : \n        param_section = ' (param {})' . format ( ' ' . join ( map ( format_lang_type , func_type . param_types ) ) ) if func_type . param_types else '' \n        result_section = ' (result {})' . format ( format_lang_type ( func_type . return_type ) ) if func_type . return_type else '' \n        yield 'func' + param_section + result_section \n    if format_locals and func_body . locals : \n        yield '(locals {})' . format ( ' ' . join ( itertools . chain . from_iterable ( itertools . repeat ( format_lang_type ( x . type ) , x . count ) for x in func_body . locals ) ) ) \n    level = 1 \n    for cur_insn in decode_bytecode ( func_body . code ) : \n        if cur_insn . op . flags & INSN_LEAVE_BLOCK : \n            level -= 1 \n        yield ' ' * ( level * indent ) + format_instruction ( cur_insn ) \n        if cur_insn . op . flags & INSN_ENTER_BLOCK : \n            level += 1 "}
{"5860": "\ndef decode_module ( module , decode_name_subsections = 0 ) : \n    module_wnd = memoryview ( module ) \n    hdr = ModuleHeader ( ) \n    hdr_len , hdr_data , _ = hdr . from_raw ( None , module_wnd ) \n    yield ModuleFragment ( hdr , hdr_data ) \n    module_wnd = module_wnd [ hdr_len : ] \n    while module_wnd : \n        sec = Section ( ) \n        sec_len , sec_data , _ = sec . from_raw ( None , module_wnd ) \n        if ( decode_name_subsections and sec_data . id == SEC_UNK and sec_data . name == SEC_NAME ) : \n            sec_wnd = sec_data . payload \n            while sec_wnd : \n                subsec = NameSubSection ( ) \n                subsec_len , subsec_data , _ = subsec . from_raw ( None , sec_wnd ) \n                yield ModuleFragment ( subsec , subsec_data ) \n                sec_wnd = sec_wnd [ subsec_len : ] \n        else : \n            yield ModuleFragment ( sec , sec_data ) \n        module_wnd = module_wnd [ sec_len : ] "}
{"5861": "\ndef deprecated_func ( func ) : \n    first_usage = [ 1 ] \n    \n    @ functools . wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        if first_usage [ 0 ] : \n            warnings . warn ( \"Call to deprecated function {}.\" . format ( func . __name__ ) , DeprecationWarning , ) \n            first_usage [ 0 ] = 0 \n        return func ( * args , ** kwargs ) \n    return wrapper "}
{"5875": "\ndef load ( self , data , session = None , instance = None , transient = 0 , * args , ** kwargs ) : \n    self . _session = session or self . _session \n    self . _transient = transient or self . _transient \n    if not ( self . transient or self . session ) : \n        raise ValueError ( \"Deserialization requires a session\" ) \n    self . instance = instance or self . instance \n    try : \n        return super ( ModelSchema , self ) . load ( data , * args , ** kwargs ) \n    finally : \n        self . instance = None "}
{"5885": "\ndef textacy_cleaner ( text : str ) -> str : \n    return preprocess_text ( text , fix_unicode = 1 , lowercase = 1 , transliterate = 1 , no_urls = 1 , no_emails = 1 , no_phone_numbers = 1 , no_numbers = 1 , no_currency_symbols = 1 , no_punct = 1 , no_contractions = 0 , no_accents = 1 ) "}
{"5891": "\ndef token_count_pandas ( self ) : \n    freq_df = pd . DataFrame . from_dict ( self . indexer . word_counts , orient = 'index' ) \n    freq_df . columns = [ 'count' ] \n    return freq_df . sort_values ( 'count' , ascending = 0 ) "}
{"5898": "\ndef equals_order_sensitive ( self , other ) : \n    if not isinstance ( other , Mapping ) or len ( self ) != len ( other ) : \n        return 0 \n    return all ( i == j for ( i , j ) in izip ( iteritems ( self ) , iteritems ( other ) ) ) "}
{"5901": "\ndef move_to_end ( self , key , last = 1 ) : \n    node = self . _fwdm [ key ] \n    node . prv . nxt = node . nxt \n    node . nxt . prv = node . prv \n    sntl = self . _sntl \n    if last : \n        last = sntl . prv \n        node . prv = last \n        node . nxt = sntl \n        sntl . prv = last . nxt = node \n    else : \n        first = sntl . nxt \n        node . prv = sntl \n        node . nxt = first \n        sntl . nxt = first . prv = node "}
{"5902": "\ndef write_temp_file ( text = \"\" ) : \n    with NamedTemporaryFile ( mode = 'w+t' , suffix = '.yml' , delete = 0 ) as tempfile : \n        tempfile . write ( text ) \n        return tempfile . name "}
{"5903": "\ndef get_contacts ( address_books , query , method = \"all\" , reverse = 0 , group = 0 , sort = \"first_name\" ) : \n    contacts = [ ] \n    for address_book in address_books : \n        contacts . extend ( address_book . search ( query , method = method ) ) \n    if group : \n        if sort == \"first_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) ) \n        elif sort == \"last_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) ) \n        else : \n            raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) \n    else : \n        if sort == \"first_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) \n        elif sort == \"last_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) \n        else : \n            raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) "}
{"5904": "\ndef merge_args_into_config ( args , config ) : \n    if \"display\" in args and args . display : \n        config . set_display_by_name ( args . display ) \n    if \"group_by_addressbook\" in args and args . group_by_addressbook : \n        config . set_group_by_addressbook ( 1 ) \n    if \"reverse\" in args and args . reverse : \n        config . set_reverse ( 1 ) \n    if \"sort\" in args and args . sort : \n        config . sort = args . sort \n    if \"vcard_version\" in args and args . vcard_version : \n        config . set_preferred_vcard_version ( args . vcard_version ) \n    if \"search_in_source_files\" in args and args . search_in_source_files : \n        config . set_search_in_source_files ( 1 ) \n    if \"skip_unparsable\" in args and args . skip_unparsable : \n        config . set_skip_unparsable ( 1 ) \n    if \"addressbook\" in args and not args . addressbook : \n        args . addressbook = [ abook . name for abook in config . abooks ] \n    if \"target_addressbook\" in args and not args . target_addressbook : \n        args . target_addressbook = [ abook . name for abook in config . abooks ] "}
{"5911": "\ndef modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : \n    if selected_vcard . get_version ( ) not in config . supported_vcard_versions : \n        print ( \"Warning:\\nThe selected contact is based on vcard version %s \" \"but khard only supports the creation and modification of vcards\" \" with version 3.0 and 4.0.\\nIf you proceed, the contact will be\" \" converted to vcard version %s but beware: This could corrupt \" \"the contact file or cause data loss.\" % ( selected_vcard . get_version ( ) , config . get_preferred_vcard_version ( ) ) ) \n        while 1 : \n            input_string = input ( \"Do you want to proceed anyway (y/n)? \" ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_existing_contact_with_new_user_input ( selected_vcard , input_from_stdin_or_file , config . localize_dates ( ) ) \n        except ValueError as err : \n            print ( err ) \n            sys . exit ( 1 ) \n        if selected_vcard == new_contact : \n            print ( \"Nothing changed\\n\\n%s\" % new_contact . print_vcard ( ) ) \n        else : \n            print ( \"Modification\\n\\n%s\\n\" % new_contact . print_vcard ( ) ) \n            while 1 : \n                input_string = input ( \"Do you want to proceed (y/n)? \" ) \n                if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break \n                if input_string . lower ( ) == \"y\" : \n                    new_contact . write_to_file ( overwrite = 1 ) \n                    if open_editor : \n                        modify_existing_contact ( new_contact ) \n                    else : \n                        print ( \"Done\" ) \n                    break \n    else : \n        modify_existing_contact ( selected_vcard ) "}
{"5912": "\ndef remove_subcommand ( selected_vcard , force ) : \n    if not force : \n        while 1 : \n            input_string = input ( \"Deleting contact %s from address book %s. Are you sure? \" \"(y/n): \" % ( selected_vcard , selected_vcard . address_book ) ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    selected_vcard . delete_vcard_file ( ) \n    print ( \"Contact %s deleted successfully\" % selected_vcard . get_full_name ( ) ) "}
{"5914": "\ndef merge_subcommand ( vcard_list , selected_address_books , search_terms , target_uid ) : \n    if target_uid != \"\" and search_terms != \"\" : \n        print ( \"You can not specify a target uid and target search terms for a \" \"merge.\" ) \n        sys . exit ( 1 ) \n    if target_uid != \"\" : \n        target_vcards = get_contacts ( selected_address_books , target_uid , method = \"uid\" ) \n        if len ( target_vcards ) != 1 : \n            if not target_vcards : \n                print ( \"Found no contact for target uid %s\" % target_uid ) \n            else : \n                print ( \"Found multiple contacts for target uid %s\" % target_uid ) \n                for vcard in target_vcards : \n                    print ( \"    %s: %s\" % ( vcard , vcard . get_uid ( ) ) ) \n            sys . exit ( 1 ) \n    else : \n        target_vcards = get_contact_list_by_user_selection ( selected_address_books , search_terms , 0 ) \n    source_vcard = choose_vcard_from_list ( \"Select contact from which to merge\" , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no source contact for merging\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"Merge from %s from address book %s\\n\\n\" % ( source_vcard , source_vcard . address_book ) ) \n    target_vcard = choose_vcard_from_list ( \"Select contact into which to merge\" , target_vcards ) \n    if target_vcard is None : \n        print ( \"Found no target contact for merging\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"Merge into %s from address book %s\\n\\n\" % ( target_vcard , target_vcard . address_book ) ) \n    if source_vcard == target_vcard : \n        print ( \"The selected contacts are already identical\" ) \n    else : \n        merge_existing_contacts ( source_vcard , target_vcard , 1 ) "}
{"5915": "\ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no contact\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"%s contact %s from address book %s\" % ( action . title ( ) , source_vcard , source_vcard . address_book ) ) \n    if len ( target_address_book_list ) == 1 and target_address_book_list [ 0 ] == source_vcard . address_book : \n        print ( \"The address book %s already contains the contact %s\" % ( target_address_book_list [ 0 ] , source_vcard ) ) \n        sys . exit ( 1 ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if abook != source_vcard . address_book ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            print ( \"Error: address book list is empty\" ) \n            sys . exit ( 1 ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , 1 ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n    else : \n        if source_vcard == target_vcard : \n            print ( \"Target contact: %s\" % target_vcard ) \n            if action == \"move\" : \n                copy_contact ( source_vcard , selected_target_address_book , 1 ) \n            else : \n                print ( \"The selected contacts are already identical\" ) \n        else : \n            print ( \"The address book %s already contains the contact %s\\n\\n\" \"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n\" \"Possible actions:\\n\" \"  a: %s anyway\\n\" \"  m: Merge from source into target contact\\n\" \"  o: Overwrite target contact\\n\" \"  q: Quit\" % ( target_vcard . address_book , source_vcard , source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , \"Move\" if action == \"move\" else \"Copy\" ) ) \n            while 1 : \n                input_string = input ( \"Your choice: \" ) \n                if input_string . lower ( ) == \"a\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    break \n                if input_string . lower ( ) == \"o\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    target_vcard . delete_vcard_file ( ) \n                    break \n                if input_string . lower ( ) == \"m\" : \n                    merge_existing_contacts ( source_vcard , target_vcard , action == \"move\" ) \n                    break \n                if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break "}
{"5917": "\ndef _convert_boolean_config_value ( config , name , default = 1 ) : \n    if name not in config : \n        config [ name ] = default \n    elif config [ name ] == \"yes\" : \n        config [ name ] = 1 \n    elif config [ name ] == \"no\" : \n        config [ name ] = 0 \n    else : \n        raise ValueError ( \"Error in config file\\nInvalid value for %s \" \"parameter\\nPossible values: yes, no\" % name ) "}
{"5934": "\ndef _find_vcard_files ( self , search = None , search_in_source_files = 0 ) : \n    files = glob . glob ( os . path . join ( self . path , \"*.vcf\" ) ) \n    if search and search_in_source_files : \n        for filename in files : \n            with open ( filename , \"r\" ) as filehandle : \n                if re . search ( search , filehandle . read ( ) , re . IGNORECASE | re . DOTALL ) : \n                    yield filename \n    else : \n        yield from files "}
{"5935": "\ndef load ( self , query = None , search_in_source_files = 0 ) : \n    if self . _loaded : \n        return \n    logging . debug ( 'Loading Vdir %s with query %s' , self . name , query ) \n    errors = 0 \n    for filename in self . _find_vcard_files ( search = query , search_in_source_files = search_in_source_files ) : \n        try : \n            card = CarddavObject . from_file ( self , filename , self . _private_objects , self . _localize_dates ) \n        except ( IOError , vobject . base . ParseError ) as err : \n            verb = \"open\" if isinstance ( err , IOError ) else \"parse\" \n            logging . debug ( \"Error: Could not %s file %s\\n%s\" , verb , filename , err ) \n            if self . _skip : \n                errors += 1 \n            else : \n                logging . error ( \"The vcard file %s of address book %s could not be \" \"parsed\\nUse --debug for more information or \" \"--skip-unparsable to proceed\" , filename , self . name ) \n                sys . exit ( 2 ) \n        else : \n            uid = card . get_uid ( ) \n            if not uid : \n                logging . warning ( \"Card %s from address book %s has no UID \" \"and will not be availbale.\" , card , self . name ) \n            elif uid in self . contacts : \n                logging . warning ( \"Card %s and %s from address book %s have the same \" \"UID. The former will not be availbale.\" , card , self . contacts [ uid ] , self . name ) \n            else : \n                self . contacts [ uid ] = card \n    self . _loaded = 1 \n    if errors : \n        logging . warning ( \"%d of %d vCard files of address book %s could not be parsed.\" , errors , len ( self . contacts ) + errors , self ) \n    logging . debug ( 'Loded %s contacts from address book %s.' , len ( self . contacts ) , self . name ) "}
{"5940": "\ndef dispatch ( parser , argv = None , add_help_command = 1 , completion = 1 , pre_call = None , output_file = sys . stdout , errors_file = sys . stderr , raw_output = 0 , namespace = None , skip_unknown_args = 0 ) : \n    if completion : \n        autocomplete ( parser ) \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    if add_help_command : \n        if argv and argv [ 0 ] == 'help' : \n            argv . pop ( 0 ) \n            argv . append ( '--help' ) \n    if skip_unknown_args : \n        parse_args = parser . parse_known_args \n    else : \n        parse_args = parser . parse_args \n    if not namespace : \n        namespace = ArghNamespace ( ) \n    namespace_obj = parse_args ( argv , namespace = namespace ) \n    function = _get_function_from_namespace_obj ( namespace_obj ) \n    if function : \n        lines = _execute_command ( function , namespace_obj , errors_file , pre_call = pre_call ) \n    else : \n        lines = [ parser . format_usage ( ) ] \n    if output_file is None : \n        if sys . version_info < ( 3 , 0 ) : \n            f = compat . BytesIO ( ) \n        else : \n            f = compat . StringIO ( ) \n    else : \n        f = output_file \n    for line in lines : \n        io . dump ( line , f ) \n        if not raw_output : \n            io . dump ( '\\n' , f ) \n    if output_file is None : \n        f . seek ( 0 ) \n        return f . read ( ) "}
{"5944": "\ndef add_commands ( parser , functions , namespace = None , namespace_kwargs = None , func_kwargs = None , title = None , description = None , help = None ) : \n    if DEST_FUNCTION in parser . _defaults : \n        _require_support_for_default_command_with_subparsers ( ) \n    namespace_kwargs = namespace_kwargs or { } \n    if title : \n        warnings . warn ( 'argument `title` is deprecated in add_commands(),' ' use `parser_kwargs` instead' , DeprecationWarning ) \n        namespace_kwargs [ 'description' ] = title \n    if help : \n        warnings . warn ( 'argument `help` is deprecated in add_commands(),' ' use `parser_kwargs` instead' , DeprecationWarning ) \n        namespace_kwargs [ 'help' ] = help \n    if description : \n        warnings . warn ( 'argument `description` is deprecated in add_commands(),' ' use `parser_kwargs` instead' , DeprecationWarning ) \n        namespace_kwargs [ 'description' ] = description \n    subparsers_action = get_subparsers ( parser , create = 1 ) \n    if namespace : \n        subsubparser_kw = { 'help' : namespace_kwargs . get ( 'title' ) , } \n        subsubparser = subparsers_action . add_parser ( namespace , ** subsubparser_kw ) \n        subparsers_action = subsubparser . add_subparsers ( ** namespace_kwargs ) \n    else : \n        assert not namespace_kwargs , ( '`parser_kwargs` only makes sense ' 'with `namespace`.' ) \n    for func in functions : \n        cmd_name , func_parser_kwargs = _extract_command_meta_from_func ( func ) \n        if func_kwargs : \n            func_parser_kwargs . update ( func_kwargs ) \n        command_parser = subparsers_action . add_parser ( cmd_name , ** func_parser_kwargs ) \n        set_default_command ( command_parser , func ) "}
{"5947": "\ndef confirm ( action , default = None , skip = 0 ) : \n    MAX_ITERATIONS = 3 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , 1 : ( 'Y' , 'n' ) , 0 : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = 1 \n                while not choice and cnt < MAX_ITERATIONS : \n                    choice = safe_input ( prompt ) \n                    cnt += 1 \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return 1 \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return 0 \n    if default is not None : \n        return default \n    return None "}
{"5955": "\ndef estimate_work_lua ( conn , index , prefix ) : \n    if index . endswith ( ':idx' ) : \n        args = [ ] if not prefix else list ( prefix ) \n        if args : \n            args [ 0 ] = '-inf' if args [ 0 ] is None else repr ( float ( args [ 0 ] ) ) \n            args [ 1 ] = 'inf' if args [ 1 ] is None else repr ( float ( args [ 1 ] ) ) \n        return _estimate_work_lua ( conn , [ index ] , args , force_eval = 1 ) \n    elif index . endswith ( ':geo' ) : \n        return _estimate_work_lua ( conn , [ index ] , filter ( None , [ prefix ] ) , force_eval = 1 ) \n    start , end = _start_end ( prefix ) \n    return _estimate_work_lua ( conn , [ index ] , [ start , '(' + end ] , force_eval = 1 ) "}
{"5960": "\ndef refresh_indices ( model , block_size = 100 ) : \n    conn = _connect ( model ) \n    max_id = int ( conn . get ( '%s:%s:' % ( model . _namespace , model . _pkey ) ) or '0' ) \n    block_size = max ( block_size , 10 ) \n    for i in range ( 1 , max_id + 1 , block_size ) : \n        models = model . get ( list ( range ( i , i + block_size ) ) ) \n        models \n        session . commit ( all = 1 ) \n        yield min ( i + block_size , max_id ) , max_id "}
{"5961": "\ndef clean_old_index ( model , block_size = 100 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2 ] ) ) \n    has_hscan = version >= [ 2 , 8 ] \n    pipe = conn . pipeline ( 1 ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10 ) \n    force_hscan = kwargs . get ( 'force_hscan' , 0 ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = 0 \n        while cursor != b'0' : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , 0 , 0 ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned += block_size \n            if scanned > max_id : \n                max_id = scanned + 1 \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while cursor != b'0' : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , 1 , 0 ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned += block_size \n                if scanned > max_id : \n                    max_id = scanned + 1 \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        for i in range ( 1 , max_id + 1 , block_size ) : \n            ids = list ( range ( i , min ( i + block_size , max_id + 1 ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - 1 ) , max_id \n    yield max_id , max_id "}
{"5965": "\ndef save ( self , full = 0 , force = 0 ) : \n    was_new = self . _new \n    if was_new : \n        self . _before_insert ( ) \n    else : \n        self . _before_update ( ) \n    new = self . to_dict ( ) \n    ret , data = self . _apply_changes ( self . _last , new , full or self . _new or force , is_new = self . _new or force ) \n    self . _last = data \n    self . _new = 0 \n    self . _modified = 0 \n    self . _deleted = 0 \n    if was_new : \n        self . _after_insert ( ) \n    else : \n        self . _after_update ( ) \n    return ret "}
{"5966": "\ndef delete ( self , ** kwargs ) : \n    if kwargs . get ( 'skip_on_delete_i_really_mean_it' ) is not SKIP_ON_DELETE : \n        self . _before_delete ( ) \n        _on_delete ( self ) \n    session . forget ( self ) \n    self . _apply_changes ( self . _last , { } , delete = 1 , _conn = kwargs . get ( '_conn' ) ) \n    self . _modified = 1 \n    self . _deleted = 1 \n    if kwargs . get ( 'skip_on_delete_i_really_mean_it' ) is not SKIP_ON_DELETE : \n        self . _after_delete ( ) "}
{"5967": "\ndef get ( cls , ids ) : \n    conn = _connect ( cls ) \n    single = not isinstance ( ids , ( list , tuple , set , frozenset ) ) \n    if single : \n        ids = [ ids ] \n    pks = [ '%s:%s' % ( cls . _namespace , id ) for id in map ( int , ids ) ] \n    out = list ( map ( session . get , pks ) ) \n    if None in out : \n        pipe = conn . pipeline ( 1 ) \n        idxs = [ ] \n        for i , data in enumerate ( out ) : \n            if data is None : \n                idxs . append ( i ) \n                pipe . hgetall ( pks [ i ] ) \n        for i , data in zip ( idxs , pipe . execute ( ) ) : \n            if data : \n                if six . PY3 : \n                    data = dict ( ( k . decode ( ) , v . decode ( ) ) for k , v in data . items ( ) ) \n                out [ i ] = cls ( _loading = 1 , ** data ) \n        out = [ x for x in out if x ] \n    if single : \n        return out [ 0 ] if out else None \n    return out "}
{"5972": "\ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = 1 ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while 1 : \n        try : \n            call_item = call_queue . get ( block = 1 , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = 0 ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                print ( previous_tb ) \n            sys . exit ( 1 ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = 1 ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = 1 ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        else : \n            if ( ( _last_memory_leak_check is None ) or ( time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY ) ) : \n                gc . collect ( ) \n                _last_memory_leak_check = time ( ) "}
{"5973": "\ndef _add_call_item_to_queue ( pending_work_items , running_work_items , work_ids , call_queue ) : \n    while 1 : \n        if call_queue . full ( ) : \n            return \n        try : \n            work_id = work_ids . get ( block = 0 ) \n        except queue . Empty : \n            return \n        else : \n            work_item = pending_work_items [ work_id ] \n            if work_item . future . set_running_or_notify_cancel ( ) : \n                running_work_items += [ work_id ] \n                call_queue . put ( _CallItem ( work_id , work_item . fn , work_item . args , work_item . kwargs ) , block = 1 ) \n            else : \n                del pending_work_items [ work_id ] \n                continue "}
{"5975": "\ndef wrap_non_picklable_objects ( obj , keep_wrapper = 1 ) : \n    if not cloudpickle : \n        raise ImportError ( \"could not import cloudpickle. Please install \" \"cloudpickle to allow extended serialization. \" \"(`pip install cloudpickle`).\" ) \n    if inspect . isclass ( obj ) : \n        class CloudpickledClassWrapper ( CloudpickledObjectWrapper ) : \n            def __init__ ( self , * args , ** kwargs ) : \n                self . _obj = obj ( * args , ** kwargs ) \n                self . _keep_wrapper = keep_wrapper \n        CloudpickledClassWrapper . __name__ = obj . __name__ \n        return CloudpickledClassWrapper \n    return _wrap_non_picklable_objects ( obj , keep_wrapper = keep_wrapper ) "}
{"5976": "\ndef start ( self , initializer = None , initargs = ( ) ) : \n    assert self . _state . value == State . INITIAL \n    if ( initializer is not None and not hasattr ( initializer , '__call__' ) ) : \n        raise TypeError ( 'initializer must be a callable' ) \n    reader , writer = mp . Pipe ( duplex = 0 ) \n    self . _process = Process ( target = type ( self ) . _run_server , args = ( self . _registry , self . _address , bytes ( self . _authkey ) , self . _serializer , writer , initializer , initargs ) , ) \n    ident = ':' . join ( str ( i ) for i in self . _process . _identity ) \n    self . _process . name = type ( self ) . __name__ + '-' + ident \n    self . _process . start ( ) \n    writer . close ( ) \n    self . _address = reader . recv ( ) \n    reader . close ( ) \n    self . _state . value = State . STARTED \n    self . shutdown = mp . util . Finalize ( self , type ( self ) . _finalize_manager , args = ( self . _process , self . _address , self . _authkey , self . _state , self . _Client ) , exitpriority = 0 ) "}
{"5978": "\ndef get_reusable_executor ( max_workers = None , context = None , timeout = 10 , kill_workers = 0 , reuse = \"auto\" , job_reducers = None , result_reducers = None , initializer = None , initargs = ( ) ) : \n    with _executor_lock : \n        global _executor , _executor_kwargs \n        executor = _executor \n        if max_workers is None : \n            if reuse is 1 and executor is not None : \n                max_workers = executor . _max_workers \n            else : \n                max_workers = cpu_count ( ) \n        elif max_workers <= 0 : \n            raise ValueError ( \"max_workers must be greater than 0, got {}.\" . format ( max_workers ) ) \n        if isinstance ( context , STRING_TYPE ) : \n            context = get_context ( context ) \n        if context is not None and context . get_start_method ( ) == \"fork\" : \n            raise ValueError ( \"Cannot use reusable executor with the 'fork' \" \"context\" ) \n        kwargs = dict ( context = context , timeout = timeout , job_reducers = job_reducers , result_reducers = result_reducers , initializer = initializer , initargs = initargs ) \n        if executor is None : \n            mp . util . debug ( \"Create a executor with max_workers={}.\" . format ( max_workers ) ) \n            executor_id = _get_next_executor_id ( ) \n            _executor_kwargs = kwargs \n            _executor = executor = _ReusablePoolExecutor ( _executor_lock , max_workers = max_workers , executor_id = executor_id , ** kwargs ) \n        else : \n            if reuse == 'auto' : \n                reuse = kwargs == _executor_kwargs \n            if ( executor . _flags . broken or executor . _flags . shutdown or not reuse ) : \n                if executor . _flags . broken : \n                    reason = \"broken\" \n                elif executor . _flags . shutdown : \n                    reason = \"shutdown\" \n                else : \n                    reason = \"arguments have changed\" \n                mp . util . debug ( \"Creating a new executor with max_workers={} as the \" \"previous instance cannot be reused ({}).\" . format ( max_workers , reason ) ) \n                executor . shutdown ( wait = 1 , kill_workers = kill_workers ) \n                _executor = executor = _executor_kwargs = None \n                return get_reusable_executor ( max_workers = max_workers , ** kwargs ) \n            else : \n                mp . util . debug ( \"Reusing existing executor with max_workers={}.\" . format ( executor . _max_workers ) ) \n                executor . _resize ( max_workers ) \n    return executor "}
{"5980": "\ndef get_preparation_data ( name , init_main_module = 1 ) : \n    _check_not_importing_main ( ) \n    d = dict ( log_to_stderr = util . _log_to_stderr , authkey = bytes ( process . current_process ( ) . authkey ) , ) \n    if util . _logger is not None : \n        d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) \n        if len ( util . _logger . handlers ) > 0 : \n            h = util . _logger . handlers [ 0 ] \n            d [ 'log_fmt' ] = h . formatter . _fmt \n    sys_path = [ p for p in sys . path ] \n    try : \n        i = sys_path . index ( '' ) \n    except ValueError : \n        pass \n    else : \n        sys_path [ i ] = process . ORIGINAL_DIR \n    d . update ( name = name , sys_path = sys_path , sys_argv = sys . argv , orig_dir = process . ORIGINAL_DIR , dir = os . getcwd ( ) ) \n    if sys . platform != \"win32\" : \n        from . import semaphore_tracker \n        semaphore_tracker . ensure_running ( ) \n        d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid \n    if init_main_module : \n        main_module = sys . modules [ '__main__' ] \n        try : \n            main_mod_name = getattr ( main_module . __spec__ , \"name\" , None ) \n        except BaseException : \n            main_mod_name = None \n        if main_mod_name is not None : \n            d [ 'init_main_from_name' ] = main_mod_name \n        elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : \n            main_path = getattr ( main_module , '__file__' , None ) \n            if main_path is not None : \n                if ( not os . path . isabs ( main_path ) and process . ORIGINAL_DIR is not None ) : \n                    main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) \n                d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) \n                d [ 'main_path' ] = d [ 'init_main_from_path' ] \n    return d "}
{"5990": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        curframe = self . proc . curframe \n        if curframe : \n            line_no = inspect . getlineno ( curframe ) \n            offset = curframe . f_lasti \n            self . msg ( \"PC offset is %d.\" % offset ) \n            offset = max ( offset , 0 ) \n            code = curframe . f_code \n            co_code = code . co_code \n            disassemble_bytes ( self . msg , self . msg_nocr , co_code , offset , line_no , line_no - 1 , line_no + 1 , constants = code . co_consts , cells = code . co_cellvars , varnames = code . co_varnames , freevars = code . co_freevars , linestarts = dict ( findlinestarts ( code ) ) , end_offset = offset + 10 ) \n            pass \n        pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return 0 "}
{"5992": "\ndef arg_split ( s , posix = 0 ) : \n    args_list = [ [ ] ] \n    if isinstance ( s , bytes ) : \n        s = s . decode ( \"utf-8\" ) \n    lex = shlex . shlex ( s , posix = posix ) \n    lex . whitespace_split = 1 \n    args = list ( lex ) \n    for arg in args : \n        if ';;' == arg : \n            args_list . append ( [ ] ) \n        else : \n            args_list [ - 1 ] . append ( arg ) \n            pass \n        pass \n    return args_list "}
{"5993": "\ndef get_stack ( f , t , botframe , proc_obj = None ) : \n    exclude_frame = lambda f : 0 \n    if proc_obj : \n        settings = proc_obj . debugger . settings \n        if not settings [ 'dbg_trepan' ] : \n            exclude_frame = lambda f : proc_obj . core . ignore_filter . is_included ( f ) \n            pass \n        pass \n    stack = [ ] \n    if t and t . tb_frame is f : \n        t = t . tb_next \n    while f is not None : \n        if exclude_frame ( f ) : \n            break \n        stack . append ( ( f , f . f_lineno ) ) \n        f = f . f_back \n        pass \n    stack . reverse ( ) \n    i = max ( 0 , len ( stack ) - 1 ) \n    while t is not None : \n        stack . append ( ( t . tb_frame , t . tb_lineno ) ) \n        t = t . tb_next \n        pass \n    return stack , i "}
{"5994": "\ndef run_hooks ( obj , hooks , * args ) : \n    for hook in hooks : \n        if hook ( obj , * args ) : \n            return 1 \n        pass \n    return 0 "}
{"5998": "\ndef process_commands ( self ) : \n    if self . core . execution_status != 'No program' : \n        self . setup ( ) \n        self . location ( ) \n        pass \n    leave_loop = run_hooks ( self , self . preloop_hooks ) \n    self . continue_running = 0 \n    while not leave_loop : \n        try : \n            run_hooks ( self , self . precmd_hooks ) \n            leave_loop = self . process_command ( ) \n            if leave_loop or self . continue_running : \n                break \n        except EOFError : \n            if len ( self . debugger . intf ) > 1 : \n                del self . debugger . intf [ - 1 ] \n                self . last_command = '' \n            else : \n                if self . debugger . intf [ - 1 ] . output : \n                    self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) \n                    raise Mexcept . DebuggerQuit \n                    pass \n                break \n            pass \n        pass \n    return run_hooks ( self , self . postcmd_hooks ) "}
{"6004": "\ndef disassemble ( msg , msg_nocr , section , co , lasti = - 1 , start_line = - 1 , end_line = None , relative_pos = 0 , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    return disassemble_bytes ( msg , msg_nocr , co . co_code , lasti , co . co_firstlineno , start_line , end_line , relative_pos , co . co_varnames , co . co_names , co . co_consts , co . co_cellvars , co . co_freevars , dict ( findlinestarts ( co ) ) , highlight , start_offset = start_offset , end_offset = end_offset ) "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - 1 , cur_line = 0 , start_line = - 1 , end_line = None , relative_pos = 0 , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    statement_count = 10000 \n    if end_line is None : \n        end_line = 10000 \n    elif relative_pos : \n        end_line += start_line - 1 \n        pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if start_line > cur_line : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and offset > end_offset : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( start_line > cur_line ) or start_offset and start_offset > offset ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count -= 1 \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( cur_line > end_line ) or ( end_offset and offset > end_offset ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and start_offset <= offset : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if offset == lasti : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6007": "\ndef get_call_function_name ( frame ) : \n    f_back = frame . f_back \n    if not f_back : \n        return None \n    if 'CALL_FUNCTION' != Mbytecode . op_at_frame ( f_back ) : \n        return None \n    co = f_back . f_code \n    code = co . co_code \n    linestarts = dict ( dis . findlinestarts ( co ) ) \n    offset = f_back . f_lasti \n    while offset >= 0 : \n        if offset in linestarts : \n            op = code [ offset ] \n            offset += 1 \n            arg = code [ offset ] \n            extended_arg = 0 \n            while 1 : \n                if PYTHON_VERSION >= 3.6 : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 8 ) \n                        continue \n                    arg = code [ offset ] + extended_arg \n                else : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 256 ) \n                        continue \n                    arg = code [ offset ] + code [ offset + 1 ] * 256 + extended_arg \n                break \n            return co . co_names [ arg ] \n        offset -= 1 \n        pass \n    return None "}
{"6010": "\ndef short_help ( self , subcmd_cb , subcmd_name , label = 0 ) : \n    entry = self . lookup ( subcmd_name ) \n    if entry : \n        if label : \n            prefix = entry . name \n        else : \n            prefix = '' \n            pass \n        if hasattr ( entry , 'short_help' ) : \n            if prefix : \n                prefix += ' -- ' \n            self . cmd_obj . msg ( prefix + entry . short_help ) \n            pass \n        pass \n    else : \n        self . undefined_subcmd ( \"help\" , subcmd_name ) \n        pass \n    return "}
{"6013": "\ndef debug ( dbg_opts = None , start_opts = None , post_mortem = 1 , step_ignore = 1 , level = 0 ) : \n    if not isinstance ( Mdebugger . debugger_obj , Mdebugger . Trepan ) : \n        Mdebugger . debugger_obj = Mdebugger . Trepan ( dbg_opts ) \n        Mdebugger . debugger_obj . core . add_ignore ( debug , stop ) \n        pass \n    core = Mdebugger . debugger_obj . core \n    frame = sys . _getframe ( 0 + level ) \n    core . set_next ( frame ) \n    if start_opts and 'startup-profile' in start_opts and start_opts [ 'startup-profile' ] : \n        dbg_initfiles = start_opts [ 'startup-profile' ] \n        from trepan import options \n        options . add_startup_file ( dbg_initfiles ) \n        for init_cmdfile in dbg_initfiles : \n            core . processor . queue_startfile ( init_cmdfile ) \n    if not core . is_started ( ) : \n        core . start ( start_opts ) \n        pass \n    if post_mortem : \n        debugger_on_post_mortem ( ) \n        pass \n    if 0 == step_ignore : \n        frame = sys . _getframe ( 1 + level ) \n        core . stop_reason = 'at a debug() call' \n        old_trace_hook_suspend = core . trace_hook_suspend \n        core . trace_hook_suspend = 1 \n        core . processor . event_processor ( frame , 'line' , None ) \n        core . trace_hook_suspend = old_trace_hook_suspend \n    else : \n        core . step_ignore = step_ignore - 1 \n        pass \n    return "}
{"6015": "\ndef run ( self , args ) : \n    if not self . proc . curframe : \n        self . errmsg ( \"No line number information available.\" ) \n        return \n    if len ( args ) == 3 : \n        answer = self . lineinfo ( args [ 2 ] ) \n        if answer [ 0 ] : \n            item , filename , lineno = answer \n            if not os . path . isfile ( filename ) : \n                filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n            self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) \n        return \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    if not os . path . isfile ( filename ) : \n        filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n        pass \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) \n    msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) \n    if self . proc . event : \n        msg2 += ', %s event' % self . proc . event \n        pass \n    self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) \n    return 0 "}
{"6019": "\ndef get_onoff ( errmsg , arg , default = None , print_error = 1 ) : \n    if not arg : \n        if default is None : \n            if print_error : \n                errmsg ( \"Expecting 'on', 1, 'off', or 0. Got nothing.\" ) \n                pass \n            raise ValueError \n        return default \n    if arg == '1' or arg == 'on' : \n        return 1 \n    if arg == '0' or arg == 'off' : \n        return 0 \n    if print_error : \n        errmsg ( \"Expecting 'on', 1, 'off', or 0. Got: %s.\" % str ( arg ) ) \n    raise ValueError "}
{"6024": "\ndef run_show_val ( obj , name ) : \n    val = obj . debugger . settings [ obj . name ] \n    obj . msg ( \"%s is %s.\" % ( obj . name , obj . cmd . proc . _saferepr ( val ) , ) ) \n    return 0 "}
{"6028": "\ndef set_default_bg ( ) : \n    term = environ . get ( 'TERM' , None ) \n    if term : \n        if ( term . startswith ( 'xterm' , ) or term . startswith ( 'eterm' ) or term == 'dtterm' ) : \n            return 0 \n    return 1 "}
{"6029": "\ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        print ( \"midpoint\" , midpoint , 'vs' , ( 16 * 5 + 16 * g + 16 * b ) ) \n        midpoint = 383 if term and term == 'xterm-256color' else 117963 \n    if ( ( 16 * 5 + 16 * g + 16 * b ) < midpoint ) : \n        return 1 \n    else : \n        return 0 "}
{"6031": "\ndef all ( self ) : \n    found = 0 \n    s = [ ] \n    for display in self . list : \n        if not found : \n            s . append ( \"\"\"Auto-display expressions now in effect:Num Enb Expression\"\"\" ) \n            found = 1 \n            pass \n        s . append ( display . format ( ) ) \n    return s "}
{"6033": "\ndef format ( self , show_enabled = 1 ) : \n    what = '' \n    if show_enabled : \n        if self . enabled : \n            what += ' y ' \n        else : \n            what += ' n ' \n            pass \n        pass \n    if self . fmt : \n        what += self . fmt + ' ' \n        pass \n    what += self . arg \n    return '%3d: %s' % ( self . number , what ) "}
{"6037": "\ndef run ( self , args ) : \n    if len ( args ) == 1 : \n        position_str = '0' \n    elif len ( args ) == 2 : \n        name_or_id = args [ 1 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id , 0 ) \n        if frame is None : \n            position_str = name_or_id \n        else : \n            position_str = '0' \n            self . find_and_set_debugged_frame ( frame , thread_id ) \n            pass \n    elif len ( args ) == 3 : \n        name_or_id = args [ 1 ] \n        position_str = args [ 2 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id ) \n        if frame is None : \n            return \n        self . find_and_set_debugged_frame ( frame , thread_id ) \n        pass \n    self . one_arg_run ( position_str ) \n    return 0 "}
{"6038": "\ndef pprint_simple_array ( val , displaywidth , msg_nocr , msg , lineprefix = '' ) : \n    if type ( val ) != list : \n        return 0 \n    numeric = 1 \n    for i in range ( len ( val ) ) : \n        if not ( type ( val [ i ] ) in [ bool , float , int ] ) : \n            numeric = 0 \n            if not ( type ( val [ i ] ) in [ bool , float , int , bytes ] ) : \n                return 0 \n            pass \n        pass \n    mess = columnize ( [ repr ( v ) for v in val ] , opts = { \"arrange_array\" : 1 , \"lineprefix\" : lineprefix , \"displaywidth\" : int ( displaywidth ) - 3 , 'ljust' : not numeric } ) \n    msg_nocr ( mess ) \n    return 1 "}
{"6041": "\ndef canonic_signame ( name_num ) : \n    signum = lookup_signum ( name_num ) \n    if signum is None : \n        try : \n            num = int ( name_num ) \n            signame = lookup_signame ( num ) \n            if signame is None : \n                return None \n        except : \n            return 0 \n        return signame \n    signame = name_num . upper ( ) \n    if not signame . startswith ( 'SIG' ) : \n        return 'SIG' + signame \n    return signame "}
{"6042": "\ndef set_signal_replacement ( self , signum , handle ) : \n    signame = lookup_signame ( signum ) \n    if signame is None : \n        self . dbgr . intf [ - 1 ] . errmsg ( ( \"%s is not a signal number\" \" I know about.\" ) % signum ) \n        return 0 \n    self . sigs [ signame ] . pass_along = 1 \n    if self . check_and_adjust_sighandler ( signame , self . sigs ) : \n        self . sigs [ signame ] . old_handler = handle \n        return 1 \n    return 0 "}
{"6044": "\ndef info_signal ( self , args ) : \n    if len ( args ) == 0 : \n        return None \n    signame = args [ 0 ] \n    if signame in [ 'handle' , 'signal' ] : \n        if len ( args ) == 1 : \n            self . dbgr . core . processor . section ( self . header ) \n            for signame in self . siglist : \n                self . print_info_signal_entry ( signame ) \n            return 1 \n        else : \n            signame = args [ 1 ] \n            pass \n        pass \n    signame = self . is_name_or_number ( signame ) \n    self . dbgr . core . processor . section ( self . header ) \n    self . print_info_signal_entry ( signame ) \n    return 1 "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return 1 \n    args = arg . split ( ) \n    signame = args [ 0 ] \n    signame = self . is_name_or_number ( args [ 0 ] ) \n    if not signame : \n        return \n    if len ( args ) == 1 : \n        self . info_signal ( [ signame ] ) \n        return 1 \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ 1 : ] : \n        if attr . startswith ( 'no' ) : \n            on = 0 \n            attr = attr [ 2 : ] \n        else : \n            on = 1 \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        elif 'print' . startswith ( attr ) and len ( attr ) >= 2 : \n            self . handle_print ( signame , on ) \n        elif 'pass' . startswith ( attr ) : \n            self . handle_pass ( signame , on ) \n        elif 'ignore' . startswith ( attr ) : \n            self . handle_ignore ( signame , on ) \n        elif 'stack' . startswith ( attr ) : \n            self . handle_print_stack ( signame , on ) \n        else : \n            self . dbgr . intf [ - 1 ] . errmsg ( 'Invalid arguments' ) \n            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6047": "\ndef handle ( self , signum , frame ) : \n    if self . print_method : \n        self . print_method ( '\\nProgram received signal %s.' % self . signame ) \n    if self . print_stack : \n        import traceback \n        strings = traceback . format_stack ( frame ) \n        for s in strings : \n            if s [ - 1 ] == '\\n' : \n                s = s [ 0 : - 1 ] \n            self . print_method ( s ) \n            pass \n        pass \n    if self . b_stop : \n        core = self . dbgr . core \n        old_trace_hook_suspend = core . trace_hook_suspend \n        core . trace_hook_suspend = 1 \n        core . stop_reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) \n        core . processor . event_processor ( frame , 'signal' , signum ) \n        core . trace_hook_suspend = old_trace_hook_suspend \n        pass \n    if self . pass_along : \n        if self . old_handler : \n            self . old_handler ( signum , frame ) \n            pass \n        pass \n    return "}
{"6053": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        if mainfile : \n            part1 = \"Python program '%s' is stopped\" % mainfile \n        else : \n            part1 = 'Program is stopped' \n            pass \n        if self . proc . event : \n            msg = 'via a %s event.' % self . proc . event \n        else : \n            msg = '.' \n        self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        if self . proc . curframe : \n            self . msg ( \"PC offset is %d.\" % self . proc . curframe . f_lasti ) \n        if self . proc . event == 'return' : \n            val = self . proc . event_arg \n            part1 = 'Return value is' \n            self . msg ( Mmisc . wrapped_lines ( part1 , self . proc . _saferepr ( val ) , self . settings [ 'width' ] ) ) \n            pass \n        elif self . proc . event == 'exception' : \n            exc_type , exc_value , exc_tb = self . proc . event_arg \n            self . msg ( 'Exception type: %s' % self . proc . _saferepr ( exc_type ) ) \n            if exc_value : \n                self . msg ( 'Exception value: %s' % self . proc . _saferepr ( exc_value ) ) \n                pass \n            pass \n        self . msg ( 'It stopped %s.' % self . core . stop_reason ) \n        if self . proc . event in [ 'signal' , 'exception' , 'c_exception' ] : \n            self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) \n            pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return 0 "}
{"6055": "\ndef post_mortem ( exc = None , frameno = 1 , dbg = None ) : \n    if dbg is None : \n        if Mdebugger . debugger_obj is None : \n            Mdebugger . debugger_obj = Mdebugger . Trepan ( ) \n            pass \n        dbg = Mdebugger . debugger_obj \n        pass \n    re_bogus_file = re . compile ( \"^<.+>$\" ) \n    if exc [ 0 ] is None : \n        exc = get_last_or_frame_exception ( ) \n        if exc [ 0 ] is None : \n            print ( \"Can't find traceback for post_mortem \" \"in sys.last_traceback or sys.exec_info()\" ) \n            return \n        pass \n    exc_type , exc_value , exc_tb = exc \n    dbg . core . execution_status = ( 'Terminated with unhandled exception %s' % exc_type ) \n    if exc_tb is not None : \n        while exc_tb . tb_next is not None : \n            filename = exc_tb . tb_frame . f_code . co_filename \n            if ( dbg . mainpyfile and 0 == len ( dbg . mainpyfile ) and not re_bogus_file . match ( filename ) ) : \n                dbg . mainpyfile = filename \n                pass \n            exc_tb = exc_tb . tb_next \n            pass \n        dbg . core . processor . curframe = exc_tb . tb_frame \n        pass \n    if 0 == len ( dbg . program_sys_argv ) : \n        dbg . program_sys_argv = list ( sys . argv [ 1 : ] ) \n        dbg . program_sys_argv [ : 0 ] = [ dbg . mainpyfile ] \n    try : \n        f = exc_tb . tb_frame \n        if f and f . f_lineno != exc_tb . tb_lineno : \n            f = f . f_back \n        dbg . core . processor . event_processor ( f , 'exception' , exc , 'Trepan3k:pm' ) \n    except DebuggerRestart : \n        while 1 : \n            sys . argv = list ( dbg . _program_sys_argv ) \n            dbg . msg ( \"Restarting %s with arguments:\\n\\t%s\" % ( dbg . filename ( dbg . mainpyfile ) , \" \" . join ( dbg . _program_sys_argv [ 1 : ] ) ) ) \n            try : \n                dbg . run_script ( dbg . mainpyfile ) \n            except DebuggerRestart : \n                pass \n            pass \n    except DebuggerQuit : \n        pass \n    return "}
{"6064": "\ndef is_stop_here ( self , frame , event , arg ) : \n    lineno = frame . f_lineno \n    filename = frame . f_code . co_filename \n    if self . different_line and event == 'line' : \n        if self . last_lineno == lineno and self . last_filename == filename : \n            return 0 \n        pass \n    self . last_lineno = lineno \n    self . last_filename = filename \n    if self . stop_level is not None : \n        if frame != self . last_frame : \n            self . last_level = Mstack . count_frames ( frame ) \n            self . last_frame = frame \n            pass \n        if self . last_level > self . stop_level : \n            return 0 \n        elif self . last_level == self . stop_level and self . stop_on_finish and event in [ 'return' , 'c_return' ] : \n            self . stop_level = None \n            self . stop_reason = \"in return for 'finish' command\" \n            return 1 \n        pass \n    if self . _is_step_next_stop ( event ) : \n        self . stop_reason = 'at a stepping statement' \n        return 1 \n    return 0 "}
{"6065": "\ndef set_next ( self , frame , step_ignore = 0 , step_events = None ) : \n    self . step_events = None \n    self . stop_level = Mstack . count_frames ( frame ) \n    self . last_level = self . stop_level \n    self . last_frame = frame \n    self . stop_on_finish = 0 \n    self . step_ignore = step_ignore \n    return "}
{"6067": "\ndef run ( self , args ) : \n    if len ( args ) == 0 : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return 0 \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ 0 ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m += \" cached in debugger\" \n        if filename_cache [ filename ] != filename : \n            m += ' as:' \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m += '.' \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( len ( matches ) > 1 ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        elif len ( matches ) == 1 : \n            canonic_name = pyficache . unmap_file ( matches [ 0 ] ) \n            m += \" matched debugger cache file:\\n  \" + canonic_name \n            self . msg ( m ) \n        else : \n            self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if name == v ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ 1 : ] : \n        processed_arg = 0 \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = 1 \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = 1 \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = 0 , arrange_vertical = 0 , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = 1 \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6068": "\ndef checkfuncname ( b , frame ) : \n    if not b . funcname : \n        if b . line != frame . f_lineno : \n            return 0 \n        return 1 \n    if frame . f_code . co_name != b . funcname : \n        return 0 \n    if not b . func_first_executable_line : \n        b . func_first_executable_line = frame . f_lineno \n    if b . func_first_executable_line != frame . f_lineno : \n        return 0 \n    return 1 "}
{"6069": "\ndef delete_breakpoint ( self , bp ) : \n    bpnum = bp . number \n    self . bpbynumber [ bpnum ] = None \n    index = ( bp . filename , bp . line ) \n    if index not in self . bplist : \n        return 0 \n    self . bplist [ index ] . remove ( bp ) \n    if not self . bplist [ index ] : \n        del self . bplist [ index ] \n    return 1 "}
{"6070": "\ndef delete_breakpoint_by_number ( self , bpnum ) : \n    success , msg , bp = self . get_breakpoint ( bpnum ) \n    if not success : \n        return 0 , msg \n    self . delete_breakpoint ( bp ) \n    return ( 1 , '' ) "}
{"6071": "\ndef en_disable_all_breakpoints ( self , do_enable = 1 ) : \n    bp_list = [ bp for bp in self . bpbynumber if bp ] \n    bp_nums = [ ] \n    if do_enable : \n        endis = 'en' \n    else : \n        endis = 'dis' \n        pass \n    if not bp_list : \n        return \"No breakpoints to %sable\" % endis \n    for bp in bp_list : \n        bp . enabled = do_enable \n        bp_nums . append ( str ( bp . number ) ) \n        pass \n    return ( \"Breakpoints %sabled: %s\" % ( endis , \", \" . join ( bp_nums ) ) ) "}
{"6072": "\ndef en_disable_breakpoint_by_number ( self , bpnum , do_enable = 1 ) : \n    success , msg , bp = self . get_breakpoint ( bpnum ) \n    if not success : \n        return success , msg \n    if do_enable : \n        endis = 'en' \n    else : \n        endis = 'dis' \n        pass \n    if bp . enabled == do_enable : \n        return ( 0 , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) \n    bp . enabled = do_enable \n    return ( 1 , '' ) "}
{"6076": "\ndef confirm ( self , prompt , default ) : \n    while 1 : \n        try : \n            self . write_confirm ( prompt , default ) \n            reply = self . readline ( '' ) . strip ( ) . lower ( ) \n        except EOFError : \n            return default \n        if reply in ( 'y' , 'yes' ) : \n            return 1 \n        elif reply in ( 'n' , 'no' ) : \n            return 0 \n        else : \n            self . msg ( \"Please answer y or n.\" ) \n            pass \n        pass \n    return default "}
{"6080": "\ndef restore_original_login ( request ) : \n    original_session = request . session . get ( la_settings . USER_SESSION_FLAG ) \n    logout ( request ) \n    if not original_session : \n        return \n    try : \n        original_user_pk = signer . unsign ( original_session , max_age = timedelta ( days = la_settings . USER_SESSION_DAYS_TIMESTAMP ) . total_seconds ( ) ) \n        user = get_user_model ( ) . objects . get ( pk = original_user_pk ) \n        messages . info ( request , la_settings . MESSAGE_LOGIN_REVERT . format ( username = user . __dict__ [ username_field ] ) , extra_tags = la_settings . MESSAGE_EXTRA_TAGS , ) \n        login_as ( user , request , store_original_user = 0 ) \n        if la_settings . USER_SESSION_FLAG in request . session : \n            del request . session [ la_settings . USER_SESSION_FLAG ] \n    except SignatureExpired : \n        pass "}
{"6082": "\ndef iterate_docs ( client , expanded = 0 , progress = 0 ) : \n    num_docs = client . get ( ) [ 'document_count' ] \n    progress_bar = None \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Downloading documents' , total = num_docs ) \n        for offset in range ( 0 , num_docs , DOCS_PER_BATCH ) : \n            response = client . get ( 'docs' , offset = offset , limit = DOCS_PER_BATCH ) \n            docs = response [ 'result' ] \n            for doc in docs : \n                if expanded : \n                    for field in UNNECESSARY_FIELDS : \n                        doc . pop ( field , None ) \n                else : \n                    doc = { field : doc [ field ] for field in CONCISE_FIELDS } \n                if progress : \n                    progress_bar . update ( ) \n                yield doc \n    finally : \n        if progress : \n            progress_bar . close ( ) "}
{"6084": "\ndef transcode_to_stream ( input_filename , date_format = None ) : \n    tmp = tempfile . TemporaryFile ( ) \n    for entry in open_json_or_csv_somehow ( input_filename , date_format = date_format ) : \n        tmp . write ( json . dumps ( entry , ensure_ascii = 0 ) . encode ( 'utf-8' ) ) \n        tmp . write ( b'\\n' ) \n    tmp . seek ( 0 ) \n    return tmp "}
{"6098": "\ndef wait_for_build ( self , interval = 5 , path = None ) : \n    path = path or '' \n    start = time . time ( ) \n    next_log = 0 \n    while 1 : \n        response = self . get ( path ) [ 'last_build_info' ] \n        if not response : \n            raise ValueError ( 'This project is not building!' ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6099": "\ndef get_root_url ( url , warn = 1 ) : \n    parsed_url = urlparse ( url ) \n    if not parsed_url . scheme : \n        raise ValueError ( 'Please supply a full URL, beginning with http:// ' 'or https:// .' ) \n    root_url = '%s://%s/api/v4' % ( parsed_url . scheme , parsed_url . netloc ) \n    if warn and not parsed_url . path . startswith ( '/api/v4' ) : \n        logger . warning ( 'Using %s as the root url' % root_url ) \n    return root_url "}
{"6100": "\ndef save_token ( self , token_file = None ) : \n    tokens = self . _json_request ( 'get' , self . root_url + '/user/tokens/' ) \n    long_lived = [ token [ 'type' ] == 'long_lived' for token in tokens ] \n    if any ( long_lived ) : \n        dic = tokens [ long_lived . index ( 1 ) ] \n    else : \n        dic = self . _json_request ( 'post' , self . root_url + '/user/tokens/' ) \n    token = dic [ 'token' ] \n    token_file = token_file or get_token_filename ( ) \n    if os . path . exists ( token_file ) : \n        saved_tokens = json . load ( open ( token_file ) ) \n    else : \n        saved_tokens = { } \n    saved_tokens [ urlparse ( self . root_url ) . netloc ] = token \n    directory , filename = os . path . split ( token_file ) \n    if directory and not os . path . exists ( directory ) : \n        os . makedirs ( directory ) \n    with open ( token_file , 'w' ) as f : \n        json . dump ( saved_tokens , f ) \n    return token "}
{"6106": "\ndef wait_for ( self , job_id , base_path = None , interval = 5 ) : \n    if base_path is None : \n        base_path = 'jobs/id' \n    path = '%s%d' % ( ensure_trailing_slash ( base_path ) , job_id ) \n    start = time . time ( ) \n    next_log = 0 \n    while 1 : \n        response = self . get ( path ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6111": "\ndef create_project_with_docs ( client , docs , language , name , account = None , progress = 0 ) : \n    description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) \n    if account is not None : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description , account_id = account , ) \n    else : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description ) \n    proj_id = proj_record [ 'project_id' ] \n    proj_client = client . client_for_path ( 'projects/' + proj_id ) \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Uploading documents' ) \n        else : \n            progress_bar = None \n        for batch in _batches ( docs , BATCH_SIZE ) : \n            docs_to_upload = [ _simplify_doc ( doc ) for doc in batch ] \n            proj_client . post ( 'upload' , docs = docs_to_upload ) \n            if progress : \n                progress_bar . update ( BATCH_SIZE ) \n    finally : \n        if progress : \n            progress_bar . close ( ) \n    print ( 'The server is building project {!r}.' . format ( proj_id ) ) \n    proj_client . post ( 'build' ) \n    while 1 : \n        time . sleep ( 10 ) \n        proj_status = proj_client . get ( ) \n        build_info = proj_status [ 'last_build_info' ] \n        if 'success' in build_info : \n            if not build_info [ 'success' ] : \n                raise LuminosoServerError ( build_info [ 'reason' ] ) \n            return proj_status "}
{"6112": "\ndef upload_docs ( client , input_filename , language , name , account = None , progress = 0 ) : \n    docs = iterate_json_lines ( input_filename ) \n    return create_project_with_docs ( client , docs , language , name , account , progress = progress ) "}
{"6113": "\ndef _main ( argv ) : \n    parser = argparse . ArgumentParser ( description = DESCRIPTION , formatter_class = argparse . RawDescriptionHelpFormatter , ) \n    parser . add_argument ( '-b' , '--base-url' , default = URL_BASE , help = 'API root url, default: %s' % URL_BASE , ) \n    parser . add_argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) \n    parser . add_argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) \n    parser . add_argument ( '-t' , '--token' , help = \"API authentication token\" ) \n    parser . add_argument ( '-s' , '--save-token' , action = 'store_true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) \n    parser . add_argument ( 'input_filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) \n    parser . add_argument ( 'project_name' , nargs = '?' , default = None , help = 'What the project should be called' , ) \n    args = parser . parse_args ( argv ) \n    if args . save_token : \n        if not args . token : \n            raise ValueError ( \"error: no token provided\" ) \n        LuminosoClient . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) \n    client = LuminosoClient . connect ( url = args . base_url , token = args . token ) \n    name = args . project_name \n    if name is None : \n        name = input ( 'Enter a name for the project: ' ) \n        if not name : \n            print ( 'Aborting because no name was provided.' ) \n            return \n    result = upload_docs ( client , args . input_filename , args . language , name , account = args . account_id , progress = 1 , ) \n    print ( 'Project {!r} created with {} documents' . format ( result [ 'project_id' ] , result [ 'document_count' ] ) ) "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = 0 , stage = 0 ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        print ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == 0 : \n            print ( 'No such project exists!' ) \n            return \n        if len ( projects ) > 1 : \n            print ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ 0 ] [ 'project_id' ] \n        print ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000 ) : \n        counter += 1 \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        print ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        print ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6115": "\ndef upload_file ( filename , server , account , projname , language = None , username = None , password = None , append = 0 , stage = 0 , date_format = None ) : \n    stream = transcode_to_stream ( filename , date_format ) \n    upload_stream ( stream_json_lines ( stream ) , server , account , projname , language = language , username = username , password = password , append = append , stage = stage ) "}
{"6119": "\ndef _post_login_page ( self ) : \n    data = { 'IDToken1' : self . username , 'IDToken2' : self . password , 'SunQueryParamsString' : base64 . b64encode ( b'realm=particuliers' ) , 'encoded' : 'true' , 'gx_charset' : 'UTF-8' } \n    try : \n        self . _session . post ( LOGIN_URL , data = data , allow_redirects = 0 , timeout = self . _timeout ) \n    except OSError : \n        raise PyLinkyError ( \"Can not submit login form\" ) \n    if 'iPlanetDirectoryPro' not in self . _session . cookies : \n        raise PyLinkyError ( \"Login error: Please check your username/password.\" ) \n    return 1 "}
{"6120": "\ndef _get_data ( self , p_p_resource_id , start_date = None , end_date = None ) : \n    data = { '_' + REQ_PART + '_dateDebut' : start_date , '_' + REQ_PART + '_dateFin' : end_date } \n    params = { 'p_p_id' : REQ_PART , 'p_p_lifecycle' : 2 , 'p_p_state' : 'normal' , 'p_p_mode' : 'view' , 'p_p_resource_id' : p_p_resource_id , 'p_p_cacheability' : 'cacheLevelPage' , 'p_p_col_id' : 'column-1' , 'p_p_col_pos' : 1 , 'p_p_col_count' : 3 } \n    try : \n        raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = 0 , timeout = self . _timeout ) \n        if 300 <= raw_res . status_code < 400 : \n            raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = 0 , timeout = self . _timeout ) \n    except OSError as e : \n        raise PyLinkyError ( \"Could not access enedis.fr: \" + str ( e ) ) \n    if raw_res . text is \"\" : \n        raise PyLinkyError ( \"No data\" ) \n    if 302 == raw_res . status_code and \"/messages/maintenance.html\" in raw_res . text : \n        raise PyLinkyError ( \"Site in maintenance\" ) \n    try : \n        json_output = raw_res . json ( ) \n    except ( OSError , json . decoder . JSONDecodeError , simplejson . errors . JSONDecodeError ) as e : \n        raise PyLinkyError ( \"Impossible to decode response: \" + str ( e ) + \"\\nResponse was: \" + str ( raw_res . text ) ) \n    if json_output . get ( 'etat' ) . get ( 'valeur' ) == 'erreur' : \n        raise PyLinkyError ( \"Enedis.fr answered with an error: \" + str ( json_output ) ) \n    return json_output . get ( 'graphe' ) "}
{"6125": "\ndef on_message ( self , message ) : \n    change = tornado . escape . json_decode ( message ) \n    ref = change . get ( 'ref' ) \n    if not ref : \n        return \n    node = self . view . xpath ( '//*[@ref=\"{}\"]' . format ( ref ) , first = 1 ) \n    if node is None : \n        return \n    if change . get ( 'type' ) and change . get ( 'name' ) : \n        if change [ 'type' ] == 'event' : \n            trigger = getattr ( node , change [ 'name' ] ) \n            trigger ( ) \n        if change [ 'type' ] == 'update' : \n            setattr ( node , change [ 'name' ] , change [ 'value' ] ) "}
{"6131": "\ndef init_widget ( self ) : \n    widget = self . widget \n    d = self . declaration \n    ref = d . ref \n    CACHE [ ref ] = atomref ( self ) \n    widget . set ( 'ref' , ref ) \n    if d . text : \n        self . set_text ( d . text ) \n    if d . tail : \n        self . set_tail ( d . tail ) \n    if d . style : \n        self . set_style ( d . style ) \n    if d . cls : \n        self . set_cls ( d . cls ) \n    if d . attrs : \n        self . set_attrs ( d . attrs ) \n    if d . id : \n        widget . set ( 'id' , d . id ) \n    if d . draggable : \n        self . set_draggable ( d . draggable ) \n    for name , member in d . members ( ) . items ( ) : \n        if not member . metadata : \n            continue \n        meta = member . metadata \n        if not ( meta . get ( 'd_member' ) and meta . get ( 'd_final' ) ) : \n            continue \n        elif not meta . get ( 'attr' , 1 ) : \n            continue \n        elif isinstance ( member , Event ) : \n            continue \n        value = getattr ( d , name ) \n        if value : \n            self . set_attribute ( name , value ) "}
{"6136": "\ndef set_attribute ( self , name , value ) : \n    if value is 1 : \n        self . widget . set ( name , name ) \n    elif value is 0 : \n        del self . widget . attrib [ name ] \n    else : \n        self . widget . set ( name , str ( value ) ) "}
{"6158": "\ndef remove_item_from_basket ( self , idx ) : \n    params = { 'basketItemId' : idx , 'wizardItemDelete' : 0 } \n    return self . __post ( '/Basket/RemoveBasketItem' , json = params ) "}
{"6165": "\ndef add_exit ( self ) : \n    if self . items : \n        if self . items [ - 1 ] is not self . exit_item : \n            self . items . append ( self . exit_item ) \n            return 1 \n    return 0 "}
{"6171": "\ndef top_group ( df , aggregate_by : List [ str ] , value : str , limit : int , order : str = 'asc' , function : str = 'sum' , group : Union [ str , List [ str ] ] = None ) : \n    aggregate_by = aggregate_by or [ ] \n    group_top = group or [ ] \n    df2 = df . groupby ( group_top + aggregate_by ) . agg ( function ) . reset_index ( ) \n    df2 = top ( df2 , group = group , value = value , limit = limit , order = order ) . reset_index ( drop = 1 ) \n    df2 = df2 [ group_top + aggregate_by ] \n    df = df2 . merge ( df , on = group_top + aggregate_by ) \n    return df "}
{"6174": "\ndef change_date_format ( df , * , column : str , output_format : str , input_format : str = None , new_column : str = None , new_time_zone = None ) : \n    new_column = new_column or column \n    df [ new_column ] = ( pd . to_datetime ( df [ column ] , format = input_format , utc = 1 ) . dt . tz_convert ( new_time_zone ) . dt . strftime ( output_format ) ) \n    return df "}
{"6176": "\ndef rank ( df , value_cols : Union [ str , List [ str ] ] , group_cols : List [ str ] = None , rank_cols_names : List [ str ] = None , method = 'min' , ascending : bool = 1 ) : \n    value_cols = [ value_cols ] if not isinstance ( value_cols , list ) else value_cols \n    for col in value_cols : \n        if not np . issubdtype ( df [ col ] . dtype , np . number ) : \n            raise TypeError ( col + \" specified in value_cols must be of numeric type\" ) \n    if rank_cols_names is None : \n        rank_cols_names = [ x + '_rank' for x in value_cols ] \n    if group_cols is None : \n        df [ rank_cols_names ] = df [ value_cols ] . rank ( method = method , ascending = ascending ) \n    else : \n        df [ rank_cols_names ] = ( df . groupby ( group_cols ) [ value_cols ] . rank ( method = method , ascending = ascending ) ) \n    if method != 'average' : \n        df [ rank_cols_names ] = df [ rank_cols_names ] . astype ( 'int' ) \n    return df "}
{"6177": "\ndef waterfall ( df , date : str , value : str , start : Dict [ str , str ] , end : Dict [ str , str ] , upperGroup : Dict [ str , str ] , insideGroup : Dict [ str , str ] = None , filters : List [ str ] = None ) : \n    if len ( df ) == 0 : \n        return df \n    if filters is not None : \n        if isinstance ( filters , str ) : \n            filters = [ filters ] \n        def sub_waterfall ( df ) : \n            wa_df = waterfall ( df , date , value , start , end , upperGroup , insideGroup ) \n            for filters_col in filters : \n                wa_df [ filters_col ] = df [ filters_col ] . values [ 0 ] \n            return wa_df \n        list_of_sub_df = [ df [ ( df [ filters ] . values == i ) . all ( axis = 1 ) ] for i in df [ filters ] . drop_duplicates ( ) . values ] \n        return pd . concat ( [ sub_waterfall ( df ) for df in list_of_sub_df ] , sort = 0 ) \n    groups = { 'upperGroup' : { 'type' : 'parent' , 'id' : 'upperGroup' , 'order' : { 'by' : [ 'upperGroup_order' , 'groups' ] , 'ascending' : [ 1 , 1 ] } , 'obj' : upperGroup } } \n    if insideGroup is not None : \n        groups [ 'insideGroup' ] = { 'type' : 'child' , 'id' : 'insideGroup' , 'order' : { 'by' : [ 'type' , 'insideGroup_order' , 'label' ] , 'ascending' : [ 0 , 1 , 1 ] } , 'obj' : insideGroup } \n    df = _compute_rename ( df , date , value , groups ) \n    agg_conf = { 'value' : sum } \n    agg_conf . update ( { f'{col}_label' : 'first' for col in groups . keys ( ) } ) \n    agg_conf . update ( { f'{col}_order' : 'first' for col in groups . keys ( ) } ) \n    df = df . groupby ( list ( groups . keys ( ) ) + [ 'date' ] ) . agg ( agg_conf ) . reset_index ( ) \n    df_start , df_end = _compute_start_end ( df , start , end ) \n    df = _compute_value_diff ( df , start , end , groups ) \n    middle = _compute_upper_group ( df ) \n    if insideGroup is not None : \n        middle = pd . concat ( [ middle , _compute_inside_group ( df ) ] ) \n    ret = _compute_order ( df_start , df_end , middle , groups ) \n    return ret "}
{"6183": "\ndef groupby ( df , * , group_cols : Union [ str , List [ str ] ] , aggregations : Dict [ str , Union [ str , List [ str ] ] ] ) : \n    df = df . groupby ( group_cols , as_index = 0 ) . agg ( aggregations ) \n    if df . columns . nlevels == 2 : \n        level_0 = df . columns . get_level_values ( 0 ) \n        level_1 = df . columns . get_level_values ( 1 ) \n        new_columns = [ ( f'{x}_{y}' if x else y ) for ( x , y ) in zip ( level_1 , level_0 ) ] \n        df . columns = new_columns \n    return df "}
{"6184": "\ndef cumsum ( df , new_column : str , column : str , index : list , date_column : str , date_format : str ) : \n    logging . getLogger ( __name__ ) . warning ( f\"DEPRECATED: use compute_cumsum\" ) \n    date_temp = '__date_temp__' \n    if isinstance ( index , str ) : \n        index = [ index ] \n    levels = list ( range ( 0 , len ( index ) ) ) \n    df [ date_temp ] = pd . to_datetime ( df [ date_column ] , format = date_format ) \n    reference_cols = [ date_temp , date_column ] \n    df = df . groupby ( index + reference_cols ) . sum ( ) \n    df [ new_column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) \n    df . reset_index ( inplace = 1 ) \n    del df [ date_temp ] \n    return df "}
{"6192": "\ndef combine_columns_aggregation ( df , id_cols : List [ str ] , cols_for_combination : Dict [ str , str ] , agg_func : Union [ str , List [ str ] , Dict [ str , str ] ] = 'sum' ) : \n    requesters_cols = list ( cols_for_combination . keys ( ) ) \n    requester_combination = [ list ( item ) for i in range ( 0 , len ( requesters_cols ) + 1 ) for item in itertools . combinations ( requesters_cols , i ) ] \n    dfs_result = [ ] \n    for comb in requester_combination : \n        df_tmp = df . groupby ( id_cols + comb ) . agg ( agg_func ) . reset_index ( ) \n        for key in ( set ( cols_for_combination . keys ( ) ) - set ( comb ) ) : \n            df_tmp [ key ] = cols_for_combination [ key ] \n        dfs_result . append ( df_tmp ) \n    return pd . concat ( dfs_result , sort = 0 , ignore_index = 1 ) "}
{"6194": "\ndef clean_cachedir_old_entries ( cachedir : StoreBackendBase , func_name : str , limit : int ) -> int : \n    if limit < 1 : \n        raise ValueError ( \"'limit' must be greater or equal to 1\" ) \n    cache_entries = get_cachedir_entries ( cachedir , func_name ) \n    cache_entries = sorted ( cache_entries , key = lambda e : e . last_access , reverse = 1 ) \n    cache_entries_to_remove = cache_entries [ limit : ] \n    for entry in cache_entries_to_remove : \n        shutil . rmtree ( entry . path , ignore_errors = 1 ) \n    return len ( cache_entries_to_remove ) "}
{"6195": "\ndef roll_up ( df , levels : List [ str ] , groupby_vars : List [ str ] , extra_groupby_cols : List [ str ] = None , var_name : str = 'type' , value_name : str = 'value' , agg_func : str = 'sum' , drop_levels : List [ str ] = None ) : \n    dfs = list ( ) \n    groupby_cols_cpy = list ( levels ) \n    levels_cpy = list ( levels ) \n    levels_cpy . reverse ( ) \n    extra_groupby_cols = extra_groupby_cols or [ ] \n    drop_levels = drop_levels or [ ] \n    previous_level = None \n    for top_level in levels_cpy : \n        gb_df = getattr ( df . groupby ( groupby_cols_cpy + extra_groupby_cols ) [ groupby_vars ] , agg_func ) ( ) . reset_index ( ) \n        gb_df [ var_name ] = top_level \n        gb_df [ value_name ] = gb_df [ top_level ] \n        dfs . append ( gb_df ) \n        if previous_level in drop_levels : \n            del dfs [ - 2 ] \n        previous_level = top_level \n        groupby_cols_cpy . pop ( ) \n    return pd . concat ( dfs , sort = 0 ) . reset_index ( ) "}
{"6196": "\ndef argmax ( df , column : str , groups : Union [ str , List [ str ] ] = None ) : \n    if groups is None : \n        df = df [ df [ column ] == df [ column ] . max ( ) ] . reset_index ( drop = 1 ) \n    else : \n        group_max = df . groupby ( groups ) [ column ] . transform ( 'max' ) \n        df = ( df . loc [ df [ column ] == group_max , : ] . drop_duplicates ( ) . reset_index ( drop = 1 ) ) \n    return df "}
{"6197": "\ndef argmin ( df , column : str , groups : Union [ str , List [ str ] ] = None ) : \n    if groups is None : \n        df = df [ df [ column ] == df [ column ] . min ( ) ] . reset_index ( drop = 1 ) \n    else : \n        group_min = df . groupby ( groups ) [ column ] . transform ( 'min' ) \n        df = ( df . loc [ df [ column ] == group_min , : ] . drop_duplicates ( ) . reset_index ( drop = 1 ) ) \n    return df "}
{"6205": "\ndef ada_family_core ( params , gparams , learning_rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum_regularization = 0.0001 ) : \n    _ , _ , _ , args = inspect . getargvalues ( inspect . currentframe ( ) ) \n    logging . info ( \"ada_family_core: %s\" % str ( args . items ( ) ) ) \n    free_parameters = [ ] \n    if method == \"FINETUNING_ADAGRAD\" : \n        method = \"ADAGRAD\" \n        gsum_regularization = 0 \n    oneMinusBeta = 1 - beta \n    gsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = 1 ) , dtype = FLOATX ) , name = \"gsum_%s\" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] \n    xsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = 1 ) , dtype = FLOATX ) , name = \"xsum_%s\" % param . name ) if method == 'ADADELTA' else None for param in params ] \n    if method == 'ADAGRAD' : \n        for gsum in gsums : \n            gsum . set_value ( gsum . get_value ( ) ** 0 ) \n    updates = OrderedDict ( ) \n    for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : \n        if method == 'ADADELTA' : \n            updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) \n            dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam \n            updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) \n            updates [ param ] = param * oneMinusBeta + dparam \n        elif method == 'ADAGRAD' : \n            updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum_regularization * gsum \n            updates [ param ] = param * oneMinusBeta - learning_rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) \n        else : \n            updates [ param ] = param * oneMinusBeta - gparam * learning_rate \n    if method == 'ADADELTA' : \n        free_parameters . extend ( gsums + xsums ) \n    elif method == 'ADAGRAD' : \n        free_parameters . extend ( gsums ) \n    for k in updates : \n        if updates [ k ] . dtype != FLOATX : \n            updates [ k ] = updates [ k ] . astype ( FLOATX ) \n    return updates . items ( ) , free_parameters "}
{"6215": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    iteration = 0 \n    while 1 : \n        if not iteration % self . config . test_frequency and test_set : \n            try : \n                self . test ( iteration , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not iteration % self . validation_frequency and valid_set : \n            try : \n                if not self . evaluate ( iteration , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        train_message = \"\" \n        try : \n            train_message = self . train_func ( train_set ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if not iteration % self . config . monitor_frequency : \n            logging . info ( 'monitor (iter=%i) %s' , iteration + 1 , train_message ) \n        iteration += 1 \n        if hasattr ( self . network , \"iteration_callback\" ) : \n            self . network . iteration_callback ( ) \n        yield train_message \n    if valid_set : \n        self . set_params ( self . best_params ) \n    if test_set : \n        self . test ( 0 , test_set ) "}
{"6230": "\ndef load_params ( self , path , exclude_free_params = 0 ) : \n    self . network . load_params ( path , exclude_free_params = exclude_free_params ) \n    self . best_params = self . copy_params ( ) \n    if self . network . train_logger . progress ( ) > 0 or self . network . train_logger . epoch ( ) > 0 : \n        self . skip ( self . network . train_logger . progress ( ) , self . network . train_logger . epoch ( ) - 1 ) "}
{"6231": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    self . _epoch = 0 \n    while 1 : \n        if self . _skip_epochs > 0 : \n            logging . info ( \"skipping one epoch ...\" ) \n            self . _skip_epochs -= 1 \n            self . _epoch += 1 \n            yield None \n            continue \n        if not self . _epoch % self . config . test_frequency and test_set : \n            try : \n                self . _run_test ( self . _epoch , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not self . _epoch % self . validation_frequency and valid_set : \n            try : \n                if not self . _run_valid ( self . _epoch , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        try : \n            costs = self . _run_train ( self . _epoch , train_set , train_size ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if np . isnan ( costs [ 0 ] [ 1 ] ) : \n            logging . info ( \"NaN detected in costs, rollback to last parameters\" ) \n            self . set_params ( * self . checkpoint ) \n        else : \n            self . _epoch += 1 \n            self . network . epoch_callback ( ) \n        yield dict ( costs ) \n    if valid_set and self . config . get ( \"save_best_parameters\" , 1 ) : \n        self . set_params ( * self . best_params ) \n    if test_set : \n        self . _run_test ( - 1 , test_set ) "}
{"6233": "\ndef _run_valid ( self , epoch , valid_set , dry_run = 0 , save_path = None ) : \n    costs = self . valid_step ( valid_set ) \n    _ , J = costs [ 0 ] \n    new_best = 0 \n    if self . best_cost - J > self . best_cost * self . min_improvement : \n        self . best_params = self . copy_params ( ) \n        new_best = 1 \n        if not dry_run : \n            self . best_cost = J \n            self . best_epoch = epoch \n        self . save_checkpoint ( save_path ) \n    self . report ( dict ( costs ) , type = \"valid\" , epoch = 0 if dry_run else epoch , new_best = new_best ) \n    self . last_run_costs = costs \n    return epoch - self . best_epoch < self . patience "}
{"6234": "\ndef report ( self , score_map , type = \"valid\" , epoch = - 1 , new_best = 0 ) : \n    type_str = type \n    if len ( type_str ) < 5 : \n        type_str += \" \" * ( 5 - len ( type_str ) ) \n    info = \" \" . join ( \"%s=%.2f\" % el for el in score_map . items ( ) ) \n    current_epoch = epoch if epoch > 0 else self . current_epoch ( ) \n    epoch_str = \"epoch={}\" . format ( current_epoch + 1 ) \n    if epoch < 0 : \n        epoch_str = \"dryrun\" \n        sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) \n    marker = \" *\" if new_best else \"\" \n    message = \"{} ({}) {}{}\" . format ( type_str , epoch_str , info , marker ) \n    self . network . train_logger . record ( message ) \n    logging . info ( message ) "}
{"6243": "\ndef encode ( self , x ) : \n    if not self . encoding_network : \n        self . encoding_network = NeuralNetwork ( self . input_dim , self . input_tensor ) \n        self . encoding_network . input_variables = self . input_variables \n        for layer in self . encoding_layes : \n            self . encoding_network . stack_layer ( layer , no_setup = 1 ) \n    return self . encoding_network . compute ( * x ) "}
{"6244": "\ndef decode ( self , x ) : \n    if not self . rep_dim : \n        raise Exception ( \"rep_dim must be set to decode.\" ) \n    if not self . decoding_network : \n        self . decoding_network = NeuralNetwork ( self . rep_dim ) \n        for layer in self . decoding_layers : \n            self . decoding_network . stack_layer ( layer , no_setup = 1 ) \n    return self . decoding_network . compute ( x ) "}
{"6251": "\ndef save_params ( self , path , new_thread = 0 ) : \n    save_logger . info ( path ) \n    param_variables = self . all_parameters \n    params = [ p . get_value ( ) . copy ( ) for p in param_variables ] \n    if new_thread : \n        thread = Thread ( target = save_network_params , args = ( params , path ) ) \n        thread . start ( ) \n    else : \n        save_network_params ( params , path ) \n    self . train_logger . save ( path ) "}
{"6252": "\ndef load_params ( self , path , exclude_free_params = 0 ) : \n    if not os . path . exists ( path ) : \n        return ; \n    logging . info ( \"loading parameters from %s\" % path ) \n    if exclude_free_params : \n        params_to_load = self . parameters \n    else : \n        params_to_load = self . all_parameters \n    if path . endswith ( \".gz\" ) : \n        opener = gzip . open if path . lower ( ) . endswith ( '.gz' ) else open \n        handle = opener ( path , 'rb' ) \n        saved_params = pickle . load ( handle ) \n        handle . close ( ) \n        for target , source in zip ( params_to_load , saved_params ) : \n            logging . info ( '%s: setting value %s' , target . name , source . shape ) \n            target . set_value ( source ) \n    elif path . endswith ( \".npz\" ) : \n        arrs = np . load ( path ) \n        for target , idx in zip ( params_to_load , range ( len ( arrs . keys ( ) ) ) ) : \n            source = arrs [ 'arr_%d' % idx ] \n            logging . info ( '%s: setting value %s' , target . name , source . shape ) \n            target . set_value ( source ) \n    else : \n        raise Exception ( \"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path ) \n    self . train_logger . load ( path ) "}
{"6261": "\ndef load_params ( self , path , exclude_free_params = 0 ) : \n    from deepy . core import graph \n    from deepy . core . comp_graph import ComputationalGraph \n    model = graph . compile ( blocks = [ self ] ) \n    model . load_params ( path , exclude_free_params = exclude_free_params ) "}
{"6270": "\ndef resolve_provider_class ( class_ ) : \n    if isinstance ( class_ , str ) : \n        path = '.' . join ( [ __package__ , 'providers' , class_ ] ) \n        return import_string ( class_ , 1 ) or import_string ( path ) \n    else : \n        return class_ "}
{"6277": "\ndef valid ( self ) : \n    if self . expiration_time : \n        return self . expiration_time > int ( time . time ( ) ) \n    else : \n        return 1 "}
{"6278": "\ndef expire_soon ( self , seconds ) : \n    if self . expiration_time : \n        return self . expiration_time < int ( time . time ( ) ) + int ( seconds ) \n    else : \n        return 0 "}
{"6283": "\ndef _access_user_info ( self ) : \n    response = super ( Bitbucket , self ) . _access_user_info ( ) \n    response . data . setdefault ( \"email\" , None ) \n    email_response = self . access ( self . user_email_url ) \n    if email_response . data : \n        for item in email_response . data : \n            if item . get ( \"primary\" , 0 ) : \n                response . data . update ( email = item . get ( \"email\" , None ) ) \n    return response "}
{"6291": "\ndef _split_url ( url ) : \n    split = parse . urlsplit ( url ) \n    base = parse . urlunsplit ( ( split . scheme , split . netloc , split . path , 0 , 0 ) ) \n    params = parse . parse_qsl ( split . query , 1 ) \n    return base , params "}
{"6292": "\ndef cross_origin ( app , * args , ** kwargs ) : \n    _options = kwargs \n    _real_decorator = cors . decorate ( app , * args , run_middleware = 0 , with_context = 0 , ** kwargs ) \n    def wrapped_decorator ( f ) : \n        spf = SanicPluginsFramework ( app ) \n        try : \n            plugin = spf . register_plugin ( cors , skip_reg = 1 ) \n        except ValueError as e : \n            assert e . args and len ( e . args ) > 1 \n            plugin = e . args [ 1 ] \n        context = cors . get_context_from_spf ( spf ) \n        log = context . log \n        log ( logging . DEBUG , \"Enabled {:s} for cross_origin using options: {}\" . format ( str ( f ) , str ( _options ) ) ) \n        return _real_decorator ( f ) \n    return wrapped_decorator "}
{"6293": "\ndef set_cors_headers ( req , resp , context , options ) : \n    try : \n        request_context = context . request [ id ( req ) ] \n    except AttributeError : \n        LOG . debug ( \"Cannot find the request context. Is request already finished?\" ) \n        return resp \n    evaluated = request_context . get ( SANIC_CORS_EVALUATED , 0 ) \n    if evaluated : \n        LOG . debug ( 'CORS have been already evaluated, skipping' ) \n        return resp \n    if resp is None : \n        return None \n    if resp . headers is None : \n        resp . headers = CIMultiDict ( ) \n    headers_to_set = get_cors_headers ( options , req . headers , req . method ) \n    LOG . debug ( 'Settings CORS headers: %s' , str ( headers_to_set ) ) \n    try : \n        resp . headers . extend ( headers_to_set ) \n    except Exception as e1 : \n        for k , v in headers_to_set . items ( ) : \n            try : \n                resp . headers . add ( k , v ) \n            except Exception as e2 : \n                resp . headers [ k ] = v \n        return resp "}
{"6297": "\ndef isclose ( a , b , * , rel_tol = 1e-09 , abs_tol = 0.0 ) : \n    try : \n        return math . isclose ( a , b , rel_tol = rel_tol , abs_tol = abs_tol ) \n    except AttributeError : \n        if ( rel_tol < 0.0 ) or ( abs_tol < 0.0 ) : \n            raise ValueError ( \"Tolerances must be non-negative, but are rel_tol: {} and abs_tol: {}\" . format ( rel_tol , abs_tol ) ) \n        if math . isnan ( a ) or math . isnan ( b ) : \n            return 0 \n        if ( a == b ) : \n            return 1 \n        if math . isinf ( a ) or math . isinf ( b ) : \n            return 0 \n        diff = abs ( a - b ) \n        return ( diff <= rel_tol * abs ( b ) ) or ( diff <= rel_tol * abs ( a ) ) or ( diff <= abs_tol ) "}
{"6302": "\ndef _execute_sox_cmd ( self , cmd , console_output = 0 ) : \n    on_windows = platform . system ( ) . lower ( ) == \"windows\" \n    def _get_random_tmp_file ( ) : \n        if on_windows : \n            rand_string = \"\" . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 8 ) ) \n            tmp = self . name + \"_\" + rand_string \n            WinTempFile = collections . namedtuple ( \"WinTempFile\" , \"name\" ) \n            tmp = WinTempFile ( tmp ) \n        else : \n            tmp = tempfile . NamedTemporaryFile ( ) \n        return tmp \n    tmp = _get_random_tmp_file ( ) \n    othertmp = _get_random_tmp_file ( ) \n    self . export ( tmp . name , format = \"WAV\" ) \n    stdout = stderr = subprocess . PIPE if console_output else subprocess . DEVNULL \n    command = cmd . format ( inputfile = tmp . name , outputfile = othertmp . name ) \n    res = subprocess . call ( command . split ( ' ' ) , stdout = stdout , stderr = stderr ) \n    assert res == 0 , \"Sox did not work as intended, or perhaps you don't have Sox installed?\" \n    other = AudioSegment ( pydub . AudioSegment . from_wav ( othertmp . name ) , self . name ) \n    if on_windows : \n        os . remove ( tmp . name ) \n        os . remove ( othertmp . name ) \n    else : \n        tmp . close ( ) \n        othertmp . close ( ) \n    return other "}
{"6303": "\ndef filter_silence ( self , duration_s = 1 , threshold_percentage = 1 , console_output = 0 ) : \n    command = \"sox {inputfile} -t wav {outputfile} silence -l 1 0.1 \" + str ( threshold_percentage ) + \"% -1 \" + str ( float ( duration_s ) ) + \" \" + str ( threshold_percentage ) + \"%\" \n    try : \n        result = self . _execute_sox_cmd ( command ) \n    except pydub . exceptions . CouldntDecodeError : \n        warnings . warn ( \"After silence filtering, the resultant WAV file is corrupted, and so its data cannot be retrieved. Perhaps try a smaller threshold value.\" , stacklevel = 2 ) \n        result = AudioSegment ( self . seg , self . name ) \n    return result "}
{"6304": "\ndef fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = 0 ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample can be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples can be specified.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    if duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    end_sample = start_sample + num_samples \n    if end_sample > len ( self . get_array_of_samples ( ) ) and not zero_pad : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    elif end_sample > len ( self . get_array_of_samples ( ) ) and zero_pad : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n        zeros = np . zeros ( end_sample - len ( arr ) ) \n        arr = np . append ( arr , zeros ) \n    else : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n    audioslice = np . array ( arr [ start_sample : end_sample ] ) \n    fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2 ) ) + 1 ) ] \n    step_size = self . frame_rate / num_samples \n    bins = np . arange ( 0 , int ( round ( num_samples / 2 ) ) + 1 , 1.0 ) * step_size \n    return bins , fft_result "}
{"6305": "\ndef generate_frames ( self , frame_duration_ms , zero_pad = 1 ) : \n    Frame = collections . namedtuple ( \"Frame\" , \"bytes timestamp duration\" ) \n    bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000 ) * self . sample_width ) \n    offset = 0 \n    timestamp = 0.0 \n    frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width \n    while offset + bytes_per_frame < len ( self . raw_data ) : \n        yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) \n        timestamp += frame_duration_s \n        offset += bytes_per_frame \n    if zero_pad : \n        rest = self . raw_data [ offset : ] \n        zeros = bytes ( bytes_per_frame - len ( rest ) ) \n        yield Frame ( rest + zeros , timestamp , frame_duration_s ) "}
{"6308": "\ndef resample ( self , sample_rate_Hz = None , sample_width = None , channels = None , console_output = 0 ) : \n    if sample_rate_Hz is None : \n        sample_rate_Hz = self . frame_rate \n    if sample_width is None : \n        sample_width = self . sample_width \n    if channels is None : \n        channels = self . channels \n    command = \"sox {inputfile} -b \" + str ( sample_width * 8 ) + \" -r \" + str ( sample_rate_Hz ) + \" -t wav {outputfile} channels \" + str ( channels ) \n    return self . _execute_sox_cmd ( command , console_output = console_output ) "}
{"6328": "\ndef lowpass_filter ( data , cutoff , fs , order = 5 ) : \n    nyq = 0.5 * fs \n    normal_cutoff = cutoff / nyq \n    b , a = signal . butter ( order , normal_cutoff , btype = 'low' , analog = 0 ) \n    y = signal . lfilter ( b , a , data ) \n    return y "}
{"6331": "\ndef equal_ignore_order ( a , b ) : \n    unmatched = list ( b ) \n    for element in a : \n        try : \n            unmatched . remove ( element ) \n        except ValueError : \n            return 0 \n    return not unmatched "}
{"6332": "\ndef group_audit_ranks ( filenames , measurer , similarity_bound = 0.05 ) : \n    def _partition_groups ( feature_scores ) : \n        groups = [ ] \n        for feature , score in feature_scores : \n            added_to_group = 0 \n            for i , group in enumerate ( groups ) : \n                mean_score , group_feature_scores = group \n                if abs ( mean_score - score ) < similarity_bound : \n                    groups [ i ] [ 1 ] . append ( ( feature , score ) ) \n                    groups [ i ] [ 0 ] = sum ( [ s for _ , s in group_feature_scores ] ) / len ( group_feature_scores ) \n                    added_to_group = 1 \n                    break \n            if not added_to_group : \n                groups . append ( [ score , [ ( feature , score ) ] ] ) \n        return [ [ feature for feature , score in group ] for _ , group in groups ] \n    score_dict = { } \n    features = [ ] \n    for filename in filenames : \n        with open ( filename ) as audit_file : \n            header_line = audit_file . readline ( ) [ : - 1 ] \n            feature = header_line [ header_line . index ( \":\" ) + 1 : ] \n            features . append ( feature ) \n        confusion_matrices = load_audit_confusion_matrices ( filename ) \n        for rep_level , matrix in confusion_matrices : \n            score = measurer ( matrix ) \n            if rep_level not in score_dict : \n                score_dict [ rep_level ] = { } \n            score_dict [ rep_level ] [ feature ] = score \n    score_keys = sorted ( score_dict . keys ( ) ) \n    groups = [ features ] \n    while score_keys : \n        key = score_keys . pop ( ) \n        new_groups = [ ] \n        for group in groups : \n            group_features = [ ( f , score_dict [ key ] [ f ] ) for f in group ] \n            sub_groups = _partition_groups ( group_features ) \n            new_groups . extend ( sub_groups ) \n        groups = new_groups \n    return groups "}
{"6341": "\ndef push_sample ( self , x , timestamp = 0.0 , pushthrough = 1 ) : \n    if len ( x ) == self . channel_count : \n        if self . channel_format == cf_string : \n            x = [ v . encode ( 'utf-8' ) for v in x ] \n        handle_error ( self . do_push_sample ( self . obj , self . sample_type ( * x ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n    else : \n        raise ValueError ( \"length of the data must correspond to the \" \"stream's channel count.\" ) "}
{"6342": "\ndef push_chunk ( self , x , timestamp = 0.0 , pushthrough = 1 ) : \n    try : \n        n_values = self . channel_count * len ( x ) \n        data_buff = ( self . value_type * n_values ) . from_buffer ( x ) \n        handle_error ( self . do_push_chunk ( self . obj , data_buff , c_long ( n_values ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n    except TypeError : \n        if len ( x ) : \n            if type ( x [ 0 ] ) is list : \n                x = [ v for sample in x for v in sample ] \n            if self . channel_format == cf_string : \n                x = [ v . encode ( 'utf-8' ) for v in x ] \n            if len ( x ) % self . channel_count == 0 : \n                constructor = self . value_type * len ( x ) \n                handle_error ( self . do_push_chunk ( self . obj , constructor ( * x ) , c_long ( len ( x ) ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n            else : \n                raise ValueError ( \"each sample must have the same number of \" \"channels.\" ) "}
{"6364": "\ndef do_fuzzyindex ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    token = Token ( word ) \n    neighbors = make_fuzzy ( token ) \n    neighbors = [ ( n , DB . zcard ( dbkeys . token_key ( n ) ) ) for n in neighbors ] \n    neighbors . sort ( key = lambda n : n [ 1 ] , reverse = 1 ) \n    for token , freq in neighbors : \n        if freq == 0 : \n            break \n        print ( white ( token ) , blue ( freq ) ) "}
{"6372": "\ndef do_BESTSCORE ( self , word ) : \n    key = keys . token_key ( indexed_string ( word ) [ 0 ] ) \n    for _id , score in DB . zrevrange ( key , 0 , 20 , withscores = 1 ) : \n        result = Result ( _id ) \n        print ( white ( result ) , blue ( score ) , green ( result . _id ) ) "}
{"6374": "\ndef send ( r , stream = 0 ) : \n    r . send ( stream = stream ) \n    return r . response "}
{"6375": "\ndef map ( requests , stream = 1 , pool = None , size = 1 , exception_handler = None ) : \n    pool = pool if pool else Pool ( size ) \n    requests = list ( requests ) \n    requests = pool . map ( send , requests ) \n    ret = [ ] \n    for request in requests : \n        if request . response is not None : \n            ret . append ( request . response ) \n        elif exception_handler and hasattr ( request , 'exception' ) : \n            ret . append ( exception_handler ( request , request . exception ) ) \n        else : \n            ret . append ( None ) \n    if not pool : \n        pool . close ( ) \n    return ret "}
{"6379": "\ndef find_files ( directory , pattern , recursive = 1 ) : \n    if not os . path . isdir ( directory ) : \n        if os . path . exists ( directory ) : \n            raise IOError ( directory + ' is not directory' ) \n        else : \n            raise IOError ( directory + \" does not exists\" ) \n    if recursive : \n        for root , _ , files in os . walk ( directory ) : \n            for basename in files : \n                if fnmatch . fnmatch ( basename , pattern ) : \n                    filename = os . path . join ( root , basename ) \n                    yield filename \n    else : \n        root = directory \n        for basename in os . listdir ( root ) : \n            if fnmatch . fnmatch ( basename , pattern ) : \n                filename = os . path . join ( root , basename ) \n                if os . path . isfile ( filename ) : \n                    yield filename "}
{"6381": "\ndef StaticForEach ( parentUnit , items , bodyFn , name = \"\" ) : \n    items = list ( items ) \n    itemsCnt = len ( items ) \n    if itemsCnt == 0 : \n        return [ ] \n    elif itemsCnt == 1 : \n        return bodyFn ( items [ 0 ] , 0 ) \n    else : \n        index = parentUnit . _reg ( name + \"for_index\" , Bits ( log2ceil ( itemsCnt + 1 ) , signed = 0 ) , defVal = 0 ) \n        ackSig = parentUnit . _sig ( name + \"for_ack\" ) \n        statementLists = [ ] \n        for i , ( statementList , ack ) in [ ( i , bodyFn ( item , i ) ) for i , item in enumerate ( items ) ] : \n            statementLists . append ( statementList + [ ( ackSig ( ack ) ) , ] ) \n        If ( ackSig , If ( index . _eq ( itemsCnt - 1 ) , index ( 0 ) ) . Else ( index ( index + 1 ) ) ) \n        return Switch ( index ) . addCases ( enumerate ( statementLists ) ) . Default ( bodyFn ( items [ 0 ] , 0 ) [ 0 ] , ackSig ( 1 ) ) "}
{"6398": "\ndef checkIfIsTooSimple ( proc ) : \n    try : \n        a , = proc . statements \n        if isinstance ( a , Assignment ) : \n            return 1 \n    except ValueError : \n        pass \n    return 0 "}
{"6400": "\ndef reduceProcesses ( processes ) : \n    processes . sort ( key = lambda x : ( x . name , maxStmId ( x ) ) , reverse = 1 ) \n    for _ , procs in groupedby ( processes , lambda p : p . rank ) : \n        for iA , pA in enumerate ( procs ) : \n            if pA is None : \n                continue \n            for iB , pB in enumerate ( islice ( procs , iA + 1 , None ) ) : \n                if pB is None : \n                    continue \n                try : \n                    pA = tryToMerge ( pA , pB ) \n                except IncompatibleStructure : \n                    continue \n                procs [ iA + 1 + iB ] = None \n        for p in procs : \n            if p is not None : \n                yield p "}
{"6402": "\ndef toRtl ( unitOrCls : Unit , name : str = None , serializer : GenericSerializer = VhdlSerializer , targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : \n    if not isinstance ( unitOrCls , Unit ) : \n        u = unitOrCls ( ) \n    else : \n        u = unitOrCls \n    u . _loadDeclarations ( ) \n    if name is not None : \n        assert isinstance ( name , str ) \n        u . _name = name \n    globScope = serializer . getBaseNameScope ( ) \n    mouduleScopes = { } \n    serializedClasses = { } \n    serializedConfiguredUnits = { } \n    doSerialize = 1 \n    createFiles = saveTo is not None \n    if createFiles : \n        os . makedirs ( saveTo , exist_ok = 1 ) \n        files = UniqList ( ) \n    else : \n        codeBuff = [ ] \n    for obj in u . _toRtl ( targetPlatform ) : \n        doSerialize = serializer . serializationDecision ( obj , serializedClasses , serializedConfiguredUnits ) \n        if doSerialize : \n            if isinstance ( obj , Entity ) : \n                s = globScope . fork ( 1 ) \n                s . setLevel ( 2 ) \n                ctx = serializer . getBaseContext ( ) \n                ctx . scope = s \n                mouduleScopes [ obj ] = ctx \n                ctx . currentUnit = obj . origin \n                sc = serializer . Entity ( obj , ctx ) \n                if createFiles : \n                    fName = obj . name + serializer . fileExtension \n                    fileMode = 'w' \n            elif isinstance ( obj , Architecture ) : \n                try : \n                    ctx = mouduleScopes [ obj . entity ] \n                except KeyError : \n                    raise SerializerException ( \"Entity should be serialized\" \" before architecture of %s\" % ( obj . getEntityName ( ) ) ) \n                sc = serializer . Architecture ( obj , ctx ) \n                if createFiles : \n                    fName = obj . getEntityName ( ) + serializer . fileExtension \n                    fileMode = 'a' \n            else : \n                if hasattr ( obj , \"_hdlSources\" ) : \n                    for fn in obj . _hdlSources : \n                        if isinstance ( fn , str ) : \n                            shutil . copy2 ( fn , saveTo ) \n                            files . append ( fn ) \n                            continue \n                else : \n                    sc = serializer . asHdl ( obj ) \n            if sc : \n                if createFiles : \n                    fp = os . path . join ( saveTo , fName ) \n                    files . append ( fp ) \n                    with open ( fp , fileMode ) as f : \n                        if fileMode == 'a' : \n                            f . write ( \"\\n\" ) \n                        f . write ( serializer . formatter ( sc ) ) \n                else : \n                    codeBuff . append ( sc ) \n        elif not createFiles : \n            try : \n                name = '\"%s\"' % obj . name \n            except AttributeError : \n                name = \"\" \n            codeBuff . append ( serializer . comment ( \"Object of class %s, %s was not serialized as specified\" % ( obj . __class__ . __name__ , name ) ) ) \n    if createFiles : \n        return files \n    else : \n        return serializer . formatter ( \"\\n\" . join ( codeBuff ) ) "}
{"6406": "\ndef synthesize ( self , name , interfaces , targetPlatform ) : \n    ent = Entity ( name ) \n    ent . _name = name + \"_inst\" \n    for _ , v in self . params . items ( ) : \n        ent . generics . append ( v ) \n    if isinstance ( interfaces , set ) : \n        intfSet = interfaces \n    else : \n        intfSet = set ( interfaces ) \n    for s in interfaces : \n        pi = portItemfromSignal ( s , ent ) \n        pi . registerInternSig ( s ) \n        ent . ports . append ( pi ) \n        s . hidden = 0 \n    removeUnconnectedSignals ( self ) \n    markVisibilityOfSignals ( self , name , self . signals , intfSet ) \n    for proc in targetPlatform . beforeHdlArchGeneration : \n        proc ( self ) \n    arch = Architecture ( ent ) \n    for p in statements_to_HWProcesses ( self . statements ) : \n        arch . processes . append ( p ) \n    for s in self . signals : \n        if s not in intfSet and not s . hidden : \n            arch . variables . append ( s ) \n    for u in self . subUnits : \n        arch . componentInstances . append ( u ) \n    for su in distinctBy ( self . subUnits , lambda x : x . name ) : \n        arch . components . append ( su ) \n    self . synthesised = 1 \n    return [ ent , arch ] "}
{"6422": "\ndef framesFromTransTmpl ( transaction : 'TransTmpl' , wordWidth : int , maxFrameLen : Union [ int , float ] = inf , maxPaddingWords : Union [ int , float ] = inf , trimPaddingWordsOnStart : bool = 0 , trimPaddingWordsOnEnd : bool = 0 ) -> Generator [ 'FrameTmpl' , None , None ] : \n    isFirstInFrame = 1 \n    partsPending = 0 \n    startOfThisFrame = 0 \n    assert maxFrameLen > 0 \n    assert maxPaddingWords >= 0 \n    if maxPaddingWords < inf : \n        assert trimPaddingWordsOnStart or trimPaddingWordsOnEnd , \"Padding has to be cut off somewhere\" \n    it = TransTmplWordIterator ( wordWidth ) \n    lastWordI = 0 \n    endOfThisFrame = maxFrameLen \n    parts = [ ] \n    for wordI , word in it . groupByWordIndex ( transaction , 0 ) : \n        if wordI * wordWidth >= endOfThisFrame : \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : \n                _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            else : \n                _endOfThisFrame = wordI * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = 1 \n            partsPending = 0 \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI \n        if ( not isFirstInFrame and trimPaddingWordsOnEnd and wordI - lastWordI > 1 ) : \n            _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = 1 \n            partsPending = 0 \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI - 1 \n        if isFirstInFrame : \n            partsPending = 1 \n            isFirstInFrame = 0 \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnStart and paddingWords > maxPaddingWords : \n                startOfThisFrame += paddingWords * wordWidth \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n        parts . extend ( word ) \n        lastWordI = wordI \n    endOfThisFrame = transaction . bitAddrEnd \n    withPadding = not ( trimPaddingWordsOnEnd or trimPaddingWordsOnStart ) \n    if partsPending or ( withPadding and endOfThisFrame != startOfThisFrame ) : \n        endOfLastWord = ( lastWordI + 1 ) * wordWidth \n        if endOfThisFrame < endOfLastWord : \n            endOfThisFrame = endOfLastWord \n        else : \n            paddingWords = it . fullWordCnt ( endOfLastWord , endOfThisFrame ) \n            if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : \n                endOfThisFrame -= paddingWords * wordWidth \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , endOfThisFrame ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , parts ) \n        parts = [ ] \n        startOfThisFrame = endOfThisFrame \n    while withPadding and startOfThisFrame < transaction . bitAddrEnd : \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , transaction . bitAddrEnd ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , [ ] ) \n        startOfThisFrame = endOfThisFrame "}
{"6423": "\ndef walkWords ( self , showPadding : bool = 0 ) : \n    wIndex = 0 \n    lastEnd = self . startBitAddr \n    parts = [ ] \n    for p in self . parts : \n        end = p . startOfPart \n        if showPadding and end != lastEnd : \n            while end != lastEnd : \n                assert end >= lastEnd , ( end , lastEnd ) \n                endOfWord = ceil ( ( lastEnd + 1 ) / self . wordWidth ) * self . wordWidth \n                endOfPadding = min ( endOfWord , end ) \n                _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n                parts . append ( _p ) \n                if endOfPadding >= endOfWord : \n                    yield ( wIndex , parts ) \n                    wIndex += 1 \n                    parts = [ ] \n                lastEnd = endOfPadding \n        if self . _wordIndx ( lastEnd ) != self . _wordIndx ( p . startOfPart ) : \n            yield ( wIndex , parts ) \n            wIndex += 1 \n            parts = [ ] \n            lastEnd = p . endOfPart \n        parts . append ( p ) \n        lastEnd = p . endOfPart \n        if lastEnd % self . wordWidth == 0 : \n            yield ( wIndex , parts ) \n            wIndex += 1 \n            parts = [ ] \n    if showPadding and ( parts or lastEnd != self . endBitAddr or lastEnd % self . wordWidth != 0 ) : \n        end = ceil ( self . endBitAddr / self . wordWidth ) * self . wordWidth \n        while end != lastEnd : \n            assert end >= lastEnd , ( end , lastEnd ) \n            endOfWord = ( ( lastEnd // self . wordWidth ) + 1 ) * self . wordWidth \n            endOfPadding = min ( endOfWord , end ) \n            _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n            _p . parent = self \n            parts . append ( _p ) \n            if endOfPadding >= endOfWord : \n                yield ( wIndex , parts ) \n                wIndex += 1 \n                parts = [ ] \n            lastEnd = endOfPadding \n        if parts : \n            yield ( wIndex , parts ) "}
{"6424": "\ndef packData ( self , data ) : \n    typeOfWord = simBitsT ( self . wordWidth , None ) \n    fieldToVal = self . _fieldToTPart \n    if fieldToVal is None : \n        fieldToVal = self . _fieldToTPart = self . fieldToDataDict ( self . origin . dtype , data , { } ) \n    for _ , transParts in self . walkWords ( showPadding = 1 ) : \n        actualVldMask = 0 \n        actualVal = 0 \n        for tPart in transParts : \n            high , low = tPart . getBusWordBitRange ( ) \n            fhigh , flow = tPart . getFieldBitRange ( ) \n            if not tPart . isPadding : \n                val = fieldToVal . get ( tPart . tmpl . origin , None ) \n            else : \n                val = None \n            if val is None : \n                newBits = 0 \n                vld = 0 \n            else : \n                newBits = selectBitRange ( val , flow , fhigh - flow ) \n                vld = mask ( high - low ) << low \n            actualVal = setBitRange ( actualVal , low , high - low , newBits ) \n            actualVldMask = setBitRange ( actualVal , low , high - low , vld ) \n        yield typeOfWord . getValueCls ( ) ( actualVal , typeOfWord , actualVldMask , - 1 ) "}
{"6426": "\ndef _discover_enclosure_for_statements ( statements : List [ 'HdlStatement' ] , outputs : List [ 'HdlStatement' ] ) : \n    result = set ( ) \n    if not statements : \n        return result \n    for stm in statements : \n        stm . _discover_enclosure ( ) \n    for o in outputs : \n        has_driver = 0 \n        for stm in statements : \n            if o in stm . _outputs : \n                assert not has_driver \n                has_driver = 0 \n                if o in stm . _enclosed_for : \n                    result . add ( o ) \n            else : \n                pass \n    return result "}
{"6431": "\ndef _is_mergable_statement_list ( cls , stmsA , stmsB ) : \n    if stmsA is None and stmsB is None : \n        return 1 \n    elif stmsA is None or stmsB is None : \n        return 0 \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = _get_stm_with_branches ( a_it ) \n    b = _get_stm_with_branches ( b_it ) \n    while a is not None or b is not None : \n        if a is None or b is None or not a . _is_mergable ( b ) : \n            return 0 \n        a = _get_stm_with_branches ( a_it ) \n        b = _get_stm_with_branches ( b_it ) \n    return 1 "}
{"6433": "\ndef _merge_statement_lists ( stmsA : List [ \"HdlStatement\" ] , stmsB : List [ \"HdlStatement\" ] ) -> List [ \"HdlStatement\" ] : \n    if stmsA is None and stmsB is None : \n        return None \n    tmp = [ ] \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = None \n    b = None \n    a_empty = 0 \n    b_empty = 0 \n    while not a_empty and not b_empty : \n        while not a_empty : \n            a = next ( a_it , None ) \n            if a is None : \n                a_empty = 1 \n                break \n            elif a . rank == 0 : \n                tmp . append ( a ) \n                a = None \n            else : \n                break \n        while not b_empty : \n            b = next ( b_it , None ) \n            if b is None : \n                b_empty = 1 \n                break \n            elif b . rank == 0 : \n                tmp . append ( b ) \n                b = None \n            else : \n                break \n        if a is not None or b is not None : \n            a . _merge_with_other_stm ( b ) \n            tmp . append ( a ) \n            a = None \n            b = None \n    return tmp "}
{"6434": "\ndef _try_reduce_list ( statements : List [ \"HdlStatement\" ] ) : \n    io_change = 0 \n    new_statements = [ ] \n    for stm in statements : \n        reduced , _io_change = stm . _try_reduce ( ) \n        new_statements . extend ( reduced ) \n        io_change |= _io_change \n    new_statements , rank_decrease = HdlStatement . _merge_statements ( new_statements ) \n    return new_statements , rank_decrease , io_change "}
{"6435": "\ndef _on_parent_event_dependent ( self ) : \n    if not self . _is_completly_event_dependent : \n        self . _is_completly_event_dependent = 1 \n        for stm in self . _iter_stms ( ) : \n            stm . _on_parent_event_dependent ( ) "}
{"6442": "\ndef walkFlattenFields ( sigOrVal , skipPadding = 1 ) : \n    t = sigOrVal . _dtype \n    if isinstance ( t , Bits ) : \n        yield sigOrVal \n    elif isinstance ( t , HUnion ) : \n        yield from walkFlattenFields ( sigOrVal . _val , skipPadding = skipPadding ) \n    elif isinstance ( t , HStruct ) : \n        for f in t . fields : \n            isPadding = f . name is None \n            if not isPadding or not skipPadding : \n                if isPadding : \n                    v = f . dtype . fromPy ( None ) \n                else : \n                    v = getattr ( sigOrVal , f . name ) \n                yield from walkFlattenFields ( v ) \n    elif isinstance ( t , HArray ) : \n        for item in sigOrVal : \n            yield from walkFlattenFields ( item ) \n    else : \n        raise NotImplementedError ( t ) "}
{"6443": "\ndef HStruct_unpack ( structT , data , getDataFn = None , dataWidth = None ) : \n    if getDataFn is None : \n        assert dataWidth is not None \n        def _getDataFn ( x ) : \n            return toHVal ( x ) . _auto_cast ( Bits ( dataWidth ) ) \n        getDataFn = _getDataFn \n    val = structT . fromPy ( None ) \n    fData = iter ( data ) \n    actualOffset = 0 \n    actual = None \n    for v in walkFlattenFields ( val , skipPadding = 0 ) : \n        required = v . _dtype . bit_length ( ) \n        if actual is None : \n            actualOffset = 0 \n            try : \n                actual = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            if dataWidth is None : \n                dataWidth = actual . _dtype . bit_length ( ) \n            actuallyHave = dataWidth \n        else : \n            actuallyHave = actual . _dtype . bit_length ( ) - actualOffset \n        while actuallyHave < required : \n            try : \n                d = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            actual = d . _concat ( actual ) \n            actuallyHave += dataWidth \n        if actuallyHave >= required : \n            _v = actual [ ( required + actualOffset ) : actualOffset ] \n            _v = _v . _auto_cast ( v . _dtype ) \n            v . val = _v . val \n            v . vldMask = _v . vldMask \n            v . updateTime = _v . updateTime \n            actuallyHave -= required \n            actualOffset += required \n        if actuallyHave == 0 : \n            actual = None \n    if actual is not None : \n        assert actual . _dtype . bit_length ( ) - actualOffset < dataWidth , \"It should be just a padding at the end of frame\" \n    return val "}
{"6446": "\ndef simEvalCond ( simulator , * conds ) : \n    _cond = 1 \n    _vld = 1 \n    for v in conds : \n        val = bool ( v . val ) \n        fullVld = v . vldMask == 1 \n        if fullVld : \n            if not val : \n                return 0 , 1 \n        else : \n            return 0 , 0 \n        _cond = _cond and val \n        _vld = _vld and fullVld \n    return _cond , _vld "}
{"6450": "\ndef vec ( val , width , signed = None ) : \n    return Bits ( width , signed , forceVector = 1 ) . fromPy ( val ) "}
{"6463": "\ndef hardcodeRomIntoProcess ( cls , rom ) : \n    processes = [ ] \n    signals = [ ] \n    for e in rom . endpoints : \n        assert isinstance ( e , Operator ) and e . operator == AllOps . INDEX , e \n        me , index = e . operands \n        assert me is rom \n        romValSig = rom . ctx . sig ( rom . name , dtype = e . result . _dtype ) \n        signals . append ( romValSig ) \n        romValSig . hidden = 0 \n        cases = [ ( toHVal ( i ) , [ romValSig ( v ) , ] ) for i , v in enumerate ( rom . defVal . val ) ] \n        statements = [ SwitchContainer ( index , cases ) , ] \n        for ( _ , ( stm , ) ) in cases : \n            stm . parentStm = statements [ 0 ] \n        p = HWProcess ( rom . name , statements , { index , } , { index , } , { romValSig , } ) \n        processes . append ( p ) \n        def replaceOrigRomIndexExpr ( x ) : \n            if x is e . result : \n                return romValSig \n            else : \n                return x \n        for _e in e . result . endpoints : \n            _e . operands = tuple ( map ( replaceOrigRomIndexExpr , _e . operands ) ) \n            e . result = romValSig \n    return processes , signals "}
{"6465": "\ndef _registerIntfInImpl ( self , iName , intf ) : \n    self . _registerInterface ( iName , intf , isPrivate = 1 ) \n    self . _loadInterface ( intf , 0 ) \n    intf . _signalsForInterface ( self . _ctx ) "}
{"6468": "\ndef getBaseNameScope ( cls ) : \n    s = NameScope ( 0 ) \n    s . setLevel ( 1 ) \n    s [ 0 ] . update ( cls . _keywords_dict ) \n    return s "}
{"6469": "\ndef serializationDecision ( cls , obj , serializedClasses , serializedConfiguredUnits ) : \n    isDeclaration = isinstance ( obj , Entity ) \n    isDefinition = isinstance ( obj , Architecture ) \n    if isDeclaration : \n        unit = obj . origin \n    elif isDefinition : \n        unit = obj . entity . origin \n    else : \n        return 1 \n    assert isinstance ( unit , Unit ) \n    sd = unit . _serializeDecision \n    if sd is None : \n        return 1 \n    else : \n        prevPriv = serializedClasses . get ( unit . __class__ , None ) \n        seriazlize , nextPriv = sd ( unit , obj , isDeclaration , prevPriv ) \n        serializedClasses [ unit . __class__ ] = nextPriv \n        return seriazlize "}
{"6470": "\ndef HdlType ( cls , typ : HdlType , ctx : SerializerCtx , declaration = 0 ) : \n    if isinstance ( typ , Bits ) : \n        sFn = cls . HdlType_bits \n    elif isinstance ( typ , HEnum ) : \n        sFn = cls . HdlType_enum \n    elif isinstance ( typ , HArray ) : \n        sFn = cls . HdlType_array \n    elif isinstance ( typ , Integer ) : \n        sFn = cls . HdlType_int \n    elif isinstance ( typ , HBool ) : \n        sFn = cls . HdlType_bool \n    else : \n        raise NotImplementedError ( \"type declaration is not implemented\" \" for type %s\" % ( typ . name ) ) \n    return sFn ( typ , ctx , declaration = declaration ) "}
{"6471": "\ndef IfContainer ( cls , ifc : IfContainer , ctx : SerializerCtx ) : \n    childCtx = ctx . withIndent ( ) \n    def asHdl ( statements ) : \n        return [ cls . asHdl ( s , childCtx ) for s in statements ] \n    try : \n        cond = cls . condAsHdl ( ifc . cond , 1 , ctx ) \n    except UnsupportedEventOpErr as e : \n        cond = None \n    if cond is None : \n        assert not ifc . elIfs \n        assert not ifc . ifFalse \n        stmBuff = [ cls . asHdl ( s , ctx ) for s in ifc . ifTrue ] \n        return \"\\n\" . join ( stmBuff ) \n    elIfs = [ ] \n    ifTrue = ifc . ifTrue \n    ifFalse = ifc . ifFalse \n    if ifFalse is None : \n        ifFalse = [ ] \n    for c , statements in ifc . elIfs : \n        try : \n            elIfs . append ( ( cls . condAsHdl ( c , 1 , ctx ) , asHdl ( statements ) ) ) \n        except UnsupportedEventOpErr as e : \n            if len ( ifc . elIfs ) == 1 and not ifFalse : \n                ifFalse = statements \n            else : \n                raise e \n    return cls . ifTmpl . render ( indent = getIndent ( ctx . indent ) , cond = cond , ifTrue = asHdl ( ifTrue ) , elIfs = elIfs , ifFalse = asHdl ( ifFalse ) ) "}
{"6472": "\ndef getBaseCond ( c ) : \n    isNegated = 0 \n    try : \n        drivers = c . drivers \n    except AttributeError : \n        return ( c , isNegated ) \n    if len ( drivers ) == 1 : \n        d = list ( c . drivers ) [ 0 ] \n        if isinstance ( d , Operator ) and d . operator == AllOps . NOT : \n            c = d . operands [ 0 ] \n            isNegated = 1 \n    return ( c , isNegated ) "}
{"6478": "\ndef _loadFromHType ( self , dtype : HdlType , bitAddr : int ) -> None : \n    self . bitAddr = bitAddr \n    childrenAreChoice = 0 \n    if isinstance ( dtype , Bits ) : \n        ld = self . _loadFromBits \n    elif isinstance ( dtype , HStruct ) : \n        ld = self . _loadFromHStruct \n    elif isinstance ( dtype , HArray ) : \n        ld = self . _loadFromArray \n    elif isinstance ( dtype , HStream ) : \n        ld = self . _loadFromHStream \n    elif isinstance ( dtype , HUnion ) : \n        ld = self . _loadFromUnion \n        childrenAreChoice = 1 \n    else : \n        raise TypeError ( \"expected instance of HdlType\" , dtype ) \n    self . bitAddrEnd = ld ( dtype , bitAddr ) \n    self . childrenAreChoice = childrenAreChoice "}
{"6485": "\ndef _registerParameter ( self , pName , parameter ) -> None : \n    nameAvailabilityCheck ( self , pName , parameter ) \n    try : \n        hasName = parameter . _name is not None \n    except AttributeError : \n        hasName = 0 \n    if not hasName : \n        parameter . _name = pName \n    parameter . _registerScope ( pName , self ) \n    if parameter . hasGenericName : \n        parameter . name = pName \n    if parameter . _parent is None : \n        parameter . _parent = self \n    self . _params . append ( parameter ) "}
{"6488": "\ndef _registerInterface ( self , iName , intf , isPrivate = 0 ) : \n    nameAvailabilityCheck ( self , iName , intf ) \n    assert intf . _parent is None \n    intf . _parent = self \n    intf . _name = iName \n    intf . _ctx = self . _ctx \n    if isPrivate : \n        self . _private_interfaces . append ( intf ) \n        intf . _isExtern = 0 \n    else : \n        self . _interfaces . append ( intf ) \n        intf . _isExtern = 1 "}
{"6500": "\ndef iterBits ( sigOrVal : Union [ RtlSignal , Value ] , bitsInOne : int = 1 , skipPadding : bool = 1 , fillup : bool = 0 ) : \n    bw = BitWalker ( sigOrVal , skipPadding , fillup ) \n    for _ in range ( ceil ( sigOrVal . _dtype . bit_length ( ) / bitsInOne ) ) : \n        yield bw . get ( bitsInOne ) \n    bw . assertIsOnEnd ( ) "}
{"6501": "\ndef _serializeExclude_eval ( parentUnit , obj , isDeclaration , priv ) : \n    if isDeclaration : \n        prepareEntity ( obj , parentUnit . __class__ . __name__ , priv ) \n    if priv is None : \n        priv = parentUnit \n    return 0 , priv "}
{"6503": "\ndef _serializeParamsUniq_eval ( parentUnit , obj , isDeclaration , priv ) : \n    params = paramsToValTuple ( parentUnit ) \n    if priv is None : \n        priv = { } \n    if isDeclaration : \n        try : \n            prevUnit = priv [ params ] \n        except KeyError : \n            priv [ params ] = parentUnit \n            return 1 , priv \n        prepareEntity ( obj , prevUnit . _entity . name , prevUnit ) \n        return 0 , priv \n    return priv [ params ] is parentUnit , priv "}
{"6509": "\ndef simUnitVcd ( simModel , stimulFunctions , outputFile = sys . stdout , until = 100 * Time . ns ) : \n    assert isinstance ( simModel , SimModel ) , \"Class of SimModel is required (got %r)\" % ( simModel ) \n    if isinstance ( outputFile , str ) : \n        d = os . path . dirname ( outputFile ) \n        if d : \n            os . makedirs ( d , exist_ok = 1 ) \n        with open ( outputFile , 'w' ) as f : \n            return _simUnitVcd ( simModel , stimulFunctions , f , until ) \n    else : \n        return _simUnitVcd ( simModel , stimulFunctions , outputFile , until ) "}
{"6511": "\ndef connectSig ( self , signal ) : \n    if self . direction == DIRECTION . IN : \n        if self . src is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %r\" % ( self . name , self . src ) ) \n        self . src = signal \n        signal . endpoints . append ( self ) \n    elif self . direction == DIRECTION . OUT : \n        if self . dst is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %r\" % ( self . name , self . dst ) ) \n        self . dst = signal \n        signal . drivers . append ( self ) \n    else : \n        raise NotImplementedError ( self ) \n    signal . hidden = 0 \n    signal . ctx . subUnits . add ( self . unit ) "}
{"6515": "\ndef isEvDependentOn ( sig , process ) -> bool : \n    if sig is None : \n        return 0 \n    return process in sig . simFallingSensProcs or process in sig . simRisingSensProcs "}
{"6518": "\ndef _scheduleCombUpdateDoneEv ( self ) -> Event : \n    assert not self . _combUpdateDonePlaned , self . now \n    cud = Event ( self ) \n    cud . process_to_wake . append ( self . __deleteCombUpdateDoneEv ( ) ) \n    self . _add_process ( cud , PRIORITY_AGENTS_UPDATE_DONE ) \n    self . _combUpdateDonePlaned = 1 \n    self . combUpdateDoneEv = cud \n    return cud "}
{"6519": "\ndef _scheduleApplyValues ( self ) -> None : \n    assert not self . _applyValPlaned , self . now \n    self . _add_process ( self . _applyValues ( ) , PRIORITY_APPLY_COMB ) \n    self . _applyValPlaned = 1 \n    if self . _runSeqProcessesPlaned : \n        return \n    assert not self . _seqProcsToRun and not self . _runSeqProcessesPlaned , self . now \n    self . _add_process ( self . _runSeqProcesses ( ) , PRIORITY_APPLY_SEQ ) \n    self . _runSeqProcessesPlaned = 1 "}
{"6520": "\ndef _conflictResolveStrategy ( self , newValue : set ) -> Tuple [ Callable [ [ Value ] , bool ] , bool ] : \n    invalidate = 0 \n    resLen = len ( newValue ) \n    if resLen == 3 : \n        val , indexes , isEvDependent = newValue \n        return ( mkArrayUpdater ( val , indexes , invalidate ) , isEvDependent ) \n    else : \n        val , isEvDependent = newValue \n        return ( mkUpdater ( val , invalidate ) , isEvDependent ) "}
{"6522": "\ndef _runSeqProcesses ( self ) -> Generator [ None , None , None ] : \n    updates = [ ] \n    for proc in self . _seqProcsToRun : \n        try : \n            outContainer = self . _outputContainers [ proc ] \n        except KeyError : \n            outContainer = None \n        proc ( self , outContainer ) \n        if outContainer is not None : \n            updates . append ( outContainer ) \n    self . _seqProcsToRun = UniqList ( ) \n    self . _runSeqProcessesPlaned = 0 \n    for cont in updates : \n        for sigName , sig in cont . _all_signals : \n            newVal = getattr ( cont , sigName ) \n            if newVal is not None : \n                v = self . _conflictResolveStrategy ( newVal ) \n                updater , _ = v \n                sig . simUpdateVal ( self , updater ) \n                setattr ( cont , sigName , None ) \n    return \n    yield "}
{"6523": "\ndef _applyValues ( self ) -> Generator [ None , None , None ] : \n    va = self . _valuesToApply \n    self . _applyValPlaned = 0 \n    lav = self . config . logApplyingValues \n    if va and lav : \n        lav ( self , va ) \n    self . _valuesToApply = [ ] \n    addSp = self . _seqProcsToRun . append \n    for s , vUpdater , isEventDependent , comesFrom in va : \n        if isEventDependent : \n            addSp ( comesFrom ) \n        else : \n            s . simUpdateVal ( self , vUpdater ) \n    self . _runCombProcesses ( ) \n    if self . _valuesToApply and not self . _applyValPlaned : \n        self . _scheduleApplyValues ( ) \n    return \n    yield "}
{"6530": "\ndef HWProcess ( cls , proc , ctx ) : \n    body = proc . statements \n    extraVars = [ ] \n    extraVarsSerialized = [ ] \n    hasToBeVhdlProcess = arr_any ( body , lambda x : isinstance ( x , ( IfContainer , SwitchContainer , WhileContainer , WaitStm ) ) ) \n    sensitivityList = sorted ( map ( lambda s : cls . sensitivityListItem ( s , ctx ) , proc . sensitivityList ) ) \n    if hasToBeVhdlProcess : \n        childCtx = ctx . withIndent ( ) \n    else : \n        childCtx = copy ( ctx ) \n    def createTmpVarFn ( suggestedName , dtype ) : \n        s = RtlSignal ( None , None , dtype , virtualOnly = 1 ) \n        s . name = ctx . scope . checkedName ( suggestedName , s ) \n        s . hidden = 0 \n        serializedS = cls . SignalItem ( s , childCtx , declaration = 1 ) \n        extraVars . append ( s ) \n        extraVarsSerialized . append ( serializedS ) \n        return s \n    childCtx . createTmpVarFn = createTmpVarFn \n    statemets = [ cls . asHdl ( s , childCtx ) for s in body ] \n    proc . name = ctx . scope . checkedName ( proc . name , proc ) \n    extraVarsInit = [ ] \n    for s in extraVars : \n        if isinstance ( s . defVal , RtlSignalBase ) or s . defVal . vldMask : \n            a = Assignment ( s . defVal , s , virtualOnly = 1 ) \n            extraVarsInit . append ( cls . Assignment ( a , childCtx ) ) \n        else : \n            assert s . drivers , s \n        for d in s . drivers : \n            extraVarsInit . append ( cls . asHdl ( d , childCtx ) ) \n    _hasToBeVhdlProcess = hasToBeVhdlProcess \n    hasToBeVhdlProcess = extraVars or hasToBeVhdlProcess \n    if hasToBeVhdlProcess and not _hasToBeVhdlProcess : \n        oneIndent = getIndent ( 1 ) \n        statemets = list ( map ( lambda x : oneIndent + x , statemets ) ) \n    return cls . processTmpl . render ( indent = getIndent ( ctx . indent ) , name = proc . name , hasToBeVhdlProcess = hasToBeVhdlProcess , extraVars = extraVarsSerialized , sensitivityList = \", \" . join ( sensitivityList ) , statements = extraVarsInit + statemets ) "}
{"6534": "\ndef setup_platform ( hass , config , add_entities , discovery_info = None ) : \n    host = config . get ( CONF_HOST ) \n    token = config . get ( CONF_ACCESS_TOKEN ) \n    name = config . get ( CONF_NAME ) \n    volume_step = config . get ( CONF_VOLUME_STEP ) \n    device_type = config . get ( CONF_DEVICE_CLASS ) \n    device = VizioDevice ( host , token , name , volume_step , device_type ) \n    if device . validate_setup ( ) is 0 : \n        _LOGGER . error ( \"Failed to set up Vizio platform, \" \"please check if host and API key are correct\" ) \n        return \n    elif ( token is None or token == \"\" ) and device_type == \"tv\" : \n        _LOGGER . error ( \"Failed to set up Vizio platform, \" \"if device_class is 'tv' then an auth_token needs \" \"to be provided, otherwise if device_class is \" \"'soundbar' then add the right device_class to config\" ) \n        return \n    if config . get ( CONF_SUPPRESS_WARNING ) : \n        from requests . packages import urllib3 \n        _LOGGER . warning ( \"InsecureRequestWarning is disabled \" \"because of Vizio platform configuration\" ) \n        urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) \n    add_entities ( [ device ] , 1 ) "}
{"6542": "\ndef remove_piece_at ( self , square , into_hand = 0 ) : \n    piece_type = self . piece_type_at ( square ) \n    if piece_type == NONE : \n        return \n    if into_hand : \n        self . add_piece_into_hand ( piece_type , self . turn ) \n    mask = BB_SQUARES [ square ] \n    self . piece_bb [ piece_type ] ^= mask \n    color = int ( bool ( self . occupied [ WHITE ] & mask ) ) \n    self . pieces [ square ] = NONE \n    self . occupied . ixor ( mask , color , square ) \n    if color == BLACK : \n        piece_index = ( piece_type - 1 ) * 2 \n    else : \n        piece_index = ( piece_type - 1 ) * 2 + 1 \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ] "}
{"6543": "\ndef set_piece_at ( self , square , piece , from_hand = 0 , into_hand = 0 ) : \n    if from_hand : \n        self . remove_piece_from_hand ( piece . piece_type , self . turn ) \n    self . remove_piece_at ( square , into_hand ) \n    self . pieces [ square ] = piece . piece_type \n    mask = BB_SQUARES [ square ] \n    piece_type = piece . piece_type \n    self . piece_bb [ piece_type ] |= mask \n    if piece_type == KING : \n        self . king_squares [ piece . color ] = square \n    self . occupied . ixor ( mask , piece . color , square ) \n    if piece . color == BLACK : \n        piece_index = ( piece . piece_type - 1 ) * 2 \n    else : \n        piece_index = ( piece . piece_type - 1 ) * 2 + 1 \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ] "}
{"6546": "\ndef is_game_over ( self ) : \n    try : \n        next ( self . generate_legal_moves ( ) . __iter__ ( ) ) \n    except StopIteration : \n        return 1 \n    if self . is_fourfold_repetition ( ) : \n        return 1 \n    return 0 "}
{"6547": "\ndef is_checkmate ( self ) : \n    if not self . is_check ( ) : \n        return 0 \n    try : \n        next ( self . generate_legal_moves ( ) . __iter__ ( ) ) \n        return 0 \n    except StopIteration : \n        return 1 "}
{"6548": "\ndef is_fourfold_repetition ( self ) : \n    zobrist_hash = self . zobrist_hash ( ) \n    if self . transpositions [ zobrist_hash ] < 4 : \n        return 0 \n    return 1 "}
{"6550": "\ndef sfen ( self ) : \n    sfen = [ ] \n    empty = 0 \n    for square in SQUARES : \n        piece = self . piece_at ( square ) \n        if not piece : \n            empty += 1 \n        else : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            sfen . append ( piece . symbol ( ) ) \n        if BB_SQUARES [ square ] & BB_FILE_1 : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            if square != I1 : \n                sfen . append ( '/' ) \n    sfen . append ( ' ' ) \n    if self . turn == WHITE : \n        sfen . append ( 'w' ) \n    else : \n        sfen . append ( 'b' ) \n    sfen . append ( ' ' ) \n    pih_len = 0 \n    for color in COLORS : \n        p = self . pieces_in_hand [ color ] \n        pih_len += len ( p ) \n        for piece_type in sorted ( p . keys ( ) , reverse = 1 ) : \n            if p [ piece_type ] >= 1 : \n                if p [ piece_type ] > 1 : \n                    sfen . append ( str ( p [ piece_type ] ) ) \n                piece = Piece ( piece_type , color ) \n                sfen . append ( piece . symbol ( ) ) \n    if pih_len == 0 : \n        sfen . append ( '-' ) \n    sfen . append ( ' ' ) \n    sfen . append ( str ( self . move_number ) ) \n    return '' . join ( sfen ) "}
{"6556": "\ndef from_usi ( cls , usi ) : \n    if usi == '0000' : \n        return cls . null ( ) \n    elif len ( usi ) == 4 : \n        if usi [ 1 ] == '*' : \n            piece = Piece . from_symbol ( usi [ 0 ] ) \n            return cls ( None , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , 0 , piece . piece_type ) \n        else : \n            return cls ( SQUARE_NAMES . index ( usi [ 0 : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) ) \n    elif len ( usi ) == 5 and usi [ 4 ] == '+' : \n        return cls ( SQUARE_NAMES . index ( usi [ 0 : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , 1 ) \n    else : \n        raise ValueError ( 'expected usi string to be of length 4 or 5' ) "}
{"6559": "\ndef load_config_from_cli ( config : GoodConf , argv : List [ str ] ) -> List [ str ] : \n    from django . core . management . base import BaseCommand \n    original_parser = BaseCommand . create_parser \n    def patched_parser ( self , prog_name , subcommand ) : \n        parser = original_parser ( self , prog_name , subcommand ) \n        argparser_add_argument ( parser , config ) \n        return parser \n    BaseCommand . create_parser = patched_parser \n    try : \n        parser = argparse . ArgumentParser ( add_help = 0 ) \n        argparser_add_argument ( parser , config ) \n        config_arg , default_args = parser . parse_known_args ( argv ) \n        config . load ( config_arg . config ) \n        yield default_args \n    finally : \n        BaseCommand . create_parser = original_parser "}
{"6562": "\ndef load ( self , filename : str = None ) : \n    if filename : \n        self . config_file = _find_file ( filename ) \n    else : \n        if self . file_env_var and self . file_env_var in os . environ : \n            self . config_file = _find_file ( os . environ [ self . file_env_var ] ) \n        if not self . config_file : \n            for filename in self . default_files : \n                self . config_file = _find_file ( filename , require = 0 ) \n                if self . config_file : \n                    break \n    if self . config_file : \n        config = _load_config ( self . config_file ) \n        log . info ( \"Loading config from %s\" , self . config_file ) \n    else : \n        config = { } \n        log . info ( \"No config file specified. \" \"Loading with environment variables.\" ) \n    self . set_values ( config ) "}
{"6570": "\ndef _compute_missing_rates ( self , currency ) : \n    rates = self . _rates [ currency ] \n    tmp = defaultdict ( lambda : [ None , None ] ) \n    for date in sorted ( rates ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist += 1 \n            tmp [ date ] [ 0 ] = closest_rate , dist \n    for date in sorted ( rates , reverse = 1 ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist += 1 \n            tmp [ date ] [ 1 ] = closest_rate , dist \n    for date in sorted ( tmp ) : \n        ( r0 , d0 ) , ( r1 , d1 ) = tmp [ date ] \n        rates [ date ] = ( r0 * d1 + r1 * d0 ) / ( d0 + d1 ) \n        if self . verbose : \n            print ( ( '{0}: filling {1} missing rate using {2} ({3}d old) and ' '{4} ({5}d later)' ) . format ( currency , date , r0 , d0 , r1 , d1 ) ) "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = 1 ) : \n    scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( index < 0 ) . any ( ) or ( index > n ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( index == n ) \n    index [ omegas ] -= 1 \n    offset [ omegas ] += interval_length \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ 0 ] = 1.0 \n    T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    for i in range ( 2 , coefficient_count ) : \n        T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2 ] \n    components = ( T . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        components = components [ : , 0 ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ 0 ] = 0.0 \n    dT [ 1 ] = 1.0 \n    if coefficient_count > 2 : \n        dT [ 2 ] = twot1 + twot1 \n        for i in range ( 3 , coefficient_count ) : \n            dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2 ] + T [ i - 1 ] + T [ i - 1 ] \n    dT *= 2.0 \n    dT /= interval_length \n    rates = ( dT . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        rates = rates [ : , 0 ] \n    return components , rates "}
{"6596": "\ndef delete_file_if_needed ( instance , filefield_name ) : \n    if instance . pk : \n        model_class = type ( instance ) \n        if model_class . objects . filter ( pk = instance . pk ) . exclude ( ** { '%s__isnull' % filefield_name : 1 } ) . exclude ( ** { '%s__exact' % filefield_name : '' } ) . exists ( ) : \n            old_file = getattr ( model_class . objects . only ( filefield_name ) . get ( pk = instance . pk ) , filefield_name ) \n        else : \n            old_file = None \n        if old_file : \n            if ( old_file . name == getattr ( instance , filefield_name ) ) is 0 : \n                DatabaseFileStorage ( ) . delete ( old_file . name ) "}
{"6639": "\ndef prune ( self , symbol : SecuritySymbol ) : \n    from . repositories import PriceRepository \n    assert isinstance ( symbol , SecuritySymbol ) \n    self . logger . debug ( f\"pruning prices for {symbol}\" ) \n    repo = PriceRepository ( ) \n    query = ( repo . query . filter ( dal . Price . namespace == symbol . namespace ) . filter ( dal . Price . symbol == symbol . mnemonic ) . order_by ( dal . Price . date . desc ( ) ) . order_by ( dal . Price . time . desc ( ) ) ) \n    all_prices = query . all ( ) \n    deleted = 0 \n    first = 1 \n    for single in all_prices : \n        if not first : \n            repo . query . filter ( dal . Price . id == single . id ) . delete ( ) \n            deleted = 1 \n            self . logger . debug ( f\"deleting {single.id}\" ) \n        else : \n            first = 0 \n    repo . save ( ) \n    return deleted "}
{"6643": "\ndef update_child_calls ( self ) : \n    for node in filter ( lambda n : len ( n . arg_name ) , self . child_list ) : \n        self . data [ \"bound_args\" ] . arguments [ node . arg_name ] = node . partial ( ) \n    self . updated = 1 "}
{"6644": "\ndef descend ( self , include_me = 1 ) : \n    if include_me : \n        yield self \n    for child in self . child_list : \n        yield child \n        yield from child . descend ( ) "}
{"6647": "\ndef has_equal_ast ( state , incorrect_msg = None , code = None , exact = 1 , append = None ) : \n    if utils . v2_only ( ) : \n        state . assert_is_not ( [ \"object_assignments\" ] , \"has_equal_ast\" , [ \"check_object\" ] ) \n        state . assert_is_not ( [ \"function_calls\" ] , \"has_equal_ast\" , [ \"check_function\" ] ) \n    if code and incorrect_msg is None : \n        raise InstructorError ( \"If you manually specify the code to match inside has_equal_ast(), \" \"you have to explicitly set the `incorrect_msg` argument.\" ) \n    if ( append is None ) : \n        append = incorrect_msg is None \n    if incorrect_msg is None : \n        incorrect_msg = \"Expected `{{sol_str}}`, but got `{{stu_str}}`.\" \n    def parse_tree ( tree ) : \n        crnt = ( tree . body [ 0 ] if isinstance ( tree , ast . Module ) and len ( tree . body ) == 1 else tree ) \n        return ast . dump ( crnt . value if isinstance ( crnt , ast . Expr ) else crnt ) \n    stu_rep = parse_tree ( state . student_ast ) \n    sol_rep = parse_tree ( state . solution_ast if not code else ast . parse ( code ) ) \n    fmt_kwargs = { \"sol_str\" : state . solution_code if not code else code , \"stu_str\" : state . student_code , } \n    _msg = state . build_message ( incorrect_msg , fmt_kwargs , append = append ) \n    if exact and not code : \n        state . do_test ( EqualTest ( stu_rep , sol_rep , Feedback ( _msg , state ) ) ) \n    elif not sol_rep in stu_rep : \n        state . report ( Feedback ( _msg , state ) ) \n    return state "}
{"6648": "\ndef has_code ( state , text , pattern = 1 , not_typed_msg = None ) : \n    if not not_typed_msg : \n        if pattern : \n            not_typed_msg = \"Could not find the correct pattern in your code.\" \n        else : \n            not_typed_msg = \"Could not find the following text in your code: %r\" % text \n    student_code = state . student_code \n    _msg = state . build_message ( not_typed_msg ) \n    state . do_test ( StringContainsTest ( student_code , text , pattern , Feedback ( _msg , state ) ) ) \n    return state "}
{"6649": "\ndef has_import ( state , name , same_as = 0 , not_imported_msg = \"Did you import `{{pkg}}`?\" , incorrect_as_msg = \"Did you import `{{pkg}}` as `{{alias}}`?\" , ) : \n    student_imports = state . ast_dispatcher ( \"imports\" , state . student_ast ) \n    solution_imports = state . ast_dispatcher ( \"imports\" , state . solution_ast ) \n    if name not in solution_imports : \n        raise InstructorError ( \"`has_import()` couldn't find an import of the package %s in your solution code.\" % name ) \n    fmt_kwargs = { \"pkg\" : name , \"alias\" : solution_imports [ name ] } \n    _msg = state . build_message ( not_imported_msg , fmt_kwargs ) \n    state . do_test ( DefinedCollTest ( name , student_imports , _msg ) ) \n    if same_as : \n        _msg = state . build_message ( incorrect_as_msg , fmt_kwargs ) \n        state . do_test ( EqualTest ( solution_imports [ name ] , student_imports [ name ] , _msg ) ) \n    return state "}
{"6650": "\ndef has_output ( state , text , pattern = 1 , no_output_msg = None ) : \n    if not no_output_msg : \n        no_output_msg = \"You did not output the correct things.\" \n    _msg = state . build_message ( no_output_msg ) \n    state . do_test ( StringContainsTest ( state . raw_student_output , text , pattern , _msg ) ) \n    return state "}
{"6651": "\ndef has_printout ( state , index , not_printed_msg = None , pre_code = None , name = None , copy = 0 ) : \n    extra_msg = \"If you want to check printouts done in e.g. a for loop, you have to use a `check_function('print')` chain instead.\" \n    state . assert_root ( \"has_printout\" , extra_msg = extra_msg ) \n    if not_printed_msg is None : \n        not_printed_msg = ( \"Have you used `{{sol_call}}` to do the appropriate printouts?\" ) \n    try : \n        sol_call_ast = state . ast_dispatcher ( \"function_calls\" , state . solution_ast ) [ \"print\" ] [ index ] [ \"node\" ] \n    except ( KeyError , IndexError ) : \n        raise InstructorError ( \"`has_printout({})` couldn't find the {} print call in your solution.\" . format ( index , utils . get_ord ( index + 1 ) ) ) \n    out_sol , str_sol = getOutputInProcess ( tree = sol_call_ast , process = state . solution_process , context = state . solution_context , env = state . solution_env , pre_code = pre_code , copy = copy , ) \n    sol_call_str = state . solution_ast_tokens . get_text ( sol_call_ast ) \n    if isinstance ( str_sol , Exception ) : \n        raise InstructorError ( \"Evaluating the solution expression {} raised error in solution process.\" \"Error: {} - {}\" . format ( sol_call_str , type ( out_sol ) , str_sol ) ) \n    _msg = state . build_message ( not_printed_msg , { \"sol_call\" : sol_call_str } ) \n    has_output ( state , out_sol . strip ( ) , pattern = 0 , no_output_msg = _msg ) \n    return state "}
{"6654": "\ndef check_function ( state , name , index = 0 , missing_msg = None , params_not_matched_msg = None , expand_msg = None , signature = 1 , ) : \n    append_missing = missing_msg is None \n    append_params_not_matched = params_not_matched_msg is None \n    if missing_msg is None : \n        missing_msg = MISSING_MSG \n    if expand_msg is None : \n        expand_msg = PREPEND_MSG \n    if params_not_matched_msg is None : \n        params_not_matched_msg = SIG_ISSUE_MSG \n    stu_out = state . ast_dispatcher ( \"function_calls\" , state . student_ast ) \n    sol_out = state . ast_dispatcher ( \"function_calls\" , state . solution_ast ) \n    student_mappings = state . ast_dispatcher ( \"mappings\" , state . student_ast ) \n    fmt_kwargs = { \"times\" : get_times ( index + 1 ) , \"ord\" : get_ord ( index + 1 ) , \"index\" : index , \"mapped_name\" : get_mapped_name ( name , student_mappings ) , } \n    try : \n        sol_parts = { ** sol_out [ name ] [ index ] } \n    except KeyError : \n        raise InstructorError ( \"`check_function()` couldn't find a call of `%s()` in the solution code. Make sure you get the mapping right!\" % name ) \n    except IndexError : \n        raise InstructorError ( \"`check_function()` couldn't find %s calls of `%s()` in your solution code.\" % ( index + 1 , name ) ) \n    try : \n        stu_parts = { ** stu_out [ name ] [ index ] } \n    except ( KeyError , IndexError ) : \n        _msg = state . build_message ( missing_msg , fmt_kwargs , append = append_missing ) \n        state . report ( Feedback ( _msg , state ) ) \n    if signature : \n        signature = None if isinstance ( signature , bool ) else signature \n        get_sig = partial ( getSignatureInProcess , name = name , signature = signature , manual_sigs = state . get_manual_sigs ( ) , ) \n        try : \n            sol_sig = get_sig ( mapped_name = sol_parts [ \"name\" ] , process = state . solution_process ) \n            sol_parts [ \"args\" ] = bind_args ( sol_sig , sol_parts [ \"args\" ] ) \n        except Exception as e : \n            raise InstructorError ( \"`check_function()` couldn't match the %s call of `%s` to its signature:\\n%s \" % ( get_ord ( index + 1 ) , name , e ) ) \n        try : \n            stu_sig = get_sig ( mapped_name = stu_parts [ \"name\" ] , process = state . student_process ) \n            stu_parts [ \"args\" ] = bind_args ( stu_sig , stu_parts [ \"args\" ] ) \n        except Exception : \n            _msg = state . build_message ( params_not_matched_msg , fmt_kwargs , append = append_params_not_matched ) \n            state . report ( Feedback ( _msg , StubState ( stu_parts [ \"node\" ] , state . highlighting_disabled ) ) ) \n    append_message = { \"msg\" : expand_msg , \"kwargs\" : fmt_kwargs } \n    child = part_to_child ( stu_parts , sol_parts , append_message , state , node_name = \"function_calls\" ) \n    return child "}
{"6658": "\ndef defined_items ( self ) : \n    return self . __class__ ( [ ( k , v ) for k , v in self . items ( ) if v is not self . EMPTY ] , is_empty = 0 ) "}
{"6669": "\ndef init_config ( self , config ) : \n    self . config . update ( config ) \n    self . config . setdefault ( 'LDAP_PORT' , 389 ) \n    self . config . setdefault ( 'LDAP_HOST' , None ) \n    self . config . setdefault ( 'LDAP_USE_SSL' , 0 ) \n    self . config . setdefault ( 'LDAP_READONLY' , 1 ) \n    self . config . setdefault ( 'LDAP_CHECK_NAMES' , 1 ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_CREDENTIALS' , 0 ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_PREFIX' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_SUFFIX' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_GET_USER_INFO' , 1 ) \n    self . config . setdefault ( 'LDAP_ALWAYS_SEARCH_BIND' , 0 ) \n    self . config . setdefault ( 'LDAP_BASE_DN' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_USER_DN' , None ) \n    self . config . setdefault ( 'LDAP_BIND_USER_PASSWORD' , None ) \n    self . config . setdefault ( 'LDAP_SEARCH_FOR_GROUPS' , 1 ) \n    self . config . setdefault ( 'LDAP_FAIL_AUTH_ON_MULTIPLE_FOUND' , 0 ) \n    self . config . setdefault ( 'LDAP_USER_DN' , '' ) \n    self . config . setdefault ( 'LDAP_GROUP_DN' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_AUTHENTICATION_TYPE' , 'SIMPLE' ) \n    self . config . setdefault ( 'LDAP_USER_SEARCH_SCOPE' , 'LEVEL' ) \n    self . config . setdefault ( 'LDAP_USER_OBJECT_FILTER' , '(objectclass=person)' ) \n    self . config . setdefault ( 'LDAP_USER_LOGIN_ATTR' , 'uid' ) \n    self . config . setdefault ( 'LDAP_USER_RDN_ATTR' , 'uid' ) \n    self . config . setdefault ( 'LDAP_GET_USER_ATTRIBUTES' , ldap3 . ALL_ATTRIBUTES ) \n    self . config . setdefault ( 'LDAP_GROUP_SEARCH_SCOPE' , 'LEVEL' ) \n    self . config . setdefault ( 'LDAP_GROUP_OBJECT_FILTER' , '(objectclass=group)' ) \n    self . config . setdefault ( 'LDAP_GROUP_MEMBERS_ATTR' , 'uniqueMember' ) \n    self . config . setdefault ( 'LDAP_GET_GROUP_ATTRIBUTES' , ldap3 . ALL_ATTRIBUTES ) \n    self . config . setdefault ( 'LDAP_ADD_SERVER' , 1 ) \n    if self . config [ 'LDAP_ADD_SERVER' ] : \n        self . add_server ( hostname = self . config [ 'LDAP_HOST' ] , port = self . config [ 'LDAP_PORT' ] , use_ssl = self . config [ 'LDAP_USE_SSL' ] ) "}
{"6680": "\ndef connection ( self ) : \n    ctx = stack . top \n    if ctx is None : \n        raise Exception ( \"Working outside of the Flask application \" \"context. If you wish to make a connection outside of a flask\" \" application context, please handle your connections \" \"and use manager.make_connection()\" ) \n    if hasattr ( ctx , 'ldap3_manager_main_connection' ) : \n        return ctx . ldap3_manager_main_connection \n    else : \n        connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) , contextualise = 0 ) \n        connection . bind ( ) \n        if ctx is not None : \n            ctx . ldap3_manager_main_connection = connection \n        return connection "}
{"6681": "\ndef make_connection ( self , bind_user = None , bind_password = None , ** kwargs ) : \n    return self . _make_connection ( bind_user , bind_password , contextualise = 0 , ** kwargs ) "}
{"6682": "\ndef _make_connection ( self , bind_user = None , bind_password = None , contextualise = 1 , ** kwargs ) : \n    authentication = ldap3 . ANONYMOUS \n    if bind_user : \n        authentication = getattr ( ldap3 , self . config . get ( 'LDAP_BIND_AUTHENTICATION_TYPE' ) ) \n    log . debug ( \"Opening connection with bind user '{0}'\" . format ( bind_user or 'Anonymous' ) ) \n    connection = ldap3 . Connection ( server = self . _server_pool , read_only = self . config . get ( 'LDAP_READONLY' ) , user = bind_user , password = bind_password , client_strategy = ldap3 . SYNC , authentication = authentication , check_names = self . config [ 'LDAP_CHECK_NAMES' ] , raise_exceptions = 1 , ** kwargs ) \n    if contextualise : \n        self . _contextualise_connection ( connection ) \n    return connection "}
{"6685": "\ndef label_search ( self , key = None , value = None ) : \n    if key is not None : \n        key = key . lower ( ) \n    if value is not None : \n        value = value . lower ( ) \n    show_details = 1 \n    if key is None and value is None : \n        url = '%s/labels/search' % ( self . base ) \n        show_details = 0 \n    elif key is not None and value is not None : \n        url = '%s/labels/search/%s/key/%s/value' % ( self . base , key , value ) \n    elif key is None : \n        url = '%s/labels/search/%s/value' % ( self . base , value ) \n    else : \n        url = '%s/labels/search/%s/key' % ( self . base , key ) \n    result = self . _get ( url ) \n    if len ( result ) == 0 : \n        bot . info ( \"No labels found.\" ) \n        sys . exit ( 0 ) \n    bot . info ( \"Labels\\n\" ) \n    rows = [ ] \n    for l in result : \n        if show_details is 1 : \n            entry = [ \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) , \"\\n%s\\n\\n\" % \"\\n\" . join ( l [ 'containers' ] ) ] \n        else : \n            entry = [ \"N=%s\" % len ( l [ 'containers' ] ) , \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) ] \n        rows . append ( entry ) \n    bot . table ( rows ) \n    return rows "}
{"6688": "\ndef speak ( self ) : \n    if self . quiet is 0 : \n        bot . info ( '[client|%s] [database|%s]' % ( self . client_name , self . database ) ) \n        self . _speak ( ) "}
{"6689": "\ndef announce ( self , command = None ) : \n    if command is not None : \n        if command not in [ 'get' ] and self . quiet is 0 : \n            self . speak ( ) "}
{"6691": "\ndef update_headers ( self , fields = None ) : \n    do_reset = 1 \n    if hasattr ( self , 'headers' ) : \n        if self . headers is not None : \n            do_reset = 0 \n    if do_reset is 1 : \n        self . _reset_headers ( ) \n    if fields is not None : \n        for key , value in fields . items ( ) : \n            self . headers [ key ] = value \n    header_names = \",\" . join ( list ( self . headers . keys ( ) ) ) \n    bot . debug ( \"Headers found: %s\" % header_names ) "}
{"6692": "\ndef require_secrets ( self , params = None ) : \n    name = self . client_name \n    has_secrets = 1 \n    if not hasattr ( self , 'secrets' ) : \n        has_secrets = 0 \n    elif hasattr ( self , 'secrets' ) : \n        if self . secrets is None : \n            has_secrets = 0 \n    elif self . client_name not in self . secrets : \n        has_secrets = 0 \n    if has_secrets is 0 : \n        message = '%s requires client secrets.' % name \n        bot . error ( message ) \n        sys . exit ( 1 ) \n    if params is not None : \n        if not isinstance ( params , list ) : \n            params = [ params ] \n        for param in params : \n            if param not in self . secrets [ name ] : \n                has_secrets = 0 \n            elif self . secrets [ name ] [ param ] in [ None , '' ] : \n                has_secrets = 0 \n        if has_secrets is 0 : \n            message = 'Missing %s in client secrets.' % param \n            bot . error ( message ) \n            sys . exit ( 1 ) "}
{"6693": "\ndef download ( url , file_name , headers = None , show_progress = 1 ) : \n    fd , tmp_file = tempfile . mkstemp ( prefix = ( \"%s.tmp.\" % file_name ) ) \n    os . close ( fd ) \n    if DISABLE_SSL_CHECK is 1 : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    verify = not DISABLE_SSL_CHECK \n    response = stream ( url , headers = headers , stream_to = tmp_file ) \n    shutil . move ( tmp_file , file_name ) \n    return file_name "}
{"6694": "\ndef stream ( url , headers , stream_to = None , retry = 1 ) : \n    bot . debug ( \"GET %s\" % url ) \n    if DISABLE_SSL_CHECK is 1 : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    response = requests . get ( url , headers = headers , verify = not DISABLE_SSL_CHECK , stream = 1 ) \n    if response . status_code in [ 401 , 403 ] : \n        headers = update_token ( headers ) \n        return stream ( url , headers , stream_to , retry = 0 ) \n    elif response . status_code == 200 : \n        content_size = None \n        if 'Content-Length' in response . headers : \n            progress = 0 \n            content_size = int ( response . headers [ 'Content-Length' ] ) \n            bot . show_progress ( progress , content_size , length = 35 ) \n        chunk_size = 1 << 20 \n        with open ( stream_to , 'wb' ) as filey : \n            for chunk in response . iter_content ( chunk_size = chunk_size ) : \n                filey . write ( chunk ) \n                if content_size is not None : \n                    progress += chunk_size \n                    bot . show_progress ( iteration = progress , total = content_size , length = 35 , carriage_return = 0 ) \n        sys . stdout . write ( '\\n' ) \n        return stream_to \n    bot . error ( \"Problem with stream, response %s\" % ( response . status_code ) ) \n    sys . exit ( 1 ) "}
{"6702": "\ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    print ( results ) \n    if name is not None : \n        for result in results : \n            matches = 0 \n            if name in result . name : \n                matches = 1 \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = 1 \n            if matches is 1 : \n                content = self . _print_log ( result . name ) \n    else : \n        if len ( results ) > 0 : \n            latest = results [ 0 ] \n            for result in results : \n                if result . time_created >= latest . time_created : \n                    latest = result \n            content = self . _print_log ( result . name ) \n    return content "}
{"6708": "\ndef add ( backend , variable , value , force = 0 ) : \n    print ( '[add]' ) \n    settings = read_client_secrets ( ) \n    prefix = 'SREGISTRY_%s_' % backend . upper ( ) \n    if not variable . startswith ( prefix ) : \n        variable = '%s%s' % ( prefix , variable ) \n    variable = variable . upper ( ) \n    bot . info ( \"%s %s\" % ( variable , value ) ) \n    if backend in settings : \n        if variable in settings [ backend ] and force is 0 : \n            previous = settings [ backend ] [ variable ] \n            bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) \n            sys . exit ( 1 ) \n    if backend not in settings : \n        settings [ backend ] = { } \n    settings [ backend ] [ variable ] = value \n    update_secrets ( settings ) "}
{"6716": "\ndef delete ( self , url , headers = None , return_json = 1 , default_headers = 1 ) : \n    bot . debug ( 'DELETE %s' % url ) \n    return self . _call ( url , headers = headers , func = requests . delete , return_json = return_json , default_headers = default_headers ) "}
{"6718": "\ndef paginate_get ( self , url , headers = None , return_json = 1 , start_page = None ) : \n    geturl = '%s&page=1' % ( url ) \n    if start_page is not None : \n        geturl = '%s&page=%s' % ( url , start_page ) \n    results = [ ] \n    while geturl is not None : \n        result = self . _get ( url , headers = headers , return_json = return_json ) \n        if isinstance ( result , dict ) : \n            if 'results' in result : \n                results = results + result [ 'results' ] \n            geturl = result [ 'next' ] \n        else : \n            return result \n    return results "}
{"6719": "\ndef verify ( self ) : \n    from sregistry . defaults import DISABLE_SSL_CHECK \n    if DISABLE_SSL_CHECK is 1 : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    return not DISABLE_SSL_CHECK "}
{"6720": "\ndef remove ( self , image , force = 0 ) : \n    q = parse_image_name ( remove_uri ( image ) ) \n    if q [ 'registry' ] == None : \n        q [ 'registry' ] = self . base \n    q = self . _add_https ( q ) \n    url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"delete\" , names = q ) \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    self . _update_headers ( fields = headers ) \n    continue_delete = 1 \n    if force is 0 : \n        response = input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) \n        while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in \"ynyesno\" : \n            response = input ( \"Please answer yes or no: \" ) \n        if response [ 0 ] . lower ( ) . strip ( ) in \"no\" : \n            continue_delete = 0 \n    if continue_delete is 1 : \n        response = self . _delete ( url ) \n        message = self . _read_response ( response ) \n        bot . info ( \"Response %s, %s\" % ( response . status_code , message ) ) \n    else : \n        bot . info ( \"Delete cancelled.\" ) "}
{"6724": "\ndef check_install ( software = None , quiet = 1 ) : \n    if software is None : \n        software = \"singularity\" \n    cmd = [ software , '--version' ] \n    try : \n        version = run_command ( cmd , software ) \n    except : \n        return 0 \n    if version is not None : \n        if quiet is 0 and version [ 'return_code' ] == 0 : \n            version = version [ 'message' ] \n            bot . info ( \"Found %s version %s\" % ( software . upper ( ) , version ) ) \n        return 1 \n    return 0 "}
{"6727": "\ndef run_command ( cmd , sudo = 0 ) : \n    if sudo is 1 : \n        cmd = [ 'sudo' ] + cmd \n    try : \n        output = Popen ( cmd , stderr = STDOUT , stdout = PIPE ) \n    except FileNotFoundError : \n        cmd . pop ( 0 ) \n        output = Popen ( cmd , stderr = STDOUT , stdout = PIPE ) \n    t = output . communicate ( ) [ 0 ] , output . returncode \n    output = { 'message' : t [ 0 ] , 'return_code' : t [ 1 ] } \n    if isinstance ( output [ 'message' ] , bytes ) : \n        output [ 'message' ] = output [ 'message' ] . decode ( 'utf-8' ) \n    return output "}
{"6736": "\ndef get_client ( image = None , quiet = 0 , ** kwargs ) : \n    from sregistry . defaults import SREGISTRY_CLIENT \n    if not check_install ( ) : \n        bot . warning ( 'Singularity is not installed, function might be limited.' ) \n    client_name = get_uri ( image ) \n    if client_name is not None : \n        SREGISTRY_CLIENT = client_name \n    if SREGISTRY_CLIENT == 'aws' : \n        from . aws import Client \n    elif SREGISTRY_CLIENT == 'docker' : \n        from . docker import Client \n    elif SREGISTRY_CLIENT == 'dropbox' : \n        from . dropbox import Client \n    elif SREGISTRY_CLIENT == 'gitlab' : \n        from . gitlab import Client \n    elif SREGISTRY_CLIENT == 'globus' : \n        from . globus import Client \n    elif SREGISTRY_CLIENT == 'nvidia' : \n        from . nvidia import Client \n    elif SREGISTRY_CLIENT == 'hub' : \n        from . hub import Client \n    elif SREGISTRY_CLIENT == 'google-drive' : \n        from . google_drive import Client \n    elif SREGISTRY_CLIENT == 'google-compute' : \n        from . google_storage import Client \n    elif SREGISTRY_CLIENT == 'google-storage' : \n        from . google_storage import Client \n    elif SREGISTRY_CLIENT == 'google-build' : \n        from . google_build import Client \n    elif SREGISTRY_CLIENT == 'registry' : \n        from . registry import Client \n    elif SREGISTRY_CLIENT == 's3' : \n        from . s3 import Client \n    elif SREGISTRY_CLIENT == 'swift' : \n        from . swift import Client \n    else : \n        from . hub import Client \n    Client . client_name = SREGISTRY_CLIENT \n    Client . quiet = quiet \n    Client . _credential_cache = get_credential_cache ( ) \n    if SREGISTRY_DATABASE is not None : \n        from sregistry . database import ( init_db , add , cp , get , mv , rm , rmi , images , inspect , rename , get_container , get_collection , get_or_create_collection ) \n        Client . _init_db = init_db \n        Client . add = add \n        Client . cp = cp \n        Client . get = get \n        Client . inspect = inspect \n        Client . mv = mv \n        Client . rename = rename \n        Client . rm = rm \n        Client . rmi = rmi \n        Client . images = images \n        Client . get_or_create_collection = get_or_create_collection \n        Client . get_container = get_container \n        Client . get_collection = get_collection \n    else : \n        from sregistry . database import ( add , init_db ) \n        Client . add = add \n        Client . _init_db = init_db \n    cli = Client ( ) \n    if hasattr ( Client , '_init_db' ) : \n        cli . _init_db ( SREGISTRY_DATABASE ) \n    return cli "}
{"6739": "\ndef get_manifest ( self , repo_name , digest = None , version = \"v1\" ) : \n    accepts = { 'config' : \"application/vnd.docker.container.image.v1+json\" , 'v1' : \"application/vnd.docker.distribution.manifest.v1+json\" , 'v2' : \"application/vnd.docker.distribution.manifest.v2+json\" } \n    url = self . _get_manifest_selfLink ( repo_name , digest ) \n    bot . verbose ( \"Obtaining manifest: %s %s\" % ( url , version ) ) \n    headers = { 'Accept' : accepts [ version ] } \n    try : \n        manifest = self . _get ( url , headers = headers , quiet = 1 ) \n        manifest [ 'selfLink' ] = url \n    except : \n        manifest = None \n    return manifest "}
{"6757": "\ndef init_db ( self , db_path ) : \n    self . database = 'sqlite:///%s' % db_path \n    self . storage = SREGISTRY_STORAGE \n    bot . debug ( \"Database located at %s\" % self . database ) \n    self . engine = create_engine ( self . database , convert_unicode = 1 ) \n    self . session = scoped_session ( sessionmaker ( autocommit = 0 , autoflush = 0 , bind = self . engine ) ) \n    Base . query = self . session . query_property ( ) \n    Base . metadata . create_all ( bind = self . engine ) \n    self . Base = Base "}
{"6762": "\ndef share ( self , query , share_to = None ) : \n    names = parse_image_name ( remove_uri ( query ) ) \n    dropbox_path = '/%s' % names [ 'storage' ] \n    if self . exists ( dropbox_path ) is 1 : \n        try : \n            share = self . dbx . sharing_create_shared_link_with_settings ( dropbox_path ) \n        except ApiError as err : \n            share = self . dbx . sharing_create_shared_link ( dropbox_path ) \n        bot . info ( share . url ) \n    return share . url "}
{"6767": "\ndef destroy ( self , name ) : \n    instances = self . _get_instances ( ) \n    project = self . _get_project ( ) \n    zone = self . _get_zone ( ) \n    found = 0 \n    if 'items' in instances : \n        for instance in instances [ 'items' ] : \n            if instance [ 'name' ] == name : \n                found = 1 \n                break \n    if found : \n        bot . info ( 'Killing instance %s' % name ) \n        return self . _compute_service . instances ( ) . delete ( project = project , zone = zone , instance = name ) . execute ( ) "}
{"6770": "\ndef get_tmpdir ( requested_tmpdir = None , prefix = \"\" , create = 1 ) : \n    from sregistry . defaults import SREGISTRY_TMPDIR \n    tmpdir = requested_tmpdir or SREGISTRY_TMPDIR \n    prefix = prefix or \"sregistry-tmp\" \n    prefix = \"%s.%s\" % ( prefix , next ( tempfile . _get_candidate_names ( ) ) ) \n    tmpdir = os . path . join ( tmpdir , prefix ) \n    if not os . path . exists ( tmpdir ) and create is 1 : \n        os . mkdir ( tmpdir ) \n    return tmpdir "}
{"6771": "\ndef extract_tar ( archive , output_folder , handle_whiteout = 0 ) : \n    from . terminal import run_command \n    if handle_whiteout is 1 : \n        return _extract_tar ( archive , output_folder ) \n    args = '-xf' \n    if archive . endswith ( \".tar.gz\" ) : \n        args = '-xzf' \n    command = [ \"tar\" , args , archive , \"-C\" , output_folder , \"--exclude=dev/*\" ] \n    if not bot . is_quiet ( ) : \n        print ( \"Extracting %s\" % archive ) \n    return run_command ( command ) "}
{"6774": "\ndef read_file ( filename , mode = \"r\" , readlines = 1 ) : \n    with open ( filename , mode ) as filey : \n        if readlines is 1 : \n            content = filey . readlines ( ) \n        else : \n            content = filey . read ( ) \n    return content "}
{"6782": "\ndef inspect ( self , name ) : \n    print ( name ) \n    container = self . get ( name ) \n    if container is not None : \n        collection = container . collection . name \n        fields = container . __dict__ . copy ( ) \n        fields [ 'collection' ] = collection \n        fields [ 'metrics' ] = json . loads ( fields [ 'metrics' ] ) \n        del fields [ '_sa_instance_state' ] \n        fields [ 'created_at' ] = str ( fields [ 'created_at' ] ) \n        print ( json . dumps ( fields , indent = 4 , sort_keys = 1 ) ) \n        return fields "}
{"6783": "\ndef rename ( self , image_name , path ) : \n    container = self . get ( image_name , quiet = 1 ) \n    if container is not None : \n        if container . image is not None : \n            dirname = os . path . dirname ( container . image ) \n            names = parse_image_name ( remove_uri ( path ) ) \n            storage = os . path . join ( self . storage , os . path . dirname ( names [ 'storage' ] ) ) \n            if not os . path . exists ( storage ) : \n                os . mkdir ( storage ) \n            fullpath = os . path . abspath ( os . path . join ( dirname , names [ 'storage' ] ) ) \n            container = self . cp ( move_to = fullpath , container = container , command = \"rename\" ) \n            if container is not None : \n                container . uri = names [ 'uri' ] \n                self . session . commit ( ) \n                return container \n    bot . warning ( '%s not found' % ( image_name ) ) "}
{"6784": "\ndef mv ( self , image_name , path ) : \n    container = self . get ( image_name , quiet = 1 ) \n    if container is not None : \n        name = container . uri or container . get_uri ( ) \n        image = container . image or '' \n        if os . path . exists ( image ) : \n            filename = os . path . basename ( image ) \n            filedir = os . path . abspath ( path ) \n            if not os . path . isdir ( path ) : \n                filename = os . path . basename ( path ) \n                filedir = os . path . dirname ( path ) \n            if filedir == '' : \n                filedir = os . getcwd ( ) \n            fullpath = os . path . abspath ( os . path . join ( filedir , filename ) ) \n            return self . cp ( move_to = fullpath , container = container , command = \"move\" ) \n    bot . warning ( '%s not found' % ( image_name ) ) "}
{"6785": "\ndef rmi ( self , image_name ) : \n    container = self . rm ( image_name , delete = 1 ) \n    if container is not None : \n        bot . info ( \"[rmi] %s\" % container ) "}
{"6786": "\ndef add ( self , image_path = None , image_uri = None , image_name = None , url = None , metadata = None , save = 1 , copy = 0 ) : \n    from sregistry . database . models import ( Container , Collection ) \n    if image_path is not None : \n        if not os . path . exists ( image_path ) and save is 1 : \n            bot . error ( 'Cannot find %s' % image_path ) \n            sys . exit ( 1 ) \n    if image_uri is None : \n        bot . error ( 'You must provide an image uri <collection>/<namespace>' ) \n        sys . exit ( 1 ) \n    names = parse_image_name ( remove_uri ( image_uri ) ) \n    bot . debug ( 'Adding %s to registry' % names [ 'uri' ] ) \n    metadata = self . get_metadata ( image_path , names = names ) \n    collection = self . get_or_create_collection ( names [ 'collection' ] ) \n    version = names . get ( 'version' ) \n    if version == None : \n        if image_path != None : \n            version = get_image_hash ( image_path ) \n        else : \n            version = '' \n        names = parse_image_name ( remove_uri ( image_uri ) , version = version ) \n    if save is 1 and image_path is not None : \n        if image_name is None : \n            image_name = self . _get_storage_name ( names ) \n        if copy is 1 : \n            copyfile ( image_path , image_name ) \n        else : \n            shutil . move ( image_path , image_name ) \n        image_path = image_name \n    if url is None and \"url\" in metadata : \n        url = metadata [ 'url' ] \n    container = self . get_container ( name = names [ 'image' ] , collection_id = collection . id , tag = names [ 'tag' ] , version = version ) \n    if container is None : \n        action = \"new\" \n        container = Container ( metrics = json . dumps ( metadata ) , name = names [ 'image' ] , image = image_path , client = self . client_name , tag = names [ 'tag' ] , version = version , url = url , uri = names [ 'uri' ] , collection_id = collection . id ) \n        self . session . add ( container ) \n        collection . containers . append ( container ) \n    else : \n        action = \"update\" \n        metrics = json . loads ( container . metrics ) \n        metrics . update ( metadata ) \n        container . url = url \n        container . client = self . client_name \n        if image_path is not None : \n            container . image = image_path \n        container . metrics = json . dumps ( metrics ) \n    self . session . commit ( ) \n    bot . info ( \"[container][%s] %s\" % ( action , names [ 'uri' ] ) ) \n    return container "}
{"6788": "\ndef parse_header ( recipe , header = \"from\" , remove_header = 1 ) : \n    parsed_header = None \n    fromline = [ x for x in recipe . split ( '\\n' ) if \"%s:\" % header in x . lower ( ) ] \n    if len ( fromline ) == 0 : \n        return \"\" \n    if len ( fromline ) > 0 : \n        fromline = fromline [ 0 ] \n        parsed_header = fromline . strip ( ) \n    if remove_header is 1 : \n        parsed_header = fromline . split ( ':' , 1 ) [ - 1 ] . strip ( ) \n    return parsed_header "}
{"6794": "\ndef useColor ( self ) : \n    COLORIZE = get_user_color_preference ( ) \n    if COLORIZE is not None : \n        return COLORIZE \n    streams = [ self . errorStream , self . outputStream ] \n    for stream in streams : \n        if not hasattr ( stream , 'isatty' ) : \n            return 0 \n        if not stream . isatty ( ) : \n            return 0 \n    return 1 "}
{"6795": "\ndef emitError ( self , level ) : \n    if level in [ ABORT , ERROR , WARNING , VERBOSE , VERBOSE1 , VERBOSE2 , VERBOSE3 , DEBUG ] : \n        return 1 \n    return 0 "}
{"6798": "\ndef push ( self , path , name , tag = None ) : \n    endpoint , remote = self . _parse_endpoint_name ( name ) \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    q = parse_image_name ( image ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    if not hasattr ( self , 'transfer_client' ) : \n        self . _init_transfer_client ( ) \n    endpoints = self . _get_endpoints ( ) \n    if len ( endpoints [ 'my-endpoints' ] ) == 0 : \n        bot . error ( 'You must have a personal endpoint to transfer the container' ) \n        sys . exit ( 1 ) \n    source_endpoint = None \n    for eid , contender in endpoints [ 'my-endpoints' ] . items ( ) : \n        if contender [ 'gcp_connected' ] is 1 : \n            source_endpoint = contender \n            break \n    if source_endpoint is None : \n        bot . error ( 'No activated local endpoints online! Go online to transfer' ) \n        sys . exit ( 1 ) \n    self . _create_endpoint_cache ( endpoint ) \n    added = self . add ( image_path = path , image_uri = q [ 'uri' ] , copy = 1 ) \n    label = \"Singularity Registry Transfer for %s\" % added . name \n    tdata = globus_sdk . TransferData ( self . transfer_client , source_endpoint [ 'id' ] , endpoint , label = label , sync_level = \"checksum\" ) \n    image = \".singularity/shub/%s\" % image \n    tdata . add_item ( added . image , image ) \n    bot . info ( 'Requesting transfer from local %s to %s:%s' % ( SREGISTRY_STORAGE , endpoint , image ) ) \n    transfer_result = self . transfer_client . submit_transfer ( tdata ) \n    bot . info ( transfer_result [ 'message' ] ) \n    return transfer_result "}
{"6808": "\ndef has_gravatar ( email ) : \n    url = get_gravatar_url ( email , default = GRAVATAR_DEFAULT_IMAGE_404 ) \n    try : \n        request = Request ( url ) \n        request . get_method = lambda : 'HEAD' \n        return 200 == urlopen ( request ) . code \n    except ( HTTPError , URLError ) : \n        return 0 "}
{"6812": "\ndef enumerate_resonance_smiles ( smiles ) : \n    mol = Chem . MolFromSmiles ( smiles ) \n    mesomers = ResonanceEnumerator ( ) . enumerate ( mol ) \n    return { Chem . MolToSmiles ( m , isomericSmiles = 1 ) for m in mesomers } "}
{"6814": "\ndef normalize ( self , mol ) : \n    log . debug ( 'Running Normalizer' ) \n    fragments = [ ] \n    for fragment in Chem . GetMolFrags ( mol , asMols = 1 ) : \n        fragments . append ( self . _normalize_fragment ( fragment ) ) \n    outmol = fragments . pop ( ) \n    for fragment in fragments : \n        outmol = Chem . CombineMols ( outmol , fragment ) \n    Chem . SanitizeMol ( outmol ) \n    return outmol "}
{"6815": "\ndef _apply_transform ( self , mol , rule ) : \n    mols = [ mol ] \n    for n in six . moves . range ( 20 ) : \n        products = { } \n        for mol in mols : \n            for product in [ x [ 0 ] for x in rule . RunReactants ( ( mol , ) ) ] : \n                if Chem . SanitizeMol ( product , catchErrors = 1 ) == 0 : \n                    products [ Chem . MolToSmiles ( product , isomericSmiles = 1 ) ] = product \n        if products : \n            mols = [ products [ s ] for s in sorted ( products ) ] \n        else : \n            return mols [ 0 ] if n > 0 else None "}
{"6816": "\ndef canonicalize ( self , mol ) : \n    tautomers = self . _enumerate_tautomers ( mol ) \n    if len ( tautomers ) == 1 : \n        return tautomers [ 0 ] \n    highest = None \n    for t in tautomers : \n        smiles = Chem . MolToSmiles ( t , isomericSmiles = 1 ) \n        log . debug ( 'Tautomer: %s' , smiles ) \n        score = 0 \n        ssr = Chem . GetSymmSSSR ( t ) \n        for ring in ssr : \n            btypes = { t . GetBondBetweenAtoms ( * pair ) . GetBondType ( ) for pair in pairwise ( ring ) } \n            elements = { t . GetAtomWithIdx ( idx ) . GetAtomicNum ( ) for idx in ring } \n            if btypes == { BondType . AROMATIC } : \n                log . debug ( 'Score +100 (aromatic ring)' ) \n                score += 100 \n                if elements == { 6 } : \n                    log . debug ( 'Score +150 (carbocyclic aromatic ring)' ) \n                    score += 150 \n        for tscore in self . scores : \n            for match in t . GetSubstructMatches ( tscore . smarts ) : \n                log . debug ( 'Score %+d (%s)' , tscore . score , tscore . name ) \n                score += tscore . score \n        for atom in t . GetAtoms ( ) : \n            if atom . GetAtomicNum ( ) in { 15 , 16 , 34 , 52 } : \n                hs = atom . GetTotalNumHs ( ) \n                if hs : \n                    log . debug ( 'Score %+d (%s-H bonds)' , - hs , atom . GetSymbol ( ) ) \n                    score -= hs \n        if not highest or highest [ 'score' ] < score or ( highest [ 'score' ] == score and smiles < highest [ 'smiles' ] ) : \n            log . debug ( 'New highest tautomer: %s (%s)' , smiles , score ) \n            highest = { 'smiles' : smiles , 'tautomer' : t , 'score' : score } \n    return highest [ 'tautomer' ] "}
{"6819": "\ndef standardize_smiles ( smiles ) : \n    mol = Chem . MolFromSmiles ( smiles , sanitize = 0 ) \n    mol = Standardizer ( ) . standardize ( mol ) \n    return Chem . MolToSmiles ( mol , isomericSmiles = 1 ) "}
{"6820": "\ndef enumerate_tautomers_smiles ( smiles ) : \n    mol = Chem . MolFromSmiles ( smiles , sanitize = 0 ) \n    mol = Standardizer ( ) . standardize ( mol ) \n    tautomers = TautomerEnumerator ( ) . enumerate ( mol ) \n    return { Chem . MolToSmiles ( m , isomericSmiles = 1 ) for m in tautomers } "}
{"6821": "\ndef canonicalize_tautomer_smiles ( smiles ) : \n    mol = Chem . MolFromSmiles ( smiles , sanitize = 0 ) \n    mol = Standardizer ( ) . standardize ( mol ) \n    tautomer = TautomerCanonicalizer ( ) . canonicalize ( mol ) \n    return Chem . MolToSmiles ( tautomer , isomericSmiles = 1 ) "}
{"6822": "\ndef standardize ( self , mol ) : \n    mol = copy . deepcopy ( mol ) \n    Chem . SanitizeMol ( mol ) \n    mol = Chem . RemoveHs ( mol ) \n    mol = self . disconnect_metals ( mol ) \n    mol = self . normalize ( mol ) \n    mol = self . reionize ( mol ) \n    Chem . AssignStereochemistry ( mol , force = 1 , cleanIt = 1 ) \n    return mol "}
{"6823": "\ndef tautomer_parent ( self , mol , skip_standardize = 0 ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    tautomer = self . canonicalize_tautomer ( mol ) \n    tautomer = self . standardize ( tautomer ) \n    return tautomer "}
{"6824": "\ndef fragment_parent ( self , mol , skip_standardize = 0 ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    fragment = self . largest_fragment ( mol ) \n    return fragment "}
{"6825": "\ndef stereo_parent ( self , mol , skip_standardize = 0 ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    else : \n        mol = copy . deepcopy ( mol ) \n    Chem . RemoveStereochemistry ( mol ) \n    return mol "}
{"6826": "\ndef isotope_parent ( self , mol , skip_standardize = 0 ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    else : \n        mol = copy . deepcopy ( mol ) \n    for atom in mol . GetAtoms ( ) : \n        atom . SetIsotope ( 0 ) \n    return mol "}
{"6827": "\ndef charge_parent ( self , mol , skip_standardize = 0 ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    fragment = self . fragment_parent ( mol , skip_standardize = 1 ) \n    if fragment : \n        uncharged = self . uncharge ( fragment ) \n        uncharged = self . standardize ( uncharged ) \n        return uncharged "}
{"6828": "\ndef super_parent ( self , mol , skip_standardize = 0 ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    mol = self . charge_parent ( mol , skip_standardize = 1 ) \n    mol = self . isotope_parent ( mol , skip_standardize = 1 ) \n    mol = self . stereo_parent ( mol , skip_standardize = 1 ) \n    mol = self . tautomer_parent ( mol , skip_standardize = 1 ) \n    mol = self . standardize ( mol ) \n    return mol "}
{"6829": "\ndef main ( ) : \n    parser = MolvsParser ( epilog = 'use \"molvs <command> -h\" to show help for a specific command' ) \n    subparsers = parser . add_subparsers ( title = 'Available commands' ) \n    common_parser = MolvsParser ( add_help = 0 ) \n    common_parser . add_argument ( 'infile' , nargs = '?' , help = 'input filename' , type = argparse . FileType ( 'r' ) , default = sys . stdin ) \n    common_parser . add_argument ( '-i' , '--intype' , help = 'input filetype' , choices = FILETYPES ) \n    common_parser . add_argument ( '-:' , '--smiles' , help = 'input SMILES instead of file' , metavar = '<smiles>' ) \n    common_parser . add_argument ( '-O' , '--outfile' , help = 'output filename' , type = argparse . FileType ( 'w' ) , default = sys . stdout , metavar = '<outfile>' ) \n    standardize_parser = subparsers . add_parser ( 'standardize' , help = 'standardize a molecule' , parents = [ common_parser ] ) \n    standardize_parser . add_argument ( '-o' , '--outtype' , help = 'output filetype' , choices = FILETYPES ) \n    standardize_parser . set_defaults ( func = standardize_main ) \n    validate_parser = subparsers . add_parser ( 'validate' , help = 'validate a molecule' , parents = [ common_parser ] ) \n    validate_parser . set_defaults ( func = validate_main ) \n    args = parser . parse_args ( ) \n    try : \n        args . func ( args ) \n    except Exception as e : \n        sys . stderr . write ( 'Error: %s\\n\\n' . encode ( ) % e . message ) \n        parser . print_help ( ) \n        sys . exit ( 2 ) "}
{"6830": "\ndef remove ( self , mol ) : \n    log . debug ( 'Running FragmentRemover' ) \n    for frag in self . fragments : \n        if mol . GetNumAtoms ( ) == 0 or ( self . leave_last and len ( Chem . GetMolFrags ( mol ) ) <= 1 ) : \n            break \n        removed = Chem . DeleteSubstructs ( mol , frag . smarts , onlyFrags = 1 ) \n        if not mol . GetNumAtoms ( ) == removed . GetNumAtoms ( ) : \n            log . info ( 'Removed fragment: %s' , frag . name ) \n        if self . leave_last and removed . GetNumAtoms ( ) == 0 : \n            break \n        mol = removed \n    return mol "}
{"6831": "\ndef choose ( self , mol ) : \n    log . debug ( 'Running LargestFragmentChooser' ) \n    fragments = Chem . GetMolFrags ( mol , asMols = 1 ) \n    largest = None \n    for f in fragments : \n        smiles = Chem . MolToSmiles ( f , isomericSmiles = 1 ) \n        log . debug ( 'Fragment: %s' , smiles ) \n        organic = is_organic ( f ) \n        if self . prefer_organic : \n            if largest and largest [ 'organic' ] and not organic : \n                continue \n            if largest and organic and not largest [ 'organic' ] : \n                largest = None \n        atoms = 0 \n        for a in f . GetAtoms ( ) : \n            atoms += 1 + a . GetTotalNumHs ( ) \n        if largest and atoms < largest [ 'atoms' ] : \n            continue \n        weight = rdMolDescriptors . CalcExactMolWt ( f ) \n        if largest and atoms == largest [ 'atoms' ] and weight < largest [ 'weight' ] : \n            continue \n        if largest and atoms == largest [ 'atoms' ] and weight == largest [ 'weight' ] and smiles > largest [ 'smiles' ] : \n            continue \n        log . debug ( 'New largest fragment: %s (%s)' , smiles , atoms ) \n        largest = { 'smiles' : smiles , 'fragment' : f , 'atoms' : atoms , 'weight' : weight , 'organic' : organic } \n    return largest [ 'fragment' ] "}
{"6832": "\ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = 0 , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = 0 ) : \n    f , j = get_f_and_j ( mu ) \n    if nt > 1 : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = 0 , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = 0 , method = method ) \n    if verbose : \n        print ( nfo ) \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , 1 ] , 'g--' ) \n        plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) \n        if savefig == 'None' : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6839": "\ndef get_issues ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) \n    is_only_today = 0 \n    if not os . path . exists ( path ) : \n        all_issues = repo . iter_issues ( state = 'all' ) \n        is_only_today = 1 \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2 : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_issues = repo . iter_issues ( state = 'all' ) \n                is_only_today = 1 \n        if not is_only_today : \n            all_issues = repo . iter_issues ( since = date , state = 'all' ) \n    for issue in all_issues : \n        self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) \n    closed_issues = 0 \n    for issue in repo . iter_issues ( state = 'closed' ) : \n        if issue is not None : \n            closed_issues += 1 \n    return closed_issues "}
{"6842": "\ndef get_commits ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) \n    is_only_today = 0 \n    if not os . path . exists ( path ) : \n        all_commits = repo . iter_commits ( ) \n        is_only_today = 1 \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2 : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_commits = repo . iter_commits ( ) \n                is_only_today = 1 \n        if not is_only_today : \n            all_commits = repo . iter_commits ( since = date ) \n    for commit in all_commits : \n        self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) \n    count = 0 \n    for commit in repo . iter_commits ( ) : \n        count += 1 \n    return count "}
{"6843": "\ndef write_org_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' , is_list = 0 ) : \n    path = ( '../github-data/' + organization + '-org/' + path_ending_type + '/' + str ( date ) + '.json' ) \n    self . checkDir ( path ) \n    with open ( path , 'w' ) as out_clear : \n        out_clear . close ( ) \n    with open ( path , 'a' ) as out : \n        if is_list : \n            out . write ( '[' ) \n        for item in dict_to_write : \n            out . write ( json . dumps ( dict_to_write [ item ] , sort_keys = 1 , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) \n        out . seek ( - 1 , os . SEEK_END ) \n        out . truncate ( ) \n        if is_list : \n            out . write ( ']' ) \n    out . close ( ) "}
{"6852": "\ndef query_repos ( gh_session , orgs = None , repos = None , public_only = 1 ) : \n    if orgs is None : \n        orgs = [ ] \n    if repos is None : \n        repos = [ ] \n    if public_only : \n        privacy = 'public' \n    else : \n        privacy = 'all' \n    _check_api_limits ( gh_session , 10 ) \n    for org_name in orgs : \n        org = gh_session . organization ( org_name ) \n        num_repos = org . public_repos_count \n        _check_api_limits ( gh_session , _num_requests_needed ( num_repos ) ) \n        for repo in org . repositories ( type = privacy ) : \n            _check_api_limits ( gh_session , 10 ) \n            yield repo \n    for repo_name in repos : \n        _check_api_limits ( gh_session , 10 ) \n        org , name = repo_name . split ( '/' ) \n        yield gh_session . repository ( org , name ) \n    if not ( orgs or repos ) : \n        for repo in gh_session . all_repositories ( ) : \n            yield repo "}
{"6855": "\ndef from_gitlab ( klass , repository , labor_hours = 1 ) : \n    if not isinstance ( repository , gitlab . v4 . objects . Project ) : \n        raise TypeError ( 'Repository must be a gitlab Repository object' ) \n    project = klass ( ) \n    logger . debug ( 'GitLab: repository_id=%d path_with_namespace=%s' , repository . id , repository . path_with_namespace , ) \n    project [ 'name' ] = repository . name \n    project [ 'repositoryURL' ] = repository . http_url_to_repo \n    project [ 'description' ] = repository . description \n    project [ 'permissions' ] [ 'licenses' ] = None \n    web_url = repository . web_url \n    public_server = web_url . startswith ( 'https://gitlab.com' ) \n    if repository . visibility in ( 'public' ) and public_server : \n        project [ 'permissions' ] [ 'usageType' ] = 'openSource' \n    elif date_parse ( repository . created_at ) < POLICY_START_DATE : \n        project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' \n    if labor_hours : \n        project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) \n    else : \n        project [ 'laborHours' ] = 0 \n    project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list \n    project [ 'contact' ] = { 'email' : '' , 'URL' : web_url , } \n    project [ 'organization' ] = repository . namespace [ 'name' ] \n    project [ 'status' ] = 'Development' \n    project [ 'vcs' ] = 'git' \n    project [ 'homepageURL' ] = repository . web_url \n    api_url = repository . manager . gitlab . _url \n    archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) \n    project [ 'downloadURL' ] = api_url + archive_suffix \n    project [ 'date' ] = { 'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 'metadataLastUpdated' : '' , } \n    _prune_dict_null_str ( project ) \n    return project "}
{"6862": "\ndef write_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' ) : \n    for repo in dict_to_write : \n        if len ( dict_to_write [ repo ] ) != 0 : \n            path = ( '../github-data/' + organization + '/' + repo + '/' + path_ending_type + '/' + str ( date ) + '.json' ) \n            self . checkDir ( path ) \n            with open ( path , 'w' ) as out : \n                out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = 1 , indent = 4 , separators = ( ',' , ': ' ) ) ) \n            out . close ( ) "}
{"6875": "\ndef query_repos ( gl_session , repos = None ) : \n    if repos is None : \n        repos = [ ] \n    for repo in repos : \n        yield gl_session . projects . get ( repo ) \n    if not repos : \n        for project in gl_session . projects . list ( as_list = 0 ) : \n            yield project "}
{"6879": "\ndef _readGQL ( self , filePath , verbose = 0 ) : \n    if not os . path . isfile ( filePath ) : \n        raise RuntimeError ( \"Query file '%s' does not exist.\" % ( filePath ) ) \n    lastModified = os . path . getmtime ( filePath ) \n    absPath = os . path . abspath ( filePath ) \n    if absPath == self . __queryPath and lastModified == self . __queryTimestamp : \n        _vPrint ( verbose , \"Using cached query '%s'\" % ( os . path . basename ( self . __queryPath ) ) ) \n        query_in = self . __query \n    else : \n        _vPrint ( verbose , \"Reading '%s' ... \" % ( filePath ) , end = \"\" , flush = 1 ) \n        with open ( filePath , \"r\" ) as q : \n            query_in = re . sub ( r'#.*(\\n|\\Z)' , '\\n' , q . read ( ) ) \n            query_in = re . sub ( r'\\s+' , ' ' , query_in ) \n            query_in = re . sub ( r'(\\A\\s+)|(\\s+\\Z)' , '' , query_in ) \n        _vPrint ( verbose , \"File read!\" ) \n        self . __queryPath = absPath \n        self . __queryTimestamp = lastModified \n        self . __query = query_in \n    return query_in "}
{"6881": "\ndef _submitQuery ( self , gitquery , gitvars = { } , verbose = 0 , rest = 0 ) : \n    errOut = DEVNULL if not verbose else None \n    authhead = 'Authorization: bearer ' + self . __githubApiToken \n    bashcurl = 'curl -iH TMPauthhead -X POST -d TMPgitquery https://api.github.com/graphql' if not rest else 'curl -iH TMPauthhead https://api.github.com' + gitquery \n    bashcurl_list = bashcurl . split ( ) \n    bashcurl_list [ 2 ] = authhead \n    if not rest : \n        gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) \n        bashcurl_list [ 6 ] = gitqueryJSON \n    fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) \n    _vPrint ( verbose , \"\\n\" + fullResponse ) \n    fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) \n    heads = fullResponse [ 0 ] . split ( '\\r\\n' ) \n    if len ( fullResponse ) > 1 : \n        result = fullResponse [ 1 ] \n    else : \n        result = \"\" \n    http = heads [ 0 ] . split ( ) \n    statusNum = int ( http [ 1 ] ) \n    headDict = { } \n    headDict [ \"http\" ] = heads [ 0 ] \n    for header in heads [ 1 : ] : \n        h = header . split ( ': ' ) \n        headDict [ h [ 0 ] ] = h [ 1 ] \n    linkDict = None \n    if \"Link\" in headDict : \n        linkProperties = headDict [ \"Link\" ] . split ( ', ' ) \n        propDict = { } \n        for item in linkProperties : \n            divided = re . split ( r'<https://api.github.com|>; rel=\"|\"' , item ) \n            propDict [ divided [ 2 ] ] = divided [ 1 ] \n        linkDict = propDict \n    return { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } "}
{"6882": "\ndef _awaitReset ( self , utcTimeStamp , verbose = 1 ) : \n    resetTime = pytz . utc . localize ( datetime . utcfromtimestamp ( utcTimeStamp ) ) \n    _vPrint ( verbose , \"--- Current Timestamp\" ) \n    _vPrint ( verbose , \"      %s\" % ( time . strftime ( '%c' ) ) ) \n    now = pytz . utc . localize ( datetime . utcnow ( ) ) \n    waitTime = round ( ( resetTime - now ) . total_seconds ( ) ) + 1 \n    _vPrint ( verbose , \"--- Current UTC Timestamp\" ) \n    _vPrint ( verbose , \"      %s\" % ( now . strftime ( '%c' ) ) ) \n    _vPrint ( verbose , \"--- GITHUB NEEDS A BREAK Until UTC Timestamp\" ) \n    _vPrint ( verbose , \"      %s\" % ( resetTime . strftime ( '%c' ) ) ) \n    self . _countdown ( waitTime , printString = \"--- Waiting %*d seconds...\" , verbose = verbose ) \n    _vPrint ( verbose , \"--- READY!\" ) "}
{"6883": "\ndef _countdown ( self , waitTime = 0 , printString = \"Waiting %*d seconds...\" , verbose = 1 ) : \n    if waitTime <= 0 : \n        waitTime = self . __retryDelay \n    for remaining in range ( waitTime , 0 , - 1 ) : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , remaining ) , end = \"\" , flush = 1 ) \n        time . sleep ( 1 ) \n    if verbose : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , 0 ) ) "}
{"6884": "\ndef fileLoad ( self , filePath = None , updatePath = 1 ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        raise FileNotFoundError ( \"Data file '%s' does not exist.\" % ( filePath ) ) \n    else : \n        print ( \"Importing existing data file '%s' ... \" % ( filePath ) , end = \"\" , flush = 1 ) \n        with open ( filePath , \"r\" ) as q : \n            data_raw = q . read ( ) \n        print ( \"Imported!\" ) \n        self . data = json . loads ( data_raw ) \n        if updatePath : \n            self . filePath = filePath "}
{"6885": "\ndef fileSave ( self , filePath = None , updatePath = 0 ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        print ( \"Data file '%s' does not exist, will create new file.\" % ( filePath ) ) \n        if not os . path . exists ( os . path . split ( filePath ) [ 0 ] ) : \n            os . makedirs ( os . path . split ( filePath ) [ 0 ] ) \n    dataJsonString = json . dumps ( self . data , indent = 4 , sort_keys = 1 ) \n    print ( \"Writing to file '%s' ... \" % ( filePath ) , end = \"\" , flush = 1 ) \n    with open ( filePath , \"w\" ) as fileout : \n        fileout . write ( dataJsonString ) \n    print ( \"Wrote file!\" ) \n    if updatePath : \n        self . filePath = filePath "}
{"6892": "\ndef get_tfvc_repos ( url , token , collection , project ) : \n    branch_list = [ ] \n    tfvc_client = create_tfs_tfvc_client ( '{url}/{collection_name}' . format ( url = url , collection_name = collection . name ) , token ) \n    logger . debug ( 'Retrieving Tfvc Branches for Project: {project_name}' . format ( project_name = project . name ) ) \n    branches = tfvc_client . get_branches ( project . id , 1 , 1 , 0 , 1 ) \n    if branches : \n        branch_list . extend ( branches ) \n    else : \n        logger . debug ( 'No Tfvcc Branches in Project: {project_name}' . format ( project_name = project . name ) ) \n    return branch_list "}
{"6896": "\ndef configure ( backends , raise_errors = 0 ) : \n    good_backends = [ ] \n    for backend in backends : \n        clspath = backend [ 'class' ] \n        options = backend . get ( 'options' , { } ) \n        if isinstance ( clspath , str ) : \n            modpath , clsname = split_clspath ( clspath ) \n            try : \n                __import__ ( modpath ) \n                module = sys . modules [ modpath ] \n                cls = getattr ( module , clsname ) \n            except Exception : \n                logger . exception ( 'Exception while importing %s' , clspath ) \n                if raise_errors : \n                    raise \n                continue \n        else : \n            cls = clspath \n        try : \n            good_backends . append ( cls ( options ) ) \n        except Exception : \n            logger . exception ( 'Exception thrown while instantiating %s, %s' , clspath , options ) \n            if raise_errors : \n                raise \n    _change_metrics ( good_backends ) "}
{"6933": "\ndef _read_ready ( self ) : \n    try : \n        data = os . read ( self . _fileno , self . max_size ) \n    except InterruptedError : \n        pass \n    except OSError as exc : \n        self . _fatal_error ( exc , \"Fatal read error on file descriptor read\" ) \n    else : \n        if data : \n            self . _protocol . data_received ( data ) \n        else : \n            if self . _loop . get_debug ( ) : \n                logger . info ( \"%r was closed by the kernel\" , self ) \n            self . _closing = 0 \n            self . pause_reading ( ) \n            self . _loop . call_soon ( self . _protocol . eof_received ) \n            self . _loop . call_soon ( self . _call_connection_lost , None ) "}
{"6934": "\ndef _close ( self , error = None ) : \n    self . _closing = 1 \n    self . pause_reading ( ) \n    self . _loop . call_soon ( self . _call_connection_lost , error ) "}
{"6940": "\ndef get_event ( self ) : \n    while 1 : \n        prefix = yield from self . _stream . readexactly ( PREFIX . size ) \n        if prefix == b'' : \n            return \n        wd , flags , cookie , length = PREFIX . unpack ( prefix ) \n        path = yield from self . _stream . readexactly ( length ) \n        if wd not in self . aliases : \n            continue \n        decoded_path = struct . unpack ( '%ds' % length , path ) [ 0 ] . rstrip ( b'\\x00' ) . decode ( 'utf-8' ) \n        return Event ( flags = flags , cookie = cookie , name = decoded_path , alias = self . aliases [ wd ] , ) "}
{"6945": "\ndef is_starved ( self ) : \n    for conn in itervalues ( self . conns ) : \n        if conn . in_flight > 0 and conn . in_flight >= ( conn . last_rdy * 0.85 ) : \n            return 1 \n    return 0 "}
{"6946": "\ndef connect_to_nsqd ( self , host , port ) : \n    assert isinstance ( host , string_types ) \n    assert isinstance ( port , int ) \n    conn = AsyncConn ( host , port , ** self . conn_kwargs ) \n    conn . on ( 'identify' , self . _on_connection_identify ) \n    conn . on ( 'identify_response' , self . _on_connection_identify_response ) \n    conn . on ( 'auth' , self . _on_connection_auth ) \n    conn . on ( 'auth_response' , self . _on_connection_auth_response ) \n    conn . on ( 'error' , self . _on_connection_error ) \n    conn . on ( 'close' , self . _on_connection_close ) \n    conn . on ( 'ready' , self . _on_connection_ready ) \n    conn . on ( 'message' , self . _on_message ) \n    conn . on ( 'heartbeat' , self . _on_heartbeat ) \n    conn . on ( 'backoff' , functools . partial ( self . _on_backoff_resume , success = 0 ) ) \n    conn . on ( 'resume' , functools . partial ( self . _on_backoff_resume , success = 1 ) ) \n    conn . on ( 'continue' , functools . partial ( self . _on_backoff_resume , success = None ) ) \n    if conn . id in self . conns : \n        return \n    now = time . time ( ) \n    last_connect_attempt = self . connection_attempts . get ( conn . id ) \n    if last_connect_attempt and last_connect_attempt > now - 10 : \n        return \n    self . connection_attempts [ conn . id ] = now \n    logger . info ( '[%s:%s] connecting to nsqd' , conn . id , self . name ) \n    conn . connect ( ) \n    return conn "}
{"6948": "\ndef set_max_in_flight ( self , max_in_flight ) : \n    assert isinstance ( max_in_flight , int ) \n    self . max_in_flight = max_in_flight \n    if max_in_flight == 0 : \n        for conn in itervalues ( self . conns ) : \n            if conn . rdy > 0 : \n                logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) \n                self . _send_rdy ( conn , 0 ) \n        self . total_rdy = 0 \n    else : \n        self . need_rdy_redistributed = 1 \n        self . _redistribute_rdy_state ( ) "}
{"6981": "\ndef write_assembly ( self , output_file , filtered = 1 ) : \n    logger . debug ( \"Writing the filtered assembly into: {}\" . format ( output_file ) ) \n    with open ( output_file , \"w\" ) as fh : \n        for contig_id , contig in self . contigs . items ( ) : \n            if contig_id not in self . filtered_ids and filtered : \n                fh . write ( \">{}_{}\\\\n{}\\\\n\" . format ( self . sample , contig [ \"header\" ] , contig [ \"sequence\" ] ) ) "}
{"6984": "\ndef inner_fork_insanity_checks ( pipeline_string ) : \n    list_of_forks = [ ] \n    left_indexes = [ ] \n    for pos , char in enumerate ( pipeline_string ) : \n        if char == FORK_TOKEN : \n            left_indexes . append ( pos ) \n        elif char == CLOSE_TOKEN and len ( left_indexes ) > 0 : \n            list_of_forks . append ( pipeline_string [ left_indexes [ - 1 ] + 1 : pos ] ) \n            left_indexes = left_indexes [ : - 1 ] \n    list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = 1 ) \n    for fork in list_of_forks : \n        for subfork in list_of_forks : \n            if subfork in list_of_forks and subfork != fork : \n                fork_simplified = fork . replace ( \"({})\" . format ( subfork ) , \"\" ) \n            else : \n                fork_simplified = fork \n        if not len ( fork_simplified . split ( LANE_TOKEN ) ) > 1 : \n            raise SanityError ( \"One of the forks doesn't have '|' \" \"separator between the processes to fork. This is\" \" the prime suspect: '({})'\" . format ( fork ) ) "}
{"7003": "\ndef log_parser ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . log_retry = 0 \n    if size_stamp and size_stamp == self . log_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    r = \".* (.*) \\[.*\\].*\\[(.*)\\].*process > (.*) \\((.*)\\).*\" \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if \"Submitted process >\" in line or \"Re-submitted process >\" in line or \"Cached process >\" in line : \n                m = re . match ( r , line ) \n                if not m : \n                    continue \n                time_start = m . group ( 1 ) \n                workdir = m . group ( 2 ) \n                process = m . group ( 3 ) \n                tag = m . group ( 4 ) \n                if time_start + tag not in self . stored_log_ids : \n                    self . stored_log_ids . append ( time_start + tag ) \n                else : \n                    continue \n                if process not in self . processes : \n                    continue \n                p = self . processes [ process ] \n                if tag in list ( p [ \"finished\" ] ) + list ( p [ \"retry\" ] ) : \n                    continue \n                if tag in list ( p [ \"failed\" ] ) and \"Re-submitted process >\" in line : \n                    p [ \"retry\" ] . add ( tag ) \n                    self . send = 1 \n                    continue \n                p [ \"barrier\" ] = \"R\" \n                if tag not in p [ \"submitted\" ] : \n                    p [ \"submitted\" ] . add ( tag ) \n                    if tag not in self . process_tags [ process ] : \n                        self . process_tags [ process ] [ tag ] = { \"workdir\" : self . _expand_path ( workdir ) , \"start\" : time_start } \n                        self . send = 1 \n                    elif not self . process_tags [ process ] [ tag ] [ \"start\" ] : \n                        self . process_tags [ process ] [ tag ] [ \"start\" ] = time_start \n                        self . send = 1 \n    self . _update_pipeline_status ( ) "}
{"7005": "\ndef display_overview ( self ) : \n    stay_alive = 1 \n    self . screen = curses . initscr ( ) \n    self . screen . keypad ( 1 ) \n    self . screen . nodelay ( - 1 ) \n    curses . cbreak ( ) \n    curses . noecho ( ) \n    curses . start_color ( ) \n    self . screen_lines = self . screen . getmaxyx ( ) [ 0 ] \n    try : \n        while stay_alive : \n            self . _curses_keybindings ( ) \n            self . update_inspection ( ) \n            self . flush_overview ( ) \n            sleep ( self . refresh_rate ) \n    except FileNotFoundError : \n        sys . stderr . write ( colored_print ( \"ERROR: nextflow log and/or trace files are no longer \" \"reachable!\" , \"red_bold\" ) ) \n    except Exception as e : \n        sys . stderr . write ( str ( e ) ) \n    finally : \n        curses . nocbreak ( ) \n        self . screen . keypad ( 0 ) \n        curses . echo ( ) \n        curses . endwin ( ) "}
{"7017": "\ndef list_recipes ( full = 0 ) : \n    logger . info ( colored_print ( \"\\n===== L I S T   O F   R E C I P E S =====\\n\" , \"green_bold\" ) ) \n    prefix = \"{}.\" . format ( recipes . __name__ ) \n    for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : \n        _module = importer . find_module ( modname ) . load_module ( modname ) \n        _recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) ] \n        for cls in _recipe_classes : \n            recipe_cls = cls ( ) \n            if hasattr ( recipe_cls , \"name\" ) : \n                logger . info ( colored_print ( \"=> {}\" . format ( recipe_cls . name ) , \"blue_bold\" ) ) \n                if full : \n                    logger . info ( colored_print ( \"\\t {}\" . format ( recipe_cls . __doc__ ) , \"purple_bold\" ) ) \n                    logger . info ( colored_print ( \"Pipeline string: {}\\n\" . format ( recipe_cls . pipeline_str ) , \"yellow_bold\" ) ) \n    sys . exit ( 0 ) "}
{"7018": "\ndef validate_pipeline ( pipeline_string ) : \n    if \"(\" in pipeline_string or \")\" in pipeline_string or \"|\" in pipeline_string : \n        logger . error ( colored_print ( \"Please provide a valid task list!\" , \"red_bold\" ) ) \n        return 0 \n    return 1 "}
{"7019": "\ndef build_upstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 1 ] is not None : \n            if len ( process_descriptions [ task ] [ 1 ] . split ( \"|\" ) ) > 1 : \n                local_forks = process_descriptions [ task ] [ 1 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , 0 , 1 , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 1 ] in total_tasks : \n                    task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_upstream ( process_descriptions , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                else : \n                    logger . error ( colored_print ( \"{} not in provided protocols as \" \"input for {}\" . format ( process_descriptions [ task ] [ 1 ] , task ) , \"red_bold\" ) ) \n                    sys . exit ( ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7020": "\ndef build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 2 ] is not None : \n            if len ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) ) > 1 : \n                local_forks = process_descriptions [ task ] [ 2 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , 0 , 1 , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 2 ] in total_tasks : \n                    task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7022": "\ndef run_auto_pipeline ( self , tasks ) : \n    self . forks = self . define_pipeline_string ( self . process_descriptions , tasks , 1 , 1 , self . count_forks , tasks , self . forks ) \n    self . pipeline_string = self . build_pipeline_string ( self . forks ) \n    return self . pipeline_string "}
{"7024": "\ndef write_report ( storage_dic , output_file , sample_id ) : \n    with open ( output_file , \"w\" ) as fh , open ( \".report.json\" , \"w\" ) as json_rep : \n        fh . write ( \"Sample,Total length,Total trimmed,%,5end Trim,3end Trim,\" \"bad_reads\\\\n\" ) \n        for sample , vals in storage_dic . items ( ) : \n            fh . write ( \"{},{}\\\\n\" . format ( sample , \",\" . join ( [ str ( x ) for x in vals . values ( ) ] ) ) ) \n            json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"trimmed\" , \"value\" : vals [ \"total_trim_perc\" ] , \"table\" : \"qc\" , \"columnBar\" : 1 } , ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"sparkline\" : vals [ \"clean_len\" ] } } ] , \"badReads\" : vals [ \"bad_reads\" ] } \n            json_rep . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7030": "\ndef iter_filter ( self , filters , databases = None , fields = None , filter_behavior = \"and\" ) : \n    if filter_behavior not in [ \"and\" , \"or\" ] : \n        raise ValueError ( \"Filter behavior must be either 'and' or 'or'\" ) \n    for dic in self . storage . values ( ) : \n        _pass = 0 \n        flag = [ ] \n        if databases : \n            if dic [ \"database\" ] not in databases : \n                continue \n        for f in filters : \n            val = dic [ f [ 0 ] ] \n            if not self . _test_truth ( val , f [ 1 ] , f [ 2 ] ) : \n                flag . append ( 0 ) \n            else : \n                flag . append ( 1 ) \n        if filter_behavior == \"and\" : \n            if all ( flag ) : \n                _pass = 1 \n        elif filter_behavior == \"or\" : \n            if any ( flag ) : \n                _pass = 1 \n        if _pass : \n            if fields : \n                yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) \n            else : \n                yield dic "}
{"7034": "\ndef main ( sample_id , assembly_file , coverage_bp_file = None ) : \n    logger . info ( \"Starting assembly report\" ) \n    assembly_obj = Assembly ( assembly_file , sample_id ) \n    logger . info ( \"Retrieving summary statistics for assembly\" ) \n    assembly_obj . get_summary_stats ( \"{}_assembly_report.csv\" . format ( sample_id ) ) \n    size_dist = [ len ( x ) for x in assembly_obj . contigs . values ( ) ] \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Contigs\" , \"value\" : assembly_obj . summary_info [ \"ncontigs\" ] , \"table\" : \"assembly\" , \"columnBar\" : 1 } , { \"header\" : \"Assembled BP\" , \"value\" : assembly_obj . summary_info [ \"total_len\" ] , \"table\" : \"assembly\" , \"columnBar\" : 1 } , ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"size_dist\" : size_dist } } ] } \n    if coverage_bp_file : \n        try : \n            window = 2000 \n            gc_sliding_data = assembly_obj . get_gc_sliding ( window = window ) \n            cov_sliding_data = assembly_obj . get_coverage_sliding ( coverage_bp_file , window = window ) \n            total_bp = sum ( [ sum ( x ) for x in assembly_obj . contig_coverage . values ( ) ] ) \n            json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ \"genomeSliding\" ] = { \"gcData\" : gc_sliding_data , \"covData\" : cov_sliding_data , \"window\" : window , \"xbars\" : assembly_obj . _get_window_labels ( window ) , \"assemblyFile\" : os . path . basename ( assembly_file ) } \n            json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ \"sparkline\" ] = total_bp \n        except : \n            logger . error ( \"Unexpected error creating sliding window data:\\\\n\" \"{}\" . format ( traceback . format_exc ( ) ) ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        status_fh . write ( \"pass\" ) "}
{"7036": "\ndef get_summary_stats ( self , output_csv = None ) : \n    contig_size_list = [ ] \n    self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) \n    for contig_id , sequence in self . contigs . items ( ) : \n        logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) \n        contig_len = len ( sequence ) \n        contig_size_list . append ( contig_len ) \n        self . summary_info [ \"total_len\" ] += contig_len \n        self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) \n        self . summary_info [ \"missing_data\" ] += sequence . count ( \"N\" ) \n    logger . debug ( \"Getting average contig size\" ) \n    self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) \n    logger . debug ( \"Getting average GC content\" ) \n    self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) \n    logger . debug ( \"Getting N50\" ) \n    cum_size = 0 \n    for l in sorted ( contig_size_list , reverse = 1 ) : \n        cum_size += l \n        if cum_size >= self . summary_info [ \"total_len\" ] / 2 : \n            self . summary_info [ \"n50\" ] = l \n            break \n    if output_csv : \n        logger . debug ( \"Writing report to csv\" ) \n        with open ( output_csv , \"w\" ) as fh : \n            summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) \n            fh . write ( summary_line ) "}
{"7042": "\ndef get_trim_index ( biased_list ) : \n    if set ( biased_list ) == { 0 } : \n        return 0 \n    if set ( biased_list [ : 5 ] ) == { 0 } : \n        return 0 \n    for i , val in enumerate ( biased_list ) : \n        if val and set ( biased_list [ i + 1 : i + 3 ] ) == { 0 } : \n            return i + 1 \n    return len ( biased_list ) "}
{"7043": "\ndef trim_range ( data_file ) : \n    logger . debug ( \"Starting trim range assessment\" ) \n    target_nuc_bias = \">>Per base sequence content\" \n    logger . debug ( \"Target string to start nucleotide bias assessment set to \" \"{}\" . format ( target_nuc_bias ) ) \n    gather = 0 \n    biased = [ ] \n    with open ( data_file ) as fh : \n        for line in fh : \n            if line . startswith ( target_nuc_bias ) : \n                logger . debug ( \"Found target string at line: {}\" . format ( line ) ) \n                next ( fh ) \n                gather = 1 \n            elif line . startswith ( \">>END_MODULE\" ) and gather : \n                logger . debug ( \"Stopping parsing at line: {}\" . format ( line ) ) \n                break \n            elif gather : \n                g , a , t , c = [ float ( x ) for x in line . strip ( ) . split ( ) [ 1 : ] ] \n                gc = ( g + 0.1 ) / ( c + 0.1 ) \n                at = ( a + 0.1 ) / ( t + 0.1 ) \n                if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2 : \n                    biased . append ( 0 ) \n                else : \n                    biased . append ( 1 ) \n    logger . debug ( \"Finished bias assessment with result: {}\" . format ( biased ) ) \n    biased_5end , biased_3end = biased [ : int ( len ( biased ) / 2 ) ] , biased [ int ( len ( biased ) / 2 ) : ] [ : : - 1 ] \n    logger . debug ( \"Getting optimal trim range from biased list\" ) \n    trim_nt = [ 0 , 0 ] \n    trim_nt [ 0 ] = get_trim_index ( biased_5end ) \n    logger . debug ( \"Optimal trim range at 5' end set to: {}\" . format ( trim_nt [ 0 ] ) ) \n    trim_nt [ 1 ] = len ( biased ) - get_trim_index ( biased_3end ) \n    logger . debug ( \"Optimal trim range at 3' end set to: {}\" . format ( trim_nt [ 1 ] ) ) \n    return trim_nt "}
{"7046": "\ndef check_summary_health ( summary_file , ** kwargs ) : \n    fail_sensitive = kwargs . get ( \"fail_sensitive\" , [ \"Per base sequence quality\" , \"Overrepresented sequences\" , \"Sequence Length Distribution\" , \"Per sequence GC content\" ] ) \n    logger . debug ( \"Fail sensitive categories: {}\" . format ( fail_sensitive ) ) \n    must_pass = kwargs . get ( \"must_pass\" , [ \"Per base N content\" , \"Adapter Content\" ] ) \n    logger . debug ( \"Must pass categories: {}\" . format ( must_pass ) ) \n    warning_fail_sensitive = kwargs . get ( \"warning_fail_sensitive\" , [ \"Per base sequence quality\" , \"Overrepresented sequences\" , ] ) \n    warning_must_pass = kwargs . get ( \"warning_must_pass\" , [ \"Per base sequence content\" ] ) \n    summary_info = get_summary ( summary_file ) \n    health = 1 \n    failed = [ ] \n    warning = [ ] \n    for cat , test in summary_info . items ( ) : \n        logger . debug ( \"Assessing category {} with result {}\" . format ( cat , test ) ) \n        if cat in fail_sensitive and test == \"FAIL\" : \n            health = 0 \n            failed . append ( \"{}:{}\" . format ( cat , test ) ) \n            logger . error ( \"Category {} failed a fail sensitive \" \"category\" . format ( cat ) ) \n        if cat in must_pass and test != \"PASS\" : \n            health = 0 \n            failed . append ( \"{}:{}\" . format ( cat , test ) ) \n            logger . error ( \"Category {} failed a must pass category\" . format ( cat ) ) \n        if cat in warning_fail_sensitive and test == \"FAIL\" : \n            warning . append ( \"Failed category: {}\" . format ( cat ) ) \n            logger . warning ( \"Category {} flagged at a fail sensitive \" \"category\" . format ( cat ) ) \n        if cat in warning_must_pass and test != \"PASS\" : \n            warning . append ( \"Did not pass category: {}\" . format ( cat ) ) \n            logger . warning ( \"Category {} flagged at a must pass \" \"category\" . format ( cat ) ) \n    return health , failed , warning "}
{"7050": "\ndef _search_tree_backwards ( self , template , parent_lanes ) : \n    for p in self . processes [ : : - 1 ] : \n        if p . lane not in parent_lanes : \n            continue \n        if p . template == template : \n            return 1 \n    return 0 "}
{"7073": "\ndef update_trace_watch ( self ) : \n    size_stamp = os . path . getsize ( self . trace_file ) \n    self . trace_retry = 0 \n    if size_stamp and size_stamp == self . trace_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating trace size stamp to: {}\" . format ( size_stamp ) ) \n        self . trace_sizestamp = size_stamp \n    with open ( self . trace_file ) as fh : \n        header = next ( fh ) . strip ( ) \n        while not header : \n            header = next ( fh ) . strip ( ) \n        hm = self . _header_mapping ( header ) \n        for line in fh : \n            if line . strip ( ) == \"\" : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            if fields [ hm [ \"task_id\" ] ] in self . stored_ids : \n                continue \n            if fields [ hm [ \"process\" ] ] == \"report\" : \n                self . report_queue . append ( self . _expand_path ( fields [ hm [ \"hash\" ] ] ) ) \n                self . send = 1 \n            self . stored_ids . append ( fields [ hm [ \"task_id\" ] ] ) "}
{"7079": "\ndef main ( fastq_pair , adapter_file , cpus ) : \n    logger . info ( \"Starting fastqc\" ) \n    if os . path . exists ( adapter_file ) : \n        logger . info ( \"Adapters file provided: {}\" . format ( adapter_file ) ) \n        adapters = convert_adatpers ( adapter_file ) \n    else : \n        logger . info ( \"Adapters file '{}' not provided or does not \" \"exist\" . format ( adapter_file ) ) \n        adapters = None \n    cli = [ \"fastqc\" , \"--extract\" , \"--nogroup\" , \"--format\" , \"fastq\" , \"--threads\" , str ( cpus ) ] \n    if adapters : \n        cli += [ \"--adapters\" , \"{}\" . format ( adapters ) ] \n    cli += fastq_pair \n    logger . debug ( \"Running fastqc subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE , shell = 0 ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished fastqc subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished fastqc subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished fastqc with return code: {}\" . format ( p . returncode ) ) \n    logger . info ( \"Checking if FastQC output was correctly generated\" ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        for fastq in fastq_pair : \n            fpath = join ( fastq . rsplit ( \".\" , 2 ) [ 0 ] + \"_fastqc\" , \"fastqc_data.txt\" ) \n            logger . debug ( \"Checking path: {}\" . format ( fpath ) ) \n            if not exists ( fpath ) : \n                logger . warning ( \"Path does not exist: {}\" . format ( fpath ) ) \n                status_fh . write ( \"fail\" ) \n                return \n            logger . debug ( \"Found path: {}\" . format ( fpath ) ) \n            status_fh . write ( \"pass\" ) \n    logger . info ( \"Retrieving relevant FastQC output files\" ) \n    for i , fastq in enumerate ( fastq_pair ) : \n        fastqc_dir = fastq . rsplit ( \".\" , 2 ) [ 0 ] + \"_fastqc\" \n        summary_file = join ( fastqc_dir , \"summary.txt\" ) \n        logger . debug ( \"Retrieving summary file: {}\" . format ( summary_file ) ) \n        fastqc_data_file = join ( fastqc_dir , \"fastqc_data.txt\" ) \n        logger . debug ( \"Retrieving data file: {}\" . format ( fastqc_data_file ) ) \n        os . rename ( fastqc_data_file , \"pair_{}_data\" . format ( i + 1 ) ) \n        os . rename ( summary_file , \"pair_{}_summary\" . format ( i + 1 ) ) "}
{"7091": "\ndef filter_assembly ( assembly_file , minimum_coverage , coverage_info , output_file ) : \n    write_flag = 0 \n    with open ( assembly_file ) as fh , open ( output_file , \"w\" ) as out_fh : \n        for line in fh : \n            if line . startswith ( \">\" ) : \n                write_flag = 0 \n                header = line . strip ( ) [ 1 : ] \n                contig_cov = coverage_info [ header ] [ \"cov\" ] \n                if contig_cov >= minimum_coverage : \n                    write_flag = 1 \n                    out_fh . write ( line ) \n            elif write_flag : \n                out_fh . write ( line ) "}
{"7102": "\ndef compute_gaussian_krnl ( M ) : \n    g = signal . gaussian ( M , M // 3. , sym = 1 ) \n    G = np . dot ( g . reshape ( - 1 , 1 ) , g . reshape ( 1 , - 1 ) ) \n    G [ M // 2 : , : M // 2 ] = - G [ M // 2 : , : M // 2 ] \n    G [ : M // 2 , M // 2 : ] = - G [ : M // 2 , M // 2 : ] \n    return G "}
{"7116": "\ndef compute_similarity ( F , bound_idxs , dirichlet = 0 , xmeans = 0 , k = 5 , offset = 4 ) : \n    feat_segments = get_feat_segments ( F , bound_idxs ) \n    fmcs = feat_segments_to_2dfmc_max ( feat_segments , offset ) \n    if len ( fmcs ) == 0 : \n        return np . arange ( len ( bound_idxs ) - 1 ) \n    if dirichlet : \n        k_init = np . min ( [ fmcs . shape [ 0 ] , k ] ) \n        if fmcs . shape [ 1 ] > 500 : \n            labels_est = compute_labels_kmeans ( fmcs , k = k ) \n        else : \n            dpgmm = mixture . DPGMM ( n_components = k_init , covariance_type = 'full' ) \n            dpgmm . fit ( fmcs ) \n            k = len ( dpgmm . means_ ) \n            labels_est = dpgmm . predict ( fmcs ) \n    if xmeans : \n        xm = XMeans ( fmcs , plot = 0 ) \n        k = xm . estimate_K_knee ( th = 0.01 , maxK = 8 ) \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    else : \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    return labels_est "}
{"7118": "\ndef partial_fit ( self , X , Y ) : \n    for ( xi , yi ) in itertools . izip ( X , Y ) : \n        prev_mean = None \n        prev_length = None \n        if self . scatter_within_ is None : \n            d , n = xi . shape \n            if yi [ 0 ] > 0 : \n                yi = np . concatenate ( [ np . array ( [ 0 ] ) , yi ] ) \n            if yi [ - 1 ] < n : \n                yi = np . concatenate ( [ yi , np . array ( [ n ] ) ] ) \n            self . scatter_within_ = self . sigma * np . eye ( d ) \n            self . scatter_ordinal_ = np . zeros ( d ) \n        for ( seg_start , seg_end ) in zip ( yi [ : - 1 ] , yi [ 1 : ] ) : \n            seg_length = seg_end - seg_start \n            if seg_length < 2 : \n                continue \n            seg_mean = np . mean ( xi [ : , seg_start : seg_end ] , axis = 1 , keepdims = 1 ) \n            seg_cov = np . cov ( xi [ : , seg_start : seg_end ] ) \n            self . scatter_within_ = self . scatter_within_ + seg_length * seg_cov \n            if prev_mean is not None : \n                diff_ord = seg_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + seg_length * np . dot ( diff_ord , diff_ord . T ) \n                diff_ord = prev_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + prev_length * np . dot ( diff_ord , diff_ord . T ) \n            prev_mean = seg_mean \n            prev_length = seg_length \n    e_vals , e_vecs = scipy . linalg . eig ( self . scatter_ordinal_ , self . scatter_within_ ) \n    self . e_vals_ = e_vals \n    self . e_vecs_ = e_vecs \n    self . components_ = e_vecs . T \n    return self "}
{"7119": "\ndef read_references ( audio_path , annotator_id = 0 ) : \n    ds_path = os . path . dirname ( os . path . dirname ( audio_path ) ) \n    jam_path = os . path . join ( ds_path , ds_config . references_dir , os . path . basename ( audio_path ) [ : - 4 ] + ds_config . references_ext ) \n    jam = jams . load ( jam_path , validate = 0 ) \n    ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n    ref_inters , ref_labels = ann . to_interval_values ( ) \n    ref_times = utils . intervals_to_times ( ref_inters ) \n    return ref_times , ref_labels "}
{"7121": "\ndef save_estimations ( file_struct , times , labels , boundaries_id , labels_id , ** params ) : \n    params . pop ( \"features\" , None ) \n    dur = get_duration ( file_struct . features_file ) \n    if 'numpy' in str ( type ( times ) ) : \n        inters = utils . times_to_intervals ( times ) \n        assert len ( inters ) == len ( labels ) , \"Number of boundary intervals \" \"(%d) and labels (%d) do not match\" % ( len ( inters ) , len ( labels ) ) \n        inters = [ inters ] \n        labels = [ labels ] \n    else : \n        inters = [ ] \n        for level in range ( len ( times ) ) : \n            est_inters = utils . times_to_intervals ( times [ level ] ) \n            inters . append ( est_inters ) \n            assert len ( inters [ level ] ) == len ( labels [ level ] ) , \"Number of boundary intervals (%d) and labels (%d) do not \" \"match in level %d\" % ( len ( inters [ level ] ) , len ( labels [ level ] ) , level ) \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jams . Annotation ( namespace = namespace ) \n    if os . path . isfile ( file_struct . est_file ) : \n        jam = jams . load ( file_struct . est_file , validate = 0 ) \n        curr_ann = find_estimation ( jam , boundaries_id , labels_id , params ) \n        if curr_ann is not None : \n            curr_ann . data = ann . data \n            ann = curr_ann \n        else : \n            jam . annotations . append ( ann ) \n    else : \n        jam = jams . JAMS ( ) \n        jam . file_metadata . duration = dur \n        jam . annotations . append ( ann ) \n    ann . annotation_metadata . version = msaf . __version__ \n    ann . annotation_metadata . data_source = \"MSAF\" \n    sandbox = { } \n    sandbox [ \"boundaries_id\" ] = boundaries_id \n    sandbox [ \"labels_id\" ] = labels_id \n    sandbox [ \"timestamp\" ] = datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) \n    for key in params : \n        sandbox [ key ] = params [ key ] \n    ann . sandbox = sandbox \n    for i , ( level_inters , level_labels ) in enumerate ( zip ( inters , labels ) ) : \n        for bound_inter , label in zip ( level_inters , level_labels ) : \n            dur = float ( bound_inter [ 1 ] ) - float ( bound_inter [ 0 ] ) \n            label = chr ( int ( label ) + 65 ) \n            if params [ \"hier\" ] : \n                value = { \"label\" : label , \"level\" : i } \n            else : \n                value = label \n            ann . append ( time = bound_inter [ 0 ] , duration = dur , value = value ) \n    jam . save ( file_struct . est_file ) "}
{"7142": "\ndef main ( ) : \n    parser = argparse . ArgumentParser ( description = \"Runs the speficied algorithm(s) on the MSAF \" \"formatted dataset.\" , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) \n    parser . add_argument ( \"in_path\" , action = \"store\" , help = \"Input dataset\" ) \n    parser . add_argument ( \"-f\" , action = \"store\" , dest = \"feature\" , default = \"pcp\" , type = str , help = \"Type of features\" , choices = [ \"pcp\" , \"tonnetz\" , \"mfcc\" , \"cqt\" , \"tempogram\" ] ) \n    parser . add_argument ( \"-b\" , action = \"store_true\" , dest = \"annot_beats\" , help = \"Use annotated beats\" , default = 0 ) \n    parser . add_argument ( \"-fs\" , action = \"store_true\" , dest = \"framesync\" , help = \"Use frame-synchronous features\" , default = 0 ) \n    parser . add_argument ( \"-bid\" , action = \"store\" , help = \"Boundary algorithm identifier\" , dest = \"boundaries_id\" , default = \"gt\" , choices = [ \"gt\" ] + io . get_all_boundary_algorithms ( ) ) \n    parser . add_argument ( \"-lid\" , action = \"store\" , help = \"Label algorithm identifier\" , dest = \"labels_id\" , default = None , choices = io . get_all_label_algorithms ( ) ) \n    parser . add_argument ( \"-j\" , action = \"store\" , dest = \"n_jobs\" , default = 4 , type = int , help = \"The number of threads to use\" ) \n    args = parser . parse_args ( ) \n    start_time = time . time ( ) \n    process ( args . in_path , annot_beats = args . annot_beats , feature = args . feature , framesync = args . framesync , boundaries_id = args . boundaries_id , labels_id = args . labels_id , n_jobs = args . n_jobs ) \n    logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start_time ) ) "}
{"7144": "\ndef compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , bins = 251 , annotator_id = 0 ) : \n    if config [ \"hier\" ] : \n        ref_times , ref_labels , ref_levels = msaf . io . read_hier_references ( ref_file , annotation_id = annotator_id , exclude_levels = [ \"segment_salami_function\" ] ) \n    else : \n        jam = jams . load ( ref_file , validate = 0 ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n        ref_inter , ref_labels = ann . to_interval_values ( ) \n    est_inter , est_labels = io . read_estimations ( est_file , boundaries_id , labels_id , ** config ) \n    logging . info ( \"Evaluating %s\" % os . path . basename ( est_file ) ) \n    if config [ \"hier\" ] : \n        assert len ( est_inter ) == len ( est_labels ) , \"Same number of levels \" \"are required in the boundaries and labels for the hierarchical \" \"evaluation.\" \n        est_times = [ ] \n        est_labels = [ ] \n        est_inter = sorted ( est_inter , key = lambda level : len ( level ) ) \n        for inter in est_inter : \n            est_times . append ( msaf . utils . intervals_to_times ( inter ) ) \n            est_labels . append ( np . ones ( len ( est_times [ - 1 ] ) - 1 ) * - 1 ) \n        utils . align_end_hierarchies ( est_times , ref_times , thres = 1 ) \n        est_hier = [ utils . times_to_intervals ( times ) for times in est_times ] \n        ref_hier = [ utils . times_to_intervals ( times ) for times in ref_times ] \n        res = { } \n        res [ \"t_recall10\" ] , res [ \"t_precision10\" ] , res [ \"t_measure10\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 10 ) \n        res [ \"t_recall15\" ] , res [ \"t_precision15\" ] , res [ \"t_measure15\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 15 ) \n        res [ \"track_id\" ] = os . path . basename ( est_file ) [ : - 5 ] \n        return res \n    else : \n        return compute_results ( ref_inter , est_inter , ref_labels , est_labels , bins , est_file ) "}
{"7148": "\ndef process ( in_path , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , annot_beats = 0 , framesync = 0 , feature = \"pcp\" , hier = 0 , save = 0 , out_file = None , n_jobs = 4 , annotator_id = 0 , config = None ) : \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n    config [ \"hier\" ] = hier \n    config . pop ( \"features\" , None ) \n    if out_file is None : \n        out_file = get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) \n    if os . path . exists ( out_file ) : \n        logging . warning ( \"Results already exists, reading from file %s\" % out_file ) \n        results = pd . read_csv ( out_file ) \n        print_results ( results ) \n        return results \n    if os . path . isfile ( in_path ) : \n        evals = [ process_track ( in_path , boundaries_id , labels_id , config , annotator_id = annotator_id ) ] \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        logging . info ( \"Evaluating %d tracks...\" % len ( file_structs ) ) \n        evals = Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) \n    results = pd . DataFrame ( ) \n    for e in evals : \n        if e != [ ] : \n            results = results . append ( e , ignore_index = 1 ) \n    logging . info ( \"%d tracks analyzed\" % len ( results ) ) \n    print_results ( results ) \n    if save : \n        logging . info ( \"Writing results in %s\" % out_file ) \n        results . to_csv ( out_file ) \n    return results "}
{"7149": "\ndef AddConfigVar ( name , doc , configparam , root = config ) : \n    if root is config : \n        configparam . fullname = name \n    sections = name . split ( '.' ) \n    if len ( sections ) > 1 : \n        if not hasattr ( root , sections [ 0 ] ) : \n            class SubObj ( object ) : \n                _i_am_a_config_class = 1 \n            setattr ( root . __class__ , sections [ 0 ] , SubObj ( ) ) \n        newroot = getattr ( root , sections [ 0 ] ) \n        if ( not getattr ( newroot , '_i_am_a_config_class' , 0 ) or isinstance ( newroot , type ) ) : \n            raise TypeError ( 'Internal config nodes must be config class instances' , newroot ) \n        return AddConfigVar ( '.' . join ( sections [ 1 : ] ) , doc , configparam , root = newroot ) \n    else : \n        if hasattr ( root , name ) : \n            raise AttributeError ( 'This name is already taken' , configparam . fullname ) \n        configparam . doc = doc \n        if not callable ( configparam . default ) : \n            configparam . __get__ ( root , type ( root ) , delete_key = 1 ) \n        else : \n            try : \n                fetch_val_for_key ( configparam . fullname ) \n                configparam . __get__ ( root , type ( root ) , delete_key = 1 ) \n            except KeyError : \n                pass \n        setattr ( root . __class__ , sections [ 0 ] , configparam ) \n        _config_var_list . append ( configparam ) "}
{"7150": "\ndef compute_all_features ( file_struct , framesync ) : \n    for feature_id in msaf . features_registry : \n        logging . info ( \"Computing %s for file %s\" % ( feature_id , file_struct . audio_file ) ) \n        feats = Features . select_features ( feature_id , file_struct , 0 , framesync ) \n        feats . features "}
{"7173": "\ndef compute_labels ( X , rank , R , bound_idxs , niter = 300 ) : \n    try : \n        F , G = cnmf ( X , rank , niter = niter , hull = 0 ) \n    except : \n        return [ 1 ] \n    label_frames = filter_activation_matrix ( G . T , R ) \n    label_frames = np . asarray ( label_frames , dtype = int ) \n    labels = [ ] \n    bound_inters = zip ( bound_idxs [ : - 1 ] , bound_idxs [ 1 : ] ) \n    for bound_inter in bound_inters : \n        if bound_inter [ 1 ] - bound_inter [ 0 ] <= 0 : \n            labels . append ( np . max ( label_frames ) + 1 ) \n        else : \n            labels . append ( most_frequent ( label_frames [ bound_inter [ 0 ] : bound_inter [ 1 ] ] ) ) \n    return labels "}
{"7177": "\ndef run_hierarchical ( audio_file , bounds_module , labels_module , frame_times , config , annotator_id = 0 ) : \n    if bounds_module is None : \n        raise NoHierBoundaryError ( \"A boundary algorithm is needed when using \" \"hierarchical segmentation.\" ) \n    features = config [ \"features\" ] . features \n    S = bounds_module . Segmenter ( audio_file , ** config ) \n    est_idxs , est_labels = S . processHierarchical ( ) \n    if labels_module is not None and bounds_module . __name__ != labels_module . __name__ : \n        flat_config = deepcopy ( config ) \n        flat_config [ \"hier\" ] = 0 \n        for i , level_idxs in enumerate ( est_idxs ) : \n            S = labels_module . Segmenter ( audio_file , in_bound_idxs = level_idxs , ** flat_config ) \n            est_labels [ i ] = S . processFlat ( ) [ 1 ] \n    est_times = [ ] \n    cleaned_est_labels = [ ] \n    for level in range ( len ( est_idxs ) ) : \n        est_level_times , est_level_labels = utils . process_segmentation_level ( est_idxs [ level ] , est_labels [ level ] , features . shape [ 0 ] , frame_times , config [ \"features\" ] . dur ) \n        est_times . append ( est_level_times ) \n        cleaned_est_labels . append ( est_level_labels ) \n    est_labels = cleaned_est_labels \n    return est_times , est_labels "}
{"7181": "\ndef process ( in_path , annot_beats = 0 , feature = \"pcp\" , framesync = 0 , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , hier = 0 , sonify_bounds = 0 , plot = 0 , n_jobs = 4 , annotator_id = 0 , config = None , out_bounds = \"out_bounds.wav\" , out_sr = 22050 ) : \n    np . random . seed ( 123 ) \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n        config [ \"features\" ] = None \n    config [ \"hier\" ] = hier \n    if not os . path . exists ( in_path ) : \n        raise NoAudioFileError ( \"File or directory does not exists, %s\" % in_path ) \n    if os . path . isfile ( in_path ) : \n        file_struct = msaf . io . FileStruct ( in_path ) \n        file_struct . features_file = msaf . config . features_tmp_file \n        config [ \"features\" ] = Features . select_features ( feature , file_struct , annot_beats , framesync ) \n        est_times , est_labels = run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n        if sonify_bounds : \n            logging . info ( \"Sonifying boundaries in %s...\" % out_bounds ) \n            audio_hq , sr = librosa . load ( in_path , sr = out_sr ) \n            utils . sonify_clicks ( audio_hq , est_times , out_bounds , out_sr ) \n        if plot : \n            plotting . plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id ) \n        msaf . utils . ensure_dir ( os . path . dirname ( file_struct . est_file ) ) \n        io . save_estimations ( file_struct , est_times , est_labels , boundaries_id , labels_id , ** config ) \n        return est_times , est_labels \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        return Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) "}
{"7186": "\ndef set_task ( translator , translit = 0 ) : \n    task = str ( ) \n    queue = list ( ) \n    output = ( 'translit' if translit else 'trans' ) \n    stream = partial ( write_stream , output = output ) \n    workers = ThreadPoolExecutor ( max_workers = 8 ) \n    try : \n        while 1 : \n            task = yield \n            queue . append ( task ) \n    except GeneratorExit : \n        list ( map ( stream , workers . map ( translator , queue ) ) ) "}
{"7187": "\ndef spool ( iterable , maxlen = 1250 ) : \n    words = int ( ) \n    text = str ( ) \n    try : \n        while 1 : \n            while words < maxlen : \n                stream = yield \n                text = reduce ( accumulator , stream , text ) \n                words = reduce ( accumulator , stream , words ) \n            iterable . send ( text ) \n            words = int ( ) \n            text = str ( ) \n    except GeneratorExit : \n        iterable . send ( text ) \n        iterable . close ( ) "}
{"7189": "\ndef push_url ( interface ) : \n    \n    @ functools . wraps ( interface ) \n    def connection ( * args , ** kwargs ) : \n        session = Session ( ) \n        session . mount ( 'http://' , HTTPAdapter ( max_retries = 2 ) ) \n        session . mount ( 'https://' , HTTPAdapter ( max_retries = 2 ) ) \n        request = Request ( ** interface ( * args , ** kwargs ) ) \n        prepare = session . prepare_request ( request ) \n        response = session . send ( prepare , verify = 1 ) \n        if response . status_code != requests . codes . ok : \n            response . raise_for_status ( ) \n        cleanup = re . subn ( r',(?=,)' , '' , response . content . decode ( 'utf-8' ) ) [ 0 ] \n        return json . loads ( cleanup . replace ( r'\\xA0' , r' ' ) . replace ( '[,' , '[1,' ) , encoding = 'UTF-8' ) \n    return connection "}
{"7201": "\ndef nearest_pois ( self , distance , category , num_pois = 1 , max_distance = None , imp_name = None , include_poi_ids = 0 ) : \n    if max_distance is None : \n        max_distance = distance \n    if category not in self . poi_category_names : \n        assert 0 , \"Need to call set_pois for this category\" \n    if num_pois > self . max_pois : \n        assert 0 , \"Asking for more pois than set in init_pois\" \n    imp_num = self . _imp_name_to_num ( imp_name ) \n    dists , poi_ids = self . net . find_all_nearest_pois ( distance , num_pois , category . encode ( 'utf-8' ) , imp_num ) \n    dists [ dists == - 1 ] = max_distance \n    df = pd . DataFrame ( dists , index = self . node_ids ) \n    df . columns = list ( range ( 1 , num_pois + 1 ) ) \n    if include_poi_ids : \n        df2 = pd . DataFrame ( poi_ids , index = self . node_ids ) \n        df2 . columns = [ \"poi%d\" % i for i in range ( 1 , num_pois + 1 ) ] \n        for col in df2 . columns : \n            s = df2 [ col ] . astype ( 'int' ) \n            df2 [ col ] = self . poi_category_indexes [ category ] . values [ s ] \n            df2 . loc [ s == - 1 , col ] = np . nan \n        df = pd . concat ( [ df , df2 ] , axis = 1 ) \n    return df "}
{"7207": "\ndef isregex ( value ) : \n    if not value : \n        return 0 \n    return any ( ( isregex_expr ( value ) , isinstance ( value , retype ) ) ) "}
{"7208": "\ndef compare ( self , value , expectation , regex_expr = 0 ) : \n    return compare ( value , expectation , regex_expr = regex_expr ) "}
{"7210": "\ndef compare ( expr , value , regex_expr = 0 ) : \n    if expr == value : \n        return 1 \n    negate = 0 \n    if isinstance ( expr , str ) : \n        negate = expr . startswith ( NEGATE ) \n        expr = strip_negate ( expr ) if negate else expr \n    try : \n        test ( expr , value , regex_expr = regex_expr ) \n    except Exception as err : \n        if negate : \n            return 1 \n        else : \n            raise err \n    return 1 "}
{"7212": "\ndef match ( self , request ) : \n    errors = [ ] \n    def match ( matcher ) : \n        try : \n            return matcher . match ( request ) \n        except Exception as err : \n            err = '{}: {}' . format ( type ( matcher ) . __name__ , err ) \n            errors . append ( err ) \n            return 0 \n    return all ( [ match ( matcher ) for matcher in self ] ) , errors "}
{"7230": "\ndef persist ( self , status = None ) : \n    self . _persist = status if type ( status ) is bool else 1 "}
{"7232": "\ndef reply ( self , status = 200 , new_response = 0 , ** kw ) : \n    res = Response ( ** kw ) if new_response else self . _response \n    res . status ( status or res . _status ) \n    res . mock = self \n    self . _response = res \n    return res "}
{"7233": "\ndef match ( self , request ) : \n    if self . _times <= 0 : \n        raise PookExpiredMock ( 'Mock expired' ) \n    for test in self . filters : \n        if not test ( request , self ) : \n            return 0 , [ ] \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    matches , errors = self . matchers . match ( request ) \n    if not matches : \n        return 0 , errors \n    self . _calls . append ( request ) \n    self . _matches += 1 \n    if not self . _persist : \n        self . _times -= 1 \n    if self . _error : \n        raise self . _error \n    for callback in self . callbacks : \n        callback ( request , self ) \n    return 1 , [ ] "}
{"7236": "\ndef enable_network ( self , * hostnames ) : \n    def hostname_filter ( hostname , req ) : \n        if isregex ( hostname ) : \n            return hostname . match ( req . url . hostname ) \n        return req . url . hostname == hostname \n    for hostname in hostnames : \n        self . use_network_filter ( partial ( hostname_filter , hostname ) ) \n    self . networking = 1 "}
{"7239": "\ndef activate ( self ) : \n    if self . active : \n        return None \n    self . mock_engine . activate ( ) \n    self . active = 1 "}
{"7240": "\ndef disable ( self ) : \n    if not self . active : \n        return None \n    self . mock_engine . disable ( ) \n    self . active = 0 "}
{"7242": "\ndef match ( self , request ) : \n    for test in self . filters : \n        if not test ( request , self ) : \n            return 0 \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    match_errors = [ ] \n    for mock in self . mocks [ : ] : \n        try : \n            matches , errors = mock . match ( request . copy ( ) ) \n            if len ( errors ) : \n                match_errors += errors \n            if matches : \n                return mock \n        except PookExpiredMock : \n            self . mocks . remove ( mock ) \n    if not self . should_use_network ( request ) : \n        msg = 'pook error!\\n\\n' \n        msg += ( '=> Cannot match any mock for the ' 'following request:\\n{}' . format ( request ) ) \n        if self . debug : \n            err = '\\n\\n' . join ( [ str ( err ) for err in match_errors ] ) \n            if err : \n                msg += '\\n\\n=> Detailed matching errors:\\n{}\\n' . format ( err ) \n        raise PookNoMatches ( msg ) \n    self . unmatched_reqs . append ( request ) "}
{"7245": "\ndef use ( network = 0 ) : \n    global _engine \n    __engine = _engine \n    activated = __engine . active \n    if activated : \n        __engine . disable ( ) \n    _engine = Engine ( network = network ) \n    _engine . activate ( ) \n    yield _engine \n    _engine . disable ( ) \n    if network : \n        _engine . disable_network ( ) \n    _engine = __engine \n    if activated : \n        _engine . activate ( ) "}
{"7247": "\ndef remove_interceptor ( self , name ) : \n    for index , interceptor in enumerate ( self . interceptors ) : \n        matches = ( type ( interceptor ) . __name__ == name or getattr ( interceptor , 'name' ) == name ) \n        if matches : \n            self . interceptors . pop ( index ) \n            return 1 \n    return 0 "}
{"7254": "\ndef hunt_repeated_yaml_keys ( data ) : \n    loader = yaml . Loader ( data ) \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + 1 \n        return node \n    def construct_mapping ( node , deep = 0 ) : \n        mapping = dict ( ) \n        errors = dict ( ) \n        for key_node , value_node in node . value : \n            key = key_node . value \n            if key in mapping : \n                if key in errors : \n                    errors [ key ] . append ( key_node . __line__ ) \n                else : \n                    errors [ key ] = [ mapping [ key ] , key_node . __line__ ] \n            mapping [ key ] = key_node . __line__ \n        return errors \n    loader . compose_node = compose_node \n    loader . construct_mapping = construct_mapping \n    data = loader . get_single_data ( ) \n    return data "}
{"7255": "\ndef base_regression ( Q , slope = None ) : \n    if slope is None : \n        slope = ( Q [ dtavgii ] - Q [ tavgii ] * Q [ davgii ] / Q [ sii ] ) / ( Q [ tsqii ] - Q [ tavgii ] ** 2 / Q [ sii ] ) \n        only_intercept = 0 \n    else : \n        only_intercept = 1 \n    intercept = ( Q [ davgii ] - Q [ tavgii ] * slope ) / Q [ sii ] \n    if only_intercept : \n        return { 'slope' : slope , 'intercept' : intercept , 'chisq' : 0.5 * ( Q [ dsqii ] / Q [ sii ] - Q [ davgii ] ** 2 / Q [ sii ] ** 2 ) } \n    chisq = 0.5 * ( Q [ dsqii ] - Q [ davgii ] ** 2 / Q [ sii ] - ( Q [ dtavgii ] - Q [ davgii ] * Q [ tavgii ] / Q [ sii ] ) ** 2 / ( Q [ tsqii ] - Q [ tavgii ] ** 2 / Q [ sii ] ) ) \n    estimator_hessian = np . array ( [ [ Q [ tsqii ] , Q [ tavgii ] ] , [ Q [ tavgii ] , Q [ sii ] ] ] ) \n    return { 'slope' : slope , 'intercept' : intercept , 'chisq' : chisq , 'hessian' : estimator_hessian , 'cov' : np . linalg . inv ( estimator_hessian ) } "}
{"7256": "\ndef CovInv ( self ) : \n    self . recurse ( full_matrix = 1 ) \n    return self . tree . root . cinv "}
{"7257": "\ndef recurse ( self , full_matrix = 0 ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        n_leaves = len ( n . _ii ) \n        if full_matrix : \n            M = np . zeros ( ( n_leaves , n_leaves ) , dtype = float ) \n        r = np . zeros ( n_leaves , dtype = float ) \n        c_count = 0 \n        for c in n : \n            ssq = self . branch_variance ( c ) \n            nc = len ( c . _ii ) \n            if c . is_terminal ( ) : \n                if full_matrix : \n                    M [ c_count , c_count ] = 1.0 / ssq \n                r [ c_count ] = 1.0 / ssq \n            else : \n                if full_matrix : \n                    M [ c_count : c_count + nc , c_count : c_count + nc ] = c . cinv - ssq * np . outer ( c . r , c . r ) / ( 1 + ssq * c . s ) \n                r [ c_count : c_count + nc ] = c . r / ( 1 + ssq * c . s ) \n            c_count += nc \n        if full_matrix : \n            n . cinv = M \n        n . r = r \n        n . s = n . r . sum ( ) "}
{"7258": "\ndef _calculate_averages ( self ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        Q = np . zeros ( 6 , dtype = float ) \n        for c in n : \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            Q += self . propagate_averages ( c , tv , bv , var ) \n        n . Q = Q \n    for n in self . tree . find_clades ( order = 'preorder' ) : \n        O = np . zeros ( 6 , dtype = float ) \n        if n == self . tree . root : \n            n . Qtot = n . Q \n            continue \n        for c in n . up : \n            if c == n : \n                continue \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O += self . propagate_averages ( c , tv , bv , var ) \n        if n . up != self . tree . root : \n            c = n . up \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O += self . propagate_averages ( c , tv , bv , var , outgroup = 1 ) \n        n . O = O \n        if not n . is_terminal ( ) : \n            tv = self . tip_value ( n ) \n            bv = self . branch_value ( n ) \n            var = self . branch_variance ( n ) \n            n . Qtot = n . Q + self . propagate_averages ( n , tv , bv , var , outgroup = 1 ) "}
{"7259": "\ndef propagate_averages ( self , n , tv , bv , var , outgroup = 0 ) : \n    if n . is_terminal ( ) and outgroup == 0 : \n        if tv is None or np . isinf ( tv ) or np . isnan ( tv ) : \n            res = np . array ( [ 0 , 0 , 0 , 0 , 0 , 0 ] ) \n        elif var == 0 : \n            res = np . array ( [ np . inf , np . inf , np . inf , np . inf , np . inf , np . inf ] ) \n        else : \n            res = np . array ( [ tv / var , bv / var , tv ** 2 / var , bv * tv / var , bv ** 2 / var , 1.0 / var ] , dtype = float ) \n    else : \n        tmpQ = n . O if outgroup else n . Q \n        denom = 1.0 / ( 1 + var * tmpQ [ sii ] ) \n        res = np . array ( [ tmpQ [ tavgii ] * denom , ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ tsqii ] - var * tmpQ [ tavgii ] ** 2 * denom , tmpQ [ dtavgii ] + tmpQ [ tavgii ] * bv - var * tmpQ [ tavgii ] * ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ dsqii ] + 2 * bv * tmpQ [ davgii ] + bv ** 2 * tmpQ [ sii ] - var * ( tmpQ [ davgii ] ** 2 + 2 * bv * tmpQ [ davgii ] * tmpQ [ sii ] + bv ** 2 * tmpQ [ sii ] ** 2 ) * denom , tmpQ [ sii ] * denom ] ) \n    return res "}
{"7262": "\ndef find_best_root ( self , force_positive = 1 , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if n == self . tree . root : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( chisq < best_root [ \"chisq\" ] ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( 1 - x ) , var * ( 1 - x ) , outgroup = 1 ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if reg [ \"slope\" ] >= 0 or ( force_positive == 0 ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        print ( \"TreeRegression.find_best_root: No valid root found!\" , force_positive ) \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( 1 - y ) , var * ( 1 - y ) , outgroup = 1 ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3 , 3 ) ) \n        estimator_hessian [ : 2 , : 2 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2 , 2 ] = ( deriv [ 0 ] [ 1 ] + deriv [ 1 ] [ 1 ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ 0 ] [ 0 ] - deriv [ 1 ] [ 0 ] ) ** 2 \n        estimator_hessian [ 0 , 2 ] = estimator_hessian [ 2 , 0 ] \n        estimator_hessian [ 1 , 2 ] = estimator_hessian [ 2 , 1 ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7263": "\ndef set_Tc ( self , Tc , T = None ) : \n    if isinstance ( Tc , Iterable ) : \n        if len ( Tc ) == len ( T ) : \n            x = np . concatenate ( ( [ - ttconf . BIG_NUMBER ] , T , [ ttconf . BIG_NUMBER ] ) ) \n            y = np . concatenate ( ( [ Tc [ 0 ] ] , Tc , [ Tc [ - 1 ] ] ) ) \n            self . Tc = interp1d ( x , y ) \n        else : \n            self . logger ( \"need Tc values and Timepoints of equal length\" , 2 , warn = 1 ) \n            self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ 1e-5 , 1e-5 ] ) \n    else : \n        self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ Tc + ttconf . TINY_NUMBER , Tc + ttconf . TINY_NUMBER ] ) \n    self . calc_integral_merger_rate ( ) "}
{"7267": "\ndef optimize_Tc ( self ) : \n    from scipy . optimize import minimize_scalar \n    initial_Tc = self . Tc \n    def cost ( Tc ) : \n        self . set_Tc ( Tc ) \n        return - self . total_LH ( ) \n    sol = minimize_scalar ( cost , bounds = [ ttconf . TINY_NUMBER , 10.0 ] ) \n    if \"success\" in sol and sol [ \"success\" ] : \n        self . set_Tc ( sol [ 'x' ] ) \n    else : \n        self . logger ( \"merger_models:optimze_Tc: optimization of coalescent time scale failed: \" + str ( sol ) , 0 , warn = 1 ) \n        self . set_Tc ( initial_Tc . y , T = initial_Tc . x ) "}
{"7268": "\ndef prof2seq ( profile , gtr , sample_from_prof = 0 , normalize = 1 ) : \n    if normalize : \n        tmp_profile , pre = normalize_profile ( profile , return_offset = 0 ) \n    else : \n        tmp_profile = profile \n    if sample_from_prof : \n        cumdis = tmp_profile . cumsum ( axis = 1 ) . T \n        randnum = np . random . random ( size = cumdis . shape [ 1 ] ) \n        idx = np . argmax ( cumdis >= randnum , axis = 0 ) \n    else : \n        idx = tmp_profile . argmax ( axis = 1 ) \n    seq = gtr . alphabet [ idx ] \n    prof_values = tmp_profile [ np . arange ( tmp_profile . shape [ 0 ] ) , idx ] \n    return seq , prof_values , idx "}
{"7269": "\ndef normalize_profile ( in_profile , log = 0 , return_offset = 1 ) : \n    if log : \n        tmp_prefactor = in_profile . max ( axis = 1 ) \n        tmp_prof = np . exp ( in_profile . T - tmp_prefactor ) . T \n    else : \n        tmp_prefactor = 0.0 \n        tmp_prof = in_profile \n    norm_vector = tmp_prof . sum ( axis = 1 ) \n    return ( np . copy ( np . einsum ( 'ai,a->ai' , tmp_prof , 1.0 / norm_vector ) ) , ( np . log ( norm_vector ) + tmp_prefactor ) if return_offset else None ) "}
{"7271": "\ndef set_gtr ( self , in_gtr , ** kwargs ) : \n    if isinstance ( in_gtr , str ) : \n        self . _gtr = GTR . standard ( model = in_gtr , ** kwargs ) \n        self . _gtr . logger = self . logger \n    elif isinstance ( in_gtr , GTR ) or isinstance ( in_gtr , GTR_site_specific ) : \n        self . _gtr = in_gtr \n        self . _gtr . logger = self . logger \n    else : \n        self . logger ( \"TreeAnc.gtr_setter: can't interpret GTR model\" , 1 , warn = 1 ) \n        raise TypeError ( \"Cannot set GTR model in TreeAnc class: GTR or \" \"string expected\" ) \n    if self . _gtr . ambiguous is None : \n        self . fill_overhangs = 0 "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = 0 \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        elif l . is_terminal ( ) : \n            self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , 0 , warn = 1 ) \n            failed_leaves += 1 \n            l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n            if failed_leaves > self . tree . count_terminals ( ) / 3 : \n                self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , 0 , warn = 1 ) \n                self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2 , warn = 1 ) \n                break \n        else : \n            pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , 0 , warn = 1 ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7275": "\ndef _prepare_nodes ( self ) : \n    self . tree . root . up = None \n    self . tree . root . bad_branch = self . tree . root . bad_branch if hasattr ( self . tree . root , 'bad_branch' ) else 0 \n    internal_node_count = 0 \n    for clade in self . tree . get_nonterminals ( order = 'preorder' ) : \n        internal_node_count += 1 \n        if clade . name is None : \n            clade . name = \"NODE_\" + format ( self . _internal_node_count , '07d' ) \n            self . _internal_node_count += 1 \n        for c in clade . clades : \n            if c . is_terminal ( ) : \n                c . bad_branch = c . bad_branch if hasattr ( c , 'bad_branch' ) else 0 \n            c . up = clade \n    for clade in self . tree . get_nonterminals ( order = 'postorder' ) : \n        clade . bad_branch = all ( [ c . bad_branch for c in clade ] ) \n    self . _calc_dist2root ( ) \n    self . _internal_node_count = max ( internal_node_count , self . _internal_node_count ) "}
{"7277": "\ndef reconstruct_anc ( self , method = 'probabilistic' , infer_gtr = 0 , marginal = 0 , ** kwargs ) : \n    self . logger ( \"TreeAnc.infer_ancestral_sequences with method: %s, %s\" % ( method , 'marginal' if marginal else 'joint' ) , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.infer_ancestral_sequences: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    if method in [ 'ml' , 'probabilistic' ] : \n        if marginal : \n            _ml_anc = self . _ml_anc_marginal \n        else : \n            _ml_anc = self . _ml_anc_joint \n    else : \n        _ml_anc = self . _fitch_anc \n    if infer_gtr : \n        tmp = self . infer_gtr ( marginal = marginal , ** kwargs ) \n        if tmp == ttconf . ERROR : \n            return tmp \n        N_diff = _ml_anc ( ** kwargs ) \n    else : \n        N_diff = _ml_anc ( ** kwargs ) \n    return N_diff "}
{"7278": "\ndef get_branch_mutation_matrix ( self , node , full_sequence = 0 ) : \n    pp , pc = self . marginal_branch_profile ( node ) \n    expQt = self . gtr . expQt ( self . _branch_length_to_gtr ( node ) ) \n    if len ( expQt . shape ) == 3 : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ija->aij' , pc , pp , expQt ) \n    else : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ij->aij' , pc , pp , expQt ) \n    normalizer = mut_matrix_stack . sum ( axis = 2 ) . sum ( axis = 1 ) \n    mut_matrix_stack = np . einsum ( 'aij,a->aij' , mut_matrix_stack , 1.0 / normalizer ) \n    if full_sequence : \n        return mut_matrix_stack [ self . full_to_reduced_sequence_map ] \n    else : \n        return mut_matrix_stack "}
{"7279": "\ndef expanded_sequence ( self , node , include_additional_constant_sites = 0 ) : \n    if include_additional_constant_sites : \n        L = self . seq_len \n    else : \n        L = self . seq_len - self . additional_constant_sites \n    return node . cseq [ self . full_to_reduced_sequence_map [ : L ] ] "}
{"7283": "\ndef sequence_LH ( self , pos = None , full_sequence = 0 ) : \n    if not hasattr ( self . tree , \"total_sequence_LH\" ) : \n        self . logger ( \"TreeAnc.sequence_LH: you need to run marginal ancestral inference first!\" , 1 ) \n        self . infer_ancestral_sequences ( marginal = 1 ) \n    if pos is not None : \n        if full_sequence : \n            compressed_pos = self . full_to_reduced_sequence_map [ pos ] \n        else : \n            compressed_pos = pos \n        return self . tree . sequence_LH [ compressed_pos ] \n    else : \n        return self . tree . total_sequence_LH "}
{"7286": "\ndef optimize_branch_length ( self , mode = 'joint' , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\" % mode , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    store_old_dist = 0 \n    if 'store_old' in kwargs : \n        store_old_dist = kwargs [ 'store_old' ] \n    if mode == 'marginal' : \n        if not hasattr ( self . tree . root , \"marginal_profile\" ) : \n            self . infer_ancestral_sequences ( marginal = 1 ) \n    max_bl = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            continue \n        if store_old_dist : \n            node . _old_length = node . branch_length \n        if mode == 'marginal' : \n            new_len = self . optimal_marginal_branch_length ( node ) \n        elif mode == 'joint' : \n            new_len = self . optimal_branch_length ( node ) \n        else : \n            self . logger ( \"treeanc.optimize_branch_length: unsupported optimization mode\" , 4 , warn = 1 ) \n            new_len = node . branch_length \n        if new_len < 0 : \n            continue \n        self . logger ( \"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\" \" Updating branch length...\" % ( node . branch_length , new_len , len ( node . mutations ) * self . one_mutation ) , 5 ) \n        node . branch_length = new_len \n        node . mutation_length = new_len \n        max_bl = max ( max_bl , new_len ) \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    if max_bl > 0.15 and mode == 'joint' : \n        self . logger ( \"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\" \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\" \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\" \" \\n\\t ****branch_length_mode='input'\" , 0 , warn = 1 ) \n    self . _prepare_nodes ( ) \n    return ttconf . SUCCESS "}
{"7287": "\ndef optimize_branch_length_global ( self , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length_global: running branch length optimization...\" , 1 ) \n    def neg_log ( s ) : \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            n . branch_length = si ** 2 \n        self . infer_ancestral_sequences ( marginal = 1 ) \n        gradient = [ ] \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            if n . up : \n                pp , pc = self . marginal_branch_profile ( n ) \n                Qtds = self . gtr . expQsds ( si ) . T \n                Qt = self . gtr . expQs ( si ) . T \n                res = pp . dot ( Qt ) \n                overlap = np . sum ( res * pc , axis = 1 ) \n                res_ds = pp . dot ( Qtds ) \n                overlap_ds = np . sum ( res_ds * pc , axis = 1 ) \n                logP = np . sum ( self . multiplicity * overlap_ds / overlap ) \n                gradient . append ( logP ) \n            else : \n                gradient . append ( 2 * ( si ** 2 - 0.001 ) ) \n        print ( - self . tree . sequence_marginal_LH ) \n        return ( - self . tree . sequence_marginal_LH + ( s [ 0 ] ** 2 - 0.001 ) ** 2 , - 1.0 * np . array ( gradient ) ) \n    from scipy . optimize import minimize \n    x0 = np . sqrt ( [ n . branch_length for n in self . tree . find_clades ( order = 'preorder' ) ] ) \n    sol = minimize ( neg_log , x0 , jac = 1 ) \n    for new_len , node in zip ( sol [ 'x' ] , self . tree . find_clades ( ) ) : \n        self . logger ( \"Optimization results: old_len=%.4f, new_len=%.4f \" \" Updating branch length...\" % ( node . branch_length , new_len ) , 5 ) \n        node . branch_length = new_len ** 2 \n        node . mutation_length = new_len ** 2 \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    self . _prepare_nodes ( ) "}
{"7289": "\ndef optimize_seq_and_branch_len ( self , reuse_branch_len = 1 , prune_short = 1 , marginal_sequences = 0 , branch_length_mode = 'joint' , max_iter = 5 , infer_gtr = 0 , ** kwargs ) : \n    if branch_length_mode == 'marginal' : \n        marginal_sequences = 1 \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: sequences...\" , 1 ) \n    if reuse_branch_len : \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = infer_gtr , marginal = marginal_sequences , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = 0 , mode = branch_length_mode ) \n    else : \n        N_diff = self . reconstruct_anc ( method = 'fitch' , infer_gtr = infer_gtr , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = 0 , marginal = 0 ) \n    n = 0 \n    while n < max_iter : \n        n += 1 \n        if prune_short : \n            self . prune_short_branches ( ) \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = 0 , marginal = marginal_sequences , ** kwargs ) \n        self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\" \" #Nuc changed since prev reconstructions: %d\" % ( n , N_diff ) , 2 ) \n        if N_diff < 1 : \n            break \n        self . optimize_branch_len ( verbose = 0 , store_old = 0 , mode = branch_length_mode ) \n    self . tree . unconstrained_sequence_LH = ( self . tree . sequence_LH * self . multiplicity ) . sum ( ) \n    self . _prepare_nodes ( ) \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self . tree . unconstrained_sequence_LH , 2 ) \n    return ttconf . SUCCESS "}
{"7294": "\ndef _check_fix_Q ( self , fixed_mu = 0 ) : \n    self . Pi /= self . Pi . sum ( ) \n    self . W += self . break_degen + self . break_degen . T \n    np . fill_diagonal ( self . W , 0 ) \n    Wdiag = - ( self . Q ) . sum ( axis = 0 ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W /= scale_factor \n    if not fixed_mu : \n        self . mu *= scale_factor \n    if ( self . Q . sum ( axis = 0 ) < 1e-10 ) . sum ( ) < self . alphabet . shape [ 0 ] : \n        print ( \"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\" , self . Q . sum ( axis = 0 ) ) \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7295": "\ndef prob_t_compressed ( self , seq_pair , multiplicity , t , return_log = 0 ) : \n    if t < 0 : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        tmp_eQT = self . expQt ( t ) \n        bad_indices = ( tmp_eQT == 0 ) \n        logQt = np . log ( tmp_eQT + ttconf . TINY_NUMBER * ( bad_indices ) ) \n        logQt [ np . isnan ( logQt ) | np . isinf ( logQt ) | bad_indices ] = - ttconf . BIG_NUMBER \n        logP = np . sum ( logQt [ seq_pair [ : , 1 ] , seq_pair [ : , 0 ] ] * multiplicity ) \n    return logP if return_log else np . exp ( logP ) "}
{"7296": "\ndef optimal_t ( self , seq_p , seq_ch , pattern_multiplicity = None , ignore_gaps = 0 ) : \n    seq_pair , multiplicity = self . compress_sequence_pair ( seq_p , seq_ch , pattern_multiplicity = pattern_multiplicity , ignore_gaps = ignore_gaps ) \n    return self . optimal_t_compressed ( seq_pair , multiplicity ) "}
{"7297": "\ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = 0 , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2 , return_log = 1 ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2 , return_log = 1 ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2 \n        if 'success' not in opt : \n            opt [ 'success' ] = 1 \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4 , warn = 1 ) \n    except : \n        import scipy \n        print ( 'legacy scipy' , scipy . __version__ ) \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2 \n        opt = { 'success' : 1 } \n    if new_len > .9 * ttconf . MAX_BRANCH_LENGTH : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4 , warn = 1 ) \n    if opt [ \"success\" ] != 1 : \n        new_len = np . sum ( multiplicity [ seq_pair [ : , 1 ] != seq_pair [ : , 0 ] ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7298": "\ndef prob_t_profiles ( self , profile_pair , multiplicity , t , return_log = 0 , ignore_gaps = 1 ) : \n    if t < 0 : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        Qt = self . expQt ( t ) \n        if len ( Qt . shape ) == 3 : \n            res = np . einsum ( 'ai,ija,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        else : \n            res = np . einsum ( 'ai,ij,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        if ignore_gaps and ( self . gap_index is not None ) : \n            non_gap_frac = ( 1 - profile_pair [ 0 ] [ : , self . gap_index ] ) * ( 1 - profile_pair [ 1 ] [ : , self . gap_index ] ) \n            logP = np . sum ( multiplicity * np . log ( res ) * non_gap_frac ) \n        else : \n            logP = np . sum ( multiplicity * np . log ( res ) ) \n    return logP if return_log else np . exp ( logP ) "}
{"7299": "\ndef evolve ( self , profile , t , return_log = 0 ) : \n    Qt = self . expQt ( t ) . T \n    res = profile . dot ( Qt ) \n    return np . log ( res ) if return_log else res "}
{"7302": "\ndef clock_filter ( self , reroot = 'least-squares' , n_iqd = None , plot = 0 ) : \n    if n_iqd is None : \n        n_iqd = ttconf . NIQD \n    if type ( reroot ) is list and len ( reroot ) == 1 : \n        reroot = str ( reroot [ 0 ] ) \n    terminals = self . tree . get_terminals ( ) \n    if reroot : \n        if self . reroot ( root = 'least-squares' if reroot == 'best' else reroot , covariation = 0 ) == ttconf . ERROR : \n            return ttconf . ERROR \n    else : \n        self . get_clock_model ( covariation = 0 ) \n    clock_rate = self . clock_model [ 'slope' ] \n    icpt = self . clock_model [ 'intercept' ] \n    res = { } \n    for node in terminals : \n        if hasattr ( node , 'raw_date_constraint' ) and ( node . raw_date_constraint is not None ) : \n            res [ node ] = node . dist2root - clock_rate * np . mean ( node . raw_date_constraint ) - icpt \n    residuals = np . array ( list ( res . values ( ) ) ) \n    iqd = np . percentile ( residuals , 75 ) - np . percentile ( residuals , 25 ) \n    for node , r in res . items ( ) : \n        if abs ( r ) > n_iqd * iqd and node . up . up is not None : \n            self . logger ( 'TreeTime.ClockFilter: marking %s as outlier, residual %f interquartile distances' % ( node . name , r / iqd ) , 3 , warn = 1 ) \n            node . bad_branch = 1 \n        else : \n            node . bad_branch = 0 \n    if reroot and self . reroot ( root = reroot ) == ttconf . ERROR : \n        return ttconf . ERROR \n    if plot : \n        self . plot_root_to_tip ( ) \n    return ttconf . SUCCESS "}
{"7303": "\ndef plot_root_to_tip ( self , add_internal = 0 , label = 1 , ax = None ) : \n    Treg = self . setup_TreeRegression ( ) \n    if self . clock_model and 'cov' in self . clock_model : \n        cf = self . clock_model [ 'valid_confidence' ] \n    else : \n        cf = 0 \n    Treg . clock_plot ( ax = ax , add_internal = add_internal , confidence = cf , n_sigma = 2 , regression = self . clock_model ) "}
{"7304": "\ndef resolve_polytomies ( self , merge_compressed = 0 ) : \n    self . logger ( \"TreeTime.resolve_polytomies: resolving multiple mergers...\" , 1 ) \n    poly_found = 0 \n    for n in self . tree . find_clades ( ) : \n        if len ( n . clades ) > 2 : \n            prior_n_clades = len ( n . clades ) \n            self . _poly ( n , merge_compressed ) \n            poly_found += prior_n_clades - len ( n . clades ) \n    obsolete_nodes = [ n for n in self . tree . find_clades ( ) if len ( n . clades ) == 1 and n . up is not None ] \n    for node in obsolete_nodes : \n        self . logger ( 'TreeTime.resolve_polytomies: remove obsolete node ' + node . name , 4 ) \n        if node . up is not None : \n            self . tree . collapse ( node ) \n    if poly_found : \n        self . logger ( 'TreeTime.resolve_polytomies: introduces %d new nodes' % poly_found , 3 ) \n    else : \n        self . logger ( 'TreeTime.resolve_polytomies: No more polytomies to resolve' , 3 ) \n    return poly_found "}
{"7305": "\ndef print_lh ( self , joint = 1 ) : \n    try : \n        u_lh = self . tree . unconstrained_sequence_LH \n        if joint : \n            s_lh = self . tree . sequence_joint_LH \n            t_lh = self . tree . positional_joint_LH \n            c_lh = self . tree . coalescent_joint_LH \n        else : \n            s_lh = self . tree . sequence_marginal_LH \n            t_lh = self . tree . positional_marginal_LH \n            c_lh = 0 \n        print ( \"###  Tree Log-Likelihood  ###\\n\" \" Sequence log-LH without constraints: \\t%1.3f\\n\" \" Sequence log-LH with constraints:    \\t%1.3f\\n\" \" TreeTime sequence log-LH:            \\t%1.3f\\n\" \" Coalescent log-LH:                   \\t%1.3f\\n\" \"#########################\" % ( u_lh , s_lh , t_lh , c_lh ) ) \n    except : \n        print ( \"ERROR. Did you run the corresponding inference (joint/marginal)?\" ) "}
{"7306": "\ndef add_coalescent_model ( self , Tc , ** kwargs ) : \n    from . merger_models import Coalescent \n    self . logger ( 'TreeTime.run: adding coalescent prior with Tc=' + str ( Tc ) , 1 ) \n    self . merger_model = Coalescent ( self . tree , date2dist = self . date2dist , logger = self . logger ) \n    if Tc == 'skyline' : \n        self . merger_model . optimize_skyline ( ** kwargs ) \n        self . logger ( \"optimized a skyline \" , 2 ) \n    else : \n        if Tc in [ 'opt' , 'const' ] : \n            self . merger_model . optimize_Tc ( ) \n            self . logger ( \"optimized Tc to %f\" % self . merger_model . Tc . y [ 0 ] , 2 ) \n        else : \n            try : \n                self . merger_model . set_Tc ( Tc ) \n            except : \n                self . logger ( \"setting of coalescent time scale failed\" , 1 , warn = 1 ) \n    self . merger_model . attach_to_tree ( ) "}
{"7307": "\ndef _find_best_root ( self , covariation = 1 , force_positive = 1 , slope = 0 , ** kwarks ) : \n    for n in self . tree . find_clades ( ) : \n        n . branch_length = n . mutation_length \n    self . logger ( \"TreeTime._find_best_root: searching for the best root position...\" , 2 ) \n    Treg = self . setup_TreeRegression ( covariation = covariation ) \n    return Treg . optimal_reroot ( force_positive = force_positive , slope = slope ) [ 'node' ] "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2 : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    elif keyval [ 0 ] not in [ 'alphabet' ] : \n                        keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = 0 \n        except : \n            print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = 0 \n    return gtr "}
{"7311": "\ndef ancestral_reconstruction ( params ) : \n    if assure_tree ( params , tmp_dir = 'ancestral_tmp' ) : \n        return 1 \n    outdir = get_outdir ( params , '_ancestral' ) \n    basename = get_basename ( params , outdir ) \n    gtr = create_gtr ( params ) \n    aln , ref , fixed_pi = read_if_vcf ( params ) \n    is_vcf = 1 if ref is not None else 0 \n    treeanc = TreeAnc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = 1 , fill_overhangs = not params . keep_overhangs ) \n    ndiff = treeanc . infer_ancestral_sequences ( 'ml' , infer_gtr = params . gtr == 'infer' , marginal = params . marginal , fixed_pi = fixed_pi ) \n    if ndiff == ttconf . ERROR : \n        return 1 \n    if params . gtr == \"infer\" : \n        print ( '\\nInferred GTR model:' ) \n        print ( treeanc . gtr ) \n    export_sequences_and_tree ( treeanc , basename , is_vcf , params . zero_based , report_ambiguous = params . report_ambiguous ) \n    return 0 "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = 1 ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( log_prob < 0.693147 ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if L < 2 : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7313": "\ndef delta_function ( cls , x_pos , weight = 1. , min_width = MIN_INTEGRATION_PEAK ) : \n    distribution = cls ( x_pos , 0. , is_log = 1 , min_width = min_width ) \n    distribution . weight = weight \n    return distribution "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if n_delta > 1 : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif n_delta == 1 : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n        peak = y_vals . min ( ) \n        ind = ( y_vals - peak ) < BIG_NUMBER / 1000 \n        n_points = ind . sum ( ) \n        if n_points == 0 : \n            print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n            x_vals = [ 0 , 1 ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = 1 , min_width = min_width , kind = 'linear' ) \n        elif n_points == 1 : \n            res = Distribution . delta_function ( x_vals [ 0 ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = 1 , min_width = min_width , kind = 'linear' , assume_sorted = 1 ) \n    return res "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , 0 ) \n        return ttconf . ERROR \n    bad_branch_counter = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = 1 ) \n                node . raw_date_constraint = None \n                node . bad_branch = 1 \n            else : \n                try : \n                    tmp = np . mean ( tmp_date ) \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = 0 \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = 1 ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = 1 \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = 1 \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter += 1 \n    if bad_branch_counter > self . tree . count_terminals ( ) - 3 : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , 1 , warn = 1 ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7316": "\ndef setup_TreeRegression ( self , covariation = 1 ) : \n    from . treeregression import TreeRegression \n    tip_value = lambda x : np . mean ( x . raw_date_constraint ) if ( x . is_terminal ( ) and ( x . bad_branch is 0 ) ) else None \n    branch_value = lambda x : x . mutation_length \n    if covariation : \n        om = self . one_mutation \n        branch_variance = lambda x : ( ( x . clock_length if hasattr ( x , 'clock_length' ) else x . mutation_length ) + ( self . tip_slack ** 2 * om if x . is_terminal ( ) else 0.0 ) ) * om \n    else : \n        branch_variance = lambda x : 1.0 if x . is_terminal ( ) else 0.0 \n    Treg = TreeRegression ( self . tree , tip_value = tip_value , branch_value = branch_value , branch_variance = branch_variance ) \n    Treg . valid_confidence = covariation \n    return Treg "}
{"7317": "\ndef make_time_tree ( self , time_marginal = 0 , clock_rate = None , ** kwargs ) : \n    self . logger ( \"ClockTree: Maximum likelihood tree optimization with temporal constraints\" , 1 ) \n    self . init_date_constraints ( clock_rate = clock_rate , ** kwargs ) \n    if time_marginal : \n        self . _ml_t_marginal ( assign_dates = time_marginal == \"assign\" ) \n    else : \n        self . _ml_t_joint ( ) \n    self . convert_dates ( ) "}
{"7319": "\ndef convert_dates ( self ) : \n    from datetime import datetime , timedelta \n    now = numeric_date ( ) \n    for node in self . tree . find_clades ( ) : \n        years_bp = self . date2dist . to_years ( node . time_before_present ) \n        if years_bp < 0 and self . real_dates : \n            if not hasattr ( node , \"bad_branch\" ) or node . bad_branch is 0 : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: The node is later than today, but it is not \" \"marked as \\\"BAD\\\", which indicates the error in the \" \"likelihood optimization.\" , 4 , warn = 1 ) \n            else : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: node which is marked as \\\"BAD\\\" optimized \" \"later than present day\" , 4 , warn = 1 ) \n        node . numdate = now - years_bp \n        year = np . floor ( node . numdate ) \n        days = max ( 0 , 365.25 * ( node . numdate - year ) - 1 ) \n        try : \n            n_date = datetime ( year , 1 , 1 ) + timedelta ( days = days ) \n            node . date = datetime . strftime ( n_date , \"%Y-%m-%d\" ) \n        except : \n            n_date = datetime ( 1900 , 1 , 1 ) + timedelta ( days = days ) \n            node . date = \"%04d-%02d-%02d\" % ( year , n_date . month , n_date . day ) "}
{"7321": "\ndef get_max_posterior_region ( self , node , fraction = 0.9 ) : \n    if node . marginal_inverse_cdf == \"delta\" : \n        return np . array ( [ node . numdate , node . numdate ] ) \n    min_max = ( node . marginal_pos_LH . xmin , node . marginal_pos_LH . xmax ) \n    min_date , max_date = [ self . date2dist . to_numdate ( x ) for x in min_max ] [ : : - 1 ] \n    if node . marginal_pos_LH . peak_pos == min_max [ 0 ] : \n        return self . get_confidence_interval ( node , ( 0 , fraction ) ) \n    elif node . marginal_pos_LH . peak_pos == min_max [ 1 ] : \n        return self . get_confidence_interval ( node , ( 1.0 - fraction , 1.0 ) ) \n    else : \n        rate_contribution = self . date_uncertainty_due_to_rate ( node , ( ( 1 - fraction ) * 0.5 , 1.0 - ( 1.0 - fraction ) * 0.5 ) ) \n        from scipy . interpolate import interp1d \n        from scipy . optimize import minimize_scalar as minimize \n        pidx = np . argmin ( node . marginal_pos_LH . y ) \n        pval = np . min ( node . marginal_pos_LH . y ) \n        left = interp1d ( node . marginal_pos_LH . y [ : ( pidx + 1 ) ] - pval , node . marginal_pos_LH . x [ : ( pidx + 1 ) ] , kind = 'linear' , fill_value = min_max [ 0 ] , bounds_error = 0 ) \n        right = interp1d ( node . marginal_pos_LH . y [ pidx : ] - pval , node . marginal_pos_LH . x [ pidx : ] , kind = 'linear' , fill_value = min_max [ 1 ] , bounds_error = 0 ) \n        def func ( x , thres ) : \n            interval = np . array ( [ left ( x ) , right ( x ) ] ) . squeeze ( ) \n            return ( thres - np . diff ( node . marginal_cdf ( np . array ( interval ) ) ) ) ** 2 \n        sol = minimize ( func , bracket = [ 0 , 10 ] , args = ( fraction , ) ) \n        if sol [ 'success' ] : \n            mutation_contribution = self . date2dist . to_numdate ( np . array ( [ right ( sol [ 'x' ] ) , left ( sol [ 'x' ] ) ] ) . squeeze ( ) ) \n        else : \n            mutation_contribution = None \n        return self . combine_confidence ( node . numdate , ( min_date , max_date ) , c1 = rate_contribution , c2 = mutation_contribution ) "}
{"7325": "\ndef from_regression ( cls , clock_model ) : \n    dc = cls ( ) \n    dc . clock_rate = clock_model [ 'slope' ] \n    dc . intercept = clock_model [ 'intercept' ] \n    dc . chisq = clock_model [ 'chisq' ] if 'chisq' in clock_model else None \n    dc . valid_confidence = clock_model [ 'valid_confidence' ] if 'valid_confidence' in clock_model else 0 \n    if 'cov' in clock_model and dc . valid_confidence : \n        dc . cov = clock_model [ 'cov' ] \n    dc . r_val = clock_model [ 'r_val' ] \n    return dc "}
{"7327": "\ndef close ( self ) : \n    self . client . close ( ) \n    self . _client = None \n    self . connected = 0 \n    self . logger . debug ( 'Connection closed.' ) "}
{"7328": "\ndef receive ( self ) : \n    start = 0 \n    while 1 : \n        idx = self . _buffer . find ( INST_TERM . encode ( ) , start ) \n        if idx != - 1 : \n            line = self . _buffer [ : idx + 1 ] . decode ( ) \n            self . _buffer = self . _buffer [ idx + 1 : ] \n            self . logger . debug ( 'Received instruction: %s' % line ) \n            return line \n        else : \n            start = len ( self . _buffer ) \n            buf = self . client . recv ( BUF_LEN ) \n            if not buf : \n                self . close ( ) \n                self . logger . debug ( 'Failed to receive instruction. Closing.' ) \n                return None \n            self . _buffer . extend ( buf ) "}
{"7331": "\ndef handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , ** kwargs ) : \n    if protocol not in PROTOCOLS : \n        self . logger . debug ( 'Invalid protocol: %s' % protocol ) \n        raise GuacamoleError ( 'Cannot start Handshake. Missing protocol.' ) \n    if audio is None : \n        audio = list ( ) \n    if video is None : \n        video = list ( ) \n    if image is None : \n        image = list ( ) \n    self . logger . debug ( 'Send `select` instruction.' ) \n    self . send_instruction ( Instruction ( 'select' , protocol ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) \n    if not instruction : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Connection Lost!' ) \n    if instruction . opcode != 'args' : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) \n    self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) \n    self . send_instruction ( Instruction ( 'size' , width , height , dpi ) ) \n    self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) \n    self . send_instruction ( Instruction ( 'audio' , * audio ) ) \n    self . logger . debug ( 'Send `video` instruction (%s)' % video ) \n    self . send_instruction ( Instruction ( 'video' , * video ) ) \n    self . logger . debug ( 'Send `image` instruction (%s)' % image ) \n    self . send_instruction ( Instruction ( 'image' , * image ) ) \n    connection_args = [ kwargs . get ( arg . replace ( '-' , '_' ) , '' ) for arg in instruction . args ] \n    self . logger . debug ( 'Send `connect` instruction (%s)' % connection_args ) \n    self . send_instruction ( Instruction ( 'connect' , * connection_args ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) \n    if instruction . opcode != 'ready' : \n        self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) \n    if instruction . args : \n        self . _id = instruction . args [ 0 ] \n        self . logger . debug ( 'Established connection with client id: %s' % self . id ) \n    self . logger . debug ( 'Handshake completed.' ) \n    self . connected = 1 "}
{"7338": "\ndef class_url ( cls ) : \n    base = 'v{0}' . format ( getattr ( cls , 'RESOURCE_VERSION' , '1' ) ) \n    return \"/{0}/{1}\" . format ( base , class_to_api_name ( cls . class_name ( ) , pluralize = 0 ) ) "}
{"7346": "\ndef range ( self , chromosome , start , stop , exact = 0 ) : \n    return self . _clone ( filters = [ GenomicFilter ( chromosome , start , stop , exact ) ] ) "}
{"7347": "\ndef position ( self , chromosome , position , exact = 0 ) : \n    return self . _clone ( filters = [ GenomicFilter ( chromosome , position , exact = exact ) ] ) "}
{"7352": "\ndef migrate ( self , target , follow = 1 , ** kwargs ) : \n    from solvebio import Dataset \n    from solvebio import DatasetMigration \n    if isinstance ( target , Dataset ) : \n        target_id = target . id \n    else : \n        target_id = target \n    limit = kwargs . pop ( 'limit' , None ) \n    if not limit and self . _limit < float ( 'inf' ) : \n        limit = self . _limit \n    params = self . _build_query ( limit = limit ) \n    params . pop ( 'offset' , None ) \n    params . pop ( 'ordering' , None ) \n    migration = DatasetMigration . create ( source_id = self . _dataset_id , target_id = target_id , source_params = params , client = self . _client , ** kwargs ) \n    if follow : \n        migration . follow ( ) \n    return migration "}
{"7354": "\ndef download_vault_folder ( remote_path , local_path , dry_run = 0 , force = 0 ) : \n    local_path = os . path . normpath ( os . path . expanduser ( local_path ) ) \n    if not os . access ( local_path , os . W_OK ) : \n        raise Exception ( 'Write access to local path ({}) is required' . format ( local_path ) ) \n    full_path , path_dict = solvebio . Object . validate_full_path ( remote_path ) \n    vault = solvebio . Vault . get_by_full_path ( path_dict [ 'vault' ] ) \n    print ( 'Downloading all files from {} to {}' . format ( full_path , local_path ) ) \n    if path_dict [ 'path' ] == '/' : \n        parent_object_id = None \n    else : \n        parent_object = solvebio . Object . get_by_full_path ( remote_path , assert_type = 'folder' ) \n        parent_object_id = parent_object . id \n    print ( 'Creating local directory structure at: {}' . format ( local_path ) ) \n    if not os . path . exists ( local_path ) : \n        if not dry_run : \n            os . makedirs ( local_path ) \n    folders = vault . folders ( parent_object_id = parent_object_id ) \n    for f in folders : \n        path = os . path . normpath ( local_path + f . path ) \n        if not os . path . exists ( path ) : \n            print ( 'Creating folder: {}' . format ( path ) ) \n            if not dry_run : \n                os . makedirs ( path ) \n    files = vault . files ( parent_object_id = parent_object_id ) \n    for f in files : \n        path = os . path . normpath ( local_path + f . path ) \n        if os . path . exists ( path ) : \n            if force : \n                print ( 'Deleting local file (force download): {}' . format ( path ) ) \n                if not dry_run : \n                    os . remove ( path ) \n            else : \n                print ( 'Skipping file (already exists): {}' . format ( path ) ) \n                continue \n        print ( 'Downloading file: {}' . format ( path ) ) \n        if not dry_run : \n            f . download ( path ) "}
{"7357": "\ndef request ( self , method , url , ** kwargs ) : \n    opts = { 'allow_redirects' : 1 , 'auth' : self . _auth , 'data' : { } , 'files' : None , 'headers' : dict ( self . _headers ) , 'params' : { } , 'timeout' : 80 , 'verify' : 1 } \n    raw = kwargs . pop ( 'raw' , 0 ) \n    debug = kwargs . pop ( 'debug' , 0 ) \n    opts . update ( kwargs ) \n    method = method . upper ( ) \n    if opts [ 'files' ] : \n        opts [ 'headers' ] . pop ( 'Content-Type' , None ) \n    else : \n        opts [ 'data' ] = json . dumps ( opts [ 'data' ] ) \n    if not url . startswith ( self . _host ) : \n        url = urljoin ( self . _host , url ) \n    logger . debug ( 'API %s Request: %s' % ( method , url ) ) \n    if debug : \n        self . _log_raw_request ( method , url , ** opts ) \n    try : \n        response = self . _session . request ( method , url , ** opts ) \n    except Exception as e : \n        _handle_request_error ( e ) \n    if 429 == response . status_code : \n        delay = int ( response . headers [ 'retry-after' ] ) + 1 \n        logger . warn ( 'Too many requests. Retrying in {0}s.' . format ( delay ) ) \n        time . sleep ( delay ) \n        return self . request ( method , url , ** kwargs ) \n    if not ( 200 <= response . status_code < 400 ) : \n        _handle_api_error ( response ) \n    if raw or response . status_code in [ 204 , 301 , 302 ] : \n        return response \n    return response . json ( ) "}
{"7365": "\ndef _normalize_tabular_data ( tabular_data , headers , sort = 1 ) : \n    if hasattr ( tabular_data , \"keys\" ) and hasattr ( tabular_data , \"values\" ) : \n        if hasattr ( tabular_data . values , \"__call__\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            rows = list ( izip_longest ( * list ( tabular_data . values ( ) ) ) ) \n        elif hasattr ( tabular_data , \"index\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            vals = tabular_data . values \n            names = tabular_data . index \n            rows = [ [ v ] + list ( row ) for v , row in zip ( names , vals ) ] \n        else : \n            raise ValueError ( \"tabular data doesn't appear to be a dict \" \"or a DataFrame\" ) \n        if headers == \"keys\" : \n            headers = list ( map ( _text_type , keys ) ) \n    else : \n        rows = list ( tabular_data ) \n        if headers == \"keys\" and len ( rows ) > 0 : \n            headers = list ( map ( _text_type , list ( range ( len ( rows [ 0 ] ) ) ) ) ) \n    if headers == \"firstrow\" and len ( rows ) > 0 : \n        headers = list ( map ( _text_type , rows [ 0 ] ) ) \n        rows = rows [ 1 : ] \n    headers = list ( headers ) \n    rows = list ( map ( list , rows ) ) \n    if sort and len ( rows ) > 1 : \n        rows = sorted ( rows , key = lambda x : x [ 0 ] ) \n    if headers and len ( rows ) > 0 : \n        nhs = len ( headers ) \n        ncols = len ( rows [ 0 ] ) \n        if nhs < ncols : \n            headers = [ \"\" ] * ( ncols - nhs ) + headers \n    return rows , headers "}
{"7370": "\ndef migrate ( self , target , follow = 1 , ** kwargs ) : \n    if 'id' not in self or not self [ 'id' ] : \n        raise Exception ( 'No source dataset ID found. ' 'Please instantiate the Dataset ' 'object with an ID.' ) \n    if isinstance ( target , Dataset ) : \n        target_id = target . id \n    else : \n        target_id = target \n    migration = DatasetMigration . create ( source_id = self [ 'id' ] , target_id = target_id , ** kwargs ) \n    if follow : \n        migration . follow ( ) \n    return migration "}
{"7374": "\ndef validate_api_host_url ( url ) : \n    if not url : \n        raise SolveError ( 'No SolveBio API host is set' ) \n    parsed = urlparse ( url ) \n    if parsed . scheme not in [ 'http' , 'https' ] : \n        raise SolveError ( 'Invalid API host: %s. ' 'Missing url scheme (HTTP or HTTPS).' % url ) \n    elif not parsed . netloc : \n        raise SolveError ( 'Invalid API host: %s.' % url ) \n    return 1 "}
{"7377": "\ndef evaluate ( self , data = None , data_type = 'string' , is_list = 0 ) : \n    payload = { 'data' : data , 'expression' : self . expr , 'data_type' : data_type , 'is_list' : is_list } \n    res = self . _client . post ( '/v1/evaluate' , payload ) \n    return res [ 'result' ] "}
{"7381": "\ndef adapter ( data , headers , table_format = None , preserve_whitespace = 0 , ** kwargs ) : \n    keys = ( 'floatfmt' , 'numalign' , 'stralign' , 'showindex' , 'disable_numparse' ) \n    tkwargs = { 'tablefmt' : table_format } \n    tkwargs . update ( filter_dict_by_key ( kwargs , keys ) ) \n    if table_format in supported_markup_formats : \n        tkwargs . update ( numalign = None , stralign = None ) \n    tabulate . PRESERVE_WHITESPACE = preserve_whitespace \n    return iter ( tabulate . tabulate ( data , headers , ** tkwargs ) . split ( '\\n' ) ) "}
{"7382": "\ndef get_user_config_dir ( app_name , app_author , roaming = 1 , force_xdg = 1 ) : \n    if WIN : \n        key = 'APPDATA' if roaming else 'LOCALAPPDATA' \n        folder = os . path . expanduser ( os . environ . get ( key , '~' ) ) \n        return os . path . join ( folder , app_author , app_name ) \n    if MAC and not force_xdg : \n        return os . path . join ( os . path . expanduser ( '~/Library/Application Support' ) , app_name ) \n    return os . path . join ( os . path . expanduser ( os . environ . get ( 'XDG_CONFIG_HOME' , '~/.config' ) ) , _pathify ( app_name ) ) "}
{"7383": "\ndef get_system_config_dirs ( app_name , app_author , force_xdg = 1 ) : \n    if WIN : \n        folder = os . environ . get ( 'PROGRAMDATA' ) \n        return [ os . path . join ( folder , app_author , app_name ) ] \n    if MAC and not force_xdg : \n        return [ os . path . join ( '/Library/Application Support' , app_name ) ] \n    dirs = os . environ . get ( 'XDG_CONFIG_DIRS' , '/etc/xdg' ) \n    paths = [ os . path . expanduser ( x ) for x in dirs . split ( os . pathsep ) ] \n    return [ os . path . join ( d , _pathify ( app_name ) ) for d in paths ] "}
{"7384": "\ndef read_default_config ( self ) : \n    if self . validate : \n        self . default_config = ConfigObj ( configspec = self . default_file , list_values = 0 , _inspec = 1 , encoding = 'utf8' ) \n        valid = self . default_config . validate ( Validator ( ) , copy = 1 , preserve_errors = 1 ) \n        if valid is not 1 : \n            for name , section in valid . items ( ) : \n                if section is 1 : \n                    continue \n                for key , value in section . items ( ) : \n                    if isinstance ( value , ValidateError ) : \n                        raise DefaultConfigValidationError ( 'section [{}], key \"{}\": {}' . format ( name , key , value ) ) \n    elif self . default_file : \n        self . default_config , _ = self . read_config_file ( self . default_file ) \n    self . update ( self . default_config ) "}
{"7389": "\ndef write_default_config ( self , overwrite = 0 ) : \n    destination = self . user_config_file ( ) \n    if not overwrite and os . path . exists ( destination ) : \n        return \n    with io . open ( destination , mode = 'wb' ) as f : \n        self . default_config . write ( f ) "}
{"7390": "\ndef read_config_files ( self , files ) : \n    errors = { } \n    for _file in files : \n        config , valid = self . read_config_file ( _file ) \n        self . update ( config ) \n        if valid is not 1 : \n            errors [ _file ] = valid \n    return errors or 1 "}
{"7393": "\ndef call_in_sequence ( self , cmds , shell = 1 ) : \n    for cmd in cmds : \n        if subprocess . call ( cmd , shell = shell ) == 1 : \n            sys . exit ( 1 ) "}
{"7394": "\ndef apply_options ( self , cmd , options = ( ) ) : \n    for option in ( self . default_cmd_options + options ) : \n        cmd = self . apply_option ( cmd , option , active = getattr ( self , option , 0 ) ) \n    return cmd "}
{"7395": "\ndef apply_option ( self , cmd , option , active = 1 ) : \n    return re . sub ( r'{{{}\\:(?P<option>[^}}]*)}}' . format ( option ) , '\\g<option>' if active else '' , cmd ) "}
{"7396": "\ndef initialize_options ( self ) : \n    self . branch = 'master' \n    self . fix = 0 \n    super ( lint , self ) . initialize_options ( ) "}
{"7405": "\ndef isNum ( self , type ) : \n    if type in ( CKA_CERTIFICATE_TYPE , CKA_CLASS , CKA_KEY_GEN_MECHANISM , CKA_KEY_TYPE , CKA_MODULUS_BITS , CKA_VALUE_BITS , CKA_VALUE_LEN ) : \n        return 1 \n    return 0 "}
{"7406": "\ndef isBool ( self , type ) : \n    if type in ( CKA_ALWAYS_SENSITIVE , CKA_DECRYPT , CKA_DERIVE , CKA_ENCRYPT , CKA_EXTRACTABLE , CKA_HAS_RESET , CKA_LOCAL , CKA_MODIFIABLE , CKA_NEVER_EXTRACTABLE , CKA_PRIVATE , CKA_RESET_ON_INIT , CKA_SECONDARY_AUTH , CKA_SENSITIVE , CKA_SIGN , CKA_SIGN_RECOVER , CKA_TOKEN , CKA_TRUSTED , CKA_UNWRAP , CKA_VERIFY , CKA_VERIFY_RECOVER , CKA_WRAP , CKA_WRAP_WITH_TRUSTED ) : \n        return 1 \n    return 0 "}
{"7410": "\ndef findObjects ( self , template = ( ) ) : \n    t = self . _template2ckattrlist ( template ) \n    result = PyKCS11 . LowLevel . ckobjlist ( 10 ) \n    rv = self . lib . C_FindObjectsInit ( self . session , t ) \n    if rv != CKR_OK : \n        raise PyKCS11Error ( rv ) \n    res = [ ] \n    while 1 : \n        rv = self . lib . C_FindObjects ( self . session , result ) \n        if rv != CKR_OK : \n            raise PyKCS11Error ( rv ) \n        for x in result : \n            a = CK_OBJECT_HANDLE ( self ) \n            a . assign ( x . value ( ) ) \n            res . append ( a ) \n        if len ( result ) == 0 : \n            break \n    rv = self . lib . C_FindObjectsFinal ( self . session ) \n    if rv != CKR_OK : \n        raise PyKCS11Error ( rv ) \n    return res "}
{"7417": "\ndef _handle_single_chunk ( self , event ) : \n    if not event . starts_same_month_as ( self . month ) and not event . repeats ( 'NEVER' ) : \n        return \n    mycount = defaultdict ( list ) \n    r = Repeater ( mycount , self . year , self . month , day = event . l_start_date . day , end_repeat = event . end_repeat , event = event , count_first = 1 , end_on = event . l_end_date . day , num = 1 ) \n    if event . starts_same_month_as ( self . month ) : \n        if not event . ends_same_month_as ( self . month ) : \n            r . end_on = None \n    else : \n        r . day = 1 \n    r . repeat ( ) \n    for k , v in r . count . items ( ) : \n        self . count [ k ] . extend ( v ) "}
{"7432": "\ndef download_verified ( ) : \n    user_obj = store . user ( current_user . email ) \n    user_institutes = user_obj . get ( 'institutes' ) \n    temp_excel_dir = os . path . join ( variants_bp . static_folder , 'verified_folder' ) \n    os . makedirs ( temp_excel_dir , exist_ok = 1 ) \n    written_files = controllers . verified_excel_file ( store , user_institutes , temp_excel_dir ) \n    if written_files : \n        today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n        data = io . BytesIO ( ) \n        with zipfile . ZipFile ( data , mode = 'w' ) as z : \n            for f_name in pathlib . Path ( temp_excel_dir ) . iterdir ( ) : \n                zipfile . ZipFile \n                z . write ( f_name , os . path . basename ( f_name ) ) \n        data . seek ( 0 ) \n        shutil . rmtree ( temp_excel_dir ) \n        return send_file ( data , mimetype = 'application/zip' , as_attachment = 1 , attachment_filename = '_' . join ( [ 'scout' , 'verified_variants' , today ] ) + '.zip' ) \n    else : \n        flash ( \"No verified variants could be exported for user's institutes\" , 'warning' ) \n        return redirect ( request . referrer ) "}
{"7434": "\ndef add_incomplete_penetrance ( genes , alias_genes , hpo_lines ) : \n    LOG . info ( \"Add incomplete penetrance info\" ) \n    for hgnc_symbol in get_incomplete_penetrance_genes ( hpo_lines ) : \n        for hgnc_id in get_correct_ids ( hgnc_symbol , alias_genes ) : \n            genes [ hgnc_id ] [ 'incomplete_penetrance' ] = 1 "}
{"7452": "\ndef gene ( store , hgnc_id ) : \n    res = { 'builds' : { '37' : None , '38' : None } , 'symbol' : None , 'description' : None , 'ensembl_id' : None , 'record' : None } \n    for build in res [ 'builds' ] : \n        record = store . hgnc_gene ( hgnc_id , build = build ) \n        if record : \n            record [ 'position' ] = \"{this[chromosome]}:{this[start]}-{this[end]}\" . format ( this = record ) \n            res [ 'aliases' ] = record [ 'aliases' ] \n            res [ 'hgnc_id' ] = record [ 'hgnc_id' ] \n            res [ 'description' ] = record [ 'description' ] \n            res [ 'builds' ] [ build ] = record \n            res [ 'symbol' ] = record [ 'hgnc_symbol' ] \n            res [ 'description' ] = record [ 'description' ] \n            res [ 'entrez_id' ] = record . get ( 'entrez_id' ) \n            res [ 'pli_score' ] = record . get ( 'pli_score' ) \n            add_gene_links ( record , int ( build ) ) \n            res [ 'omim_id' ] = record . get ( 'omim_id' ) \n            res [ 'incomplete_penetrance' ] = record . get ( 'incomplete_penetrance' , 0 ) \n            res [ 'inheritance_models' ] = record . get ( 'inheritance_models' , [ ] ) \n            for transcript in record [ 'transcripts' ] : \n                transcript [ 'position' ] = ( \"{this[chrom]}:{this[start]}-{this[end]}\" . format ( this = transcript ) ) \n                add_tx_links ( transcript , build ) \n            for phenotype in record . get ( 'phenotypes' , [ ] ) : \n                phenotype [ 'omim_link' ] = omim ( phenotype . get ( 'mim_number' ) ) \n            if not res [ 'record' ] : \n                res [ 'record' ] = record \n    if not any ( res . values ( ) ) : \n        raise ValueError \n    return res "}
{"7453": "\ndef genes_to_json ( store , query ) : \n    gene_query = store . hgnc_genes ( query , search = 1 ) \n    json_terms = [ { 'name' : \"{} | {} ({})\" . format ( gene [ 'hgnc_id' ] , gene [ 'hgnc_symbol' ] , ', ' . join ( gene [ 'aliases' ] ) ) , 'id' : gene [ 'hgnc_id' ] } for gene in gene_query ] \n    return json_terms "}
{"7457": "\ndef sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : \n    skip_count = ( per_page * max ( page - 1 , 0 ) ) \n    more_variants = 1 if variants_query . count ( ) > ( skip_count + per_page ) else 0 \n    genome_build = case_obj . get ( 'genome_build' , '37' ) \n    if genome_build not in [ '37' , '38' ] : \n        genome_build = '37' \n    return { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , } "}
{"7459": "\ndef str_variant ( store , institute_id , case_name , variant_id ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    variant_obj = store . variant ( variant_id ) \n    variant_case ( store , case_obj , variant_obj ) \n    variant_obj [ 'callers' ] = callers ( variant_obj , category = 'str' ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = 1 ) \n    return { 'institute' : institute_obj , 'case' : case_obj , 'variant' : variant_obj , 'overlapping_snvs' : overlapping_snvs , 'manual_rank_options' : MANUAL_RANK_OPTIONS , 'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS } "}
{"7460": "\ndef sv_variant ( store , institute_id , case_name , variant_id = None , variant_obj = None , add_case = 1 , get_overlapping = 1 ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    if not variant_obj : \n        variant_obj = store . variant ( variant_id ) \n    if add_case : \n        variant_case ( store , case_obj , variant_obj ) \n    variant_obj [ 'frequencies' ] = [ ( '1000G' , variant_obj . get ( 'thousand_genomes_frequency' ) ) , ( '1000G (left)' , variant_obj . get ( 'thousand_genomes_frequency_left' ) ) , ( '1000G (right)' , variant_obj . get ( 'thousand_genomes_frequency_right' ) ) , ( 'ClinGen CGH (benign)' , variant_obj . get ( 'clingen_cgh_benign' ) ) , ( 'ClinGen CGH (pathogenic)' , variant_obj . get ( 'clingen_cgh_pathogenic' ) ) , ( 'ClinGen NGI' , variant_obj . get ( 'clingen_ngi' ) ) , ( 'SweGen' , variant_obj . get ( 'swegen' ) ) , ( 'Decipher' , variant_obj . get ( 'decipher' ) ) , ] \n    variant_obj [ 'callers' ] = callers ( variant_obj , category = 'sv' ) \n    overlapping_snvs = [ ] \n    if get_overlapping : \n        overlapping_snvs = ( parse_variant ( store , institute_obj , case_obj , variant ) for variant in store . overlapping ( variant_obj ) ) \n    for gene_obj in variant_obj [ 'genes' ] : \n        if gene_obj . get ( 'common' ) : \n            ensembl_id = gene_obj [ 'common' ] [ 'ensembl_id' ] \n            try : \n                build = int ( gene_obj [ 'common' ] . get ( 'build' , '37' ) ) \n            except Exception : \n                build = 37 \n            gene_obj [ 'ensembl_link' ] = ensembl ( ensembl_id , build = build ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = 1 ) \n    case_clinvars = store . case_to_clinVars ( case_obj . get ( 'display_name' ) ) \n    if variant_id in case_clinvars : \n        variant_obj [ 'clinvar_clinsig' ] = case_clinvars . get ( variant_id ) [ 'clinsig' ] \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return { 'institute' : institute_obj , 'case' : case_obj , 'variant' : variant_obj , 'overlapping_snvs' : overlapping_snvs , 'manual_rank_options' : MANUAL_RANK_OPTIONS , 'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS } "}
{"7461": "\ndef parse_variant ( store , institute_obj , case_obj , variant_obj , update = 0 , genome_build = '37' , get_compounds = 1 ) : \n    has_changed = 0 \n    compounds = variant_obj . get ( 'compounds' , [ ] ) \n    if compounds and get_compounds : \n        if 'not_loaded' not in compounds [ 0 ] : \n            new_compounds = store . update_variant_compounds ( variant_obj ) \n            variant_obj [ 'compounds' ] = new_compounds \n            has_changed = 1 \n        variant_obj [ 'compounds' ] = sorted ( variant_obj [ 'compounds' ] , key = lambda compound : - compound [ 'combined_score' ] ) \n    variant_genes = variant_obj . get ( 'genes' ) \n    if variant_genes is not None : \n        for gene_obj in variant_genes : \n            if not gene_obj [ 'hgnc_id' ] : \n                continue \n            if gene_obj . get ( 'hgnc_symbol' ) is None : \n                hgnc_gene = store . hgnc_gene ( gene_obj [ 'hgnc_id' ] , build = genome_build ) \n                if not hgnc_gene : \n                    continue \n                has_changed = 1 \n                gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] \n    if update and has_changed : \n        variant_obj = store . update_variant ( variant_obj ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = 1 ) \n    if variant_genes : \n        variant_obj . update ( get_predictions ( variant_genes ) ) \n        if variant_obj . get ( 'category' ) == 'cancer' : \n            variant_obj . update ( get_variant_info ( variant_genes ) ) \n    for compound_obj in compounds : \n        compound_obj . update ( get_predictions ( compound_obj . get ( 'genes' , [ ] ) ) ) \n    if isinstance ( variant_obj . get ( 'acmg_classification' ) , int ) : \n        acmg_code = ACMG_MAP [ variant_obj [ 'acmg_classification' ] ] \n        variant_obj [ 'acmg_classification' ] = ACMG_COMPLETE_MAP [ acmg_code ] \n    variant_length = variant_obj . get ( 'length' ) \n    variant_obj [ 'length' ] = { 100000000000 : 'inf' , - 1 : 'n.d.' } . get ( variant_length , variant_length ) \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return variant_obj "}
{"7480": "\ndef cancer_variants ( store , request_args , institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    form = CancerFiltersForm ( request_args ) \n    variants_query = store . variants ( case_obj [ '_id' ] , category = 'cancer' , query = form . data ) . limit ( 50 ) \n    data = dict ( institute = institute_obj , case = case_obj , variants = ( parse_variant ( store , institute_obj , case_obj , variant , update = 1 ) for variant in variants_query ) , form = form , variant_type = request_args . get ( 'variant_type' , 'clinical' ) , ) \n    return data "}
{"7502": "\ndef check_weekday ( year , month , day , reverse = 0 ) : \n    d = date ( year , month , day ) \n    while d . weekday ( ) in ( 5 , 6 ) : \n        if reverse : \n            d -= timedelta ( days = 1 ) \n        else : \n            d += timedelta ( days = 1 ) \n    return d . year , d . month , d . day "}
{"7504": "\ndef add_peddy_information ( config_data ) : \n    ped_info = { } \n    ped_check = { } \n    sex_check = { } \n    relations = [ ] \n    if config_data . get ( 'peddy_ped' ) : \n        file_handle = open ( config_data [ 'peddy_ped' ] , 'r' ) \n        for ind_info in parse_peddy_ped ( file_handle ) : \n            ped_info [ ind_info [ 'sample_id' ] ] = ind_info \n    if config_data . get ( 'peddy_ped_check' ) : \n        file_handle = open ( config_data [ 'peddy_ped_check' ] , 'r' ) \n        for pair_info in parse_peddy_ped_check ( file_handle ) : \n            ped_check [ ( pair_info [ 'sample_a' ] , pair_info [ 'sample_b' ] ) ] = pair_info \n    if config_data . get ( 'peddy_sex_check' ) : \n        file_handle = open ( config_data [ 'peddy_sex_check' ] , 'r' ) \n        for ind_info in parse_peddy_sex_check ( file_handle ) : \n            sex_check [ ind_info [ 'sample_id' ] ] = ind_info \n    if not ped_info : \n        return \n    analysis_inds = { } \n    for ind in config_data [ 'samples' ] : \n        ind_id = ind [ 'sample_id' ] \n        analysis_inds [ ind_id ] = ind \n    for ind_id in analysis_inds : \n        ind = analysis_inds [ ind_id ] \n        if ind_id in ped_info : \n            ind [ 'predicted_ancestry' ] = ped_info [ ind_id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) \n        if ind_id in sex_check : \n            if sex_check [ ind_id ] [ 'error' ] : \n                ind [ 'confirmed_sex' ] = 0 \n            else : \n                ind [ 'confirmed_sex' ] = 1 \n        for parent in [ 'mother' , 'father' ] : \n            if ind [ parent ] != '0' : \n                for pair in ped_check : \n                    if ( ind_id in pair and ind [ parent ] in pair ) : \n                        if ped_check [ pair ] [ 'parent_error' ] : \n                            analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = 0 \n                        else : \n                            if 'confirmed_parent' not in analysis_inds [ ind [ parent ] ] : \n                                analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = 1 "}
{"7511": "\ndef is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if ps_terms : \n            return 1 \n        if pm_terms : \n            if pp_terms : \n                return 1 \n            if len ( pm_terms ) >= 2 : \n                return 1 \n        if len ( pp_terms ) >= 2 : \n            return 1 \n    if ps_terms : \n        if len ( ps_terms ) >= 2 : \n            return 1 \n        if pm_terms : \n            if len ( pm_terms ) >= 3 : \n                return 1 \n            elif len ( pm_terms ) >= 2 : \n                if len ( pp_terms ) >= 2 : \n                    return 1 \n            elif len ( pp_terms ) >= 4 : \n                return 1 \n    return 0 "}
{"7512": "\ndef is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if pm_terms : \n            return 1 \n    if ps_terms : \n        if pm_terms : \n            return 1 \n        if len ( pp_terms ) >= 2 : \n            return 1 \n    if pm_terms : \n        if len ( pm_terms ) >= 3 : \n            return 1 \n        elif len ( pm_terms ) >= 2 : \n            if len ( pp_terms ) >= 2 : \n                return 1 \n        elif len ( pp_terms ) >= 4 : \n            return 1 \n    return 0 "}
{"7513": "\ndef is_likely_benign ( bs_terms , bp_terms ) : \n    if bs_terms : \n        if bp_terms : \n            return 1 \n    if len ( bp_terms ) >= 2 : \n        return 1 \n    return 0 "}
{"7514": "\ndef get_acmg ( acmg_terms ) : \n    prediction = 'uncertain_significance' \n    pvs = 0 \n    ps_terms = [ ] \n    pm_terms = [ ] \n    pp_terms = [ ] \n    ba = 0 \n    bs_terms = [ ] \n    bp_terms = [ ] \n    for term in acmg_terms : \n        if term . startswith ( 'PVS' ) : \n            pvs = 1 \n        elif term . startswith ( 'PS' ) : \n            ps_terms . append ( term ) \n        elif term . startswith ( 'PM' ) : \n            pm_terms . append ( term ) \n        elif term . startswith ( 'PP' ) : \n            pp_terms . append ( term ) \n        elif term . startswith ( 'BA' ) : \n            ba = 1 \n        elif term . startswith ( 'BS' ) : \n            bs_terms . append ( term ) \n        elif term . startswith ( 'BP' ) : \n            bp_terms . append ( term ) \n    pathogenic = is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) \n    likely_pathogenic = is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) \n    benign = is_benign ( ba , bs_terms ) \n    likely_benign = is_likely_benign ( bs_terms , bp_terms ) \n    if ( pathogenic or likely_pathogenic ) : \n        if ( benign or likely_benign ) : \n            prediction = 'uncertain_significance' \n        elif pathogenic : \n            prediction = 'pathogenic' \n        else : \n            prediction = 'likely_pathogenic' \n    else : \n        if benign : \n            prediction = 'benign' \n        if likely_benign : \n            prediction = 'likely_benign' \n    return prediction "}
{"7515": "\ndef add_gene_info ( self , variant_obj , gene_panels = None ) : \n    gene_panels = gene_panels or [ ] \n    variant_obj [ 'has_refseq' ] = 0 \n    extra_info = { } \n    for panel_obj in gene_panels : \n        for gene_info in panel_obj [ 'genes' ] : \n            hgnc_id = gene_info [ 'hgnc_id' ] \n            if hgnc_id not in extra_info : \n                extra_info [ hgnc_id ] = [ ] \n            extra_info [ hgnc_id ] . append ( gene_info ) \n    for variant_gene in variant_obj . get ( 'genes' , [ ] ) : \n        hgnc_id = variant_gene [ 'hgnc_id' ] \n        hgnc_gene = self . hgnc_gene ( hgnc_id ) \n        if not hgnc_gene : \n            continue \n        transcripts_dict = { } \n        for transcript in hgnc_gene . get ( 'transcripts' , [ ] ) : \n            tx_id = transcript [ 'ensembl_transcript_id' ] \n            transcripts_dict [ tx_id ] = transcript \n        hgnc_gene [ 'transcripts_dict' ] = transcripts_dict \n        if hgnc_gene . get ( 'incomplete_penetrance' ) : \n            variant_gene [ 'omim_penetrance' ] = 1 \n        panel_info = extra_info . get ( hgnc_id , [ ] ) \n        disease_associated = set ( ) \n        disease_associated_no_version = set ( ) \n        manual_penetrance = 0 \n        mosaicism = 0 \n        manual_inheritance = set ( ) \n        for gene_info in panel_info : \n            for tx in gene_info . get ( 'disease_associated_transcripts' , [ ] ) : \n                stripped = re . sub ( r'\\.[0-9]' , '' , tx ) \n                disease_associated_no_version . add ( stripped ) \n                disease_associated . add ( tx ) \n            if gene_info . get ( 'reduced_penetrance' ) : \n                manual_penetrance = 1 \n            if gene_info . get ( 'mosaicism' ) : \n                mosaicism = 1 \n            manual_inheritance . update ( gene_info . get ( 'inheritance_models' , [ ] ) ) \n        variant_gene [ 'disease_associated_transcripts' ] = list ( disease_associated ) \n        variant_gene [ 'manual_penetrance' ] = manual_penetrance \n        variant_gene [ 'mosaicism' ] = mosaicism \n        variant_gene [ 'manual_inheritance' ] = list ( manual_inheritance ) \n        for transcript in variant_gene . get ( 'transcripts' , [ ] ) : \n            tx_id = transcript [ 'transcript_id' ] \n            if not tx_id in transcripts_dict : \n                continue \n            hgnc_transcript = transcripts_dict [ tx_id ] \n            if hgnc_transcript . get ( 'is_primary' ) : \n                transcript [ 'is_primary' ] = 1 \n            if not hgnc_transcript . get ( 'refseq_id' ) : \n                continue \n            refseq_id = hgnc_transcript [ 'refseq_id' ] \n            transcript [ 'refseq_id' ] = refseq_id \n            variant_obj [ 'has_refseq' ] = 1 \n            if refseq_id in disease_associated_no_version : \n                transcript [ 'is_disease_associated' ] = 1 \n            transcript [ 'refseq_identifiers' ] = hgnc_transcript . get ( 'refseq_identifiers' , [ ] ) \n        variant_gene [ 'common' ] = hgnc_gene \n        variant_gene [ 'disease_terms' ] = self . disease_terms ( hgnc_id ) \n    return variant_obj "}
{"7517": "\ndef sanger_variants ( self , institute_id = None , case_id = None ) : \n    query = { 'validation' : { '$exists' : 1 } } \n    if institute_id : \n        query [ 'institute_id' ] = institute_id \n    if case_id : \n        query [ 'case_id' ] = case_id \n    return self . variant_collection . find ( query ) "}
{"7521": "\ndef get_causatives ( self , institute_id , case_id = None ) : \n    causatives = [ ] \n    if case_id : \n        case_obj = self . case_collection . find_one ( { \"_id\" : case_id } ) \n        causatives = [ causative for causative in case_obj [ 'causatives' ] ] \n    elif institute_id : \n        query = self . case_collection . aggregate ( [ { '$match' : { 'collaborators' : institute_id , 'causatives' : { '$exists' : 1 } } } , { '$unwind' : '$causatives' } , { '$group' : { '_id' : '$causatives' } } ] ) \n        causatives = [ item [ '_id' ] for item in query ] \n    return causatives "}
{"7526": "\ndef evaluated_variants ( self , case_id ) : \n    query = { '$and' : [ { 'case_id' : case_id } , { '$or' : [ { 'acmg_classification' : { '$exists' : 1 } } , { 'manual_rank' : { '$exists' : 1 } } , { 'dismiss_variant' : { '$exists' : 1 } } , ] } ] , } \n    variants = { } \n    for var in self . variant_collection . find ( query ) : \n        variants [ var [ 'variant_id' ] ] = self . add_gene_info ( var ) \n    event_query = { '$and' : [ { 'case' : case_id } , { 'category' : 'variant' } , { 'verb' : 'comment' } , ] } \n    comment_variants = { event [ 'variant_id' ] for event in self . event_collection . find ( event_query ) } \n    for var_id in comment_variants : \n        if var_id in variants : \n            continue \n        variant_obj = self . variant ( var_id , case_id = case_id ) \n        if not variant_obj : \n            continue \n        variant_obj [ 'is_commented' ] = 1 \n        variants [ var_id ] = variant_obj \n    return variants . values ( ) "}
{"7527": "\ndef get_region_vcf ( self , case_obj , chrom = None , start = None , end = None , gene_obj = None , variant_type = 'clinical' , category = 'snv' , rank_threshold = None ) : \n    rank_threshold = rank_threshold or - 100 \n    variant_file = None \n    if variant_type == 'clinical' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv' ) \n        elif category == 'sv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) \n        elif category == 'str' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) \n    elif variant_type == 'research' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) \n        elif category == 'sv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) \n    if not variant_file : \n        raise SyntaxError ( \"Vcf file does not seem to exist\" ) \n    vcf_obj = VCF ( variant_file ) \n    region = \"\" \n    if gene_obj : \n        chrom = gene_obj [ 'chromosome' ] \n        start = gene_obj [ 'start' ] \n        end = gene_obj [ 'end' ] \n    if chrom : \n        if ( start and end ) : \n            region = \"{0}:{1}-{2}\" . format ( chrom , start , end ) \n        else : \n            region = \"{0}\" . format ( chrom ) \n    else : \n        rank_threshold = rank_threshold or 5 \n    with tempfile . NamedTemporaryFile ( mode = 'w' , delete = 0 ) as temp : \n        file_name = str ( pathlib . Path ( temp . name ) ) \n        for header_line in vcf_obj . raw_header . split ( '\\n' ) : \n            if len ( header_line ) > 3 : \n                temp . write ( header_line + '\\n' ) \n        for variant in vcf_obj ( region ) : \n            temp . write ( str ( variant ) ) \n    return file_name "}
{"7533": "\ndef load_transcripts ( adapter , transcripts_lines = None , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    if transcripts_lines is None : \n        transcripts_lines = fetch_ensembl_transcripts ( build = build ) \n    transcripts_dict = parse_transcripts ( transcripts_lines ) \n    for ens_tx_id in list ( transcripts_dict ) : \n        parsed_tx = transcripts_dict [ ens_tx_id ] \n        ens_gene_id = parsed_tx [ 'ensembl_gene_id' ] \n        gene_obj = ensembl_genes . get ( ens_gene_id ) \n        if not gene_obj : \n            transcripts_dict . pop ( ens_tx_id ) \n            LOG . debug ( \"Gene %s does not exist in build %s\" , ens_gene_id , build ) \n            continue \n        parsed_tx [ 'hgnc_id' ] = gene_obj [ 'hgnc_id' ] \n        parsed_tx [ 'primary_transcripts' ] = set ( gene_obj . get ( 'primary_transcripts' , [ ] ) ) \n    ref_seq_transcripts = 0 \n    nr_primary_transcripts = 0 \n    nr_transcripts = len ( transcripts_dict ) \n    transcript_objs = [ ] \n    with progressbar ( transcripts_dict . values ( ) , label = \"Building transcripts\" , length = nr_transcripts ) as bar : \n        for tx_data in bar : \n            tx_data [ 'is_primary' ] = 0 \n            primary_transcripts = tx_data [ 'primary_transcripts' ] \n            refseq_identifier = None \n            refseq_identifiers = [ ] \n            for category in TRANSCRIPT_CATEGORIES : \n                identifiers = tx_data [ category ] \n                if not identifiers : \n                    continue \n                for refseq_id in identifiers : \n                    refseq_identifiers . append ( refseq_id ) \n                    ref_seq_transcripts += 1 \n                    if refseq_id in primary_transcripts : \n                        refseq_identifier = refseq_id \n                        tx_data [ 'is_primary' ] = 1 \n                        nr_primary_transcripts += 1 \n                    if not refseq_identifier : \n                        refseq_identifier = refseq_id \n            if refseq_identifier : \n                tx_data [ 'refseq_id' ] = refseq_identifier \n            if refseq_identifiers : \n                tx_data [ 'refseq_identifiers' ] = refseq_identifiers \n            tx_obj = build_transcript ( tx_data , build ) \n            transcript_objs . append ( tx_obj ) \n    LOG . info ( \"Loading transcripts...\" ) \n    if len ( transcript_objs ) > 0 : \n        adapter . load_transcript_bulk ( transcript_objs ) \n    LOG . info ( 'Number of transcripts in build %s: %s' , build , nr_transcripts ) \n    LOG . info ( 'Number of transcripts with refseq identifier: %s' , ref_seq_transcripts ) \n    LOG . info ( 'Number of primary transcripts: %s' , nr_primary_transcripts ) \n    return transcript_objs "}
{"7543": "\ndef parse_matches ( patient_id , match_objs ) : \n    LOG . info ( 'Parsing MatchMaker matches for patient {}' . format ( patient_id ) ) \n    parsed_matches = [ ] \n    for match_obj in match_objs : \n        milliseconds_date = match_obj [ 'created' ] [ '$date' ] \n        mdate = datetime . datetime . fromtimestamp ( milliseconds_date / 1000.0 ) \n        match_type = 'external' \n        matching_patients = [ ] \n        parsed_match = { 'match_oid' : match_obj [ '_id' ] [ '$oid' ] , 'match_date' : mdate } \n        if match_obj [ 'data' ] [ 'patient' ] [ 'id' ] == patient_id : \n            match_results = match_obj [ 'results' ] \n            for node_result in match_results : \n                if match_obj [ 'match_type' ] == 'internal' : \n                    match_type = 'internal' \n                for patient in node_result [ 'patients' ] : \n                    match_patient = { 'patient_id' : patient [ 'patient' ] [ 'id' ] , 'score' : patient [ 'score' ] , 'patient' : patient [ 'patient' ] , 'node' : node_result [ 'node' ] } \n                    matching_patients . append ( match_patient ) \n        else : \n            m_patient = match_obj [ 'data' ] [ 'patient' ] \n            contact_institution = m_patient [ 'contact' ] . get ( 'institution' ) \n            if contact_institution and 'Scout software user' in contact_institution : \n                match_type = 'internal' \n            score = None \n            for res in match_obj [ 'results' ] : \n                for patient in res [ 'patients' ] : \n                    LOG . info ( 'Looping in else, patient:{}' . format ( patient [ 'patient' ] [ 'id' ] ) ) \n                    if patient [ 'patient' ] [ 'id' ] == patient_id : \n                        score = patient [ 'score' ] \n                        match_patient = { 'patient_id' : m_patient [ 'id' ] , 'score' : score , 'patient' : m_patient , 'node' : res [ 'node' ] } \n                        matching_patients . append ( match_patient ) \n        parsed_match [ 'match_type' ] = match_type \n        parsed_match [ 'patients' ] = matching_patients \n        parsed_matches . append ( parsed_match ) \n    parsed_matches = sorted ( parsed_matches , key = lambda k : k [ 'match_date' ] , reverse = 1 ) \n    return parsed_matches "}
{"7544": "\ndef cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n    else : \n        models = adapter . cases ( collaborator = institute , name_query = display_name ) \n        models = [ case_obj for case_obj in models ] \n    if not models : \n        LOG . info ( \"No cases could be found\" ) \n        return \n    header = [ 'case_id' , 'display_name' , 'institute' ] \n    if variants_treshold : \n        LOG . info ( \"Only show cases with more than %s variants\" , variants_treshold ) \n        nr_variants = 1 \n    if nr_variants : \n        LOG . info ( \"Displaying number of variants for each case\" ) \n        header . append ( 'clinical' ) \n        header . append ( 'research' ) \n    click . echo ( \"#\" + '\\t' . join ( header ) ) \n    for model in models : \n        output_str = \"{:<12}\\t{:<12}\\t{:<12}\" \n        output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] \n        if nr_variants : \n            output_str += \"\\t{:<12}\\t{:<12}\" \n            nr_clinical = 0 \n            nr_research = 0 \n            variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) \n            i = 0 \n            for i , var in enumerate ( variants , 1 ) : \n                if var [ 'variant_type' ] == 'clinical' : \n                    nr_clinical += 1 \n                else : \n                    nr_research += 1 \n            output_values . extend ( [ nr_clinical , nr_research ] ) \n            if variants_treshold and i < variants_treshold : \n                LOG . debug ( \"Case %s had to few variants, skipping\" , model [ '_id' ] ) \n                continue \n        click . echo ( output_str . format ( * output_values ) ) "}
{"7546": "\ndef login ( ) : \n    if 'next' in request . args : \n        session [ 'next_url' ] = request . args [ 'next' ] \n    if current_app . config . get ( 'GOOGLE' ) : \n        callback_url = url_for ( '.authorized' , _external = 1 ) \n        return google . authorize ( callback = callback_url ) \n    user_email = request . args . get ( 'email' ) \n    user_obj = store . user ( user_email ) \n    if user_obj is None : \n        flash ( \"email not whitelisted: {}\" . format ( user_email ) , 'warning' ) \n        return redirect ( url_for ( 'public.index' ) ) \n    return perform_login ( user_obj ) "}
{"7550": "\ndef events ( self , institute , case = None , variant_id = None , level = None , comments = 0 , panel = None ) : \n    query = { } \n    if variant_id : \n        if comments : \n            LOG . debug ( \"Fetching all comments for institute {0} case {1} variant {2}\" . format ( institute [ '_id' ] , case [ '_id' ] , variant_id ) ) \n            query = { '$or' : [ { 'category' : 'variant' , 'variant_id' : variant_id , 'verb' : 'comment' , 'level' : 'global' } , { 'category' : 'variant' , 'variant_id' : variant_id , 'institute' : institute [ '_id' ] , 'case' : case [ '_id' ] , 'verb' : 'comment' , 'level' : 'specific' } ] } \n        else : \n            query [ 'institute' ] = institute [ '_id' ] \n            query [ 'category' ] = 'variant' \n            query [ 'variant_id' ] = variant_id \n            query [ 'case' ] = case [ '_id' ] \n    else : \n        query [ 'institute' ] = institute [ '_id' ] \n        if panel : \n            query [ 'panel' ] = panel \n        else : \n            query [ 'category' ] = 'case' \n            if case : \n                query [ 'case' ] = case [ '_id' ] \n            if comments : \n                query [ 'verb' ] = 'comment' \n    return self . event_collection . find ( query ) . sort ( 'created_at' , pymongo . DESCENDING ) "}
{"7552": "\ndef add_phenotype ( self , institute , case , user , link , hpo_term = None , omim_term = None , is_group = 0 ) : \n    hpo_results = [ ] \n    try : \n        if hpo_term : \n            hpo_results = [ hpo_term ] \n        elif omim_term : \n            LOG . debug ( \"Fetching info for mim term {0}\" . format ( omim_term ) ) \n            disease_obj = self . disease_term ( omim_term ) \n            if disease_obj : \n                for hpo_term in disease_obj . get ( 'hpo_terms' , [ ] ) : \n                    hpo_results . append ( hpo_term ) \n        else : \n            raise ValueError ( 'Must supply either hpo or omim term' ) \n    except ValueError as e : \n        raise e \n    existing_terms = set ( term [ 'phenotype_id' ] for term in case . get ( 'phenotype_terms' , [ ] ) ) \n    updated_case = case \n    phenotype_terms = [ ] \n    for hpo_term in hpo_results : \n        LOG . debug ( \"Fetching info for hpo term {0}\" . format ( hpo_term ) ) \n        hpo_obj = self . hpo_term ( hpo_term ) \n        if hpo_obj is None : \n            raise ValueError ( \"Hpo term: %s does not exist in database\" % hpo_term ) \n        phenotype_id = hpo_obj [ '_id' ] \n        description = hpo_obj [ 'description' ] \n        if phenotype_id not in existing_terms : \n            phenotype_term = dict ( phenotype_id = phenotype_id , feature = description ) \n            phenotype_terms . append ( phenotype_term ) \n            LOG . info ( \"Creating event for adding phenotype term for case\" \" {0}\" . format ( case [ 'display_name' ] ) ) \n            self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'add_phenotype' , subject = case [ 'display_name' ] , content = phenotype_id ) \n        if is_group : \n            updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$addToSet' : { 'phenotype_terms' : { '$each' : phenotype_terms } , 'phenotype_groups' : { '$each' : phenotype_terms } , } , } , return_document = pymongo . ReturnDocument . AFTER ) \n        else : \n            updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$addToSet' : { 'phenotype_terms' : { '$each' : phenotype_terms } , } , } , return_document = pymongo . ReturnDocument . AFTER ) \n    LOG . debug ( \"Case updated\" ) \n    return updated_case "}
{"7553": "\ndef remove_phenotype ( self , institute , case , user , link , phenotype_id , is_group = 0 ) : \n    LOG . info ( \"Removing HPO term from case {0}\" . format ( case [ 'display_name' ] ) ) \n    if is_group : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$pull' : { 'phenotype_terms' : { 'phenotype_id' : phenotype_id } , 'phenotype_groups' : { 'phenotype_id' : phenotype_id } , } , } , return_document = pymongo . ReturnDocument . AFTER ) \n    else : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$pull' : { 'phenotype_terms' : { 'phenotype_id' : phenotype_id } , } , } , return_document = pymongo . ReturnDocument . AFTER ) \n    LOG . info ( \"Creating event for removing phenotype term {0}\" \" from case {1}\" . format ( phenotype_id , case [ 'display_name' ] ) ) \n    self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'remove_phenotype' , subject = case [ 'display_name' ] ) \n    LOG . debug ( \"Case updated\" ) \n    return updated_case "}
{"7556": "\ndef check_coordinates ( chromosome , pos , coordinates ) : \n    chrom_match = CHR_PATTERN . match ( chromosome ) \n    chrom = chrom_match . group ( 2 ) \n    if chrom != coordinates [ 'chrom' ] : \n        return 0 \n    if ( pos >= coordinates [ 'start' ] and pos <= coordinates [ 'end' ] ) : \n        return 1 \n    return 0 "}
{"7561": "\ndef all_month_events ( self , year , month , category = None , tag = None , loc = 0 , cncl = 0 ) : \n    kwargs = self . _get_kwargs ( category , tag ) \n    ym_first , ym_last = self . get_first_and_last ( year , month ) \n    pref = [ ] \n    if loc : \n        pref . append ( \"location\" ) \n    if cncl : \n        pref . append ( \"cancellations\" ) \n    r = Q ( repeat = \"YEARLY\" ) \n    dstart_mo = Q ( start_date__month = month ) \n    dend_mo = Q ( end_date__month = month ) \n    dstart_yr = Q ( start_date__year = year ) \n    dend_yr = Q ( end_date__year = year ) \n    return self . model . objects . filter ( r & ( dstart_mo | dend_mo ) | ( ~ Q ( repeat = \"NEVER\" ) ) | ( ( dstart_yr | dend_yr ) & ( dstart_mo | dend_yr ) ) , Q ( end_repeat = None ) | Q ( end_repeat__gte = ym_first ) , start_date__lte = ym_last ) . filter ( ** kwargs ) . prefetch_related ( * pref ) . order_by ( 'start_date' ) . distinct ( ) "}
{"7569": "\ndef migrate_case ( adapter : MongoAdapter , scout_case : dict , archive_data : dict ) : \n    collaborators = list ( set ( scout_case [ 'collaborators' ] + archive_data [ 'collaborators' ] ) ) \n    if collaborators != scout_case [ 'collaborators' ] : \n        LOG . info ( f\"set collaborators: {', '.join(collaborators)}\" ) \n        scout_case [ 'collaborators' ] = collaborators \n    if len ( scout_case . get ( 'assignees' , [ ] ) ) == 0 : \n        scout_user = adapter . user ( archive_data [ 'assignee' ] ) \n        if scout_user : \n            scout_case [ 'assignees' ] = [ archive_data [ 'assignee' ] ] \n        else : \n            LOG . warning ( f\"{archive_data['assignee']}: unable to find assigned user\" ) \n    for key in [ 'suspects' , 'causatives' ] : \n        scout_case [ key ] = scout_case . get ( key , [ ] ) \n        for archive_variant in archive_data [ key ] : \n            variant_id = get_variantid ( archive_variant , scout_case [ '_id' ] ) \n            scout_variant = adapter . variant ( variant_id ) \n            if scout_variant : \n                if scout_variant [ '_id' ] in scout_case [ key ] : \n                    LOG . info ( f\"{scout_variant['_id']}: variant already in {key}\" ) \n                else : \n                    LOG . info ( f\"{scout_variant['_id']}: add to {key}\" ) \n                    scout_variant [ key ] . append ( scout_variant [ '_id' ] ) \n            else : \n                LOG . warning ( f\"{scout_variant['_id']}: unable to find variant ({key})\" ) \n                scout_variant [ key ] . append ( variant_id ) \n    if not scout_case . get ( 'synopsis' ) : \n        scout_case [ 'synopsis' ] = archive_data [ 'synopsis' ] \n    scout_case [ 'is_migrated' ] = 1 \n    adapter . case_collection . find_one_and_replace ( { '_id' : scout_case [ '_id' ] } , scout_case , ) \n    scout_institute = adapter . institute ( scout_case [ 'owner' ] ) \n    scout_user = adapter . user ( 'mans.magnusson@scilifelab.se' ) \n    for key in [ 'phenotype_terms' , 'phenotype_groups' ] : \n        for archive_term in archive_data [ key ] : \n            adapter . add_phenotype ( institute = scout_institute , case = scout_case , user = scout_user , link = f\"/{scout_case['owner']}/{scout_case['display_name']}\" , hpo_term = archive_term [ 'phenotype_id' ] , is_group = key == 'phenotype_groups' , ) "}
{"7571": "\ndef research ( context , case_id , institute , force ) : \n    LOG . info ( \"Running scout load research\" ) \n    adapter = context . obj [ 'adapter' ] \n    if case_id : \n        if not institute : \n            splitted_case = case_id . split ( '-' ) \n            if len ( splitted_case ) > 1 : \n                institute_obj = adapter . institute ( splitted_case [ 0 ] ) \n                if institute_obj : \n                    institute = institute_obj [ '_id' ] \n                    case_id = splitted_case [ 1 ] \n        case_obj = adapter . case ( institute_id = institute , case_id = case_id ) \n        if case_obj is None : \n            LOG . warning ( \"No matching case found\" ) \n            context . abort ( ) \n        else : \n            case_objs = [ case_obj ] \n    else : \n        case_objs = adapter . cases ( research_requested = 1 ) \n    default_threshold = 8 \n    files = 0 \n    for case_obj in case_objs : \n        if force or case_obj [ 'research_requested' ] : \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) : \n                files = 1 \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'snv' ) \n                LOG . info ( \"Load research SNV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'snv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : \n                files = 1 \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'sv' ) \n                LOG . info ( \"Load research SV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'sv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) : \n                files = 1 \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'cancer' ) \n                LOG . info ( \"Load research cancer for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'cancer' , rank_threshold = default_threshold , ) \n            if not files : \n                LOG . warning ( \"No research files found for case %s\" , case_id ) \n                context . abort ( ) \n            case_obj [ 'is_research' ] = 1 \n            case_obj [ 'research_requested' ] = 0 \n            adapter . update_case ( case_obj ) \n        else : \n            LOG . warn ( \"research not requested, use '--force'\" ) "}
{"7572": "\ndef load_hgnc_genes ( adapter , genes = None , ensembl_lines = None , hgnc_lines = None , exac_lines = None , mim2gene_lines = None , genemap_lines = None , hpo_lines = None , build = '37' , omim_api_key = '' ) : \n    gene_objects = list ( ) \n    if not genes : \n        if ensembl_lines is None : \n            ensembl_lines = fetch_ensembl_genes ( build = build ) \n        hgnc_lines = hgnc_lines or fetch_hgnc ( ) \n        exac_lines = exac_lines or fetch_exac_constraint ( ) \n        if not ( mim2gene_lines and genemap_lines ) : \n            if not omim_api_key : \n                raise SyntaxError ( \"Need to provide omim api key\" ) \n            mim_files = fetch_mim_files ( omim_api_key , mim2genes = 1 , genemap2 = 1 ) \n            mim2gene_lines = mim_files [ 'mim2genes' ] \n            genemap_lines = mim_files [ 'genemap2' ] \n        if not hpo_lines : \n            hpo_files = fetch_hpo_files ( hpogenes = 1 ) \n            hpo_lines = hpo_files [ 'hpogenes' ] \n        genes = link_genes ( ensembl_lines = ensembl_lines , hgnc_lines = hgnc_lines , exac_lines = exac_lines , mim2gene_lines = mim2gene_lines , genemap_lines = genemap_lines , hpo_lines = hpo_lines ) \n    non_existing = 0 \n    nr_genes = len ( genes ) \n    with progressbar ( genes . values ( ) , label = \"Building genes\" , length = nr_genes ) as bar : \n        for gene_data in bar : \n            if not gene_data . get ( 'chromosome' ) : \n                LOG . debug ( \"skipping gene: %s. No coordinates found\" , gene_data . get ( 'hgnc_symbol' , '?' ) ) \n                non_existing += 1 \n                continue \n            gene_obj = build_hgnc_gene ( gene_data , build = build ) \n            gene_objects . append ( gene_obj ) \n    LOG . info ( \"Loading genes build %s\" , build ) \n    adapter . load_hgnc_bulk ( gene_objects ) \n    LOG . info ( \"Loading done. %s genes loaded\" , len ( gene_objects ) ) \n    LOG . info ( \"Nr of genes without coordinates in build %s: %s\" , build , non_existing ) \n    return gene_objects "}
{"7574": "\ndef create_app ( config_file = None , config = None ) : \n    app = Flask ( __name__ ) \n    app . config . from_pyfile ( 'config.py' ) \n    app . jinja_env . add_extension ( 'jinja2.ext.do' ) \n    if config : \n        app . config . update ( config ) \n    if config_file : \n        app . config . from_pyfile ( config_file ) \n    app . mme_nodes = mme_nodes ( app . config . get ( 'MME_URL' ) , app . config . get ( 'MME_TOKEN' ) ) \n    app . config [ \"JSON_SORT_KEYS\" ] = 0 \n    current_log_level = logger . getEffectiveLevel ( ) \n    coloredlogs . install ( level = 'DEBUG' if app . debug else current_log_level ) \n    configure_extensions ( app ) \n    register_blueprints ( app ) \n    register_filters ( app ) \n    if not ( app . debug or app . testing ) and app . config . get ( 'MAIL_USERNAME' ) : \n        configure_email_logging ( app ) \n    \n    @ app . before_request \n    def check_user ( ) : \n        if not app . config . get ( 'LOGIN_DISABLED' ) and request . endpoint : \n            static_endpoint = 'static' in request . endpoint or 'report' in request . endpoint \n            public_endpoint = getattr ( app . view_functions [ request . endpoint ] , 'is_public' , 0 ) \n            relevant_endpoint = not ( static_endpoint or public_endpoint ) \n            if relevant_endpoint and not current_user . is_authenticated : \n                next_url = \"{}?{}\" . format ( request . path , request . query_string . decode ( ) ) \n                login_url = url_for ( 'login.login' , next = next_url ) \n                return redirect ( login_url ) \n    return app "}
{"7577": "\ndef configure_coverage ( app ) : \n    app . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = 1 if app . debug else 0 \n    if chanjo_api : \n        chanjo_api . init_app ( app ) \n        configure_template_filters ( app ) \n        app . register_blueprint ( report_bp , url_prefix = '/reports' ) \n    babel = Babel ( app ) \n    \n    @ babel . localeselector \n    def get_locale ( ) : \n        accept_languages = current_app . config . get ( 'ACCEPT_LANGUAGES' , [ 'en' ] ) \n        session_language = request . args . get ( 'lang' ) \n        if session_language in accept_languages : \n            current_app . logger . info ( \"using session language: %s\" , session_language ) \n            return session_language \n        user_language = current_app . config . get ( 'REPORT_LANGUAGE' ) \n        if user_language : \n            return user_language \n        return request . accept_languages . best_match ( accept_languages ) "}
{"7579": "\ndef build_panel ( panel_info , adapter ) : \n    panel_name = panel_info . get ( 'panel_id' , panel_info . get ( 'panel_name' ) ) \n    if not panel_name : \n        raise KeyError ( \"Panel has to have a id\" ) \n    panel_obj = dict ( panel_name = panel_name ) \n    LOG . info ( \"Building panel with name: {0}\" . format ( panel_name ) ) \n    try : \n        institute_id = panel_info [ 'institute' ] \n    except KeyError as err : \n        raise KeyError ( \"Panel has to have a institute\" ) \n    if adapter . institute ( institute_id ) is None : \n        raise IntegrityError ( \"Institute %s could not be found\" % institute_id ) \n    panel_obj [ 'institute' ] = panel_info [ 'institute' ] \n    panel_obj [ 'version' ] = float ( panel_info [ 'version' ] ) \n    try : \n        panel_obj [ 'date' ] = panel_info [ 'date' ] \n    except KeyError as err : \n        raise KeyError ( \"Panel has to have a date\" ) \n    panel_obj [ 'display_name' ] = panel_info . get ( 'display_name' , panel_obj [ 'panel_name' ] ) \n    gene_objs = [ ] \n    fail = 0 \n    for gene_info in panel_info . get ( 'genes' , [ ] ) : \n        try : \n            gene_obj = build_gene ( gene_info , adapter ) \n            gene_objs . append ( gene_obj ) \n        except IntegrityError as err : \n            LOG . warning ( err ) \n            fail = 1 \n    if fail : \n        raise IntegrityError ( \"Some genes did not exist in database. Please see log messages.\" ) \n    panel_obj [ 'genes' ] = gene_objs \n    return panel_obj "}
{"7589": "\ndef demo ( context ) : \n    LOG . info ( \"Running scout setup demo\" ) \n    institute_name = context . obj [ 'institute_name' ] \n    user_name = context . obj [ 'user_name' ] \n    user_mail = context . obj [ 'user_mail' ] \n    adapter = context . obj [ 'adapter' ] \n    LOG . info ( \"Setting up database %s\" , context . obj [ 'mongodb' ] ) \n    setup_scout ( adapter = adapter , institute_id = institute_name , user_name = user_name , user_mail = user_mail , demo = 1 ) "}
{"7595": "\ndef update_institute ( self , internal_id , sanger_recipient = None , coverage_cutoff = None , frequency_cutoff = None , display_name = None , remove_sanger = None , phenotype_groups = None , group_abbreviations = None , add_groups = None ) : \n    add_groups = add_groups or 0 \n    institute_obj = self . institute ( internal_id ) \n    if not institute_obj : \n        raise IntegrityError ( \"Institute {} does not exist in database\" . format ( internal_id ) ) \n    updates = { } \n    updated_institute = institute_obj \n    if sanger_recipient : \n        user_obj = self . user ( sanger_recipient ) \n        if not user_obj : \n            raise IntegrityError ( \"user {} does not exist in database\" . format ( sanger_recipient ) ) \n        LOG . info ( \"Updating sanger recipients for institute: {0} with {1}\" . format ( internal_id , sanger_recipient ) ) \n        updates [ '$push' ] = { 'sanger_recipients' : remove_sanger } \n    if remove_sanger : \n        LOG . info ( \"Removing sanger recipient {0} from institute: {1}\" . format ( remove_sanger , internal_id ) ) \n        updates [ '$pull' ] = { 'sanger_recipients' : remove_sanger } \n    if coverage_cutoff : \n        LOG . info ( \"Updating coverage cutoff for institute: {0} to {1}\" . format ( internal_id , coverage_cutoff ) ) \n        updates [ '$set' ] = { 'coverage_cutoff' : coverage_cutoff } \n    if frequency_cutoff : \n        LOG . info ( \"Updating frequency cutoff for institute: {0} to {1}\" . format ( internal_id , frequency_cutoff ) ) \n        if not '$set' in updates : \n            updates [ '$set' ] = { } \n        updates [ '$set' ] = { 'frequency_cutoff' : frequency_cutoff } \n    if display_name : \n        LOG . info ( \"Updating display name for institute: {0} to {1}\" . format ( internal_id , display_name ) ) \n        if not '$set' in updates : \n            updates [ '$set' ] = { } \n        updates [ '$set' ] = { 'display_name' : display_name } \n    if phenotype_groups : \n        if group_abbreviations : \n            group_abbreviations = list ( group_abbreviations ) \n        existing_groups = { } \n        if add_groups : \n            existing_groups = institute_obj . get ( 'phenotype_groups' , PHENOTYPE_GROUPS ) \n        for i , hpo_term in enumerate ( phenotype_groups ) : \n            hpo_obj = self . hpo_term ( hpo_term ) \n            if not hpo_obj : \n                raise IntegrityError ( \"Term {} does not exist\" . format ( hpo_term ) ) \n            hpo_id = hpo_obj [ 'hpo_id' ] \n            description = hpo_obj [ 'description' ] \n            abbreviation = None \n            if group_abbreviations : \n                abbreviation = group_abbreviations [ i ] \n            existing_groups [ hpo_term ] = { 'name' : description , 'abbr' : abbreviation } \n        updates [ '$set' ] = { 'phenotype_groups' : existing_groups } \n    if updates : \n        if not '$set' in updates : \n            updates [ '$set' ] = { } \n        updates [ '$set' ] [ 'updated_at' ] = datetime . now ( ) \n        updated_institute = self . institute_collection . find_one_and_update ( { '_id' : internal_id } , updates , return_document = pymongo . ReturnDocument . AFTER ) \n        LOG . info ( \"Institute updated\" ) \n    return updated_institute "}
{"7597": "\ndef match_date ( date ) : \n    date_pattern = re . compile ( \"^(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])\" ) \n    if re . match ( date_pattern , date ) : \n        return 1 \n    return 0 "}
{"7602": "\ndef check_connection ( host = 'localhost' , port = 27017 , username = None , password = None , authdb = None , max_delay = 1 ) : \n    if username and password : \n        uri = ( \"mongodb://{}:{}@{}:{}/{}\" . format ( quote_plus ( username ) , quote_plus ( password ) , host , port , authdb ) ) \n        log_uri = ( \"mongodb://{}:****@{}:{}/{}\" . format ( quote_plus ( username ) , host , port , authdb ) ) \n    else : \n        log_uri = uri = \"mongodb://%s:%s\" % ( host , port ) \n    LOG . info ( \"Test connection with uri: %s\" , log_uri ) \n    client = MongoClient ( uri , serverSelectionTimeoutMS = max_delay ) \n    try : \n        client . server_info ( ) \n    except ( ServerSelectionTimeoutError , OperationFailure ) as err : \n        LOG . warning ( err ) \n        return 0 \n    return 1 "}
{"7604": "\ndef load_delivery_report ( adapter : MongoAdapter , report_path : str , case_id : str , update : bool = 0 ) : \n    case_obj = adapter . case ( case_id = case_id , ) \n    if case_obj is None : \n        raise DataNotFoundError ( \"no case found\" ) \n    if not case_obj . get ( 'delivery_report' ) : \n        _put_report_in_case_root ( case_obj , report_path ) \n    else : \n        if update : \n            _put_report_in_case_root ( case_obj , report_path ) \n        else : \n            raise IntegrityError ( 'Existing delivery report found, use update = True to ' 'overwrite' ) \n    logger . info ( 'Saving report for case {} in database' . format ( case_obj [ '_id' ] ) ) \n    return adapter . replace_case ( case_obj ) "}
{"7614": "\ndef update_clinvar_id ( self , clinvar_id , submission_id ) : \n    updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$set' : { 'clinvar_subm_id' : clinvar_id , 'updated_at' : datetime . now ( ) } } , upsert = 1 , return_document = pymongo . ReturnDocument . AFTER ) \n    return updated_submission "}
{"7616": "\ndef add_to_submission ( self , submission_id , submission_objects ) : \n    LOG . info ( \"Adding new variants and case data to clinvar submission '%s'\" , submission_id ) \n    for var_obj in submission_objects [ 0 ] : \n        try : \n            result = self . clinvar_collection . insert_one ( var_obj ) \n            self . clinvar_submission_collection . update_one ( { '_id' : submission_id } , { '$push' : { 'variant_data' : str ( result . inserted_id ) } } , upsert = 1 ) \n        except pymongo . errors . DuplicateKeyError : \n            LOG . error ( \"Attepted to insert a clinvar variant which is already in DB!\" ) \n    if submission_objects [ 1 ] : \n        for case_obj in submission_objects [ 1 ] : \n            try : \n                result = self . clinvar_collection . insert_one ( case_obj ) \n                self . clinvar_submission_collection . update_one ( { '_id' : submission_id } , { '$push' : { 'case_data' : str ( result . inserted_id ) } } , upsert = 1 ) \n            except pymongo . errors . DuplicateKeyError : \n                LOG . error ( \"One or more casedata object is already present in clinvar collection!\" ) \n    updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : submission_id } , { '$set' : { 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) \n    return updated_submission "}
{"7625": "\ndef check_panels ( adapter , panels , default_panels = None ) : \n    default_panels = default_panels or [ ] \n    panels_exist = 1 \n    for panel in default_panels : \n        if panel not in panels : \n            log . warning ( \"Default panels have to be defined in panels\" ) \n            panels_exist = 0 \n    for panel in panels : \n        if not adapter . gene_panel ( panel ) : \n            log . warning ( \"Panel {} does not exist in database\" . format ( panel ) ) \n            panels_exist = 0 \n    return panels_exist "}
{"7627": "\ndef load_scout ( adapter , config , ped = None , update = 0 ) : \n    log . info ( \"Check that the panels exists\" ) \n    if not check_panels ( adapter , config . get ( 'gene_panels' , [ ] ) , config . get ( 'default_gene_panels' ) ) : \n        raise ConfigError ( \"Some panel(s) does not exist in the database\" ) \n    case_obj = adapter . load_case ( config , update = update ) \n    return case_obj "}
{"7633": "\ndef diseases ( context , api_key ) : \n    adapter = context . obj [ 'adapter' ] \n    api_key = api_key or context . obj . get ( 'omim_api_key' ) \n    if not api_key : \n        LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) \n        context . abort ( ) \n    try : \n        mim_files = fetch_mim_files ( api_key , genemap2 = 1 ) \n    except Exception as err : \n        LOG . warning ( err ) \n        context . abort ( ) \n    LOG . info ( \"Dropping DiseaseTerms\" ) \n    adapter . disease_term_collection . drop ( ) \n    LOG . debug ( \"DiseaseTerms dropped\" ) \n    load_disease_terms ( adapter = adapter , genemap_lines = mim_files [ 'genemap2' ] , ) \n    LOG . info ( \"Successfully loaded all disease terms\" ) "}
{"7639": "\ndef build_hgnc_gene ( gene_info , build = '37' ) : \n    try : \n        hgnc_id = int ( gene_info [ 'hgnc_id' ] ) \n    except KeyError as err : \n        raise KeyError ( \"Gene has to have a hgnc_id\" ) \n    except ValueError as err : \n        raise ValueError ( \"hgnc_id has to be integer\" ) \n    try : \n        hgnc_symbol = gene_info [ 'hgnc_symbol' ] \n    except KeyError as err : \n        raise KeyError ( \"Gene has to have a hgnc_symbol\" ) \n    try : \n        ensembl_id = gene_info [ 'ensembl_gene_id' ] \n    except KeyError as err : \n        raise KeyError ( \"Gene has to have a ensembl_id\" ) \n    try : \n        chromosome = gene_info [ 'chromosome' ] \n    except KeyError as err : \n        raise KeyError ( \"Gene has to have a chromosome\" ) \n    try : \n        start = int ( gene_info [ 'start' ] ) \n    except KeyError as err : \n        raise KeyError ( \"Gene has to have a start position\" ) \n    except TypeError as err : \n        raise TypeError ( \"Gene start has to be a integer\" ) \n    try : \n        end = int ( gene_info [ 'end' ] ) \n    except KeyError as err : \n        raise KeyError ( \"Gene has to have a end position\" ) \n    except TypeError as err : \n        raise TypeError ( \"Gene end has to be a integer\" ) \n    gene_obj = HgncGene ( hgnc_id = hgnc_id , hgnc_symbol = hgnc_symbol , ensembl_id = ensembl_id , chrom = chromosome , start = start , end = end , build = build , ) \n    if gene_info . get ( 'description' ) : \n        gene_obj [ 'description' ] = gene_info [ 'description' ] \n    if gene_info . get ( 'previous_symbols' ) : \n        gene_obj [ 'aliases' ] = gene_info [ 'previous_symbols' ] \n    if gene_info . get ( 'entrez_id' ) : \n        gene_obj [ 'entrez_id' ] = int ( gene_info [ 'entrez_id' ] ) \n    if gene_info . get ( 'omim_id' ) : \n        gene_obj [ 'omim_id' ] = int ( gene_info [ 'omim_id' ] ) \n    if gene_info . get ( 'pli_score' ) : \n        gene_obj [ 'pli_score' ] = float ( gene_info [ 'pli_score' ] ) \n    if gene_info . get ( 'ref_seq' ) : \n        gene_obj [ 'primary_transcripts' ] = gene_info [ 'ref_seq' ] \n    if gene_info . get ( 'ucsc_id' ) : \n        gene_obj [ 'ucsc_id' ] = gene_info [ 'ucsc_id' ] \n    if gene_info . get ( 'uniprot_ids' ) : \n        gene_obj [ 'uniprot_ids' ] = gene_info [ 'uniprot_ids' ] \n    if gene_info . get ( 'vega_id' ) : \n        gene_obj [ 'vega_id' ] = gene_info [ 'vega_id' ] \n    if gene_info . get ( 'incomplete_penetrance' ) : \n        gene_obj [ 'incomplete_penetrance' ] = 1 \n    if gene_info . get ( 'inheritance_models' ) : \n        gene_obj [ 'inheritance_models' ] = gene_info [ 'inheritance_models' ] \n    phenotype_objs = [ ] \n    for phenotype_info in gene_info . get ( 'phenotypes' , [ ] ) : \n        phenotype_objs . append ( build_phenotype ( phenotype_info ) ) \n    if phenotype_objs : \n        gene_obj [ 'phenotypes' ] = phenotype_objs \n    for key in list ( gene_obj ) : \n        if gene_obj [ key ] is None : \n            gene_obj . pop ( key ) \n    return gene_obj "}
{"7641": "\ndef load_omim_panel ( self , api_key , institute = None ) : \n    existing_panel = self . gene_panel ( panel_id = 'OMIM-AUTO' ) \n    if not existing_panel : \n        LOG . warning ( \"OMIM-AUTO does not exists in database\" ) \n        LOG . info ( 'Creating a first version' ) \n        version = 1.0 \n    if existing_panel : \n        version = float ( math . floor ( existing_panel [ 'version' ] ) + 1 ) \n    LOG . info ( \"Setting version to %s\" , version ) \n    try : \n        mim_files = fetch_mim_files ( api_key = api_key , genemap2 = 1 , mim2genes = 1 ) \n    except Exception as err : \n        raise err \n    date_string = None \n    for line in mim_files [ 'genemap2' ] : \n        if 'Generated' in line : \n            date_string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) \n    date_obj = get_date ( date_string ) \n    if existing_panel : \n        if existing_panel [ 'date' ] == date_obj : \n            LOG . warning ( \"There is no new version of OMIM\" ) \n            return \n    panel_data = { } \n    panel_data [ 'path' ] = None \n    panel_data [ 'type' ] = 'clinical' \n    panel_data [ 'date' ] = date_obj \n    panel_data [ 'panel_id' ] = 'OMIM-AUTO' \n    panel_data [ 'institute' ] = institute or 'cust002' \n    panel_data [ 'version' ] = version \n    panel_data [ 'display_name' ] = 'OMIM-AUTO' \n    panel_data [ 'genes' ] = [ ] \n    alias_genes = self . genes_by_alias ( ) \n    genes = get_omim_panel_genes ( genemap2_lines = mim_files [ 'genemap2' ] , mim2gene_lines = mim_files [ 'mim2genes' ] , alias_genes = alias_genes , ) \n    for gene in genes : \n        panel_data [ 'genes' ] . append ( gene ) \n    panel_obj = build_panel ( panel_data , self ) \n    if existing_panel : \n        new_genes = self . compare_mim_panels ( existing_panel , panel_obj ) \n        if new_genes : \n            self . update_mim_version ( new_genes , panel_obj , old_version = existing_panel [ 'version' ] ) \n        else : \n            LOG . info ( \"The new version of omim does not differ from the old one\" ) \n            LOG . info ( \"No update is added\" ) \n            return \n    self . add_gene_panel ( panel_obj ) "}
{"7652": "\ndef apply_pending ( self , panel_obj , version ) : \n    updates = { } \n    new_panel = deepcopy ( panel_obj ) \n    new_panel [ 'pending' ] = [ ] \n    new_panel [ 'date' ] = dt . datetime . now ( ) \n    info_fields = [ 'disease_associated_transcripts' , 'inheritance_models' , 'reduced_penetrance' , 'mosaicism' , 'database_entry_version' , 'comment' ] \n    new_genes = [ ] \n    for update in panel_obj . get ( 'pending' , [ ] ) : \n        hgnc_id = update [ 'hgnc_id' ] \n        if update [ 'action' ] != 'add' : \n            updates [ hgnc_id ] = update \n            continue \n        info = update . get ( 'info' , { } ) \n        gene_obj = { 'hgnc_id' : hgnc_id , 'symbol' : update [ 'symbol' ] } \n        for field in info_fields : \n            if field in info : \n                gene_obj [ field ] = info [ field ] \n        new_genes . append ( gene_obj ) \n    for gene in panel_obj [ 'genes' ] : \n        hgnc_id = gene [ 'hgnc_id' ] \n        if hgnc_id not in updates : \n            new_genes . append ( gene ) \n            continue \n        current_update = updates [ hgnc_id ] \n        action = current_update [ 'action' ] \n        info = current_update [ 'info' ] \n        if action == 'delete' : \n            continue \n        elif action == 'edit' : \n            for field in info_fields : \n                if field in info : \n                    gene [ field ] = info [ field ] \n            new_genes . append ( gene ) \n    new_panel [ 'genes' ] = new_genes \n    new_panel [ 'version' ] = float ( version ) \n    inserted_id = None \n    if new_panel [ 'version' ] == panel_obj [ 'version' ] : \n        result = self . panel_collection . find_one_and_replace ( { '_id' : panel_obj [ '_id' ] } , new_panel , return_document = pymongo . ReturnDocument . AFTER ) \n        inserted_id = result [ '_id' ] \n    else : \n        new_panel . pop ( '_id' ) \n        panel_obj [ 'is_archived' ] = 1 \n        self . update_panel ( panel_obj = panel_obj , date_obj = panel_obj [ 'date' ] ) \n        inserted_id = self . panel_collection . insert_one ( new_panel ) . inserted_id \n    return inserted_id "}
{"7660": "\ndef build_query ( self , case_id , query = None , variant_ids = None , category = 'snv' ) : \n    query = query or { } \n    mongo_query = { } \n    gene_query = None \n    for criterion in FUNDAMENTAL_CRITERIA : \n        if criterion == 'case_id' : \n            LOG . debug ( \"Building a mongo query for %s\" % case_id ) \n            mongo_query [ 'case_id' ] = case_id \n        elif criterion == 'variant_ids' and variant_ids : \n            LOG . debug ( \"Adding variant_ids %s to query\" % ', ' . join ( variant_ids ) ) \n            mongo_query [ 'variant_id' ] = { '$in' : variant_ids } \n        elif criterion == 'category' : \n            LOG . debug ( \"Querying category %s\" % category ) \n            mongo_query [ 'category' ] = category \n        elif criterion == 'variant_type' : \n            mongo_query [ 'variant_type' ] = query . get ( 'variant_type' , 'clinical' ) \n            LOG . debug ( \"Set variant type to %s\" , mongo_query [ 'variant_type' ] ) \n        elif criterion in [ 'hgnc_symbols' , 'gene_panels' ] and gene_query is None : \n            gene_query = self . gene_filter ( query , mongo_query ) \n        elif criterion == 'chrom' and query . get ( 'chrom' ) : \n            self . coordinate_filter ( query , mongo_query ) \n        elif criterion == 'variant_ids' and variant_ids : \n            LOG . debug ( \"Adding variant_ids %s to query\" % ', ' . join ( variant_ids ) ) \n            mongo_query [ 'variant_id' ] = { '$in' : variant_ids } \n    primary_terms = 0 \n    secondary_terms = 0 \n    for term in PRIMARY_CRITERIA : \n        if query . get ( term ) : \n            primary_terms = 1 \n    for term in SECONDARY_CRITERIA : \n        if query . get ( term ) : \n            secondary_terms = 1 \n    if primary_terms is 1 : \n        clinsign_filter = self . clinsig_query ( query , mongo_query ) \n    if secondary_terms is 1 : \n        secondary_filter = self . secondary_query ( query , mongo_query ) \n        if primary_terms is 0 : \n            if gene_query : \n                mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] \n            else : \n                mongo_query [ '$and' ] = secondary_filter \n        if primary_terms is 1 : \n            if query . get ( 'clinsig_confident_always_returned' ) == 1 : \n                if gene_query : \n                    mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$or' : [ { '$and' : secondary_filter } , clinsign_filter ] } ] \n                else : \n                    mongo_query [ '$or' ] = [ { '$and' : secondary_filter } , clinsign_filter ] \n            else : \n                secondary_filter . append ( clinsign_filter ) \n                if gene_query : \n                    mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] \n                else : \n                    mongo_query [ '$and' ] = secondary_filter \n    elif primary_terms is 1 : \n        mongo_query [ 'clnsig' ] = clinsign_filter [ 'clnsig' ] \n        if gene_query : \n            mongo_query [ '$and' ] = [ { '$or' : gene_query } ] \n    elif gene_query : \n        mongo_query [ '$and' ] = [ { '$or' : gene_query } ] \n    LOG . info ( \"mongo query: %s\" , mongo_query ) \n    return mongo_query "}
{"7661": "\ndef clinsig_query ( self , query , mongo_query ) : \n    LOG . debug ( 'clinsig is a query parameter' ) \n    trusted_revision_level = [ 'mult' , 'single' , 'exp' , 'guideline' ] \n    rank = [ ] \n    str_rank = [ ] \n    clnsig_query = { } \n    for item in query [ 'clinsig' ] : \n        rank . append ( int ( item ) ) \n        rank . append ( CLINSIG_MAP [ int ( item ) ] ) \n        str_rank . append ( CLINSIG_MAP [ int ( item ) ] ) \n    if query . get ( 'clinsig_confident_always_returned' ) == 1 : \n        LOG . debug ( \"add CLINSIG filter with trusted_revision_level\" ) \n        clnsig_query = { \"clnsig\" : { '$elemMatch' : { '$or' : [ { '$and' : [ { 'value' : { '$in' : rank } } , { 'revstat' : { '$in' : trusted_revision_level } } ] } , { '$and' : [ { 'value' : re . compile ( '|' . join ( str_rank ) ) } , { 'revstat' : re . compile ( '|' . join ( trusted_revision_level ) ) } ] } ] } } } \n    else : \n        LOG . debug ( \"add CLINSIG filter for rank: %s\" % ', ' . join ( str ( query [ 'clinsig' ] ) ) ) \n        clnsig_query = { \"clnsig\" : { '$elemMatch' : { '$or' : [ { 'value' : { '$in' : rank } } , { 'value' : re . compile ( '|' . join ( str_rank ) ) } ] } } } \n    return clnsig_query "}
{"7665": "\ndef parse_panel ( csv_stream ) : \n    reader = csv . DictReader ( csv_stream , delimiter = ';' , quoting = csv . QUOTE_NONE ) \n    genes = [ ] \n    for gene_row in reader : \n        if not gene_row [ 'HGNC_IDnumber' ] . strip ( ) . isdigit ( ) : \n            continue \n        transcripts_raw = gene_row . get ( 'Disease_associated_transcript' ) \n        if transcripts_raw : \n            transcripts_list = [ tx . split ( ':' , 1 ) [ - 1 ] . strip ( ) for tx in transcripts_raw . split ( ',' ) ] \n        else : \n            transcripts_list = [ ] \n        models_raw = gene_row . get ( 'Genetic_disease_model' ) \n        models_list = [ model . strip ( ) for model in models_raw . split ( ',' ) ] if models_raw else [ ] \n        panel_gene = dict ( symbol = gene_row [ 'HGNC_symbol' ] . strip ( ) if gene_row . get ( 'HGNC_symbol' ) else None , hgnc_id = int ( gene_row [ 'HGNC_IDnumber' ] . strip ( ) ) , disease_associated_transcripts = transcripts_list , reduced_penetrance = 1 if gene_row . get ( 'Reduced_penetrance' ) else None , mosaicism = 1 if gene_row . get ( 'Mosaicism' ) else None , inheritance_models = models_list , database_entry_version = gene_row . get ( 'Database_entry_version' ) , ) \n        genes . append ( panel_gene ) \n    return genes "}
{"7672": "\ndef hgnc_genes ( self , hgnc_symbol , build = '37' , search = 0 ) : \n    LOG . debug ( \"Fetching genes with symbol %s\" % hgnc_symbol ) \n    if search : \n        full_query = self . hgnc_collection . find ( { '$or' : [ { 'aliases' : hgnc_symbol } , { 'hgnc_id' : int ( hgnc_symbol ) if hgnc_symbol . isdigit ( ) else None } , ] , 'build' : build } ) \n        if full_query . count ( ) != 0 : \n            return full_query \n        return self . hgnc_collection . find ( { 'aliases' : { '$regex' : hgnc_symbol , '$options' : 'i' } , 'build' : build } ) \n    return self . hgnc_collection . find ( { 'build' : build , 'aliases' : hgnc_symbol } ) "}
{"7694": "\ndef case_diagnosis ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    level = 'phenotype' if 'phenotype' in request . form else 'gene' \n    omim_id = request . form [ 'omim_id' ] \n    remove = 1 if request . args . get ( 'remove' ) == 'yes' else 0 \n    store . diagnose ( institute_obj , case_obj , user_obj , link , level = level , omim_id = omim_id , remove = remove ) \n    return redirect ( request . referrer ) "}
{"7707": "\ndef vcf2cytosure ( institute_id , case_name , individual_id ) : \n    ( display_name , vcf2cytosure ) = controllers . vcf2cytosure ( store , institute_id , case_name , individual_id ) \n    outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) \n    filename = os . path . basename ( vcf2cytosure ) \n    log . debug ( \"Attempt to deliver file {0} from dir {1}\" . format ( filename , outdir ) ) \n    attachment_filename = display_name + \".vcf2cytosure.cgh\" \n    return send_from_directory ( outdir , filename , attachment_filename = attachment_filename , as_attachment = 1 ) "}
{"7710": "\ndef case_report_content ( store , institute_obj , case_obj ) : \n    variant_types = { 'causatives_detailed' : 'causatives' , 'suspects_detailed' : 'suspects' , 'classified_detailed' : 'acmg_classification' , 'tagged_detailed' : 'manual_rank' , 'dismissed_detailed' : 'dismiss_variant' , 'commented_detailed' : 'is_commented' , } \n    data = case_obj \n    for individual in data [ 'individuals' ] : \n        try : \n            sex = int ( individual . get ( 'sex' , 0 ) ) \n        except ValueError as err : \n            sex = 0 \n        individual [ 'sex_human' ] = SEX_MAP [ sex ] \n        individual [ 'phenotype_human' ] = PHENOTYPE_MAP . get ( individual [ 'phenotype' ] ) \n    data [ 'comments' ] = store . events ( institute_obj , case = case_obj , comments = 1 ) \n    data [ 'manual_rank_options' ] = MANUAL_RANK_OPTIONS \n    data [ 'dismissed_options' ] = DISMISS_VARIANT_OPTIONS \n    data [ 'genetic_models' ] = dict ( GENETIC_MODELS ) \n    data [ 'report_created_at' ] = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M\" ) \n    evaluated_variants = { } \n    for vt in variant_types : \n        evaluated_variants [ vt ] = [ ] \n    for var_type in [ 'causatives' , 'suspects' ] : \n        vt = '_' . join ( [ var_type , 'detailed' ] ) \n        for var_id in case_obj . get ( var_type , [ ] ) : \n            variant_obj = store . variant ( var_id ) \n            if not variant_obj : \n                continue \n            evaluated_variants [ vt ] . append ( variant_obj ) \n    for var_obj in store . evaluated_variants ( case_id = case_obj [ '_id' ] ) : \n        for vt in variant_types : \n            keyword = variant_types [ vt ] \n            if keyword in var_obj : \n                evaluated_variants [ vt ] . append ( var_obj ) \n    for var_type in evaluated_variants : \n        decorated_variants = [ ] \n        for var_obj in evaluated_variants [ var_type ] : \n            if var_obj [ 'category' ] == 'snv' : \n                decorated_info = variant_decorator ( store = store , institute_obj = institute_obj , case_obj = case_obj , variant_id = None , variant_obj = var_obj , add_case = 0 , add_other = 0 , get_overlapping = 0 ) \n            else : \n                decorated_info = sv_variant ( store = store , institute_id = institute_obj [ '_id' ] , case_name = case_obj [ 'display_name' ] , variant_obj = var_obj , add_case = 0 , get_overlapping = 0 ) \n            decorated_variants . append ( decorated_info [ 'variant' ] ) \n        data [ var_type ] = decorated_variants \n    return data "}
{"7718": "\ndef get_sanger_unevaluated ( store , institute_id , user_id ) : \n    sanger_ordered_by_case = store . sanger_ordered ( institute_id , user_id ) \n    unevaluated = [ ] \n    for item in sanger_ordered_by_case : \n        case_id = item [ '_id' ] \n        case_obj = store . case ( case_id = case_id ) \n        if not case_obj : \n            continue \n        case_display_name = case_obj . get ( 'display_name' ) \n        varid_list = item [ 'vars' ] \n        unevaluated_by_case = { } \n        unevaluated_by_case [ case_display_name ] = [ ] \n        for var_id in varid_list : \n            variant_obj = store . variant ( document_id = var_id , case_id = case_id ) \n            if variant_obj is None or variant_obj . get ( 'sanger_ordered' ) is None or variant_obj . get ( 'sanger_ordered' ) is 0 : \n                continue \n            validation = variant_obj . get ( 'validation' , 'not_evaluated' ) \n            if validation in [ 'True positive' , 'False positive' ] : \n                continue \n            unevaluated_by_case [ case_display_name ] . append ( variant_obj [ '_id' ] ) \n        if len ( unevaluated_by_case [ case_display_name ] ) > 0 : \n            unevaluated . append ( unevaluated_by_case ) \n    return unevaluated "}
{"7723": "\ndef genes ( context , build , api_key ) : \n    LOG . info ( \"Running scout update genes\" ) \n    adapter = context . obj [ 'adapter' ] \n    api_key = api_key or context . obj . get ( 'omim_api_key' ) \n    if not api_key : \n        LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) \n        context . abort ( ) \n    try : \n        mim_files = fetch_mim_files ( api_key , mim2genes = 1 , morbidmap = 1 , genemap2 = 1 ) \n    except Exception as err : \n        LOG . warning ( err ) \n        context . abort ( ) \n    LOG . warning ( \"Dropping all gene information\" ) \n    adapter . drop_genes ( build ) \n    LOG . info ( \"Genes dropped\" ) \n    LOG . warning ( \"Dropping all transcript information\" ) \n    adapter . drop_transcripts ( build ) \n    LOG . info ( \"transcripts dropped\" ) \n    hpo_genes = fetch_hpo_genes ( ) \n    if build : \n        builds = [ build ] \n    else : \n        builds = [ '37' , '38' ] \n    hgnc_lines = fetch_hgnc ( ) \n    exac_lines = fetch_exac_constraint ( ) \n    for build in builds : \n        ensembl_genes = fetch_ensembl_genes ( build = build ) \n        hgnc_genes = load_hgnc_genes ( adapter = adapter , ensembl_lines = ensembl_genes , hgnc_lines = hgnc_lines , exac_lines = exac_lines , mim2gene_lines = mim_files [ 'mim2genes' ] , genemap_lines = mim_files [ 'genemap2' ] , hpo_lines = hpo_genes , build = build , ) \n        ensembl_genes = { } \n        for gene_obj in hgnc_genes : \n            ensembl_id = gene_obj [ 'ensembl_id' ] \n            ensembl_genes [ ensembl_id ] = gene_obj \n        ensembl_transcripts = fetch_ensembl_transcripts ( build = build ) \n        transcripts = load_transcripts ( adapter , ensembl_transcripts , build , ensembl_genes ) \n    adapter . update_indexes ( ) \n    LOG . info ( \"Genes, transcripts and Exons loaded\" ) "}
{"7725": "\ndef build_transcript ( transcript_info , build = '37' ) : \n    try : \n        transcript_id = transcript_info [ 'ensembl_transcript_id' ] \n    except KeyError : \n        raise KeyError ( \"Transcript has to have ensembl id\" ) \n    build = build \n    is_primary = transcript_info . get ( 'is_primary' , 0 ) \n    refseq_id = transcript_info . get ( 'refseq_id' ) \n    refseq_identifiers = transcript_info . get ( 'refseq_identifiers' ) \n    try : \n        chrom = transcript_info [ 'chrom' ] \n    except KeyError : \n        raise KeyError ( \"Transcript has to have a chromosome\" ) \n    try : \n        start = int ( transcript_info [ 'transcript_start' ] ) \n    except KeyError : \n        raise KeyError ( \"Transcript has to have start\" ) \n    except TypeError : \n        raise TypeError ( \"Transcript start has to be integer\" ) \n    try : \n        end = int ( transcript_info [ 'transcript_end' ] ) \n    except KeyError : \n        raise KeyError ( \"Transcript has to have end\" ) \n    except TypeError : \n        raise TypeError ( \"Transcript end has to be integer\" ) \n    try : \n        hgnc_id = int ( transcript_info [ 'hgnc_id' ] ) \n    except KeyError : \n        raise KeyError ( \"Transcript has to have a hgnc id\" ) \n    except TypeError : \n        raise TypeError ( \"hgnc id has to be integer\" ) \n    transcript_obj = HgncTranscript ( transcript_id = transcript_id , hgnc_id = hgnc_id , chrom = chrom , start = start , end = end , is_primary = is_primary , refseq_id = refseq_id , refseq_identifiers = refseq_identifiers , build = build ) \n    for key in list ( transcript_obj ) : \n        if transcript_obj [ key ] is None : \n            transcript_obj . pop ( key ) \n    return transcript_obj "}
{"7730": "\ndef update_variant_rank ( self , case_obj , variant_type = 'clinical' , category = 'snv' ) : \n    variants = self . variant_collection . find ( { 'case_id' : case_obj [ '_id' ] , 'category' : category , 'variant_type' : variant_type , } ) . sort ( 'rank_score' , pymongo . DESCENDING ) \n    LOG . info ( \"Updating variant_rank for all variants\" ) \n    requests = [ ] \n    for index , var_obj in enumerate ( variants ) : \n        if len ( requests ) > 5000 : \n            try : \n                self . variant_collection . bulk_write ( requests , ordered = 0 ) \n                requests = [ ] \n            except BulkWriteError as err : \n                LOG . warning ( \"Updating variant rank failed\" ) \n                raise err \n        operation = pymongo . UpdateOne ( { '_id' : var_obj [ '_id' ] } , { '$set' : { 'variant_rank' : index + 1 , } } ) \n        requests . append ( operation ) \n    try : \n        self . variant_collection . bulk_write ( requests , ordered = 0 ) \n    except BulkWriteError as err : \n        LOG . warning ( \"Updating variant rank failed\" ) \n        raise err \n    LOG . info ( \"Updating variant_rank done\" ) "}
{"7731": "\ndef update_variant_compounds ( self , variant , variant_objs = None ) : \n    compound_objs = [ ] \n    for compound in variant . get ( 'compounds' , [ ] ) : \n        not_loaded = 1 \n        gene_objs = [ ] \n        if variant_objs : \n            variant_obj = variant_objs . get ( compound [ 'variant' ] ) \n        else : \n            variant_obj = self . variant_collection . find_one ( { '_id' : compound [ 'variant' ] } ) \n        if variant_obj : \n            not_loaded = 0 \n            compound [ 'rank_score' ] = variant_obj [ 'rank_score' ] \n            for gene in variant_obj . get ( 'genes' , [ ] ) : \n                gene_obj = { 'hgnc_id' : gene [ 'hgnc_id' ] , 'hgnc_symbol' : gene . get ( 'hgnc_symbol' ) , 'region_annotation' : gene . get ( 'region_annotation' ) , 'functional_annotation' : gene . get ( 'functional_annotation' ) , } \n                gene_objs . append ( gene_obj ) \n                compound [ 'genes' ] = gene_objs \n        compound [ 'not_loaded' ] = not_loaded \n        compound_objs . append ( compound ) \n    return compound_objs "}
{"7733": "\ndef update_mongo_compound_variants ( self , bulk ) : \n    requests = [ ] \n    for var_id in bulk : \n        var_obj = bulk [ var_id ] \n        if not var_obj . get ( 'compounds' ) : \n            continue \n        operation = pymongo . UpdateOne ( { '_id' : var_obj [ '_id' ] } , { '$set' : { 'compounds' : var_obj [ 'compounds' ] } } ) \n        requests . append ( operation ) \n    if not requests : \n        return \n    try : \n        self . variant_collection . bulk_write ( requests , ordered = 0 ) \n    except BulkWriteError as err : \n        LOG . warning ( \"Updating compounds failed\" ) \n        raise err "}
{"7734": "\ndef update_case_compounds ( self , case_obj , build = '37' ) : \n    case_id = case_obj [ '_id' ] \n    categories = set ( ) \n    variant_types = set ( ) \n    for file_type in FILE_TYPE_MAP : \n        if case_obj . get ( 'vcf_files' , { } ) . get ( file_type ) : \n            categories . add ( FILE_TYPE_MAP [ file_type ] [ 'category' ] ) \n            variant_types . add ( FILE_TYPE_MAP [ file_type ] [ 'variant_type' ] ) \n    coding_intervals = self . get_coding_intervals ( build = build ) \n    for chrom in CHROMOSOMES : \n        intervals = coding_intervals . get ( chrom , IntervalTree ( ) ) \n        for var_type in variant_types : \n            for category in categories : \n                LOG . info ( \"Updating compounds on chromosome:{0}, type:{1}, category:{2} for case:{3}\" . format ( chrom , var_type , category , case_id ) ) \n                query = { 'variant_type' : var_type , 'chrom' : chrom , } \n                variant_objs = self . variants ( case_id = case_id , query = query , category = category , nr_of_variants = - 1 , sort_key = 'position' ) \n                bulk = { } \n                current_region = None \n                special = 0 \n                for var_obj in variant_objs : \n                    var_id = var_obj [ '_id' ] \n                    var_chrom = var_obj [ 'chromosome' ] \n                    var_start = var_obj [ 'position' ] \n                    var_end = var_obj [ 'end' ] + 1 \n                    update_bulk = 1 \n                    new_region = None \n                    genomic_regions = coding_intervals . get ( var_chrom , IntervalTree ( ) ) . search ( var_start , var_end ) \n                    if genomic_regions : \n                        new_region = genomic_regions . pop ( ) . data \n                    if new_region and ( new_region == current_region ) : \n                        update_bulk = 0 \n                    current_region = new_region \n                    if update_bulk and bulk : \n                        self . update_compounds ( bulk ) \n                        self . update_mongo_compound_variants ( bulk ) \n                        bulk = { } \n                    if new_region : \n                        bulk [ var_id ] = var_obj \n                if not bulk : \n                    continue \n                self . update_compounds ( bulk ) \n                self . update_mongo_compound_variants ( bulk ) \n    LOG . info ( \"All compounds updated\" ) \n    return "}
{"7740": "\ndef diagnose ( self , institute , case , user , link , level , omim_id , remove = 0 ) : \n    if level == 'phenotype' : \n        case_key = 'diagnosis_phenotypes' \n    elif level == 'gene' : \n        case_key = 'diagnosis_genes' \n    else : \n        raise TypeError ( 'wrong level' ) \n    diagnosis_list = case . get ( case_key , [ ] ) \n    omim_number = int ( omim_id . split ( ':' ) [ - 1 ] ) \n    updated_case = None \n    if remove and omim_number in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$pull' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    elif omim_number not in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$push' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    if updated_case : \n        self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'update_diagnosis' , subject = case [ 'display_name' ] , content = omim_id ) \n    return updated_case "}
{"7741": "\ndef mark_checked ( self , institute , case , user , link , unmark = 0 ) : \n    LOG . info ( \"Updating checked status of {}\" . format ( case [ 'display_name' ] ) ) \n    status = 'not checked' if unmark else 'checked' \n    self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'check_case' , subject = status ) \n    LOG . info ( \"Updating {0}'s checked status {1}\" . format ( case [ 'display_name' ] , status ) ) \n    analysis_checked = 0 if unmark else 1 \n    updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$set' : { 'analysis_checked' : analysis_checked } } , return_document = pymongo . ReturnDocument . AFTER ) \n    LOG . debug ( \"Case updated\" ) \n    return updated_case "}
{"7742": "\ndef order_verification ( self , institute , case , user , link , variant ) : \n    LOG . info ( \"Creating event for ordering validation for variant\" \" {0}\" . format ( variant [ 'display_name' ] ) ) \n    updated_variant = self . variant_collection . find_one_and_update ( { '_id' : variant [ '_id' ] } , { '$set' : { 'sanger_ordered' : 1 } } , return_document = pymongo . ReturnDocument . AFTER ) \n    self . create_event ( institute = institute , case = case , user = user , link = link , category = 'variant' , verb = 'sanger' , variant = variant , subject = variant [ 'display_name' ] , ) \n    LOG . info ( \"Creating event for ordering sanger for case\" \" {0}\" . format ( case [ 'display_name' ] ) ) \n    self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'sanger' , variant = variant , subject = variant [ 'display_name' ] , ) \n    return updated_variant "}
{"7775": "\ndef formatmonth ( self , theyear , themonth , withyear = 1 , net = None , qs = None , template = 'happenings/partials/calendar/month_table.html' ) : \n    context = self . get_context ( ) \n    context [ 'month_start_date' ] = date ( self . yr , self . mo , 1 ) \n    context [ 'week_rows' ] = [ ] \n    for week in self . monthdays2calendar ( theyear , themonth ) : \n        week_row = [ ] \n        for day , weekday in week : \n            week_row . append ( self . formatday ( day , weekday ) ) \n        context [ 'week_rows' ] . append ( week_row ) \n    nxt , prev = get_next_and_prev ( net ) \n    extra_qs = ( '&' + '&' . join ( qs ) ) if qs else '' \n    context [ 'prev_qs' ] = mark_safe ( '?cal_prev=%d%s' % ( prev , extra_qs ) ) \n    context [ 'next_qs' ] = mark_safe ( '?cal_next=%d%s' % ( nxt , extra_qs ) ) \n    context [ 'withyear' ] = withyear \n    return render_to_string ( template , context ) "}
{"7777": "\ndef formatmonthname ( self , theyear , themonth , withyear = 1 ) : \n    display_month = month_name [ themonth ] \n    if isinstance ( display_month , six . binary_type ) and self . encoding : \n        display_month = display_month . decode ( self . encoding ) \n    if withyear : \n        s = u'%s %s' % ( display_month , theyear ) \n    else : \n        s = u'%s' % display_month \n    return ( '<tr><th colspan=\"5\" class=\"month\">' '<button id=\"cal-today-btn\" class=\"btn btn-small\">' 'Today</button> %s</th></tr>' % s ) "}
{"7780": "\ndef parse_gene ( gene_info ) : \n    gene = { } \n    identifier = None \n    hgnc_id = None \n    try : \n        if 'hgnc_id' in gene_info : \n            hgnc_id = int ( gene_info [ 'hgnc_id' ] ) \n        elif 'hgnc_idnumber' in gene_info : \n            hgnc_id = int ( gene_info [ 'hgnc_idnumber' ] ) \n        elif 'hgncid' in gene_info : \n            hgnc_id = int ( gene_info [ 'hgncid' ] ) \n    except ValueError as e : \n        raise SyntaxError ( \"Invalid hgnc id: {0}\" . format ( hgnc_id ) ) \n    gene [ 'hgnc_id' ] = hgnc_id \n    identifier = hgnc_id \n    hgnc_symbol = None \n    if 'hgnc_symbol' in gene_info : \n        hgnc_symbol = gene_info [ 'hgnc_symbol' ] \n    elif 'hgncsymbol' in gene_info : \n        hgnc_symbol = gene_info [ 'hgncsymbol' ] \n    elif 'symbol' in gene_info : \n        hgnc_symbol = gene_info [ 'symbol' ] \n    gene [ 'hgnc_symbol' ] = hgnc_symbol \n    if not identifier : \n        if hgnc_symbol : \n            identifier = hgnc_symbol \n        else : \n            raise SyntaxError ( \"No gene identifier could be found\" ) \n    gene [ 'identifier' ] = identifier \n    transcripts = \"\" \n    if 'disease_associated_transcripts' in gene_info : \n        transcripts = gene_info [ 'disease_associated_transcripts' ] \n    elif 'disease_associated_transcript' in gene_info : \n        transcripts = gene_info [ 'disease_associated_transcript' ] \n    elif 'transcripts' in gene_info : \n        transcripts = gene_info [ 'transcripts' ] \n    gene [ 'transcripts' ] = [ transcript . strip ( ) for transcript in transcripts . split ( ',' ) if transcript ] \n    models = \"\" \n    if 'genetic_disease_models' in gene_info : \n        models = gene_info [ 'genetic_disease_models' ] \n    elif 'genetic_disease_model' in gene_info : \n        models = gene_info [ 'genetic_disease_model' ] \n    elif 'inheritance_models' in gene_info : \n        models = gene_info [ 'inheritance_models' ] \n    elif 'genetic_inheritance_models' in gene_info : \n        models = gene_info [ 'genetic_inheritance_models' ] \n    gene [ 'inheritance_models' ] = [ model . strip ( ) for model in models . split ( ',' ) if model . strip ( ) in VALID_MODELS ] \n    gene [ 'mosaicism' ] = 1 if gene_info . get ( 'mosaicism' ) else 0 \n    gene [ 'reduced_penetrance' ] = 1 if gene_info . get ( 'reduced_penetrance' ) else 0 \n    gene [ 'database_entry_version' ] = gene_info . get ( 'database_entry_version' ) \n    return gene "}
{"7781": "\ndef parse_genes ( gene_lines ) : \n    genes = [ ] \n    header = [ ] \n    hgnc_identifiers = set ( ) \n    delimiter = '\\t' \n    delimiters = [ '\\t' , ' ' , ';' ] \n    for i , line in enumerate ( gene_lines ) : \n        line = line . rstrip ( ) \n        if not len ( line ) > 0 : \n            continue \n        if line . startswith ( '#' ) : \n            if not line . startswith ( '##' ) : \n                line_length = 0 \n                delimiter = None \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if len ( head_line ) > line_length : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                header = [ word . lower ( ) for word in line [ 1 : ] . split ( delimiter ) ] \n        else : \n            if i == 0 : \n                line_length = 0 \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if len ( head_line ) > line_length : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                if ( 'hgnc' in line or 'HGNC' in line ) : \n                    header = [ word . lower ( ) for word in line . split ( delimiter ) ] \n                    continue \n                if line . split ( delimiter ) [ 0 ] . isdigit ( ) : \n                    header = [ 'hgnc_id' ] \n                else : \n                    header = [ 'hgnc_symbol' ] \n            splitted_line = line . split ( delimiter ) \n            gene_info = dict ( zip ( header , splitted_line ) ) \n            info_found = 0 \n            for key in gene_info : \n                if gene_info [ key ] : \n                    info_found = 1 \n                    break \n            if not info_found : \n                continue \n            try : \n                gene = parse_gene ( gene_info ) \n            except Exception as e : \n                LOG . warning ( e ) \n                raise SyntaxError ( \"Line {0} is malformed\" . format ( i + 1 ) ) \n            identifier = gene . pop ( 'identifier' ) \n            if not identifier in hgnc_identifiers : \n                hgnc_identifiers . add ( identifier ) \n                genes . append ( gene ) \n    return genes "}
{"7789": "\ndef get_case_groups ( adapter , total_cases , institute_id = None , slice_query = None ) : \n    cases = [ { 'status' : 'all' , 'count' : total_cases , 'percent' : 1 } ] \n    pipeline = [ ] \n    group = { '$group' : { '_id' : '$status' , 'count' : { '$sum' : 1 } } } \n    subquery = { } \n    if institute_id and slice_query : \n        subquery = adapter . cases ( owner = institute_id , name_query = slice_query , yield_query = 1 ) \n    elif institute_id : \n        subquery = adapter . cases ( owner = institute_id , yield_query = 1 ) \n    elif slice_query : \n        subquery = adapter . cases ( name_query = slice_query , yield_query = 1 ) \n    query = { '$match' : subquery } if subquery else { } \n    if query : \n        pipeline . append ( query ) \n    pipeline . append ( group ) \n    res = adapter . case_collection . aggregate ( pipeline ) \n    for status_group in res : \n        cases . append ( { 'status' : status_group [ '_id' ] , 'count' : status_group [ 'count' ] , 'percent' : status_group [ 'count' ] / total_cases } ) \n    return cases "}
{"7798": "\ndef generate_hpo_gene_list ( self , * hpo_terms ) : \n    genes = { } \n    for term in hpo_terms : \n        hpo_obj = self . hpo_term ( term ) \n        if hpo_obj : \n            for hgnc_id in hpo_obj [ 'genes' ] : \n                if hgnc_id in genes : \n                    genes [ hgnc_id ] += 1 \n                else : \n                    genes [ hgnc_id ] = 1 \n        else : \n            LOG . warning ( \"Term %s could not be found\" , term ) \n    sorted_genes = sorted ( genes . items ( ) , key = operator . itemgetter ( 1 ) , reverse = 1 ) \n    return sorted_genes "}
{"7799": "\ndef read_hdf5 ( self , filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = 1 ) : \n    print ( \"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\" ) \n    self . header = { } \n    self . filename = filename \n    self . h5 = h5py . File ( filename ) \n    for key , val in self . h5 [ b'data' ] . attrs . items ( ) : \n        if six . PY3 : \n            key = bytes ( key , 'ascii' ) \n        if key == b'src_raj' : \n            self . header [ key ] = Angle ( val , unit = 'hr' ) \n        elif key == b'src_dej' : \n            self . header [ key ] = Angle ( val , unit = 'deg' ) \n        else : \n            self . header [ key ] = val \n    self . n_ints_in_file = self . h5 [ b\"data\" ] . shape [ 0 ] \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    if load_data : \n        self . data = self . h5 [ b\"data\" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] \n        self . file_size_bytes = os . path . getsize ( self . filename ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] ) \n        self . n_ints_in_file = 0 \n        self . file_size_bytes = os . path . getsize ( self . filename ) "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = 1 ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7806": "\ndef _calc_extent ( self , plot_f = None , plot_t = None , MJD_time = 0 ) : \n    plot_f_begin = plot_f [ 0 ] \n    plot_f_end = plot_f [ - 1 ] + ( plot_f [ 1 ] - plot_f [ 0 ] ) \n    plot_t_begin = self . timestamps [ 0 ] \n    plot_t_end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) \n    if MJD_time : \n        extent = ( plot_f_begin , plot_f_begin_end , plot_t_begin , plot_t_end ) \n    else : \n        extent = ( plot_f_begin , plot_f_end , 0.0 , ( plot_t_end - plot_t_begin ) * 24. * 60. * 60 ) \n    return extent "}
{"7807": "\ndef plot_waterfall ( self , f_start = None , f_stop = None , if_id = 0 , logged = 1 , cb = 1 , MJD_time = 0 , ** kwargs ) : \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if self . header [ b'foff' ] < 0 : \n        plot_data = plot_data [ ... , : : - 1 ] \n        plot_f = plot_f [ : : - 1 ] \n    if logged : \n        plot_data = db ( plot_data ) \n    dec_fac_x , dec_fac_y = 1 , 1 \n    if plot_data . shape [ 0 ] > MAX_IMSHOW_POINTS [ 0 ] : \n        dec_fac_x = int ( plot_data . shape [ 0 ] / MAX_IMSHOW_POINTS [ 0 ] ) \n    if plot_data . shape [ 1 ] > MAX_IMSHOW_POINTS [ 1 ] : \n        dec_fac_y = int ( plot_data . shape [ 1 ] / MAX_IMSHOW_POINTS [ 1 ] ) \n    plot_data = rebin ( plot_data , dec_fac_x , dec_fac_y ) \n    try : \n        plt . title ( self . header [ b'source_name' ] ) \n    except KeyError : \n        plt . title ( self . filename ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plt . imshow ( plot_data , aspect = 'auto' , origin = 'lower' , rasterized = 1 , interpolation = 'nearest' , extent = extent , cmap = 'viridis' , ** kwargs ) \n    if cb : \n        plt . colorbar ( ) \n    plt . xlabel ( \"Frequency [MHz]\" ) \n    if MJD_time : \n        plt . ylabel ( \"Time [MJD]\" ) \n    else : \n        plt . ylabel ( \"Time [s]\" ) "}
{"7808": "\ndef plot_time_series ( self , f_start = None , f_stop = None , if_id = 0 , logged = 1 , orientation = 'h' , MJD_time = 0 , ** kwargs ) : \n    ax = plt . gca ( ) \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if logged and self . header [ b'nbits' ] >= 8 : \n        plot_data = db ( plot_data ) \n    if len ( plot_data . shape ) > 1 : \n        plot_data = plot_data . mean ( axis = 1 ) \n    else : \n        plot_data = plot_data . mean ( ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plot_t = np . linspace ( extent [ 2 ] , extent [ 3 ] , len ( self . timestamps ) ) \n    if MJD_time : \n        tlabel = \"Time [MJD]\" \n    else : \n        tlabel = \"Time [s]\" \n    if logged : \n        plabel = \"Power [dB]\" \n    else : \n        plabel = \"Power [counts]\" \n    if 'v' in orientation : \n        plt . plot ( plot_data , plot_t , ** kwargs ) \n        plt . xlabel ( plabel ) \n    else : \n        plt . plot ( plot_t , plot_data , ** kwargs ) \n        plt . xlabel ( tlabel ) \n        plt . ylabel ( plabel ) \n    ax . autoscale ( axis = 'both' , tight = 1 ) "}
{"7813": "\ndef calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = 1 , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( diode_cross , max_load = 150 ) \n    cross_dat = obs . data \n    tsamp = obs . header [ 'tsamp' ] \n    dio_ncoarse = obs . calc_n_coarse_chan ( ) \n    dio_nchans = obs . header [ 'nchans' ] \n    dio_chan_per_coarse = dio_nchans / dio_ncoarse \n    obs = None \n    Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) \n    cross_dat = None \n    print ( 'Calculating Mueller Matrix variables' ) \n    gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    Idat = None \n    Qdat = None \n    Udat = None \n    Vdat = None \n    print ( 'Opening ' + cross_pols ) \n    cross_obs = Waterfall ( cross_pols , max_load = 150 ) \n    obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) \n    obs_nchans = cross_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = obs_nchans / obs_ncoarse \n    print ( 'Grabbing Stokes parameters' ) \n    I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) \n    print ( 'Applying Mueller Matrix' ) \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) \n    if onefile == 1 : \n        cross_obs . data [ : , 0 , : ] = np . squeeze ( I ) \n        cross_obs . data [ : , 1 , : ] = np . squeeze ( Q ) \n        cross_obs . data [ : , 2 , : ] = np . squeeze ( U ) \n        cross_obs . data [ : , 3 , : ] = np . squeeze ( V ) \n        cross_obs . write_to_fil ( cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        print ( 'Calibrated Stokes parameters written to ' + cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        return \n    obs = Waterfall ( obs_I , max_load = 150 ) \n    obs . data = I \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    print ( 'Calibrated Stokes I written to ' + cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    obs . data = Q \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    print ( 'Calibrated Stokes Q written to ' + cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    obs . data = U \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    print ( 'Calibrated Stokes U written to ' + cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    obs . data = V \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.V.polcal.fil' ) \n    print ( 'Calibrated Stokes V written to ' + cross_pols [ : - 15 ] + '.V.polcal.fil' ) "}
{"7820": "\ndef plot_Stokes_diode ( dio_cross , diff = 1 , feedtype = 'l' , ** kwargs ) : \n    if diff == 1 : \n        Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    else : \n        obs = Waterfall ( dio_cross , max_load = 150 ) \n        freqs = obs . populate_freqs ( ) \n        tsamp = obs . header [ 'tsamp' ] \n        data = obs . data \n        I , Q , U , V = get_stokes ( data , feedtype ) \n        I_OFF , I_ON = foldcal ( I , tsamp , ** kwargs ) \n        Q_OFF , Q_ON = foldcal ( Q , tsamp , ** kwargs ) \n        U_OFF , U_ON = foldcal ( U , tsamp , ** kwargs ) \n        V_OFF , V_ON = foldcal ( V , tsamp , ** kwargs ) \n    if diff == 1 : \n        plt . plot ( freqs , Idiff , 'k-' , label = 'I' ) \n        plt . plot ( freqs , Qdiff , 'r-' , label = 'Q' ) \n        plt . plot ( freqs , Udiff , 'g-' , label = 'U' ) \n        plt . plot ( freqs , Vdiff , 'm-' , label = 'V' ) \n    else : \n        plt . plot ( freqs , I_ON , 'k-' , label = 'I ON' ) \n        plt . plot ( freqs , I_OFF , 'k--' , label = 'I OFF' ) \n        plt . plot ( freqs , Q_ON , 'r-' , label = 'Q ON' ) \n        plt . plot ( freqs , Q_OFF , 'r--' , label = 'Q OFF' ) \n        plt . plot ( freqs , U_ON , 'g-' , label = 'U ON' ) \n        plt . plot ( freqs , U_OFF , 'g--' , label = 'U OFF' ) \n        plt . plot ( freqs , V_ON , 'm-' , label = 'V ON' ) \n        plt . plot ( freqs , V_OFF , 'm--' , label = 'V OFF' ) \n    plt . legend ( ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . title ( 'Uncalibrated Full Stokes Noise Diode Spectrum' ) \n    plt . ylabel ( 'Power (Counts)' ) "}
{"7822": "\ndef plot_gain_offsets ( dio_cross , dio_chan_per_coarse = 8 , feedtype = 'l' , ax1 = None , ax2 = None , legend = 1 , ** kwargs ) : \n    Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    obs = Waterfall ( dio_cross , max_load = 150 ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    coarse_G = gain_offsets ( I , Q , U , V , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    coarse_freqs = convert_to_coarse ( freqs , dio_chan_per_coarse ) \n    XX_OFF , XX_ON = foldcal ( np . expand_dims ( data [ : , 0 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    YY_OFF , YY_ON = foldcal ( np . expand_dims ( data [ : , 1 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    if ax1 == None : \n        plt . subplot ( 211 ) \n    else : \n        axG = plt . axes ( ax1 ) \n        plt . setp ( axG . get_xticklabels ( ) , visible = 0 ) \n    plt . plot ( coarse_freqs , coarse_G , 'ko' , markersize = 2 ) \n    plt . ylabel ( r'$\\frac{\\Delta G}{2}$' , rotation = 90 ) \n    if feedtype == 'l' : \n        plt . title ( 'XY Gain Difference' ) \n    if feedtype == 'c' : \n        plt . title ( 'LR Gain Difference' ) \n    plt . grid ( 1 ) \n    if ax2 == None : \n        plt . subplot ( 212 ) \n    else : \n        axXY = plt . axes ( ax2 , sharex = axG ) \n    if feedtype == 'l' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'XX' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'YY' ) \n    if feedtype == 'c' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'LL' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'RR' ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . ylabel ( 'Power (Counts)' ) \n    if legend == 1 : \n        plt . legend ( ) "}
{"7823": "\ndef open_file ( filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = 1 , max_load = 1. ) : \n    if not os . path . isfile ( filename ) : \n        type ( filename ) \n        print ( filename ) \n        raise IOError ( \"No such file or directory: \" + filename ) \n    filename = os . path . expandvars ( os . path . expanduser ( filename ) ) \n    ext = filename . split ( \".\" ) [ - 1 ] . strip ( ) . lower ( ) \n    if six . PY3 : \n        ext = bytes ( ext , 'ascii' ) \n    if h5py . is_hdf5 ( filename ) : \n        return H5Reader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    elif sigproc . is_filterbank ( filename ) : \n        return FilReader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    else : \n        raise NotImplementedError ( 'Cannot open this type of file with Waterfall' ) "}
{"7824": "\ndef _setup_selection_range ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , init = 0 ) : \n    if init is 1 : \n        if t_start is None : \n            t_start = self . t_begin \n        if t_stop is None : \n            t_stop = self . t_end \n        if f_start is None : \n            f_start = self . f_begin \n        if f_stop is None : \n            f_stop = self . f_end \n    else : \n        if f_start is None : \n            f_start = self . f_start \n        if f_stop is None : \n            f_stop = self . f_stop \n        if t_start is None : \n            t_start = self . t_start \n        if t_stop is None : \n            t_stop = self . t_stop \n    if t_stop >= 0 and t_start >= 0 and t_stop < t_start : \n        t_stop , t_start = t_start , t_stop \n        logger . warning ( 'Given t_stop < t_start, assuming reversed values.' ) \n    if f_stop and f_start and f_stop < f_start : \n        f_stop , f_start = f_start , f_stop \n        logger . warning ( 'Given f_stop < f_start, assuming reversed values.' ) \n    if t_start >= self . t_begin and t_start < self . t_end : \n        self . t_start = int ( t_start ) \n    else : \n        if init is 0 or t_start != None : \n            logger . warning ( 'Setting t_start = %f, since t_start not given or not valid.' % self . t_begin ) \n        self . t_start = self . t_begin \n    if t_stop <= self . t_end and t_stop > self . t_begin : \n        self . t_stop = int ( t_stop ) \n    else : \n        if init is 0 or t_stop : \n            logger . warning ( 'Setting t_stop = %f, since t_stop not given or not valid.' % self . t_end ) \n        self . t_stop = self . t_end \n    if f_start >= self . f_begin and f_start < self . f_end : \n        self . f_start = f_start \n    else : \n        if init is 0 or f_start : \n            logger . warning ( 'Setting f_start = %f, since f_start not given or not valid.' % self . f_begin ) \n        self . f_start = self . f_begin \n    if f_stop <= self . f_end and f_stop > self . f_begin : \n        self . f_stop = f_stop \n    else : \n        if init is 0 or f_stop : \n            logger . warning ( 'Setting f_stop = %f, since f_stop not given or not valid.' % self . f_end ) \n        self . f_stop = self . f_end \n    self . selection_shape = self . _calc_selection_shape ( ) "}
{"7829": "\ndef populate_timestamps ( self , update_header = 0 ) : \n    ii_start , ii_stop = 0 , self . n_ints_in_file \n    if self . t_start : \n        ii_start = self . t_start \n    if self . t_stop : \n        ii_stop = self . t_stop \n    t0 = self . header [ b'tstart' ] \n    t_delt = self . header [ b'tsamp' ] \n    if update_header : \n        timestamps = ii_start * t_delt / 24. / 60. / 60. + t0 \n    else : \n        timestamps = np . arange ( ii_start , ii_stop ) * t_delt / 24. / 60. / 60. + t0 \n    return timestamps "}
{"7833": "\ndef isheavy ( self ) : \n    selection_size_bytes = self . _calc_selection_size ( ) \n    if selection_size_bytes > self . MAX_DATA_ARRAY_SIZE : \n        return 1 \n    else : \n        return 0 "}
{"7835": "\ndef read_all ( self , reverse = 1 ) : \n    raise NotImplementedError ( 'To be implemented' ) \n    self . filfile . seek ( int ( self . datastart ) ) \n    data = np . fromfile ( self . filfile , dtype = self . dtype ) . reshape ( self . blocksize , self . channels ) \n    if reverse : \n        data = data [ : , : : - 1 ] \n    return data "}
{"7836": "\ndef read_row ( self , rownumber , reverse = 1 ) : \n    raise NotImplementedError ( 'To be implemented' ) \n    self . filfile . seek ( int ( self . datastart + self . channels * rownumber * ( int ( self . nbits / 8 ) ) ) ) \n    data = np . fromfile ( self . filfile , count = self . channels , dtype = self . dtype ) . reshape ( 1 , self . channels ) \n    if reverse : \n        data = data [ : , : : - 1 ] \n    return data "}
{"7838": "\ndef __update_header ( self ) : \n    if self . header [ b'foff' ] < 0 : \n        self . header [ b'fch1' ] = self . container . f_stop \n    else : \n        self . header [ b'fch1' ] = self . container . f_start \n    self . header [ b'nchans' ] = self . container . selection_shape [ self . freq_axis ] \n    self . header [ b'tstart' ] = self . container . populate_timestamps ( update_header = 1 ) "}
{"7848": "\ndef find_n_data_blocks ( self ) : \n    self . file_obj . seek ( 0 ) \n    header0 , data_idx0 = self . read_header ( ) \n    self . file_obj . seek ( data_idx0 ) \n    block_size = int ( header0 [ 'BLOCSIZE' ] ) \n    n_bits = int ( header0 [ 'NBITS' ] ) \n    self . file_obj . seek ( int ( header0 [ 'BLOCSIZE' ] ) , 1 ) \n    n_blocks = 1 \n    end_found = 0 \n    while not end_found : \n        try : \n            header , data_idx = self . read_header ( ) \n            self . file_obj . seek ( data_idx ) \n            self . file_obj . seek ( header [ 'BLOCSIZE' ] , 1 ) \n            n_blocks += 1 \n        except EndOfFileError : \n            end_found = 1 \n            break \n    self . file_obj . seek ( 0 ) \n    return n_blocks "}
{"7855": "\ndef foldcal ( data , tsamp , diode_p = 0.04 , numsamps = 1000 , switch = 0 , inds = 0 ) : \n    halfper = diode_p / 2.0 \n    foldt = halfper / tsamp \n    onesec = 1 / tsamp \n    ints = np . arange ( 0 , numsamps ) \n    t_switch = ( onesec + ints * foldt ) \n    t_switch = t_switch . astype ( 'int' ) \n    ONints = np . array ( np . reshape ( t_switch [ : ] , ( numsamps / 2 , 2 ) ) ) \n    ONints [ : , 0 ] = ONints [ : , 0 ] + 1 \n    OFFints = np . array ( np . reshape ( t_switch [ 1 : - 1 ] , ( numsamps / 2 - 1 , 2 ) ) ) \n    OFFints [ : , 0 ] = OFFints [ : , 0 ] + 1 \n    av_ON = [ ] \n    av_OFF = [ ] \n    for i in ONints : \n        if i [ 1 ] != i [ 0 ] : \n            av_ON . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) \n    for i in OFFints : \n        if i [ 1 ] != i [ 0 ] : \n            av_OFF . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) \n    if switch == 0 : \n        if inds == 0 : \n            return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) \n        else : \n            return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , ONints , OFFints \n    if switch == 1 : \n        if inds == 0 : \n            return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) \n        else : \n            return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , OFFints , ONints "}
{"7856": "\ndef integrate_calib ( name , chan_per_coarse , fullstokes = 0 , ** kwargs ) : \n    obs = Waterfall ( name , max_load = 150 ) \n    data = obs . data \n    if fullstokes == 0 and data . shape [ 1 ] > 1 : \n        data = data [ : , 0 , : ] + data [ : , 1 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    if fullstokes == 1 : \n        data = data [ : , 0 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    tsamp = obs . header [ 'tsamp' ] \n    OFF , ON = foldcal ( data , tsamp , ** kwargs ) \n    freqs = obs . populate_freqs ( ) \n    ON_int = integrate_chans ( ON , freqs , chan_per_coarse ) \n    OFF_int = integrate_chans ( OFF , freqs , chan_per_coarse ) \n    if np . sum ( ON_int ) < np . sum ( OFF_int ) : \n        temp = ON_int \n        ON_int = OFF_int \n        OFF_int = temp \n    return OFF_int , ON_int "}
{"7857": "\ndef get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) : \n    const = calflux / np . power ( calfreq , spec_in ) \n    if oneflux == 0 : \n        return const * np . power ( centerfreqs , spec_in ) \n    else : \n        return const * np . power ( np . mean ( centerfreqs ) , spec_in ) "}
{"7860": "\ndef diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = 1 , oneflux = 0 , ** kwargs ) : \n    obs = Waterfall ( calON_obs , max_load = 150 ) \n    freqs = obs . populate_freqs ( ) \n    ncoarse = obs . calc_n_coarse_chan ( ) \n    nchans = obs . header [ 'nchans' ] \n    chan_per_coarse = nchans / ncoarse \n    f_ON , f_OFF = f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) \n    centerfreqs = get_centerfreqs ( freqs , chan_per_coarse ) \n    calfluxes = get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) \n    C_o = calfluxes / ( 1 / f_ON - 1 / f_OFF ) \n    Tsys = C_o / f_OFF \n    if average == 1 : \n        return np . mean ( C_o ) , np . mean ( Tsys ) \n    else : \n        return C_o , Tsys "}
{"7861": "\ndef get_Tsys ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , oneflux = 0 , ** kwargs ) : \n    return diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = 0 , oneflux = 0 , ** kwargs ) [ 1 ] "}
{"7862": "\ndef calibrate_fluxes ( main_obs_name , dio_name , dspec , Tsys , fullstokes = 0 , ** kwargs ) : \n    main_obs = Waterfall ( main_obs_name , max_load = 150 ) \n    ncoarse = main_obs . calc_n_coarse_chan ( ) \n    dio_obs = Waterfall ( dio_name , max_load = 150 ) \n    dio_chan_per_coarse = dio_obs . header [ 'nchans' ] / ncoarse \n    dOFF , dON = integrate_calib ( dio_name , dio_chan_per_coarse , fullstokes , ** kwargs ) \n    main_dat = main_obs . data \n    scale_facs = dspec / ( dON - dOFF ) \n    print ( scale_facs ) \n    nchans = main_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = nchans / ncoarse \n    ax0_size = np . size ( main_dat , 0 ) \n    ax1_size = np . size ( main_dat , 1 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , ncoarse , obs_chan_per_coarse ) ) \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = main_dat * scale_facs \n    main_dat = main_dat - Tsys \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , nchans ) ) \n    main_obs . data = main_dat \n    main_obs . write_to_filterbank ( main_obs_name [ : - 4 ] + '.fluxcal.fil' ) \n    print ( 'Finished: calibrated product written to ' + main_obs_name [ : - 4 ] + '.fluxcal.fil' ) "}
{"7863": "\ndef len_header ( filename ) : \n    with open ( filename , 'rb' ) as f : \n        header_sub_count = 0 \n        eoh_found = 0 \n        while not eoh_found : \n            header_sub = f . read ( 512 ) \n            header_sub_count += 1 \n            if b'HEADER_END' in header_sub : \n                idx_end = header_sub . index ( b'HEADER_END' ) + len ( b'HEADER_END' ) \n                eoh_found = 1 \n                break \n        idx_end = ( header_sub_count - 1 ) * 512 + idx_end \n    return idx_end "}
{"7864": "\ndef is_filterbank ( filename ) : \n    with open ( filename , 'rb' ) as fh : \n        is_fil = 1 \n        try : \n            keyword , value , idx = read_next_header_keyword ( fh ) \n            try : \n                assert keyword == b'HEADER_START' \n            except AssertionError : \n                is_fil = 0 \n        except KeyError : \n            is_fil = 0 \n        return is_fil "}
{"7865": "\ndef fix_header ( filename , keyword , new_value ) : \n    hd = read_header ( filename ) \n    hi = read_header ( filename , return_idxs = 1 ) \n    idx = hi [ keyword ] \n    dtype = header_keyword_types [ keyword ] \n    dtype_to_type = { b'<l' : np . int32 , b'str' : bytes , b'<d' : np . float64 , b'angle' : to_sigproc_angle } \n    value_dtype = dtype_to_type [ dtype ] \n    if isinstance ( value_dtype , bytes ) : \n        if len ( hd [ keyword ] ) == len ( new_value ) : \n            val_str = np . int32 ( len ( new_value ) ) . tostring ( ) + new_value \n        else : \n            raise RuntimeError ( \"String size mismatch. Cannot update without rewriting entire file.\" ) \n    else : \n        val_str = value_dtype ( new_value ) . tostring ( ) \n    with open ( filename , 'rb+' ) as fh : \n        fh . seek ( idx ) \n        fh . write ( val_str ) "}
{"7875": "\ndef parse_lines ( text , ignore_invalid = 0 ) : \n    json_zone_file = defaultdict ( list ) \n    record_lines = text . split ( \"\\n\" ) \n    parser = make_parser ( ) \n    for record_line in record_lines : \n        record_token = tokenize_line ( record_line ) \n        try : \n            json_zone_file = parse_line ( parser , record_token , json_zone_file ) \n        except InvalidLineException : \n            if ignore_invalid : \n                continue \n            else : \n                raise \n    return json_zone_file "}
{"7876": "\ndef parse_zone_file ( text , ignore_invalid = 0 ) : \n    text = remove_comments ( text ) \n    text = flatten ( text ) \n    text = remove_class ( text ) \n    text = add_default_name ( text ) \n    json_zone_file = parse_lines ( text , ignore_invalid = ignore_invalid ) \n    return json_zone_file "}
{"7885": "\ndef add_record ( self , schema , _bump_stack_level = 0 ) : \n    full_name = get_full_name ( schema ) \n    has_namespace = '.' in full_name \n    self . _force_add ( full_name , schema , _bump_stack_level , _raise_on_existing = has_namespace ) \n    if has_namespace and schema . __name__ not in self . _schema_map : \n        self . _force_add ( schema . __name__ , schema , _bump_stack_level ) \n    return schema "}
{"7889": "\ndef from_class ( metacls , cls , auto_store = 1 ) : \n    if auto_store : \n        def wrap ( cls ) : \n            return cls \n    else : \n        wrap = no_auto_store ( ) \n    return wrap ( metacls . __new__ ( metacls , cls . __name__ , ( Record , ) , dict ( cls . __dict__ ) ) ) "}
{"7890": "\ndef get_schema_dict ( record , state = None ) : \n    state = state or SchemaGeneratorState ( ) \n    schema = OrderedDict ( [ ( 'type' , 'object' ) , ( 'id' , record . _schema_name ) , ] ) \n    fields = dict ( ) \n    for field_name , field_type in record . _fields . iteritems ( ) : \n        fields [ field_name ] = field_type . jsonschema_type_schema ( state ) \n    required = set ( fields . keys ( ) ) \n    schema [ 'properties' ] = fields \n    schema [ 'required' ] = sorted ( list ( required ) ) \n    schema [ 'additionalProperties' ] = 0 \n    state . record_schemas [ record . _schema_name ] = schema \n    return schema "}
{"7897": "\ndef all_include_attributes ( self , attributes ) : \n    self . reload ( expand = 1 , attributes = attributes ) \n    entities = [ Entity ( self , r , attributes = attributes ) for r in self . _resources ] \n    self . reload ( ) \n    return entities "}
{"7898": "\ndef _get_entity_from_href ( self , result ) : \n    href_result = result [ 'href' ] \n    if self . collection . _href . startswith ( href_result ) : \n        return Entity ( self . collection , result , incomplete = 1 ) \n    href_match = re . match ( r\"(https?://.+/api[^?]*)/([a-z_-]+)\" , href_result ) \n    if not href_match : \n        raise ValueError ( \"Malformed href: {}\" . format ( href_result ) ) \n    collection_name = href_match . group ( 2 ) \n    entry_point = href_match . group ( 1 ) \n    new_collection = Collection ( self . collection . api , \"{}/{}\" . format ( entry_point , collection_name ) , collection_name ) \n    return Entity ( new_collection , result , incomplete = 1 ) "}
{"7914": "\ndef makePlot ( pdf = 0 , png = 0 ) : \n    logdistancekpc = np . linspace ( - 1 , np . log10 ( 20.0 ) , 100 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    lines = { } \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ 0 ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( vmag > 14 ) & ( vmag < 16 ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ 1 ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ 1 ] ) \n        relparerrors = parerrors * 10 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10 ** logdistancekpc , relparerrors , '--k' , lw = 1 ) \n        plt . loglog ( 10 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n    else : \n        plt . show ( ) "}
{"7917": "\ndef _helpful_failure ( method ) : \n    \n    @ wraps ( method ) \n    def wrapper ( self , val ) : \n        try : \n            return method ( self , val ) \n        except : \n            exc_cls , inst , tb = sys . exc_info ( ) \n            if hasattr ( inst , '_RERAISE' ) : \n                _ , expr , _ , inner_val = Q . __debug_info__ \n                Q . __debug_info__ = QDebug ( self , expr , val , inner_val ) \n                raise \n            if issubclass ( exc_cls , KeyError ) : \n                exc_cls = QKeyError \n            prettyval = repr ( val ) \n            if len ( prettyval ) > 150 : \n                prettyval = \"<%s instance>\" % ( type ( val ) . __name__ ) \n            msg = \"{0}\\n\\n\\tEncountered when evaluating {1}{2}\" . format ( inst , prettyval , self ) \n            new_exc = exc_cls ( msg ) \n            new_exc . _RERAISE = 1 \n            Q . __debug_info__ = QDebug ( self , self , val , val ) \n            six . reraise ( exc_cls , new_exc , tb ) \n    return wrapper "}
{"7934": "\ndef clean_code ( code , comments = 1 , macros = 0 , pragmas = 0 ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = 0 \n        in_pragma = 0 \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while idx < len ( code ) - 1 : \n            if comment_start is None and code [ idx : idx + 2 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            elif comment_start is None and code [ idx : idx + 2 ] == '/*' : \n                comment_start = idx \n            elif comment_start is not None and code [ idx : idx + 2 ] == '*/' : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                idx -= idx - comment_start \n                comment_start = None \n            idx += 1 \n    return code "}
{"7945": "\ndef userselect_block ( blocks , default = None , debug = 0 ) : \n    print ( \"Blocks found in assembly file:\" ) \n    print ( \"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) \n    for idx , b in blocks : \n        print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) \n        if debug : \n            ln = b [ 'first_line' ] \n            print ( ' ' * 4 + 'Code:' ) \n            for l in b [ 'lines' ] : \n                print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) \n                ln += 1 \n            print ( ' ' * 4 + 'Metadata:' ) \n            print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) \n    block_idx = - 1 \n    while not ( 0 <= block_idx < len ( blocks ) ) : \n        block_idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - 1 \n    return block_idx "}
{"7947": "\ndef iaca_instrumentation ( input_file , output_file , block_selection = 'auto' , pointer_increment = 'auto_with_manual_fallback' , debug = 0 ) : \n    assembly_orig = input_file . readlines ( ) \n    if input_file is output_file : \n        output_file . seek ( 0 ) \n        output_file . truncate ( ) \n    if debug : \n        block_selection = 'manual' \n    assembly = strip_and_uncomment ( copy ( assembly_orig ) ) \n    assembly = strip_unreferenced_labels ( assembly ) \n    blocks = find_asm_blocks ( assembly ) \n    if block_selection == 'auto' : \n        block_idx = select_best_block ( blocks ) \n    elif block_selection == 'manual' : \n        block_idx = userselect_block ( blocks , default = select_best_block ( blocks ) , debug = debug ) \n    elif isinstance ( block_selection , int ) : \n        block_idx = block_selection \n    else : \n        raise ValueError ( \"block_selection has to be an integer, 'auto' or 'manual' \" ) \n    block = blocks [ block_idx ] [ 1 ] \n    if pointer_increment == 'auto' : \n        if block [ 'pointer_increment' ] is None : \n            raise RuntimeError ( \"pointer_increment could not be detected automatically. Use \" \"--pointer-increment to set manually to byte offset of store \" \"pointer address between consecutive assembly block iterations.\" ) \n    elif pointer_increment == 'auto_with_manual_fallback' : \n        if block [ 'pointer_increment' ] is None : \n            block [ 'pointer_increment' ] = userselect_increment ( block ) \n    elif pointer_increment == 'manual' : \n        block [ 'pointer_increment' ] = userselect_increment ( block ) \n    elif isinstance ( pointer_increment , int ) : \n        block [ 'pointer_increment' ] = pointer_increment \n    else : \n        raise ValueError ( \"pointer_increment has to be an integer, 'auto', 'manual' or  \" \"'auto_with_manual_fallback' \" ) \n    instrumented_asm = insert_markers ( assembly_orig , block [ 'first_line' ] , block [ 'last_line' ] ) \n    output_file . writelines ( instrumented_asm ) \n    return block "}
{"7950": "\ndef space ( start , stop , num , endpoint = 1 , log = 0 , base = 10 ) : \n    assert type ( start ) is int and type ( stop ) is int and type ( num ) is int , \"start, stop and num need to be intergers\" \n    assert num >= 2 , \"num has to be atleast 2\" \n    if log : \n        start = math . log ( start , base ) \n        stop = math . log ( stop , base ) \n    if endpoint : \n        step_length = float ( ( stop - start ) ) / float ( num - 1 ) \n    else : \n        step_length = float ( ( stop - start ) ) / float ( num ) \n    i = 0 \n    while i < num : \n        if log : \n            yield int ( round ( base ** ( start + i * step_length ) ) ) \n        else : \n            yield int ( round ( start + i * step_length ) ) \n        i += 1 "}
{"7955": "\ndef symbol_pos_int ( * args , ** kwargs ) : \n    kwargs . update ( { 'positive' : 1 , 'integer' : 1 } ) \n    return sympy . Symbol ( * args , ** kwargs ) "}
{"7963": "\ndef array_sizes ( self , in_bytes = 0 , subs_consts = 0 ) : \n    var_sizes = { } \n    for var_name , var_info in self . variables . items ( ) : \n        var_type , var_size = var_info \n        if var_size is None : \n            continue \n        var_sizes [ var_name ] = reduce ( operator . mul , var_size , 1 ) \n        if in_bytes : \n            element_size = self . datatypes_size [ var_type ] \n            var_sizes [ var_name ] *= element_size \n    if subs_consts : \n        return { k : self . subs_consts ( v ) for k , v in var_sizes . items ( ) } \n    else : \n        return var_sizes "}
{"7967": "\ndef get_loop_stack ( self , subs_consts = 0 ) : \n    for l in self . _loop_stack : \n        if subs_consts : \n            yield { 'index' : l [ 0 ] , 'start' : self . subs_consts ( l [ 1 ] ) , 'stop' : self . subs_consts ( l [ 2 ] ) , 'increment' : self . subs_consts ( l [ 3 ] ) } \n        else : \n            yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] } "}
{"7968": "\ndef index_order ( self , sources = 1 , destinations = 1 ) : \n    if sources : \n        arefs = chain ( * self . sources . values ( ) ) \n    else : \n        arefs = [ ] \n    if destinations : \n        arefs = chain ( arefs , * self . destinations . values ( ) ) \n    ret = [ ] \n    for a in [ aref for aref in arefs if aref is not None ] : \n        ref = [ ] \n        for expr in a : \n            ref . append ( expr . free_symbols ) \n        ret . append ( ref ) \n    return ret "}
{"7969": "\ndef compile_sympy_accesses ( self , sources = 1 , destinations = 1 ) : \n    sympy_accesses = defaultdict ( list ) \n    for var_name in self . variables : \n        if sources : \n            for r in self . sources . get ( var_name , [ ] ) : \n                if r is None : \n                    continue \n                sympy_accesses [ var_name ] . append ( self . access_to_sympy ( var_name , r ) ) \n        if destinations : \n            for w in self . destinations . get ( var_name , [ ] ) : \n                if w is None : \n                    continue \n                sympy_accesses [ var_name ] . append ( self . access_to_sympy ( var_name , w ) ) \n    return sympy_accesses "}
{"7983": "\ndef _build_const_declartions ( self , with_init = 1 ) : \n    decls = [ ] \n    index_type = self . get_index_type ( ) \n    i = 2 \n    for k in self . constants : \n        type_decl = c_ast . TypeDecl ( k . name , [ 'const' ] , c_ast . IdentifierType ( index_type ) ) \n        init = None \n        if with_init : \n            init = c_ast . FuncCall ( c_ast . ID ( 'atoi' ) , c_ast . ExprList ( [ c_ast . ArrayRef ( c_ast . ID ( 'argv' ) , c_ast . Constant ( 'int' , str ( i ) ) ) ] ) ) \n        i += 1 \n        decls . append ( c_ast . Decl ( k . name , [ 'const' ] , [ ] , [ ] , type_decl , init , None ) ) \n    return decls "}
{"7986": "\ndef _build_array_declarations ( self , with_init = 1 ) : \n    array_declarations = deepcopy ( self . get_array_declarations ( ) ) \n    array_dict = [ ] \n    for d in array_declarations : \n        array_dict . append ( transform_multidim_to_1d_decl ( d ) ) \n        transform_array_decl_to_malloc ( d , with_init = with_init ) \n    return array_declarations , dict ( array_dict ) "}
{"7990": "\ndef _build_kernel_function_declaration ( self , name = 'kernel' ) : \n    array_declarations , array_dimensions = self . _build_array_declarations ( with_init = 0 ) \n    scalar_declarations = self . _build_scalar_declarations ( with_init = 0 ) \n    const_declarations = self . _build_const_declartions ( with_init = 0 ) \n    return c_ast . FuncDecl ( args = c_ast . ParamList ( params = array_declarations + scalar_declarations + const_declarations ) , type = c_ast . TypeDecl ( declname = name , quals = [ ] , type = c_ast . IdentifierType ( names = [ 'void' ] ) ) ) "}
{"7991": "\ndef _build_scalar_declarations ( self , with_init = 1 ) : \n    scalar_declarations = [ deepcopy ( d ) for d in self . kernel_ast . block_items if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . TypeDecl ] \n    if with_init : \n        random . seed ( 2342 ) \n        for d in scalar_declarations : \n            if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : \n                d . init = c_ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) \n            elif d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : \n                d . init = c_ast . Constant ( 'int' , 2 ) \n    return scalar_declarations "}
{"7992": "\ndef get_kernel_code ( self , openmp = 0 , as_filename = 0 , name = 'kernel' ) : \n    assert self . kernel_ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" \n    file_name = 'kernel' \n    if openmp : \n        file_name += '-omp' \n    file_name += '.c' \n    fp , already_available = self . _get_intermediate_file ( file_name , machine_and_compiler_dependent = 0 ) \n    if already_available : \n        code = fp . read ( ) \n    else : \n        array_declarations , array_dimensions = self . _build_array_declarations ( ) \n        if openmp : \n            kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) \n            for aref in find_node_type ( kernel , c_ast . ArrayRef ) : \n                transform_multidim_to_1d_ref ( aref , array_dimensions ) \n            omp_pragmas = [ p for p in find_node_type ( kernel , c_ast . Pragma ) if 'omp' in p . string ] \n            if not omp_pragmas : \n                kernel . insert ( 0 , c_ast . Pragma ( \"omp for\" ) ) \n        else : \n            kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) \n            for aref in find_node_type ( kernel , c_ast . ArrayRef ) : \n                transform_multidim_to_1d_ref ( aref , array_dimensions ) \n        function_ast = c_ast . FuncDef ( decl = c_ast . Decl ( name = name , type = self . _build_kernel_function_declaration ( name = name ) , quals = [ ] , storage = [ ] , funcspec = [ ] , init = None , bitsize = None ) , body = c_ast . Compound ( block_items = kernel ) , param_decls = None ) \n        code = CGenerator ( ) . visit ( function_ast ) \n        code = '#include \"kerncraft.h\"\\n\\n' + code \n        fp . write ( code ) \n    fp . close ( ) \n    if as_filename : \n        return fp . name \n    else : \n        return code "}
{"7994": "\ndef get_main_code ( self , as_filename = 0 , kernel_function_name = 'kernel' ) : \n    assert self . kernel_ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" \n    fp , already_available = self . _get_intermediate_file ( 'main.c' , machine_and_compiler_dependent = 0 ) \n    if already_available : \n        code = fp . read ( ) \n    else : \n        parser = CParser ( ) \n        template_code = self . CODE_TEMPLATE \n        template_ast = parser . parse ( clean_code ( template_code , macros = 1 , comments = 1 , pragmas = 0 ) ) \n        ast = deepcopy ( template_ast ) \n        replace_id ( ast , \"DECLARE_CONSTS\" , self . _build_const_declartions ( with_init = 1 ) ) \n        array_declarations , array_dimensions = self . _build_array_declarations ( ) \n        replace_id ( ast , \"DECLARE_ARRAYS\" , array_declarations ) \n        replace_id ( ast , \"DECLARE_INIT_SCALARS\" , self . _build_scalar_declarations ( ) ) \n        replace_id ( ast , \"DUMMY_CALLS\" , self . _build_dummy_calls ( ) ) \n        ast . ext . insert ( 0 , self . _build_kernel_function_declaration ( name = kernel_function_name ) ) \n        replace_id ( ast , \"KERNEL_CALL\" , self . _build_kernel_call ( ) ) \n        replace_id ( ast , \"INIT_ARRAYS\" , self . _build_array_initializations ( array_dimensions ) ) \n        code = CGenerator ( ) . visit ( ast ) \n        code = '\\n' . join ( [ l for l in template_code . split ( '\\n' ) if l . startswith ( \"#include\" ) ] ) + '\\n\\n' + code \n        fp . write ( code ) \n    fp . close ( ) \n    if as_filename : \n        return fp . name \n    else : \n        return code "}
{"7995": "\ndef iaca_analysis ( self , micro_architecture , asm_block = 'auto' , pointer_increment = 'auto_with_manual_fallback' , verbose = 0 ) : \n    asm_filename = self . compile_kernel ( assembly = 1 , verbose = verbose ) \n    asm_marked_filename = os . path . splitext ( asm_filename ) [ 0 ] + '-iaca.s' \n    with open ( asm_filename , 'r' ) as in_file , open ( asm_marked_filename , 'w' ) as out_file : \n        self . asm_block = iaca . iaca_instrumentation ( in_file , out_file , block_selection = asm_block , pointer_increment = pointer_increment ) \n    obj_name = self . assemble_to_object ( asm_marked_filename , verbose = verbose ) \n    return iaca . iaca_analyse_instrumented_binary ( obj_name , micro_architecture ) , self . asm_block "}
{"7996": "\ndef build_executable ( self , lflags = None , verbose = 0 , openmp = 0 ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = 1 , fp = 0 ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = 1 ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            print ( 'Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) \n            sys . exit ( 1 ) \n        compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] \n        if os . environ . get ( 'LIKWID_LIB' ) == '' : \n            compiler_args = compiler_args [ : - 1 ] \n        if lflags is None : \n            lflags = [ ] \n        lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            print ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            print ( \"Build failed:\" , e , file = sys . stderr ) \n            sys . exit ( 1 ) \n    else : \n        if verbose : \n            print ( 'Executing (build_executable): ' , 'using cached' , out_filename ) \n    return out_filename "}
{"8006": "\ndef _align_iteration_with_cl_boundary ( self , iteration , subtract = 1 ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    cacheline_size = self . machine [ 'cacheline size' ] \n    elements_per_cacheline = int ( cacheline_size // element_size ) \n    inner_loop = list ( self . kernel . get_loop_stack ( subs_consts = 1 ) ) [ - 1 ] \n    inner_increment = inner_loop [ 'increment' ] \n    o = self . kernel . compile_global_offsets ( iteration = iteration ) [ 0 ] \n    if len ( o [ 1 ] ) : \n        first_offset = min ( o [ 1 ] ) \n    else : \n        first_offset = min ( o [ 0 ] ) \n    diff = first_offset - ( int ( first_offset ) >> self . csim . first_level . cl_bits << self . csim . first_level . cl_bits ) \n    if diff == 0 : \n        return iteration \n    elif subtract : \n        return iteration - ( diff // element_size ) // inner_increment \n    else : \n        return iteration + ( elements_per_cacheline - diff // element_size ) // inner_increment "}
{"8016": "\ndef parse_description ( ) : \n    from os . path import dirname , join , exists \n    readme_fpath = join ( dirname ( __file__ ) , 'README.md' ) \n    if exists ( readme_fpath ) : \n        textlines = [ ] \n        with open ( readme_fpath , 'r' ) as f : \n            capture = 0 \n            for line in f . readlines ( ) : \n                if '# Purpose' in line : \n                    capture = 1 \n                elif line . startswith ( '##' ) : \n                    break \n                elif capture : \n                    textlines += [ line ] \n        text = '' . join ( textlines ) . strip ( ) \n        text = text . replace ( '\\n\\n' , '_NLHACK_' ) \n        text = text . replace ( '\\n' , ' ' ) \n        text = text . replace ( '_NLHACK_' , '\\n\\n' ) \n        return text \n    return '' "}
{"8019": "\ndef _record_purchase ( sailthru_client , email , item , purchase_incomplete , message_id , options ) : \n    try : \n        sailthru_response = sailthru_client . purchase ( email , [ item ] , incomplete = purchase_incomplete , message_id = message_id , options = options ) \n        if not sailthru_response . is_ok ( ) : \n            error = sailthru_response . get_error ( ) \n            logger . error ( \"Error attempting to record purchase in Sailthru: %s\" , error . get_message ( ) ) \n            return not can_retry_sailthru_request ( error ) \n    except SailthruClientError as exc : \n        logger . exception ( \"Exception attempting to record purchase for %s in Sailthru - %s\" , email , text_type ( exc ) ) \n        return 0 \n    return 1 "}
{"8021": "\ndef _get_course_content_from_ecommerce ( course_id , site_code = None ) : \n    api = get_ecommerce_client ( site_code = site_code ) \n    try : \n        api_response = api . courses ( course_id ) . get ( ) \n    except Exception : \n        logger . exception ( 'An error occurred while retrieving data for course run [%s] from the Catalog API.' , course_id , exc_info = 1 ) \n        return { } \n    return { 'title' : api_response . get ( 'name' ) , 'verification_deadline' : api_response . get ( 'verification_deadline' ) } "}
{"8022": "\ndef _update_unenrolled_list ( sailthru_client , email , course_url , unenroll ) : \n    try : \n        sailthru_response = sailthru_client . api_get ( \"user\" , { \"id\" : email , \"fields\" : { \"vars\" : 1 } } ) \n        if not sailthru_response . is_ok ( ) : \n            error = sailthru_response . get_error ( ) \n            logger . error ( \"Error attempting to read user record from Sailthru: %s\" , error . get_message ( ) ) \n            return not can_retry_sailthru_request ( error ) \n        response_json = sailthru_response . json \n        unenroll_list = [ ] \n        if response_json and \"vars\" in response_json and response_json [ \"vars\" ] and \"unenrolled\" in response_json [ \"vars\" ] : \n            unenroll_list = response_json [ \"vars\" ] [ \"unenrolled\" ] \n        changed = 0 \n        if unenroll : \n            if course_url not in unenroll_list : \n                unenroll_list . append ( course_url ) \n                changed = 1 \n        elif course_url in unenroll_list : \n            unenroll_list . remove ( course_url ) \n            changed = 1 \n        if changed : \n            sailthru_response = sailthru_client . api_post ( 'user' , { 'id' : email , 'key' : 'email' , 'vars' : { 'unenrolled' : unenroll_list } } ) \n            if not sailthru_response . is_ok ( ) : \n                error = sailthru_response . get_error ( ) \n                logger . error ( \"Error attempting to update user record in Sailthru: %s\" , error . get_message ( ) ) \n                return not can_retry_sailthru_request ( error ) \n        return 1 \n    except SailthruClientError as exc : \n        logger . exception ( \"Exception attempting to update user record for %s in Sailthru - %s\" , email , text_type ( exc ) ) \n        return 0 "}
{"8025": "\ndef get_logger_config ( log_dir = '/var/tmp' , logging_env = 'no_env' , edx_filename = 'edx.log' , dev_env = 0 , debug = 0 , local_loglevel = 'INFO' , service_variant = 'ecomworker' ) : \n    if local_loglevel not in [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ] : \n        local_loglevel = 'INFO' \n    hostname = platform . node ( ) . split ( '.' ) [ 0 ] \n    syslog_format = ( '[service_variant={service_variant}]' '[%(name)s][env:{logging_env}] %(levelname)s ' '[{hostname}  %(process)d] [%(filename)s:%(lineno)d] ' '- %(message)s' ) . format ( service_variant = service_variant , logging_env = logging_env , hostname = hostname ) \n    if debug : \n        handlers = [ 'console' ] \n    else : \n        handlers = [ 'local' ] \n    logger_config = { 'version' : 1 , 'disable_existing_loggers' : 0 , 'formatters' : { 'standard' : { 'format' : '%(asctime)s %(levelname)s %(process)d ' '[%(name)s] %(filename)s:%(lineno)d - %(message)s' , } , 'syslog_format' : { 'format' : syslog_format } , 'raw' : { 'format' : '%(message)s' } , } , 'handlers' : { 'console' : { 'level' : 'DEBUG' if debug else 'INFO' , 'class' : 'logging.StreamHandler' , 'formatter' : 'standard' , 'stream' : sys . stdout , } , } , 'loggers' : { 'requests' : { 'handlers' : handlers , 'level' : 'WARNING' , 'propagate' : 1 } , '' : { 'handlers' : handlers , 'level' : 'DEBUG' , 'propagate' : 0 } , } } \n    if dev_env : \n        edx_file_loc = os . path . join ( log_dir , edx_filename ) \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'class' : 'logging.handlers.RotatingFileHandler' , 'level' : local_loglevel , 'formatter' : 'standard' , 'filename' : edx_file_loc , 'maxBytes' : 1024 * 1024 * 2 , 'backupCount' : 5 , } , } ) \n    else : \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'level' : local_loglevel , 'class' : 'logging.handlers.SysLogHandler' , 'address' : '/var/run/syslog' if sys . platform == 'darwin' else '/dev/log' , 'formatter' : 'syslog_format' , 'facility' : SysLogHandler . LOG_LOCAL0 , } , } ) \n    return logger_config "}
{"8027": "\ndef fulfill_order ( self , order_number , site_code = None , email_opt_in = 0 ) : \n    max_fulfillment_retries = get_configuration ( 'MAX_FULFILLMENT_RETRIES' , site_code = site_code ) \n    api = get_ecommerce_client ( site_code = site_code ) \n    try : \n        logger . info ( 'Requesting fulfillment of order [%s].' , order_number ) \n        api . orders ( order_number ) . fulfill . put ( email_opt_in = email_opt_in ) \n    except exceptions . HttpClientError as exc : \n        status_code = exc . response . status_code \n        if status_code == 406 : \n            logger . info ( 'Order [%s] has already been fulfilled. Ignoring.' , order_number ) \n            raise Ignore ( ) \n        else : \n            logger . warning ( 'Fulfillment of order [%s] failed because of HttpClientError. Retrying' , order_number , exc_info = 1 ) \n            _retry_order ( self , exc , max_fulfillment_retries , order_number ) \n    except ( exceptions . HttpServerError , exceptions . Timeout , SSLError ) as exc : \n        _retry_order ( self , exc , max_fulfillment_retries , order_number ) "}
{"8033": "\ndef get_value_by_version ( d ) : \n    from oplus import CONF \n    cv = CONF . eplus_version [ : 2 ] \n    for v , value in sorted ( d . items ( ) , reverse = 1 ) : \n        if cv >= v : \n            return value "}
{"8034": "\ndef eplus_version ( self ) : \n    if len ( self . eplus_available_versions ) == 0 : \n        raise RuntimeError ( \"Energy plus is not install, can't use oplus package.\" ) \n    if self . _eplus_version is not None : \n        return self . _eplus_version \n    return sorted ( self . eplus_available_versions . keys ( ) , reverse = 1 ) [ 0 ] "}
{"8043": "\ndef _update_value_inert ( self , index , value ) : \n    field_descriptor = self . _table . _dev_descriptor . get_field_descriptor ( index ) \n    value = field_descriptor . deserialize ( value , index ) \n    if isinstance ( value , Link ) : \n        current_link = self . _data . get ( index ) \n        if current_link is not None : \n            current_link . unregister ( ) \n    if isinstance ( value , RecordHook ) : \n        current_record_hook = self . _data . get ( index ) \n        if current_record_hook is not None : \n            current_record_hook . unregister ( ) \n    if isinstance ( value , ExternalFile ) : \n        current_external_file = self . _data . get ( index ) \n        if current_external_file is not None : \n            current_external_file . _dev_unregister ( ) \n    if value in ( None , NONE_RECORD_HOOK , NONE_LINK , NONE_EXTERNAL_FILE ) : \n        self . _dev_set_none_without_unregistering ( index , check_not_required = 0 ) \n        return \n    old_hook = None \n    if index == 0 and not self . _table . _dev_auto_pk : \n        old_hook = self . _data . get ( 0 ) \n    self . _data [ index ] = value \n    if old_hook is not None : \n        self . _table . _dev_record_pk_was_updated ( old_hook . target_value ) "}
{"8055": "\ndef http_request ( url , post_data = None ) : \n    logger . debug ( 'Requesting URL: %s' % url ) \n    buf = bio ( ) \n    curl = pycurl . Curl ( ) \n    curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) \n    if config ( ) [ 'server' ] [ 'insecure' ] : \n        curl . setopt ( curl . SSL_VERIFYPEER , 0 ) \n        curl . setopt ( curl . SSL_VERIFYHOST , 0 ) \n    if config ( ) [ 'server' ] [ 'certificate' ] : \n        curl . setopt ( curl . SSL_VERIFYPEER , 1 ) \n        curl . setopt ( curl . SSL_VERIFYHOST , 2 ) \n        curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) \n    if post_data : \n        curl . setopt ( curl . HTTPPOST , post_data ) \n    curl . setopt ( curl . WRITEFUNCTION , buf . write ) \n    curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) \n    curl . setopt ( pycurl . USERPWD , \"%s:%s\" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) \n    curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) \n    curl . setopt ( curl . FAILONERROR , 1 ) \n    curl . setopt ( curl . FOLLOWLOCATION , 1 ) \n    curl . perform ( ) \n    curl . close ( ) \n    result = buf . getvalue ( ) \n    buf . close ( ) \n    return result "}
{"8064": "\ndef update_configuration ( cfgfile = None ) : \n    configobj . DEFAULT_INTERPOLATION = 'template' \n    cfgfile = configuration_file ( cfgfile ) \n    cfg = configobj . ConfigObj ( cfgfile , configspec = cfgspec , encoding = 'utf-8' ) \n    validator = Validator ( ) \n    val = cfg . validate ( validator ) \n    if val is not 1 : \n        raise ValueError ( 'Invalid configuration: %s' % val ) \n    if len ( cfg [ 'capture' ] [ 'files' ] ) != len ( cfg [ 'capture' ] [ 'flavors' ] ) : \n        raise ValueError ( 'List of files and flavors do not match' ) \n    globals ( ) [ '__config' ] = cfg \n    logger_init ( ) \n    if cfg [ 'server' ] . get ( 'url' , '' ) . endswith ( '/' ) : \n        logger . warning ( 'Base URL ends with /. This is most likely a ' 'configuration error. The URL should contain nothing ' 'of the service paths.' ) \n    logger . info ( 'Configuration loaded from %s' % cfgfile ) \n    check ( ) \n    return cfg "}
{"8101": "\ndef final_err_table ( df , num_cut_offs = 51 ) : \n    cutoffs = df . cutoff . values \n    min_ = min ( cutoffs ) \n    max_ = max ( cutoffs ) \n    margin = ( max_ - min_ ) * 0.05 \n    sampled_cutoffs = np . linspace ( min_ - margin , max_ + margin , num_cut_offs , dtype = np . float32 ) \n    ix = find_nearest_matches ( np . float32 ( df . cutoff . values ) , sampled_cutoffs ) \n    sampled_df = df . iloc [ ix ] . copy ( ) \n    sampled_df . cutoff = sampled_cutoffs \n    sampled_df . reset_index ( inplace = 1 , drop = 1 ) \n    return sampled_df "}
{"8102": "\ndef summary_err_table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : \n    qvalues = to_one_dim_array ( qvalues ) \n    ix = find_nearest_matches ( np . float32 ( df . qvalue . values ) , qvalues ) \n    df_sub = df . iloc [ ix ] . copy ( ) \n    for i_sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : \n        if i1 == i0 : \n            df_sub . iloc [ i_sub + 1 , : ] = None \n    df_sub . qvalue = qvalues \n    df_sub . reset_index ( inplace = 1 , drop = 1 ) \n    return df_sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ] "}
{"8103": "\ndef error_statistics ( target_scores , decoy_scores , parametric , pfdr , pi0_lambda , pi0_method = \"smoother\" , pi0_smooth_df = 3 , pi0_smooth_log_pi0 = 0 , compute_lfdr = 0 , lfdr_trunc = 1 , lfdr_monotone = 1 , lfdr_transf = \"probit\" , lfdr_adj = 1.5 , lfdr_eps = np . power ( 10.0 , - 8 ) ) : \n    target_scores = to_one_dim_array ( target_scores ) \n    target_scores = np . sort ( target_scores [ ~ np . isnan ( target_scores ) ] ) \n    decoy_scores = to_one_dim_array ( decoy_scores ) \n    decoy_scores = np . sort ( decoy_scores [ ~ np . isnan ( decoy_scores ) ] ) \n    if parametric : \n        target_pvalues = pnorm ( target_scores , decoy_scores ) \n    else : \n        target_pvalues = pemp ( target_scores , decoy_scores ) \n    pi0 = pi0est ( target_pvalues , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) \n    target_qvalues = qvalue ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) \n    metrics = stat_metrics ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) \n    error_stat = pd . DataFrame ( { 'cutoff' : target_scores , 'pvalue' : target_pvalues , 'qvalue' : target_qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) \n    if compute_lfdr : \n        error_stat [ 'pep' ] = lfdr ( target_pvalues , pi0 [ 'pi0' ] , lfdr_trunc , lfdr_monotone , lfdr_transf , lfdr_adj , lfdr_eps ) \n    return error_stat , pi0 "}
{"8104": "\ndef find_cutoff ( tt_scores , td_scores , cutoff_fdr , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) : \n    error_stat , pi0 = error_statistics ( tt_scores , td_scores , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , 0 ) \n    if not len ( error_stat ) : \n        raise click . ClickException ( \"Too little data for calculating error statistcs.\" ) \n    i0 = ( error_stat . qvalue - cutoff_fdr ) . abs ( ) . idxmin ( ) \n    cutoff = error_stat . iloc [ i0 ] [ \"cutoff\" ] \n    return cutoff "}
{"8115": "\ndef delete_group ( self , group_id ) : \n    self . _valid_group_id ( group_id ) \n    url = \"{}/group/{}\" . format ( self . API , group_id ) \n    self . _delete_resource ( url ) \n    return 1 "}
{"8119": "\ndef is_effective_member ( self , group_id , netid ) : \n    self . _valid_group_id ( group_id ) \n    netid = re . sub ( '@washington.edu' , '' , netid ) \n    url = \"{}/group/{}/effective_member/{}\" . format ( self . API , group_id , netid ) \n    try : \n        data = self . _get_resource ( url ) \n        return 1 \n    except DataFailureException as ex : \n        if ex . status == 404 : \n            return 0 \n        else : \n            raise "}
{"8123": "\ndef get_stdin ( self , os_path = None , skip_sub_command = 0 ) : \n    sub_command = None if skip_sub_command else self . stdin_sub_command \n    inn , path = self . _get_in_and_path ( self . stdin , self . stdin_root , sub_command , os_path ) \n    if hasattr ( inn , 'stdout' ) : \n        return inn . stdout \n    return inn "}
{"8124": "\ndef get_stdout ( self , os_path = None , skip_sub_command = 0 ) : \n    sub_command = None if skip_sub_command else self . stdout_sub_command \n    out , path = self . _get_out_and_path ( self . stdout , self . stdout_root , sub_command , os_path ) \n    if hasattr ( out , 'stdin' ) : \n        return out . stdin \n    return out "}
{"8125": "\ndef get_stderr ( self , os_path = None , skip_sub_command = 0 ) : \n    sub_command = None if skip_sub_command else self . stderr_sub_command \n    out , path = self . _get_out_and_path ( self . stderr , self . stderr_root , sub_command , os_path ) \n    if hasattr ( out , 'stdin' ) : \n        return out . stdin \n    return out "}
{"8126": "\ndef get_debug ( self , os_path = None , skip_sub_command = 0 ) : \n    sub_command = None if skip_sub_command else self . debug_sub_command \n    out , path = self . _get_out_and_path ( self . debug , self . debug_root , sub_command , os_path ) \n    if hasattr ( out , 'stdin' ) : \n        return out . stdin \n    return out "}
{"8127": "\ndef with_stdin ( self , os_path = None , skip_sub_command = 0 , disk_closed_callback = None ) : \n    sub_command = None if skip_sub_command else self . stdin_sub_command \n    inn , path = self . _get_in_and_path ( self . stdin , self . stdin_root , sub_command , os_path ) \n    try : \n        if hasattr ( inn , 'stdout' ) : \n            yield inn . stdout \n        else : \n            yield inn \n    finally : \n        if hasattr ( inn , 'stdout' ) : \n            self . _close ( inn . stdout ) \n        self . _wait ( inn , path ) \n        self . _close ( inn ) \n        if disk_closed_callback and path : \n            disk_closed_callback ( path ) "}
{"8128": "\ndef with_stdout ( self , os_path = None , skip_sub_command = 0 , disk_closed_callback = None ) : \n    sub_command = None if skip_sub_command else self . stdout_sub_command \n    out , path = self . _get_out_and_path ( self . stdout , self . stdout_root , sub_command , os_path ) \n    try : \n        if hasattr ( out , 'stdin' ) : \n            yield out . stdin \n        else : \n            yield out \n    finally : \n        if hasattr ( out , 'stdin' ) : \n            self . _close ( out . stdin ) \n        self . _wait ( out , path ) \n        self . _close ( out ) \n        if disk_closed_callback and path : \n            disk_closed_callback ( path ) "}
{"8129": "\ndef with_stderr ( self , os_path = None , skip_sub_command = 0 , disk_closed_callback = None ) : \n    sub_command = None if skip_sub_command else self . stderr_sub_command \n    out , path = self . _get_out_and_path ( self . stderr , self . stderr_root , sub_command , os_path ) \n    try : \n        if hasattr ( out , 'stdin' ) : \n            yield out . stdin \n        else : \n            yield out \n    finally : \n        if hasattr ( out , 'stdin' ) : \n            self . _close ( out . stdin ) \n        self . _wait ( out , path ) \n        self . _close ( out ) \n        if disk_closed_callback and path : \n            disk_closed_callback ( path ) "}
{"8130": "\ndef with_debug ( self , os_path = None , skip_sub_command = 0 , disk_closed_callback = None ) : \n    sub_command = None if skip_sub_command else self . debug_sub_command \n    out , path = self . _get_out_and_path ( self . debug , self . debug_root , sub_command , os_path ) \n    try : \n        if hasattr ( out , 'stdin' ) : \n            yield out . stdin \n        else : \n            yield out \n    finally : \n        if hasattr ( out , 'stdin' ) : \n            self . _close ( out . stdin ) \n        self . _wait ( out , path ) \n        self . _close ( out ) \n        if disk_closed_callback and path : \n            disk_closed_callback ( path ) "}
{"8131": "\ndef cli_empty_account ( context , yes_empty_account = 0 , until_empty = 0 ) : \n    if not yes_empty_account : \n        raise ReturnCode ( 'called cli_empty_account without setting yes_empty_account=True' ) \n    marker = None \n    while 1 : \n        with context . client_manager . with_client ( ) as client : \n            status , reason , headers , contents = client . get_account ( marker = marker , headers = context . headers , query = context . query , cdn = context . cdn ) \n        if status // 100 != 2 : \n            if status == 404 and context . ignore_404 : \n                return \n            raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n        if not contents : \n            if until_empty and marker : \n                marker = None \n                continue \n            break \n        for item in contents : \n            cli_delete ( context , item [ 'name' ] , context . headers , recursive = 1 ) \n        marker = item [ 'name' ] "}
{"8132": "\ndef cli_empty_container ( context , path , until_empty = 0 ) : \n    path = path . rstrip ( '/' ) . decode ( 'utf8' ) \n    conc = Concurrency ( context . concurrency ) \n    def check_conc ( ) : \n        for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n            if exc_value : \n                with context . io_manager . with_stderr ( ) as fp : \n                    fp . write ( str ( exc_value ) ) \n                    fp . write ( '\\n' ) \n                    fp . flush ( ) \n    marker = None \n    while 1 : \n        with context . client_manager . with_client ( ) as client : \n            status , reason , headers , contents = client . get_container ( path , marker = marker , headers = context . headers , query = context . query , cdn = context . cdn ) \n        if status // 100 != 2 : \n            if status == 404 and context . ignore_404 : \n                return \n            raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            if until_empty and marker : \n                marker = None \n                continue \n            break \n        for item in contents : \n            newpath = '%s/%s' % ( path , item [ 'name' ] ) \n            new_context = context . copy ( ) \n            new_context . ignore_404 = 1 \n            check_conc ( ) \n            conc . spawn ( newpath , cli_delete , new_context , newpath ) \n        marker = item [ 'name' ] \n        conc . join ( ) \n        check_conc ( ) "}
{"8135": "\ndef error ( self , msg , file = None ) : \n    self . error_encountered = 1 \n    file . write ( self . error_prefix ) \n    file . write ( msg ) \n    file . write ( '\\n' ) \n    file . flush ( ) "}
{"8139": "\ndef request ( self , method , path , contents , headers , decode_json = 0 , stream = 0 , query = None , cdn = 0 ) : \n    raise Exception ( 'request method not implemented' ) "}
{"8140": "\ndef post_account ( self , headers = None , query = None , cdn = 0 , body = None ) : \n    return self . request ( 'POST' , '' , body or '' , headers , query = query , cdn = cdn ) "}
{"8141": "\ndef delete_account ( self , headers = None , yes_i_mean_delete_the_account = 0 , query = None , cdn = 0 , body = None ) : \n    if not yes_i_mean_delete_the_account and ( not body or not query or 'bulk-delete' not in query ) : \n        return ( 0 , 'yes_i_mean_delete_the_account was not set to True' , { } , '' ) \n    return self . request ( 'DELETE' , '' , body or '' , headers , query = query , cdn = cdn ) "}
{"8142": "\ndef put_container ( self , container , headers = None , query = None , cdn = 0 , body = None ) : \n    path = self . _container_path ( container ) \n    return self . request ( 'PUT' , path , body or '' , headers , query = query , cdn = cdn ) "}
{"8143": "\ndef head_object ( self , container , obj , headers = None , query = None , cdn = 0 ) : \n    path = self . _object_path ( container , obj ) \n    return self . request ( 'HEAD' , path , '' , headers , query = query , cdn = cdn ) "}
{"8144": "\ndef get_object ( self , container , obj , headers = None , stream = 1 , query = None , cdn = 0 ) : \n    path = self . _object_path ( container , obj ) \n    return self . request ( 'GET' , path , '' , headers , query = query , stream = stream , cdn = cdn ) "}
{"8145": "\ndef put_object ( self , container , obj , contents , headers = None , query = None , cdn = 0 ) : \n    path = self . _object_path ( container , obj ) \n    return self . request ( 'PUT' , path , contents , headers , query = query , cdn = cdn ) "}
{"8146": "\ndef post_object ( self , container , obj , headers = None , query = None , cdn = 0 , body = None ) : \n    path = self . _object_path ( container , obj ) \n    return self . request ( 'POST' , path , body or '' , headers , query = query , cdn = cdn ) "}
{"8153": "\ndef cli_fordo ( context , path = None ) : \n    path = path . lstrip ( '/' ) if path else None \n    if path and '/' in path : \n        raise ReturnCode ( 'path must be an empty string or a container name; was %r' % path ) \n    limit = context . query . get ( 'limit' ) \n    delimiter = context . query . get ( 'delimiter' ) \n    prefix = context . query . get ( 'prefix' ) \n    marker = context . query . get ( 'marker' ) \n    end_marker = context . query . get ( 'end_marker' ) \n    conc = Concurrency ( context . concurrency ) \n    while 1 : \n        with context . client_manager . with_client ( ) as client : \n            if not path : \n                status , reason , headers , contents = client . get_account ( headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            else : \n                status , reason , headers , contents = client . get_container ( path , headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            if status // 100 != 2 : \n                if status == 404 and context . ignore_404 : \n                    return \n                if hasattr ( contents , 'read' ) : \n                    contents . read ( ) \n                if not path : \n                    raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n                else : \n                    raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            break \n        for item in contents : \n            name = ( path + '/' if path else '' ) + item . get ( 'name' , item . get ( 'subdir' ) ) \n            args = list ( context . remaining_args ) \n            try : \n                index = args . index ( '<item>' ) \n            except ValueError : \n                raise ReturnCode ( 'No \"<item>\" designation found in the \"do\" clause.' ) \n            args [ index ] = name \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( name , _cli_call , context , name , args ) \n        marker = contents [ - 1 ] [ 'name' ] \n        if limit : \n            break \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            conc . join ( ) \n            raise exc_value "}
{"8154": "\ndef get_client ( self ) : \n    client = None \n    try : \n        client = self . clients . get ( block = 0 ) \n    except queue . Empty : \n        pass \n    if not client : \n        self . client_id += 1 \n        kwargs = dict ( self . kwargs ) \n        kwargs [ 'verbose_id' ] = kwargs . get ( 'verbose_id' , '' ) + str ( self . client_id ) \n        client = self . client_class ( * self . args , ** kwargs ) \n    return client "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = Crypto . Random . new ( ) . read ( 16 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = 1 \n    left = None \n    if content_length is not None and content_length >= 0 : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and size > left : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and left > 0 : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16 ) \n            break \n        if left is not None : \n            left -= len ( chunk ) \n            if left <= 0 : \n                reading = 0 \n        block = chunk \n        trailing = len ( block ) % 16 \n        while trailing : \n            size = 16 - trailing \n            if left is not None and size > left : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and left > 0 : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = 0 \n                chunk = chr ( trailing ) * ( 16 - trailing ) \n            elif left is not None : \n                left -= len ( chunk ) \n                if left <= 0 : \n                    reading = 0 \n            block += chunk \n            trailing = len ( block ) % 16 \n        yield encryptor . encrypt ( block ) "}
{"8156": "\ndef aes_decrypt ( key , stdin , chunk_size = 65536 ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = stdin . read ( 16 ) \n    while len ( iv ) < 16 : \n        chunk = stdin . read ( 16 - len ( iv ) ) \n        if not chunk : \n            raise IOError ( 'EOF reading IV' ) \n    decryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    data = '' \n    while 1 : \n        chunk = stdin . read ( chunk_size ) \n        if not chunk : \n            if len ( data ) != 16 : \n                raise IOError ( 'EOF reading encrypted stream' ) \n            data = decryptor . decrypt ( data ) \n            trailing = ord ( data [ - 1 ] ) \n            if trailing > 15 : \n                raise IOError ( 'EOF reading encrypted stream or trailing value corrupted ' '%s' % trailing ) \n            yield data [ : trailing ] \n            break \n        data += chunk \n        if len ( data ) > 16 : \n            trailing = ( len ( data ) % 16 ) or 16 \n            yield decryptor . decrypt ( data [ : - trailing ] ) \n            data = data [ - trailing : ] "}
{"8157": "\ndef cli_put_directory_structure ( context , path ) : \n    if not context . input_ : \n        raise ReturnCode ( 'called cli_put_directory_structure without context.input_ set' ) \n    if not os . path . isdir ( context . input_ ) : \n        raise ReturnCode ( '%r is not a directory' % context . input_ ) \n    if not path : \n        raise ReturnCode ( 'uploading a directory structure requires at least a container ' 'name' ) \n    new_context = context . copy ( ) \n    new_context . input_ = None \n    container = path . split ( '/' , 1 ) [ 0 ] \n    cli_put_container ( new_context , container ) \n    ilen = len ( context . input_ ) \n    if not context . input_ . endswith ( os . sep ) : \n        ilen += 1 \n    conc = Concurrency ( context . concurrency ) \n    for ( dirpath , dirnames , filenames ) in os . walk ( context . input_ ) : \n        if not dirnames and not filenames : \n            new_context = context . copy ( ) \n            new_context . headers = dict ( context . headers ) \n            new_context . headers [ 'content-type' ] = 'text/directory' \n            new_context . headers [ 'x-object-meta-mtime' ] = '%f' % os . path . getmtime ( context . input_ ) \n            new_context . input_ = None \n            new_context . empty = 1 \n            new_path = path \n            if path [ - 1 ] != '/' : \n                new_path += '/' \n            new_path += dirpath [ ilen : ] \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n        else : \n            for fname in filenames : \n                new_context = context . copy ( ) \n                new_context . input_ = os . path . join ( dirpath , fname ) \n                new_path = path \n                if path [ - 1 ] != '/' : \n                    new_path += '/' \n                if dirpath [ ilen : ] : \n                    new_path += dirpath [ ilen : ] + '/' \n                new_path += fname \n                for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                    if exc_value : \n                        conc . join ( ) \n                        raise exc_value \n                conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            raise exc_value "}
{"8162": "\ndef cli_tempurl ( context , method , path , seconds = None , use_container = 0 ) : \n    with contextlib . nested ( context . io_manager . with_stdout ( ) , context . client_manager . with_client ( ) ) as ( fp , client ) : \n        method = method . upper ( ) \n        path = path . lstrip ( '/' ) \n        seconds = seconds if seconds is not None else 3600 \n        if '/' not in path : \n            raise ReturnCode ( 'invalid tempurl path %r; should have a / within it' % path ) \n        if use_container : \n            key_type = 'container' \n            container = path . split ( '/' , 1 ) [ 0 ] \n            status , reason , headers , contents = client . head_container ( container ) \n        else : \n            key_type = 'account' \n            status , reason , headers , contents = client . head_account ( ) \n        if status // 100 != 2 : \n            raise ReturnCode ( 'obtaining X-%s-Meta-Temp-Url-Key: %s %s' % ( key_type . title ( ) , status , reason ) ) \n        key = headers . get ( 'x-%s-meta-temp-url-key' % key_type ) \n        if not key : \n            raise ReturnCode ( 'there is no X-%s-Meta-Temp-Url-Key set for this %s' % ( key_type . title ( ) , key_type ) ) \n        url = client . storage_url + '/' + path \n        fp . write ( generate_temp_url ( method , url , seconds , key ) ) \n        fp . write ( '\\n' ) \n        fp . flush ( ) "}
{"8165": "\ndef is_empty ( self ) : \n    something = self . read ( 1 ) \n    if something : \n        if self . buf : \n            self . buf = something + self . buf \n        else : \n            self . buf = something \n        return 0 \n    else : \n        return 1 "}
{"8170": "\ndef forwards ( self , orm ) : \n    for title in orm [ 'hero_slider.SliderItemTitle' ] . objects . all ( ) : \n        title . is_published = 1 \n        title . save ( ) "}
{"8182": "\ndef execute_perceval_job ( backend , backend_args , qitems , task_id , category , archive_args = None , max_retries = MAX_JOB_RETRIES ) : \n    rq_job = rq . get_current_job ( ) \n    job = PercevalJob ( rq_job . id , task_id , backend , category , rq_job . connection , qitems ) \n    logger . debug ( \"Running job #%s (task: %s) (%s) (cat:%s)\" , job . job_id , task_id , backend , category ) \n    if not job . has_archiving ( ) and archive_args : \n        raise AttributeError ( \"archive attributes set but archive is not supported\" ) \n    run_job = 1 \n    resume = 0 \n    failures = 0 \n    while run_job : \n        try : \n            job . run ( backend_args , archive_args = archive_args , resume = resume ) \n        except AttributeError as e : \n            raise e \n        except Exception as e : \n            logger . debug ( \"Error running job %s (%s) - %s\" , job . job_id , backend , str ( e ) ) \n            failures += 1 \n            if not job . has_resuming ( ) or failures >= max_retries : \n                logger . error ( \"Cancelling job #%s (task: %s) (%s)\" , job . job_id , task_id , backend ) \n                raise e \n            logger . warning ( \"Resuming job #%s (task: %s) (%s) due to a failure (n %s, max %s)\" , job . job_id , task_id , backend , failures , max_retries ) \n            resume = 1 \n        else : \n            run_job = 0 \n    result = job . result \n    logger . debug ( \"Job #%s (task: %s) completed (%s) - %s items (%s) fetched\" , result . job_id , task_id , result . backend , str ( result . nitems ) , result . category ) \n    return result "}
{"8184": "\ndef run ( self , backend_args , archive_args = None , resume = 0 ) : \n    args = backend_args . copy ( ) \n    if archive_args : \n        self . initialize_archive_manager ( archive_args [ 'archive_path' ] ) \n    if not resume : \n        max_date = backend_args . get ( 'from_date' , None ) \n        offset = backend_args . get ( 'offset' , None ) \n        if max_date : \n            max_date = datetime_to_utc ( max_date ) . timestamp ( ) \n        self . _result = JobResult ( self . job_id , self . task_id , self . backend , self . category , None , max_date , 0 , offset = offset , nresumed = 0 ) \n    else : \n        if self . result . max_date : \n            args [ 'from_date' ] = unixtime_to_datetime ( self . result . max_date ) \n        if self . result . offset : \n            args [ 'offset' ] = self . result . offset \n        self . _result . nresumed += 1 \n    for item in self . _execute ( args , archive_args ) : \n        self . conn . rpush ( self . qitems , pickle . dumps ( item ) ) \n        self . _result . nitems += 1 \n        self . _result . last_uuid = item [ 'uuid' ] \n        if not self . result . max_date or self . result . max_date < item [ 'updated_on' ] : \n            self . _result . max_date = item [ 'updated_on' ] \n        if 'offset' in item : \n            self . _result . offset = item [ 'offset' ] "}
{"8186": "\ndef create_index ( idx_url , clean = 0 ) : \n    try : \n        r = requests . get ( idx_url ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s)\" % idx_url \n        raise ElasticSearchError ( cause = cause ) \n    if r . status_code != 200 : \n        r = requests . put ( idx_url ) \n        if r . status_code != 200 : \n            logger . info ( \"Can't create index %s (%s)\" , idx_url , r . status_code ) \n            cause = \"Error creating Elastic Search index %s\" % idx_url \n            raise ElasticSearchError ( cause = cause ) \n        logger . info ( \"Index %s created\" , idx_url ) \n        return 1 \n    elif r . status_code == 200 and clean : \n        requests . delete ( idx_url ) \n        requests . put ( idx_url ) \n        logger . info ( \"Index deleted and created (index: %s)\" , idx_url ) \n        return 1 \n    return 0 "}
{"8189": "\ndef write_items ( cls , writer , items_generator ) : \n    while 1 : \n        items = items_generator ( ) \n        writer . write ( items ) \n        time . sleep ( 1 ) "}
{"8191": "\ndef remove_task ( self , task_id ) : \n    try : \n        self . _scheduler . cancel_task ( task_id ) \n    except NotFoundError as e : \n        logger . info ( \"Cannot cancel %s task because it does not exist.\" , task_id ) \n        return 0 \n    else : \n        return 1 "}
{"8201": "\ndef schedule_task ( self , task_id ) : \n    task = self . registry . get ( task_id ) \n    job_args = self . _build_job_arguments ( task ) \n    archiving_cfg = task . archiving_cfg \n    fetch_from_archive = 0 if not archiving_cfg else archiving_cfg . fetch_from_archive \n    queue = Q_ARCHIVE_JOBS if fetch_from_archive else Q_CREATION_JOBS \n    job_id = self . _scheduler . schedule_job_task ( queue , task . task_id , job_args , delay = 0 ) \n    logger . info ( \"Job #%s (task: %s) scheduled\" , job_id , task . task_id ) \n    return job_id "}
{"8208": "\ndef register ( view = None , * , admin_site = None , admin_class = ModelAdminView ) : \n    if not admin_site : \n        admin_site = site \n    def wrapped ( inner_view ) : \n        module = inner_view . __module__ \n        app_label = re . search ( r\"\\.?(\\w+)\\.admin\" , module ) . group ( 1 ) \n        app_config = apps . get_app_config ( app_label ) \n        label = getattr ( inner_view , \"label\" , None ) \n        if not label : \n            label = re . sub ( \"(Admin)|(View)\" , \"\" , inner_view . __name__ ) . lower ( ) \n        inner_view . label = label \n        model_name = label . capitalize ( ) \n        verbose_name = getattr ( inner_view , \"verbose_name\" , model_name ) \n        inner_view . verbose_name = verbose_name \n        access_perm_codename = \"can_access_\" + model_name . lower ( ) \n        access_perm_name = _ ( \"Can access {verbose_name}\" ) . format ( verbose_name = verbose_name ) \n        permissions = tuple ( [ ( access_perm_codename , access_perm_name ) ] + list ( getattr ( inner_view , \"permissions\" , [ ] ) ) ) \n        model = type ( model_name , ( Model , ) , { \"__module__\" : module + \".__models__\" , \"View\" : inner_view , \"app_config\" : app_config , \"Meta\" : type ( \"Meta\" , ( object , ) , dict ( managed = 0 , abstract = 1 , app_label = app_config . label , verbose_name = verbose_name , verbose_name_plural = verbose_name , permissions = permissions , ) , ) , } , ) \n        admin_site . _registry [ model ] = admin_class ( model , admin_site ) \n        return inner_view \n    if view is None : \n        return wrapped \n    return wrapped ( view ) "}
{"8210": "\ndef get_view_name ( self , respect_name = 1 ) : \n    if isinstance ( self , type ) : \n        view = self \n    else : \n        view = self . __class__ \n    if respect_name : \n        name = getattr ( view , \"name\" , None ) \n        if name is not None : \n            return name \n    name = view . __name__ \n    for suffix in ( \"ViewSet\" , \"View\" , \"API\" , \"Admin\" ) : \n        name = formatting . remove_trailing_string ( name , suffix ) \n    name = formatting . camelcase_to_spaces ( name ) \n    suffix = getattr ( view , \"suffix\" , None ) \n    if suffix : \n        name += \" \" + suffix \n    return name "}
{"8221": "\ndef parse_bool ( value ) : \n    boolean = parse_str ( value ) . capitalize ( ) \n    if boolean in ( \"True\" , \"Yes\" , \"On\" , \"1\" ) : \n        return 1 \n    elif boolean in ( \"False\" , \"No\" , \"Off\" , \"0\" ) : \n        return 0 \n    else : \n        raise ValueError ( 'Unable to parse boolean value \"{}\"' . format ( value ) ) "}
{"8230": "\ndef find_unique_points ( explored_parameters ) : \n    ranges = [ param . f_get_range ( copy = 0 ) for param in explored_parameters ] \n    zipped_tuples = list ( zip ( * ranges ) ) \n    try : \n        unique_elements = OrderedDict ( ) \n        for idx , val_tuple in enumerate ( zipped_tuples ) : \n            if val_tuple not in unique_elements : \n                unique_elements [ val_tuple ] = [ ] \n            unique_elements [ val_tuple ] . append ( idx ) \n        return list ( unique_elements . items ( ) ) \n    except TypeError : \n        logger = logging . getLogger ( 'pypet.find_unique' ) \n        logger . error ( 'Your parameter entries could not be hashed, ' 'now I am sorting slowly in O(N**2).' ) \n        unique_elements = [ ] \n        for idx , val_tuple in enumerate ( zipped_tuples ) : \n            matches = 0 \n            for added_tuple , pos_list in unique_elements : \n                matches = 1 \n                for idx2 , val in enumerate ( added_tuple ) : \n                    if not explored_parameters [ idx2 ] . _equal_values ( val_tuple [ idx2 ] , val ) : \n                        matches = 0 \n                        break \n                if matches : \n                    pos_list . append ( idx ) \n                    break \n            if not matches : \n                unique_elements . append ( ( val_tuple , [ idx ] ) ) \n        return unique_elements "}
{"8231": "\ndef _change_logging_kwargs ( kwargs ) : \n    log_levels = kwargs . pop ( 'log_level' , None ) \n    log_folder = kwargs . pop ( 'log_folder' , 'logs' ) \n    logger_names = kwargs . pop ( 'logger_names' , '' ) \n    if log_levels is None : \n        log_levels = kwargs . pop ( 'log_levels' , logging . INFO ) \n    log_multiproc = kwargs . pop ( 'log_multiproc' , 1 ) \n    if not isinstance ( logger_names , ( tuple , list ) ) : \n        logger_names = [ logger_names ] \n    if not isinstance ( log_levels , ( tuple , list ) ) : \n        log_levels = [ log_levels ] \n    if len ( log_levels ) == 1 : \n        log_levels = [ log_levels [ 0 ] for _ in logger_names ] \n    dictionary = copy . deepcopy ( LOGGING_DICT ) \n    prefixes = [ '' ] \n    if not log_multiproc : \n        for key in list ( dictionary . keys ( ) ) : \n            if key . startswith ( 'multiproc_' ) : \n                del dictionary [ key ] \n    else : \n        prefixes . append ( 'multiproc_' ) \n    for prefix in prefixes : \n        for handler_dict in dictionary [ prefix + 'handlers' ] . values ( ) : \n            if 'filename' in handler_dict : \n                filename = os . path . join ( log_folder , handler_dict [ 'filename' ] ) \n                filename = os . path . normpath ( filename ) \n                handler_dict [ 'filename' ] = filename \n        dictionary [ prefix + 'loggers' ] = { } \n        logger_dict = dictionary [ prefix + 'loggers' ] \n        for idx , logger_name in enumerate ( logger_names ) : \n            logger_dict [ logger_name ] = { 'level' : log_levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } \n    kwargs [ 'log_config' ] = dictionary "}
{"8239": "\ndef _check_and_replace_parser_args ( parser , section , option , rename_func , make_dirs = 1 ) : \n    args = parser . get ( section , option , raw = 1 ) \n    strings = get_strings ( args ) \n    replace = 0 \n    for string in strings : \n        isfilename = any ( x in string for x in FILENAME_INDICATORS ) \n        if isfilename : \n            newstring = rename_func ( string ) \n            if make_dirs : \n                try_make_dirs ( newstring ) \n            raw_string = string . replace ( '\\\\' , '\\\\\\\\' ) \n            raw_newstring = newstring . replace ( '\\\\' , '\\\\\\\\' ) \n            args = args . replace ( raw_string , raw_newstring ) \n            replace = 1 \n    if replace : \n        parser . set ( section , option , args ) "}
{"8241": "\ndef _find_multiproc_options ( parser ) : \n    sections = parser . sections ( ) \n    if not any ( section . startswith ( 'multiproc_' ) for section in sections ) : \n        return None \n    mp_parser = NoInterpolationParser ( ) \n    for section in sections : \n        if section . startswith ( 'multiproc_' ) : \n            new_section = section . replace ( 'multiproc_' , '' ) \n            mp_parser . add_section ( new_section ) \n            options = parser . options ( section ) \n            for option in options : \n                val = parser . get ( section , option , raw = 1 ) \n                mp_parser . set ( new_section , option , val ) \n    return mp_parser "}
{"8243": "\ndef check_log_config ( self ) : \n    if self . report_progress : \n        if self . report_progress is 1 : \n            self . report_progress = ( 5 , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , ( int , float ) ) : \n            self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , str ) : \n            self . report_progress = ( 5 , self . report_progress , logging . INFO ) \n        elif len ( self . report_progress ) == 2 : \n            self . report_progress = ( self . report_progress [ 0 ] , self . report_progress [ 1 ] , logging . INFO ) \n    if self . log_config : \n        if self . log_config == pypetconstants . DEFAULT_LOGGING : \n            pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n            init_path = os . path . join ( pypet_path , 'logging' ) \n            self . log_config = os . path . join ( init_path , 'default.ini' ) \n        if isinstance ( self . log_config , str ) : \n            if not os . path . isfile ( self . log_config ) : \n                raise ValueError ( 'Could not find the logger init file ' '`%s`.' % self . log_config ) \n            parser = NoInterpolationParser ( ) \n            parser . read ( self . log_config ) \n        elif isinstance ( self . log_config , cp . RawConfigParser ) : \n            parser = self . log_config \n        else : \n            parser = None \n        if parser is not None : \n            self . _sp_config = self . _parser_to_string_io ( parser ) \n            self . _mp_config = self . _find_multiproc_options ( parser ) \n            if self . _mp_config is not None : \n                self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) \n        elif isinstance ( self . log_config , dict ) : \n            self . _sp_config = self . log_config \n            self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) \n    if self . log_stdout : \n        if self . log_stdout is 1 : \n            self . log_stdout = ( 'STDOUT' , logging . INFO ) \n        if isinstance ( self . log_stdout , str ) : \n            self . log_stdout = ( self . log_stdout , logging . INFO ) \n        if isinstance ( self . log_stdout , int ) : \n            self . log_stdout = ( 'STDOUT' , self . log_stdout ) "}
{"8246": "\ndef make_logging_handlers_and_tools ( self , multiproc = 0 ) : \n    log_stdout = self . log_stdout \n    if sys . stdout is self . _stdout_to_logger : \n        log_stdout = 0 \n    if self . log_config : \n        if multiproc : \n            proc_log_config = self . _mp_config \n        else : \n            proc_log_config = self . _sp_config \n        if proc_log_config : \n            if isinstance ( proc_log_config , dict ) : \n                new_dict = self . _handle_dict_config ( proc_log_config ) \n                dictConfig ( new_dict ) \n            else : \n                parser = self . _handle_config_parsing ( proc_log_config ) \n                memory_file = self . _parser_to_string_io ( parser ) \n                fileConfig ( memory_file , disable_existing_loggers = 0 ) \n    if log_stdout : \n        std_name , std_level = self . log_stdout \n        stdout = StdoutToLogger ( std_name , log_level = std_level ) \n        stdout . start ( ) \n        self . _tools . append ( stdout ) "}
{"8247": "\ndef finalize ( self , remove_all_handlers = 1 ) : \n    for tool in self . _tools : \n        tool . finalize ( ) \n    self . _tools = [ ] \n    self . _stdout_to_logger = None \n    for config in ( self . _sp_config , self . _mp_config ) : \n        if hasattr ( config , 'close' ) : \n            config . close ( ) \n    self . _sp_config = None \n    self . _mp_config = None \n    if remove_all_handlers : \n        self . tabula_rasa ( ) "}
{"8248": "\ndef start ( self ) : \n    if sys . stdout is not self : \n        self . _original_steam = sys . stdout \n        sys . stdout = self \n        self . _redirection = 1 \n    if self . _redirection : \n        print ( 'Established redirection of `stdout`.' ) "}
{"8249": "\ndef write ( self , buf ) : \n    if not self . _recursion : \n        self . _recursion = 1 \n        try : \n            for line in buf . rstrip ( ) . splitlines ( ) : \n                self . _logger . log ( self . _log_level , line . rstrip ( ) ) \n        finally : \n            self . _recursion = 0 \n    else : \n        sys . __stderr__ . write ( 'ERROR: Recursion in Stream redirection!' ) "}
{"8250": "\ndef results_equal ( a , b ) : \n    if a . v_is_parameter and b . v_is_parameter : \n        raise ValueError ( 'Both inputs are not results.' ) \n    if a . v_is_parameter or b . v_is_parameter : \n        return 0 \n    if a . v_full_name != b . v_full_name : \n        return 0 \n    if hasattr ( a , '_data' ) and not hasattr ( b , '_data' ) : \n        return 0 \n    if hasattr ( a , '_data' ) : \n        akeyset = set ( a . _data . keys ( ) ) \n        bkeyset = set ( b . _data . keys ( ) ) \n        if akeyset != bkeyset : \n            return 0 \n        for key in a . _data : \n            val = a . _data [ key ] \n            bval = b . _data [ key ] \n            if not nested_equal ( val , bval ) : \n                return 0 \n    return 1 "}
{"8251": "\ndef parameters_equal ( a , b ) : \n    if ( not b . v_is_parameter and not a . v_is_parameter ) : \n        raise ValueError ( 'Both inputs are not parameters' ) \n    if ( not b . v_is_parameter or not a . v_is_parameter ) : \n        return 0 \n    if a . v_full_name != b . v_full_name : \n        return 0 \n    if a . f_is_empty ( ) and b . f_is_empty ( ) : \n        return 1 \n    if a . f_is_empty ( ) != b . f_is_empty ( ) : \n        return 0 \n    if not a . _values_of_same_type ( a . f_get ( ) , b . f_get ( ) ) : \n        return 0 \n    if not a . _equal_values ( a . f_get ( ) , b . f_get ( ) ) : \n        return 0 \n    if a . f_has_range ( ) != b . f_has_range ( ) : \n        return 0 \n    if a . f_has_range ( ) : \n        if a . f_get_range_length ( ) != b . f_get_range_length ( ) : \n            return 0 \n        for myitem , bitem in zip ( a . f_get_range ( copy = 0 ) , b . f_get_range ( copy = 0 ) ) : \n            if not a . _values_of_same_type ( myitem , bitem ) : \n                return 0 \n            if not a . _equal_values ( myitem , bitem ) : \n                return 0 \n    return 1 "}
{"8252": "\ndef manual_run ( turn_into_run = 1 , store_meta_data = 1 , clean_up = 1 ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( traj , * args , ** kwargs ) : \n            do_wrap = not traj . _run_by_environment \n            if do_wrap : \n                traj . f_start_run ( turn_into_run = turn_into_run ) \n            result = func ( traj , * args , ** kwargs ) \n            if do_wrap : \n                traj . f_finalize_run ( store_meta_data = store_meta_data , clean_up = clean_up ) \n            return result \n        return new_func \n    return wrapper "}
{"8256": "\ndef retry ( n , errors , wait = 0.0 , logger_name = None ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( * args , ** kwargs ) : \n            retries = 0 \n            while 1 : \n                try : \n                    result = func ( * args , ** kwargs ) \n                    if retries and logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'Retry of `%s` successful' % func . __name__ ) \n                    return result \n                except errors : \n                    if retries >= n : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                        raise \n                    elif logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                    retries += 1 \n                    if wait : \n                        time . sleep ( wait ) \n        return new_func \n    return wrapper "}
{"8258": "\ndef add_params ( traj ) : \n    traj . v_standard_parameter = Brian2Parameter \n    traj . v_fast_access = 1 \n    traj . f_add_parameter ( 'Net.C' , 281 * pF ) \n    traj . f_add_parameter ( 'Net.gL' , 30 * nS ) \n    traj . f_add_parameter ( 'Net.EL' , - 70.6 * mV ) \n    traj . f_add_parameter ( 'Net.VT' , - 50.4 * mV ) \n    traj . f_add_parameter ( 'Net.DeltaT' , 2 * mV ) \n    traj . f_add_parameter ( 'Net.tauw' , 40 * ms ) \n    traj . f_add_parameter ( 'Net.a' , 4 * nS ) \n    traj . f_add_parameter ( 'Net.b' , 0.08 * nA ) \n    traj . f_add_parameter ( 'Net.I' , .8 * nA ) \n    traj . f_add_parameter ( 'Net.Vcut' , 'vm > 0*mV' ) \n    traj . f_add_parameter ( 'Net.N' , 50 ) \n    eqs = '''    dvm/dt=(gL*(EL-vm)+gL*DeltaT*exp((vm-VT)/DeltaT)+I-w)/C : volt    dw/dt=(a*(vm-EL)-w)/tauw : amp    Vr:volt    ''' \n    traj . f_add_parameter ( 'Net.eqs' , eqs ) \n    traj . f_add_parameter ( 'reset' , 'vm=Vr;w+=b' ) "}
{"8259": "\ndef run_net ( traj ) : \n    eqs = traj . eqs \n    namespace = traj . Net . f_to_dict ( short_names = 1 , fast_access = 1 ) \n    neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) \n    neuron . vm = traj . EL \n    neuron . w = traj . a * ( neuron . vm - traj . EL ) \n    neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) \n    print ( 'Initial Run' ) \n    net = Network ( neuron ) \n    net . run ( 100 * ms , report = 'text' ) \n    MSpike = SpikeMonitor ( neuron ) \n    net . add ( MSpike ) \n    MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) \n    net . add ( MStateV ) \n    print ( 'Measurement run' ) \n    net . run ( 500 * ms , report = 'text' ) \n    traj . v_standard_result = Brian2MonitorResult \n    traj . f_add_result ( 'SpikeMonitor' , MSpike ) \n    traj . f_add_result ( 'StateMonitorV' , MStateV ) "}
{"8260": "\ndef euler_scheme ( traj , diff_func ) : \n    steps = traj . steps \n    initial_conditions = traj . initial_conditions \n    dimension = len ( initial_conditions ) \n    result_array = np . zeros ( ( steps , dimension ) ) \n    func_params_dict = traj . func_params . f_to_dict ( short_names = 1 , fast_access = 1 ) \n    result_array [ 0 ] = initial_conditions \n    for idx in range ( 1 , steps ) : \n        result_array [ idx ] = diff_func ( result_array [ idx - 1 ] , ** func_params_dict ) * traj . dt + result_array [ idx - 1 ] \n    traj . f_add_result ( 'euler_evolution' , data = result_array , comment = 'Our time series data!' ) "}
{"8267": "\ndef compact_hdf5_file ( filename , name = None , index = None , keep_backup = 1 ) : \n    if name is None and index is None : \n        index = - 1 \n    tmp_traj = load_trajectory ( name , index , as_new = 0 , load_all = pypetconstants . LOAD_NOTHING , force = 1 , filename = filename ) \n    service = tmp_traj . v_storage_service \n    complevel = service . complevel \n    complib = service . complib \n    shuffle = service . shuffle \n    fletcher32 = service . fletcher32 \n    name_wo_ext , ext = os . path . splitext ( filename ) \n    tmp_filename = name_wo_ext + '_tmp' + ext \n    abs_filename = os . path . abspath ( filename ) \n    abs_tmp_filename = os . path . abspath ( tmp_filename ) \n    command = [ 'ptrepack' , '-v' , '--complib' , complib , '--complevel' , str ( complevel ) , '--shuffle' , str ( int ( shuffle ) ) , '--fletcher32' , str ( int ( fletcher32 ) ) , abs_filename , abs_tmp_filename ] \n    str_command = ' ' . join ( command ) \n    print ( 'Executing command `%s`' % str_command ) \n    retcode = subprocess . call ( command ) \n    if retcode != 0 : \n        print ( '#### ERROR: Compacting `%s` failed with errorcode %s! ####' % ( filename , str ( retcode ) ) ) \n    else : \n        print ( '#### Compacting successful ####' ) \n        print ( 'Renaming files' ) \n        if keep_backup : \n            backup_file_name = name_wo_ext + '_backup' + ext \n            os . rename ( filename , backup_file_name ) \n        else : \n            os . remove ( filename ) \n        os . rename ( tmp_filename , filename ) \n        print ( '### Compacting and Renaming finished ####' ) \n    return retcode "}
{"8268": "\ndef _explored_parameters_in_group ( traj , group_node ) : \n    explored = 0 \n    for param in traj . f_get_explored_parameters ( ) : \n        if param in group_node : \n            explored = 1 \n            break \n    return explored "}
{"8292": "\ndef _remove_subtree ( self , start_node , name , predicate = None ) : \n    def _delete_from_children ( node , child_name ) : \n        del node . _children [ child_name ] \n        if child_name in node . _groups : \n            del node . _groups [ child_name ] \n        elif child_name in node . _leaves : \n            del node . _leaves [ child_name ] \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    def _remove_subtree_inner ( node , predicate ) : \n        if not predicate ( node ) : \n            return 0 \n        elif node . v_is_group : \n            for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , list ( node . _groups . keys ( ) ) ) : \n                child_ = node . _children [ name_ ] \n                child_deleted = _remove_subtree_inner ( child_ , predicate ) \n                if child_deleted : \n                    _delete_from_children ( node , name_ ) \n                    del child_ \n            for link_ in list ( node . _links . keys ( ) ) : \n                node . f_remove_link ( link_ ) \n            if len ( node . _children ) == 0 : \n                self . _delete_node ( node ) \n                return 1 \n            else : \n                return 0 \n        else : \n            self . _delete_node ( node ) \n            return 1 \n    if name in start_node . _links : \n        start_node . f_remove_link ( name ) \n    else : \n        child = start_node . _children [ name ] \n        if predicate is None : \n            predicate = lambda x : 1 \n        if _remove_subtree_inner ( child , predicate ) : \n            _delete_from_children ( start_node , name ) \n            del child \n            return 1 \n        else : \n            return 0 "}
{"8294": "\ndef _remove_node_or_leaf ( self , instance , recursive = 0 ) : \n    full_name = instance . v_full_name \n    split_name = deque ( full_name . split ( '.' ) ) \n    self . _remove_along_branch ( self . _root_instance , split_name , recursive ) "}
{"8295": "\ndef _remove_along_branch ( self , actual_node , split_name , recursive = 0 ) : \n    if len ( split_name ) == 0 : \n        if actual_node . v_is_group and actual_node . f_has_children ( ) : \n            if recursive : \n                for child in list ( actual_node . _children . keys ( ) ) : \n                    actual_node . f_remove_child ( child , recursive = 1 ) \n            else : \n                raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) \n        self . _delete_node ( actual_node ) \n        return 1 \n    name = split_name . popleft ( ) \n    if name in actual_node . _links : \n        if len ( split_name ) > 0 : \n            raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) \n        actual_node . f_remove_link ( name ) \n    else : \n        child = actual_node . _children [ name ] \n        if self . _remove_along_branch ( child , split_name , recursive = recursive ) : \n            del actual_node . _children [ name ] \n            if name in actual_node . _groups : \n                del actual_node . _groups [ name ] \n            elif name in actual_node . _leaves : \n                del actual_node . _leaves [ name ] \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n            del child \n            return 0 "}
{"8296": "\ndef _translate_shortcut ( self , name ) : \n    if isinstance ( name , int ) : \n        return 1 , self . _root_instance . f_wildcard ( '$' , name ) \n    if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2 : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return 1 , self . _root_instance . f_wildcard ( '$' , int ( index ) ) \n            elif index == 'A' : \n                return 1 , self . _root_instance . f_wildcard ( '$' , - 1 ) \n    if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2 : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return 1 , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) \n            elif index == 'A' : \n                return 1 , self . _root_instance . f_wildcard ( '$set' , - 1 ) \n    if name in SHORTCUT_SET : \n        if name == 'par' : \n            return 1 , 'parameters' \n        elif name == 'dpar' : \n            return 1 , 'derived_parameters' \n        elif name == 'res' : \n            return 1 , 'results' \n        elif name == 'conf' : \n            return 1 , 'config' \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    return 0 , name "}
{"8299": "\ndef _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , add_prefix = 1 , check_naming = 1 ) : \n    args = list ( args ) \n    create_new = 1 \n    name = '' \n    instance = None \n    constructor = None \n    add_link = type_name == LINK \n    if add_link : \n        name = args [ 0 ] \n        instance = args [ 1 ] \n        create_new = 0 \n    elif len ( args ) == 1 and len ( kwargs ) == 0 : \n        item = args [ 0 ] \n        try : \n            name = item . v_full_name \n            instance = item \n            create_new = 0 \n        except AttributeError : \n            pass \n    if create_new : \n        if len ( args ) > 0 and inspect . isclass ( args [ 0 ] ) : \n            constructor = args . pop ( 0 ) \n        if len ( args ) > 0 and isinstance ( args [ 0 ] , str ) : \n            name = args . pop ( 0 ) \n        elif 'name' in kwargs : \n            name = kwargs . pop ( 'name' ) \n        elif 'full_name' in kwargs : \n            name = kwargs . pop ( 'full_name' ) \n        else : \n            raise ValueError ( 'Could not determine a name of the new item you want to add. ' 'Either pass the name as positional argument or as a keyword ' 'argument `name`.' ) \n    split_names = name . split ( '.' ) \n    if check_naming : \n        for idx , name in enumerate ( split_names ) : \n            translated_shortcut , name = self . _translate_shortcut ( name ) \n            replaced , name = self . _replace_wildcards ( name ) \n            if translated_shortcut or replaced : \n                split_names [ idx ] = name \n        faulty_names = self . _check_names ( split_names , start_node ) \n        if faulty_names : \n            full_name = '.' . join ( split_names ) \n            raise ValueError ( 'Your Parameter/Result/Node `%s` contains the following not admissible names: ' '%s please choose other names.' % ( full_name , faulty_names ) ) \n        if add_link : \n            if instance is None : \n                raise ValueError ( 'You must provide an instance to link to!' ) \n            if instance . v_is_root : \n                raise ValueError ( 'You cannot create a link to the root node' ) \n            if start_node . v_is_root and name in SUBTREE_MAPPING : \n                raise ValueError ( '`%s` is a reserved name for a group under root.' % name ) \n            if not self . _root_instance . f_contains ( instance , with_links = 0 , shortcuts = 0 ) : \n                raise ValueError ( 'You can only link to items within the trajectory tree!' ) \n    if add_prefix : \n        split_names = self . _add_prefix ( split_names , start_node , group_type_name ) \n    if group_type_name == GROUP : \n        add_leaf = type_name != group_type_name and not add_link \n        group_type_name , type_name = self . _determine_types ( start_node , split_names [ 0 ] , add_leaf , add_link ) \n    if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : \n        raise TypeError ( 'You are not allowed to add config or parameter data or groups ' 'during a single run.' ) \n    return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) "}
{"8300": "\ndef _add_to_tree ( self , start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) : \n    try : \n        act_node = start_node \n        last_idx = len ( split_names ) - 1 \n        add_link = type_name == LINK \n        link_added = 0 \n        for idx , name in enumerate ( split_names ) : \n            if name not in act_node . _children : \n                if idx == last_idx : \n                    if add_link : \n                        new_node = self . _create_link ( act_node , name , instance ) \n                        link_added = 1 \n                    elif group_type_name != type_name : \n                        new_node = self . _create_any_param_or_result ( act_node , name , type_name , instance , constructor , args , kwargs ) \n                        self . _flat_leaf_storage_dict [ new_node . v_full_name ] = new_node \n                    else : \n                        new_node = self . _create_any_group ( act_node , name , group_type_name , instance , constructor , args , kwargs ) \n                else : \n                    new_node = self . _create_any_group ( act_node , name , group_type_name ) \n                if name in self . _root_instance . _run_information : \n                    self . _root_instance . _run_parent_groups [ act_node . v_full_name ] = act_node \n                if self . _root_instance . _is_run : \n                    if link_added : \n                        self . _root_instance . _new_links [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n                    else : \n                        self . _root_instance . _new_nodes [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n            else : \n                if name in act_node . _links : \n                    raise AttributeError ( 'You cannot hop over links when adding ' 'data to the tree. ' 'There is a link called `%s` under `%s`.' % ( name , act_node . v_full_name ) ) \n                if idx == last_idx : \n                    if self . _root_instance . _no_clobber : \n                        self . _logger . warning ( 'You already have a group/instance/link `%s` ' 'under `%s`. ' 'However, you set `v_no_clobber=True`, ' 'so I will ignore your addition of ' 'data.' % ( name , act_node . v_full_name ) ) \n                    else : \n                        raise AttributeError ( 'You already have a group/instance/link `%s` ' 'under `%s`' % ( name , act_node . v_full_name ) ) \n            act_node = act_node . _children [ name ] \n        return act_node \n    except : \n        self . _logger . error ( 'Failed adding `%s` under `%s`.' % ( name , start_node . v_full_name ) ) \n        raise "}
{"8304": "\ndef _create_any_param_or_result ( self , parent_node , name , type_name , instance , constructor , args , kwargs ) : \n    root = self . _root_instance \n    full_name = self . _make_full_name ( parent_node . v_full_name , name ) \n    if instance is None : \n        if constructor is None : \n            if type_name == RESULT : \n                constructor = root . _standard_result \n            elif type_name in [ PARAMETER , CONFIG , DERIVED_PARAMETER ] : \n                constructor = root . _standard_parameter \n            else : \n                constructor = root . _standard_leaf \n        instance = root . _construct_instance ( constructor , full_name , * args , ** kwargs ) \n    else : \n        instance . _rename ( full_name ) \n    self . _set_details_tree_node ( parent_node , name , instance ) \n    where_dict = self . _map_type_to_dict ( type_name ) \n    full_name = instance . _full_name \n    if full_name in where_dict : \n        raise AttributeError ( full_name + ' is already part of trajectory,' ) \n    if type_name != RESULT and full_name in root . _changed_default_parameters : \n        self . _logger . info ( 'You have marked parameter %s for change before, so here you go!' % full_name ) \n        change_args , change_kwargs = root . _changed_default_parameters . pop ( full_name ) \n        instance . f_set ( * change_args , ** change_kwargs ) \n    where_dict [ full_name ] = instance \n    self . _add_to_nodes_and_leaves ( instance ) \n    parent_node . _children [ name ] = instance \n    parent_node . _leaves [ name ] = instance \n    if full_name in self . _root_instance . _explored_parameters : \n        instance . _explored = 1 \n        self . _root_instance . _explored_parameters [ full_name ] = instance \n    self . _logger . debug ( 'Added `%s` to trajectory.' % full_name ) \n    return instance "}
{"8306": "\ndef _iter_nodes ( self , node , recursive = 0 , max_depth = float ( 'inf' ) , with_links = 1 , in_search = 0 , predicate = None ) : \n    def _run_predicate ( x , run_name_set ) : \n        branch = x . v_run_branch \n        return branch == 'trajectory' or branch in run_name_set \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if predicate is None : \n        predicate = lambda x : 1 \n    elif isinstance ( predicate , ( tuple , list ) ) : \n        run_list = predicate \n        run_name_set = set ( ) \n        for item in run_list : \n            if item == - 1 : \n                run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - 1 ) ) \n            elif isinstance ( item , int ) : \n                run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) \n            else : \n                run_name_set . add ( item ) \n        predicate = lambda x : _run_predicate ( x , run_name_set ) \n    if recursive : \n        return NaturalNamingInterface . _recursive_traversal_bfs ( node , self . _root_instance . _linked_by , max_depth , with_links , in_search , predicate ) \n    else : \n        iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if predicate ( x [ 2 ] ) ) \n        if in_search : \n            return iterator \n        else : \n            return ( x [ 2 ] for x in iterator ) "}
{"8308": "\ndef _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = 1 , in_search = 0 , predicate = None ) : \n    if predicate is None : \n        predicate = lambda x : 1 \n    iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) \n    start = 1 \n    visited_linked_nodes = set ( [ ] ) \n    while 1 : \n        try : \n            depth , name , item = next ( iterator_queue ) \n            full_name = item . _full_name \n            if start or predicate ( item ) : \n                if full_name in visited_linked_nodes : \n                    if in_search : \n                        yield depth , name , item \n                elif depth <= max_depth : \n                    if start : \n                        start = 0 \n                    else : \n                        if in_search : \n                            yield depth , name , item \n                        else : \n                            yield item \n                    if full_name in linked_by : \n                        visited_linked_nodes . add ( full_name ) \n                    if not item . _is_leaf and depth < max_depth : \n                        child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) \n                        iterator_queue . add ( child_iterator ) \n        except StopIteration : \n            break "}
{"8310": "\ndef _search ( self , node , key , max_depth = float ( 'inf' ) , with_links = 1 , crun = None ) : \n    if key in node . _children and ( with_links or key not in node . _links ) : \n        return node . _children [ key ] , 1 \n    try : \n        result = self . _very_fast_search ( node , key , max_depth , with_links , crun ) \n        if result : \n            return result \n    except pex . TooManyGroupsError : \n        pass \n    except pex . NotUniqueNodeError : \n        pass \n    nodes_iterator = self . _iter_nodes ( node , recursive = 1 , max_depth = max_depth , in_search = 1 , with_links = with_links ) \n    result_node = None \n    result_depth = float ( 'inf' ) \n    for depth , name , child in nodes_iterator : \n        if depth > result_depth : \n            break \n        if key == name : \n            if result_node is not None : \n                raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once within ' 'the same depth %d. ' 'Full name of first occurrence is `%s` and of ' 'second `%s`' % ( key , child . v_depth , result_node . v_full_name , child . v_full_name ) ) \n            result_node = child \n            result_depth = depth \n    return result_node , result_depth "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = 1 ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - 1 ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = 0 ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if key != candidate . v_name or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if parent_full_name != '' : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if len ( candidate_split_name ) > max_depth : \n                break \n            if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            elif shortcuts : \n                candidate_set = set ( candidate_split_name ) \n                climbing = 1 \n                for name in split_name : \n                    if name not in candidate_set : \n                        climbing = 0 \n                        break \n                if climbing : \n                    count = 0 \n                    candidate_length = len ( candidate_split_name ) \n                    for idx in range ( candidate_length ) : \n                        if idx + split_length - count > candidate_length : \n                            break \n                        if split_name [ count ] == candidate_split_name [ idx ] : \n                            count += 1 \n                            if count == len ( split_name ) : \n                                result_list . append ( candidate ) \n                                full_name_set . add ( candidate . v_full_name ) \n                                break \n    return result_list "}
{"8313": "\ndef _add_group_from_storage ( self , args , kwargs ) : \n    return self . _nn_interface . _add_generic ( self , type_name = GROUP , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = 0 , check_naming = 0 ) "}
{"8314": "\ndef _add_leaf_from_storage ( self , args , kwargs ) : \n    return self . _nn_interface . _add_generic ( self , type_name = LEAF , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = 0 , check_naming = 0 ) "}
{"8315": "\ndef f_dir_data ( self ) : \n    if ( self . _nn_interface is not None and self . _nn_interface . _root_instance is not None and self . v_root . v_auto_load ) : \n        try : \n            if self . v_is_root : \n                self . f_load ( recursive = 1 , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON , with_meta_data = 0 , with_run_information = 0 ) \n            else : \n                self . f_load ( recursive = 1 , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON ) \n        except Exception as exc : \n            pass \n    return list ( self . _children . keys ( ) ) "}
{"8317": "\ndef f_get_parent ( self ) : \n    if self . v_is_root : \n        raise TypeError ( 'Root does not have a parent' ) \n    elif self . v_location == '' : \n        return self . v_root \n    else : \n        return self . v_root . f_get ( self . v_location , fast_access = 0 , shortcuts = 0 ) "}
{"8318": "\ndef f_add_group ( self , * args , ** kwargs ) : \n    return self . _nn_interface . _add_generic ( self , type_name = GROUP , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = 0 ) "}
{"8319": "\ndef f_add_link ( self , name_or_item , full_name_or_item = None ) : \n    if isinstance ( name_or_item , str ) : \n        name = name_or_item \n        if isinstance ( full_name_or_item , str ) : \n            instance = self . v_root . f_get ( full_name_or_item ) \n        else : \n            instance = full_name_or_item \n    else : \n        instance = name_or_item \n        name = instance . v_name \n    return self . _nn_interface . _add_generic ( self , type_name = LINK , group_type_name = GROUP , args = ( name , instance ) , kwargs = { } , add_prefix = 0 ) "}
{"8321": "\ndef f_add_leaf ( self , * args , ** kwargs ) : \n    return self . _nn_interface . _add_generic ( self , type_name = LEAF , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = 0 ) "}
{"8322": "\ndef f_remove ( self , recursive = 1 , predicate = None ) : \n    parent = self . f_get_parent ( ) \n    parent . f_remove_child ( self . v_name , recursive = recursive , predicate = predicate ) "}
{"8323": "\ndef f_remove_child ( self , name , recursive = 0 , predicate = None ) : \n    if name not in self . _children : \n        raise ValueError ( 'Your group `%s` does not contain the child `%s`.' % ( self . v_full_name , name ) ) \n    else : \n        child = self . _children [ name ] \n        if ( name not in self . _links and not child . v_is_leaf and child . f_has_children ( ) and not recursive ) : \n            raise TypeError ( 'Cannot remove child. It is a group with children. Use' ' f_remove with ``recursive = True``' ) \n        else : \n            self . _nn_interface . _remove_subtree ( self , name , predicate ) "}
{"8324": "\ndef f_contains ( self , item , with_links = 1 , shortcuts = 0 , max_depth = None ) : \n    try : \n        search_string = item . v_full_name \n        parent_full_name = self . v_full_name \n        if not search_string . startswith ( parent_full_name ) : \n            return 0 \n        if parent_full_name != '' : \n            search_string = search_string [ len ( parent_full_name ) + 1 : ] \n        else : \n            search_string = search_string \n        shortcuts = 0 \n    except AttributeError : \n        search_string = item \n        item = None \n    if search_string == '' : \n        return 0 \n    try : \n        result = self . f_get ( search_string , shortcuts = shortcuts , max_depth = max_depth , with_links = with_links ) \n    except AttributeError : \n        return 0 \n    if item is not None : \n        return id ( item ) == id ( result ) \n    else : \n        return 1 "}
{"8325": "\ndef f_get_default ( self , name , default = None , fast_access = 1 , with_links = 1 , shortcuts = 1 , max_depth = None , auto_load = 0 ) : \n    try : \n        return self . f_get ( name , fast_access = fast_access , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load , with_links = with_links ) \n    except ( AttributeError , pex . DataNotInStorageError ) : \n        return default "}
{"8326": "\ndef f_get_children ( self , copy = 1 ) : \n    if copy : \n        return self . _children . copy ( ) \n    else : \n        return self . _children "}
{"8327": "\ndef f_get_groups ( self , copy = 1 ) : \n    if copy : \n        return self . _groups . copy ( ) \n    else : \n        return self . _groups "}
{"8328": "\ndef f_get_leaves ( self , copy = 1 ) : \n    if copy : \n        return self . _leaves . copy ( ) \n    else : \n        return self . _leaves "}
{"8329": "\ndef f_get_links ( self , copy = 1 ) : \n    if copy : \n        return self . _links . copy ( ) \n    else : \n        return self . _links "}
{"8330": "\ndef f_store_child ( self , name , recursive = 0 , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not self . f_contains ( name , shortcuts = 0 ) : \n        raise ValueError ( 'Your group `%s` does not (directly) contain the child `%s`. ' 'Please not that shortcuts are not allowed for `f_store_child`.' % ( self . v_full_name , name ) ) \n    traj = self . _nn_interface . _root_instance \n    storage_service = traj . v_storage_service \n    storage_service . store ( pypetconstants . TREE , self , name , trajectory_name = traj . v_name , recursive = recursive , store_data = store_data , max_depth = max_depth ) "}
{"8331": "\ndef f_store ( self , recursive = 1 , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    traj = self . _nn_interface . _root_instance \n    storage_service = traj . v_storage_service \n    storage_service . store ( pypetconstants . GROUP , self , trajectory_name = traj . v_name , recursive = recursive , store_data = store_data , max_depth = max_depth ) "}
{"8332": "\ndef f_load_child ( self , name , recursive = 0 , load_data = pypetconstants . LOAD_DATA , max_depth = None ) : \n    traj = self . _nn_interface . _root_instance \n    storage_service = traj . v_storage_service \n    storage_service . load ( pypetconstants . TREE , self , name , trajectory_name = traj . v_name , load_data = load_data , recursive = recursive , max_depth = max_depth ) \n    return self . f_get ( name , shortcuts = 0 ) "}
{"8333": "\ndef f_load ( self , recursive = 1 , load_data = pypetconstants . LOAD_DATA , max_depth = None ) : \n    traj = self . _nn_interface . _root_instance \n    storage_service = traj . v_storage_service \n    storage_service . load ( pypetconstants . GROUP , self , trajectory_name = traj . v_name , load_data = load_data , recursive = recursive , max_depth = max_depth ) \n    return self "}
{"8343": "\ndef add_commit_variables ( traj , commit ) : \n    git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) \n    git_short_name = str ( commit . hexsha [ 0 : 7 ] ) \n    git_commit_name = 'commit_%s_' % git_short_name \n    git_commit_name = 'git.' + git_commit_name + git_time_value \n    if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = 0 ) : \n        git_commit_name += '.' \n        traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) \n        traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) \n        traj . f_add_config ( git_commit_name + 'committed_date' , commit . committed_date , comment = 'Date of commit as unix epoch seconds' ) \n        traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , comment = 'The commit message' ) "}
{"8344": "\ndef make_git_commit ( environment , git_repository , user_message , git_fail ) : \n    repo = git . Repo ( git_repository ) \n    index = repo . index \n    traj = environment . v_trajectory \n    if traj . v_comment : \n        commentstr = ', Comment: `%s`' % traj . v_comment \n    else : \n        commentstr = '' \n    if user_message : \n        user_message += ' -- ' \n    message = '%sTrajectory: `%s`, Time: `%s`, %s' % ( user_message , traj . v_name , traj . v_time , commentstr ) \n    diff = index . diff ( None ) \n    if diff : \n        if git_fail : \n            raise pex . GitDiffError ( 'Found not committed changes!' ) \n        repo . git . add ( '-u' ) \n        commit = index . commit ( message ) \n        new_commit = 1 \n    else : \n        commit = repo . commit ( None ) \n        new_commit = 0 \n    add_commit_variables ( traj , commit ) \n    return new_commit , commit . hexsha "}
{"8347": "\ndef progressbar ( index , total , percentage_step = 10 , logger = 'print' , log_level = logging . INFO , reprint = 1 , time = 1 , length = 20 , fmt_string = None , reset = 0 ) : \n    return _progressbar ( index = index , total = total , percentage_step = percentage_step , logger = logger , log_level = log_level , reprint = reprint , time = time , length = length , fmt_string = fmt_string , reset = reset ) "}
{"8348": "\ndef _get_argspec ( func ) : \n    if inspect . isclass ( func ) : \n        func = func . __init__ \n    if not inspect . isfunction ( func ) : \n        return [ ] , 0 \n    parameters = inspect . signature ( func ) . parameters \n    args = [ ] \n    uses_starstar = 0 \n    for par in parameters . values ( ) : \n        if ( par . kind == inspect . Parameter . POSITIONAL_OR_KEYWORD or par . kind == inspect . Parameter . KEYWORD_ONLY ) : \n            args . append ( par . name ) \n        elif par . kind == inspect . Parameter . VAR_KEYWORD : \n            uses_starstar = 1 \n    return args , uses_starstar "}
{"8352": "\ndef racedirs ( path ) : \n    if os . path . isfile ( path ) : \n        raise IOError ( 'Path `%s` is already a file not a directory' ) \n    while 1 : \n        try : \n            if os . path . isdir ( path ) : \n                break \n            os . makedirs ( path ) \n        except EnvironmentError as exc : \n            if exc . errno != 17 : \n                raise "}
{"8355": "\ndef f_to_dict ( self , copy = 1 ) : \n    if copy : \n        return self . _dict . copy ( ) \n    else : \n        return self . _dict "}
{"8358": "\ndef make_ordinary_result ( result , key , trajectory = None , reload = 1 ) : \n    shared_data = result . f_get ( key ) \n    if trajectory is not None : \n        shared_data . traj = trajectory \n    shared_data . _request_data ( 'make_ordinary' ) \n    result . f_remove ( key ) \n    if reload : \n        trajectory . f_load_item ( result , load_data = pypetconstants . OVERWRITE_DATA ) \n    return result "}
{"8367": "\ndef send_done ( self ) : \n    self . start ( test_connection = 0 ) \n    self . _logger . debug ( 'Sending shutdown signal' ) \n    self . _req_rep ( ZMQServer . DONE ) "}
{"8368": "\ndef finalize ( self ) : \n    if self . _context is not None : \n        if self . _socket is not None : \n            self . _close_socket ( confused = 0 ) \n        self . _context . term ( ) \n        self . _context = None \n        self . _poll = None "}
{"8369": "\ndef start ( self , test_connection = 1 ) : \n    if self . _context is None : \n        self . _logger . debug ( 'Starting Client' ) \n        self . _context = zmq . Context ( ) \n        self . _poll = zmq . Poller ( ) \n        self . _start_socket ( ) \n        if test_connection : \n            self . test_ping ( ) "}
{"8370": "\ndef _req_rep_retry ( self , request ) : \n    retries_left = self . RETRIES \n    while retries_left : \n        self . _logger . log ( 1 , 'Sending REQ `%s`' , request ) \n        self . _send_request ( request ) \n        socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) \n        if socks . get ( self . _socket ) == zmq . POLLIN : \n            response = self . _receive_response ( ) \n            self . _logger . log ( 1 , 'Received REP `%s`' , response ) \n            return response , self . RETRIES - retries_left \n        else : \n            self . _logger . debug ( 'No response from server (%d retries left)' % retries_left ) \n            self . _close_socket ( confused = 1 ) \n            retries_left -= 1 \n            if retries_left == 0 : \n                raise RuntimeError ( 'Server seems to be offline!' ) \n            time . sleep ( self . SLEEP ) \n            self . _start_socket ( ) "}
{"8371": "\ndef acquire ( self ) : \n    self . start ( test_connection = 0 ) \n    while 1 : \n        str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) \n        response = str_response . split ( LockerServer . DELIMITER ) \n        if response [ 0 ] == LockerServer . GO : \n            return 1 \n        elif response [ 0 ] == LockerServer . LOCK_ERROR and retries > 0 : \n            self . _logger . error ( str_response + '; Probably due to retry' ) \n            return 1 \n        elif response [ 0 ] == LockerServer . WAIT : \n            time . sleep ( self . SLEEP ) \n        else : \n            raise RuntimeError ( 'Response `%s` not understood' % response ) "}
{"8372": "\ndef listen ( self ) : \n    count = 0 \n    self . _start ( ) \n    while 1 : \n        result = self . _socket . recv_pyobj ( ) \n        if isinstance ( result , tuple ) : \n            request , data = result \n        else : \n            request = result \n            data = None \n        if request == self . SPACE : \n            if self . queue . qsize ( ) + count < self . queue_maxsize : \n                self . _socket . send_string ( self . SPACE_AVAILABLE ) \n                count += 1 \n            else : \n                self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) \n        elif request == self . PING : \n            self . _socket . send_string ( self . PONG ) \n        elif request == self . DATA : \n            self . _socket . send_string ( self . STORING ) \n            self . queue . put ( data ) \n            count -= 1 \n        elif request == self . DONE : \n            self . _socket . send_string ( ZMQServer . CLOSED ) \n            self . queue . put ( ( 'DONE' , [ ] , { } ) ) \n            self . _close ( ) \n            break \n        else : \n            raise RuntimeError ( 'I did not understand your request %s' % request ) "}
{"8373": "\ndef put ( self , data , block = 1 ) : \n    self . start ( test_connection = 0 ) \n    while 1 : \n        response = self . _req_rep ( QueuingServerMessageListener . SPACE ) \n        if response == QueuingServerMessageListener . SPACE_AVAILABLE : \n            self . _req_rep ( ( QueuingServerMessageListener . DATA , data ) ) \n            break \n        else : \n            time . sleep ( 0.01 ) "}
{"8375": "\ndef _handle_data ( self , msg , args , kwargs ) : \n    stop = 0 \n    try : \n        if msg == 'DONE' : \n            stop = 1 \n        elif msg == 'STORE' : \n            if 'msg' in kwargs : \n                store_msg = kwargs . pop ( 'msg' ) \n            else : \n                store_msg = args [ 0 ] \n                args = args [ 1 : ] \n            if 'stuff_to_store' in kwargs : \n                stuff_to_store = kwargs . pop ( 'stuff_to_store' ) \n            else : \n                stuff_to_store = args [ 0 ] \n                args = args [ 1 : ] \n            trajectory_name = kwargs [ 'trajectory_name' ] \n            if self . _trajectory_name != trajectory_name : \n                if self . _storage_service . is_open : \n                    self . _close_file ( ) \n                self . _trajectory_name = trajectory_name \n                self . _open_file ( ) \n            self . _storage_service . store ( store_msg , stuff_to_store , * args , ** kwargs ) \n            self . _storage_service . store ( pypetconstants . FLUSH , None ) \n            self . _check_and_collect_garbage ( ) \n        else : \n            raise RuntimeError ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) \n    except Exception : \n        self . _logger . exception ( 'ERROR occurred during storing!' ) \n        time . sleep ( 0.01 ) \n        pass \n    return stop "}
{"8376": "\ndef run ( self ) : \n    try : \n        while 1 : \n            msg , args , kwargs = self . _receive_data ( ) \n            stop = self . _handle_data ( msg , args , kwargs ) \n            if stop : \n                break \n    finally : \n        if self . _storage_service . is_open : \n            self . _close_file ( ) \n        self . _trajectory_name = '' "}
{"8377": "\ndef _receive_data ( self ) : \n    result = self . queue . get ( block = 1 ) \n    if hasattr ( self . queue , 'task_done' ) : \n        self . queue . task_done ( ) \n    return result "}
{"8378": "\ndef _receive_data ( self ) : \n    while 1 : \n        while len ( self . _buffer ) < self . max_size and self . conn . poll ( ) : \n            data = self . _read_chunks ( ) \n            if data is not None : \n                self . _buffer . append ( data ) \n        if len ( self . _buffer ) > 0 : \n            return self . _buffer . popleft ( ) "}
{"8391": "\ndef main ( ) : \n    rules_to_test = [ 10 , 30 , 90 , 110 , 184 ] \n    steps = 250 \n    ncells = 400 \n    seed = 100042 \n    initial_states = [ 'single' , 'random' ] \n    folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_original' ) \n    if not os . path . isdir ( folder ) : \n        os . makedirs ( folder ) \n    filename = os . path . join ( folder , 'all_patterns.p' ) \n    print ( 'Computing all patterns' ) \n    all_patterns = [ ] \n    for idx , rule_number in enumerate ( rules_to_test ) : \n        for initial_name in initial_states : \n            initial_state = make_initial_state ( initial_name , ncells , seed = seed ) \n            pattern = cellular_automaton_1D ( initial_state , rule_number , steps ) \n            all_patterns . append ( ( rule_number , initial_name , pattern ) ) \n        progressbar ( idx , len ( rules_to_test ) , reprint = 1 ) \n    with open ( filename , 'wb' ) as file : \n        pickle . dump ( all_patterns , file = file ) \n    print ( 'Plotting all patterns' ) \n    for idx , pattern_tuple in enumerate ( all_patterns ) : \n        rule_number , initial_name , pattern = pattern_tuple \n        filename = os . path . join ( folder , 'rule_%s_%s.png' % ( str ( rule_number ) , initial_name ) ) \n        plot_pattern ( pattern , rule_number , filename ) \n        progressbar ( idx , len ( all_patterns ) , reprint = 1 ) "}
{"8394": "\ndef load ( self , msg , stuff_to_load , * args , ** kwargs ) : \n    opened = 1 \n    try : \n        opened = self . _srvc_opening_routine ( 'r' , kwargs = kwargs ) \n        if msg == pypetconstants . TRAJECTORY : \n            self . _trj_load_trajectory ( stuff_to_load , * args , ** kwargs ) \n        elif msg == pypetconstants . LEAF : \n            self . _prm_load_parameter_or_result ( stuff_to_load , * args , ** kwargs ) \n        elif msg == pypetconstants . GROUP : \n            self . _grp_load_group ( stuff_to_load , * args , ** kwargs ) \n        elif msg == pypetconstants . TREE : \n            self . _tree_load_sub_branch ( stuff_to_load , * args , ** kwargs ) \n        elif msg == pypetconstants . LIST : \n            self . _srvc_load_several_items ( stuff_to_load , * args , ** kwargs ) \n        else : \n            raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) \n    except pt . NoSuchNodeError as exc : \n        self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) \n        raise pex . DataNotInStorageError ( repr ( exc ) ) \n    except : \n        self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) \n        raise \n    finally : \n        self . _srvc_closing_routine ( opened ) "}
{"8395": "\ndef store ( self , msg , stuff_to_store , * args , ** kwargs ) : \n    opened = 1 \n    try : \n        opened = self . _srvc_opening_routine ( 'a' , msg , kwargs ) \n        if msg == pypetconstants . MERGE : \n            self . _trj_merge_trajectories ( * args , ** kwargs ) \n        elif msg == pypetconstants . BACKUP : \n            self . _trj_backup_trajectory ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . PREPARE_MERGE : \n            self . _trj_prepare_merge ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . TRAJECTORY : \n            self . _trj_store_trajectory ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . SINGLE_RUN : \n            self . _srn_store_single_run ( stuff_to_store , * args , ** kwargs ) \n        elif msg in pypetconstants . LEAF : \n            self . _prm_store_parameter_or_result ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . DELETE : \n            self . _all_delete_parameter_or_result_or_group ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . GROUP : \n            self . _grp_store_group ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . TREE : \n            self . _tree_store_sub_branch ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . DELETE_LINK : \n            self . _lnk_delete_link ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . LIST : \n            self . _srvc_store_several_items ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . ACCESS_DATA : \n            return self . _hdf5_interact_with_data ( stuff_to_store , * args , ** kwargs ) \n        elif msg == pypetconstants . OPEN_FILE : \n            opened = 0 \n            self . _keep_open = 1 \n            self . _node_processing_timer . active = 0 \n        elif msg == pypetconstants . CLOSE_FILE : \n            opened = 1 \n            self . _keep_open = 0 \n        elif msg == pypetconstants . FLUSH : \n            self . _hdf5file . flush ( ) \n        else : \n            raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) \n    except : \n        self . _logger . error ( 'Failed storing `%s`' % str ( stuff_to_store ) ) \n        raise \n    finally : \n        self . _srvc_closing_routine ( opened ) "}
{"8399": "\ndef _srvc_closing_routine ( self , closing ) : \n    if ( not self . _keep_open and closing and self . is_open ) : \n        f_fd = self . _hdf5file . fileno ( ) \n        self . _hdf5file . flush ( ) \n        try : \n            os . fsync ( f_fd ) \n            try : \n                self . _hdf5store . flush ( fsync = 1 ) \n            except TypeError : \n                f_fd = self . _hdf5store . _handle . fileno ( ) \n                self . _hdf5store . flush ( ) \n                os . fsync ( f_fd ) \n        except OSError as exc : \n            errmsg = ( 'Encountered OSError while flushing file.' 'If you are using Windows, don`t worry! ' 'I will ignore the error and try to close the file. ' 'Original error: %s' % repr ( exc ) ) \n            self . _logger . debug ( errmsg ) \n        self . _hdf5store . close ( ) \n        if self . _hdf5file . isopen : \n            self . _logger . error ( 'Could not close HDF5 file!' ) \n        self . _hdf5file = None \n        self . _hdf5store = None \n        self . _trajectory_group = None \n        self . _trajectory_name = None \n        self . _trajectory_index = None \n        self . _overview_group_ = None \n        self . _logger . debug ( 'Closing HDF5 file' ) \n        return 1 \n    else : \n        return 0 "}
{"8401": "\ndef _trj_backup_trajectory ( self , traj , backup_filename = None ) : \n    self . _logger . info ( 'Storing backup of %s.' % traj . v_name ) \n    mypath , _ = os . path . split ( self . _filename ) \n    if backup_filename is None : \n        backup_filename = os . path . join ( '%s' % mypath , 'backup_%s.hdf5' % traj . v_name ) \n    backup_hdf5file = pt . open_file ( filename = backup_filename , mode = 'a' , title = backup_filename ) \n    if '/' + self . _trajectory_name in backup_hdf5file : \n        raise ValueError ( 'I cannot backup  `%s` into file `%s`, there is already a ' 'trajectory with that name.' % ( traj . v_name , backup_filename ) ) \n    backup_root = backup_hdf5file . root \n    self . _trajectory_group . _f_copy ( newparent = backup_root , recursive = 1 ) \n    backup_hdf5file . flush ( ) \n    backup_hdf5file . close ( ) \n    self . _logger . info ( 'Finished backup of %s.' % traj . v_name ) "}
{"8404": "\ndef _trj_load_meta_data ( self , traj , load_data , as_new , with_run_information , force ) : \n    metatable = self . _overview_group . info \n    metarow = metatable [ 0 ] \n    try : \n        version = metarow [ 'version' ] . decode ( 'utf-8' ) \n    except ( IndexError , ValueError ) as ke : \n        self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) \n        version = '`COULD NOT BE LOADED`' \n    try : \n        python = metarow [ 'python' ] . decode ( 'utf-8' ) \n    except ( IndexError , ValueError ) as ke : \n        self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) \n        python = '`COULD NOT BE LOADED`' \n    self . _trj_check_version ( version , python , force ) \n    self . _grp_load_group ( traj , load_data = load_data , with_links = 0 , recursive = 0 , _traj = traj , _as_new = as_new , _hdf5_group = self . _trajectory_group ) \n    if as_new : \n        length = int ( metarow [ 'length' ] ) \n        for irun in range ( length ) : \n            traj . _add_run_info ( irun ) \n    else : \n        traj . _comment = metarow [ 'comment' ] . decode ( 'utf-8' ) \n        traj . _timestamp = float ( metarow [ 'timestamp' ] ) \n        traj . _trajectory_timestamp = traj . _timestamp \n        traj . _time = metarow [ 'time' ] . decode ( 'utf-8' ) \n        traj . _trajectory_time = traj . _time \n        traj . _name = metarow [ 'name' ] . decode ( 'utf-8' ) \n        traj . _trajectory_name = traj . _name \n        traj . _version = version \n        traj . _python = python \n        single_run_table = self . _overview_group . runs \n        if with_run_information : \n            for row in single_run_table . iterrows ( ) : \n                name = row [ 'name' ] . decode ( 'utf-8' ) \n                idx = int ( row [ 'idx' ] ) \n                timestamp = float ( row [ 'timestamp' ] ) \n                time_ = row [ 'time' ] . decode ( 'utf-8' ) \n                completed = int ( row [ 'completed' ] ) \n                summary = row [ 'parameter_summary' ] . decode ( 'utf-8' ) \n                hexsha = row [ 'short_environment_hexsha' ] . decode ( 'utf-8' ) \n                try : \n                    runtime = row [ 'runtime' ] . decode ( 'utf-8' ) \n                    finish_timestamp = float ( row [ 'finish_timestamp' ] ) \n                except ( IndexError , ValueError ) as ke : \n                    runtime = '' \n                    finish_timestamp = 0.0 \n                    self . _logger . debug ( 'Could not load runtime, ' + repr ( ke ) ) \n                info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time_ , 'completed' : completed , 'name' : name , 'parameter_summary' : summary , 'short_environment_hexsha' : hexsha } \n                traj . _add_run_info ( ** info_dict ) \n        else : \n            traj . _length = single_run_table . nrows \n    self . _trj_load_exploration ( traj ) \n    self . _srvc_load_hdf5_settings ( ) "}
{"8405": "\ndef _tree_load_sub_branch ( self , traj_node , branch_name , load_data = pypetconstants . LOAD_DATA , with_links = 1 , recursive = 0 , max_depth = None , _trajectory = None , _as_new = 0 , _hdf5_group = None ) : \n    if load_data == pypetconstants . LOAD_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if _trajectory is None : \n        _trajectory = traj_node . v_root \n    if _hdf5_group is None : \n        hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) \n        if hdf5_group_name == '' : \n            _hdf5_group = self . _trajectory_group \n        else : \n            try : \n                _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_group_name ) \n            except pt . NoSuchNodeError : \n                self . _logger . error ( 'Cannot find `%s` the hdf5 node `%s` does not exist!' % ( traj_node . v_full_name , hdf5_group_name ) ) \n                raise \n    split_names = branch_name . split ( '.' ) \n    final_group_name = split_names . pop ( ) \n    current_depth = 1 \n    for name in split_names : \n        if current_depth > max_depth : \n            return \n        _hdf5_group = getattr ( _hdf5_group , name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = 0 , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) \n        current_depth += 1 \n        traj_node = traj_node . _children [ name ] \n    if current_depth <= max_depth : \n        _hdf5_group = getattr ( _hdf5_group , final_group_name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) "}
{"8407": "\ndef _trj_fill_run_table ( self , traj , start , stop ) : \n    def _make_row ( info_dict ) : \n        row = ( info_dict [ 'idx' ] , info_dict [ 'name' ] , info_dict [ 'time' ] , info_dict [ 'timestamp' ] , info_dict [ 'finish_timestamp' ] , info_dict [ 'runtime' ] , info_dict [ 'parameter_summary' ] , info_dict [ 'short_environment_hexsha' ] , info_dict [ 'completed' ] ) \n        return row \n    runtable = getattr ( self . _overview_group , 'runs' ) \n    rows = [ ] \n    updated_run_information = traj . _updated_run_information \n    for idx in range ( start , stop ) : \n        info_dict = traj . _run_information [ traj . _single_run_ids [ idx ] ] \n        rows . append ( _make_row ( info_dict ) ) \n        updated_run_information . discard ( idx ) \n    if rows : \n        runtable . append ( rows ) \n        runtable . flush ( ) \n    rows = [ ] \n    indices = [ ] \n    for idx in updated_run_information : \n        info_dict = traj . f_get_run_information ( idx , copy = 0 ) \n        rows . append ( _make_row ( info_dict ) ) \n        indices . append ( idx ) \n    if rows : \n        runtable . modify_coordinates ( indices , rows ) \n    traj . _updated_run_information = set ( ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = 0 , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = 0 \n        maximum_display_other = 10 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if counter < maximum_display_other : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                elif counter == maximum_display_other : \n                    self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter += 1 \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = 1 , recursive = 1 , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = 1 "}
{"8412": "\ndef _tree_store_sub_branch ( self , traj_node , branch_name , store_data = pypetconstants . STORE_DATA , with_links = 1 , recursive = 0 , max_depth = None , hdf5_group = None ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if hdf5_group is None : \n        location = traj_node . v_full_name \n        hdf5_location = location . replace ( '.' , '/' ) \n        try : \n            if location == '' : \n                hdf5_group = self . _trajectory_group \n            else : \n                hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_location ) \n        except pt . NoSuchNodeError : \n            self . _logger . debug ( 'Cannot store `%s` the parental hdf5 node with path `%s` does ' 'not exist on disk.' % ( traj_node . v_name , hdf5_location ) ) \n            if traj_node . v_is_leaf : \n                self . _logger . error ( 'Cannot store `%s` the parental hdf5 ' 'node with path `%s` does ' 'not exist on disk! The child ' 'you want to store is a leaf node,' 'that cannot be stored without ' 'the parental node existing on ' 'disk.' % ( traj_node . v_name , hdf5_location ) ) \n                raise \n            else : \n                self . _logger . debug ( 'I will try to store the path from trajectory root to ' 'the child now.' ) \n                self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , traj_node . v_full_name + '.' + branch_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth + traj_node . v_depth , hdf5_group = self . _trajectory_group ) \n                return \n    current_depth = 1 \n    split_names = branch_name . split ( '.' ) \n    leaf_name = split_names . pop ( ) \n    for name in split_names : \n        if current_depth > max_depth : \n            return \n        self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , recursive = 0 , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) \n        current_depth += 1 \n        traj_node = traj_node . _children [ name ] \n        hdf5_group = getattr ( hdf5_group , name ) \n    if current_depth <= max_depth : \n        self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) "}
{"8414": "\ndef _tree_load_nodes_dfs ( self , parent_traj_node , load_data , with_links , recursive , max_depth , current_depth , trajectory , as_new , hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    loading_list = [ ( parent_traj_node , current_depth , hdf5_group ) ] \n    while loading_list : \n        parent_traj_node , current_depth , hdf5_group = loading_list . pop ( ) \n        if isinstance ( hdf5_group , pt . link . SoftLink ) : \n            if with_links : \n                self . _tree_load_link ( parent_traj_node , load_data = load_data , traj = trajectory , as_new = as_new , hdf5_soft_link = hdf5_group ) \n            continue \n        name = hdf5_group . _v_name \n        is_leaf = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . LEAF ) \n        in_trajectory = name in parent_traj_node . _children \n        if is_leaf : \n            if in_trajectory : \n                instance = parent_traj_node . _children [ name ] \n            else : \n                instance = self . _tree_create_leaf ( name , trajectory , hdf5_group ) \n                parent_traj_node . _add_leaf_from_storage ( args = ( instance , ) , kwargs = { } ) \n            self . _prm_load_parameter_or_result ( instance , load_data = load_data , _hdf5_group = hdf5_group ) \n            if as_new : \n                instance . _stored = 0 \n        else : \n            if in_trajectory : \n                traj_group = parent_traj_node . _children [ name ] \n                if load_data == pypetconstants . OVERWRITE_DATA : \n                    traj_group . v_annotations . f_empty ( ) \n                    traj_group . v_comment = '' \n            else : \n                if HDF5StorageService . CLASS_NAME in hdf5_group . _v_attrs : \n                    class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) \n                    class_constructor = trajectory . _create_class ( class_name ) \n                    instance = trajectory . _construct_instance ( class_constructor , name ) \n                    args = ( instance , ) \n                else : \n                    args = ( name , ) \n                traj_group = parent_traj_node . _add_group_from_storage ( args = args , kwargs = { } ) \n            self . _grp_load_group ( traj_group , load_data = load_data , with_links = with_links , recursive = 0 , max_depth = max_depth , _traj = trajectory , _as_new = as_new , _hdf5_group = hdf5_group ) \n            if recursive and current_depth < max_depth : \n                new_depth = current_depth + 1 \n                for children in ( hdf5_group . _v_groups , hdf5_group . _v_links ) : \n                    for new_hdf5_group_name in children : \n                        new_hdf5_group = children [ new_hdf5_group_name ] \n                        loading_list . append ( ( traj_group , new_depth , new_hdf5_group ) ) "}
{"8415": "\ndef _tree_store_nodes_dfs ( self , parent_traj_node , name , store_data , with_links , recursive , max_depth , current_depth , parent_hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    store_list = [ ( parent_traj_node , name , current_depth , parent_hdf5_group ) ] \n    while store_list : \n        parent_traj_node , name , current_depth , parent_hdf5_group = store_list . pop ( ) \n        if name in parent_traj_node . _links : \n            if with_links : \n                self . _tree_store_link ( parent_traj_node , name , parent_hdf5_group ) \n            continue \n        traj_node = parent_traj_node . _children [ name ] \n        if not hasattr ( parent_hdf5_group , name ) : \n            newly_created = 1 \n            new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , filters = self . _all_get_filters ( ) ) \n        else : \n            newly_created = 0 \n            new_hdf5_group = getattr ( parent_hdf5_group , name ) \n        if traj_node . v_is_leaf : \n            self . _prm_store_parameter_or_result ( traj_node , store_data = store_data , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n        else : \n            self . _grp_store_group ( traj_node , store_data = store_data , with_links = with_links , recursive = 0 , max_depth = max_depth , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n            if recursive and current_depth < max_depth : \n                for child in traj_node . _children . keys ( ) : \n                    store_list . append ( ( traj_node , child , current_depth + 1 , new_hdf5_group ) ) "}
{"8420": "\ndef _all_recall_native_type ( self , data , ptitem , prefix ) : \n    typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) \n    colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) \n    type_changed = 0 \n    if colltype == HDF5StorageService . COLL_SCALAR : \n        if isinstance ( data , np . ndarray ) : \n            data = np . array ( [ data ] ) [ 0 ] \n            type_changed = 1 \n        if not typestr is None : \n            if typestr != type ( data ) . __name__ : \n                if typestr == str . __name__ : \n                    data = data . decode ( self . _encoding ) \n                else : \n                    try : \n                        data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) \n                    except KeyError : \n                        data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) \n                type_changed = 1 \n    elif ( colltype == HDF5StorageService . COLL_TUPLE or colltype == HDF5StorageService . COLL_LIST ) : \n        if type ( data ) is not list and type is not tuple : \n            type_changed = 1 \n            data = list ( data ) \n        if len ( data ) > 0 : \n            first_item = data [ 0 ] \n            if not typestr == type ( first_item ) . __name__ : \n                if not isinstance ( data , list ) : \n                    data = list ( data ) \n                for idx , item in enumerate ( data ) : \n                    if typestr == str . __name__ : \n                        data [ idx ] = data [ idx ] . decode ( self . _encoding ) \n                    else : \n                        try : \n                            data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) \n                        except KeyError : \n                            data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) \n                    type_changed = 1 \n        if colltype == HDF5StorageService . COLL_TUPLE : \n            if type ( data ) is not tuple : \n                data = tuple ( data ) \n                type_changed = 1 \n    elif colltype == HDF5StorageService . COLL_EMPTY_DICT : \n        data = { } \n        type_changed = 1 \n    elif isinstance ( data , np . ndarray ) : \n        if typestr == str . __name__ : \n            data = np . core . defchararray . decode ( data , self . _encoding ) \n            type_changed = 1 \n        if colltype == HDF5StorageService . COLL_MATRIX : \n            data = np . matrix ( data ) \n            type_changed = 1 \n    return data , type_changed "}
{"8423": "\ndef _all_extract_insert_dict ( self , item , colnames , additional_info = None ) : \n    insert_dict = { } \n    if 'length' in colnames : \n        insert_dict [ 'length' ] = len ( item ) \n    if 'comment' in colnames : \n        comment = self . _all_cut_string ( item . v_comment . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH , self . _logger ) \n        insert_dict [ 'comment' ] = comment \n    if 'location' in colnames : \n        insert_dict [ 'location' ] = item . v_location . encode ( 'utf-8' ) \n    if 'name' in colnames : \n        name = item . _name if ( not item . v_is_root or not item . v_is_run ) else item . _crun \n        insert_dict [ 'name' ] = name . encode ( 'utf-8' ) \n    if 'class_name' in colnames : \n        insert_dict [ 'class_name' ] = item . f_get_class_name ( ) . encode ( 'utf-8' ) \n    if 'value' in colnames : \n        insert_dict [ 'value' ] = self . _all_cut_string ( item . f_val_to_str ( ) . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , self . _logger ) \n    if 'hexdigest' in colnames : \n        insert_dict [ 'hexdigest' ] = additional_info [ 'hexdigest' ] \n    if 'idx' in colnames : \n        insert_dict [ 'idx' ] = item . v_idx \n    if 'time' in colnames : \n        time_ = item . _time \n        insert_dict [ 'time' ] = time_ . encode ( 'utf-8' ) \n    if 'timestamp' in colnames : \n        timestamp = item . _timestamp \n        insert_dict [ 'timestamp' ] = timestamp \n    if 'range' in colnames : \n        third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 \n        item_range = itools . islice ( item . f_get_range ( copy = 0 ) , 0 , third_length ) \n        range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) \n        insert_dict [ 'range' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) \n    if 'array' in colnames : \n        third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 \n        item_range = itools . islice ( item . f_get_range ( copy = 0 ) , 0 , third_length ) \n        range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) \n        insert_dict [ 'array' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) \n    if 'version' in colnames : \n        insert_dict [ 'version' ] = item . v_version . encode ( 'utf-8' ) \n    if 'python' in colnames : \n        insert_dict [ 'python' ] = item . v_python . encode ( 'utf-8' ) \n    if 'finish_timestamp' in colnames : \n        insert_dict [ 'finish_timestamp' ] = item . _finish_timestamp_run \n    return insert_dict "}
{"8425": "\ndef _all_create_or_get_group ( self , name , parent_hdf5_group = None ) : \n    if not name in parent_hdf5_group : \n        new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , title = name , filters = self . _all_get_filters ( ) ) \n        return new_hdf5_group , 1 \n    else : \n        new_hdf5_group = parent_hdf5_group . _f_get_child ( name ) \n        return new_hdf5_group , 0 "}
{"8426": "\ndef _all_create_or_get_groups ( self , key , start_hdf5_group = None ) : \n    if start_hdf5_group is None : \n        newhdf5_group = self . _trajectory_group \n    else : \n        newhdf5_group = start_hdf5_group \n    created = 0 \n    if key == '' : \n        return newhdf5_group , created \n    split_key = key . split ( '.' ) \n    for name in split_key : \n        newhdf5_group , created = self . _all_create_or_get_group ( name , newhdf5_group ) \n    return newhdf5_group , created "}
{"8427": "\ndef _ann_store_annotations ( self , item_with_annotations , node , overwrite = 0 ) : \n    if overwrite is 1 or overwrite == 'v_annotations' : \n        annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) \n        if annotated : \n            current_attrs = node . _v_attrs \n            for attr_name in current_attrs . _v_attrnames : \n                if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : \n                    delattr ( current_attrs , attr_name ) \n            delattr ( current_attrs , HDF5StorageService . ANNOTATED ) \n            self . _hdf5file . flush ( ) \n    if not item_with_annotations . v_annotations . f_is_empty ( ) : \n        anno_dict = item_with_annotations . v_annotations . _dict \n        current_attrs = node . _v_attrs \n        changed = 0 \n        for field_name in anno_dict : \n            val = anno_dict [ field_name ] \n            field_name_with_prefix = HDF5StorageService . ANNOTATION_PREFIX + field_name \n            if field_name_with_prefix not in current_attrs : \n                setattr ( current_attrs , field_name_with_prefix , val ) \n                changed = 1 \n        if changed : \n            setattr ( current_attrs , HDF5StorageService . ANNOTATED , 1 ) \n            self . _hdf5file . flush ( ) "}
{"8429": "\ndef _grp_store_group ( self , traj_group , store_data = pypetconstants . STORE_DATA , with_links = 1 , recursive = 0 , max_depth = None , _hdf5_group = None , _newly_created = 0 ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    elif store_data == pypetconstants . STORE_DATA_SKIPPING and traj_group . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % traj_group . v_full_name ) \n    elif not recursive : \n        if _hdf5_group is None : \n            _hdf5_group , _newly_created = self . _all_create_or_get_groups ( traj_group . v_full_name ) \n        overwrite = store_data == pypetconstants . OVERWRITE_DATA \n        if ( traj_group . v_comment != '' and ( HDF5StorageService . COMMENT not in _hdf5_group . _v_attrs or overwrite ) ) : \n            setattr ( _hdf5_group . _v_attrs , HDF5StorageService . COMMENT , traj_group . v_comment ) \n        if ( ( _newly_created or overwrite ) and type ( traj_group ) not in ( nn . NNGroupNode , nn . ConfigGroup , nn . ParameterGroup , nn . DerivedParameterGroup , nn . ResultGroup ) ) : \n            setattr ( _hdf5_group . _v_attrs , HDF5StorageService . CLASS_NAME , traj_group . f_get_class_name ( ) ) \n        self . _ann_store_annotations ( traj_group , _hdf5_group , overwrite = overwrite ) \n        self . _hdf5file . flush ( ) \n        traj_group . _stored = 1 \n        self . _node_processing_timer . signal_update ( ) \n    if recursive : \n        parent_traj_group = traj_group . f_get_parent ( ) \n        parent_hdf5_group = self . _all_create_or_get_groups ( parent_traj_group . v_full_name ) [ 0 ] \n        self . _tree_store_nodes_dfs ( parent_traj_group , traj_group . v_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , parent_hdf5_group = parent_hdf5_group ) "}
{"8430": "\ndef _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = 1 , recursive = 0 , max_depth = None , _traj = None , _as_new = 0 , _hdf5_group = None ) : \n    if _hdf5_group is None : \n        _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) \n        _traj = traj_group . v_root \n    if recursive : \n        parent_traj_node = traj_group . f_get_parent ( ) \n        self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , trajectory = _traj , as_new = _as_new , hdf5_group = _hdf5_group ) \n    else : \n        if load_data == pypetconstants . LOAD_NOTHING : \n            return \n        elif load_data == pypetconstants . OVERWRITE_DATA : \n            traj_group . v_annotations . f_empty ( ) \n            traj_group . v_comment = '' \n        self . _all_load_skeleton ( traj_group , _hdf5_group ) \n        traj_group . _stored = not _as_new \n        self . _node_processing_timer . signal_update ( ) "}
{"8433": "\ndef _prm_meta_add_summary ( self , instance ) : \n    if instance . v_comment == '' : \n        return 0 \n    where = instance . v_branch \n    definitely_store_comment = 1 \n    bytes_comment = instance . v_comment . encode ( 'utf-8' ) \n    hexdigest = hashlib . sha1 ( bytes_comment ) . hexdigest ( ) \n    hexdigest = hexdigest . encode ( 'utf-8' ) \n    table_name = where + '_summary' \n    if table_name in self . _overview_group : \n        table = getattr ( self . _overview_group , table_name ) \n    else : \n        return definitely_store_comment \n    try : \n        condvars = { 'hexdigestcol' : table . cols . hexdigest , 'hexdigest' : hexdigest } \n        condition = \"\"\"(hexdigestcol == hexdigest)\"\"\" \n        row_iterator = table . where ( condition , condvars = condvars ) \n        row = None \n        try : \n            row = next ( row_iterator ) \n        except StopIteration : \n            pass \n        if row is None : \n            self . _all_store_param_or_result_table_entry ( instance , table , flags = ( HDF5StorageService . ADD_ROW , ) , additional_info = { 'hexdigest' : hexdigest } ) \n            definitely_store_comment = 1 \n        else : \n            definitely_store_comment = 0 \n            self . _all_kill_iterator ( row_iterator ) \n    except pt . NoSuchNodeError : \n        definitely_store_comment = 1 \n    return definitely_store_comment "}
{"8434": "\ndef _prm_add_meta_info ( self , instance , group , overwrite = 0 ) : \n    if overwrite : \n        flags = ( ) \n    else : \n        flags = ( HDF5StorageService . ADD_ROW , ) \n    definitely_store_comment = 1 \n    try : \n        definitely_store_comment = self . _prm_meta_add_summary ( instance ) \n        try : \n            table_name = instance . v_branch + '_overview' \n            table = getattr ( self . _overview_group , table_name ) \n            if len ( table ) < pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH : \n                self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) \n        except pt . NoSuchNodeError : \n            pass \n    except Exception as exc : \n        self . _logger . error ( 'Could not store information table due to `%s`.' % repr ( exc ) ) \n    if ( ( not self . _purge_duplicate_comments or definitely_store_comment ) and instance . v_comment != '' ) : \n        setattr ( group . _v_attrs , HDF5StorageService . COMMENT , instance . v_comment ) \n    setattr ( group . _v_attrs , HDF5StorageService . CLASS_NAME , instance . f_get_class_name ( ) ) \n    setattr ( group . _v_attrs , HDF5StorageService . LEAF , 1 ) \n    if instance . v_is_parameter and instance . v_explored : \n        try : \n            tablename = 'explored_parameters_overview' \n            table = getattr ( self . _overview_group , tablename ) \n            if len ( table ) < pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH : \n                self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) \n        except pt . NoSuchNodeError : \n            pass \n        except Exception as exc : \n            self . _logger . error ( 'Could not store information ' 'table due to `%s`.' % repr ( exc ) ) "}
{"8436": "\ndef _prm_store_parameter_or_result ( self , instance , store_data = pypetconstants . STORE_DATA , store_flags = None , overwrite = None , with_links = 0 , recursive = 0 , _hdf5_group = None , _newly_created = 0 , ** kwargs ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    elif store_data == pypetconstants . STORE_DATA_SKIPPING and instance . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % instance . v_full_name ) \n        return \n    elif store_data == pypetconstants . OVERWRITE_DATA : \n        if not overwrite : \n            overwrite = 1 \n    fullname = instance . v_full_name \n    self . _logger . debug ( 'Storing `%s`.' % fullname ) \n    if _hdf5_group is None : \n        _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) \n    store_dict = { } \n    if store_flags is None : \n        store_flags = { } \n    try : \n        if not instance . f_is_empty ( ) : \n            store_dict = instance . _store ( ) \n        try : \n            instance_flags = instance . _store_flags ( ) . copy ( ) \n        except AttributeError : \n            instance_flags = { } \n        instance_flags . update ( store_flags ) \n        store_flags = instance_flags \n        self . _prm_extract_missing_flags ( store_dict , store_flags ) \n        if overwrite : \n            if isinstance ( overwrite , str ) : \n                overwrite = [ overwrite ] \n            if overwrite is 1 : \n                to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] \n                self . _all_delete_parameter_or_result_or_group ( instance , delete_only = to_delete , _hdf5_group = _hdf5_group ) \n            elif isinstance ( overwrite , ( list , tuple ) ) : \n                overwrite_set = set ( overwrite ) \n                key_set = set ( store_dict . keys ( ) ) \n                stuff_not_to_be_overwritten = overwrite_set - key_set \n                if overwrite != 'v_annotations' and len ( stuff_not_to_be_overwritten ) > 0 : \n                    self . _logger . warning ( 'Cannot overwrite `%s`, these items are not supposed to ' 'be stored by the leaf node.' % str ( stuff_not_to_be_overwritten ) ) \n                stuff_to_overwrite = overwrite_set & key_set \n                if len ( stuff_to_overwrite ) > 0 : \n                    self . _all_delete_parameter_or_result_or_group ( instance , delete_only = list ( stuff_to_overwrite ) ) \n            else : \n                raise ValueError ( 'Your value of overwrite `%s` is not understood. ' 'Please pass `True` of a list of strings to fine grain ' 'overwriting.' % str ( overwrite ) ) \n        self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) \n        self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) \n        if _newly_created or overwrite is 1 : \n            self . _prm_add_meta_info ( instance , _hdf5_group , overwrite = not _newly_created ) \n        instance . _stored = 1 \n        self . _node_processing_timer . signal_update ( ) \n    except : \n        self . _logger . error ( 'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname ) \n        for key in store_dict . keys ( ) : \n            if key in _hdf5_group : \n                hdf5_child = _hdf5_group . _f_get_child ( key ) \n                hdf5_child . _f_remove ( recursive = 1 ) \n        if _hdf5_group . _v_nchildren == 0 : \n            _hdf5_group . _f_remove ( recursive = 1 ) \n        raise "}
{"8440": "\ndef _prm_write_pandas_data ( self , key , data , group , fullname , flag , ** kwargs ) : \n    try : \n        if 'filters' not in kwargs : \n            filters = self . _all_get_filters ( kwargs ) \n            kwargs [ 'filters' ] = filters \n        if 'format' not in kwargs : \n            kwargs [ 'format' ] = self . pandas_format \n        if 'encoding' not in kwargs : \n            kwargs [ 'encoding' ] = self . encoding \n        overwrite = kwargs . pop ( 'overwrite' , 0 ) \n        if key in group and not ( overwrite or kwargs . get ( 'append' , 0 ) ) : \n            raise ValueError ( 'DataFrame `%s` already exists in `%s`. ' 'To append pass ``append=`True```.' % ( key , fullname ) ) \n        else : \n            self . _logger . debug ( 'Appending to pandas data `%s` in `%s`' % ( key , fullname ) ) \n        if data is not None and ( kwargs [ 'format' ] == 'f' or kwargs [ 'format' ] == 'fixed' ) : \n            kwargs [ 'expectedrows' ] = data . shape [ 0 ] \n        name = group . _v_pathname + '/' + key \n        self . _hdf5store . put ( name , data , ** kwargs ) \n        self . _hdf5store . flush ( ) \n        self . _hdf5file . flush ( ) \n        frame_group = group . _f_get_child ( key ) \n        setattr ( frame_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing pandas data `%s` of `%s`.' % ( key , fullname ) ) \n        raise "}
{"8444": "\ndef _all_delete_parameter_or_result_or_group ( self , instance , delete_only = None , remove_from_item = 0 , recursive = 0 , _hdf5_group = None ) : \n    split_name = instance . v_location . split ( '.' ) \n    if _hdf5_group is None : \n        where = '/' + self . _trajectory_name + '/' + '/' . join ( split_name ) \n        node_name = instance . v_name \n        _hdf5_group = self . _hdf5file . get_node ( where = where , name = node_name ) \n    if delete_only is None : \n        if instance . v_is_group and not recursive and len ( _hdf5_group . _v_children ) != 0 : \n            raise TypeError ( 'You cannot remove the group `%s`, it has children, please ' 'use `recursive=True` to enforce removal.' % instance . v_full_name ) \n        _hdf5_group . _f_remove ( recursive = 1 ) \n    else : \n        if not instance . v_is_leaf : \n            raise ValueError ( 'You can only choose `delete_only` mode for leafs.' ) \n        if isinstance ( delete_only , str ) : \n            delete_only = [ delete_only ] \n        for delete_item in delete_only : \n            if ( remove_from_item and hasattr ( instance , '__contains__' ) and hasattr ( instance , '__delattr__' ) and delete_item in instance ) : \n                delattr ( instance , delete_item ) \n            try : \n                _hdf5_sub_group = self . _hdf5file . get_node ( where = _hdf5_group , name = delete_item ) \n                _hdf5_sub_group . _f_remove ( recursive = 1 ) \n            except pt . NoSuchNodeError : \n                self . _logger . warning ( 'Could not delete `%s` from `%s`. HDF5 node not found!' % ( delete_item , instance . v_full_name ) ) "}
{"8454": "\ndef load_trajectory ( name = None , index = None , as_new = 0 , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = 1 , load_data = None , max_depth = None , force = 0 , dynamic_imports = None , new_name = 'my_trajectory' , add_time = 1 , wildcard_functions = None , with_run_information = 1 , storage_service = storage . HDF5StorageService , ** kwargs ) : \n    if name is None and index is None : \n        raise ValueError ( 'Please specify either a name or an index' ) \n    elif name is not None and index is not None : \n        raise ValueError ( 'Please specify either a name or an index' ) \n    traj = Trajectory ( name = new_name , add_time = add_time , dynamic_imports = dynamic_imports , wildcard_functions = wildcard_functions ) \n    traj . f_load ( name = name , index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , load_data = load_data , max_depth = max_depth , force = force , with_run_information = with_run_information , storage_service = storage_service , ** kwargs ) \n    return traj "}
{"8460": "\ndef f_shrink ( self , force = 0 ) : \n    if self . _stored and not force : \n        raise TypeError ( 'Your trajectory is already stored to disk or database, shrinking is ' 'not allowed.' ) \n    for param in self . _explored_parameters . values ( ) : \n        param . f_unlock ( ) \n        try : \n            param . _shrink ( ) \n        except Exception as exc : \n            self . _logger . error ( 'Could not shrink `%s` because of:`%s`' % ( param . v_full_name , repr ( exc ) ) ) \n    self . _explored_parameters = { } \n    self . _run_information = { } \n    self . _single_run_ids = { } \n    self . _add_run_info ( 0 ) \n    self . _test_run_addition ( 1 ) "}
{"8461": "\ndef _preset ( self , name , args , kwargs ) : \n    if self . f_contains ( name , shortcuts = 0 ) : \n        raise ValueError ( 'Parameter `%s` is already part of your trajectory, use the normal' 'accessing routine to change config.' % name ) \n    else : \n        self . _changed_default_parameters [ name ] = ( args , kwargs ) "}
{"8464": "\ndef f_get_from_runs ( self , name , include_default_run = 1 , use_indices = 0 , fast_access = 0 , with_links = 1 , shortcuts = 1 , max_depth = None , auto_load = 0 ) : \n    result_dict = OrderedDict ( ) \n    old_crun = self . v_crun \n    try : \n        if len ( self . _run_parent_groups ) > 0 : \n            for run_name in self . f_iter_runs ( ) : \n                value = None \n                already_found = 0 \n                for run_parent_group in self . _run_parent_groups . values ( ) : \n                    if run_name not in run_parent_group . _children : \n                        continue \n                    try : \n                        value = run_parent_group . f_get ( run_name + '.' + name , fast_access = 0 , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                        if already_found : \n                            raise pex . NotUniqueNodeError ( '`%s` has been found several times ' 'in one run.' % name ) \n                        else : \n                            already_found = 1 \n                    except ( AttributeError , pex . DataNotInStorageError ) : \n                        pass \n                if value is None and include_default_run : \n                    for run_parent_group in self . _run_parent_groups . values ( ) : \n                        try : \n                            value = run_parent_group . f_get ( self . f_wildcard ( '$' , - 1 ) + '.' + name , fast_access = 0 , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                            if already_found : \n                                raise pex . NotUniqueNodeError ( '`%s` has been found several ' 'times in one run.' % name ) \n                            else : \n                                already_found = 1 \n                        except ( AttributeError , pex . DataNotInStorageError ) : \n                            pass \n                if value is not None : \n                    if value . v_is_leaf : \n                        value = self . _nn_interface . _apply_fast_access ( value , fast_access ) \n                    if use_indices : \n                        key = self . f_idx_to_run ( run_name ) \n                    else : \n                        key = run_name \n                    result_dict [ key ] = value \n        return result_dict \n    finally : \n        self . v_crun = old_crun "}
{"8465": "\ndef _is_completed ( self , name_or_id = None ) : \n    if name_or_id is None : \n        return all ( ( runinfo [ 'completed' ] for runinfo in self . _run_information . values ( ) ) ) \n    else : \n        return self . f_get_run_information ( name_or_id , copy = 0 ) [ 'completed' ] "}
{"8467": "\ndef _copy_from ( self , node , copy_leaves = 1 , overwrite = 0 , with_links = 1 ) : \n    def _copy_skeleton ( node_in , node_out ) : \n        new_annotations = node_out . v_annotations \n        node_in . _annotations = new_annotations \n        node_in . v_comment = node_out . v_comment \n    def _add_leaf ( leaf ) : \n        leaf_full_name = leaf . v_full_name \n        try : \n            found_leaf = self . f_get ( leaf_full_name , with_links = 0 , shortcuts = 0 , auto_load = 0 ) \n            if overwrite : \n                found_leaf . __setstate__ ( leaf . __getstate__ ( ) ) \n            return found_leaf \n        except AttributeError : \n            pass \n        if copy_leaves is 1 or ( copy_leaves == 'explored' and leaf . v_is_parameter and leaf . v_explored ) : \n            new_leaf = self . f_add_leaf ( cp . copy ( leaf ) ) \n        else : \n            new_leaf = self . f_add_leaf ( leaf ) \n        if new_leaf . v_is_parameter and new_leaf . v_explored : \n            self . _explored_parameters [ new_leaf . v_full_name ] = new_leaf \n        return new_leaf \n    def _add_group ( group ) : \n        group_full_name = group . v_full_name \n        try : \n            found_group = self . f_get ( group_full_name , with_links = 0 , shortcuts = 0 , auto_load = 0 ) \n            if overwrite : \n                _copy_skeleton ( found_group , group ) \n            return found_group \n        except AttributeError : \n            pass \n        new_group = self . f_add_group ( group_full_name ) \n        _copy_skeleton ( new_group , group ) \n        return new_group \n    is_run = self . _is_run \n    self . _is_run = 0 \n    try : \n        if node . v_is_leaf : \n            return _add_leaf ( node ) \n        elif node . v_is_group : \n            other_root = node . v_root \n            if other_root is self : \n                raise RuntimeError ( 'You cannot copy a given tree to itself!' ) \n            result = _add_group ( node ) \n            nodes_iterator = node . f_iter_nodes ( recursive = 1 , with_links = with_links ) \n            has_links = [ ] \n            if node . _links : \n                has_links . append ( node ) \n            for child in nodes_iterator : \n                if child . v_is_leaf : \n                    _add_leaf ( child ) \n                else : \n                    _add_group ( child ) \n                    if child . _links : \n                        has_links . append ( child ) \n            if with_links : \n                for current in has_links : \n                    mine = self . f_get ( current . v_full_name , with_links = 0 , shortcuts = 0 , auto_load = 0 ) \n                    my_link_set = set ( mine . _links . keys ( ) ) \n                    other_link_set = set ( current . _links . keys ( ) ) \n                    new_links = other_link_set - my_link_set \n                    for link in new_links : \n                        where_full_name = current . _links [ link ] . v_full_name \n                        mine . f_add_link ( link , where_full_name ) \n            return result \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    except Exception : \n        self . _is_run = is_run "}
{"8468": "\ndef f_explore ( self , build_dict ) : \n    for run_idx in range ( len ( self ) ) : \n        if self . f_is_completed ( run_idx ) : \n            raise TypeError ( 'You cannot explore a trajectory which has been explored before, ' 'please use `f_expand` instead.' ) \n    added_explored_parameters = [ ] \n    try : \n        length = len ( self ) \n        for key , builditerable in build_dict . items ( ) : \n            act_param = self . f_get ( key ) \n            if not act_param . v_is_leaf or not act_param . v_is_parameter : \n                raise ValueError ( '%s is not an appropriate search string for a parameter.' % key ) \n            act_param . f_unlock ( ) \n            act_param . _explore ( builditerable ) \n            added_explored_parameters . append ( act_param ) \n            full_name = act_param . v_full_name \n            self . _explored_parameters [ full_name ] = act_param \n            act_param . _explored = 1 \n            if len ( self . _explored_parameters ) == 1 : \n                length = act_param . f_get_range_length ( ) \n            elif not length == act_param . f_get_range_length ( ) : \n                raise ValueError ( 'The parameters to explore have not the same size!' ) \n        for irun in range ( length ) : \n            self . _add_run_info ( irun ) \n        self . _test_run_addition ( length ) \n    except Exception : \n        for param in added_explored_parameters : \n            param . f_unlock ( ) \n            param . _shrink ( ) \n            param . _explored = 0 \n            full_name = param . v_full_name \n            del self . _explored_parameters [ full_name ] \n        if len ( self . _explored_parameters ) == 0 : \n            self . f_shrink ( force = 1 ) \n        raise "}
{"8473": "\ndef _finalize ( self , store_meta_data = 1 ) : \n    self . _is_run = 0 \n    self . f_set_crun ( None ) \n    if store_meta_data : \n        self . f_store ( only_init = 1 ) "}
{"8474": "\ndef f_load_skeleton ( self ) : \n    self . f_load ( self . v_name , as_new = 0 , load_parameters = pypetconstants . LOAD_SKELETON , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , with_run_information = 0 ) "}
{"8475": "\ndef f_load ( self , name = None , index = None , as_new = 0 , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = 1 , load_data = None , max_depth = None , force = 0 , dynamic_imports = None , with_run_information = 1 , with_meta_data = 1 , storage_service = None , ** kwargs ) : \n    if name is None and index is None : \n        name = self . v_name \n    if as_new : \n        load_parameters = pypetconstants . LOAD_DATA \n        load_derived_parameters = pypetconstants . LOAD_NOTHING \n        load_results = pypetconstants . LOAD_NOTHING \n        load_other_data = pypetconstants . LOAD_NOTHING \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if self . v_storage_service is None or storage_service is not None or len ( kwargs ) > 0 : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = storage_service , trajectory = self , ** kwargs ) \n    if len ( unused_kwargs ) > 0 : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    if dynamic_imports is not None : \n        self . f_add_to_dynamic_imports ( dynamic_imports ) \n    if load_data is not None : \n        load_parameters = load_data \n        load_derived_parameters = load_data \n        load_results = load_data \n        load_other_data = load_data \n    self . _storage_service . load ( pypetconstants . TRAJECTORY , self , trajectory_name = name , trajectory_index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , max_depth = max_depth , with_run_information = with_run_information , with_meta_data = with_meta_data , force = force ) \n    if as_new : \n        for param in self . _parameters . values ( ) : \n            param . f_unlock ( ) "}
{"8478": "\ndef f_merge_many ( self , other_trajectories , ignore_data = ( ) , move_data = 0 , delete_other_trajectory = 0 , keep_info = 1 , keep_other_trajectory_info = 1 , merge_config = 1 , backup = 1 ) : \n    other_length = len ( other_trajectories ) \n    self . _logger . info ( 'Merging %d trajectories into the current one.' % other_length ) \n    self . f_load_skeleton ( ) \n    if backup : \n        self . f_backup ( ) \n    for idx , other in enumerate ( other_trajectories ) : \n        self . f_merge ( other , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = delete_other_trajectory , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = 0 , consecutive_merge = 1 ) \n        self . _logger . log ( 21 , 'Merged %d out of %d' % ( idx + 1 , other_length ) ) \n    self . _logger . info ( 'Storing data to disk' ) \n    self . _reversed_wildcards = { } \n    self . f_store ( ) \n    self . _logger . info ( 'Finished final storage' ) "}
{"8481": "\ndef _merge_derived_parameters ( self , other_trajectory , used_runs , rename_dict , allowed_translations , ignore_data ) : \n    other_derived_parameters = other_trajectory . _derived_parameters . copy ( ) \n    new_first_run_idx = min ( used_runs . values ( ) ) \n    run_name_dummy = other_trajectory . f_wildcard ( '$' , - 1 ) \n    for param_name in other_derived_parameters : \n        if param_name in ignore_data : \n            continue \n        split_name = param_name . split ( '.' ) \n        if not any ( x in run_name_dummy for x in split_name ) : \n            continue \n        ignore_data . add ( param_name ) \n        param = other_derived_parameters [ param_name ] \n        new_param_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) \n        if new_param_name in self : \n            my_param = self . f_get ( new_param_name , fast_access = 0 ) \n            if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : \n                continue \n        first_new_param_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = new_first_run_idx ) \n        rename_dict [ param_name ] = first_new_param_name \n        comment = param . v_comment \n        param_type = param . f_get_class_name ( ) \n        param_type = self . _create_class ( param_type ) \n        first_param = self . f_add_leaf ( param_type , first_new_param_name , comment = comment ) \n        for run_idx in used_runs . values ( ) : \n            if run_idx == new_first_run_idx : \n                continue \n            next_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = run_idx ) \n            split_name = next_name . split ( '.' ) \n            link_name = split_name . pop ( ) \n            location_name = '.' . join ( split_name ) \n            if not self . f_contains ( location_name , shortcuts = 0 ) : \n                the_group = self . f_add_group ( location_name ) \n            else : \n                the_group = self . f_get ( location_name ) \n            the_group . f_add_link ( link_name , first_param ) \n    for param_name in other_derived_parameters : \n        if param_name in ignore_data : \n            continue \n        split_name = param_name . split ( '.' ) \n        ignore_data . add ( param_name ) \n        if any ( x in other_trajectory . _reversed_wildcards and x not in allowed_translations for x in split_name ) : \n            continue \n        new_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) \n        if self . f_contains ( new_name ) : \n            my_param = self . f_get ( new_name , fast_access = 0 ) \n            param = other_derived_parameters [ param_name ] \n            if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : \n                continue \n            else : \n                self . _logger . error ( 'Could not merge parameter `%s`. ' 'I will ignore it!' % new_name ) \n        rename_dict [ param_name ] = new_name "}
{"8482": "\ndef _merge_links ( self , other_trajectory , used_runs , allowed_translations , ignore_data ) : \n    linked_items = other_trajectory . _linked_by \n    run_name_dummys = set ( [ f ( - 1 ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) \n    if len ( linked_items ) > 0 : \n        self . _logger . info ( 'Merging potential links!' ) \n        for old_linked_name in other_trajectory . _linked_by : \n            if old_linked_name in ignore_data : \n                continue \n            split_name = old_linked_name . split ( '.' ) \n            if any ( x in run_name_dummys for x in split_name ) : \n                self . _logger . warning ( 'Ignoring all links linking to `%s` because ' 'I don`t know how to resolve links under `%s` nodes.' % ( old_linked_name , str ( run_name_dummys ) ) ) \n                continue \n            old_link_dict = other_trajectory . _linked_by [ old_linked_name ] \n            split_name = old_linked_name . split ( '.' ) \n            if all ( x in allowed_translations for x in split_name ) : \n                new_linked_full_name = self . _rename_full_name ( old_linked_name , other_trajectory , used_runs = used_runs ) \n            else : \n                new_linked_full_name = old_linked_name \n            for linking_node , link_set in old_link_dict . values ( ) : \n                linking_full_name = linking_node . v_full_name \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in run_name_dummys for x in split_name ) : \n                    self . _logger . warning ( 'Ignoring links under `%s` because ' 'I don`t know how to resolve links ' 'under a `%s` node.' % ( linking_full_name , str ( run_name_dummys ) ) ) \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in allowed_translations for x in split_name ) : \n                    new_linking_full_name = self . _rename_full_name ( linking_full_name , other_trajectory , used_runs = used_runs ) \n                else : \n                    new_linking_full_name = linking_full_name \n                for link in link_set : \n                    if ( linking_full_name + '.' + link ) in ignore_data : \n                        continue \n                    if link in run_name_dummys : \n                        self . _logger . warning ( 'Ignoring link `%s` under `%s` because ' 'I don`t know how to resolve ' 'links named as `%s`.' % ( link , linking_full_name , str ( run_name_dummys ) ) ) \n                        continue \n                    try : \n                        new_linked_item = self . f_get ( new_linked_full_name , shortcuts = 0 ) \n                        if self . f_contains ( new_linking_full_name ) : \n                            new_linking_item = self . f_get ( new_linking_full_name , shortcuts = 0 ) \n                        else : \n                            new_linking_item = self . f_add_group ( new_linking_full_name ) \n                        if link in allowed_translations : \n                            run_indices , wildcards = other_trajectory . _reversed_wildcards [ link ] \n                            link = self . f_wildcard ( wildcards [ 0 ] , used_runs [ run_indices [ 0 ] ] ) \n                        if not link in new_linking_item . _links : \n                            new_linking_item . f_add_link ( link , new_linked_item ) \n                        else : \n                            self . _logger . debug ( 'Link `%s` exists already under `%s`.' % ( link , new_linked_item . v_full_name ) ) \n                    except ( AttributeError , ValueError ) as exc : \n                        self . _logger . error ( 'Could not copy link `%s` under `%s` linking ' 'to `%s` due to `%s`' % ( link , linking_full_name , old_linked_name , repr ( exc ) ) ) "}
{"8483": "\ndef _merge_config ( self , other_trajectory ) : \n    self . _logger . info ( 'Merging config!' ) \n    if 'config.git' in other_trajectory : \n        self . _logger . info ( 'Merging git commits!' ) \n        git_node = other_trajectory . f_get ( 'config.git' ) \n        param_list = [ ] \n        for param in git_node . f_iter_leaves ( with_links = 0 ) : \n            if not self . f_contains ( param . v_full_name , shortcuts = 0 ) : \n                param_list . append ( self . f_add_config ( param ) ) \n        if param_list : \n            self . f_store_items ( param_list ) \n        self . _logger . info ( 'Merging git commits successful!' ) \n    if 'config.environment' in other_trajectory : \n        self . _logger . info ( 'Merging environment config!' ) \n        env_node = other_trajectory . f_get ( 'config.environment' ) \n        param_list = [ ] \n        for param in env_node . f_iter_leaves ( with_links = 0 ) : \n            if not self . f_contains ( param . v_full_name , shortcuts = 0 ) : \n                param_list . append ( self . f_add_config ( param ) ) \n        if param_list : \n            self . f_store_items ( param_list ) \n        self . _logger . info ( 'Merging config successful!' ) \n    if 'config.merge' in other_trajectory : \n        self . _logger . info ( 'Merging merge config!' ) \n        merge_node = other_trajectory . f_get ( 'config.merge' ) \n        param_list = [ ] \n        for param in merge_node . f_iter_leaves ( with_links = 0 ) : \n            if not self . f_contains ( param . v_full_name , shortcuts = 0 ) : \n                param_list . append ( self . f_add_config ( param ) ) \n        if param_list : \n            self . f_store_items ( param_list ) \n        self . _logger . info ( 'Merging config successful!' ) "}
{"8484": "\ndef _merge_slowly ( self , other_trajectory , rename_dict ) : \n    for other_key in rename_dict : \n        new_key = rename_dict [ other_key ] \n        other_instance = other_trajectory . f_get ( other_key ) \n        if other_instance . f_is_empty ( ) : \n            with self . _nn_interface . _disable_logging : \n                other_trajectory . f_load_item ( other_instance ) \n        if not self . f_contains ( new_key ) : \n            class_name = other_instance . f_get_class_name ( ) \n            class_ = self . _create_class ( class_name ) \n            my_instance = self . f_add_leaf ( class_ , new_key ) \n        else : \n            my_instance = self . f_get ( new_key , shortcuts = 0 ) \n        if not my_instance . f_is_empty ( ) : \n            raise RuntimeError ( 'Something is wrong! Your item `%s` should be empty.' % new_key ) \n        load_dict = other_instance . _store ( ) \n        my_instance . _load ( load_dict ) \n        my_instance . f_set_annotations ( ** other_instance . v_annotations . f_to_dict ( copy = 0 ) ) \n        my_instance . v_comment = other_instance . v_comment \n        self . f_store_item ( my_instance ) \n        if other_instance . v_is_parameter : \n            other_instance . f_unlock ( ) \n            my_instance . f_unlock ( ) \n        other_instance . f_empty ( ) \n        my_instance . f_empty ( ) "}
{"8486": "\ndef f_migrate ( self , new_name = None , in_store = 0 , new_storage_service = None , ** kwargs ) : \n    if new_name is not None : \n        self . _name = new_name \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if new_storage_service is not None or len ( kwargs ) > 0 : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = new_storage_service , trajectory = self , ** kwargs ) \n    if len ( unused_kwargs ) > 0 : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    self . _stored = in_store "}
{"8487": "\ndef f_store ( self , only_init = 0 , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if self . _is_run : \n        if self . _new_nodes or self . _new_links : \n            self . _storage_service . store ( pypetconstants . SINGLE_RUN , self , trajectory_name = self . v_name , recursive = not only_init , store_data = store_data , max_depth = max_depth ) \n    else : \n        self . _storage_service . store ( pypetconstants . TRAJECTORY , self , trajectory_name = self . v_name , only_init = only_init , store_data = store_data , max_depth = max_depth ) \n        self . _stored = 1 "}
{"8490": "\ndef _make_single_run ( self ) : \n    self . _is_run = 0 \n    self . _new_nodes = OrderedDict ( ) \n    self . _new_links = OrderedDict ( ) \n    self . _is_run = 1 \n    return self "}
{"8491": "\ndef f_get_run_names ( self , sort = 1 ) : \n    if sort : \n        return [ self . f_idx_to_run ( idx ) for idx in range ( len ( self ) ) ] \n    else : \n        return list ( self . _run_information . keys ( ) ) "}
{"8492": "\ndef f_get_run_information ( self , name_or_idx = None , copy = 1 ) : \n    if name_or_idx is None : \n        if copy : \n            return cp . deepcopy ( self . _run_information ) \n        else : \n            return self . _run_information \n    try : \n        if copy : \n            return self . _run_information [ name_or_idx ] . copy ( ) \n        else : \n            return self . _run_information [ name_or_idx ] \n    except KeyError : \n        name_or_idx = self . f_idx_to_run ( name_or_idx ) \n        if copy : \n            return self . _run_information [ name_or_idx ] . copy ( ) \n        else : \n            return self . _run_information [ name_or_idx ] "}
{"8493": "\ndef f_find_idx ( self , name_list , predicate ) : \n    if self . _is_run and not self . v_full_copy : \n        raise TypeError ( 'You cannot use this function during a multiprocessing signle run and ' 'not having ``v_full_copy=True``.' ) \n    if isinstance ( name_list , str ) : \n        name_list = [ name_list ] \n    iter_list = [ ] \n    for name in name_list : \n        param = self . f_get ( name ) \n        if not param . v_is_parameter : \n            raise TypeError ( '`%s` is not a parameter it is a %s, find idx is not applicable' % ( name , str ( type ( param ) ) ) ) \n        if param . f_has_range ( ) : \n            iter_list . append ( iter ( param . f_get_range ( copy = 0 ) ) ) \n        else : \n            iter_list . append ( itools . repeat ( param . f_get ( ) , len ( self ) ) ) \n    logic_iter = map ( predicate , * iter_list ) \n    for idx , item in enumerate ( logic_iter ) : \n        if item : \n            yield idx "}
{"8494": "\ndef f_start_run ( self , run_name_or_idx = None , turn_into_run = 1 ) : \n    if self . _run_started : \n        return self \n    if run_name_or_idx is None : \n        if self . v_idx == - 1 : \n            raise ValueError ( 'Cannot start run if trajectory is not set to a particular run' ) \n    else : \n        self . f_set_crun ( run_name_or_idx ) \n    self . _run_started = 1 \n    if turn_into_run : \n        self . _make_single_run ( ) \n    self . _set_start ( ) \n    return self "}
{"8495": "\ndef f_finalize_run ( self , store_meta_data = 1 , clean_up = 1 ) : \n    if not self . _run_started : \n        return self \n    self . _set_finish ( ) \n    if clean_up and self . _is_run : \n        self . _finalize_run ( ) \n    self . _is_run = 0 \n    self . _run_started = 0 \n    self . _updated_run_information . add ( self . v_idx ) \n    if store_meta_data : \n        self . f_store ( only_init = 1 ) \n    return self "}
{"8498": "\ndef _construct_instance ( self , constructor , full_name , * args , ** kwargs ) : \n    if getattr ( constructor , 'KNOWS_TRAJECTORY' , 0 ) : \n        return constructor ( full_name , self , * args , ** kwargs ) \n    else : \n        return constructor ( full_name , * args , ** kwargs ) "}
{"8500": "\ndef _finalize_run ( self ) : \n    self . _run_information [ self . v_crun ] [ 'completed' ] = 1 \n    while len ( self . _new_links ) : \n        name_pair , child_parent_pair = self . _new_links . popitem ( last = 0 ) \n        parent_node , _ = child_parent_pair \n        _ , link = name_pair \n        parent_node . f_remove_child ( link ) \n    while len ( self . _new_nodes ) : \n        _ , child_parent_pair = self . _new_nodes . popitem ( last = 0 ) \n        parent , child = child_parent_pair \n        child_name = child . v_name \n        parent . f_remove_child ( child_name , recursive = 1 ) "}
{"8501": "\ndef f_get_config ( self , fast_access = 0 , copy = 1 ) : \n    return self . _return_item_dictionary ( self . _config , fast_access , copy ) "}
{"8502": "\ndef f_get_results ( self , fast_access = 0 , copy = 1 ) : \n    return self . _return_item_dictionary ( self . _results , fast_access , copy ) "}
{"8505": "\ndef f_remove_items ( self , iterator , recursive = 0 ) : \n    fetched_items = self . _nn_interface . _fetch_items ( REMOVE , iterator , ( ) , { } ) \n    if fetched_items : \n        for _ , item , dummy1 , dummy2 in fetched_items : \n            self . _nn_interface . _remove_node_or_leaf ( item , recursive = recursive ) \n    else : \n        self . _logger . warning ( 'Your removal was not successful, could not find a single ' 'item to remove.' ) "}
{"8506": "\ndef f_delete_links ( self , iterator_of_links , remove_from_trajectory = 0 ) : \n    to_delete_links = [ ] \n    group_link_pairs = [ ] \n    for elem in iterator_of_links : \n        if isinstance ( elem , str ) : \n            split_names = elem . split ( '.' ) \n            parent_name = '.' . join ( split_names [ : - 1 ] ) \n            link = split_names [ - 1 ] \n            parent_node = self . f_get ( parent_name ) if parent_name != '' else self \n            link_name = parent_node . v_full_name + '.' + link if parent_name != '' else link \n            to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) \n            group_link_pairs . append ( ( parent_node , link ) ) \n        else : \n            link_name = elem [ 0 ] . v_full_name + '.' + elem [ 1 ] \n            to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) \n            group_link_pairs . append ( elem ) \n    try : \n        self . _storage_service . store ( pypetconstants . LIST , to_delete_links , trajectory_name = self . v_name ) \n    except : \n        self . _logger . error ( 'Could not remove `%s` from the trajectory. Maybe the' ' item(s) was/were never stored to disk.' % str ( to_delete_links ) ) \n        raise \n    if remove_from_trajectory : \n        for group , link in group_link_pairs : \n            group . f_remove_link ( link ) "}
{"8507": "\ndef f_remove ( self , recursive = 1 , predicate = None ) : \n    if not recursive : \n        raise ValueError ( 'Nice try ;-)' ) \n    for child in list ( self . _children . keys ( ) ) : \n        self . f_remove_child ( child , recursive = 1 , predicate = predicate ) "}
{"8508": "\ndef f_delete_items ( self , iterator , * args , ** kwargs ) : \n    remove_from_trajectory = kwargs . pop ( 'remove_from_trajectory' , 0 ) \n    recursive = kwargs . get ( 'recursive' , 0 ) \n    fetched_items = self . _nn_interface . _fetch_items ( REMOVE , iterator , args , kwargs ) \n    if fetched_items : \n        try : \n            self . _storage_service . store ( pypetconstants . LIST , fetched_items , trajectory_name = self . v_name ) \n        except : \n            self . _logger . error ( 'Could not remove `%s` from the trajectory. Maybe the' ' item(s) was/were never stored to disk.' % str ( fetched_items ) ) \n            raise \n        for _ , item , dummy1 , dummy2 in fetched_items : \n            if remove_from_trajectory : \n                self . _nn_interface . _remove_node_or_leaf ( item , recursive = recursive ) \n            else : \n                item . _stored = 0 \n    else : \n        self . _logger . warning ( 'Your removal was not successful, could not find a single ' 'item to remove.' ) "}
{"8511": "\ndef _configure_pool ( kwargs ) : \n    _pool_single_run . storage_service = kwargs [ 'storage_service' ] \n    _configure_niceness ( kwargs ) \n    _configure_logging ( kwargs , extract = 0 ) "}
{"8512": "\ndef _configure_frozen_pool ( kwargs ) : \n    _frozen_pool_single_run . kwargs = kwargs \n    _configure_niceness ( kwargs ) \n    _configure_logging ( kwargs , extract = 0 ) \n    traj = kwargs [ 'traj' ] \n    traj . v_full_copy = kwargs [ 'full_copy' ] "}
{"8514": "\ndef _configure_frozen_scoop ( kwargs ) : \n    def _delete_old_scoop_rev_data ( old_scoop_rev ) : \n        if old_scoop_rev is not None : \n            try : \n                elements = shared . elements \n                for key in elements : \n                    var_dict = elements [ key ] \n                    if old_scoop_rev in var_dict : \n                        del var_dict [ old_scoop_rev ] \n                logging . getLogger ( 'pypet.scoop' ) . debug ( 'Deleted old SCOOP data from ' 'revolution `%s`.' % old_scoop_rev ) \n            except AttributeError : \n                logging . getLogger ( 'pypet.scoop' ) . error ( 'Could not delete old SCOOP data from ' 'revolution `%s`.' % old_scoop_rev ) \n    scoop_rev = kwargs . pop ( 'scoop_rev' ) \n    try : \n        old_scoop_rev = _frozen_scoop_single_run . kwargs [ 'scoop_rev' ] \n        configured = old_scoop_rev == scoop_rev \n    except ( AttributeError , KeyError ) : \n        old_scoop_rev = None \n        configured = 0 \n    if not configured : \n        _frozen_scoop_single_run . kwargs = shared . getConst ( scoop_rev , timeout = 424.2 ) \n        frozen_kwargs = _frozen_scoop_single_run . kwargs \n        frozen_kwargs [ 'scoop_rev' ] = scoop_rev \n        frozen_kwargs [ 'traj' ] . v_full_copy = frozen_kwargs [ 'full_copy' ] \n        if not scoop . IS_ORIGIN : \n            _configure_niceness ( frozen_kwargs ) \n            _configure_logging ( frozen_kwargs , extract = 0 ) \n        _delete_old_scoop_rev_data ( old_scoop_rev ) \n        logging . getLogger ( 'pypet.scoop' ) . info ( 'Configured Worker %s' % str ( scoop . worker ) ) "}
{"8515": "\ndef _scoop_single_run ( kwargs ) : \n    try : \n        try : \n            is_origin = scoop . IS_ORIGIN \n        except AttributeError : \n            is_origin = 1 \n        if not is_origin : \n            _configure_niceness ( kwargs ) \n            _configure_logging ( kwargs ) \n        return _single_run ( kwargs ) \n    except Exception : \n        scoop . logger . exception ( 'ERROR occurred during a single run!' ) \n        raise "}
{"8516": "\ndef _configure_logging ( kwargs , extract = 1 ) : \n    try : \n        logging_manager = kwargs [ 'logging_manager' ] \n        if extract : \n            logging_manager . extract_replacements ( kwargs [ 'traj' ] ) \n        logging_manager . make_logging_handlers_and_tools ( multiproc = 1 ) \n    except Exception as exc : \n        sys . stderr . write ( 'Could not configure logging system because of: %s' % repr ( exc ) ) \n        traceback . print_exc ( ) "}
{"8519": "\ndef _single_run ( kwargs ) : \n    pypet_root_logger = logging . getLogger ( 'pypet' ) \n    traj = kwargs [ 'traj' ] \n    runfunc = kwargs [ 'runfunc' ] \n    runargs = kwargs [ 'runargs' ] \n    kwrunparams = kwargs [ 'runkwargs' ] \n    clean_up_after_run = kwargs [ 'clean_up_runs' ] \n    automatic_storing = kwargs [ 'automatic_storing' ] \n    wrap_mode = kwargs [ 'wrap_mode' ] \n    idx = traj . v_idx \n    total_runs = len ( traj ) \n    pypet_root_logger . info ( '\\n=========================================\\n ' 'Starting single run #%d of %d ' '\\n=========================================\\n' % ( idx , total_runs ) ) \n    traj . f_start_run ( turn_into_run = 1 ) \n    result = runfunc ( traj , * runargs , ** kwrunparams ) \n    if automatic_storing : \n        traj . f_store ( ) \n    if wrap_mode == pypetconstants . WRAP_MODE_LOCAL : \n        result = ( ( traj . v_idx , result ) , traj . f_get_run_information ( traj . v_idx , copy = 0 ) , traj . v_storage_service . references ) \n        traj . v_storage_service . free_references ( ) \n    else : \n        result = ( ( traj . v_idx , result ) , traj . f_get_run_information ( traj . v_idx , copy = 0 ) ) \n    traj . f_finalize_run ( store_meta_data = 0 , clean_up = clean_up_after_run ) \n    pypet_root_logger . info ( '\\n=========================================\\n ' 'Finished single run #%d of %d ' '\\n=========================================\\n' % ( idx , total_runs ) ) \n    return result "}
{"8520": "\ndef _wrap_handling ( kwargs ) : \n    _configure_logging ( kwargs , extract = 0 ) \n    handler = kwargs [ 'handler' ] \n    graceful_exit = kwargs [ 'graceful_exit' ] \n    if graceful_exit : \n        sigint_handling . start ( ) \n    handler . run ( ) "}
{"8525": "\ndef _equal_values ( self , val1 , val2 ) : \n    if self . f_supports ( val1 ) != self . f_supports ( val2 ) : \n        return 0 \n    if not self . f_supports ( val1 ) and not self . f_supports ( val2 ) : \n        raise TypeError ( 'I do not support the types of both inputs (`%s` and `%s`), ' 'therefore I cannot judge whether ' 'the two are equal.' % ( str ( type ( val1 ) ) , str ( type ( val2 ) ) ) ) \n    if not self . _values_of_same_type ( val1 , val2 ) : \n        return 0 \n    return comparisons . nested_equal ( val1 , val2 ) "}
{"8526": "\ndef f_get_range ( self , copy = 1 ) : \n    if not self . f_has_range ( ) : \n        raise TypeError ( 'Your parameter `%s` is not array, so cannot return array.' % self . v_full_name ) \n    elif copy : \n        return self . _explored_range [ : ] \n    else : \n        return self . _explored_range "}
{"8527": "\ndef _explore ( self , explore_iterable ) : \n    if self . v_locked : \n        raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) \n    if self . f_has_range ( ) : \n        raise TypeError ( 'Your parameter `%s` is already explored, ' 'cannot _explore it further!' % self . _name ) \n    if self . _data is None : \n        raise TypeError ( 'Your parameter `%s` has no default value, please specify one ' 'via `f_set` before exploration. ' % self . v_full_name ) \n    data_list = self . _data_sanity_checks ( explore_iterable ) \n    self . _explored_range = data_list \n    self . _explored = 1 \n    self . f_lock ( ) "}
{"8530": "\ndef _store ( self ) : \n    if self . _data is not None : \n        store_dict = { 'data' : ObjectTable ( data = { 'data' : [ self . _data ] } ) } \n    if self . f_has_range ( ) : \n        store_dict [ 'explored_data' ] = ObjectTable ( data = { 'data' : self . _explored_range } ) \n    self . _locked = 1 \n    return store_dict "}
{"8531": "\ndef _load ( self , load_dict ) : \n    if self . v_locked : \n        raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) \n    if 'data' in load_dict : \n        self . _data = load_dict [ 'data' ] [ 'data' ] [ 0 ] \n        self . _default = self . _data \n    else : \n        self . _logger . warning ( 'Your parameter `%s` is empty, ' 'I did not find any data on disk.' % self . v_full_name ) \n    if 'explored_data' in load_dict : \n        self . _explored_range = [ x for x in load_dict [ 'explored_data' ] [ 'data' ] . tolist ( ) ] \n        self . _explored = 1 \n    self . _locked = 1 "}
{"8532": "\ndef _load ( self , load_dict ) : \n    if self . v_locked : \n        raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) \n    try : \n        self . _data = load_dict [ 'data' + ArrayParameter . IDENTIFIER ] \n        if 'explored_data' + ArrayParameter . IDENTIFIER in load_dict : \n            explore_table = load_dict [ 'explored_data' + ArrayParameter . IDENTIFIER ] \n            idx = explore_table [ 'idx' ] \n            explore_list = [ ] \n            for name_idx in idx : \n                arrayname = self . _build_name ( name_idx ) \n                explore_list . append ( load_dict [ arrayname ] ) \n            self . _explored_range = [ x for x in explore_list ] \n            self . _explored = 1 \n    except KeyError : \n        super ( ArrayParameter , self ) . _load ( load_dict ) \n    self . _default = self . _data \n    self . _locked = 1 "}
{"8533": "\ndef _equal_values ( self , val1 , val2 ) : \n    if self . _is_supported_matrix ( val1 ) : \n        if self . _is_supported_matrix ( val2 ) : \n            _ , _ , hash_tuple_1 = self . _serialize_matrix ( val1 ) \n            _ , _ , hash_tuple_2 = self . _serialize_matrix ( val2 ) \n            return hash ( hash_tuple_1 ) == hash ( hash_tuple_2 ) \n        else : \n            return 0 \n    else : \n        return super ( SparseParameter , self ) . _equal_values ( val1 , val2 ) "}
{"8538": "\ndef _load ( self , load_dict ) : \n    if self . v_locked : \n        raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) \n    try : \n        is_dia = load_dict [ 'data%sis_dia' % SparseParameter . IDENTIFIER ] \n        name_list = self . _get_name_list ( is_dia ) \n        rename_list = [ 'data%s%s' % ( SparseParameter . IDENTIFIER , name ) for name in name_list ] \n        data_list = [ load_dict [ name ] for name in rename_list ] \n        self . _data = self . _reconstruct_matrix ( data_list ) \n        if 'explored_data' + SparseParameter . IDENTIFIER in load_dict : \n            explore_table = load_dict [ 'explored_data' + SparseParameter . IDENTIFIER ] \n            idx_col = explore_table [ 'idx' ] \n            dia_col = explore_table [ 'is_dia' ] \n            explore_list = [ ] \n            for irun , name_id in enumerate ( idx_col ) : \n                is_dia = dia_col [ irun ] \n                try : \n                    name_list = self . _build_names ( name_id , is_dia ) \n                    data_list = [ load_dict [ name ] for name in name_list ] \n                except KeyError : \n                    name_list = self . _build_names_old ( name_id , is_dia ) \n                    data_list = [ load_dict [ name ] for name in name_list ] \n                matrix = self . _reconstruct_matrix ( data_list ) \n                explore_list . append ( matrix ) \n            self . _explored_range = explore_list \n            self . _explored = 1 \n    except KeyError : \n        super ( SparseParameter , self ) . _load ( load_dict ) \n    self . _default = self . _data \n    self . _locked = 1 "}
{"8539": "\ndef _store ( self ) : \n    store_dict = { } \n    if self . _data is not None : \n        dump = pickle . dumps ( self . _data , protocol = self . v_protocol ) \n        store_dict [ 'data' ] = dump \n        store_dict [ PickleParameter . PROTOCOL ] = self . v_protocol \n    if self . f_has_range ( ) : \n        store_dict [ 'explored_data' ] = ObjectTable ( columns = [ 'idx' ] , index = list ( range ( len ( self ) ) ) ) \n        smart_dict = { } \n        count = 0 \n        for idx , val in enumerate ( self . _explored_range ) : \n            obj_id = id ( val ) \n            if obj_id in smart_dict : \n                name_id = smart_dict [ obj_id ] \n                add = 0 \n            else : \n                name_id = count \n                add = 1 \n            name = self . _build_name ( name_id ) \n            store_dict [ 'explored_data' ] [ 'idx' ] [ idx ] = name_id \n            if add : \n                store_dict [ name ] = pickle . dumps ( val , protocol = self . v_protocol ) \n                smart_dict [ obj_id ] = name_id \n                count += 1 \n    self . _locked = 1 \n    return store_dict "}
{"8540": "\ndef _load ( self , load_dict ) : \n    if self . v_locked : \n        raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) \n    if 'data' in load_dict : \n        dump = load_dict [ 'data' ] \n        self . _data = pickle . loads ( dump ) \n    else : \n        self . _logger . warning ( 'Your parameter `%s` is empty, ' 'I did not find any data on disk.' % self . v_full_name ) \n    try : \n        self . v_protocol = load_dict [ PickleParameter . PROTOCOL ] \n    except KeyError : \n        self . v_protocol = PickleParameter . _get_protocol ( dump ) \n    if 'explored_data' in load_dict : \n        explore_table = load_dict [ 'explored_data' ] \n        name_col = explore_table [ 'idx' ] \n        explore_list = [ ] \n        for name_id in name_col : \n            arrayname = self . _build_name ( name_id ) \n            loaded = pickle . loads ( load_dict [ arrayname ] ) \n            explore_list . append ( loaded ) \n        self . _explored_range = explore_list \n        self . _explored = 1 \n    self . _default = self . _data \n    self . _locked = 1 "}
{"8543": "\ndef f_to_dict ( self , copy = 1 ) : \n    if copy : \n        return self . _data . copy ( ) \n    else : \n        return self . _data "}
{"8547": "\ndef _supports ( self , item ) : \n    if SparseParameter . _is_supported_matrix ( item ) : \n        return 1 \n    else : \n        return super ( SparseResult , self ) . _supports ( item ) "}
{"8553": "\ndef main ( ) : \n    folder = os . getcwd ( ) \n    print ( 'Merging all files' ) \n    merge_all_in_folder ( folder , delete_other_files = 1 , dynamic_imports = FunctionParameter , backup = 0 ) \n    print ( 'Done' ) "}
{"8564": "\ndef execute_network_pre_run ( self , traj , network , network_dict , component_list , analyser_list ) : \n    self . _execute_network_run ( traj , network , network_dict , component_list , analyser_list , pre_run = 1 ) "}
{"8565": "\ndef execute_network_run ( self , traj , network , network_dict , component_list , analyser_list ) : \n    self . _execute_network_run ( traj , network , network_dict , component_list , analyser_list , pre_run = 0 ) "}
{"8566": "\ndef _extract_subruns ( self , traj , pre_run = 0 ) : \n    if pre_run : \n        durations_list = traj . f_get_all ( self . _pre_durations_group_name ) \n    else : \n        durations_list = traj . f_get_all ( self . _durations_group_name ) \n    subruns = { } \n    orders = [ ] \n    for durations in durations_list : \n        for duration_param in durations . f_iter_leaves ( with_links = 0 ) : \n            if 'order' in duration_param . v_annotations : \n                order = duration_param . v_annotations . order \n            else : \n                raise RuntimeError ( 'Your duration parameter %s has no order. Please add ' 'an order in `v_annotations.order`.' % duration_param . v_full_name ) \n            if order in subruns : \n                raise RuntimeError ( 'Your durations must differ in their order, there are two ' 'with order %d.' % order ) \n            else : \n                subruns [ order ] = duration_param \n                orders . append ( order ) \n    return [ subruns [ order ] for order in sorted ( orders ) ] "}
{"8567": "\ndef _execute_network_run ( self , traj , network , network_dict , component_list , analyser_list , pre_run = 0 ) : \n    subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) \n    subrun_number = 0 \n    while len ( subrun_list ) > 0 : \n        current_subrun = subrun_list . pop ( 0 ) \n        for component in component_list : \n            component . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . _logger . info ( 'STARTING subrun `%s` (#%d) lasting %s.' % ( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) \n        network . run ( duration = current_subrun . f_get ( ) , report = self . _report , report_period = self . _report_period ) \n        for analyser in analyser_list : \n            analyser . analyse ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for component in component_list : \n            component . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        subrun_number += 1 "}
{"8569": "\ndef pre_run_network ( self , traj ) : \n    self . pre_build ( traj ) \n    self . _logger . info ( '\\n------------------------\\n' 'Pre-Running the Network\\n' '------------------------' ) \n    self . _network = self . _network_constructor ( * self . _brian_list ) \n    self . network_runner . execute_network_pre_run ( traj , self . _network , self . _network_dict , self . components , self . analysers ) \n    self . _logger . info ( '\\n-----------------------------\\n' 'Network Simulation successful\\n' '-----------------------------' ) \n    self . _pre_run = 1 \n    if hasattr ( self . _network , 'store' ) : \n        self . _network . store ( 'pre_run' ) "}
{"8573": "\ndef next ( self ) : \n    while 1 : \n        try : \n            return next ( self . _current ) \n        except StopIteration : \n            try : \n                self . _current = iter ( self . _chain . popleft ( ) ) \n            except IndexError : \n                raise StopIteration ( 'Reached end of iterator chain' ) "}
{"8574": "\ndef merge_all_in_folder ( folder , ext = '.hdf5' , dynamic_imports = None , storage_service = None , force = 0 , ignore_data = ( ) , move_data = 0 , delete_other_files = 0 , keep_info = 1 , keep_other_trajectory_info = 1 , merge_config = 1 , backup = 1 ) : \n    in_dir = os . listdir ( folder ) \n    all_files = [ ] \n    for file in in_dir : \n        full_file = os . path . join ( folder , file ) \n        if os . path . isfile ( full_file ) : \n            _ , extension = os . path . splitext ( full_file ) \n            if extension == ext : \n                all_files . append ( full_file ) \n    all_files = sorted ( all_files ) \n    trajs = [ ] \n    for full_file in all_files : \n        traj = load_trajectory ( index = - 1 , storage_service = storage_service , filename = full_file , load_data = 0 , force = force , dynamic_imports = dynamic_imports ) \n        trajs . append ( traj ) \n    first_traj = trajs . pop ( 0 ) \n    first_traj . f_merge_many ( trajs , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = 0 , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = backup ) \n    if delete_other_files : \n        for file in all_files [ 1 : ] : \n            os . remove ( file ) \n    return first_traj "}
{"8575": "\ndef _handle_sigint ( self , signum , frame ) : \n    if self . hit : \n        prompt = 'Exiting immediately!' \n        raise KeyboardInterrupt ( prompt ) \n    else : \n        self . hit = 1 \n        prompt = ( '\\nYou killed the process(es) via `SIGINT` (`CTRL+C`). ' 'I am trying to exit ' 'gracefully. Using `SIGINT` (`CTRL+C`) ' 'again will cause an immediate exit.\\n' ) \n        sys . stderr . write ( prompt ) "}
{"8576": "\ndef config_from_file ( filename , config = None ) : \n    if config : \n        try : \n            with open ( filename , 'w' ) as fdesc : \n                fdesc . write ( json . dumps ( config ) ) \n        except IOError as error : \n            logger . exception ( error ) \n            return 0 \n        return 1 \n    else : \n        if os . path . isfile ( filename ) : \n            try : \n                with open ( filename , 'r' ) as fdesc : \n                    return json . loads ( fdesc . read ( ) ) \n            except IOError as error : \n                return 0 \n        else : \n            return { } "}
{"8579": "\ndef refresh_tokens ( self ) : \n    url = 'https://api.ecobee.com/token' \n    params = { 'grant_type' : 'refresh_token' , 'refresh_token' : self . refresh_token , 'client_id' : self . api_key } \n    request = requests . post ( url , params = params ) \n    if request . status_code == requests . codes . ok : \n        self . access_token = request . json ( ) [ 'access_token' ] \n        self . refresh_token = request . json ( ) [ 'refresh_token' ] \n        self . write_tokens_to_file ( ) \n        return 1 \n    else : \n        self . request_pin ( ) "}
{"8580": "\ndef get_thermostats ( self ) : \n    url = 'https://api.ecobee.com/1/thermostat' \n    header = { 'Content-Type' : 'application/json;charset=UTF-8' , 'Authorization' : 'Bearer ' + self . access_token } \n    params = { 'json' : ( '{\"selection\":{\"selectionType\":\"registered\",' '\"includeRuntime\":\"true\",' '\"includeSensors\":\"true\",' '\"includeProgram\":\"true\",' '\"includeEquipmentStatus\":\"true\",' '\"includeEvents\":\"true\",' '\"includeWeather\":\"true\",' '\"includeSettings\":\"true\"}}' ) } \n    try : \n        request = requests . get ( url , headers = header , params = params ) \n    except RequestException : \n        logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" ) \n        return None \n    if request . status_code == requests . codes . ok : \n        self . authenticated = 1 \n        self . thermostats = request . json ( ) [ 'thermostatList' ] \n        return self . thermostats \n    else : \n        self . authenticated = 0 \n        logger . info ( \"Error connecting to Ecobee while attempting to get \" \"thermostat data.  Refreshing tokens and trying again.\" ) \n        if self . refresh_tokens ( ) : \n            return self . get_thermostats ( ) \n        else : \n            return None "}
{"8587": "\ndef resume_program ( self , index , resume_all = 0 ) : \n    body = { \"selection\" : { \"selectionType\" : \"thermostats\" , \"selectionMatch\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"resumeProgram\" , \"params\" : { \"resumeAll\" : resume_all } } ] } \n    log_msg_action = \"resume program\" \n    return self . make_request ( body , log_msg_action ) "}
{"8602": "\ndef process_received_ack ( self , pkt ) : \n    if isack ( pkt ) : \n        try : \n            self . event = self . client . handle_ack ( pkt , self . time_sent_request ) \n        except AddrFormatError as err : \n            logger . error ( err ) \n            raise self . SELECTING ( ) \n        logger . info ( 'DHCPACK of %s from %s' % ( self . client . client_ip , self . client . server_ip ) ) \n        return 1 \n    return 0 "}
{"8603": "\ndef process_received_nak ( self , pkt ) : \n    if isnak ( pkt ) : \n        logger . info ( 'DHCPNAK of %s from %s' , self . client . client_ip , self . client . server_ip ) \n        return 1 \n    return 0 "}
{"8605": "\ndef BOUND ( self ) : \n    logger . debug ( 'In state: BOUND' ) \n    logger . info ( '(%s) state changed %s -> bound' , self . client . iface , STATES2NAMES [ self . current_state ] ) \n    self . current_state = STATE_BOUND \n    self . client . lease . info_lease ( ) \n    if self . script is not None : \n        self . script . script_init ( self . client . lease , self . current_state ) \n        self . script . script_go ( ) \n    else : \n        try : \n            set_net ( self . client . lease ) \n        except Exception as e : \n            logger . error ( 'Can not set IP' , exc_info = 1 ) "}
{"8632": "\ndef equal ( self , cwd ) : \n    cmd = [ \"diff\" ] \n    cmd . append ( \"-q\" ) \n    cmd . append ( self . left . get_name ( ) ) \n    cmd . append ( self . right . get_name ( ) ) \n    try : \n        Process ( cmd ) . run ( cwd = cwd , suppress_output = 1 ) \n    except SubprocessError as e : \n        if e . get_returncode ( ) == 1 : \n            return 0 \n        else : \n            raise e \n    return 1 "}
{"8634": "\ndef delete_next ( self , remove = 0 , backup = 0 ) : \n    patch = self . db . top_patch ( ) \n    if patch : \n        after = self . series . patch_after ( patch ) \n    else : \n        after = self . series . first_patch ( ) \n    if not after : \n        raise QuiltError ( \"No next patch\" ) \n    self . _delete_patch ( after , remove = remove , backup = backup ) "}
{"8635": "\ndef delete_patch ( self , patch_name = None , remove = 0 , backup = 0 ) : \n    if patch_name : \n        patch = Patch ( patch_name ) \n    else : \n        patch = self . db . top_patch ( ) \n        if not patch : \n            raise NoAppliedPatch ( self . db ) \n    self . _delete_patch ( patch , remove = remove , backup = backup ) "}
{"8636": "\ndef _file_in_patch ( self , filename , patch , ignore ) : \n    file = self . quilt_pc + File ( os . path . join ( patch . get_name ( ) , filename ) ) \n    if file . exists ( ) : \n        if ignore : \n            return 1 \n        else : \n            raise QuiltError ( \"File %s is already in patch %s\" % ( filename , patch . get_name ( ) ) ) \n    return 0 "}
{"8637": "\ndef _backup_file ( self , file , patch ) : \n    dest_dir = self . quilt_pc + patch . get_name ( ) \n    file_dir = file . get_directory ( ) \n    if file_dir : \n        dest_dir = dest_dir + file_dir \n    backup = Backup ( ) \n    backup . backup_file ( file , dest_dir , copy_empty = 1 ) "}
{"8638": "\ndef add_file ( self , filename , patch_name = None , ignore = 0 ) : \n    file = File ( filename ) \n    if patch_name : \n        patch = Patch ( patch_name ) \n    else : \n        patch = self . db . top_patch ( ) \n        if not patch : \n            raise NoAppliedPatch ( self . db ) \n    exists = self . _file_in_patch ( filename , patch , ignore ) \n    if exists : \n        return \n    self . _file_in_next_patches ( filename , patch ) \n    if file . is_link ( ) : \n        raise QuiltError ( \"Cannot add symbolic link %s\" % filename ) \n    self . _backup_file ( file , patch ) \n    if file . exists ( ) : \n        os . chmod ( filename , file . get_mode ( ) | stat . S_IWUSR | stat . S_IRUSR ) \n    self . file_added ( file , patch ) "}
{"8639": "\ndef run ( self , suppress_output = 0 , inputdata = None , ** kw ) : \n    if inputdata is not None : \n        kw [ \"stdin\" ] = subprocess . PIPE \n    if suppress_output : \n        kw [ \"stdout\" ] = open ( os . devnull , \"w\" ) \n        kw [ \"stderr\" ] = kw [ \"stdout\" ] \n    try : \n        try : \n            process = subprocess . Popen ( self . cmd , ** kw ) \n        finally : \n            if suppress_output : \n                kw [ \"stdout\" ] . close ( ) \n    except OSError as e : \n        msg = \"Failed starting command {!r}: {}\" . format ( self . cmd , e ) \n        raise QuiltError ( msg ) \n    if inputdata is not None : \n        process . stdin . write ( inputdata ) \n        process . stdin . close ( ) \n    ret = process . wait ( ) \n    if ret != 0 : \n        raise SubprocessError ( self . cmd , ret ) "}
{"8641": "\ndef copy ( self , dest , symlinks = 0 ) : \n    if isinstance ( dest , Directory ) : \n        dest = dest . get_name ( ) \n    shutil . copytree ( self . dirname , dest ) "}
{"8645": "\ndef backup_file ( self , file , dest_dir , copy_empty = 0 ) : \n    if file . exists ( ) : \n        if not copy_empty and file . is_empty ( ) : \n            return None \n        dest_dir . create ( ) \n        file . copy ( dest_dir ) \n        return dest_dir + file . get_basefile ( ) \n    elif copy_empty : \n        dest_dir = dest_dir + file . get_directory ( ) \n        dest_dir . create ( ) \n        dest_file = dest_dir + file . get_basefile ( ) \n        dest_file . touch ( ) \n        return dest_file \n    else : \n        return None "}
{"8646": "\ndef refresh ( self , patch_name = None , edit = 0 ) : \n    if patch_name : \n        patch = Patch ( patch_name ) \n    else : \n        patch = self . db . top_patch ( ) \n        if not patch : \n            raise QuiltError ( \"No patch applied. Nothing to refresh.\" ) \n    pc_dir = self . quilt_pc + patch . get_name ( ) \n    patch_file = self . quilt_patches + File ( patch . get_name ( ) ) \n    files = pc_dir . content ( ) [ 1 ] \n    with TmpFile ( prefix = \"pquilt-\" ) as tmpfile : \n        f = tmpfile . open ( ) \n        if patch_file . exists ( ) : \n            header = patch . get_header ( self . quilt_patches ) \n            tmpfile . write ( header ) \n        for file_name in files : \n            if file_name == \".timestamp\" : \n                continue \n            orig_file = pc_dir + File ( file_name ) \n            new_file = File ( file_name ) \n            left_label , right_label , index = self . _get_labels ( file_name , orig_file , new_file ) \n            self . _write_index ( tmpfile , index ) \n            diff = Diff ( orig_file , new_file ) \n            diff . run ( self . cwd , fd = f , left_label = left_label , right_label = right_label ) \n        if tmpfile . is_empty ( ) : \n            raise QuiltError ( \"Nothing to refresh.\" ) \n        if edit : \n            self . edit_patch ( tmpfile ) \n            tpatch = Patch ( tmpfile . get_name ( ) ) \n            tpatch . run ( pc_dir . get_name ( ) , dry_run = 1 , quiet = 1 ) \n        if patch_file . exists ( ) : \n            diff = Diff ( patch_file , tmpfile ) \n            if diff . equal ( self . cwd ) : \n                raise QuiltError ( \"Nothing to refresh.\" ) \n        tmpfile . copy ( patch_file ) \n    timestamp = pc_dir + File ( \".timestamp\" ) \n    timestamp . touch ( ) \n    refresh = self . quilt_pc + File ( patch . get_name ( ) + \"~refresh\" ) \n    refresh . delete_if_exists ( ) \n    self . refreshed ( patch ) "}
{"8647": "\ndef unapply_patch ( self , patch_name , force = 0 ) : \n    self . _check ( force ) \n    patches = self . db . patches_after ( Patch ( patch_name ) ) \n    for patch in reversed ( patches ) : \n        self . _unapply_patch ( patch ) \n    self . db . save ( ) \n    self . unapplied ( self . db . top_patch ( ) ) "}
{"8648": "\ndef unapply_top_patch ( self , force = 0 ) : \n    self . _check ( force ) \n    patch = self . db . top_patch ( ) \n    self . _unapply_patch ( patch ) \n    self . db . save ( ) \n    self . unapplied ( self . db . top_patch ( ) ) "}
{"8649": "\ndef unapply_all ( self , force = 0 ) : \n    self . _check ( force ) \n    for patch in reversed ( self . db . applied_patches ( ) ) : \n        self . _unapply_patch ( patch ) \n    self . db . save ( ) \n    self . unapplied ( self . db . top_patch ( ) ) "}
{"8650": "\ndef apply_patch ( self , patch_name , force = 0 , quiet = 0 ) : \n    self . _check ( ) \n    patch = Patch ( patch_name ) \n    patches = self . series . patches_until ( patch ) [ : ] \n    applied = self . db . applied_patches ( ) \n    for patch in applied : \n        if patch in patches : \n            patches . remove ( patch ) \n    if not patches : \n        raise AllPatchesApplied ( self . series , self . db . top_patch ( ) ) \n    self . applying ( patch ) \n    try : \n        for cur_patch in patches : \n            self . _apply_patch ( cur_patch , force , quiet ) \n    finally : \n        self . db . save ( ) \n    self . applied ( self . db . top_patch ( ) ) "}
{"8651": "\ndef apply_next_patch ( self , force = 0 , quiet = 0 ) : \n    self . _check ( ) \n    top = self . db . top_patch ( ) \n    if not top : \n        patch = self . series . first_patch ( ) \n    else : \n        patch = self . series . patch_after ( top ) \n    if not patch : \n        raise AllPatchesApplied ( self . series , top ) \n    self . applying ( patch ) \n    self . _apply_patch ( patch , force , quiet ) \n    self . db . save ( ) \n    self . applied ( self . db . top_patch ( ) ) "}
{"8652": "\ndef apply_all ( self , force = 0 , quiet = 0 ) : \n    self . _check ( ) \n    top = self . db . top_patch ( ) \n    if top : \n        patches = self . series . patches_after ( top ) \n    else : \n        patches = self . series . patches ( ) \n    if not patches : \n        raise AllPatchesApplied ( self . series , top ) \n    try : \n        for patch in patches : \n            self . applying ( patch ) \n            self . _apply_patch ( patch , force , quiet ) \n    finally : \n        self . db . save ( ) \n    self . applied ( self . db . top_patch ( ) ) "}
{"8681": "\ndef process ( self ) : \n    self . rh = RelationHandler ( ) \n    self . rh . apply_file ( self . filename ) \n    logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) \n    node_ids , stop_node_ids , way_ids , reverse_map = self . __collect_ids ( ) \n    self . nh = NodeHandler ( node_ids ) \n    self . nh . apply_file ( self . filename , locations = 1 ) \n    count = 0 \n    for idx , missing_node_id in enumerate ( self . nh . missing_node_ids ) : \n        count += 1 \n        logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse_map [ missing_node_id ] , missing_node_id ) \n    if count : \n        logging . warning ( '%d nodes that appear in relations are missing.' , count ) \n    else : \n        logging . debug ( 'Lucky you! All relation member nodes were found.' ) \n    self . wh = WayHandler ( way_ids ) \n    self . wh . apply_file ( self . filename , locations = 1 ) "}
{"8693": "\ndef authenticate ( devices , params , facet , check_only ) : \n    for device in devices [ : ] : \n        try : \n            device . open ( ) \n        except : \n            devices . remove ( device ) \n    try : \n        prompted = 0 \n        while devices : \n            removed = [ ] \n            for device in devices : \n                try : \n                    return u2f . authenticate ( device , params , facet , check_only ) \n                except exc . APDUError as e : \n                    if e . code == APDU_USE_NOT_SATISFIED : \n                        if check_only : \n                            sys . stderr . write ( '\\nCorrect U2F device present!\\n' ) \n                            sys . exit ( 0 ) \n                        if not prompted : \n                            sys . stderr . write ( '\\nTouch the flashing U2F device ' 'to authenticate...\\n' ) \n                            prompted = 1 \n                    else : \n                        removed . append ( device ) \n                except exc . DeviceError : \n                    removed . append ( device ) \n            devices = [ d for d in devices if d not in removed ] \n            for d in removed : \n                d . close ( ) \n            time . sleep ( 0.25 ) \n    finally : \n        for device in devices : \n            device . close ( ) \n    sys . stderr . write ( '\\nThe required U2F device is not present!\\n' ) \n    sys . exit ( 1 ) "}
{"8695": "\ndef authenticate ( device , data , facet , check_only = 0 ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if data [ 'version' ] != VERSION : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    key_handle = websafe_decode ( data [ 'keyHandle' ] ) \n    client_data = { 'typ' : 'navigator.id.getAssertion' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param + int2byte ( len ( key_handle ) ) + key_handle \n    p1 = 0x07 if check_only else 0x03 \n    p2 = 0 \n    response = device . send_apdu ( INS_SIGN , p1 , p2 , request ) \n    return { 'clientData' : websafe_encode ( client_data ) , 'signatureData' : websafe_encode ( response ) , 'keyHandle' : data [ 'keyHandle' ] } "}
{"8698": "\ndef wrap_function ( func = None , error_threshold = None , reraise_exception = 1 , save_current_stack_trace = 1 ) : \n    if func : \n        return flawless . client . client . _wrap_function_with_error_decorator ( func = func , error_threshold = error_threshold , reraise_exception = reraise_exception , save_current_stack_trace = save_current_stack_trace ) \n    else : \n        return functools . partial ( flawless . client . client . _wrap_function_with_error_decorator , error_threshold = error_threshold , reraise_exception = reraise_exception , save_current_stack_trace = save_current_stack_trace ) "}
{"8699": "\ndef wrap_class ( cls , error_threshold = None ) : \n    methods = inspect . getmembers ( cls , inspect . ismethod ) + inspect . getmembers ( cls , inspect . isfunction ) \n    for method_name , method in methods : \n        wrapped_method = flawless . client . client . _wrap_function_with_error_decorator ( method if not im_self ( method ) else im_func ( method ) , save_current_stack_trace = 0 , error_threshold = error_threshold , ) \n        if im_self ( method ) : \n            wrapped_method = classmethod ( wrapped_method ) \n        setattr ( cls , method_name , wrapped_method ) \n    return cls "}
{"8700": "\ndef _matches_filepath_pattern ( self , filepath ) : \n    if not self . only_blame_patterns : \n        return 1 \n    for pattern in self . only_blame_patterns : \n        if pattern . match ( filepath ) : \n            return 1 \n    return 0 "}
{"8708": "\ndef validate ( validator ) : \n    def decorator ( func ) : \n        \n        @ wraps ( func ) \n        def wrapper ( image , size , validate = 1 ) : \n            if validate : \n                validator ( image , size ) \n            return func ( image , size ) \n        return wrapper \n    return decorator "}
{"8719": "\ndef save_item ( self , item , data , instance , commit = 1 ) : \n    if commit : \n        instance . save ( ) \n    return instance "}
{"8721": "\ndef load ( self , source ) : \n    self . source = open ( self . source , 'rb' ) \n    self . loaded = 1 "}
{"8724": "\ndef run_command ( self , args : List [ str ] , max_num_processes : int = None , max_stack_size : int = None , max_virtual_memory : int = None , as_root : bool = 0 , stdin : FileIO = None , timeout : int = None , check : bool = 0 , truncate_stdout : int = None , truncate_stderr : int = None ) -> 'CompletedCommand' : \n    cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] \n    if stdin is None : \n        cmd . append ( '--stdin_devnull' ) \n    if max_num_processes is not None : \n        cmd += [ '--max_num_processes' , str ( max_num_processes ) ] \n    if max_stack_size is not None : \n        cmd += [ '--max_stack_size' , str ( max_stack_size ) ] \n    if max_virtual_memory is not None : \n        cmd += [ '--max_virtual_memory' , str ( max_virtual_memory ) ] \n    if timeout is not None : \n        cmd += [ '--timeout' , str ( timeout ) ] \n    if truncate_stdout is not None : \n        cmd += [ '--truncate_stdout' , str ( truncate_stdout ) ] \n    if truncate_stderr is not None : \n        cmd += [ '--truncate_stderr' , str ( truncate_stderr ) ] \n    if not as_root : \n        cmd += [ '--linux_user_id' , str ( self . _linux_uid ) ] \n    cmd += args \n    if self . debug : \n        print ( 'running: {}' . format ( cmd ) , flush = 1 ) \n    with tempfile . TemporaryFile ( ) as f : \n        try : \n            subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = 1 ) \n            f . seek ( 0 ) \n            json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            results_json = json . loads ( f . read ( json_len ) . decode ( ) ) \n            stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stdout = tempfile . NamedTemporaryFile ( ) \n            stdout . write ( f . read ( stdout_len ) ) \n            stdout . seek ( 0 ) \n            stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stderr = tempfile . NamedTemporaryFile ( ) \n            stderr . write ( f . read ( stderr_len ) ) \n            stderr . seek ( 0 ) \n            result = CompletedCommand ( return_code = results_json [ 'return_code' ] , timed_out = results_json [ 'timed_out' ] , stdout = stdout , stderr = stderr , stdout_truncated = results_json [ 'stdout_truncated' ] , stderr_truncated = results_json [ 'stderr_truncated' ] ) \n            if ( result . return_code != 0 or results_json [ 'timed_out' ] ) and check : \n                raise subprocess . CalledProcessError ( result . return_code , cmd , output = result . stdout , stderr = result . stderr ) \n            return result \n        except subprocess . CalledProcessError as e : \n            f . seek ( 0 ) \n            print ( f . read ( ) ) \n            print ( e . stderr ) \n            raise "}
{"8725": "\ndef add_files ( self , * filenames : str , owner : str = SANDBOX_USERNAME , read_only : bool = 0 ) : \n    if owner != SANDBOX_USERNAME and owner != 'root' : \n        raise ValueError ( 'Invalid value for parameter \"owner\": {}' . format ( owner ) ) \n    with tempfile . TemporaryFile ( ) as f , tarfile . TarFile ( fileobj = f , mode = 'w' ) as tar_file : \n        for filename in filenames : \n            tar_file . add ( filename , arcname = os . path . basename ( filename ) ) \n        f . seek ( 0 ) \n        subprocess . check_call ( [ 'docker' , 'cp' , '-' , self . name + ':' + SANDBOX_WORKING_DIR_NAME ] , stdin = f ) \n        file_basenames = [ os . path . basename ( filename ) for filename in filenames ] \n        if owner == SANDBOX_USERNAME : \n            self . _chown_files ( file_basenames ) \n        if read_only : \n            chmod_cmd = [ 'chmod' , '444' ] + file_basenames \n            self . run_command ( chmod_cmd , as_root = 1 ) "}
{"8731": "\ndef get_enrollments_for_regid ( self , regid , params = { } , include_courses = 1 ) : \n    sis_user_id = self . _sis_id ( regid , sis_field = \"user\" ) \n    url = USERS_API . format ( sis_user_id ) + \"/enrollments\" \n    courses = Courses ( ) if include_courses else None \n    enrollments = [ ] \n    for datum in self . _get_paged_resource ( url , params = params ) : \n        enrollment = CanvasEnrollment ( data = datum ) \n        if include_courses : \n            course_id = datum [ \"course_id\" ] \n            course = courses . get_course ( course_id ) \n            if course . sis_course_id is not None : \n                enrollment . course = course \n                enrollment . course_url = course . course_url \n                enrollment . course_name = course . name \n                enrollment . sis_course_id = course . sis_course_id \n        else : \n            enrollment . course_url = re . sub ( r'/users/\\d+$' , '' , enrollment . html_url ) \n        enrollments . append ( enrollment ) \n    return enrollments "}
{"8742": "\ndef get_published_courses_in_account ( self , account_id , params = { } ) : \n    params [ \"published\" ] = 1 \n    return self . get_courses_in_account ( account_id , params ) "}
{"8759": "\ndef _delete_external_tool ( self , context , context_id , external_tool_id ) : \n    url = context . format ( context_id ) + \"/external_tools/{}\" . format ( external_tool_id ) \n    response = self . _delete_resource ( url ) \n    return 1 "}
{"8768": "\ndef _get_resource_url ( self , url , auto_page , data_key ) : \n    headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    response = DAO . getURL ( url , headers ) \n    if response . status != 200 : \n        raise DataFailureException ( url , response . status , response . data ) \n    data = json . loads ( response . data ) \n    self . next_page_url = self . _next_page ( response ) \n    if auto_page and self . next_page_url : \n        if isinstance ( data , list ) : \n            data . extend ( self . _get_resource_url ( self . next_page_url , 1 , data_key ) ) \n        elif isinstance ( data , dict ) and data_key is not None : \n            data [ data_key ] . extend ( self . _get_resource_url ( self . next_page_url , 1 , data_key ) [ data_key ] ) \n    return data "}
{"8770": "\ndef _get_resource ( self , url , params = None , data_key = None ) : \n    if not params : \n        params = { } \n    self . _set_as_user ( params ) \n    full_url = url + self . _params ( params ) \n    return self . _get_resource_url ( full_url , 1 , data_key ) "}
{"8775": "\ndef create_admin ( self , account_id , user_id , role ) : \n    url = ADMINS_API . format ( account_id ) \n    body = { \"user_id\" : unquote ( str ( user_id ) ) , \"role\" : role , \"send_confirmation\" : 0 } \n    return CanvasAdmin ( data = self . _post_resource ( url , body ) ) "}
{"8777": "\ndef delete_admin ( self , account_id , user_id , role ) : \n    url = ADMINS_API . format ( account_id ) + \"/{}?role={}\" . format ( user_id , quote ( role ) ) \n    response = self . _delete_resource ( url ) \n    return 1 "}
{"8805": "\ndef create_course_provisioning_report ( self , account_id , term_id = None , params = { } ) : \n    params [ \"courses\" ] = 1 \n    return self . create_report ( ReportType . PROVISIONING , account_id , term_id , params ) "}
{"8806": "\ndef create_course_sis_export_report ( self , account_id , term_id = None , params = { } ) : \n    params [ \"courses\" ] = 1 \n    return self . create_report ( ReportType . SIS_EXPORT , account_id , term_id , params ) "}
{"8810": "\ndef delete_report ( self , report ) : \n    url = ACCOUNTS_API . format ( report . account_id ) + \"/reports/{}/{}\" . format ( report . type , report . report_id ) \n    response = self . _delete_resource ( url ) \n    return 1 "}
{"8815": "\ndef empty_value ( self ) : \n    edit_empty_value = self . config . get ( 'edit_empty_value' , 0 ) \n    if edit_empty_value : \n        return edit_empty_value \n    else : \n        return unicode ( inplace_settings . INPLACEEDIT_EDIT_EMPTY_VALUE ) "}
{"8819": "\ndef _configure_registry ( self , include_process_stats : bool = 0 ) : \n    if include_process_stats : \n        self . registry . register_additional_collector ( ProcessCollector ( registry = None ) ) "}
{"8835": "\ndef _process_query ( self , query , prepared = 0 ) : \n    if prepared is 1 : \n        files = { 'query' : str ( query ) } \n        logger . debug ( 'About to submit the following query {}' . format ( query ) ) \n        res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n        if status == 200 : \n            return self . decode ( res ) , status \n        else : \n            logger . debug ( 'Disambiguation failed.' ) \n            return None , status \n    text = query [ 'text' ] \n    sentence_coordinates = [ { \"offsetStart\" : 0 , \"offsetEnd\" : len ( text ) } ] \n    total_nb_sentences = len ( sentence_coordinates ) \n    sentences_groups = [ ] \n    if len ( text ) > self . max_text_length : \n        res , status_code = self . segment ( text ) \n        if status_code == 200 : \n            sentence_coordinates = res [ 'sentences' ] \n            total_nb_sentences = len ( sentence_coordinates ) \n        else : \n            logger . error ( 'Error during the segmentation of the text.' ) \n        logger . debug ( 'Text too long, split in {} sentences; building groups of {} ' 'sentences.' . format ( total_nb_sentences , self . sentences_per_group ) ) \n        sentences_groups = self . _group_sentences ( total_nb_sentences , self . sentences_per_group ) \n    else : \n        query [ 'sentence' ] = \"true\" \n    if total_nb_sentences > 1 : \n        query [ 'sentences' ] = sentence_coordinates \n    if len ( sentences_groups ) > 0 : \n        for group in sentences_groups : \n            query [ 'processSentence' ] = group \n            res , status_code = self . _process_query ( query , prepared = 1 ) \n            if status_code == 200 : \n                if 'entities' in res : \n                    query [ 'entities' ] = res [ u'entities' ] \n                query [ 'language' ] = res [ u'language' ] \n            else : \n                logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n                return None , status_code \n    else : \n        res , status_code = self . _process_query ( query , prepared = 1 ) \n        if status_code == 200 : \n            query [ 'language' ] = res [ u'language' ] \n            if 'entities' in res : \n                query [ 'entities' ] = res [ u'entities' ] \n        else : \n            logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n            return None , status_code \n    return query , status_code "}
{"8852": "\ndef plot_mdr_grid ( mdr_instance ) : \n    var1_levels = list ( set ( [ variables [ 0 ] for variables in mdr_instance . feature_map ] ) ) \n    var2_levels = list ( set ( [ variables [ 1 ] for variables in mdr_instance . feature_map ] ) ) \n    max_count = np . array ( list ( mdr_instance . class_count_matrix . values ( ) ) ) . flatten ( ) . max ( ) \n    fig , splots = plt . subplots ( ncols = len ( var1_levels ) , nrows = len ( var2_levels ) , sharey = 1 , sharex = 1 ) \n    fig . set_figwidth ( 6 ) \n    fig . set_figheight ( 6 ) \n    for ( var1 , var2 ) in itertools . product ( var1_levels , var2_levels ) : \n        class_counts = mdr_instance . class_count_matrix [ ( var1 , var2 ) ] \n        splot = splots [ var2_levels . index ( var2 ) ] [ var1_levels . index ( var1 ) ] \n        splot . set_yticks ( [ ] ) \n        splot . set_xticks ( [ ] ) \n        splot . set_ylim ( 0 , max_count * 1.5 ) \n        splot . set_xlim ( - 0.5 , 1.5 ) \n        if var2_levels . index ( var2 ) == 0 : \n            splot . set_title ( 'X1 = {}' . format ( var1 ) , fontsize = 12 ) \n        if var1_levels . index ( var1 ) == 0 : \n            splot . set_ylabel ( 'X2 = {}' . format ( var2 ) , fontsize = 12 ) \n        bars = splot . bar ( left = range ( class_counts . shape [ 0 ] ) , height = class_counts , width = 0.5 , color = 'black' , align = 'center' ) \n        bgcolor = 'lightgrey' if mdr_instance . feature_map [ ( var1 , var2 ) ] == 0 else 'darkgrey' \n        splot . set_axis_bgcolor ( bgcolor ) \n        for index , bar in enumerate ( bars ) : \n            splot . text ( index , class_counts [ index ] + ( max_count * 0.1 ) , class_counts [ index ] , ha = 'center' ) \n    fig . tight_layout ( ) \n    return fig "}
{"8868": "\ndef _is_macro ( v : Var ) -> bool : \n    return ( Maybe ( v . meta ) . map ( lambda m : m . entry ( SYM_MACRO_META_KEY , None ) ) . or_else_get ( 0 ) ) "}
{"8873": "\ndef __resolve_bare_symbol ( ctx : ParserContext , form : sym . Symbol ) -> Union [ MaybeClass , VarRef ] : \n    assert form . ns is None \n    v = ctx . current_ns . find ( form ) \n    if v is not None : \n        return VarRef ( form = form , var = v , env = ctx . get_node_env ( ) ) \n    if \".\" in form . name : \n        raise ParserException ( \"symbol names may not contain the '.' operator\" , form = form ) \n    munged = munge ( form . name , allow_builtins = 1 ) \n    if munged in vars ( builtins ) : \n        return MaybeClass ( form = form , class_ = munged , target = vars ( builtins ) [ munged ] , env = ctx . get_node_env ( ) , ) \n    assert munged not in vars ( ctx . current_ns . module ) \n    raise ParserException ( f\"unable to resolve symbol '{form}' in this context\" , form = form ) "}
{"8875": "\ndef parse_ast ( ctx : ParserContext , form : ReaderForm ) -> Node : \n    return _parse_ast ( ctx , form ) . assoc ( top_level = 1 ) "}
{"8876": "\ndef warn_on_shadowed_var ( self ) -> bool : \n    return self . warn_on_shadowed_name or self . _opts . entry ( WARN_ON_SHADOWED_VAR , 0 ) "}
{"8877": "\ndef put_new_symbol ( self , s : sym . Symbol , binding : Binding , warn_on_shadowed_name : bool = 1 , warn_on_shadowed_var : bool = 1 , warn_if_unused : bool = 1 , ) : \n    st = self . symbol_table \n    if warn_on_shadowed_name and self . warn_on_shadowed_name : \n        if st . find_symbol ( s ) is not None : \n            logger . warning ( f\"name '{s}' shadows name from outer scope\" ) \n    if ( warn_on_shadowed_name or warn_on_shadowed_var ) and self . warn_on_shadowed_var : \n        if self . current_ns . find ( s ) is not None : \n            logger . warning ( f\"name '{s}' shadows def'ed Var from outer scope\" ) \n    if s . meta is not None and s . meta . entry ( SYM_NO_WARN_WHEN_UNUSED_META_KEY , None ) : \n        warn_if_unused = 0 \n    st . new_symbol ( s , binding , warn_if_unused = warn_if_unused ) "}
{"8880": "\ndef lrepr ( o : Any , human_readable : bool = 0 , print_dup : bool = PRINT_DUP , print_length : PrintCountSetting = PRINT_LENGTH , print_level : PrintCountSetting = PRINT_LEVEL , print_meta : bool = PRINT_META , print_readably : bool = PRINT_READABLY , ) -> str : \n    if isinstance ( o , LispObject ) : \n        return o . _lrepr ( human_readable = human_readable , print_dup = print_dup , print_length = print_length , print_level = print_level , print_meta = print_meta , print_readably = print_readably , ) \n    else : \n        return _lrepr_fallback ( o , human_readable = human_readable , print_dup = print_dup , print_length = print_length , print_level = print_level , print_meta = print_meta , print_readably = print_readably , ) "}
{"8881": "\ndef _lrepr_fallback ( o : Any , human_readable : bool = 0 , print_dup : bool = PRINT_DUP , print_length : PrintCountSetting = PRINT_LENGTH , print_level : PrintCountSetting = PRINT_LEVEL , print_meta : bool = PRINT_META , print_readably : bool = PRINT_READABLY , ) -> str : \n    kwargs = { \"human_readable\" : human_readable , \"print_dup\" : print_dup , \"print_length\" : print_length , \"print_level\" : print_level , \"print_meta\" : print_meta , \"print_readably\" : print_readably , } \n    if isinstance ( o , bool ) : \n        return _lrepr_bool ( o ) \n    elif o is None : \n        return _lrepr_nil ( o ) \n    elif isinstance ( o , str ) : \n        return _lrepr_str ( o , human_readable = human_readable , print_readably = print_readably ) \n    elif isinstance ( o , dict ) : \n        return _lrepr_py_dict ( o , ** kwargs ) \n    elif isinstance ( o , list ) : \n        return _lrepr_py_list ( o , ** kwargs ) \n    elif isinstance ( o , set ) : \n        return _lrepr_py_set ( o , ** kwargs ) \n    elif isinstance ( o , tuple ) : \n        return _lrepr_py_tuple ( o , ** kwargs ) \n    elif isinstance ( o , complex ) : \n        return _lrepr_complex ( o ) \n    elif isinstance ( o , datetime . datetime ) : \n        return _lrepr_datetime ( o ) \n    elif isinstance ( o , Decimal ) : \n        return _lrepr_decimal ( o , print_dup = print_dup ) \n    elif isinstance ( o , Fraction ) : \n        return _lrepr_fraction ( o ) \n    elif isinstance ( o , Pattern ) : \n        return _lrepr_pattern ( o ) \n    elif isinstance ( o , uuid . UUID ) : \n        return _lrepr_uuid ( o ) \n    else : \n        return repr ( o ) "}
{"8888": "\ndef munge ( s : str , allow_builtins : bool = 0 ) -> str : \n    new_str = [ ] \n    for c in s : \n        new_str . append ( _MUNGE_REPLACEMENTS . get ( c , c ) ) \n    new_s = \"\" . join ( new_str ) \n    if keyword . iskeyword ( new_s ) : \n        return f\"{new_s}_\" \n    if not allow_builtins and new_s in builtins . __dict__ : \n        return f\"{new_s}_\" \n    return new_s "}
{"8895": "\ndef _read_namespaced ( ctx : ReaderContext , allowed_suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : \n    ns : List [ str ] = [ ] \n    name : List [ str ] = [ ] \n    reader = ctx . reader \n    has_ns = 0 \n    while 1 : \n        token = reader . peek ( ) \n        if token == \"/\" : \n            reader . next_token ( ) \n            if has_ns : \n                raise SyntaxError ( \"Found '/'; expected word character\" ) \n            elif len ( name ) == 0 : \n                name . append ( \"/\" ) \n            else : \n                if \"/\" in name : \n                    raise SyntaxError ( \"Found '/' after '/'\" ) \n                has_ns = 1 \n                ns = name \n                name = [ ] \n        elif ns_name_chars . match ( token ) : \n            reader . next_token ( ) \n            name . append ( token ) \n        elif allowed_suffix is not None and token == allowed_suffix : \n            reader . next_token ( ) \n            name . append ( token ) \n        else : \n            break \n    ns_str = None if not has_ns else \"\" . join ( ns ) \n    name_str = \"\" . join ( name ) \n    if ns_str is None : \n        if \"/\" in name_str and name_str != \"/\" : \n            raise SyntaxError ( \"'/' character disallowed in names\" ) \n    assert ns_str is None or len ( ns_str ) > 0 \n    return ns_str , name_str "}
{"8896": "\ndef _read_coll ( ctx : ReaderContext , f : Callable [ [ Collection [ Any ] ] , Union [ llist . List , lset . Set , vector . Vector ] ] , end_token : str , coll_name : str , ) : \n    coll : List = [ ] \n    reader = ctx . reader \n    while 1 : \n        token = reader . peek ( ) \n        if token == \"\" : \n            raise SyntaxError ( f\"Unexpected EOF in {coll_name}\" ) \n        if whitespace_chars . match ( token ) : \n            reader . advance ( ) \n            continue \n        if token == end_token : \n            reader . next_token ( ) \n            return f ( coll ) \n        elem = _read_next ( ctx ) \n        if elem is COMMENT : \n            continue \n        coll . append ( elem ) "}
{"8900": "\ndef _read_map ( ctx : ReaderContext ) -> lmap . Map : \n    reader = ctx . reader \n    start = reader . advance ( ) \n    assert start == \"{\" \n    d : MutableMapping [ Any , Any ] = { } \n    while 1 : \n        if reader . peek ( ) == \"}\" : \n            reader . next_token ( ) \n            break \n        k = _read_next ( ctx ) \n        if k is COMMENT : \n            continue \n        while 1 : \n            if reader . peek ( ) == \"}\" : \n                raise SyntaxError ( \"Unexpected token '}'; expected map value\" ) \n            v = _read_next ( ctx ) \n            if v is COMMENT : \n                continue \n            if k in d : \n                raise SyntaxError ( f\"Duplicate key '{k}' in map literal\" ) \n            break \n        d [ k ] = v \n    return lmap . map ( d ) "}
{"8901": "\ndef _read_str ( ctx : ReaderContext , allow_arbitrary_escapes : bool = 0 ) -> str : \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    while 1 : \n        token = reader . next_token ( ) \n        if token == \"\" : \n            raise SyntaxError ( \"Unexpected EOF in string\" ) \n        if token == \"\\\\\" : \n            token = reader . next_token ( ) \n            escape_char = _STR_ESCAPE_CHARS . get ( token , None ) \n            if escape_char : \n                s . append ( escape_char ) \n                continue \n            if allow_arbitrary_escapes : \n                s . append ( \"\\\\\" ) \n            else : \n                raise SyntaxError ( \"Unknown escape sequence: \\\\{token}\" ) \n        if token == '\"' : \n            reader . next_token ( ) \n            return \"\" . join ( s ) \n        s . append ( token ) "}
{"8902": "\ndef _read_sym ( ctx : ReaderContext ) -> MaybeSymbol : \n    ns , name = _read_namespaced ( ctx , allowed_suffix = \"#\" ) \n    if not ctx . is_syntax_quoted and name . endswith ( \"#\" ) : \n        raise SyntaxError ( \"Gensym may not appear outside syntax quote\" ) \n    if ns is not None : \n        if any ( map ( lambda s : len ( s ) == 0 , ns . split ( \".\" ) ) ) : \n            raise SyntaxError ( \"All '.' separated segments of a namespace \" \"must contain at least one character.\" ) \n    if name . startswith ( \".\" ) and ns is not None : \n        raise SyntaxError ( \"Symbols starting with '.' may not have a namespace\" ) \n    if ns is None : \n        if name == \"nil\" : \n            return None \n        elif name == \"true\" : \n            return 1 \n        elif name == \"false\" : \n            return 0 \n    if ctx . is_syntax_quoted and not name . endswith ( \"#\" ) : \n        return ctx . resolve ( symbol . symbol ( name , ns ) ) \n    return symbol . symbol ( name , ns = ns ) "}
{"8904": "\ndef _read_meta ( ctx : ReaderContext ) -> IMeta : \n    start = ctx . reader . advance ( ) \n    assert start == \"^\" \n    meta = _read_next_consuming_comment ( ctx ) \n    meta_map : Optional [ lmap . Map [ LispForm , LispForm ] ] = None \n    if isinstance ( meta , symbol . Symbol ) : \n        meta_map = lmap . map ( { keyword . keyword ( \"tag\" ) : meta } ) \n    elif isinstance ( meta , keyword . Keyword ) : \n        meta_map = lmap . map ( { meta : 1 } ) \n    elif isinstance ( meta , lmap . Map ) : \n        meta_map = meta \n    else : \n        raise SyntaxError ( f\"Expected symbol, keyword, or map for metadata, not {type(meta)}\" ) \n    obj_with_meta = _read_next_consuming_comment ( ctx ) \n    try : \n        return obj_with_meta . with_meta ( meta_map ) \n    except AttributeError : \n        raise SyntaxError ( f\"Can not attach metadata to object of type {type(obj_with_meta)}\" ) "}
{"8912": "\ndef _read_character ( ctx : ReaderContext ) -> str : \n    start = ctx . reader . advance ( ) \n    assert start == \"\\\\\" \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    token = reader . peek ( ) \n    while 1 : \n        if token == \"\" or whitespace_chars . match ( token ) : \n            break \n        if not alphanumeric_chars . match ( token ) : \n            break \n        s . append ( token ) \n        token = reader . next_token ( ) \n    char = \"\" . join ( s ) \n    special = _SPECIAL_CHARS . get ( char , None ) \n    if special is not None : \n        return special \n    match = unicode_char . match ( char ) \n    if match is not None : \n        try : \n            return chr ( int ( f\"0x{match.group(1)}\" , 16 ) ) \n        except ( ValueError , OverflowError ) : \n            raise SyntaxError ( f\"Unsupported character \\\\u{char}\" ) from None \n    if len ( char ) > 1 : \n        raise SyntaxError ( f\"Unsupported character \\\\{char}\" ) \n    return char "}
{"8913": "\ndef _read_regex ( ctx : ReaderContext ) -> Pattern : \n    s = _read_str ( ctx , allow_arbitrary_escapes = 1 ) \n    try : \n        return langutil . regex_from_str ( s ) \n    except re . error : \n        raise SyntaxError ( f\"Unrecognized regex pattern syntax: {s}\" ) "}
{"8915": "\ndef _read_next_consuming_comment ( ctx : ReaderContext ) -> ReaderForm : \n    while 1 : \n        v = _read_next ( ctx ) \n        if v is ctx . eof : \n            return ctx . eof \n        if v is COMMENT or isinstance ( v , Comment ) : \n            continue \n        return v "}
{"8917": "\ndef read ( stream , resolver : Resolver = None , data_readers : DataReaders = None , eof : Any = EOF , is_eof_error : bool = 0 , ) -> Iterable [ ReaderForm ] : \n    reader = StreamReader ( stream ) \n    ctx = ReaderContext ( reader , resolver = resolver , data_readers = data_readers , eof = eof ) \n    while 1 : \n        expr = _read_next ( ctx ) \n        if expr is ctx . eof : \n            if is_eof_error : \n                raise EOFError \n            return \n        if expr is COMMENT or isinstance ( expr , Comment ) : \n            continue \n        yield expr "}
{"8918": "\ndef read_str ( s : str , resolver : Resolver = None , data_readers : DataReaders = None , eof : Any = None , is_eof_error : bool = 0 , ) -> Iterable [ ReaderForm ] : \n    with io . StringIO ( s ) as buf : \n        yield from read ( buf , resolver = resolver , data_readers = data_readers , eof = eof , is_eof_error = is_eof_error , ) "}
{"8919": "\ndef read_file ( filename : str , resolver : Resolver = None , data_readers : DataReaders = None , eof : Any = None , is_eof_error : bool = 0 , ) -> Iterable [ ReaderForm ] : \n    with open ( filename ) as f : \n        yield from read ( f , resolver = resolver , data_readers = data_readers , eof = eof , is_eof_error = is_eof_error , ) "}
{"8939": "\ndef _ast_with_loc ( py_ast : GeneratedPyAST , env : NodeEnv , include_dependencies : bool = 0 ) -> GeneratedPyAST : \n    if env . line is not None : \n        py_ast . node . lineno = env . line \n        if include_dependencies : \n            for dep in py_ast . dependencies : \n                dep . lineno = env . line \n    if env . col is not None : \n        py_ast . node . col_offset = env . col \n        if include_dependencies : \n            for dep in py_ast . dependencies : \n                dep . col_offset = env . col \n    return py_ast "}
{"8941": "\ndef _with_ast_loc_deps ( f ) : \n    \n    @ wraps ( f ) \n    def with_lineno_and_col ( ctx : GeneratorContext , node : Node , * args , ** kwargs ) -> GeneratedPyAST : \n        py_ast = f ( ctx , node , * args , ** kwargs ) \n        return _ast_with_loc ( py_ast , node . env , include_dependencies = 1 ) \n    return with_lineno_and_col "}
{"8942": "\ndef _is_dynamic ( v : Var ) -> bool : \n    return ( Maybe ( v . meta ) . map ( lambda m : m . get ( SYM_DYNAMIC_META_KEY , None ) ) . or_else_get ( 0 ) ) "}
{"8943": "\ndef _is_redefable ( v : Var ) -> bool : \n    return ( Maybe ( v . meta ) . map ( lambda m : m . get ( SYM_REDEF_META_KEY , None ) ) . or_else_get ( 0 ) ) "}
{"8946": "\ndef __should_warn_on_redef ( ctx : GeneratorContext , defsym : sym . Symbol , safe_name : str , def_meta : lmap . Map ) -> bool : \n    no_warn_on_redef = def_meta . entry ( SYM_NO_WARN_ON_REDEF_META_KEY , 0 ) \n    if no_warn_on_redef : \n        return 0 \n    elif safe_name in ctx . current_ns . module . __dict__ : \n        return 1 \n    elif defsym in ctx . current_ns . interns : \n        var = ctx . current_ns . find ( defsym ) \n        assert var is not None , f\"Var {defsym} cannot be none here\" \n        if var . meta is not None and var . meta . entry ( SYM_REDEF_META_KEY ) : \n            return 0 \n        elif var . is_bound : \n            return 1 \n        else : \n            return 0 \n    else : \n        return 0 "}
{"8953": "\ndef __if_body_to_py_ast ( ctx : GeneratorContext , node : Node , result_name : str ) -> GeneratedPyAST : \n    if node . op == NodeOp . RECUR and ctx . recur_point . type == RecurType . LOOP : \n        assert isinstance ( node , Recur ) \n        return _recur_to_py_ast ( ctx , node ) \n    elif node . op == NodeOp . DO : \n        assert isinstance ( node , Do ) \n        if_body = _synthetic_do_to_py_ast ( ctx , node . assoc ( is_body = 1 ) ) \n        return GeneratedPyAST ( node = ast . Assign ( targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = if_body . node ) , dependencies = list ( map ( statementize , if_body . dependencies ) ) , ) \n    else : \n        py_ast = gen_py_ast ( ctx , node ) \n        return GeneratedPyAST ( node = ast . Assign ( targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = py_ast . node ) , dependencies = py_ast . dependencies , ) "}
{"8954": "\ndef _if_to_py_ast ( ctx : GeneratorContext , node : If ) -> GeneratedPyAST : \n    assert node . op == NodeOp . IF \n    test_ast = gen_py_ast ( ctx , node . test ) \n    result_name = genname ( _IF_RESULT_PREFIX ) \n    then_ast = __if_body_to_py_ast ( ctx , node . then , result_name ) \n    else_ast = __if_body_to_py_ast ( ctx , node . else_ , result_name ) \n    test_name = genname ( _IF_TEST_PREFIX ) \n    test_assign = ast . Assign ( targets = [ ast . Name ( id = test_name , ctx = ast . Store ( ) ) ] , value = test_ast . node ) \n    ifstmt = ast . If ( test = ast . BoolOp ( op = ast . Or ( ) , values = [ ast . Compare ( left = ast . NameConstant ( None ) , ops = [ ast . Is ( ) ] , comparators = [ ast . Name ( id = test_name , ctx = ast . Load ( ) ) ] , ) , ast . Compare ( left = ast . NameConstant ( 0 ) , ops = [ ast . Is ( ) ] , comparators = [ ast . Name ( id = test_name , ctx = ast . Load ( ) ) ] , ) , ] , ) , values = [ ] , body = list ( map ( statementize , chain ( else_ast . dependencies , [ else_ast . node ] ) ) ) , orelse = list ( map ( statementize , chain ( then_ast . dependencies , [ then_ast . node ] ) ) ) , ) \n    return GeneratedPyAST ( node = ast . Name ( id = result_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( test_ast . dependencies , [ test_assign , ifstmt ] ) ) , ) "}
{"8958": "\ndef _recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : \n    assert node . op == NodeOp . RECUR \n    assert ctx . recur_point is not None , \"Must have set a recur point to recur\" \n    handle_recur = _RECUR_TYPE_HANDLER . get ( ctx . recur_point . type ) \n    assert ( handle_recur is not None ) , f\"No recur point handler defined for {ctx.recur_point.type}\" \n    ctx . recur_point . has_recur = 1 \n    return handle_recur ( ctx , node ) "}
{"8959": "\ndef _set_bang_to_py_ast ( ctx : GeneratorContext , node : SetBang ) -> GeneratedPyAST : \n    assert node . op == NodeOp . SET_BANG \n    val_temp_name = genname ( \"set_bang_val\" ) \n    val_ast = gen_py_ast ( ctx , node . val ) \n    target = node . target \n    assert isinstance ( target , ( HostField , Local , VarRef ) ) , f\"invalid set! target type {type(target)}\" \n    if isinstance ( target , HostField ) : \n        target_ast = _interop_prop_to_py_ast ( ctx , target , is_assigning = 1 ) \n    elif isinstance ( target , VarRef ) : \n        target_ast = _var_sym_to_py_ast ( ctx , target , is_assigning = 1 ) \n    elif isinstance ( target , Local ) : \n        target_ast = _local_sym_to_py_ast ( ctx , target , is_assigning = 1 ) \n    else : \n        raise GeneratorException ( f\"invalid set! target type {type(target)}\" , lisp_ast = target ) \n    return GeneratedPyAST ( node = ast . Name ( id = val_temp_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val_ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val_temp_name , ctx = ast . Store ( ) ) ] , value = val_ast . node , ) ] , target_ast . dependencies , [ ast . Assign ( targets = [ target_ast . node ] , value = val_ast . node ) ] , ) ) , ) "}
{"8962": "\ndef _local_sym_to_py_ast ( ctx : GeneratorContext , node : Local , is_assigning : bool = 0 ) -> GeneratedPyAST : \n    assert node . op == NodeOp . LOCAL \n    sym_entry = ctx . symbol_table . find_symbol ( sym . symbol ( node . name ) ) \n    assert sym_entry is not None \n    if node . local == LocalType . FIELD : \n        this_entry = ctx . symbol_table . find_symbol ( ctx . current_this ) \n        assert this_entry is not None , \"Field type local must have this\" \n        return GeneratedPyAST ( node = _load_attr ( f\"{this_entry.munged}.{sym_entry.munged}\" , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , ) ) \n    else : \n        return GeneratedPyAST ( node = ast . Name ( id = sym_entry . munged , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) ) ) "}
{"8964": "\ndef _var_sym_to_py_ast ( ctx : GeneratorContext , node : VarRef , is_assigning : bool = 0 ) -> GeneratedPyAST : \n    assert node . op == NodeOp . VAR \n    var = node . var \n    ns = var . ns \n    ns_name = ns . name \n    ns_module = ns . module \n    safe_ns = munge ( ns_name ) \n    var_name = var . name . name \n    py_var_ctx = ast . Store ( ) if is_assigning else ast . Load ( ) \n    if node . return_var : \n        return GeneratedPyAST ( node = ast . Call ( func = _FIND_VAR_FN_NAME , args = [ ast . Call ( func = _NEW_SYM_FN_NAME , args = [ ast . Str ( var_name ) ] , keywords = [ ast . keyword ( arg = \"ns\" , value = ast . Str ( ns_name ) ) ] , ) ] , keywords = [ ] , ) ) \n    if ctx . use_var_indirection or _is_dynamic ( var ) or _is_redefable ( var ) : \n        return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) \n    safe_name = munge ( var_name ) \n    if safe_name not in ns_module . __dict__ : \n        safe_name = munge ( var_name , allow_builtins = 1 ) \n    if safe_name in ns_module . __dict__ : \n        if ns is ctx . current_ns : \n            return GeneratedPyAST ( node = ast . Name ( id = safe_name , ctx = py_var_ctx ) ) \n        return GeneratedPyAST ( node = _load_attr ( f\"{safe_ns}.{safe_name}\" , ctx = py_var_ctx ) ) \n    if ctx . warn_on_var_indirection : \n        logger . warning ( f\"could not resolve a direct link to Var '{var_name}'\" ) \n    return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) "}
{"8965": "\ndef _interop_prop_to_py_ast ( ctx : GeneratorContext , node : HostField , is_assigning : bool = 0 ) -> GeneratedPyAST : \n    assert node . op == NodeOp . HOST_FIELD \n    target_ast = gen_py_ast ( ctx , node . target ) \n    return GeneratedPyAST ( node = ast . Attribute ( value = target_ast . node , attr = munge ( node . field ) , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , ) , dependencies = target_ast . dependencies , ) "}
{"8981": "\ndef _new_module ( name : str , doc = None ) -> types . ModuleType : \n    mod = types . ModuleType ( name , doc = doc ) \n    mod . __loader__ = None \n    mod . __package__ = None \n    mod . __spec__ = None \n    mod . __basilisp_bootstrapped__ = 0 \n    return mod "}
{"8984": "\ndef nthrest ( coll , i : int ) : \n    while 1 : \n        if coll is None : \n            return None \n        if i == 0 : \n            return coll \n        i -= 1 \n        coll = rest ( coll ) "}
{"8985": "\ndef nthnext ( coll , i : int ) -> Optional [ ISeq ] : \n    while 1 : \n        if coll is None : \n            return None \n        if i == 0 : \n            return to_seq ( coll ) \n        i -= 1 \n        coll = next_ ( coll ) "}
{"8998": "\ndef to_lisp ( o , keywordize_keys : bool = 1 ) : \n    if not isinstance ( o , ( dict , frozenset , list , set , tuple ) ) : \n        return o \n    else : \n        return _to_lisp_backup ( o , keywordize_keys = keywordize_keys ) "}
{"9000": "\ndef lrepr ( o , human_readable : bool = 0 ) -> str : \n    core_ns = Namespace . get ( sym . symbol ( CORE_NS ) ) \n    assert core_ns is not None \n    return lobj . lrepr ( o , human_readable = human_readable , print_dup = core_ns . find ( sym . symbol ( _PRINT_DUP_VAR_NAME ) ) . value , print_length = core_ns . find ( sym . symbol ( _PRINT_LENGTH_VAR_NAME ) ) . value , print_level = core_ns . find ( sym . symbol ( _PRINT_LEVEL_VAR_NAME ) ) . value , print_meta = core_ns . find ( sym . symbol ( _PRINT_META_VAR_NAME ) ) . value , print_readably = core_ns . find ( sym . symbol ( _PRINT_READABLY_VAR_NAME ) ) . value , ) "}
{"9002": "\ndef _trampoline ( f ) : \n    \n    @ functools . wraps ( f ) \n    def trampoline ( * args , ** kwargs ) : \n        while 1 : \n            ret = f ( * args , ** kwargs ) \n            if isinstance ( ret , _TrampolineArgs ) : \n                args = ret . args \n                kwargs = ret . kwargs \n                continue \n            return ret \n    return trampoline "}
{"9005": "\ndef _basilisp_fn ( f ) : \n    assert not hasattr ( f , \"meta\" ) \n    f . _basilisp_fn = 1 \n    f . meta = None \n    f . with_meta = partial ( _fn_with_meta , f ) \n    return f "}
{"9008": "\ndef add_generated_python ( generated_python : str , var_name : str = _GENERATED_PYTHON_VAR_NAME , which_ns : Optional [ str ] = None , ) -> None : \n    if which_ns is None : \n        which_ns = get_current_ns ( ) . name \n    ns_sym = sym . Symbol ( var_name , ns = which_ns ) \n    v = Maybe ( Var . find ( ns_sym ) ) . or_else ( lambda : Var . intern ( sym . symbol ( which_ns ) , sym . symbol ( var_name ) , \"\" , dynamic = 1 , meta = lmap . map ( { _PRIVATE_META_KEY : 1 } ) , ) ) \n    v . value = v . value + generated_python "}
{"9009": "\ndef bootstrap ( ns_var_name : str = NS_VAR_NAME , core_ns_name : str = CORE_NS ) -> None : \n    core_ns_sym = sym . symbol ( core_ns_name ) \n    ns_var_sym = sym . symbol ( ns_var_name , ns = core_ns_name ) \n    __NS = Maybe ( Var . find ( ns_var_sym ) ) . or_else_raise ( lambda : RuntimeException ( f\"Dynamic Var {ns_var_sym} not bound!\" ) ) \n    def in_ns ( s : sym . Symbol ) : \n        ns = Namespace . get_or_create ( s ) \n        __NS . value = ns \n        return ns \n    Var . intern_unbound ( core_ns_sym , sym . symbol ( \"unquote\" ) ) \n    Var . intern_unbound ( core_ns_sym , sym . symbol ( \"unquote-splicing\" ) ) \n    Var . intern ( core_ns_sym , sym . symbol ( \"in-ns\" ) , in_ns , meta = lmap . map ( { _REDEF_META_KEY : 1 } ) ) \n    Var . intern ( core_ns_sym , sym . symbol ( _PRINT_GENERATED_PY_VAR_NAME ) , 0 , dynamic = 1 , meta = lmap . map ( { _PRIVATE_META_KEY : 1 } ) , ) \n    Var . intern ( core_ns_sym , sym . symbol ( _GENERATED_PYTHON_VAR_NAME ) , \"\" , dynamic = 1 , meta = lmap . map ( { _PRIVATE_META_KEY : 1 } ) , ) \n    Var . intern ( core_ns_sym , sym . symbol ( _PRINT_DUP_VAR_NAME ) , lobj . PRINT_DUP , dynamic = 1 ) \n    Var . intern ( core_ns_sym , sym . symbol ( _PRINT_LENGTH_VAR_NAME ) , lobj . PRINT_LENGTH , dynamic = 1 ) \n    Var . intern ( core_ns_sym , sym . symbol ( _PRINT_LEVEL_VAR_NAME ) , lobj . PRINT_LEVEL , dynamic = 1 ) \n    Var . intern ( core_ns_sym , sym . symbol ( _PRINT_META_VAR_NAME ) , lobj . PRINT_META , dynamic = 1 ) \n    Var . intern ( core_ns_sym , sym . symbol ( _PRINT_READABLY_VAR_NAME ) , lobj . PRINT_READABLY , dynamic = 1 , ) "}
{"9010": "\ndef intern ( ns : sym . Symbol , name : sym . Symbol , val , dynamic : bool = 0 , meta = None ) -> \"Var\" : \n    var_ns = Namespace . get_or_create ( ns ) \n    var = var_ns . intern ( name , Var ( var_ns , name , dynamic = dynamic , meta = meta ) ) \n    var . root = val \n    return var "}
{"9011": "\ndef intern_unbound ( ns : sym . Symbol , name : sym . Symbol , dynamic : bool = 0 , meta = None ) -> \"Var\" : \n    var_ns = Namespace . get_or_create ( ns ) \n    return var_ns . intern ( name , Var ( var_ns , name , dynamic = dynamic , meta = meta ) ) "}
{"9017": "\ndef intern ( self , sym : sym . Symbol , var : Var , force : bool = 0 ) -> Var : \n    m : lmap . Map = self . _interns . swap ( Namespace . _intern , sym , var , force = force ) \n    return m . entry ( sym ) "}
{"9018": "\ndef _intern ( m : lmap . Map , sym : sym . Symbol , new_var : Var , force : bool = 0 ) -> lmap . Map : \n    var = m . entry ( sym , None ) \n    if var is None or force : \n        return m . assoc ( sym , new_var ) \n    return m "}
{"9029": "\ndef remove ( cls , name : sym . Symbol ) -> Optional [ \"Namespace\" ] : \n    while 1 : \n        oldval : lmap . Map = cls . _NAMESPACES . deref ( ) \n        ns : Optional [ Namespace ] = oldval . entry ( name , None ) \n        newval = oldval \n        if ns is not None : \n            newval = oldval . dissoc ( name ) \n        if cls . _NAMESPACES . compare_and_set ( oldval , newval ) : \n            return ns "}
{"9031": "\ndef __complete_alias ( self , prefix : str , name_in_ns : Optional [ str ] = None ) -> Iterable [ str ] : \n    candidates = filter ( Namespace . __completion_matcher ( prefix ) , [ ( s , n ) for s , n in self . aliases ] ) \n    if name_in_ns is not None : \n        for _ , candidate_ns in candidates : \n            for match in candidate_ns . __complete_interns ( name_in_ns , include_private_vars = 0 ) : \n                yield f\"{prefix}/{match}\" \n    else : \n        for alias , _ in candidates : \n            yield f\"{alias}/\" "}
{"9033": "\ndef __complete_interns ( self , value : str , include_private_vars : bool = 1 ) -> Iterable [ str ] : \n    if include_private_vars : \n        is_match = Namespace . __completion_matcher ( value ) \n    else : \n        _is_match = Namespace . __completion_matcher ( value ) \n        def is_match ( entry : Tuple [ sym . Symbol , Var ] ) -> bool : \n            return _is_match ( entry ) and not entry [ 1 ] . is_private \n    return map ( lambda entry : f\"{entry[0].name}\" , filter ( is_match , [ ( s , v ) for s , v in self . interns ] ) , ) "}
{"9066": "\ndef flush ( self ) : \n    debug ( 'flushing incomming socket messages' ) \n    try : \n        while 1 : \n            msg = self . socket . recv ( self . buffer_size ) \n            debug ( b'< ' + msg ) \n    except socket . error : \n        pass "}
{"9075": "\ndef _validate_yourls_response ( response , data ) : \n    try : \n        response . raise_for_status ( ) \n    except HTTPError as http_exc : \n        http_error_info = sys . exc_info ( ) \n        reraise = 0 \n        try : \n            jsondata = response . json ( ) \n        except ValueError : \n            reraise = 1 \n        else : \n            logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) \n            _handle_api_error_with_json ( http_exc , jsondata , response ) \n        if reraise : \n            six . reraise ( * http_error_info ) \n    else : \n        jsondata = response . json ( ) \n        logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) \n        if { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) : \n            status = jsondata [ 'status' ] \n            code = jsondata [ 'code' ] \n            message = jsondata [ 'message' ] \n            if status == 'fail' : \n                if code == 'error:keyword' : \n                    raise YOURLSKeywordExistsError ( message , keyword = data [ 'keyword' ] ) \n                elif code == 'error:url' : \n                    url = _json_to_shortened_url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) \n                    raise YOURLSURLExistsError ( message , url = url ) \n                else : \n                    raise YOURLSAPIError ( message ) \n            else : \n                return jsondata \n        else : \n            return jsondata "}
{"9079": "\ndef _verify_compatibility ( wave_a , wave_b , check_dep_units = 1 ) : \n    exobj = pexdoc . exh . addex ( RuntimeError , \"Waveforms are not compatible\" ) \n    ctuple = ( bool ( wave_a . indep_scale != wave_b . indep_scale ) , bool ( wave_a . dep_scale != wave_b . dep_scale ) , bool ( wave_a . indep_units != wave_b . indep_units ) , ( bool ( wave_a . dep_units != wave_b . dep_units ) if check_dep_units else 0 ) , bool ( wave_a . interp != wave_b . interp ) , ) \n    exobj ( any ( ctuple ) ) "}
{"9082": "\ndef run_trace ( mname , fname , module_prefix , callable_names , no_print , module_exclude = None , callable_exclude = None , debug = 0 , ) : \n    module_exclude = [ ] if module_exclude is None else module_exclude \n    callable_exclude = [ ] if callable_exclude is None else callable_exclude \n    par = trace_pars ( mname ) \n    start_time = datetime . datetime . now ( ) \n    with pexdoc . exdoc . ExDocCxt ( exclude = par . exclude + module_exclude , pickle_fname = par . pickle_fname , in_callables_fname = par . in_callables_fname , out_callables_fname = par . out_callables_fname , _no_print = no_print , ) as exdoc_obj : \n        fname = os . path . realpath ( os . path . join ( os . path . dirname ( __file__ ) , \"..\" , \"..\" , \"tests\" , \"test_{0}.py\" . format ( fname ) , ) ) \n        test_cmd = ( [ \"--color=yes\" ] + ( [ \"-s\" , \"-vv\" ] if debug else [ \"-q\" , \"-q\" , \"-q\" ] ) + [ \"--disable-warnings\" ] + [ \"-x\" ] + ( [ par . noption ] if par . noption else [ ] ) + [ \"-m \" + mname ] + [ fname ] ) \n        with warnings . catch_warnings ( ) : \n            warnings . filterwarnings ( \"ignore\" , category = PytestWarning ) \n            if pytest . main ( test_cmd ) : \n                raise RuntimeError ( \"Tracing did not complete successfully\" ) \n    stop_time = datetime . datetime . now ( ) \n    if not no_print : \n        print ( \"Auto-generation of exceptions documentation time: {0}\" . format ( pmisc . elapsed_time_string ( start_time , stop_time ) ) ) \n        for callable_name in callable_names : \n            callable_name = module_prefix + callable_name \n            print ( \"\\nCallable: {0}\" . format ( callable_name ) ) \n            print ( exdoc_obj . get_sphinx_doc ( callable_name , exclude = callable_exclude ) ) \n            print ( \"\\n\" ) \n    return copy . copy ( exdoc_obj ) "}
{"9089": "\ndef term_echo ( command , nindent = 0 , env = None , fpointer = None , cols = 60 ) : \n    os . environ [ \"COLUMNS\" ] = str ( cols ) \n    command_int = command \n    if env : \n        for var , repl in env . items ( ) : \n            command_int = command_int . replace ( \"${\" + var + \"}\" , repl ) \n    tokens = command_int . split ( \" \" ) \n    if ( platform . system ( ) . lower ( ) == \"windows\" ) and ( tokens [ 0 ] . endswith ( \".py\" ) ) : \n        tokens = [ sys . executable ] + tokens \n    proc = subprocess . Popen ( tokens , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n    stdout = proc . communicate ( ) [ 0 ] \n    if sys . hexversion >= 0x03000000 : \n        stdout = stdout . decode ( \"utf-8\" ) \n    stdout = stdout . split ( \"\\n\" ) \n    indent = nindent * \" \" \n    fpointer ( \"\\n\" , dedent = 0 ) \n    fpointer ( \"{0}.. code-block:: bash\\n\" . format ( indent ) , dedent = 0 ) \n    fpointer ( \"\\n\" , dedent = 0 ) \n    fpointer ( \"{0}    $ {1}\\n\" . format ( indent , command ) , dedent = 0 ) \n    for line in stdout : \n        if line . strip ( ) : \n            fpointer ( indent + \"    \" + line . replace ( \"\\t\" , \"    \" ) + \"\\n\" , dedent = 0 ) \n        else : \n            fpointer ( \"\\n\" , dedent = 0 ) \n    fpointer ( \"\\n\" , dedent = 0 ) "}
{"9098": "\ndef from_str ( cls , human_readable_str , decimal = 0 , bits = 0 ) : \n    divisor = 1000 if decimal else 1024 \n    num = [ ] \n    c = \"\" \n    for c in human_readable_str : \n        if c not in cls . digits : \n            break \n        num . append ( c ) \n    num = \"\" . join ( num ) \n    try : \n        num = int ( num ) \n    except ValueError : \n        num = float ( num ) \n    if bits : \n        num /= 8 \n    return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) ) "}
{"9100": "\ndef trace_module ( no_print = 1 ) : \n    mname = \"wave_core\" \n    fname = \"peng\" \n    module_prefix = \"peng.{0}.Waveform.\" . format ( mname ) \n    callable_names = ( \"__init__\" , ) \n    return docs . support . trace_support . run_trace ( mname , fname , module_prefix , callable_names , no_print ) "}
{"9104": "\ndef ops_to_words ( item ) : \n    unsupp_ops = [ \"~=\" , \"===\" ] \n    supp_ops = [ \">=\" , \">\" , \"==\" , \"<=\" , \"<\" , \"!=\" ] \n    tokens = sorted ( item . split ( \",\" ) , reverse = 1 ) \n    actual_tokens = [ ] \n    for req in tokens : \n        for op in unsupp_ops : \n            if req . startswith ( op ) : \n                raise RuntimeError ( \"Unsupported version specification: {0}\" . format ( op ) ) \n        for op in supp_ops : \n            if req . startswith ( op ) : \n                actual_tokens . append ( op ) \n                break \n        else : \n            raise RuntimeError ( \"Illegal comparison operator: {0}\" . format ( op ) ) \n    if len ( list ( set ( actual_tokens ) ) ) != len ( actual_tokens ) : \n        raise RuntimeError ( \"Multiple comparison operators of the same type\" ) \n    if \"!=\" in actual_tokens : \n        return ( \" and \" . join ( [ op_to_words ( token ) for token in tokens [ : - 1 ] ] ) + \" \" + op_to_words ( tokens [ - 1 ] ) ) \n    return \" and \" . join ( [ op_to_words ( token ) for token in tokens ] ) "}
{"9112": "\ndef _validate_min_max ( wave , indep_min , indep_max ) : \n    imin , imax = 0 , 0 \n    if indep_min is None : \n        indep_min = wave . _indep_vector [ 0 ] \n        imin = 1 \n    if indep_max is None : \n        indep_max = wave . _indep_vector [ - 1 ] \n        imax = 1 \n    if imin and imax : \n        return indep_min , indep_max \n    exminmax = pexdoc . exh . addex ( RuntimeError , \"Incongruent `indep_min` and `indep_max` arguments\" ) \n    exmin = pexdoc . exh . addai ( \"indep_min\" ) \n    exmax = pexdoc . exh . addai ( \"indep_max\" ) \n    exminmax ( bool ( indep_min >= indep_max ) ) \n    exmin ( bool ( ( indep_min < wave . _indep_vector [ 0 ] ) and ( not np . isclose ( indep_min , wave . _indep_vector [ 0 ] , FP_RTOL , FP_ATOL ) ) ) ) \n    exmax ( bool ( ( indep_max > wave . _indep_vector [ - 1 ] ) and ( not np . isclose ( indep_max , wave . _indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) ) ) \n    return indep_min , indep_max "}
{"9122": "\ndef fftp ( wave , npoints = None , indep_min = None , indep_max = None , unwrap = 1 , rad = 1 ) : \n    return phase ( fft ( wave , npoints , indep_min , indep_max ) , unwrap = unwrap , rad = rad ) "}
{"9127": "\ndef ifftp ( wave , npoints = None , indep_min = None , indep_max = None , unwrap = 1 , rad = 1 ) : \n    return phase ( ifft ( wave , npoints , indep_min , indep_max ) , unwrap = unwrap , rad = rad ) "}
{"9130": "\ndef group_delay ( wave ) : \n    ret = - derivative ( phase ( wave , unwrap = 1 ) / ( 2 * math . pi ) ) \n    ret . dep_name = \"group_delay({0})\" . format ( wave . dep_name ) \n    ret . dep_units = \"sec\" \n    return ret "}
{"9136": "\ndef phase ( wave , unwrap = 1 , rad = 1 ) : \n    ret = copy . copy ( wave ) \n    ret . dep_units = \"rad\" if rad else \"deg\" \n    ret . dep_name = \"phase({0})\" . format ( ret . dep_name ) \n    ret . _dep_vector = ( np . unwrap ( np . angle ( ret . _dep_vector ) ) if unwrap else np . angle ( ret . _dep_vector ) ) \n    if not rad : \n        ret . _dep_vector = np . rad2deg ( ret . _dep_vector ) \n    return ret "}
{"9139": "\ndef subwave ( wave , dep_name = None , indep_min = None , indep_max = None , indep_step = None ) : \n    ret = copy . copy ( wave ) \n    if dep_name is not None : \n        ret . dep_name = dep_name \n    _bound_waveform ( ret , indep_min , indep_max ) \n    pexdoc . addai ( \"indep_step\" , bool ( ( indep_step is not None ) and ( indep_step <= 0 ) ) ) \n    exmsg = \"Argument `indep_step` is greater than independent vector range\" \n    cond = bool ( ( indep_step is not None ) and ( indep_step > ret . _indep_vector [ - 1 ] - ret . _indep_vector [ 0 ] ) ) \n    pexdoc . addex ( RuntimeError , exmsg , cond ) \n    if indep_step : \n        indep_vector = _barange ( indep_min , indep_max , indep_step ) \n        dep_vector = _interp_dep_vector ( ret , indep_vector ) \n        ret . _set_indep_vector ( indep_vector , check = 0 ) \n        ret . _set_dep_vector ( dep_vector , check = 0 ) \n    return ret "}
{"9144": "\ndef find ( self , path , all = 0 ) : \n    bits = path . split ( '/' ) \n    dirs_to_serve = [ 'jspm_packages' , settings . SYSTEMJS_OUTPUT_DIR ] \n    if not bits or bits [ 0 ] not in dirs_to_serve : \n        return [ ] \n    return super ( SystemFinder , self ) . find ( path , all = all ) "}
{"9145": "\ndef get_short_desc ( long_desc ) : \n    found = 0 \n    olines = [ ] \n    for line in [ item . rstrip ( ) for item in long_desc . split ( \"\\n\" ) ] : \n        if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : \n            olines . append ( line ) \n        elif found and olines and ( not line ) : \n            return ( \" \" . join ( olines ) . split ( \".\" ) [ 0 ] ) . strip ( ) \n        found = line == \".. [[[end]]]\" if not found else found \n    return \"\" "}
{"9152": "\ndef _split_every ( text , sep , count , lstrip = 0 , rstrip = 0 ) : \n    ltr = \"_rl \" [ 2 * lstrip + rstrip ] . strip ( ) \n    func = lambda x : getattr ( x , ltr + \"strip\" ) ( ) if ltr != \"_\" else x \n    items = text . split ( sep ) \n    groups = zip_longest ( * [ iter ( items ) ] * count , fillvalue = \"\" ) \n    joints = ( sep . join ( group ) . rstrip ( sep ) for group in groups ) \n    return tuple ( func ( joint ) for joint in joints ) "}
{"9155": "\ndef peng ( number , frac_length , rjust = 1 ) : \n    if number == 0 : \n        number = \"0.{zrs}\" . format ( zrs = \"0\" * frac_length ) if frac_length else \"0\" \n        return \"{0} \" . format ( number . rjust ( 5 + frac_length ) ) if rjust else number \n    sign = + 1 if number >= 0 else - 1 \n    ssign = \"-\" if sign == - 1 else \"\" \n    anumber = abs ( number ) \n    if anumber < 1e-24 : \n        anumber = 1e-24 \n        number = sign * 1e-24 \n    exp = 3.0 * math . floor ( math . floor ( math . log10 ( anumber ) ) / 3.0 ) \n    mant = number / 10 ** exp \n    smant = str ( mant ) \n    ppos = smant . find ( \".\" ) \n    if len ( smant ) - ppos - 1 > frac_length : \n        mant += sign * 5 * 10 ** ( - frac_length - 1 ) \n        if abs ( mant ) >= 1000 : \n            exp += 3 \n            mant = mant / 1e3 \n        smant = str ( mant ) \n        ppos = smant . find ( \".\" ) \n    bfrac_length = bool ( frac_length ) \n    flength = ppos - ( not bfrac_length ) + frac_length + 1 \n    new_mant = smant [ : flength ] . ljust ( flength , \"0\" ) \n    if exp > 24 : \n        new_mant , exp = ( \"{sign}999.{frac}\" . format ( sign = ssign , frac = \"9\" * frac_length ) , 24 , ) \n    new_mant = new_mant . rjust ( rjust * ( 4 + bfrac_length + frac_length ) ) \n    num = \"{mant}{suffix}\" . format ( mant = new_mant , suffix = _POWER_TO_SUFFIX_DICT [ exp ] if exp else \" \" * bool ( rjust ) ) \n    return num "}
{"9160": "\ndef peng_suffix_math ( suffix , offset ) : \n    eobj = pexdoc . exh . addex ( ValueError , \"Argument `offset` is not valid\" ) \n    try : \n        return _POWER_TO_SUFFIX_DICT [ _SUFFIX_TO_POWER_DICT [ suffix ] + 3 * offset ] \n    except KeyError : \n        eobj ( 1 ) "}
{"9162": "\ndef to_scientific_string ( number , frac_length = None , exp_length = None , sign_always = 0 ) : \n    try : \n        number = - 1e20 if np . isneginf ( number ) else number \n    except : \n        pass \n    try : \n        number = + 1e20 if np . isposinf ( number ) else number \n    except : \n        pass \n    exp_length = 0 if not exp_length else exp_length \n    mant , exp = to_scientific_tuple ( number ) \n    fmant = float ( mant ) \n    if ( not frac_length ) or ( fmant == int ( fmant ) ) : \n        return \"{sign}{mant}{period}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( fmant >= 0 ) else \"\" , mant = mant , period = \".\" if frac_length else \"\" , zeros = \"0\" * frac_length if frac_length else \"\" , exp_sign = \"-\" if exp < 0 else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) \n    rounded_mant = round ( fmant , frac_length ) \n    if abs ( rounded_mant ) == 10 : \n        rounded_mant = fmant = - 1.0 if number < 0 else 1.0 \n        frac_length = 1 \n        exp = exp + 1 \n    zeros = 2 + ( 1 if ( fmant < 0 ) else 0 ) + frac_length - len ( str ( rounded_mant ) ) \n    return \"{sign}{mant}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( fmant >= 0 ) else \"\" , mant = rounded_mant , zeros = \"0\" * zeros , exp_sign = \"-\" if exp < 0 else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) "}
{"9165": "\ndef needs_ext ( self ) : \n    if settings . SYSTEMJS_DEFAULT_JS_EXTENSIONS : \n        name , ext = posixpath . splitext ( self . app ) \n        if not ext : \n            return 1 \n    return 0 "}
{"9166": "\ndef bundle ( self ) : \n    outfile , rel_path = self . get_paths ( ) \n    options = self . opts \n    if self . system . _has_jspm_log ( ) : \n        self . command += ' --log {log}' \n        options . setdefault ( 'log' , 'err' ) \n    if options . get ( 'minify' ) : \n        self . command += ' --minify' \n    if options . get ( 'skip_source_maps' ) : \n        self . command += ' --skip-source-maps' \n    try : \n        cmd = self . command . format ( app = self . app , outfile = outfile , ** options ) \n        proc = subprocess . Popen ( cmd , shell = 1 , cwd = self . system . cwd , stdout = self . stdout , stdin = self . stdin , stderr = self . stderr ) \n        result , err = proc . communicate ( ) \n        if err and self . system . _has_jspm_log ( ) : \n            fmt = 'Could not bundle \\'%s\\': \\n%s' \n            logger . warn ( fmt , self . app , err ) \n            raise BundleError ( fmt % ( self . app , err ) ) \n        if result . strip ( ) : \n            logger . info ( result ) \n    except ( IOError , OSError ) as e : \n        if isinstance ( e , BundleError ) : \n            raise \n        raise BundleError ( 'Unable to apply %s (%r): %s' % ( self . __class__ . __name__ , cmd , e ) ) \n    else : \n        if not options . get ( 'sfx' ) : \n            sourcemap = find_sourcemap_comment ( outfile ) \n            with open ( outfile , 'a' ) as of : \n                of . write ( \"\\nSystem.import('{app}{ext}');\\n{sourcemap}\" . format ( app = self . app , ext = '.js' if self . needs_ext ( ) else '' , sourcemap = sourcemap if sourcemap else '' , ) ) \n    return rel_path "}
{"9167": "\ndef trace ( self , app ) : \n    if app not in self . _trace_cache : \n        process = subprocess . Popen ( \"trace-deps.js {}\" . format ( app ) , shell = 1 , stdout = subprocess . PIPE , stderr = subprocess . PIPE , env = self . env , universal_newlines = 1 , cwd = self . _package_json_dir ) \n        out , err = process . communicate ( ) \n        if err : \n            raise TraceError ( err ) \n        self . _trace_cache [ app ] = json . loads ( out ) \n    return self . _trace_cache [ app ] "}
{"9168": "\ndef hashes_match ( self , dep_tree ) : \n    hashes = self . get_hashes ( ) \n    for module , info in dep_tree . items ( ) : \n        md5 = self . get_hash ( info [ 'path' ] ) \n        if md5 != hashes [ info [ 'path' ] ] : \n            return 0 \n    return 1 "}
{"9174": "\ndef _check_initialize_context ( self ) : \n    path = \".\" . join ( [ annotate . context_name ( x ) for x in self . contexts ] ) \n    old_interactive = type_system . interactive \n    type_system . interactive = 0 \n    for key , cmds in self . init_commands . items ( ) : \n        if path . endswith ( key ) : \n            for cmd in cmds : \n                line = self . _split_line ( cmd ) \n                self . invoke ( line ) \n    type_system . interactive = old_interactive "}
{"9177": "\ndef list_dir ( self , context ) : \n    doc = inspect . getdoc ( context ) \n    listing = \"\" \n    listing += \"\\n\" \n    listing += annotate . context_name ( context ) + \"\\n\" \n    if doc is not None : \n        doc = inspect . cleandoc ( doc ) \n        listing += doc + \"\\n\" \n    listing += \"\\nDefined Functions:\\n\" \n    is_dict = 0 \n    if isinstance ( context , dict ) : \n        funs = context . keys ( ) \n        is_dict = 1 \n    else : \n        funs = utils . find_all ( context ) \n    for fun in sorted ( funs ) : \n        override_name = None \n        if is_dict : \n            override_name = fun \n        fun = self . find_function ( context , fun ) \n        if isinstance ( fun , dict ) : \n            if is_dict : \n                listing += \" - \" + override_name + '\\n' \n            else : \n                listing += \" - \" + fun . metadata . name + '\\n' \n        else : \n            listing += \" - \" + fun . metadata . signature ( name = override_name ) + '\\n' \n        if annotate . short_description ( fun ) != \"\" : \n            listing += \"   \" + annotate . short_description ( fun ) + '\\n' \n    listing += \"\\nBuiltin Functions\\n\" \n    for bif in sorted ( self . builtins . keys ( ) ) : \n        listing += ' - ' + bif + '\\n' \n    listing += '\\n' \n    return listing "}
{"9178": "\ndef _is_flag ( cls , arg ) : \n    if arg == '--' : \n        return 0 \n    if not arg . startswith ( '-' ) : \n        return 0 \n    if arg . startswith ( '--' ) : \n        first_char = arg [ 2 ] \n    else : \n        first_char = arg [ 1 ] \n    if not first_char . isalpha ( ) : \n        return 0 \n    return 1 "}
{"9180": "\ndef _extract_arg_value ( cls , arg_name , arg_type , remaining ) : \n    next_arg = None \n    should_consume = 0 \n    if len ( remaining ) > 0 : \n        next_arg = remaining [ 0 ] \n        should_consume = 1 \n        if next_arg == '--' : \n            next_arg = None \n    if arg_type == \"bool\" : \n        if next_arg is None or next_arg . startswith ( '-' ) : \n            next_arg = 1 \n            should_consume = 0 \n    else : \n        if next_arg is None : \n            raise ArgumentError ( \"Could not find value for keyword argument\" , argument = arg_name ) \n    if should_consume : \n        remaining . pop ( 0 ) \n    return next_arg "}
{"9181": "\ndef invoke_one ( self , line ) : \n    funname = line . pop ( 0 ) \n    context = self . contexts [ - 1 ] \n    func = self . find_function ( context , funname ) \n    if isinstance ( func , dict ) : \n        self . contexts . append ( func ) \n        self . _check_initialize_context ( ) \n        return None , line , 0 \n    if func . takes_cmdline is 1 : \n        val = func ( line ) \n        line = [ ] \n    else : \n        posargs , kwargs , line = self . process_arguments ( func , line ) \n        if inspect . isclass ( func ) and not func . metadata . spec_filled ( posargs , kwargs ) : \n            raise ValidationError ( \"Not enough parameters specified to call function\" , function = func . metadata . name , signature = func . metadata . signature ( ) ) \n        val = func ( * posargs , ** kwargs ) \n    finished = 1 \n    if func . finalizer is 1 : \n        self . contexts . pop ( ) \n    elif val is not None : \n        if func . metadata . returns_data ( ) : \n            val = func . metadata . format_returnvalue ( val ) \n        else : \n            self . contexts . append ( val ) \n            self . _check_initialize_context ( ) \n            finished = 0 \n            val = None \n    return val , line , finished "}
{"9182": "\ndef invoke ( self , line ) : \n    finished = 1 \n    while len ( line ) > 0 : \n        val , line , finished = self . invoke_one ( line ) \n        if val is not None : \n            iprint ( val ) \n    return finished "}
{"9183": "\ndef invoke_string ( self , line ) : \n    line = str ( line ) \n    if len ( line ) == 0 : \n        return 1 \n    if line [ 0 ] == u'#' : \n        return 1 \n    args = self . _split_line ( line ) \n    return self . invoke ( args ) "}
{"9184": "\ndef parse_param ( param , include_desc = 0 ) : \n    param_def , _colon , desc = param . partition ( ':' ) \n    if not include_desc : \n        desc = None \n    else : \n        desc = desc . lstrip ( ) \n    if _colon == \"\" : \n        raise ValidationError ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) \n    param_name , _space , param_type = param_def . partition ( ' ' ) \n    if len ( param_type ) < 2 or param_type [ 0 ] != '(' or param_type [ - 1 ] != ')' : \n        raise ValidationError ( \"Invalid parameter type string not enclosed in ( ) characters\" , param_string = param_def , type_string = param_type ) \n    param_type = param_type [ 1 : - 1 ] \n    return param_name , ParameterInfo ( param_type , [ ] , desc ) "}
{"9185": "\ndef parse_return ( return_line , include_desc = 0 ) : \n    ret_def , _colon , desc = return_line . partition ( ':' ) \n    if _colon == \"\" : \n        raise ValidationError ( \"Invalid return declaration in docstring, missing colon\" , declaration = ret_def ) \n    if not include_desc : \n        desc = None \n    if 'show-as' in ret_def : \n        ret_type , _showas , show_type = ret_def . partition ( 'show-as' ) \n        ret_type = ret_type . strip ( ) \n        show_type = show_type . strip ( ) \n        if show_type not in ( 'string' , 'context' ) : \n            raise ValidationError ( \"Unkown show-as formatting specifier\" , found = show_type , expected = [ 'string' , 'context' ] ) \n        if show_type == 'string' : \n            return ReturnInfo ( None , str , 1 , desc ) \n        return ReturnInfo ( None , None , 0 , desc ) \n    if 'format-as' in ret_def : \n        ret_type , _showas , formatter = ret_def . partition ( 'format-as' ) \n        ret_type = ret_type . strip ( ) \n        formatter = formatter . strip ( ) \n        return ReturnInfo ( ret_type , formatter , 1 , desc ) \n    return ReturnInfo ( ret_def , None , 1 , desc ) "}
{"9188": "\ndef _join_paragraphs ( cls , lines , use_indent = 0 , leading_blanks = 0 , trailing_blanks = 0 ) : \n    curr_para = [ ] \n    paragraphs = [ ] \n    for line in lines : \n        if use_indent : \n            if line . startswith ( ' ' ) : \n                curr_para . append ( line . lstrip ( ) ) \n                continue \n            elif line == '' : \n                continue \n            else : \n                if len ( curr_para ) > 0 : \n                    paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ line . lstrip ( ) ] \n        else : \n            if len ( line ) != 0 : \n                curr_para . append ( line ) \n            else : \n                paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ ] \n    if len ( curr_para ) > 0 : \n        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n    return paragraphs "}
{"9189": "\ndef wrap_and_format ( self , width = None , include_params = 0 , include_return = 0 , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        elif isinstance ( line , BlankLine ) : \n            out . write ( '\\n' ) \n        elif isinstance ( line , ListItem ) : \n            out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ 0 ] , subsequent_indent = \"   \" , width = width ) ) \n            out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if len ( included_params ) > 0 : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        print ( \"Returns:\" ) \n        print ( \"    \" + self . return_info . type_name ) \n    return out . getvalue ( ) "}
{"9195": "\ndef is_known_type ( self , type_name ) : \n    type_name = str ( type_name ) \n    if type_name in self . known_types : \n        return 1 \n    return 0 "}
{"9196": "\ndef split_type ( self , typename ) : \n    name = self . _canonicalize_type ( typename ) \n    if '(' not in name : \n        return name , 0 , [ ] \n    base , sub = name . split ( '(' ) \n    if len ( sub ) == 0 or sub [ - 1 ] != ')' : \n        raise ArgumentError ( \"syntax error in complex type, no matching ) found\" , passed_type = typename , basetype = base , subtype_string = sub ) \n    sub = sub [ : - 1 ] \n    subs = sub . split ( ',' ) \n    return base , 1 , subs "}
{"9199": "\ndef is_known_format ( self , type , format ) : \n    typeobj = self . get_type ( type ) \n    formatter = \"format_%s\" % str ( format ) \n    if not hasattr ( typeobj , formatter ) : \n        return 0 \n    return 1 "}
{"9204": "\ndef typed_returnvalue ( self , type_name , formatter = None ) : \n    self . return_info = ReturnInfo ( type_name , formatter , 1 , None ) "}
{"9205": "\ndef custom_returnvalue ( self , printer , desc = None ) : \n    self . return_info = ReturnInfo ( None , printer , 1 , desc ) "}
{"9213": "\ndef format ( self , exclude_class = 0 ) : \n    if exclude_class : \n        msg = self . msg \n    else : \n        msg = \"%s: %s\" % ( self . __class__ . __name__ , self . msg ) \n    if len ( self . params ) != 0 : \n        paramstring = \"\\n\" . join ( [ str ( key ) + \": \" + str ( val ) for key , val in self . params . items ( ) ] ) \n        msg += \"\\nAdditional Information:\\n\" + paramstring \n    return msg "}
{"9218": "\ndef context_from_module ( module ) : \n    con = find_all ( module ) \n    if hasattr ( module , \"__doc__\" ) : \n        setattr ( con , \"__doc__\" , module . __doc__ ) \n    name = module . __name__ \n    if hasattr ( module , \"_name_\" ) : \n        name = module . _name_ \n    con = annotated ( con , name ) \n    setattr ( con , 'context' , 1 ) \n    return name , con "}
{"9220": "\ndef param ( name , type_name , * validators , ** kwargs ) : \n    def _param ( func ) : \n        func = annotated ( func ) \n        valids = _parse_validators ( validators ) \n        func . metadata . add_param ( name , type_name , valids , ** kwargs ) \n        if func . decorated : \n            return func \n        func . decorated = 1 \n        return decorate ( func , _check_and_execute ) \n    return _param "}
{"9221": "\ndef returns ( desc = None , printer = None , data = 1 ) : \n    if data is 0 : \n        raise ArgumentError ( \"Specifying non data return type in returns is no longer supported\" ) \n    def _returns ( func ) : \n        annotated ( func ) \n        func . custom_returnvalue ( printer , desc ) \n        return func \n    return _returns "}
{"9223": "\ndef context ( name = None ) : \n    def _context ( cls ) : \n        annotated ( cls , name ) \n        cls . context = 1 \n        return cls \n    return _context "}
{"9224": "\ndef docannotate ( func ) : \n    func = annotated ( func ) \n    func . metadata . load_from_doc = 1 \n    if func . decorated : \n        return func \n    func . decorated = 1 \n    return decorate ( func , _check_and_execute ) "}
{"9225": "\ndef annotated ( func , name = None ) : \n    if hasattr ( func , 'metadata' ) : \n        if name is not None : \n            func . metadata = AnnotatedMetadata ( func , name ) \n        return func \n    func . metadata = AnnotatedMetadata ( func , name ) \n    func . finalizer = 0 \n    func . takes_cmdline = 0 \n    func . decorated = 0 \n    func . context = 0 \n    return func "}
{"9228": "\ndef install ( ) : \n    load ( ) \n    tab = crontab . CronTab ( user = 1 ) \n    for task in registry : \n        tab . new ( task . command , KRONOS_BREADCRUMB ) . setall ( task . schedule ) \n    tab . write ( ) \n    return len ( registry ) "}
{"9230": "\ndef uninstall ( ) : \n    tab = crontab . CronTab ( user = 1 ) \n    count = len ( list ( tab . find_comment ( KRONOS_BREADCRUMB ) ) ) \n    tab . remove_all ( comment = KRONOS_BREADCRUMB ) \n    tab . write ( ) \n    return count "}
{"9233": "\ndef save ( self , projects ) : \n    base_path = os . path . expanduser ( self . path ) \n    if not os . path . isdir ( base_path ) : \n        return \n    logger . debug ( \"Save projects config to %s\" , base_path ) \n    for name , data in list ( projects . items ( ) ) : \n        project_file_path = self . get_project_config_path ( name ) \n        with open ( project_file_path , \"w\" ) as f : \n            yaml . dump ( data , stream = f , default_flow_style = 0 ) \n            logger . debug ( \"Project '%s' config has been writed to '%s'\" , name , project_file_path ) "}
{"9235": "\ndef get_dependent_projects ( self , recursive = 1 ) : \n    projects = { } \n    for name , ref in list ( self . dependencies . items ( ) ) : \n        try : \n            prj = self . vcp . projects [ name ] \n        except KeyError : \n            logger . error ( \"Unknown project '%s' in project '%s' dependencies!\" , name , self . name ) \n            continue \n        projects [ name ] = prj \n        if recursive : \n            projects . update ( prj . get_dependent_projects ( ) ) \n    return projects "}
{"9237": "\ndef __init ( self , project , path , force , init_languages ) : \n    status = { } \n    project . init ( path , status , force , init_languages = init_languages ) \n    failed = [ ] \n    for name , val in list ( status . items ( ) ) : \n        if val is 0 and name not in failed : \n            failed . append ( name ) \n    return failed "}
{"9255": "\ndef handle_input ( self , input ) : \n    dirs = { 'h' : ( - 1 , 0 ) , 'j' : ( 0 , 1 ) , 'k' : ( 0 , - 1 ) , 'l' : ( 1 , 0 ) , 'y' : ( - 1 , - 1 ) , 'u' : ( 1 , - 1 ) , 'n' : ( 1 , 1 ) , 'b' : ( - 1 , 1 ) , } \n    if input in dirs : \n        new_self = ( lens . player + dirs [ input ] ) ( self ) \n        if not new_self . player . inside ( ) : \n            return self , 0 \n        return new_self , 1 \n    elif input == '.' : \n        return self , 1 \n    elif input == 'q' : \n        return self . end_game ( ) , 0 \n    elif input == 't' : \n        self = lens . player . set ( Vector . random ( ) ) ( self ) \n        return self , 1 \n    else : \n        return self , 0 "}
{"9257": "\ndef end_game ( self , message = '' ) : \n    return lens . running . set ( 0 ) ( lens . message . set ( message ) ( self ) ) "}
{"9269": "\ndef get_block ( self , block_hash , verbose = 1 , ** kwargs ) : \n    return self . _call ( JSONRPCMethods . GET_BLOCK . value , params = [ block_hash , int ( verbose ) , ] , ** kwargs ) "}
{"9273": "\ndef get_raw_transaction ( self , tx_hash , verbose = 1 , ** kwargs ) : \n    return self . _call ( JSONRPCMethods . GET_RAW_TRANSACTION . value , params = [ tx_hash , int ( verbose ) , ] , ** kwargs ) "}
{"9282": "\ndef is_hash256 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return 0 \n    return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE ) "}
{"9283": "\ndef is_hash160 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return 0 \n    if not len ( s ) == 40 : \n        return 0 \n    for c in s : \n        if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : \n            return 0 \n    return 1 "}
{"9311": "\ndef auth_get_token ( self , check_scope = 1 ) : \n    res = self . auth_access_data_raw = self . _auth_token_request ( ) \n    return self . _auth_token_process ( res , check_scope = check_scope ) "}
{"9314": "\ndef mkdir ( self , name = None , folder_id = 'me/skydrive' , metadata = dict ( ) ) : \n    metadata = metadata . copy ( ) \n    if name : \n        metadata [ 'name' ] = name \n    return self ( folder_id , data = metadata , method = 'post' , auth_header = 1 ) "}
{"9315": "\ndef comment_add ( self , obj_id , message ) : \n    return self ( self . _api_url_join ( obj_id , 'comments' ) , method = 'post' , data = dict ( message = message ) , auth_header = 1 ) "}
{"9316": "\ndef decode_obj ( obj , force = 0 ) : \n    if isinstance ( obj , unicode ) : \n        return obj \n    elif isinstance ( obj , bytes ) : \n        if force_encoding is not None : \n            return obj . decode ( force_encoding ) \n        if chardet : \n            enc_guess = chardet . detect ( obj ) \n            if enc_guess [ 'confidence' ] > 0.7 : \n                return obj . decode ( enc_guess [ 'encoding' ] ) \n        return obj . decode ( 'utf-8' ) \n    else : \n        return obj if not force else repr ( obj ) "}
{"9326": "\ndef wellcome_tip ( wx_obj ) : \n    msg = ( \"Close the main window to exit & save.\\n\" \"Drag & Drop / Click the controls from the ToolBox to create new ones.\\n\" \"Left click on the created controls to select them.\\n\" \"Double click to edit the default property.\\n\" \"Right click to pop-up the context menu.\\n\" ) \n    stt = STT . SuperToolTip ( msg ) \n    stt . SetHeader ( \"Welcome to gui2py designer!\" ) \n    stt . SetDrawHeaderLine ( 1 ) \n    stt . ApplyStyle ( \"Office 2007 Blue\" ) \n    stt . SetDropShadow ( 1 ) \n    stt . SetHeaderBitmap ( images . designer . GetBitmap ( ) ) \n    stt . SetEndDelay ( 15000 ) \n    tip = CustomToolTipWindow ( wx_obj , stt ) \n    tip . CalculateBestSize ( ) \n    tip . CalculateBestPosition ( wx_obj ) \n    tip . DropShadow ( stt . GetDropShadow ( ) ) \n    if stt . GetUseFade ( ) : \n        show = lambda : tip . StartAlpha ( 1 ) \n    else : \n        show = lambda : tip . Show ( ) \n    wx . CallLater ( 1000 , show ) \n    wx . CallLater ( 30000 , tip . Destroy ) "}
{"9341": "\ndef _set_selection ( self , index , dummy = 0 ) : \n    if index is None : \n        self . wx_obj . SetSelection ( - 1 ) \n        if hasattr ( self . wx_obj , \"SetValue\" ) : \n            self . wx_obj . SetValue ( \"\" ) \n    else : \n        self . wx_obj . SetSelection ( index ) \n    wx_event = ItemContainerControlSelectEvent ( self . _commandtype , index , self . wx_obj ) \n    if hasattr ( self , \"onchange\" ) and self . onchange : \n        event = FormEvent ( name = \"change\" , wx_event = wx_event ) \n        self . onchange ( event ) "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = 0 , context = 0 , max_cols = 80 ) : \n    try : \n        name = getattr ( obj , \"name\" , \"\" ) \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + 1 + indent * 4 + ( 5 if context else 0 ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if k == \"index\" : \n                continue \n            if k == \"parent\" and parent != \"\" : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if len ( line + param ) + 3 > max_cols - padding : \n                param_lines . append ( line ) \n                line = \"\" \n            line += param + \", \" \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9346": "\ndef get ( obj_name , init = 0 ) : \n    wx_parent = None \n    if isinstance ( obj_name , basestring ) : \n        obj_parent = COMPONENTS . get ( obj_name ) \n        if not obj_parent : \n            wx_parent = wx . FindWindowByName ( obj_name ) \n            if wx_parent : \n                obj_parent = getattr ( wx_parent , \"obj\" ) \n            else : \n                for obj in COMPONENTS . values ( ) : \n                    if obj . name == obj_name : \n                        obj_parent = obj \n    else : \n        obj_parent = obj_name \n    return obj_parent or wx_parent "}
{"9357": "\ndef _updateColAttrs ( self , grid ) : \n    col = 0 \n    for column in self . columns : \n        attr = gridlib . GridCellAttr ( ) \n        if 0 : \n            attr . SetReadOnly ( ) \n        if 0 : \n            attr . SetRenderer ( renderer ) \n        grid . SetColSize ( col , column . width ) \n        grid . SetColAttr ( col , attr ) \n        col += 1 "}
{"9367": "\ndef IsEnabled ( self , * args , ** kwargs ) : \n    for i in range ( self . GetMenuItemCount ( ) ) : \n        it = self . FindItemByPosition ( i ) \n        if not it . IsEnabled ( ) : \n            return 0 \n    return 1 "}
{"9369": "\ndef IsEnabled ( self , * args , ** kwargs ) : \n    for i in range ( self . GetMenuCount ( ) ) : \n        if not self . IsEnabledTop ( i ) : \n            return 0 \n    return 1 "}
{"9374": "\ndef get_documenter ( obj , parent ) : \n    from sphinx . ext . autodoc import AutoDirective , DataDocumenter , ModuleDocumenter \n    if inspect . ismodule ( obj ) : \n        return ModuleDocumenter \n    if parent is not None : \n        parent_doc_cls = get_documenter ( parent , None ) \n    else : \n        parent_doc_cls = ModuleDocumenter \n    if hasattr ( parent , '__name__' ) : \n        parent_doc = parent_doc_cls ( FakeDirective ( ) , parent . __name__ ) \n    else : \n        parent_doc = parent_doc_cls ( FakeDirective ( ) , \"\" ) \n    classes = [ cls for cls in AutoDirective . _registry . values ( ) if cls . can_document_member ( obj , '' , 0 , parent_doc ) ] \n    if classes : \n        classes . sort ( key = lambda cls : cls . priority ) \n        return classes [ - 1 ] \n    else : \n        return DataDocumenter "}
{"9378": "\ndef alert ( message , title = \"\" , parent = None , scrolled = 0 , icon = \"exclamation\" ) : \n    if not scrolled : \n        icons = { 'exclamation' : wx . ICON_EXCLAMATION , 'error' : wx . ICON_ERROR , 'question' : wx . ICON_QUESTION , 'info' : wx . ICON_INFORMATION } \n        style = wx . OK | icons [ icon ] \n        result = dialogs . messageDialog ( parent , message , title , style ) \n    else : \n        result = dialogs . scrolledMessageDialog ( parent , message , title ) "}
{"9379": "\ndef prompt ( message = \"\" , title = \"\" , default = \"\" , multiline = 0 , password = None , parent = None ) : \n    if password : \n        style = wx . TE_PASSWORD | wx . OK | wx . CANCEL \n        result = dialogs . textEntryDialog ( parent , message , title , default , style ) \n    elif multiline : \n        style = wx . TE_MULTILINE | wx . OK | wx . CANCEL \n        result = dialogs . textEntryDialog ( parent , message , title , default , style ) \n        result . text = '\\n' . join ( result . text . splitlines ( ) ) \n    else : \n        result = dialogs . textEntryDialog ( parent , message , title , default ) \n    if result . accepted : \n        return result . text "}
{"9384": "\ndef set_has_children ( self , has_children = 1 ) : \n    self . _tree_model . _tree_view . wx_obj . SetItemHasChildren ( self . wx_item , has_children ) "}
{"9386": "\ndef show ( self , value = 1 , modal = None ) : \n    self . wx_obj . Show ( value ) \n    if modal : \n        disabler = wx . WindowDisabler ( self . wx_obj ) \n        eventloop = wx . EventLoop ( ) \n        def on_close_modal ( evt ) : \n            evt . Skip ( ) \n            eventloop . Exit ( ) \n        self . wx_obj . Bind ( wx . EVT_CLOSE , on_close_modal ) \n        eventloop . Run ( ) \n        del disabler "}
{"9389": "\ndef build_window ( res ) : \n    kwargs = dict ( res . items ( ) ) \n    wintype = kwargs . pop ( 'type' ) \n    menubar = kwargs . pop ( 'menubar' , None ) \n    components = kwargs . pop ( 'components' ) \n    panel = kwargs . pop ( 'panel' , { } ) \n    from gui import registry \n    import gui \n    winclass = registry . WINDOWS [ wintype ] \n    win = winclass ( ** kwargs ) \n    if 0 and panel is not None : \n        panel [ 'name' ] = 'panel' \n        p = gui . Panel ( win , ** panel ) \n    else : \n        p = win \n    if components : \n        for comp in components : \n            build_component ( comp , parent = p ) \n    if menubar : \n        mb = gui . MenuBar ( name = \"menu\" , parent = win ) \n        for menu in menubar : \n            build_component ( menu , parent = mb ) \n    return win "}
{"9397": "\ndef activate_item ( self , child , edit_prop = 0 , select = 0 ) : \n    d = self . tree . GetItemData ( child ) \n    if d : \n        o = d . GetData ( ) \n        self . selected_obj = o \n        callback = lambda o = o , ** kwargs : self . update ( o , ** kwargs ) \n        self . propeditor . load_object ( o , callback ) \n        if edit_prop : \n            wx . CallAfter ( self . propeditor . edit ) \n        if select and self . designer : \n            self . designer . select ( o ) \n    else : \n        self . selected_obj = None "}
{"9403": "\ndef assert_current_path ( self , path , ** kwargs ) : \n    query = CurrentPathQuery ( path , ** kwargs ) \n    \n    @ self . document . synchronize \n    def assert_current_path ( ) : \n        if not query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . failure_message ) \n    assert_current_path ( ) \n    return 1 "}
{"9404": "\ndef assert_no_current_path ( self , path , ** kwargs ) : \n    query = CurrentPathQuery ( path , ** kwargs ) \n    \n    @ self . document . synchronize \n    def assert_no_current_path ( ) : \n        if query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n    assert_no_current_path ( ) \n    return 1 "}
{"9405": "\ndef has_current_path ( self , path , ** kwargs ) : \n    try : \n        return self . assert_current_path ( path , ** kwargs ) \n    except ExpectationNotMet : \n        return 0 "}
{"9406": "\ndef has_no_current_path ( self , path , ** kwargs ) : \n    try : \n        return self . assert_no_current_path ( path , ** kwargs ) \n    except ExpectationNotMet : \n        return 0 "}
{"9411": "\ndef matches_filters ( self , node ) : \n    visible = self . visible \n    if self . options [ \"text\" ] : \n        if isregex ( self . options [ \"text\" ] ) : \n            regex = self . options [ \"text\" ] \n        elif self . exact_text is 1 : \n            regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . options [ \"text\" ] ) ) ) \n        else : \n            regex = toregex ( self . options [ \"text\" ] ) \n        text = normalize_text ( node . all_text if visible == \"all\" else node . visible_text ) \n        if not regex . search ( text ) : \n            return 0 \n    if isinstance ( self . exact_text , ( bytes_ , str_ ) ) : \n        regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . exact_text ) ) ) \n        text = normalize_text ( node . all_text if visible == \"all\" else node . visible_text ) \n        if not regex . search ( text ) : \n            return 0 \n    if visible == \"visible\" : \n        if not node . visible : \n            return 0 \n    elif visible == \"hidden\" : \n        if node . visible : \n            return 0 \n    for name , node_filter in iter ( self . _node_filters . items ( ) ) : \n        if name in self . filter_options : \n            if not node_filter . matches ( node , self . filter_options [ name ] ) : \n                return 0 \n        elif node_filter . has_default : \n            if not node_filter . matches ( node , node_filter . default ) : \n                return 0 \n    if self . options [ \"filter\" ] and not self . options [ \"filter\" ] ( node ) : \n        return 0 \n    return 1 "}
{"9421": "\ndef matches ( self , node , value ) : \n    if self . skip ( value ) : \n        return 1 \n    if not self . _valid_value ( value ) : \n        msg = \"Invalid value {value} passed to filter {name} - \" . format ( value = repr ( value ) , name = self . name ) \n        if self . default is not None : \n            warn ( msg + \"defaulting to {}\" . format ( self . default ) ) \n            value = self . default \n        else : \n            warn ( msg + \"skipping\" ) \n            return 1 \n    return self . func ( node , value ) "}
{"9422": "\ndef has_checked_field ( self , locator , ** kwargs ) : \n    kwargs [ \"checked\" ] = 1 \n    return self . has_selector ( \"field\" , locator , ** kwargs ) "}
{"9423": "\ndef has_no_checked_field ( self , locator , ** kwargs ) : \n    kwargs [ \"checked\" ] = 1 \n    return self . has_no_selector ( \"field\" , locator , ** kwargs ) "}
{"9424": "\ndef has_unchecked_field ( self , locator , ** kwargs ) : \n    kwargs [ \"checked\" ] = 0 \n    return self . has_selector ( \"field\" , locator , ** kwargs ) "}
{"9425": "\ndef has_no_unchecked_field ( self , locator , ** kwargs ) : \n    kwargs [ \"checked\" ] = 0 \n    return self . has_no_selector ( \"field\" , locator , ** kwargs ) "}
{"9426": "\ndef assert_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_text ( ) : \n        count = query . resolve_for ( self ) \n        if not ( matches_count ( count , query . options ) and ( count > 0 or expects_none ( query . options ) ) ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return 1 \n    return assert_text ( ) "}
{"9427": "\ndef assert_no_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_text ( ) : \n        count = query . resolve_for ( self ) \n        if matches_count ( count , query . options ) and ( count > 0 or expects_none ( query . options ) ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return 1 \n    return assert_no_text ( ) "}
{"9428": "\ndef assert_title ( self , title , ** kwargs ) : \n    query = TitleQuery ( title , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_title ( ) : \n        if not query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return 1 \n    return assert_title ( ) "}
{"9429": "\ndef assert_no_title ( self , title , ** kwargs ) : \n    query = TitleQuery ( title , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_title ( ) : \n        if query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return 1 \n    return assert_no_title ( ) "}
{"9430": "\ndef has_title ( self , title , ** kwargs ) : \n    try : \n        self . assert_title ( title , ** kwargs ) \n        return 1 \n    except ExpectationNotMet : \n        return 0 "}
{"9431": "\ndef has_no_title ( self , title , ** kwargs ) : \n    try : \n        self . assert_no_title ( title , ** kwargs ) \n        return 1 \n    except ExpectationNotMet : \n        return 0 "}
{"9438": "\ndef synchronize ( self , func = None , wait = None , errors = ( ) ) : \n    def decorator ( func ) : \n        \n        @ wraps ( func ) \n        def outer ( * args , ** kwargs ) : \n            seconds = wait if wait is not None else capybara . default_max_wait_time \n            def inner ( ) : \n                return func ( * args , ** kwargs ) \n            if self . session . synchronized : \n                return inner ( ) \n            else : \n                timer = Timer ( seconds ) \n                self . session . synchronized = 1 \n                try : \n                    while 1 : \n                        try : \n                            return inner ( ) \n                        except Exception as e : \n                            self . session . raise_server_error ( ) \n                            if not self . _should_catch_error ( e , errors ) : \n                                raise \n                            if timer . expired : \n                                raise \n                            sleep ( 0.05 ) \n                            if timer . stalled : \n                                raise FrozenInTime ( \"time appears to be frozen, Capybara does not work with \" \"libraries which freeze time, consider using time \" \"traveling instead\" ) \n                            if capybara . automatic_reload : \n                                self . reload ( ) \n                finally : \n                    self . session . synchronized = 0 \n        return outer \n    if func : \n        return decorator ( func ) \n    else : \n        return decorator "}
{"9441": "\ndef _cache_at_least ( self , size ) : \n    try : \n        while len ( self . _result_cache ) < size : \n            self . _result_cache . append ( next ( self . _result_iter ) ) \n        return 1 \n    except StopIteration : \n        return 0 "}
{"9442": "\ndef expects_none ( options ) : \n    if any ( options . get ( key ) is not None for key in [ \"count\" , \"maximum\" , \"minimum\" , \"between\" ] ) : \n        return matches_count ( 0 , options ) \n    else : \n        return 0 "}
{"9444": "\ndef matches_count ( count , options ) : \n    if options . get ( \"count\" ) is not None : \n        return count == int ( options [ \"count\" ] ) \n    if options . get ( \"maximum\" ) is not None and int ( options [ \"maximum\" ] ) < count : \n        return 0 \n    if options . get ( \"minimum\" ) is not None and int ( options [ \"minimum\" ] ) > count : \n        return 0 \n    if options . get ( \"between\" ) is not None and count not in options [ \"between\" ] : \n        return 0 \n    return 1 "}
{"9447": "\ndef toregex ( text , exact = 0 ) : \n    if isregex ( text ) : \n        return text \n    escaped = re . escape ( normalize_text ( text ) ) \n    if exact : \n        escaped = r\"\\A{}\\Z\" . format ( escaped ) \n    return re . compile ( escaped ) "}
{"9450": "\ndef boot ( self ) : \n    if not self . responsive : \n        type ( self ) . _ports [ self . port_key ] = self . port \n        init_func = capybara . servers [ capybara . server_name ] \n        init_args = ( self . middleware , self . port , self . host ) \n        self . server_thread = Thread ( target = init_func , args = init_args ) \n        self . server_thread . daemon = 1 \n        self . server_thread . start ( ) \n        timer = Timer ( 60 ) \n        while not self . responsive : \n            if timer . expired : \n                raise RuntimeError ( \"WSGI application timed out during boot\" ) \n            self . server_thread . join ( 0.1 ) \n    return self "}
{"9464": "\ndef read_channel ( self ) : \n    channel , message = self . protocol . channel_layer . receive_many ( [ u'slack.send' ] , block = 0 ) \n    delay = 0.1 \n    if channel : \n        self . protocols [ 0 ] . sendSlack ( message ) \n    reactor . callLater ( delay , self . read_channel ) "}
{"9469": "\ndef v2_playbook_on_task_start ( self , task , ** kwargs ) : \n    self . last_task_name = task . get_name ( ) \n    self . printed_last_task = 0 "}
{"9470": "\ndef v2_runner_on_ok ( self , result , ** kwargs ) : \n    failed = \"failed\" in result . _result \n    unreachable = \"unreachable\" in result . _result \n    if ( \"print_action\" in result . _task . tags or failed or unreachable or self . _display . verbosity > 1 ) : \n        self . _print_task ( ) \n        self . last_skipped = 0 \n        msg = unicode ( result . _result . get ( \"msg\" , \"\" ) ) or unicode ( result . _result . get ( \"reason\" , \"\" ) ) or unicode ( result . _result . get ( \"message\" , \"\" ) ) \n        stderr = [ result . _result . get ( \"exception\" , None ) , result . _result . get ( \"module_stderr\" , None ) , ] \n        stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n        self . _print_host_or_item ( result . _host , result . _result . get ( \"changed\" , 0 ) , msg , result . _result . get ( \"diff\" , None ) , is_host = 1 , error = failed or unreachable , stdout = result . _result . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n        if \"results\" in result . _result : \n            for r in result . _result [ \"results\" ] : \n                failed = \"failed\" in r \n                stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module_stderr\" , None ) ] \n                stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n                self . _print_host_or_item ( r [ \"item\" ] , r . get ( \"changed\" , 0 ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is_host = 0 , error = failed , stdout = r . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n    else : \n        self . last_skipped = 1 \n        print ( \".\" , end = \"\" ) "}
{"9471": "\ndef v2_playbook_on_stats ( self , stats ) : \n    print ( ) \n    self . printed_last_task = 0 \n    self . _print_task ( \"STATS\" ) \n    hosts = sorted ( stats . processed . keys ( ) ) \n    for host in hosts : \n        s = stats . summarize ( host ) \n        if s [ \"failures\" ] or s [ \"unreachable\" ] : \n            color = \"failed\" \n        elif s [ \"changed\" ] : \n            color = \"changed\" \n        else : \n            color = \"ok\" \n        msg = \"{}    : ok={}\\tchanged={}\\tfailed={}\\tunreachable={}\" . format ( host , s [ \"ok\" ] , s [ \"changed\" ] , s [ \"failures\" ] , s [ \"unreachable\" ] ) \n        print ( colorize ( msg , color ) ) "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if self . _display . verbosity > 1 : \n        self . _print_task ( ) \n        self . last_skipped = 0 \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if len ( reason ) < 50 : \n            line += \" -- {}\" . format ( reason ) \n            print ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            print ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            print ( self . _indent_text ( reason , 8 ) ) \n            print ( reason ) "}
{"9475": "\ndef add_model ( self , model , force = 0 ) : \n    if isinstance ( model , str ) : \n        self . _load_model ( model ) \n        return \n    try : \n        model = model ( ) \n    except Exception : \n        pass \n    if model . _yang_name not in [ a [ 0 ] for a in SUPPORTED_MODELS ] and not force : \n        raise ValueError ( \"Only models in SUPPORTED_MODELS can be added without `force=True`\" ) \n    for k , v in model : \n        self . _elements [ k ] = v \n        setattr ( self , k , v ) "}
{"9476": "\ndef get ( self , filter = 0 ) : \n    result = { } \n    for k , v in self . elements ( ) . items ( ) : \n        intermediate = v . get ( filter = filter ) \n        if intermediate : \n            result [ k ] = intermediate \n    return result "}
{"9477": "\ndef load_dict ( self , data , overwrite = 0 , auto_load_model = 1 ) : \n    for k , v in data . items ( ) : \n        if k not in self . _elements . keys ( ) and not auto_load_model : \n            raise AttributeError ( \"Model {} is not loaded\" . format ( k ) ) \n        elif k not in self . _elements . keys ( ) and auto_load_model : \n            self . _load_model ( k ) \n        attr = getattr ( self , k ) \n        _load_dict ( attr , v ) "}
{"9478": "\ndef to_dict ( self , filter = 1 ) : \n    result = { } \n    for k , v in self : \n        r = _to_dict ( v , filter ) \n        if r : \n            result [ k ] = r \n    return result "}
{"9479": "\ndef parse_config ( self , device = None , profile = None , native = None , attrs = None ) : \n    if attrs is None : \n        attrs = self . elements ( ) . values ( ) \n    for v in attrs : \n        parser = Parser ( v , device = device , profile = profile , native = native , is_config = 1 ) \n        parser . parse ( ) "}
{"9480": "\ndef parse_state ( self , device = None , profile = None , native = None , attrs = None ) : \n    if attrs is None : \n        attrs = self . elements ( ) . values ( ) \n    for v in attrs : \n        parser = Parser ( v , device = device , profile = profile , native = native , is_config = 0 ) \n        parser . parse ( ) "}
{"9484": "\ndef model_to_dict ( model , mode = \"\" , show_defaults = 0 ) : \n    def is_mode ( obj , mode ) : \n        if mode == \"\" : \n            return 1 \n        elif mode == \"config\" : \n            return obj . _yang_name == \"config\" or obj . _is_config \n        elif mode == \"state\" : \n            return obj . _yang_name == \"state\" or not obj . _is_config \n        else : \n            raise ValueError ( \"mode can only be config, state or ''. Passed: {}\" . format ( mode ) ) \n    def get_key ( key , model , parent_defining_module , show_defaults ) : \n        if not show_defaults : \n            key = \"{} {}\" . format ( key , \"[rw]\" if model . _is_config else \"[ro]\" ) \n        if parent_defining_module != model . _defining_module : \n            key = \"{}:{}\" . format ( model . _defining_module , key ) \n        return key \n    if model . _yang_type in ( \"container\" , \"list\" ) : \n        cls = model if model . _yang_type in ( \"container\" , ) else model . _contained_class ( ) \n        result = { } \n        for k , v in cls : \n            r = model_to_dict ( v , mode = mode , show_defaults = show_defaults ) \n            if r : \n                result [ get_key ( k , v , model . _defining_module , show_defaults ) ] = r \n        return result \n    else : \n        if show_defaults : \n            if model . _default is 0 : \n                if model . _yang_type != \"boolean\" : \n                    return None \n            return model . _default \n        return model . _yang_type if is_mode ( model , mode ) else None "}
{"9486": "\ndef http_post ( self , url , data = None ) : \n    if not url . startswith ( 'https://' ) : \n        raise ValueError ( 'Protocol must be HTTPS, invalid URL: %s' % url ) \n    return requests . post ( url , data , verify = 1 ) "}
{"9489": "\ndef url_query_params ( url ) : \n    return dict ( urlparse . parse_qsl ( urlparse . urlparse ( url ) . query , 1 ) ) "}
{"9491": "\ndef build_url ( base , additional_params = None ) : \n    url = urlparse . urlparse ( base ) \n    query_params = { } \n    query_params . update ( urlparse . parse_qsl ( url . query , 1 ) ) \n    if additional_params is not None : \n        query_params . update ( additional_params ) \n        for k , v in additional_params . iteritems ( ) : \n            if v is None : \n                query_params . pop ( k ) \n    return urlparse . urlunparse ( ( url . scheme , url . netloc , url . path , url . params , urllib . urlencode ( query_params ) , url . fragment ) ) "}
{"9501": "\ndef get_authorization ( self ) : \n    auth = self . authorization_class ( ) \n    header = self . get_authorization_header ( ) \n    if not header or not header . split : \n        return auth \n    header = header . split ( ) \n    if len ( header ) > 1 and header [ 0 ] == 'Bearer' : \n        auth . is_oauth = 1 \n        access_token = header [ 1 ] \n        self . validate_access_token ( access_token , auth ) \n        if not auth . is_valid : \n            auth . error = 'access_denied' \n    return auth "}
{"9516": "\ndef upload_from_url_sync ( cls , url , timeout = 30 , interval = 0.3 , until_ready = 0 , store = None , filename = None ) : \n    ffu = cls . upload_from_url ( url , store , filename ) \n    return ffu . wait ( timeout = timeout , interval = interval , until_ready = until_ready ) "}
{"9520": "\ndef _base_opration ( self , method ) : \n    uuids = self . uuids ( ) \n    while 1 : \n        chunk = list ( islice ( uuids , 0 , self . chunk_size ) ) \n        if not chunk : \n            return \n        rest_request ( method , self . storage_url , chunk ) "}
{"9524": "\ndef uploading_request ( verb , path , data = None , files = None , timeout = conf . DEFAULT ) : \n    path = path . lstrip ( '/' ) \n    url = urljoin ( conf . upload_base , path ) \n    if data is None : \n        data = { } \n    data [ 'pub_key' ] = conf . pub_key \n    data [ 'UPLOADCARE_PUB_KEY' ] = conf . pub_key \n    headers = { 'User-Agent' : _build_user_agent ( ) , } \n    try : \n        response = session . request ( str ( verb ) , url , allow_redirects = 1 , verify = conf . verify_upload_ssl , data = data , files = files , headers = headers , timeout = _get_timeout ( timeout ) , ) \n    except requests . RequestException as exc : \n        raise APIConnectionError ( exc . args [ 0 ] ) \n    if response . status_code == 204 : \n        return { } \n    if 200 <= response . status_code < 300 : \n        if _content_type_from_response ( response ) . endswith ( ( '/json' , '+json' ) ) : \n            try : \n                return response . json ( ) \n            except ValueError as exc : \n                raise APIError ( exc . args [ 0 ] ) \n    if response . status_code in ( 400 , 404 ) : \n        raise InvalidRequestError ( response . content ) \n    raise APIError ( response . content ) "}
{"9533": "\ndef is_last_li ( li , meta_data , current_numId ) : \n    if not is_li ( li , meta_data ) : \n        return 0 \n    w_namespace = get_namespace ( li , 'w' ) \n    next_el = li \n    while 1 : \n        if next_el is None : \n            return 1 \n        next_el = next_el . getnext ( ) \n        if not is_li ( next_el , meta_data ) : \n            continue \n        new_numId = get_numId ( next_el , w_namespace ) \n        if current_numId != new_numId : \n            return 1 \n        return 0 "}
{"9534": "\ndef get_single_list_nodes_data ( li , meta_data ) : \n    yield li \n    w_namespace = get_namespace ( li , 'w' ) \n    current_numId = get_numId ( li , w_namespace ) \n    starting_ilvl = get_ilvl ( li , w_namespace ) \n    el = li \n    while 1 : \n        el = el . getnext ( ) \n        if el is None : \n            break \n        if not has_text ( el ) : \n            continue \n        if _is_top_level_upper_roman ( el , meta_data ) : \n            break \n        if ( is_li ( el , meta_data ) and ( starting_ilvl > get_ilvl ( el , w_namespace ) ) ) : \n            break \n        new_numId = get_numId ( el , w_namespace ) \n        if new_numId is None or new_numId == - 1 : \n            yield el \n            continue \n        if current_numId != new_numId : \n            break \n        if is_last_li ( el , meta_data , current_numId ) : \n            yield el \n            break \n        yield el "}
{"9539": "\ndef style_is_false ( style ) : \n    if style is None : \n        return 0 \n    w_namespace = get_namespace ( style , 'w' ) \n    return style . get ( '%sval' % w_namespace ) != 'false' "}
{"9540": "\ndef is_bold ( r ) : \n    w_namespace = get_namespace ( r , 'w' ) \n    rpr = r . find ( '%srPr' % w_namespace ) \n    if rpr is None : \n        return 0 \n    bold = rpr . find ( '%sb' % w_namespace ) \n    return style_is_false ( bold ) "}
{"9541": "\ndef is_italics ( r ) : \n    w_namespace = get_namespace ( r , 'w' ) \n    rpr = r . find ( '%srPr' % w_namespace ) \n    if rpr is None : \n        return 0 \n    italics = rpr . find ( '%si' % w_namespace ) \n    return style_is_false ( italics ) "}
{"9542": "\ndef is_underlined ( r ) : \n    w_namespace = get_namespace ( r , 'w' ) \n    rpr = r . find ( '%srPr' % w_namespace ) \n    if rpr is None : \n        return 0 \n    underline = rpr . find ( '%su' % w_namespace ) \n    return style_is_false ( underline ) "}
{"9543": "\ndef is_title ( p ) : \n    w_namespace = get_namespace ( p , 'w' ) \n    styles = p . xpath ( './/w:pStyle' , namespaces = p . nsmap ) \n    if len ( styles ) == 0 : \n        return 0 \n    style = styles [ 0 ] \n    return style . get ( '%sval' % w_namespace ) == 'Title' "}
{"9546": "\ndef _get_document_data ( f , image_handler = None ) : \n    if image_handler is None : \n        def image_handler ( image_id , relationship_dict ) : \n            return relationship_dict . get ( image_id ) \n    document_xml = None \n    numbering_xml = None \n    relationship_xml = None \n    styles_xml = None \n    parser = etree . XMLParser ( strip_cdata = 0 ) \n    path , _ = os . path . split ( f . filename ) \n    media = { } \n    image_sizes = { } \n    for item in f . infolist ( ) : \n        if item . filename == 'word/document.xml' : \n            xml = f . read ( item . filename ) \n            document_xml = etree . fromstring ( xml , parser ) \n        elif item . filename == 'word/numbering.xml' : \n            xml = f . read ( item . filename ) \n            numbering_xml = etree . fromstring ( xml , parser ) \n        elif item . filename == 'word/styles.xml' : \n            xml = f . read ( item . filename ) \n            styles_xml = etree . fromstring ( xml , parser ) \n        elif item . filename == 'word/_rels/document.xml.rels' : \n            xml = f . read ( item . filename ) \n            try : \n                relationship_xml = etree . fromstring ( xml , parser ) \n            except XMLSyntaxError : \n                relationship_xml = etree . fromstring ( '<xml></xml>' , parser ) \n        if item . filename . startswith ( 'word/media/' ) : \n            media [ item . filename [ len ( 'word/' ) : ] ] = f . extract ( item . filename , path , ) \n    f . close ( ) \n    numbering_dict = get_numbering_info ( numbering_xml ) \n    image_sizes = get_image_sizes ( document_xml ) \n    relationship_dict = get_relationship_info ( relationship_xml , media , image_sizes ) \n    styles_dict = get_style_dict ( styles_xml ) \n    font_sizes_dict = defaultdict ( int ) \n    if DETECT_FONT_SIZE : \n        font_sizes_dict = get_font_sizes_dict ( document_xml , styles_dict ) \n    meta_data = MetaData ( numbering_dict = numbering_dict , relationship_dict = relationship_dict , styles_dict = styles_dict , font_sizes_dict = font_sizes_dict , image_handler = image_handler , image_sizes = image_sizes , ) \n    return document_xml , meta_data "}
{"9549": "\ndef build_tr ( tr , meta_data , row_spans ) : \n    tr_el = etree . Element ( 'tr' ) \n    w_namespace = get_namespace ( tr , 'w' ) \n    visited_nodes = [ ] \n    for el in tr : \n        if el in visited_nodes : \n            continue \n        visited_nodes . append ( el ) \n        if el . tag == '%stc' % w_namespace : \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) != 'restart' ) : \n                continue \n            texts = [ ] \n            for td_content in el : \n                if td_content in visited_nodes : \n                    continue \n                if is_li ( td_content , meta_data ) : \n                    li_nodes = get_single_list_nodes_data ( td_content , meta_data , ) \n                    list_el , list_visited_nodes = build_list ( li_nodes , meta_data , ) \n                    visited_nodes . extend ( list_visited_nodes ) \n                    texts . append ( etree . tostring ( list_el ) ) \n                elif td_content . tag == '%stbl' % w_namespace : \n                    table_el , table_visited_nodes = build_table ( td_content , meta_data , ) \n                    visited_nodes . extend ( table_visited_nodes ) \n                    texts . append ( etree . tostring ( table_el ) ) \n                elif td_content . tag == '%stcPr' % w_namespace : \n                    visited_nodes . append ( td_content ) \n                    continue \n                else : \n                    text = get_element_content ( td_content , meta_data , is_td = 1 , ) \n                    texts . append ( text ) \n            data = '<br />' . join ( t for t in texts if t is not None ) \n            td_el = etree . XML ( '<td>%s</td>' % data ) \n            colspan = get_grid_span ( el ) \n            if colspan > 1 : \n                td_el . set ( 'colspan' , '%d' % colspan ) \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) == 'restart' ) : \n                rowspan = next ( row_spans ) \n                td_el . set ( 'rowspan' , '%d' % rowspan ) \n            tr_el . append ( td_el ) \n    return tr_el "}
{"9554": "\ndef load_mnist ( flatten = 1 , labels = 0 ) : \n    fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) \n    h = gzip . open ( fn , 'rb' ) \n    if sys . version_info < ( 3 , ) : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) \n    else : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) \n    h . close ( ) \n    if not flatten : \n        timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n    if labels : \n        return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9555": "\ndef load_cifar ( flatten = 1 , labels = 0 ) : \n    def extract ( name ) : \n        print ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if sys . version_info < ( 3 , ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( 1 , 6 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9556": "\ndef plot_images ( imgs , loc , title = None , channels = 1 ) : \n    n = int ( np . sqrt ( len ( imgs ) ) ) \n    assert n * n == len ( imgs ) , 'images array must contain a square number of rows!' \n    s = int ( np . sqrt ( len ( imgs [ 0 ] ) / channels ) ) \n    assert s * s == len ( imgs [ 0 ] ) / channels , 'images must be square!' \n    img = np . zeros ( ( ( s + 1 ) * n - 1 , ( s + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( s + 1 ) : ( r + 1 ) * ( s + 1 ) - 1 , c * ( s + 1 ) : ( c + 1 ) * ( s + 1 ) - 1 ] = pix . reshape ( ( s , s , channels ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( loc ) \n    ax . xaxis . set_visible ( 0 ) \n    ax . yaxis . set_visible ( 0 ) \n    ax . set_frame_on ( 0 ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) \n    if title : \n        ax . set_title ( title ) "}
{"9557": "\ndef plot_layers ( weights , tied_weights = 0 , channels = 1 ) : \n    if hasattr ( weights [ 0 ] , 'get_value' ) : \n        weights = [ w . get_value ( ) for w in weights ] \n    k = min ( len ( weights ) , 9 ) \n    imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) \n    for i , weight in enumerate ( weights [ : - 1 ] ) : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100 + 10 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) \n    weight = weights [ - 1 ] \n    n = weight . shape [ 1 ] / channels \n    if int ( np . sqrt ( n ) ) ** 2 != n : \n        return \n    if tied_weights : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) \n    else : \n        plot_images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' ) "}
{"9558": "\ndef plot_filters ( filters ) : \n    imgs = filters . get_value ( ) \n    N , channels , x , y = imgs . shape \n    n = int ( np . sqrt ( N ) ) \n    assert n * n == N , 'filters must contain a square number of rows!' \n    assert channels == 1 or channels == 3 , 'can only plot grayscale or rgb filters!' \n    img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( 111 ) \n    ax . xaxis . set_visible ( 0 ) \n    ax . yaxis . set_visible ( 0 ) \n    ax . set_frame_on ( 0 ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) "}
{"9564": "\ndef encode ( self , x , layer = None , sample = 0 , ** kwargs ) : \n    enc = self . feed_forward ( x , ** kwargs ) [ self . _find_output ( layer ) ] \n    if sample : \n        return np . random . binomial ( n = 1 , p = enc ) . astype ( np . uint8 ) \n    return enc "}
{"9573": "\ndef batches ( dataset ) : \n    seq_lengths = dataset . variables [ 'seqLengths' ] . data \n    seq_begins = np . concatenate ( ( [ 0 ] , np . cumsum ( seq_lengths ) [ : - 1 ] ) ) \n    def sample ( ) : \n        chosen = np . random . choice ( list ( range ( len ( seq_lengths ) ) ) , BATCH_SIZE , replace = 0 ) \n        return batch_at ( dataset . variables [ 'inputs' ] . data , dataset . variables [ 'targetClasses' ] . data , seq_begins [ chosen ] , seq_lengths [ chosen ] ) \n    return sample "}
{"9575": "\ndef random_matrix ( rows , cols , mean = 0 , std = 1 , sparsity = 0 , radius = 0 , diagonal = 0 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    arr = mean + std * rng . randn ( rows , cols ) \n    if 1 > sparsity > 0 : \n        k = min ( rows , cols ) \n        mask = rng . binomial ( n = 1 , p = 1 - sparsity , size = ( rows , cols ) ) . astype ( bool ) \n        mask [ : k , : k ] |= np . eye ( k ) . astype ( bool ) \n        arr *= mask \n    if radius > 0 : \n        u , s , vT = np . linalg . svd ( arr , full_matrices = 0 ) \n        arr = np . dot ( np . dot ( u , np . diag ( radius * s / abs ( s [ 0 ] ) ) ) , vT ) \n    if diagonal != 0 : \n        arr = diagonal * np . eye ( max ( rows , cols ) ) [ : rows , : cols ] \n    return arr . astype ( FLOAT ) "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = 0 , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , 0 ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ 0 ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if algo == 'sample' : \n            algo = trainer . SampleTrainer ( self ) \n        elif algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n            algo = trainer . SupervisedPretrainer ( subalgo , self ) \n        elif algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n            algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n        else : \n            algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return 0 \n        if isinstance ( save_every , float ) : \n            return elapsed > 60 * save_every \n        if isinstance ( save_every , int ) : \n            return iteration % save_every == 0 \n        return 0 \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9602": "\ndef bind ( self , graph , reset = 1 , initialize = 1 ) : \n    if reset : \n        for k in self . _input_shapes : \n            self . _input_shapes [ k ] = None \n        for k in self . _output_shapes : \n            self . _output_shapes [ k ] = None \n    self . resolve_inputs ( graph . layers ) \n    self . resolve_outputs ( ) \n    self . activate = activations . build ( self . kwargs . get ( 'activation' , 'relu' ) , self ) \n    if initialize : \n        self . setup ( ) \n    self . log ( ) "}
{"9612": "\ndef loggabor ( self , x_pos , y_pos , sf_0 , B_sf , theta , B_theta , preprocess = 1 ) : \n    env = np . multiply ( self . band ( sf_0 , B_sf ) , self . orientation ( theta , B_theta ) ) \n    if not ( x_pos == 0. ) and not ( y_pos == 0. ) : \n        env = env . astype ( np . complex128 ) * self . trans ( x_pos * 1. , y_pos * 1. ) \n    if preprocess : \n        env *= self . f_mask \n    env /= np . sqrt ( ( np . abs ( env ) ** 2 ) . mean ( ) ) \n    env *= np . sqrt ( 2. ) \n    return env "}
{"9613": "\ndef loggabor_image ( self , x_pos , y_pos , theta , sf_0 , phase , B_sf , B_theta ) : \n    FT_lg = self . loggabor ( x_pos , y_pos , sf_0 = sf_0 , B_sf = B_sf , theta = theta , B_theta = B_theta ) \n    FT_lg = FT_lg * np . exp ( 1j * phase ) \n    return self . invert ( FT_lg , full = 0 ) "}
{"9617": "\ndef to_eaf ( self , skipempty = 1 , pointlength = 0.1 ) : \n    from pympi . Elan import Eaf \n    eaf_out = Eaf ( ) \n    if pointlength <= 0 : \n        raise ValueError ( 'Pointlength should be strictly positive' ) \n    for tier in self . get_tiers ( ) : \n        eaf_out . add_tier ( tier . name ) \n        for ann in tier . get_intervals ( 1 ) : \n            if tier . tier_type == 'TextTier' : \n                ann = ( ann [ 0 ] , ann [ 0 ] + pointlength , ann [ 1 ] ) \n            if ann [ 2 ] . strip ( ) or not skipempty : \n                eaf_out . add_annotation ( tier . name , int ( round ( ann [ 0 ] * 1000 ) ) , int ( round ( ann [ 1 ] * 1000 ) ) , ann [ 2 ] ) \n    return eaf_out "}
{"9618": "\ndef add_point ( self , point , value , check = 1 ) : \n    if self . tier_type != 'TextTier' : \n        raise Exception ( 'Tiertype must be TextTier.' ) \n    if check and any ( i for i in self . intervals if i [ 0 ] == point ) : \n        raise Exception ( 'No overlap is allowed' ) \n    self . intervals . append ( ( point , value ) ) "}
{"9619": "\ndef add_interval ( self , begin , end , value , check = 1 ) : \n    if self . tier_type != 'IntervalTier' : \n        raise Exception ( 'Tiertype must be IntervalTier' ) \n    if check : \n        if any ( i for i in self . intervals if begin < i [ 1 ] and end > i [ 0 ] ) : \n            raise Exception ( 'No overlap is allowed' ) \n        if begin > end : \n            raise Exception ( 'Begin must be smaller then end' ) \n    self . intervals . append ( ( begin , end , value ) ) "}
{"9622": "\ndef get_intervals ( self , sort = 0 ) : \n    for i in sorted ( self . intervals ) if sort else self . intervals : \n        yield i "}
{"9623": "\ndef get_all_intervals ( self ) : \n    ints = sorted ( self . get_intervals ( 1 ) ) \n    if self . tier_type == 'IntervalTier' : \n        if not ints : \n            ints . append ( ( self . xmin , self . xmax , '' ) ) \n        else : \n            if ints [ 0 ] [ 0 ] > self . xmin : \n                ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) \n            if ints [ - 1 ] [ 1 ] < self . xmax : \n                ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) \n            p = ints [ - 1 ] \n            for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : \n                if p [ 0 ] - i [ 1 ] != 0 : \n                    ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) \n                p = i \n    return ints "}
{"9631": "\ndef add_linguistic_type ( self , lingtype , constraints = None , timealignable = 1 , graphicreferences = 0 , extref = None , param_dict = None ) : \n    if param_dict : \n        self . linguistic_types [ lingtype ] = param_dict \n    else : \n        if constraints : \n            self . constraints [ constraints ] \n        self . linguistic_types [ lingtype ] = { 'LINGUISTIC_TYPE_ID' : lingtype , 'TIME_ALIGNABLE' : str ( timealignable ) . lower ( ) , 'GRAPHIC_REFERENCES' : str ( graphicreferences ) . lower ( ) , 'CONSTRAINTS' : constraints } \n        if extref is not None : \n            self . linguistic_types [ lingtype ] [ 'EXT_REF' ] = extref "}
{"9637": "\ndef extract ( self , start , end ) : \n    from copy import deepcopy \n    eaf_out = deepcopy ( self ) \n    for t in eaf_out . get_tier_names ( ) : \n        for ab , ae , value in eaf_out . get_annotation_data_for_tier ( t ) : \n            if ab > end or ae < start : \n                eaf_out . remove_annotation ( t , ( start - end ) // 2 , 0 ) \n    eaf_out . clean_time_slots ( ) \n    return eaf_out "}
{"9645": "\ndef merge_tiers ( self , tiers , tiernew = None , gapt = 0 , sep = '_' , safe = 0 ) : \n    if tiernew is None : \n        tiernew = u'{}_merged' . format ( '_' . join ( tiers ) ) \n    self . add_tier ( tiernew ) \n    aa = [ ( sys . maxsize , sys . maxsize , None ) ] + sorted ( ( a for t in tiers for a in self . get_annotation_data_for_tier ( t ) ) , reverse = 1 ) \n    l = None \n    while aa : \n        begin , end , value = aa . pop ( ) \n        if l is None : \n            l = [ begin , end , [ value ] ] \n        elif begin - l [ 1 ] >= gapt : \n            if not safe or l [ 1 ] > l [ 0 ] : \n                self . add_annotation ( tiernew , l [ 0 ] , l [ 1 ] , sep . join ( l [ 2 ] ) ) \n            l = [ begin , end , [ value ] ] \n        else : \n            if end > l [ 1 ] : \n                l [ 1 ] = end \n            l [ 2 ] . append ( value ) \n    return tiernew "}
{"9646": "\ndef remove_all_annotations_from_tier ( self , id_tier , clean = 1 ) : \n    for aid in self . tiers [ id_tier ] [ 0 ] : \n        del ( self . annotations [ aid ] ) \n    for aid in self . tiers [ id_tier ] [ 1 ] : \n        del ( self . annotations [ aid ] ) \n    self . tiers [ id_tier ] [ 0 ] . clear ( ) \n    self . tiers [ id_tier ] [ 1 ] . clear ( ) \n    if clean : \n        self . clean_time_slots ( ) "}
{"9653": "\ndef remove_tier ( self , id_tier , clean = 1 ) : \n    del ( self . tiers [ id_tier ] ) \n    if clean : \n        self . clean_time_slots ( ) "}
{"9654": "\ndef remove_tiers ( self , tiers ) : \n    for a in tiers : \n        self . remove_tier ( a , clean = 0 ) \n    self . clean_time_slots ( ) "}
{"9657": "\ndef main ( ) : \n    import optparse \n    import sys \n    import codecs \n    import locale \n    import six \n    from . algorithm import get_display \n    parser = optparse . OptionParser ( ) \n    parser . add_option ( '-e' , '--encoding' , dest = 'encoding' , default = 'utf-8' , type = 'string' , help = 'Text encoding (default: utf-8)' ) \n    parser . add_option ( '-u' , '--upper-is-rtl' , dest = 'upper_is_rtl' , default = 0 , action = 'store_true' , help = \"Treat upper case chars as strong 'R' \" 'for debugging (default: False).' ) \n    parser . add_option ( '-d' , '--debug' , dest = 'debug' , default = 0 , action = 'store_true' , help = \"Output to stderr steps taken with the algorithm\" ) \n    parser . add_option ( '-b' , '--base-dir' , dest = 'base_dir' , default = None , type = 'string' , help = \"Override base direction [L|R]\" ) \n    options , rest = parser . parse_args ( ) \n    if options . base_dir and options . base_dir not in 'LR' : \n        parser . error ( 'option -b can be L or R' ) \n    if six . PY2 : \n        sys . stdout = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stdout ) \n    if rest : \n        lines = rest \n    else : \n        lines = sys . stdin \n    for line in lines : \n        display = get_display ( line , options . encoding , options . upper_is_rtl , options . base_dir , options . debug ) \n        if not isinstance ( display , six . text_type ) : \n            display = display . decode ( options . encoding ) \n        six . print_ ( display , end = '' ) "}
{"9658": "\ndef debug_storage ( storage , base_info = 0 , chars = 1 , runs = 0 ) : \n    import codecs \n    import locale \n    import sys \n    if six . PY2 : \n        stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) \n    else : \n        stderr = sys . stderr \n    caller = inspect . stack ( ) [ 1 ] [ 3 ] \n    stderr . write ( 'in %s\\n' % caller ) \n    if base_info : \n        stderr . write ( u'  base level  : %d\\n' % storage [ 'base_level' ] ) \n        stderr . write ( u'  base dir    : %s\\n' % storage [ 'base_dir' ] ) \n    if runs : \n        stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) \n    if chars : \n        output = u'  Chars       : ' \n        for _ch in storage [ 'chars' ] : \n            if _ch != '\\n' : \n                output += _ch [ 'ch' ] \n            else : \n                output += 'C' \n        stderr . write ( output + u'\\n' ) \n        output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) \n        stderr . write ( output ) \n        _types = [ _ch [ 'type' ] . ljust ( 3 ) for _ch in storage [ 'chars' ] ] \n        for i in range ( 3 ) : \n            if i : \n                output = u'                %s\\n' \n            else : \n                output = u'  Res. types  : %s\\n' \n            stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) ) "}
{"9659": "\ndef get_base_level ( text , upper_is_rtl = 0 ) : \n    base_level = None \n    prev_surrogate = 0 \n    for _ch in text : \n        if _IS_UCS2 and ( _SURROGATE_MIN <= ord ( _ch ) <= _SURROGATE_MAX ) : \n            prev_surrogate = _ch \n            continue \n        elif prev_surrogate : \n            _ch = prev_surrogate + _ch \n            prev_surrogate = 0 \n        if upper_is_rtl and _ch . isupper ( ) : \n            base_level = 1 \n            break \n        bidi_type = bidirectional ( _ch ) \n        if bidi_type in ( 'AL' , 'R' ) : \n            base_level = 1 \n            break \n        elif bidi_type == 'L' : \n            base_level = 0 \n            break \n    if base_level is None : \n        base_level = 0 \n    return base_level "}
{"9660": "\ndef get_embedding_levels ( text , storage , upper_is_rtl = 0 , debug = 0 ) : \n    prev_surrogate = 0 \n    base_level = storage [ 'base_level' ] \n    for _ch in text : \n        if _IS_UCS2 and ( _SURROGATE_MIN <= ord ( _ch ) <= _SURROGATE_MAX ) : \n            prev_surrogate = _ch \n            continue \n        elif prev_surrogate : \n            _ch = prev_surrogate + _ch \n            prev_surrogate = 0 \n        if upper_is_rtl and _ch . isupper ( ) : \n            bidi_type = 'R' \n        else : \n            bidi_type = bidirectional ( _ch ) \n        storage [ 'chars' ] . append ( { 'ch' : _ch , 'level' : base_level , 'type' : bidi_type , 'orig' : bidi_type } ) \n    if debug : \n        debug_storage ( storage , base_info = 1 ) "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = 0 ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != 0 : \n                overflow_counter += 1 \n                continue \n            new_level = level_func ( embedding_level ) \n            if new_level < EXPLICIT_LEVEL_LIMIT : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif embedding_level == EXPLICIT_LEVEL_LIMIT - 2 : \n                almost_overflow_counter += 1 \n            else : \n                overflow_counter += 1 \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if directional_override != 'N' : \n                    _ch [ 'type' ] = directional_override \n            elif bidi_type == 'PDF' : \n                if overflow_counter : \n                    overflow_counter -= 1 \n                elif almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - 1 : \n                    almost_overflow_counter -= 1 \n                elif levels : \n                    embedding_level , directional_override = levels . pop ( ) \n            elif bidi_type == 'B' : \n                levels . clear ( ) \n                overflow_counter = almost_overflow_counter = 0 \n                embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = 1 ) "}
{"9663": "\ndef resolve_weak_types ( storage , debug = 0 ) : \n    for run in storage [ 'runs' ] : \n        prev_strong = prev_type = run [ 'sor' ] \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = storage [ 'chars' ] [ start : start + length ] \n        for _ch in chars : \n            bidi_type = _ch [ 'type' ] \n            if bidi_type == 'NSM' : \n                _ch [ 'type' ] = bidi_type = prev_type \n            if bidi_type == 'EN' and prev_strong == 'AL' : \n                _ch [ 'type' ] = 'AN' \n            if bidi_type in ( 'R' , 'L' , 'AL' ) : \n                prev_strong = bidi_type \n            prev_type = _ch [ 'type' ] \n        for _ch in chars : \n            if _ch [ 'type' ] == 'AL' : \n                _ch [ 'type' ] = 'R' \n        for idx in range ( 1 , len ( chars ) - 1 ) : \n            bidi_type = chars [ idx ] [ 'type' ] \n            prev_type = chars [ idx - 1 ] [ 'type' ] \n            next_type = chars [ idx + 1 ] [ 'type' ] \n            if bidi_type == 'ES' and ( prev_type == next_type == 'EN' ) : \n                chars [ idx ] [ 'type' ] = 'EN' \n            if bidi_type == 'CS' and prev_type == next_type and prev_type in ( 'AN' , 'EN' ) : \n                chars [ idx ] [ 'type' ] = prev_type \n        for idx in range ( len ( chars ) ) : \n            if chars [ idx ] [ 'type' ] == 'EN' : \n                for et_idx in range ( idx - 1 , - 1 , - 1 ) : \n                    if chars [ et_idx ] [ 'type' ] == 'ET' : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n                for et_idx in range ( idx + 1 , len ( chars ) ) : \n                    if chars [ et_idx ] [ 'type' ] == 'ET' : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n        for _ch in chars : \n            if _ch [ 'type' ] in ( 'ET' , 'ES' , 'CS' ) : \n                _ch [ 'type' ] = 'ON' \n        prev_strong = run [ 'sor' ] \n        for _ch in chars : \n            if _ch [ 'type' ] == 'EN' and prev_strong == 'L' : \n                _ch [ 'type' ] = 'L' \n            if _ch [ 'type' ] in ( 'L' , 'R' ) : \n                prev_strong = _ch [ 'type' ] \n    if debug : \n        debug_storage ( storage , runs = 1 ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = 1 \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = 1 \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = 0 \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if char_level > highest_level : \n            highest_level = char_level \n        if char_level % 2 and char_level < lowest_odd_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end -= 1 \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9667": "\ndef process ( self , context ) : \n    import os \n    from maya import cmds \n    current_file = cmds . file ( sceneName = 1 , query = 1 ) \n    normalised = os . path . normpath ( current_file ) \n    context . set_data ( 'currentFile' , value = normalised ) \n    context . set_data ( 'current_file' , value = normalised ) "}
{"9675": "\ndef add_to_filemenu ( ) : \n    if hasattr ( cmds , 'about' ) and not cmds . about ( batch = 1 ) : \n        mel . eval ( \"evalDeferred buildFileMenu\" ) \n        script = inspect . getsource ( _add_to_filemenu ) \n        script += \"\\n_add_to_filemenu()\" \n        cmds . evalDeferred ( script ) "}
{"9676": "\ndef maintained_selection ( ) : \n    previous_selection = cmds . ls ( selection = 1 ) \n    try : \n        yield \n    finally : \n        if previous_selection : \n            cmds . select ( previous_selection , replace = 1 , noExpand = 1 ) \n        else : \n            cmds . select ( deselect = 1 , noExpand = 1 ) "}
{"9677": "\ndef maintained_time ( ) : \n    ct = cmds . currentTime ( query = 1 ) \n    try : \n        yield \n    finally : \n        cmds . currentTime ( ct , edit = 1 ) "}
{"9706": "\ndef send ( self , data , force = 0 ) : \n    if self . _registered or force : \n        self . _sock_file . write ( '%s\\r\\n' % data ) \n        self . _sock_file . flush ( ) \n    else : \n        self . _out_buffer . append ( data ) "}
{"9707": "\ndef connect ( self ) : \n    self . _sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n    if self . use_ssl : \n        self . _sock = ssl . wrap_socket ( self . _sock ) \n    try : \n        self . _sock . connect ( ( self . server , self . port ) ) \n    except socket . error : \n        self . logger . error ( 'Unable to connect to %s on port %d' % ( self . server , self . port ) , exc_info = 1 ) \n        return 0 \n    self . _sock_file = self . _sock . makefile ( ) \n    if self . password : \n        self . set_password ( ) \n    self . register_nick ( ) \n    self . register ( ) \n    return 1 "}
{"9711": "\ndef handle_ping ( self , payload ) : \n    self . logger . info ( 'server ping: %s' % payload ) \n    self . send ( 'PONG %s' % payload , 1 ) "}
{"9712": "\ndef handle_registered ( self , server ) : \n    if not self . _registered : \n        self . logger . info ( 'Registered' ) \n        self . _registered = 1 \n        for data in self . _out_buffer : \n            self . send ( data ) \n        self . _out_buffer = [ ] "}
{"9713": "\ndef enter_event_loop ( self ) : \n    patterns = self . dispatch_patterns ( ) \n    self . logger . debug ( 'entering receive loop' ) \n    while 1 : \n        try : \n            data = self . _sock_file . readline ( ) \n        except socket . error : \n            data = None \n        if not data : \n            self . logger . info ( 'server closed connection' ) \n            self . close ( ) \n            return 1 \n        data = data . rstrip ( ) \n        for pattern , callback in patterns : \n            match = pattern . match ( data ) \n            if match : \n                callback ( ** match . groupdict ( ) ) "}
{"9719": "\ndef worker_execute ( self , nick , message , channel , task_id , command , workers = None ) : \n    if workers : \n        nicks = workers . split ( ',' ) \n        do_task = self . conn . nick in nicks \n    else : \n        do_task = 1 \n    if do_task : \n        self . task_queue . put ( ( int ( task_id ) , command ) ) \n        return '!task-received %s' % task_id "}
{"9726": "\ndef allow_request ( self , request , view ) : \n    if request . method != 'POST' : \n        return 1 \n    return super ( PostRequestThrottleMixin , self ) . allow_request ( request , view ) "}
{"9734": "\ndef stop ( self , now = 0 ) : \n    self . log . info ( \"Stopping and removing Docker service %s (id: %s)\" , self . service_name , self . service_id [ : 7 ] ) \n    yield self . docker ( 'remove_service' , self . service_id [ : 7 ] ) \n    self . log . info ( \"Docker service %s (id: %s) removed\" , self . service_name , self . service_id [ : 7 ] ) \n    self . clear_state ( ) "}
{"9743": "\ndef update_expiry ( self , commit = 1 ) : \n    self . expires = update_expiry ( self . created ) \n    if commit : \n        self . save ( ) "}
{"9759": "\ndef ping ( self , params = None ) : \n    try : \n        self . transport . perform_request ( 'HEAD' , '/' , params = params ) \n    except TransportError : \n        raise gen . Return ( 0 ) \n    raise gen . Return ( 1 ) "}
{"9764": "\ndef memory_size ( self , human_readable = 1 ) : \n    if self . _data is not None : \n        return_data = int ( self . _data [ \"memory\" ] [ \"memory_size\" ] ) * 1024 \n        if human_readable : \n            return SynoFormatHelper . bytes_to_readable ( return_data ) \n        else : \n            return return_data "}
{"9765": "\ndef network_up ( self , human_readable = 1 ) : \n    network = self . _get_network ( \"total\" ) \n    if network is not None : \n        return_data = int ( network [ \"tx\" ] ) \n        if human_readable : \n            return SynoFormatHelper . bytes_to_readable ( return_data ) \n        else : \n            return return_data "}
{"9768": "\ndef volume_size_total ( self , volume , human_readable = 1 ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        return_data = int ( volume [ \"size\" ] [ \"total\" ] ) \n        if human_readable : \n            return SynoFormatHelper . bytes_to_readable ( return_data ) \n        else : \n            return return_data "}
{"9773": "\ndef _login ( self ) : \n    api_path = \"%s/auth.cgi?api=SYNO.API.Auth&version=2\" % ( self . base_url , ) \n    login_path = \"method=login&%s\" % ( self . _encode_credentials ( ) ) \n    url = \"%s&%s&session=Core&format=cookie\" % ( api_path , login_path ) \n    result = self . _execute_get_url ( url , 0 ) \n    if result is not None : \n        self . access_token = result [ \"data\" ] [ \"sid\" ] \n        self . _debuglog ( \"Authentication Succesfull, token: \" + str ( self . access_token ) ) \n        return 1 \n    else : \n        self . _debuglog ( \"Authentication Failed\" ) \n        return 0 "}
{"9774": "\ndef _get_url ( self , url , retry_on_error = 1 ) : \n    if self . access_token is None or self . _session is None or self . _session_error : \n        self . access_token = None \n        self . _session_error = 0 \n        if self . _session is not None : \n            self . _session = None \n        self . _debuglog ( \"Creating New Session\" ) \n        self . _session = requests . Session ( ) \n        if self . _use_https : \n            self . _session . verify = 0 \n        if self . _login ( ) is 0 : \n            self . _session_error = 1 \n            self . _debuglog ( \"Login Failed, unable to process request\" ) \n            return \n    response = self . _execute_get_url ( url ) \n    if ( self . _session_error or response is None ) and retry_on_error : \n        self . _debuglog ( \"Error occured, retrying...\" ) \n        self . _get_url ( url , 0 ) \n    return response "}
{"9775": "\ndef _execute_get_url ( self , request_url , append_sid = 1 ) : \n    self . _debuglog ( \"Requesting URL: '\" + request_url + \"'\" ) \n    if append_sid : \n        self . _debuglog ( \"Appending access_token (SID: \" + self . access_token + \") to url\" ) \n        request_url = \"%s&_sid=%s\" % ( request_url , self . access_token ) \n    try : \n        resp = self . _session . get ( request_url ) \n        self . _debuglog ( \"Request executed: \" + str ( resp . status_code ) ) \n        if resp . status_code == 200 : \n            json_data = json . loads ( resp . text ) \n            if json_data [ \"success\" ] : \n                self . _debuglog ( \"Succesfull returning data\" ) \n                self . _debuglog ( str ( json_data ) ) \n                return json_data \n            else : \n                if json_data [ \"error\" ] [ \"code\" ] in { 105 , 106 , 107 , 119 } : \n                    self . _debuglog ( \"Session error: \" + str ( json_data [ \"error\" ] [ \"code\" ] ) ) \n                    self . _session_error = 1 \n                else : \n                    self . _debuglog ( \"Failed: \" + resp . text ) \n        else : \n            return None \n    except : \n        return None "}
{"9786": "\ndef _get_value ( self , key , func = None , split_val = None , as_boolean = 0 , exception_default = None ) : \n    try : \n        if as_boolean : \n            return self . config . getboolean ( key [ 0 ] , key [ 1 ] ) \n        value = self . config . get ( key [ 0 ] , key [ 1 ] ) \n        if split_val is not None : \n            value = value . split ( split_val ) \n        if func is not None : \n            return func ( value ) \n        return value \n    except ( KeyError , configparser . NoSectionError , configparser . NoOptionError ) as e : \n        if exception_default is not None : \n            return exception_default \n        raise KeyError ( e ) "}
{"9789": "\ndef _start_webserver ( self , authorize_url = None ) : \n    server_address = ( SERVER_URL , SERVER_PORT ) \n    self . server = HTTPServer ( server_address , OAuth2UtilRequestHandler ) \n    self . server . response_code = None \n    self . server . authorize_url = authorize_url \n    t = Thread ( target = self . server . serve_forever ) \n    t . daemon = 1 \n    t . start ( ) "}
{"9791": "\ndef _get_new_access_information ( self ) : \n    if not self . r . has_oauth_app_info : \n        self . _log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) \n        raise AttributeError ( 'Reddit Session invalid, please check your designated config file.' ) \n    url = self . r . get_authorize_url ( 'UsingOAuth2Util' , self . _get_value ( CONFIGKEY_SCOPE , set , split_val = ',' ) , self . _get_value ( CONFIGKEY_REFRESHABLE , as_boolean = 1 ) ) \n    self . _start_webserver ( url ) \n    if not self . _get_value ( CONFIGKEY_SERVER_MODE , as_boolean = 1 ) : \n        webbrowser . open ( url ) \n    else : \n        print ( \"Webserver is waiting for you :D. Please open {0}:{1}/{2} \" \"in your browser\" . format ( SERVER_URL , SERVER_PORT , SERVER_LINK_PATH ) ) \n    self . _wait_for_response ( ) \n    try : \n        access_information = self . r . get_access_information ( self . server . response_code ) \n    except praw . errors . OAuthException : \n        self . _log ( \"Can not authenticate, maybe the app infos (e.g. secret) are wrong.\" , logging . ERROR ) \n        raise \n    self . _change_value ( CONFIGKEY_TOKEN , access_information [ \"access_token\" ] ) \n    self . _change_value ( CONFIGKEY_REFRESH_TOKEN , access_information [ \"refresh_token\" ] ) \n    self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) "}
{"9794": "\ndef refresh ( self , force = 0 , _retry = 0 ) : \n    if _retry >= 5 : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    if time . time ( ) > self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN : \n        self . config . read ( self . configfile ) \n        if time . time ( ) < self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN : \n            self . _log ( \"Found new token\" ) \n            self . set_access_credentials ( ) \n    if force or time . time ( ) > self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN : \n        self . _log ( \"Refresh Token\" ) \n        try : \n            new_token = self . r . refresh_access_information ( self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n            self . _change_value ( CONFIGKEY_TOKEN , new_token [ \"access_token\" ] ) \n            self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) \n            self . set_access_credentials ( ) \n        except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n            self . _log ( \"Request new Token (REF)\" ) \n            self . _get_new_access_information ( ) "}
{"9797": "\ndef is_glacier ( s3_client , bucket , prefix ) : \n    response = s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix , MaxKeys = 3 ) \n    for key in response [ 'Contents' ] : \n        if key . get ( 'StorageClass' , 'STANDARD' ) == 'GLACIER' : \n            return 1 \n    return 0 "}
{"9806": "\ndef transform ( line , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = 1 ) : \n    return jsonify_good_event ( line . split ( '\\t' ) , known_fields , add_geolocation_data ) "}
{"9807": "\ndef jsonify_good_event ( event , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = 1 ) : \n    if len ( event ) != len ( known_fields ) : \n        raise SnowplowEventTransformationException ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known_fields ) , len ( event ) ) ] ) \n    else : \n        output = { } \n        errors = [ ] \n        if add_geolocation_data and event [ LATITUDE_INDEX ] != '' and event [ LONGITUDE_INDEX ] != '' : \n            output [ 'geo_location' ] = event [ LATITUDE_INDEX ] + ',' + event [ LONGITUDE_INDEX ] \n        for i in range ( len ( event ) ) : \n            key = known_fields [ i ] [ 0 ] \n            if event [ i ] != '' : \n                try : \n                    kvpairs = known_fields [ i ] [ 1 ] ( key , event [ i ] ) \n                    for kvpair in kvpairs : \n                        output [ kvpair [ 0 ] ] = kvpair [ 1 ] \n                except SnowplowEventTransformationException as sete : \n                    errors += sete . error_messages \n                except Exception as e : \n                    errors += [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known_fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] \n        if errors : \n            raise SnowplowEventTransformationException ( errors ) \n        else : \n            return output "}
{"9815": "\ndef format ( self , object , context , maxlevels , level ) : \n    try : \n        return PrettyPrinter . format ( self , object , context , maxlevels , level ) \n    except HANDLED_EXCEPTIONS as e : \n        return _format_exception ( e ) , 1 , 0 "}
{"9817": "\ndef get_token ( s , pos , brackets_are_chars = 1 , environments = 1 , ** parse_flags ) : \n    return LatexWalker ( s , ** parse_flags ) . get_token ( pos = pos , brackets_are_chars = brackets_are_chars , environments = environments ) "}
{"9819": "\ndef latex2text ( content , tolerant_parsing = 0 , keep_inline_math = 0 , keep_comments = 0 ) : \n    ( nodelist , tpos , tlen ) = latexwalker . get_latex_nodes ( content , keep_inline_math = keep_inline_math , tolerant_parsing = tolerant_parsing ) \n    return latexnodes2text ( nodelist , keep_inline_math = keep_inline_math , keep_comments = keep_comments ) "}
{"9820": "\ndef set_tex_input_directory ( self , tex_input_directory , latex_walker_init_args = None , strict_input = 1 ) : \n    self . tex_input_directory = tex_input_directory \n    self . latex_walker_init_args = latex_walker_init_args if latex_walker_init_args else { } \n    self . strict_input = strict_input \n    if tex_input_directory : \n        self . macro_dict [ 'input' ] = MacroDef ( 'input' , lambda n : self . _callback_input ( n ) ) \n        self . macro_dict [ 'include' ] = MacroDef ( 'include' , lambda n : self . _callback_input ( n ) ) \n    else : \n        self . macro_dict [ 'input' ] = MacroDef ( 'input' , discard = 1 ) \n        self . macro_dict [ 'include' ] = MacroDef ( 'include' , discard = 1 ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = 0 , brackets = 1 , substitute_bad_chars = 0 , fail_bad_chars = 0 ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and ord ( ch ) < 127 ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and lch [ 0 : 1 ] == '\\\\' else lch ) \n            elif ( ( ord ( ch ) >= 32 and ord ( ch ) <= 127 ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result += ch \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result += r'{\\bfseries ?}' \n                else : \n                    result += ch \n    return result "}
{"9889": "\ndef to_raw_text_markupless ( text , keep_whitespace = 0 , normalize_ascii = 1 ) : \n    return sent_tokenize ( remove_dates ( _remove_urls ( text ) ) , keep_whitespace , normalize_ascii ) "}
{"9890": "\ndef to_raw_text ( text , keep_whitespace = 0 , normalize_ascii = 1 ) : \n    out = text \n    out = _remove_urls ( text ) \n    out = _remove_mvar ( out ) \n    out = _remove_squiggly_bracket ( out ) \n    out = _remove_table ( out ) \n    out = _remove_brackets ( out ) \n    out = remove_remaining_double_brackets ( out ) \n    out = remove_markup ( out ) \n    out = remove_wikipedia_link . sub ( anchor_replacer , out ) \n    out = remove_bullets_nbsps . sub ( empty_space , out ) \n    out = remove_dates ( out ) \n    out = remove_math_sections ( out ) \n    out = remove_html ( out ) \n    out = sent_tokenize ( out , keep_whitespace , normalize_ascii ) \n    return out "}
{"9891": "\ndef to_raw_text_pairings ( text , keep_whitespace = 0 , normalize_ascii = 1 ) : \n    out = text \n    out = _remove_mvar ( out ) \n    out = _remove_squiggly_bracket ( out ) \n    out = _remove_table ( out ) \n    out = remove_markup ( out ) \n    out = remove_wikipedia_link . sub ( anchor_replacer , out ) \n    out = remove_bullets_nbsps . sub ( empty_space , out ) \n    out = remove_math_sections ( out ) \n    out = remove_html ( out ) \n    for sentence in sent_tokenize ( out , keep_whitespace , normalize_ascii ) : \n        yield sentence "}
{"9908": "\ndef encodeLength ( value ) : \n    encoded = bytearray ( ) \n    while 1 : \n        digit = value % 128 \n        value //= 128 \n        if value > 0 : \n            digit |= 128 \n        encoded . append ( digit ) \n        if value <= 0 : \n            break \n    return encoded "}
{"9916": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    payload = bytearray ( ) \n    varHeader = encode16Int ( self . msgId ) \n    header [ 0 ] = 0x90 \n    for code in self . granted : \n        payload . append ( code [ 0 ] | ( 0x80 if code [ 1 ] == 1 else 0x00 ) ) \n    header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9926": "\ndef format_data ( self , data , scale = 1 ) : \n    if len ( self . analytes ) == 1 : \n        d = nominal_values ( data [ self . analytes [ 0 ] ] ) \n        ds = np . array ( list ( zip ( d , np . zeros ( len ( d ) ) ) ) ) \n    else : \n        d = [ nominal_values ( data [ a ] ) for a in self . analytes ] \n        ds = np . vstack ( d ) . T \n    finite = np . isfinite ( ds ) . sum ( 1 ) == ds . shape [ 1 ] \n    sampled = np . arange ( data [ self . analytes [ 0 ] ] . size ) [ finite ] \n    ds = ds [ finite ] \n    if scale : \n        ds = self . scaler . transform ( ds ) \n    return ds , sampled "}
{"9927": "\ndef fitting_data ( self , data ) : \n    ds_fit , _ = self . format_data ( data , scale = 0 ) \n    self . scaler = preprocessing . StandardScaler ( ) . fit ( ds_fit ) \n    return self . scaler . transform ( ds_fit ) "}
{"9929": "\ndef fit_meanshift ( self , data , bandwidth = None , bin_seeding = 0 , ** kwargs ) : \n    if bandwidth is None : \n        bandwidth = cl . estimate_bandwidth ( data ) \n    ms = cl . MeanShift ( bandwidth = bandwidth , bin_seeding = bin_seeding ) \n    ms . fit ( data ) \n    return ms "}
{"9943": "\ndef tuples_2_bool ( tuples , x ) : \n    if np . ndim ( tuples ) == 1 : \n        tuples = [ tuples ] \n    out = np . zeros ( x . size , dtype = bool ) \n    for l , u in tuples : \n        out [ ( x > l ) & ( x < u ) ] = 1 \n    return out "}
{"9946": "\ndef findmins ( x , y ) : \n    return x [ np . r_ [ 0 , y [ 1 : ] < y [ : - 1 ] ] & np . r_ [ y [ : - 1 ] < y [ 1 : ] , 0 ] ] "}
{"9947": "\ndef cluster_meanshift ( data , bandwidth = None , bin_seeding = 0 , ** kwargs ) : \n    if bandwidth is None : \n        bandwidth = cl . estimate_bandwidth ( data ) \n    ms = cl . MeanShift ( bandwidth = bandwidth , bin_seeding = bin_seeding , ** kwargs ) \n    ms . fit ( data ) \n    labels = ms . labels_ \n    return labels , [ np . nan ] "}
{"9949": "\ndef cluster_DBSCAN ( data , eps = None , min_samples = None , n_clusters = None , maxiter = 200 , ** kwargs ) : \n    if n_clusters is None : \n        if eps is None : \n            eps = 0.3 \n        db = cl . DBSCAN ( eps = eps , min_samples = min_samples , ** kwargs ) . fit ( data ) \n    else : \n        clusters = 0 \n        eps_temp = 1 / .95 \n        niter = 0 \n        while clusters < n_clusters : \n            clusters_last = clusters \n            eps_temp *= 0.95 \n            db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n            clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n            if clusters < clusters_last : \n                eps_temp *= 1 / 0.95 \n                db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n                clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n                warnings . warn ( ( '\\n\\n***Unable to find {:.0f} clusters in ' 'data. Found {:.0f} with an eps of {:.2e}' '' ) . format ( n_clusters , clusters , eps_temp ) ) \n                break \n            niter += 1 \n            if niter == maxiter : \n                warnings . warn ( ( '\\n\\n***Maximum iterations ({:.0f}) reached' ', {:.0f} clusters not found.\\nDeacrease ' 'min_samples or n_clusters (or increase ' 'maxiter).' ) . format ( maxiter , n_clusters ) ) \n                break \n    labels = db . labels_ \n    core_samples_mask = np . zeros_like ( labels ) \n    core_samples_mask [ db . core_sample_indices_ ] = 1 \n    return labels , core_samples_mask "}
{"9955": "\ndef create ( config_name , srmfile = None , dataformat = None , base_on = 'DEFAULT' , make_default = 0 ) : \n    base_config = read_configuration ( base_on ) \n    config_file , cf = read_latoolscfg ( ) \n    if config_name not in cf . sections ( ) : \n        cf . add_section ( config_name ) \n    if dataformat is None : \n        dataformat = base_config [ 'dataformat' ] \n    cf . set ( config_name , 'dataformat' , dataformat ) \n    if srmfile is None : \n        srmfile = base_config [ 'srmfile' ] \n    cf . set ( config_name , 'srmfile' , srmfile ) \n    if make_default : \n        cf . set ( 'DEFAULT' , 'config' , config_name ) \n    with open ( config_file , 'w' ) as f : \n        cf . write ( f ) \n    return "}
{"9957": "\ndef exclude_downhole ( filt , threshold = 2 ) : \n    cfilt = filt . copy ( ) \n    inds = bool_2_indices ( ~ filt ) \n    rem = ( np . diff ( inds ) >= threshold ) [ : , 0 ] \n    if any ( rem ) : \n        if inds [ rem ] . shape [ 0 ] > 1 : \n            limit = inds [ rem ] [ 1 , 0 ] \n            cfilt [ limit : ] = 0 \n    return cfilt "}
{"9958": "\ndef defrag ( filt , threshold = 3 , mode = 'include' ) : \n    if bool_2_indices ( filt ) is None : \n        return filt \n    if mode == 'include' : \n        inds = bool_2_indices ( ~ filt ) + 1 \n        rep = 1 \n    if mode == 'exclude' : \n        inds = bool_2_indices ( filt ) + 1 \n        rep = 0 \n    rem = ( np . diff ( inds ) <= threshold ) [ : , 0 ] \n    cfilt = filt . copy ( ) \n    if any ( rem ) : \n        for lo , hi in inds [ rem ] : \n            cfilt [ lo : hi ] = rep \n    return cfilt "}
{"9959": "\ndef despike ( self , expdecay_despiker = 1 , exponent = None , noise_despiker = 1 , win = 3 , nlim = 12. , maxiter = 3 ) : \n    if not hasattr ( self , 'despiked' ) : \n        self . data [ 'despiked' ] = Bunch ( ) \n    out = { } \n    for a , v in self . focus . items ( ) : \n        if 'time' not in a . lower ( ) : \n            sig = v . copy ( ) \n            if expdecay_despiker : \n                if exponent is not None : \n                    sig = proc . expdecay_despike ( sig , exponent , self . tstep , maxiter ) \n                else : \n                    warnings . warn ( 'exponent is None - either provide exponent, or run at `analyse`\\nlevel to automatically calculate it.' ) \n            if noise_despiker : \n                sig = proc . noise_despike ( sig , int ( win ) , nlim , maxiter ) \n            out [ a ] = sig \n    self . data [ 'despiked' ] . update ( out ) \n    self . data [ 'total_counts' ] = sum ( self . data [ 'despiked' ] . values ( ) ) \n    self . setfocus ( 'despiked' ) \n    return "}
{"9964": "\ndef sample_stats ( self , analytes = None , filt = 1 , stat_fns = { } , eachtrace = 1 ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats [ 'analytes' ] = analytes \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) \n        for n , f in stat_fns . items ( ) : \n            self . stats [ n ] = [ ] \n            for a in analytes : \n                ind = self . filt . grab_filt ( filt , a ) \n                dat = nominal_values ( self . focus [ a ] ) \n                if eachtrace : \n                    sts = [ ] \n                    for t in np . arange ( self . n ) + 1 : \n                        sts . append ( f ( dat [ ind & ( self . ns == t ) ] ) ) \n                    self . stats [ n ] . append ( sts ) \n                else : \n                    self . stats [ n ] . append ( f ( dat [ ind ] ) ) \n            self . stats [ n ] = np . array ( self . stats [ n ] ) \n    return "}
{"9967": "\ndef filter_gradient_threshold ( self , analyte , win , threshold , recalc = 1 ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if recalc or not self . grads_calced : \n        self . grads = calc_grads ( self . Time , self . focus , [ analyte ] , win ) \n        self . grads_calced = 1 \n    below , above = filters . threshold ( abs ( self . grads [ analyte ] ) , threshold ) \n    setn = self . filt . maxset + 1 \n    self . filt . add ( analyte + '_gthresh_below' , below , 'Keep gradient below {:.3e} ' . format ( threshold ) + analyte , params , setn = setn ) \n    self . filt . add ( analyte + '_gthresh_above' , above , 'Keep gradient above {:.3e} ' . format ( threshold ) + analyte , params , setn = setn ) "}
{"9968": "\ndef calc_correlation ( self , x_analyte , y_analyte , window = 15 , filt = 1 , recalc = 1 ) : \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    if label in self . correlations and not recalc : \n        return \n    if window % 2 != 1 : \n        window += 1 \n    ind = self . filt . grab_filt ( filt , [ x_analyte , y_analyte ] ) \n    x = nominal_values ( self . focus [ x_analyte ] ) \n    x [ ~ ind ] = np . nan \n    xr = rolling_window ( x , window , pad = np . nan ) \n    y = nominal_values ( self . focus [ y_analyte ] ) \n    y [ ~ ind ] = np . nan \n    yr = rolling_window ( y , window , pad = np . nan ) \n    r , p = zip ( * map ( nan_pearsonr , xr , yr ) ) \n    r = np . array ( r ) \n    p = np . array ( p ) \n    self . correlations [ label ] = r , p \n    return "}
{"9969": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = 15 , r_threshold = 0.9 , p_threshold = 0.05 , filt = 1 , recalc = 0 ) : \n    if window % 2 != 1 : \n        window += 1 \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    setn = self . filt . maxset + 1 \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    self . calc_correlation ( x_analyte , y_analyte , window , filt , recalc ) \n    r , p = self . correlations [ label ] \n    cfilt = ( abs ( r ) > r_threshold ) & ( p < p_threshold ) \n    cfilt = ~ cfilt \n    name = x_analyte + '_' + y_analyte + '_corr' \n    self . filt . add ( name = name , filt = cfilt , info = ( x_analyte + ' vs. ' + y_analyte + ' correlation filter.' ) , params = params , setn = setn ) \n    self . filt . off ( filt = name ) \n    self . filt . on ( analyte = y_analyte , filt = name ) \n    return "}
{"9972": "\ndef histograms ( dat , keys = None , bins = 25 , logy = 0 , cmap = None , ncol = 4 ) : \n    if keys is None : \n        keys = dat . keys ( ) \n    ncol = int ( ncol ) \n    nrow = calc_nrow ( len ( keys ) , ncol ) \n    fig , axs = plt . subplots ( nrow , 4 , figsize = [ ncol * 2 , nrow * 2 ] ) \n    pn = 0 \n    for k , ax in zip ( keys , axs . flat ) : \n        tmp = nominal_values ( dat [ k ] ) \n        x = tmp [ ~ np . isnan ( tmp ) ] \n        if cmap is not None : \n            c = cmap [ k ] \n        else : \n            c = ( 0 , 0 , 0 , 0.5 ) \n        ax . hist ( x , bins = bins , color = c ) \n        if logy : \n            ax . set_yscale ( 'log' ) \n            ylab = '$log_{10}(n)$' \n        else : \n            ylab = 'n' \n        ax . set_ylim ( 1 , ax . get_ylim ( ) [ 1 ] ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( ylab ) \n        ax . set_yticklabels ( [ ] ) \n        ax . text ( .95 , .95 , k , ha = 'right' , va = 'top' , transform = ax . transAxes ) \n        pn += 1 \n    for ax in axs . flat [ pn : ] : \n        ax . set_visible ( 0 ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"9974": "\ndef load_reference_data ( name = None ) : \n    base_url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQJfCeuqrtFFMAeSpA9rguzLAo9OVuw50AHhAULuqjMJzbd3h46PK1KjF69YiJAeNAAjjMDkJK7wMpG/pub?gid={:}&single=true&output=csv' \n    gids = { 'culture_reference' : '0' , 'culture_test' : '1170065442' , 'downcore_reference' : '190752797' , 'downcore_test' : '721359794' , 'iolite_reference' : '483581945' , 'zircon_reference' : '1355554964' } \n    if name is None : \n        out = { } \n        for nm , gid in gids . items ( ) : \n            url = base_url . format ( gid ) \n            tmp = pd . read_csv ( url , header = [ 0 ] , index_col = [ 0 , 1 ] ) \n            tmp . index . names = [ 'sample' , 'rep' ] \n            tmp . columns . names = [ 'analyte' ] \n            tmp . sort_index ( 1 , inplace = 1 ) \n            out [ nm ] = tmp \n    else : \n        gid = gids [ name ] \n        url = base_url . format ( gid ) \n        out = pd . read_csv ( url , index_col = [ 0 , 1 ] ) \n        out . columns . names = [ 'analyte' ] \n        out . sort_index ( 1 , inplace = 1 ) \n    return out "}
{"9976": "\ndef elements ( all_isotopes = 1 ) : \n    el = pd . read_pickle ( pkgrs . resource_filename ( 'latools' , 'resources/elements.pkl' ) ) \n    if all_isotopes : \n        return el . set_index ( 'element' ) \n    else : \n        def wmean ( g ) : \n            return ( g . atomic_weight * g . percent ) . sum ( ) / 100 \n        iel = el . groupby ( 'element' ) . apply ( wmean ) \n        iel . name = 'atomic_weight' \n        return iel "}
{"9985": "\ndef despike ( self , expdecay_despiker = 0 , exponent = None , noise_despiker = 1 , win = 3 , nlim = 12. , exponentplot = 0 , maxiter = 4 , autorange_kwargs = { } , focus_stage = 'rawdata' ) : \n    if focus_stage != self . focus_stage : \n        self . set_focus ( focus_stage ) \n    if expdecay_despiker and exponent is None : \n        if not hasattr ( self , 'expdecay_coef' ) : \n            self . find_expcoef ( plot = exponentplot , autorange_kwargs = autorange_kwargs ) \n        exponent = self . expdecay_coef \n        time . sleep ( 0.1 ) \n    with self . pbar . set ( total = len ( self . data ) , desc = 'Despiking' ) as prog : \n        for d in self . data . values ( ) : \n            d . despike ( expdecay_despiker , exponent , noise_despiker , win , nlim , maxiter ) \n            prog . update ( ) \n    self . stages_complete . update ( [ 'despiked' ] ) \n    self . focus_stage = 'despiked' \n    return "}
{"9986": "\ndef bkg_calc_weightedmean ( self , analytes = None , weight_fwhm = None , n_min = 20 , n_max = None , cstep = None , bkg_filter = 0 , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if weight_fwhm is None : \n        weight_fwhm = 600 \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    if 'calc' not in self . bkg . keys ( ) : \n        if cstep is None : \n            cstep = weight_fwhm / 20 \n        elif cstep > weight_fwhm : \n            warnings . warn ( \"\\ncstep should be less than weight_fwhm. Your backgrounds\\n\" + \"might not behave as expected.\\n\" ) \n        bkg_t = np . linspace ( 0 , self . max_time , self . max_time // cstep ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    mean , std , stderr = gauss_weighted_stats ( self . bkg [ 'raw' ] . uTime , self . bkg [ 'raw' ] . loc [ : , analytes ] . values , self . bkg [ 'calc' ] [ 'uTime' ] , fwhm = weight_fwhm ) \n    for i , a in enumerate ( analytes ) : \n        self . bkg [ 'calc' ] [ a ] = { 'mean' : mean [ i ] , 'std' : std [ i ] , 'stderr' : stderr [ i ] } "}
{"9987": "\ndef bkg_calc_interp1d ( self , analytes = None , kind = 1 , n_min = 10 , n_max = None , cstep = None , bkg_filter = 0 , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    def pad ( a , lo = None , hi = None ) : \n        if lo is None : \n            lo = [ a [ 0 ] ] \n        if hi is None : \n            hi = [ a [ - 1 ] ] \n        return np . concatenate ( ( lo , a , hi ) ) \n    if 'calc' not in self . bkg . keys ( ) : \n        bkg_t = pad ( self . bkg [ 'summary' ] . loc [ : , ( 'uTime' , 'mean' ) ] , [ 0 ] , [ self . max_time ] ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    d = self . bkg [ 'summary' ] \n    with self . pbar . set ( total = len ( analytes ) , desc = 'Calculating Analyte Backgrounds' ) as prog : \n        for a in analytes : \n            self . bkg [ 'calc' ] [ a ] = { 'mean' : pad ( d . loc [ : , ( a , 'mean' ) ] . values ) , 'std' : pad ( d . loc [ : , ( a , 'std' ) ] . values ) , 'stderr' : pad ( d . loc [ : , ( a , 'stderr' ) ] . values ) } \n            prog . update ( ) \n    self . bkg [ 'calc' ] \n    return "}
{"9990": "\ndef make_subset ( self , samples = None , name = None ) : \n    for k , v in self . subsets . items ( ) : \n        if set ( v ) == set ( samples ) and k != 'not_in_set' : \n            return k \n    if isinstance ( samples , str ) : \n        samples = [ samples ] \n    not_exists = [ s for s in samples if s not in self . subsets [ 'All_Analyses' ] ] \n    if len ( not_exists ) > 0 : \n        raise ValueError ( ', ' . join ( not_exists ) + ' not in the list of sample names.\\nPlease check your sample names.\\nNote: Sample names are stored in the .samples attribute of your analysis.' ) \n    if name is None : \n        name = max ( [ - 1 ] + [ x for x in self . subsets . keys ( ) if isinstance ( x , int ) ] ) + 1 \n    self . _subset_names . append ( name ) \n    if samples is not None : \n        self . subsets [ name ] = samples \n        for s in samples : \n            try : \n                self . subsets [ 'not_in_set' ] . remove ( s ) \n            except ValueError : \n                pass \n    self . _has_subsets = 1 \n    return name "}
{"9991": "\ndef filter_gradient_threshold_percentile ( self , analyte , percentiles , level = 'population' , win = 15 , filt = 0 , samples = None , subset = None ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ analyte ] ) \n    self . get_gradients ( analytes = [ analyte ] , win = win , filt = filt , subset = subset ) \n    grad = self . gradients [ analyte ] [ ~ np . isnan ( self . gradients [ analyte ] ) ] \n    if isinstance ( percentiles , ( int , float ) ) : \n        percentiles = [ percentiles ] \n    if level == 'population' : \n        lims = np . percentile ( grad , percentiles ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Percentile Threshold Filter' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            setn = d . filt . maxset + 1 \n            g = calc_grads ( d . Time , d . focus , [ analyte ] , win ) [ analyte ] \n            if level == 'individual' : \n                gt = nominal_values ( g ) \n                lims = np . percentile ( gt [ ~ np . isnan ( gt ) ] , percentiles ) \n            if len ( lims ) == 1 : \n                above = g >= lims [ 0 ] \n                below = g < lims [ 0 ] \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_below' . format ( percentiles [ 0 ] ) , below , 'Gradients below {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_above' . format ( percentiles [ 0 ] ) , above , 'Gradients above {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n            elif len ( lims ) == 2 : \n                inside = ( g >= min ( lims ) ) & ( g <= max ( lims ) ) \n                outside = ( g < min ( lims ) ) | ( g > max ( lims ) ) \n                lpc = '-' . join ( [ '{:.1f}' . format ( p ) for p in percentiles ] ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_inside' , inside , 'Gradients between ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_outside' , outside , 'Gradients outside ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n            prog . update ( ) \n    return "}
{"9992": "\ndef fit_classifier ( self , name , analytes , method , samples = None , subset = None , filt = 1 , sort_by = 0 , ** kwargs ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    self . get_focus ( subset = subset , filt = filt ) \n    c = classifier ( analytes , sort_by ) \n    c . fit ( data = self . focus , method = method , ** kwargs ) \n    self . classifiers [ name ] = c \n    return name "}
{"9994": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = None , r_threshold = 0.9 , p_threshold = 0.05 , filt = 1 , samples = None , subset = None ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ x_analyte , y_analyte ] ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Correlation Filter' ) as prog : \n        for s in samples : \n            self . data [ s ] . filter_correlation ( x_analyte , y_analyte , window = window , r_threshold = r_threshold , p_threshold = p_threshold , filt = filt ) \n            prog . update ( ) "}
{"9995": "\ndef filter_on ( self , filt = None , analyte = None , samples = None , subset = None , show_status = 0 ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in samples : \n        try : \n            self . data [ s ] . filt . on ( analyte , filt ) \n        except : \n            warnings . warn ( \"filt.on failure in sample \" + s ) \n    if show_status : \n        self . filter_status ( subset = subset ) \n    return "}
{"9996": "\ndef filter_off ( self , filt = None , analyte = None , samples = None , subset = None , show_status = 0 ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in samples : \n        try : \n            self . data [ s ] . filt . off ( analyte , filt ) \n        except : \n            warnings . warn ( \"filt.off failure in sample \" + s ) \n    if show_status : \n        self . filter_status ( subset = subset ) \n    return "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = 0 ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n            if len ( self . subsets [ 'not_in_set' ] ) > 0 : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif sample is not None : \n        s += 'Sample: ' + sample + '\\n' \n        s += self . data [ sample ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s += 'Subset: ' + str ( n ) + '\\n' \n            s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n            s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return "}
{"9998": "\ndef filter_defragment ( self , threshold , mode = 'include' , filt = 1 , samples = None , subset = None ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in samples : \n        f = self . data [ s ] . filt . grab_filt ( filt ) \n        self . data [ s ] . filt . add ( name = 'defrag_{:s}_{:.0f}' . format ( mode , threshold ) , filt = filters . defrag ( f , threshold , mode ) , info = 'Defrag {:s} filter with threshold {:.0f}' . format ( mode , threshold ) , params = ( threshold , mode , filt , samples , subset ) ) "}
{"9999": "\ndef filter_nremoved ( self , filt = 1 , quiet = 0 ) : \n    rminfo = { } \n    for n in self . subsets [ 'All_Samples' ] : \n        s = self . data [ n ] \n        rminfo [ n ] = s . filt_nremoved ( filt ) \n    if not quiet : \n        maxL = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) \n        print ( '{string:{number}s}' . format ( string = 'Sample ' , number = maxL + 3 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) \n        for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : \n            print ( '{string:{number}s}' . format ( string = k , number = maxL + 3 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) \n    return rminfo "}
{"10000": "\ndef gradient_histogram ( self , analytes = None , win = 15 , filt = 0 , bins = None , samples = None , subset = None , recalc = 1 , ncol = 4 ) : \n    if analytes is None : \n        analytes = [ a for a in self . analytes if self . internal_standard not in a ] \n    if not hasattr ( self , 'gradients' ) : \n        self . gradients = Bunch ( ) \n    ncol = int ( ncol ) \n    n = len ( analytes ) \n    nrow = plot . calc_nrow ( n , ncol ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axs = plt . subplots ( nrow , ncol , figsize = [ 3. * ncol , 2.5 * nrow ] ) \n    if not isinstance ( axs , np . ndarray ) : \n        axs = [ axs ] \n    i = 0 \n    for a , ax in zip ( analytes , axs . flatten ( ) ) : \n        d = nominal_values ( self . gradients [ a ] ) \n        d = d [ ~ np . isnan ( d ) ] \n        m , u = unitpicker ( d , focus_stage = self . focus_stage , denominator = self . internal_standard ) \n        if bins is None : \n            ibins = np . linspace ( * np . percentile ( d * m , [ 1 , 99 ] ) , 50 ) \n        else : \n            ibins = bins \n        ax . hist ( d * m , bins = ibins , color = self . cmaps [ a ] ) \n        ax . axvline ( 0 , ls = 'dashed' , lw = 1 , c = ( 0 , 0 , 0 , 0.7 ) ) \n        ax . set_title ( a , loc = 'left' ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( 'N' ) \n        ax . set_xlabel ( u + '/s' ) \n        i += 1 \n    if i < ncol * nrow : \n        for ax in axs . flatten ( ) [ i : ] : \n            ax . set_visible ( 0 ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"10001": "\ndef gradient_crossplot ( self , analytes = None , win = 15 , lognorm = 1 , bins = 25 , filt = 0 , samples = None , subset = None , figsize = ( 12 , 12 ) , save = 0 , colourful = 1 , mode = 'hist2d' , recalc = 1 , ** kwargs ) : \n    if analytes is None : \n        analytes = self . analytes \n    if self . focus_stage in [ 'ratio' , 'calibrated' ] : \n        analytes = [ a for a in analytes if self . internal_standard not in a ] \n    try : \n        analytes = sorted ( analytes , key = lambda x : float ( re . findall ( '[0-9.-]+' , x ) [ 0 ] ) ) \n    except IndexError : \n        analytes = sorted ( analytes ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axes = plot . crossplot ( dat = self . gradients , keys = analytes , lognorm = lognorm , bins = bins , figsize = figsize , colourful = colourful , focus_stage = self . focus_stage , cmap = self . cmaps , denominator = self . internal_standard , mode = mode ) \n    if save : \n        fig . savefig ( self . report_dir + '/g_crossplot.png' , dpi = 200 ) \n    return fig , axes "}
{"10002": "\ndef histograms ( self , analytes = None , bins = 25 , logy = 0 , filt = 0 , colourful = 1 ) : \n    if analytes is None : \n        analytes = self . analytes \n    if self . focus_stage in [ 'ratio' , 'calibrated' ] : \n        analytes = [ a for a in analytes if self . internal_standard not in a ] \n    if colourful : \n        cmap = self . cmaps \n    else : \n        cmap = None \n    self . get_focus ( filt = filt ) \n    fig , axes = plot . histograms ( self . focus , keys = analytes , bins = bins , logy = logy , cmap = cmap ) \n    return fig , axes "}
{"10003": "\ndef trace_plots ( self , analytes = None , samples = None , ranges = 0 , focus = None , outdir = None , filt = None , scale = 'log' , figsize = [ 10 , 4 ] , stats = 0 , stat = 'nanmean' , err = 'nanstd' , subset = 'All_Analyses' ) : \n    if focus is None : \n        focus = self . focus_stage \n    if outdir is None : \n        outdir = self . report_dir + '/' + focus \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if subset is not None : \n        samples = self . _get_samples ( subset ) \n    elif samples is None : \n        samples = self . subsets [ 'All_Analyses' ] \n    elif isinstance ( samples , str ) : \n        samples = [ samples ] \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            f , a = self . data [ s ] . tplot ( analytes = analytes , figsize = figsize , scale = scale , filt = filt , ranges = ranges , stats = stats , stat = stat , err = err , focus_stage = focus ) \n            f . savefig ( outdir + '/' + s + '_traces.pdf' ) \n            plt . close ( f ) \n            prog . update ( ) \n    return "}
{"10004": "\ndef gradient_plots ( self , analytes = None , win = 15 , samples = None , ranges = 0 , focus = None , outdir = None , figsize = [ 10 , 4 ] , subset = 'All_Analyses' ) : \n    if focus is None : \n        focus = self . focus_stage \n    if outdir is None : \n        outdir = self . report_dir + '/' + focus + '_gradient' \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if subset is not None : \n        samples = self . _get_samples ( subset ) \n    elif samples is None : \n        samples = self . subsets [ 'All_Analyses' ] \n    elif isinstance ( samples , str ) : \n        samples = [ samples ] \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            f , a = self . data [ s ] . gplot ( analytes = analytes , win = win , figsize = figsize , ranges = ranges , focus_stage = focus ) \n            f . savefig ( outdir + '/' + s + '_gradients.pdf' ) \n            plt . close ( f ) \n            prog . update ( ) \n    return "}
{"10006": "\ndef sample_stats ( self , analytes = None , filt = 1 , stats = [ 'mean' , 'std' ] , eachtrace = 1 , csf_dict = { } ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats_calced = [ ] \n    stat_fns = Bunch ( ) \n    stat_dict = { 'mean' : np . nanmean , 'std' : np . nanstd , 'nanmean' : np . nanmean , 'nanstd' : np . nanstd , 'se' : stderr , 'H15_mean' : H15_mean , 'H15_std' : H15_std , 'H15_se' : H15_se } \n    for s in stats : \n        if isinstance ( s , str ) : \n            if s in stat_dict . keys ( ) : \n                self . stats_calced . append ( s ) \n                stat_fns [ s ] = stat_dict [ s ] \n            if s in csf_dict . keys ( ) : \n                self . stats_calced . append ( s ) \n                exec ( csf_dict [ s ] ) \n                stat_fns [ s ] = eval ( s ) \n        elif callable ( s ) : \n            self . stats_calced . append ( s . __name__ ) \n            stat_fns [ s . __name__ ] = s \n            if not hasattr ( self , 'custom_stat_functions' ) : \n                self . custom_stat_functions = '' \n            self . custom_stat_functions += inspect . getsource ( s ) + '\\n\\n\\n\\n' \n    with self . pbar . set ( total = len ( self . samples ) , desc = 'Calculating Stats' ) as prog : \n        for s in self . samples : \n            if self . srm_identifier not in s : \n                self . data [ s ] . sample_stats ( analytes , filt = filt , stat_fns = stat_fns , eachtrace = eachtrace ) \n                self . stats [ s ] = self . data [ s ] . stats \n            prog . update ( ) "}
{"10007": "\ndef getstats ( self , save = 1 , filename = None , samples = None , subset = None , ablation_time = 0 ) : \n    slst = [ ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in self . stats_calced : \n        for nm in [ n for n in samples if self . srm_identifier not in n ] : \n            if self . stats [ nm ] [ s ] . ndim == 2 : \n                reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) \n                ss = np . array ( [ s ] * reps . size ) \n                nms = np . array ( [ nm ] * reps . size ) \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) \n                stdf . index . set_names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = 1 ) \n            else : \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T \n                stdf . index . set_names ( [ 'statistic' , 'sample' ] , inplace = 1 ) \n            slst . append ( stdf ) \n    out = pd . concat ( slst ) \n    if ablation_time : \n        ats = self . ablation_times ( samples = samples , subset = subset ) \n        ats [ 'statistic' ] = 'nanmean' \n        ats . set_index ( 'statistic' , append = 1 , inplace = 1 ) \n        ats = ats . reorder_levels ( [ 'statistic' , 'sample' , 'rep' ] ) \n        out = out . join ( ats ) \n    out . drop ( self . internal_standard , 1 , inplace = 1 ) \n    if save : \n        if filename is None : \n            filename = 'stat_export.csv' \n        out . to_csv ( self . export_dir + '/' + filename ) \n    self . stats_df = out \n    return out "}
{"10009": "\ndef export_traces ( self , outdir = None , focus_stage = None , analytes = None , samples = None , subset = 'All_Analyses' , filt = 0 , zip_archive = 0 ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    if focus_stage is None : \n        focus_stage = self . focus_stage \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        analytes = [ a for a in analytes if a != self . internal_standard ] \n    if outdir is None : \n        outdir = os . path . join ( self . export_dir , 'trace_export' ) \n    ud = { 'rawdata' : 'counts' , 'despiked' : 'counts' , 'bkgsub' : 'background corrected counts' , 'ratios' : 'counts/count {:s}' , 'calibrated' : 'mol/mol {:s}' } \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        ud [ focus_stage ] = ud [ focus_stage ] . format ( self . internal_standard ) \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    for s in samples : \n        d = self . data [ s ] . data [ focus_stage ] \n        ind = self . data [ s ] . filt . grab_filt ( filt ) \n        out = Bunch ( ) \n        for a in analytes : \n            out [ a ] = nominal_values ( d [ a ] [ ind ] ) \n            if focus_stage not in [ 'rawdata' , 'despiked' ] : \n                out [ a + '_std' ] = std_devs ( d [ a ] [ ind ] ) \n                out [ a + '_std' ] [ out [ a + '_std' ] == 0 ] = np . nan \n        out = pd . DataFrame ( out , index = self . data [ s ] . Time [ ind ] ) \n        out . index . name = 'Time' \n        header = [ '# Sample: %s' % ( s ) , '# Data Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , '# Processed using %s configuration' % ( self . config [ 'config' ] ) , '# Analysis Stage: %s' % ( focus_stage ) , '# Unit: %s' % ud [ focus_stage ] ] \n        header = '\\n' . join ( header ) + '\\n' \n        csv = out . to_csv ( ) \n        with open ( '%s/%s_%s.csv' % ( outdir , s , focus_stage ) , 'w' ) as f : \n            f . write ( header ) \n            f . write ( csv ) \n    if zip_archive : \n        utils . zipdir ( outdir , delete = 1 ) \n    return "}
{"10011": "\ndef minimal_export ( self , target_analytes = None , path = None ) : \n    if target_analytes is None : \n        target_analytes = self . analytes \n    if isinstance ( target_analytes , str ) : \n        target_analytes = [ target_analytes ] \n    self . minimal_analytes . update ( target_analytes ) \n    zip_archive = 0 \n    if path is None : \n        path = self . export_dir + '/minimal_export.zip' \n    if path . endswith ( '.zip' ) : \n        path = path . replace ( '.zip' , '' ) \n        zip_archive = 1 \n    if not os . path . isdir ( path ) : \n        os . mkdir ( path ) \n    self . _minimal_export_traces ( path + '/data' , analytes = self . minimal_analytes ) \n    log_header = [ '# Minimal Reproduction Dataset Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , 'data_folder :: ./data/' ] \n    if hasattr ( self , 'srmdat' ) : \n        log_header . append ( 'srm_table :: ./srm.table' ) \n        els = np . unique ( [ re . sub ( '[0-9]' , '' , a ) for a in self . minimal_analytes ] ) \n        srmdat = [ ] \n        for e in els : \n            srmdat . append ( self . srmdat . loc [ self . srmdat . element == e , : ] ) \n        srmdat = pd . concat ( srmdat ) \n        with open ( path + '/srm.table' , 'w' ) as f : \n            f . write ( srmdat . to_csv ( ) ) \n    if hasattr ( self , 'custom_stat_functions' ) : \n        with open ( path + '/custom_stat_fns.py' , 'w' ) as f : \n            f . write ( self . custom_stat_functions ) \n        log_header . append ( 'custom_stat_functions :: ./custom_stat_fns.py' ) \n    log_header . append ( '# Analysis Log Start: \\n' ) \n    lss = [ ( i , l ) for i , l in enumerate ( self . log ) if 'sample_stats' in l ] \n    rep = re . compile ( \"(.*'stats': )(\\[.*?\\])(.*)\" ) \n    for i , l in lss : \n        self . log [ i ] = rep . sub ( r'\\1' + str ( self . stats_calced ) + r'\\3' , l ) \n    self . save_log ( path , 'analysis.lalog' , header = log_header ) \n    if zip_archive : \n        utils . zipdir ( directory = path , delete = 1 ) \n    return "}
{"10014": "\ndef pca_plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = 1 ) : \n    nc = pca . n_components \n    f = np . arange ( pca . n_features_ ) \n    cs = list ( itertools . combinations ( range ( nc ) , 2 ) ) \n    ind = ~ np . apply_along_axis ( any , 1 , np . isnan ( dt ) ) \n    cylim = ( pca . components_ . min ( ) , pca . components_ . max ( ) ) \n    yd = cylim [ 1 ] - cylim [ 0 ] \n    fig , axs = plt . subplots ( nc , nc , figsize = [ 3 * nc , nc * 3 ] , tight_layout = 1 ) \n    for x , y in zip ( * np . triu_indices ( nc ) ) : \n        if x == y : \n            tax = axs [ x , y ] \n            tax . bar ( f , pca . components_ [ x ] , 0.8 ) \n            tax . set_xticks ( [ ] ) \n            tax . axhline ( 0 , zorder = - 1 , c = ( 0 , 0 , 0 , 0.6 ) ) \n            tax . set_ylim ( cylim [ 0 ] - 0.2 * yd , cylim [ 1 ] + 0.2 * yd ) \n            for xi , yi , lab in zip ( f , pca . components_ [ x ] , xlabs ) : \n                if yi > 0 : \n                    yo = yd * 0.03 \n                    va = 'bottom' \n                else : \n                    yo = yd * - 0.02 \n                    va = 'top' \n                tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90 , fontsize = 8 ) \n        else : \n            xv = dt [ ind , x ] \n            yv = dt [ ind , y ] \n            if mode == 'scatter' : \n                axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) \n                axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) \n            if mode == 'hist2d' : \n                if lognorm : \n                    norm = mpl . colors . LogNorm ( ) \n                else : \n                    norm = None \n                axs [ x , y ] . hist2d ( xv , yv , 50 , cmap = plt . cm . Blues , norm = norm ) \n                axs [ y , x ] . hist2d ( yv , xv , 50 , cmap = plt . cm . Blues , norm = norm ) \n        if x == 0 : \n            axs [ y , x ] . set_ylabel ( 'PC{:.0f}' . format ( y + 1 ) ) \n        if y == nc - 1 : \n            axs [ y , x ] . set_xlabel ( 'PC{:.0f}' . format ( x + 1 ) ) \n    return fig , axs , xv , yv "}
{"10017": "\ndef noise_despike ( sig , win = 3 , nlim = 24. , maxiter = 4 ) : \n    if win % 2 != 1 : \n        win += 1 \n    kernel = np . ones ( win ) / win \n    over = np . ones ( len ( sig ) , dtype = bool ) \n    npad = int ( ( win - 1 ) / 2 ) \n    over [ : npad ] = 0 \n    over [ - npad : ] = 0 \n    nloops = 0 \n    while any ( over ) and ( nloops < maxiter ) : \n        rmean = np . convolve ( sig , kernel , 'valid' ) \n        rstd = rmean ** 0.5 \n        over [ npad : - npad ] = ( sig [ npad : - npad ] > rmean + nlim * rstd ) \n        if any ( over ) : \n            sig [ npad : - npad ] [ over [ npad : - npad ] ] = rmean [ over [ npad : - npad ] ] \n            nloops += 1 \n    return sig "}
{"10018": "\ndef expdecay_despike ( sig , expdecay_coef , tstep , maxiter = 3 ) : \n    noise = np . std ( sig [ : 5 ] ) \n    for i in [ 10 , 20 , 30 , 50 ] : \n        inoise = np . std ( sig [ : i ] ) \n        if inoise < 1.5 * noise : \n            noise = inoise \n    rms_noise3 = 3 * noise \n    i = 0 \n    f = 1 \n    while ( i < maxiter ) and f : \n        siglo = np . roll ( sig * np . exp ( tstep * expdecay_coef ) , 1 ) \n        sighi = np . roll ( sig * np . exp ( - tstep * expdecay_coef ) , - 1 ) \n        loind = ( sig < siglo - rms_noise3 ) & ( sig < np . roll ( sig , - 1 ) - rms_noise3 ) \n        hiind = ( sig > sighi + rms_noise3 ) & ( sig > np . roll ( sig , 1 ) + rms_noise3 ) \n        sig [ loind ] = sig [ np . roll ( loind , - 1 ) ] \n        sig [ hiind ] = sig [ np . roll ( hiind , - 1 ) ] \n        f = any ( np . concatenate ( [ loind , hiind ] ) ) \n        i += 1 \n    return sig "}
{"10019": "\ndef add ( self , name , filt , info = '' , params = ( ) , setn = None ) : \n    iname = '{:.0f}_' . format ( self . n ) + name \n    self . index [ self . n ] = iname \n    if setn is None : \n        setn = self . maxset + 1 \n    self . maxset = setn \n    if setn not in self . sets . keys ( ) : \n        self . sets [ setn ] = [ iname ] \n    else : \n        self . sets [ setn ] . append ( iname ) \n    self . components [ iname ] = filt \n    self . info [ iname ] = info \n    self . params [ iname ] = params \n    for a in self . analytes : \n        self . switches [ a ] [ iname ] = 0 \n    self . n += 1 \n    return "}
{"10020": "\ndef remove ( self , name = None , setn = None ) : \n    if isinstance ( name , int ) : \n        name = self . index [ name ] \n    if setn is not None : \n        name = self . sets [ setn ] \n        del self . sets [ setn ] \n    elif isinstance ( name , ( int , str ) ) : \n        name = [ name ] \n    if setn is 1 : \n        for n in name : \n            for k , v in self . sets . items ( ) : \n                if n in v : \n                    name . append ( [ m for m in v if m != n ] ) \n    for n in name : \n        for k , v in self . sets . items ( ) : \n            if n in v : \n                self . sets [ k ] = [ m for m in v if n != m ] \n        del self . components [ n ] \n        del self . info [ n ] \n        del self . params [ n ] \n        del self . keys [ n ] \n        for a in self . analytes : \n            del self . switches [ a ] [ n ] \n        return "}
{"10023": "\ndef fuzzmatch ( self , fuzzkey , multi = 0 ) : \n    keys , ratios = np . array ( [ ( f , seqm ( None , fuzzkey , f ) . ratio ( ) ) for f in self . components . keys ( ) ] ) . T \n    mratio = max ( ratios ) \n    if multi : \n        return keys [ ratios == mratio ] \n    else : \n        if sum ( ratios == mratio ) == 1 : \n            return keys [ ratios == mratio ] [ 0 ] \n        else : \n            raise ValueError ( \"\\nThe filter key provided ('{:}') matches two or more filter names equally well:\\n\" . format ( fuzzkey ) + ', ' . join ( keys [ ratios == mratio ] ) + \"\\nPlease be more specific!\" ) "}
{"10031": "\nasync def get_information ( ) : \n    jar = aiohttp . CookieJar ( unsafe = 1 ) \n    websession = aiohttp . ClientSession ( cookie_jar = jar ) \n    modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) \n    await modem . login ( password = sys . argv [ 2 ] ) \n    result = await modem . information ( ) \n    for sms in result . sms : \n        pprint . pprint ( sms ) \n    await modem . logout ( ) \n    await websession . close ( ) "}
{"10032": "\nasync def send_message ( ) : \n    jar = aiohttp . CookieJar ( unsafe = 1 ) \n    websession = aiohttp . ClientSession ( cookie_jar = jar ) \n    modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) \n    await modem . login ( password = sys . argv [ 2 ] ) \n    await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) \n    await modem . logout ( ) \n    await websession . close ( ) "}
{"10040": "\ndef process_notebook ( self , disable_warnings = 1 ) : \n    infile = self . infile \n    outfile = self . outfile \n    in_dir = os . path . dirname ( infile ) + os . path . sep \n    odir = os . path . dirname ( outfile ) + os . path . sep \n    create_dirs ( os . path . join ( odir , 'images' ) ) \n    ep = nbconvert . preprocessors . ExecutePreprocessor ( timeout = 300 ) \n    cp = nbconvert . preprocessors . ClearOutputPreprocessor ( timeout = 300 ) \n    self . nb = nb = nbformat . read ( infile , nbformat . current_nbformat ) \n    if disable_warnings : \n        for i , cell in enumerate ( nb . cells ) : \n            if cell [ 'cell_type' ] == 'code' : \n                cell = cell . copy ( ) \n                break \n        cell = cell . copy ( ) \n        cell . source = \"\"\"import logginglogging.captureWarnings(True)logging.getLogger('py.warnings').setLevel(logging.ERROR)\"\"\" \n        nb . cells . insert ( i , cell ) \n    if self . preprocess : \n        t = dt . datetime . now ( ) \n        logger . info ( 'Processing %s' , self . infile ) \n        try : \n            ep . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) \n        except nbconvert . preprocessors . execute . CellExecutionError : \n            logger . critical ( 'Error while processing %s!' , self . infile , exc_info = 1 ) \n        else : \n            logger . info ( 'Done. Seconds needed: %i' , ( dt . datetime . now ( ) - t ) . seconds ) \n        if disable_warnings : \n            nb . cells . pop ( i ) \n    self . py_file = self . get_out_file ( 'py' ) \n    if self . remove_tags : \n        tp = nbconvert . preprocessors . TagRemovePreprocessor ( timeout = 300 ) \n        for key , val in self . tag_options . items ( ) : \n            setattr ( tp , key , set ( val ) ) \n        nb4rst = deepcopy ( nb ) \n        tp . preprocess ( nb4rst , { 'metadata' : { 'path' : in_dir } } ) \n    else : \n        nb4rst = nb \n    self . create_rst ( nb4rst , in_dir , odir ) \n    if self . clear : \n        cp . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) \n    nbformat . write ( nb , outfile ) \n    self . create_py ( nb ) "}
{"10041": "\ndef create_py ( self , nb , force = 0 ) : \n    if list ( map ( int , re . findall ( '\\d+' , nbconvert . __version__ ) ) ) >= [ 4 , 2 ] : \n        py_file = os . path . basename ( self . py_file ) \n    else : \n        py_file = self . py_file \n    try : \n        level = logger . logger . level \n    except AttributeError : \n        level = logger . level \n    spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py_file , '--log-level=%s' % level , self . outfile ] ) \n    with open ( self . py_file ) as f : \n        py_content = f . read ( ) \n    py_content = re . sub ( '^\\s*get_ipython\\(\\).magic.*' , '# \\g<0>' , py_content , flags = re . MULTILINE ) \n    with open ( self . py_file , 'w' ) as f : \n        f . write ( py_content ) "}
{"10044": "\ndef get_description ( self ) : \n    def split_header ( s , get_header = 1 ) : \n        s = s . lstrip ( ) . rstrip ( ) \n        parts = s . splitlines ( ) \n        if parts [ 0 ] . startswith ( '#' ) : \n            if get_header : \n                header = re . sub ( '#+\\s*' , '' , parts . pop ( 0 ) ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ 0 ] . replace ( '\\n' , ' ' ) \n            return header , desc \n        else : \n            if get_header : \n                if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : \n                    parts = parts [ 1 : ] \n                header = parts . pop ( 0 ) \n                if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : \n                    parts . pop ( 0 ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ 0 ] . replace ( '\\n' , ' ' ) \n            return header , desc \n    first_cell = self . nb [ 'cells' ] [ 0 ] \n    if not first_cell [ 'cell_type' ] == 'markdown' : \n        return '' , '' \n    header , desc = split_header ( first_cell [ 'source' ] ) \n    if not desc and len ( self . nb [ 'cells' ] ) > 1 : \n        second_cell = self . nb [ 'cells' ] [ 1 ] \n        if second_cell [ 'cell_type' ] == 'markdown' : \n            _ , desc = split_header ( second_cell [ 'source' ] , 0 ) \n    return header , desc "}
{"10052": "\ndef pre_save ( self , model_instance , add ) : \n    file = getattr ( model_instance , self . attname ) \n    if file and not file . _committed : \n        image_file = file \n        if self . resize_source_to : \n            file . seek ( 0 ) \n            image_file = processors . process ( file , self . resize_source_to ) \n            image_file = post_processors . process ( image_file , self . resize_source_to ) \n        filename = str ( shortuuid . uuid ( ) ) + os . path . splitext ( file . name ) [ 1 ] \n        file . save ( filename , image_file , save = 0 ) \n    return file "}
{"10074": "\nasync def write ( self , towrite : bytes , await_blocking = 0 ) : \n    await self . _write ( towrite ) \n    if await_blocking : \n        return await self . flush ( ) "}
{"10075": "\nasync def readline ( self ) -> bytes : \n    while 1 : \n        line = self . _serial_instance . readline ( ) \n        if not line : \n            await asyncio . sleep ( self . _asyncio_sleep_time ) \n        else : \n            return line "}
{"10077": "\ndef as_string ( self , default_from = None ) : \n    encoding = self . charset or 'utf-8' \n    attachments = self . attachments or [ ] \n    if len ( attachments ) == 0 and not self . html : \n        msg = self . _mimetext ( self . body ) \n    elif len ( attachments ) > 0 and not self . html : \n        msg = MIMEMultipart ( ) \n        msg . attach ( self . _mimetext ( self . body ) ) \n    else : \n        msg = MIMEMultipart ( ) \n        alternative = MIMEMultipart ( 'alternative' ) \n        alternative . attach ( self . _mimetext ( self . body , 'plain' ) ) \n        alternative . attach ( self . _mimetext ( self . html , 'html' ) ) \n        msg . attach ( alternative ) \n    if self . charset : \n        msg [ 'Subject' ] = Header ( self . subject , encoding ) \n    else : \n        msg [ 'Subject' ] = self . subject \n    sender = self . sender or default_from \n    if sender is not None : \n        msg [ 'From' ] = sanitize_address ( sender , encoding ) \n    msg [ 'To' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . recipients , encoding ) ) ) ) \n    msg [ 'Date' ] = formatdate ( self . date , localtime = 1 ) \n    msg [ 'Message-ID' ] = self . msgId \n    if self . cc : \n        msg [ 'Cc' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . cc , encoding ) ) ) ) \n    if self . reply_to : \n        msg [ 'Reply-To' ] = sanitize_address ( self . reply_to , encoding ) \n    if self . extra_headers : \n        for k , v in self . extra_headers . items ( ) : \n            msg [ k ] = v \n    for attachment in attachments : \n        f = MIMEBase ( * attachment . content_type . split ( '/' ) ) \n        f . set_payload ( attachment . data ) \n        encode_base64 ( f ) \n        try : \n            attachment . filename and attachment . filename . encode ( 'ascii' ) \n        except UnicodeEncodeError : \n            filename = attachment . filename \n            if not PY3 : \n                filename = filename . encode ( 'utf8' ) \n            f . add_header ( 'Content-Disposition' , attachment . disposition , filename = ( 'UTF8' , '' , filename ) ) \n        else : \n            f . add_header ( 'Content-Disposition' , '%s;filename=%s' % ( attachment . disposition , attachment . filename ) ) \n        for key , value in attachment . headers : \n            f . add_header ( key , value ) \n        msg . attach ( f ) \n    return msg . as_string ( ) "}
{"10078": "\ndef has_bad_headers ( self , default_from = None ) : \n    sender = self . sender or default_from \n    reply_to = self . reply_to or '' \n    for val in [ self . subject , sender , reply_to ] + self . recipients : \n        for c in '\\r\\n' : \n            if c in val : \n                return 1 \n    return 0 "}
{"10089": "\ndef login ( self , username , password , generate = 'enabled' , proxies = None ) : \n    logger . debug ( \"login for: %s with generate: %s\" , username , generate ) \n    if not username or not password : \n        raise BackendException ( BACKEND_ERROR , \"Missing mandatory parameters\" ) \n    if proxies : \n        for key in proxies . keys ( ) : \n            try : \n                assert key in PROXY_PROTOCOLS \n            except AssertionError : \n                raise BackendException ( BACKEND_ERROR , \"Wrong proxy protocol \" , key ) \n    self . proxies = proxies \n    endpoint = 'login' \n    json = { u'username' : username , u'password' : password } \n    if generate == 'force' : \n        json [ 'action' ] = 'generate' \n        logger . debug ( \"Asking for generating new token\" ) \n    response = self . get_response ( method = 'POST' , endpoint = endpoint , json = json ) \n    if response . status_code == 401 : \n        logger . error ( \"Backend refused login with params %s\" , json ) \n        self . set_token ( token = None ) \n        return 0 \n    resp = self . decode ( response = response ) \n    if 'token' in resp : \n        self . set_token ( token = resp [ 'token' ] ) \n        return 1 \n    if generate == 'force' : \n        self . set_token ( token = None ) \n        raise BackendException ( BACKEND_ERROR , \"Token not provided\" ) \n    if generate == 'disabled' : \n        logger . error ( \"Token disabled ... to be implemented!\" ) \n        return 0 \n    if generate == 'enabled' : \n        logger . warning ( \"Token enabled, but none provided, require new token generation\" ) \n        return self . login ( username , password , 'force' ) \n    return 0 "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = 0 \n    items = [ ] \n    if self . processes == 1 : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = 1 \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if end > number_pages : \n                end = number_pages \n            begin += 1 \n            end += 1 \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10092": "\ndef patch ( self , endpoint , data , headers = None , inception = 0 ) : \n    if not headers : \n        raise BackendException ( BACKEND_ERROR , \"Header If-Match required for patching an object\" ) \n    response = self . get_response ( method = 'PATCH' , endpoint = endpoint , json = data , headers = headers ) \n    if response . status_code == 200 : \n        return self . decode ( response = response ) \n    if response . status_code == 412 : \n        if inception : \n            resp = self . get ( endpoint ) \n            headers = { 'If-Match' : resp [ '_etag' ] } \n            return self . patch ( endpoint , data = data , headers = headers , inception = 0 ) \n        raise BackendException ( response . status_code , response . content ) \n    else : \n        raise BackendException ( response . status_code , response . content ) "}
{"10095": "\ndef create ( source , link_name ) : \n    success = 0 \n    if not os . path . isdir ( source ) : \n        raise Exception ( \"%s is not a directory\" % source ) \n    if os . path . exists ( link_name ) : \n        raise Exception ( \"%s: junction link name already exists\" % link_name ) \n    link_name = os . path . abspath ( link_name ) \n    os . mkdir ( link_name ) \n    hlink = CreateFile ( link_name , fs . GENERIC_WRITE , fs . FILE_SHARE_READ | fs . FILE_SHARE_WRITE , None , fs . OPEN_EXISTING , fs . FILE_FLAG_OPEN_REPARSE_POINT | fs . FILE_FLAG_BACKUP_SEMANTICS , None ) \n    try : \n        if hlink == fs . INVALID_HANDLE_VALUE : \n            raise WinError ( ) \n        srcvolpath = unparsed_convert ( source ) \n        ( junctioninfo , infolen ) = new_junction_reparse_buffer ( srcvolpath ) \n        dummy = DWORD ( 0 ) \n        res = DeviceIoControl ( hlink , FSCTL_SET_REPARSE_POINT , byref ( junctioninfo ) , infolen , None , 0 , byref ( dummy ) , None ) \n        if res == 0 : \n            raise WinError ( ) \n        success = 1 \n    finally : \n        if hlink != fs . INVALID_HANDLE_VALUE : \n            CloseHandle ( hlink ) \n        if not success : \n            os . rmdir ( link_name ) "}
{"10106": "\ndef seek_next_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while 1 : \n        data_len , data = self . read ( self . read_size ) \n        data_where = 0 \n        if not data_len : \n            break \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ - 1 ] == b'\\r' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            terminator_len , terminator_data = self . read ( 1 ) \n            if terminator_len and terminator_data [ 0 ] == b'\\n' [ 0 ] : \n                data_len += 1 \n                data += b'\\n' \n            else : \n                self . file . seek ( terminator_where ) \n        while data_where < data_len : \n            terminator = self . prefix_line_terminator ( data [ data_where : ] ) \n            if terminator : \n                self . file . seek ( where + offset + data_where + len ( terminator ) ) \n                return self . file . tell ( ) \n            else : \n                data_where += 1 \n        offset += data_len \n        self . file . seek ( where + offset ) \n    return - 1 "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while 1 : \n        if offset == where : \n            break \n        read_size = self . read_size if self . read_size <= where else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ 0 ] == b'\\n' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            if terminator_where > data_len + 1 : \n                self . file . seek ( where - offset - data_len - 1 , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( 1 ) \n                if terminator_data [ 0 ] == b'\\r' [ 0 ] : \n                    data_len += 1 \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while data_where > 0 : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and offset == 0 and data_where == data_len : \n                data_where -= len ( terminator ) \n            elif terminator : \n                self . file . seek ( where - offset - ( data_len - data_where ) ) \n                return self . file . tell ( ) \n            else : \n                data_where -= 1 \n        offset += data_len \n    if where == 0 : \n        return - 1 \n    else : \n        self . file . seek ( 0 ) \n        return 0 "}
{"10110": "\ndef follow ( self ) : \n    trailing = 1 \n    while 1 : \n        where = self . file . tell ( ) \n        if where > os . fstat ( self . file . fileno ( ) ) . st_size : \n            where = 0 \n            self . file . seek ( where ) \n        line = self . file . readline ( ) \n        if line : \n            if trailing and line in self . LINE_TERMINATORS : \n                trailing = 0 \n                continue \n            terminator = self . suffix_line_terminator ( line ) \n            if terminator : \n                line = line [ : - len ( terminator ) ] \n            trailing = 0 \n            yield line \n        else : \n            trailing = 1 \n            self . file . seek ( where ) \n            yield None "}
{"10126": "\ndef model_fields_form_factory ( model ) : \n    fields = model . _meta . get_fields ( ) \n    choices = [ ] \n    for field in fields : \n        if hasattr ( field , \"verbose_name\" ) : \n            choices . append ( ( field . name , field . verbose_name ) ) \n    class ModelFieldsForm ( forms . Form ) : \n        fields = forms . MultipleChoiceField ( choices = choices , required = 0 , ) \n    return ModelFieldsForm "}
{"10129": "\ndef iter_osm_stream ( start_sqn = None , base_url = 'https://planet.openstreetmap.org/replication/minute' , expected_interval = 60 , parse_timestamps = 1 , state_dir = None ) : \n    if state_dir : \n        if not os . path . exists ( state_dir ) : \n            raise Exception ( 'Specified state_dir \"%s\" doesn\\'t exist.' % state_dir ) \n        if os . path . exists ( '%s/state.txt' % state_dir ) : \n            with open ( '%s/state.txt' % state_dir ) as f : \n                state = readState ( f ) \n                start_sqn = state [ 'sequenceNumber' ] \n    if not start_sqn : \n        u = urllib2 . urlopen ( '%s/state.txt' % base_url ) \n        state = readState ( u ) \n    else : \n        sqnStr = str ( start_sqn ) . zfill ( 9 ) \n        u = urllib2 . urlopen ( '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) ) \n        state = readState ( u ) \n    interval_fudge = 0.0 \n    while 1 : \n        sqnStr = state [ 'sequenceNumber' ] . zfill ( 9 ) \n        url = '%s/%s/%s/%s.osc.gz' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        content = urllib2 . urlopen ( url ) \n        content = StringIO . StringIO ( content . read ( ) ) \n        gzipper = gzip . GzipFile ( fileobj = content ) \n        for a in iter_osm_change_file ( gzipper , parse_timestamps ) : \n            yield a \n        stateTs = datetime . datetime . strptime ( state [ 'timestamp' ] , \"%Y-%m-%dT%H:%M:%SZ\" ) \n        yield ( None , model . Finished ( state [ 'sequenceNumber' ] , stateTs ) ) \n        nextTs = stateTs + datetime . timedelta ( seconds = expected_interval + interval_fudge ) \n        if datetime . datetime . utcnow ( ) < nextTs : \n            timeToSleep = ( nextTs - datetime . datetime . utcnow ( ) ) . total_seconds ( ) \n        else : \n            timeToSleep = 0.0 \n        time . sleep ( timeToSleep ) \n        sqnStr = str ( int ( state [ 'sequenceNumber' ] ) + 1 ) . zfill ( 9 ) \n        url = '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        delay = 1.0 \n        while 1 : \n            try : \n                u = urllib2 . urlopen ( url ) \n                interval_fudge -= ( interval_fudge / 2.0 ) \n                break \n            except urllib2 . HTTPError as e : \n                if e . code == 404 : \n                    time . sleep ( delay ) \n                    delay = min ( delay * 2 , 13 ) \n                    interval_fudge += delay \n        if state_dir : \n            with open ( '%s/state.txt' % state_dir , 'w' ) as f : \n                f . write ( u . read ( ) ) \n            with open ( '%s/state.txt' % state_dir , 'r' ) as f : \n                state = readState ( f ) \n        else : \n            state = readState ( u ) "}
{"10130": "\ndef parse_osm_file ( f , parse_timestamps = 1 ) : \n    nodes = [ ] \n    ways = [ ] \n    relations = [ ] \n    for p in iter_osm_file ( f , parse_timestamps ) : \n        if type ( p ) == model . Node : \n            nodes . append ( p ) \n        elif type ( p ) == model . Way : \n            ways . append ( p ) \n        elif type ( p ) == model . Relation : \n            relations . append ( p ) \n    return ( nodes , ways , relations ) "}
{"10131": "\ndef iter_osm_notes ( feed_limit = 25 , interval = 60 , parse_timestamps = 1 ) : \n    last_seen_guid = None \n    while 1 : \n        u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit ) \n        tree = etree . parse ( u ) \n        new_notes = [ ] \n        for note_item in tree . xpath ( '/rss/channel/item' ) : \n            title = note_item . xpath ( 'title' ) [ 0 ] . text \n            if title . startswith ( 'new note (' ) : \n                action = 'create' \n            elif title . startswith ( 'new comment (' ) : \n                action = 'comment' \n            elif title . startswith ( 'closed note (' ) : \n                action = 'close' \n            guid = note_item . xpath ( 'link' ) [ 0 ] . text \n            if last_seen_guid == guid : \n                break \n            elif last_seen_guid == None : \n                last_seen_guid = guid \n            else : \n                note_id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) \n                new_notes . append ( ( action , get_note ( note_id , parse_timestamps ) ) ) \n        for note in reversed ( new_notes ) : \n            yield note \n        yield model . Finished ( None , None ) \n        time . sleep ( interval ) "}
{"10133": "\ndef is_met ( self , user , filtered = 0 ) : \n    if filtered : \n        return 1 \n    return self . passes_filter ( user ) "}
{"10134": "\ndef user_quantity_remaining ( self , user , filtered = 1 ) : \n    if filtered : \n        if hasattr ( self . condition , \"remainder\" ) : \n            return self . condition . remainder \n    qs = type ( self . condition ) . objects . filter ( pk = self . condition . id ) \n    qs = self . pre_filter ( qs , user ) \n    if len ( qs ) > 0 : \n        return qs [ 0 ] . remainder \n    else : \n        return 0 "}
{"10138": "\ndef pre_filter ( self , queryset , user ) : \n    queryset = queryset . filter ( proposal_kind__proposalbase__presentation__cancelled = 0 ) \n    u = user \n    user_is_presenter = Q ( is_presenter = 1 , proposal_kind__proposalbase__presentation__speaker__user = u , ) \n    user_is_copresenter = Q ( is_copresenter = 1 , proposal_kind__proposalbase__presentation__additional_speakers__user = u , ) \n    return queryset . filter ( user_is_presenter | user_is_copresenter ) "}
{"10140": "\ndef _modifies_cart ( func ) : \n    \n    @ functools . wraps ( func ) \n    def inner ( self , * a , ** k ) : \n        self . _fail_if_cart_is_not_active ( ) \n        with transaction . atomic ( ) : \n            with BatchController . batch ( self . cart . user ) : \n                memoised = self . for_user ( self . cart . user ) \n                memoised . _modified_by_batch = 1 \n                return func ( self , * a , ** k ) \n    return inner "}
{"10164": "\ndef speaker_registrations ( request , form ) : \n    kinds = form . cleaned_data [ \"kind\" ] \n    presentations = schedule_models . Presentation . objects . filter ( proposal_base__kind__in = kinds , ) . exclude ( cancelled = 1 , ) \n    users = User . objects . filter ( Q ( speaker_profile__presentations__in = presentations ) | Q ( speaker_profile__copresentations__in = presentations ) ) \n    paid_carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS_PAID ) \n    paid_carts = Case ( When ( cart__in = paid_carts , then = Value ( 1 ) ) , default = Value ( 0 ) , output_field = models . IntegerField ( ) , ) \n    users = users . annotate ( paid_carts = Sum ( paid_carts ) ) \n    users = users . order_by ( \"paid_carts\" ) \n    return QuerysetReport ( \"Speaker Registration Status\" , [ \"id\" , \"speaker_profile__name\" , \"email\" , \"paid_carts\" ] , users , link_view = attendee , ) \n    return [ ] "}
{"10169": "\ndef guided_registration ( request , page_number = None ) : \n    PAGE_PROFILE = 1 \n    PAGE_TICKET = 2 \n    PAGE_PRODUCTS = 3 \n    PAGE_PRODUCTS_MAX = 4 \n    TOTAL_PAGES = 4 \n    ticket_category = inventory . Category . objects . get ( id = settings . TICKET_PRODUCT_CATEGORY ) \n    cart = CartController . for_user ( request . user ) \n    attendee = people . Attendee . get_instance ( request . user ) \n    if attendee . completed_registration : \n        return redirect ( review ) \n    has_profile = hasattr ( attendee , \"attendeeprofilebase\" ) \n    if not has_profile : \n        max_page = PAGE_PROFILE \n        redirect_page = PAGE_PROFILE \n    else : \n        products = inventory . Product . objects . filter ( productitem__cart = cart . cart ) \n        products = products . filter ( category = ticket_category ) \n        if products . count ( ) == 0 : \n            max_page = PAGE_TICKET \n            redirect_page = PAGE_TICKET \n        else : \n            max_page = PAGE_PRODUCTS_MAX \n            redirect_page = PAGE_PRODUCTS \n    if page_number is None or int ( page_number ) > max_page : \n        return redirect ( \"guided_registration\" , redirect_page ) \n    page_number = int ( page_number ) \n    next_step = redirect ( \"guided_registration\" , page_number + 1 ) \n    with BatchController . batch ( request . user ) : \n        available = ProductController . available_products ( request . user , category = ticket_category ) \n        if not available : \n            messages . error ( request , \"There are no more tickets available.\" ) \n            return redirect ( \"dashboard\" ) \n        sections = [ ] \n        if page_number == PAGE_PROFILE : \n            title = \"Attendee information\" \n            sections = _guided_registration_profile_and_voucher ( request ) \n        elif page_number == PAGE_TICKET : \n            title = \"Select ticket type\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_TICKETS_ONLY ) \n        elif page_number == PAGE_PRODUCTS : \n            title = \"Additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_ALL_ADDITIONAL ) \n        elif page_number == PAGE_PRODUCTS_MAX : \n            title = \"More additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_EXCLUDE_COMPLETE ) \n        if not sections : \n            attendee . completed_registration = 1 \n            attendee . save ( ) \n            return redirect ( \"review\" ) \n        if sections and request . method == \"POST\" : \n            for section in sections : \n                if section . form . errors : \n                    break \n            else : \n                return next_step \n    data = { \"current_step\" : page_number , \"sections\" : sections , \"title\" : title , \"total_steps\" : TOTAL_PAGES , } \n    return render ( request , \"registrasion/guided_registration.html\" , data ) "}
{"10171": "\ndef _handle_profile ( request , prefix ) : \n    attendee = people . Attendee . get_instance ( request . user ) \n    try : \n        profile = attendee . attendeeprofilebase \n        profile = people . AttendeeProfileBase . objects . get_subclass ( pk = profile . id , ) \n    except ObjectDoesNotExist : \n        profile = None \n    try : \n        speaker_profile = request . user . speaker_profile \n        speaker_name = speaker_profile . name \n    except ObjectDoesNotExist : \n        speaker_name = None \n    name_field = ProfileForm . Meta . model . name_field ( ) \n    initial = { } \n    if profile is None and name_field is not None : \n        initial [ name_field ] = speaker_name \n    form = ProfileForm ( request . POST or None , initial = initial , instance = profile , prefix = prefix ) \n    handled = 1 if request . POST else 0 \n    if request . POST and form . is_valid ( ) : \n        form . instance . attendee = attendee \n        form . save ( ) \n    return form , handled "}
{"10173": "\ndef _handle_products ( request , category , products , prefix ) : \n    current_cart = CartController . for_user ( request . user ) \n    ProductsForm = forms . ProductsForm ( category , products ) \n    items = commerce . ProductItem . objects . filter ( product__in = products , cart = current_cart . cart , ) . select_related ( \"product\" ) \n    quantities = [ ] \n    seen = set ( ) \n    for item in items : \n        quantities . append ( ( item . product , item . quantity ) ) \n        seen . add ( item . product ) \n    zeros = set ( products ) - seen \n    for product in zeros : \n        quantities . append ( ( product , 0 ) ) \n    products_form = ProductsForm ( request . POST or None , product_quantities = quantities , prefix = prefix , ) \n    if request . method == \"POST\" and products_form . is_valid ( ) : \n        if products_form . has_changed ( ) : \n            _set_quantities_from_products_form ( products_form , current_cart ) \n        if category . required : \n            carts = commerce . Cart . objects . filter ( user = request . user ) \n            items = commerce . ProductItem . objects . filter ( product__category = category , cart = carts , ) \n            if len ( items ) == 0 : \n                products_form . add_error ( None , \"You must have at least one item from this category\" , ) \n    handled = 0 if products_form . errors else 1 \n    discounts = util . lazy ( DiscountController . available_discounts , request . user , [ ] , products , ) \n    return products_form , discounts , handled "}
{"10174": "\ndef _handle_voucher ( request , prefix ) : \n    voucher_form = forms . VoucherForm ( request . POST or None , prefix = prefix ) \n    current_cart = CartController . for_user ( request . user ) \n    if ( voucher_form . is_valid ( ) and voucher_form . cleaned_data [ \"voucher\" ] . strip ( ) ) : \n        voucher = voucher_form . cleaned_data [ \"voucher\" ] \n        voucher = inventory . Voucher . normalise_code ( voucher ) \n        if len ( current_cart . cart . vouchers . filter ( code = voucher ) ) > 0 : \n            handled = 0 \n        else : \n            try : \n                current_cart . apply_voucher ( voucher ) \n            except Exception as e : \n                voucher_form . add_error ( \"voucher\" , e ) \n            handled = 1 \n    else : \n        handled = 0 \n    return ( voucher_form , handled ) "}
{"10186": "\ndef available_discounts ( cls , user , categories , products ) : \n    filtered_clauses = cls . _filtered_clauses ( user ) \n    categories = set ( categories ) \n    products = set ( products ) \n    product_categories = set ( product . category for product in products ) \n    all_categories = categories | product_categories \n    filtered_clauses = ( clause for clause in filtered_clauses if hasattr ( clause , 'product' ) and clause . product in products or hasattr ( clause , 'category' ) and clause . category in all_categories ) \n    discounts = [ ] \n    accepted_discounts = set ( ) \n    failed_discounts = set ( ) \n    for clause in filtered_clauses : \n        discount = clause . discount \n        cond = ConditionController . for_condition ( discount ) \n        past_use_count = clause . past_use_count \n        if past_use_count >= clause . quantity : \n            pass \n        elif discount not in failed_discounts : \n            is_accepted = discount in accepted_discounts \n            if is_accepted or cond . is_met ( user , filtered = 1 ) : \n                discounts . append ( DiscountAndQuantity ( discount = discount , clause = clause , quantity = clause . quantity - past_use_count , ) ) \n                accepted_discounts . add ( discount ) \n            else : \n                failed_discounts . add ( discount ) \n    return discounts "}
{"10198": "\ndef can_view ( self , user = None , access_code = None ) : \n    if user == self . invoice . user : \n        return 1 \n    if user . is_staff : \n        return 1 \n    if self . invoice . user . attendee . access_code == access_code : \n        return 1 \n    return 0 "}
{"10203": "\ndef _invoice_matches_cart ( self ) : \n    self . _refresh ( ) \n    cart = self . invoice . cart \n    if not cart : \n        return 1 \n    return cart . revision == self . invoice . cart_revision "}
{"10204": "\ndef update_validity ( self ) : \n    is_valid = self . _invoice_matches_cart ( ) \n    cart = self . invoice . cart \n    if self . invoice . is_unpaid and is_valid and cart : \n        try : \n            CartController ( cart ) . validate_cart ( ) \n        except ValidationError : \n            is_valid = 0 \n    if not is_valid : \n        if self . invoice . total_payments ( ) > 0 : \n            self . refund ( ) \n        else : \n            self . void ( ) "}
{"10212": "\ndef project_data ( self , project ) : \n    projobjects = self . cache [ 'project_objects' ] \n    objects = self . cache [ 'objects' ] \n    project_id = str ( project ) \n    if not re . match ( '^[0-9a-fA-F]{24}$' , project_id ) : \n        projects = self . api . case . get ( url_slug = project_id ) [ 'objects' ] \n        if len ( projects ) != 1 : \n            raise ValueError ( msg = 'Attribute project not a slug or ObjectId: {}' . format ( project_id ) ) \n        project_id = str ( projects [ 0 ] [ 'id' ] ) \n    if project_id not in projobjects : \n        projobjects [ project_id ] = [ ] \n        data = self . api . data . get ( case_ids__contains = project_id ) [ 'objects' ] \n        for d in data : \n            _id = d [ 'id' ] \n            if _id in objects : \n                objects [ _id ] . update ( d ) \n            else : \n                objects [ _id ] = GenData ( d , self ) \n            projobjects [ project_id ] . append ( objects [ _id ] ) \n        for d in projobjects [ project_id ] : \n            while 1 : \n                ref_annotation = { } \n                remove_annotation = [ ] \n                for path , ann in d . annotation . items ( ) : \n                    if ann [ 'type' ] . startswith ( 'data:' ) : \n                        if ann [ 'value' ] in self . cache [ 'objects' ] : \n                            annotation = self . cache [ 'objects' ] [ ann [ 'value' ] ] . annotation \n                            ref_annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) \n                        remove_annotation . append ( path ) \n                if ref_annotation : \n                    d . annotation . update ( ref_annotation ) \n                    for path in remove_annotation : \n                        del d . annotation [ path ] \n                else : \n                    break \n    return projobjects [ project_id ] "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while 1 : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if i > 0 and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    print ( ) \n    return session_id "}
{"10218": "\ndef download ( self , data_objects , field ) : \n    if not field . startswith ( 'output' ) : \n        raise ValueError ( \"Only processor results (output.* fields) can be downloaded\" ) \n    for o in data_objects : \n        o = str ( o ) \n        if re . match ( '^[0-9a-fA-F]{24}$' , o ) is None : \n            raise ValueError ( \"Invalid object id {}\" . format ( o ) ) \n        if o not in self . cache [ 'objects' ] : \n            self . cache [ 'objects' ] [ o ] = GenData ( self . api . data ( o ) . get ( ) , self ) \n        if field not in self . cache [ 'objects' ] [ o ] . annotation : \n            raise ValueError ( \"Download field {} does not exist\" . format ( field ) ) \n        ann = self . cache [ 'objects' ] [ o ] . annotation [ field ] \n        if ann [ 'type' ] != 'basic:file:' : \n            raise ValueError ( \"Only basic:file: field can be downloaded\" ) \n    for o in data_objects : \n        ann = self . cache [ 'objects' ] [ o ] . annotation [ field ] \n        url = urlparse . urljoin ( self . url , 'data/{}/{}' . format ( o , ann [ 'value' ] [ 'file' ] ) ) \n        yield requests . get ( url , stream = 1 , auth = self . auth ) "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if prev_q [ 'chr' ] < current_q [ 'chr' ] : \n            return 1 \n        if prev_q [ 'chr' ] > current_q [ 'chr' ] : \n            return 0 \n        if prev_q [ 'start' ] < current_q [ 'start' ] : \n            return 1 \n        if prev_q [ 'start' ] > current_q [ 'start' ] : \n            return 0 \n        if prev_q [ 'stop' ] < current_q [ 'stop' ] : \n            return 1 \n        if prev_q [ 'stop' ] > current_q [ 'stop' ] : \n            return 0 \n        return 1 \n    ct_pointer = 0 \n    query_pointer = 0 \n    last_query_pointer = - 1 \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while query_pointer < len ( sorted_queries ) and ct_pointer < len ( ct ) : \n        if last_query_pointer != query_pointer : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if q_chr < c_chr : \n            query_pointer += 1 \n            continue \n        if q_chr > c_chr : \n            ct_pointer += 1 \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if q_start > c_stop : \n            ct_pointer += 1 \n            continue \n        if q_stop < c_start : \n            query_pointer += 1 \n            continue \n        if search_mode == 'any' : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        elif search_mode == 'exact' and q_start == c_start and q_stop == c_stop : \n            q_alt = q . alt \n            c_alt = c . alt \n            if not ( q_alt and c_alt and q_alt != c_alt ) : \n                matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n        elif search_mode == 'include_smaller' : \n            raise NotImplementedError \n        elif search_mode == 'include_larger' : \n            raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer += 1 \n    return dict ( matches ) "}
{"10225": "\ndef update ( self , allow_partial = 1 , force = 0 , ** kwargs ) : \n    if kwargs : \n        self . __init__ ( partial = allow_partial , force = force , ** kwargs ) \n        return not self . _partial \n    if not force and CACHE . get ( hash ( self ) ) : \n        cached = CACHE [ hash ( self ) ] \n        for field in self . _SIMPLE_FIELDS | self . _COMPLEX_FIELDS : \n            v = getattr ( cached , field ) \n            setattr ( self , field , v ) \n        self . _partial = 0 \n        logging . info ( f'Loading {str(self)} from cache' ) \n        return 1 \n    resp_dict = element_lookup_by_id ( self . type , self . id ) \n    self . __init__ ( partial = 0 , ** resp_dict ) \n    return 1 "}
{"10227": "\ndef authenticate ( self ) : \n    if self . oauth : \n        return 0 \n    self . settings . apply ( 'api-asana' , self . args . asana_api , \"enter asana api key\" ) \n    self . settings . apply ( 'api-github' , self . args . github_api , \"enter github.com token\" ) \n    logging . debug ( \"authenticating asana api.\" ) \n    self . asana = Client . basic_auth ( self . settings [ 'api-asana' ] ) \n    self . asana_errors = asana_errors \n    self . asana_me = self . asana . users . me ( ) \n    logging . debug ( \"authenticating github api\" ) \n    self . github = Github ( self . settings [ 'api-github' ] ) \n    self . github_user = self . github . get_user ( ) \n    self . oauth = 1 "}
{"10228": "\ndef _list_select ( cls , lst , prompt , offset = 0 ) : \n    inp = raw_input ( \"select %s: \" % prompt ) \n    assert inp , \"value required.\" \n    try : \n        return lst [ int ( inp ) + offset ] \n    except ValueError : \n        return inp \n    except IndexError : \n        assert 0 , \"bad value.\" "}
{"10233": "\ndef save ( self ) : \n    with open ( self . filename , 'wb' ) as file : \n        self . prune ( ) \n        self . data [ 'version' ] = self . version \n        json . dump ( self . data , file , sort_keys = 1 , indent = 2 ) "}
{"10236": "\ndef flush ( callback = None ) : \n    while 1 : \n        if shutdown_event . is_set ( ) : \n            return \n        if callable ( callback ) : \n            callback ( ) \n        try : \n            item = queue . get ( timeout = 1 ) \n            queue . put ( item ) \n        except Queue . Empty : \n            return "}
{"10242": "\ndef initPort ( self ) : \n    try : \n        self . m_ser = serial . Serial ( port = self . m_ttyport , baudrate = self . m_baudrate , timeout = 0 , parity = serial . PARITY_EVEN , stopbits = serial . STOPBITS_ONE , bytesize = serial . SEVENBITS , rtscts = 0 ) \n        ekm_log ( \"Pyserial version = \" + serial . VERSION ) \n        ekm_log ( \"Port = \" + self . m_ttyport ) \n        ekm_log ( \"Rate = \" + str ( self . m_baudrate ) ) \n        time . sleep ( self . m_init_wait ) \n        return 1 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return 0 "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if period < 1 or period > 3 : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( 0 ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10249": "\ndef setMeterPassword ( self , new_pwd , pwd = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setMeterPassword\" ) \n    try : \n        if len ( new_pwd ) != 8 or len ( pwd ) != 8 : \n            self . writeCmdMsg ( \"Passwords must be exactly eight characters.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( 0 ) : \n            self . writeCmdMsg ( \"Pre command read failed: check serial line.\" ) \n        else : \n            if not self . serialCmdPwdAuth ( pwd ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_pwd = binascii . hexlify ( new_pwd . zfill ( 8 ) ) \n                req_str = \"015731023030323028\" + req_pwd + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMeterPassword): 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = 0 \n    if kwh_scale == ScaleKWH . EmptyScale : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count += 1 \n            continue \n        if len ( contents ) == 0 : \n            count += 1 \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if fld_type == FieldType . Float : \n                float_data = float ( str ( raw_data ) ) \n                divisor = 1 \n                if fld_scale == ScaleType . KWH : \n                    divisor = 1 \n                    if kwh_scale == ScaleKWH . Scale10 : \n                        divisor = 10 \n                    elif kwh_scale == ScaleKWH . Scale100 : \n                        divisor = 100 \n                    elif ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : \n                        ekm_log ( \"Unrecognized kwh scale.\" ) \n                elif fld_scale == ScaleType . Div10 : \n                    divisor = 10 \n                elif fld_scale == ScaleType . Div100 : \n                    divisor = 100 \n                elif fld_scale != ScaleType . No : \n                    ekm_log ( \"Unrecognized float scale.\" ) \n                float_data /= divisor \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            elif fld_type == FieldType . Hex : \n                hex_data = raw_data . encode ( 'hex' ) \n                def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n            elif fld_type == FieldType . Int : \n                integer_data = int ( raw_data ) \n                integer_data_str = str ( integer_data ) \n                if len ( integer_data_str ) == 0 : \n                    integer_data_str = str ( 0 ) \n                def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n            elif fld_type == FieldType . String : \n                string_data = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n            elif fld_type == FieldType . PowerFactor : \n                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n            else : \n                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count += 1 \n    return 1 "}
{"10253": "\ndef crcMeterRead ( self , raw_read , def_buf ) : \n    try : \n        if len ( raw_read ) == 0 : \n            ekm_log ( \"(\" + self . m_context + \") Empty return read.\" ) \n            return 0 \n        sent_crc = self . calc_crc16 ( raw_read [ 1 : - 2 ] ) \n        logstr = \"(\" + self . m_context + \")CRC sent = \" + str ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] ) \n        logstr += \" CRC calc = \" + sent_crc \n        ekm_log ( logstr ) \n        if int ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] , 16 ) == int ( sent_crc , 16 ) : \n            return 1 \n    except struct . error : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return 0 \n    except TypeError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return 0 \n    except ValueError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return 0 \n    return 0 "}
{"10256": "\ndef setCTRatio ( self , new_ct , password = \"00000000\" ) : \n    ret = 0 \n    self . setContext ( \"setCTRatio\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if ( ( new_ct != CTRatio . Amps_100 ) and ( new_ct != CTRatio . Amps_200 ) and ( new_ct != CTRatio . Amps_400 ) and ( new_ct != CTRatio . Amps_600 ) and ( new_ct != CTRatio . Amps_800 ) and ( new_ct != CTRatio . Amps_1000 ) and ( new_ct != CTRatio . Amps_1200 ) and ( new_ct != CTRatio . Amps_1500 ) and ( new_ct != CTRatio . Amps_2000 ) and ( new_ct != CTRatio . Amps_3000 ) and ( new_ct != CTRatio . Amps_4000 ) and ( new_ct != CTRatio . Amps_5000 ) ) : \n            self . writeCmdMsg ( \"Legal CT Ratios: 100, 200, 400, 600, \" + \"800, 1000, 1200, 1500, 2000, 3000, 4000 and 5000\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if not self . request ( 0 ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030443028\" + binascii . hexlify ( str ( new_ct ) . zfill ( 4 ) ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setCTRatio): 06 returned.\" ) \n                    ret = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return ret "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( hour < 0 ) or ( hour > 23 ) or ( minute < 0 ) or ( minute > 59 ) or ( tariff < 0 ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + 1 ) ) \n        return 0 \n    period += 1 \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return 0 \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return 0 \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return 0 \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return 1 "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season += 1 \n    schedule += 1 \n    if ( ( season < 1 ) or ( season > Extents . Seasons ) or ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( month > 12 ) or ( month < 0 ) or ( day < 0 ) or ( day > 31 ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return 0 \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return 0 \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return 0 \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return 0 \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return 1 "}
{"10259": "\ndef setSeasonSchedules ( self , cmd_dict = None , password = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setSeasonSchedules\" ) \n    if not cmd_dict : \n        cmd_dict = self . m_seasons_sched_params \n    try : \n        if not self . request ( 0 ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( 0 ) . zfill ( 24 ) ) \n                req_str = \"015731023030383028\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setSeasonSchedules): 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday += 1 \n    if ( month > 12 ) or ( month < 0 ) or ( day > 31 ) or ( day < 0 ) or ( holiday < 1 ) or ( holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return 0 \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return 0 \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return 0 \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return 1 "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( 0 ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if tableset == ReadSchedules . Schedules_1_To_4 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return 1 \n        elif tableset == ReadSchedules . Schedules_5_To_6 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n            self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                self . setContext ( \"\" ) \n                return 1 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return 0 "}
{"10263": "\ndef readMonthTariffs ( self , months_type ) : \n    self . setContext ( \"readMonthTariffs\" ) \n    try : \n        req_type = binascii . hexlify ( str ( months_type ) . zfill ( 1 ) ) \n        req_str = \"01523102303031\" + req_type + \"282903\" \n        work_table = self . m_mons \n        if months_type == ReadMonths . kWhReverse : \n            work_table = self . m_rev_mons \n        self . request ( 0 ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , work_table ) \n        self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if str ( return_crc ) == str ( work_table [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Months CRC success, type = \" + str ( req_type ) ) \n            self . setContext ( \"\" ) \n            return 1 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return 0 "}
{"10265": "\ndef readHolidayDates ( self ) : \n    self . setContext ( \"readHolidayDates\" ) \n    try : \n        req_str = \"0152310230304230282903\" \n        self . request ( 0 ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) \n        self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if str ( return_crc ) == str ( self . m_hldy [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Holidays and Schedules CRC success\" ) \n            self . setContext ( \"\" ) \n            return 1 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return 0 "}
{"10269": "\ndef serialCmdPwdAuth ( self , password_str ) : \n    result = 0 \n    try : \n        req_start = \"0150310228\" + binascii . hexlify ( password_str ) + \"2903\" \n        req_crc = self . calc_crc16 ( req_start [ 2 : ] . decode ( \"hex\" ) ) \n        req_str = req_start + req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n            ekm_log ( \"Password accepted (\" + self . getContext ( ) + \")\" ) \n            result = 1 \n        else : \n            ekm_log ( \"Password call failure no 06(\" + self . getContext ( ) + \")\" ) \n    except : \n        ekm_log ( \"Password call failure by exception(\" + self . getContext ( ) + \")\" ) \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10272": "\ndef request ( self , send_terminator = 0 ) : \n    try : \n        retA = self . requestA ( ) \n        retB = self . requestB ( ) \n        if retA and retB : \n            self . makeAB ( ) \n            self . calculateFields ( ) \n            self . updateObservers ( ) \n            return 1 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return 0 "}
{"10277": "\ndef setLCDCmd ( self , display_list , password = \"00000000\" ) : \n    result = 0 \n    try : \n        self . initLcd ( ) \n        item_cnt = len ( display_list ) \n        if ( item_cnt > 45 ) or ( item_cnt <= 0 ) : \n            ekm_log ( \"LCD item list must have between 1 and 40 items\" ) \n            return 0 \n        for display_item in display_list : \n            self . addLcdItem ( int ( display_item ) ) \n        result = self . setLCD ( password ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if seconds < 0 or seconds > 9999 : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"\" \n                req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + \"2903\" ) \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10280": "\ndef setPulseInputRatio ( self , line_in , new_cnst , password = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setPulseInputRatio\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4 ) ) \n                line_const = binascii . hexlify ( str ( line_in - 1 ) ) \n                req_str = \"01573102303041\" + line_const + \"28\" + req_const + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10281": "\ndef setZeroResettableKWH ( self , password = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setZeroResettableKWH\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"0157310230304433282903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = 0 \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                fill_len = 40 - len ( self . m_lcd_items ) \n                for lcdid in self . m_lcd_items : \n                    append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) \n                    req_table += append_val \n                for i in range ( 0 , fill_len ) : \n                    append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2 ) ) \n                    req_table += append_val \n                req_str = \"015731023030443228\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = 1 \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10285": "\ndef paragraphs ( quantity = 2 , separator = '\\n\\n' , wrap_start = '' , wrap_end = '' , html = 0 , sentences_quantity = 3 , as_list = 0 ) : \n    if html : \n        wrap_start = '<p>' \n        wrap_end = '</p>' \n        separator = '\\n\\n' \n    result = [ ] \n    for i in xrange ( 0 , quantity ) : \n        result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) \n    if as_list : \n        return result \n    else : \n        return separator . join ( result ) "}
{"10286": "\ndef text ( length = None , at_least = 10 , at_most = 15 , lowercase = 1 , uppercase = 1 , digits = 1 , spaces = 1 , punctuation = 0 ) : \n    base_string = '' \n    if lowercase : \n        base_string += string . ascii_lowercase \n    if uppercase : \n        base_string += string . ascii_uppercase \n    if digits : \n        base_string += string . digits \n    if spaces : \n        base_string += ' ' \n    if punctuation : \n        base_string += string . punctuation \n    if len ( base_string ) == 0 : \n        return '' \n    if not length : \n        length = random . randint ( at_least , at_most ) \n    result = '' \n    for i in xrange ( 0 , length ) : \n        result += random . choice ( base_string ) \n    return result "}
{"10293": "\ndef run ( config ) : \n    setup ( config ) \n    if config . exitfirst : \n        ivoire . current_result . failfast = 1 \n    ivoire . current_result . startTestRun ( ) \n    for spec in config . specs : \n        try : \n            load_by_name ( spec ) \n        except Exception : \n            ivoire . current_result . addError ( _ExampleNotRunning ( ) , sys . exc_info ( ) ) \n    ivoire . current_result . stopTestRun ( ) \n    sys . exit ( not ivoire . current_result . wasSuccessful ( ) ) "}
{"10301": "\ndef source_to_code ( self , source_bytes , source_path ) : \n    node = ast . parse ( source_bytes ) \n    transformed = ExampleTransformer ( ) . transform ( node ) \n    return compile ( transformed , source_path , \"exec\" , dont_inherit = 1 ) "}
{"10321": "\ndef dereference ( self , callback = None , args = None , kwargs = None ) : \n    if args is None : \n        args = tuple ( ) \n    if kwargs is None : \n        kwargs = { } \n    client = self . conn . client \n    should_execute = 0 \n    if self . force_expiry : \n        should_execute = 1 \n    if not should_execute : \n        self . nodelist . remove_node ( self . conn . id ) \n        self . nodelist . remove_expired_nodes ( ) \n        updated_refcount = client . incr ( self . refcount_key , - 1 ) \n        should_execute = ( updated_refcount <= 0 ) \n    try : \n        if callable ( callback ) and should_execute : \n            callback ( * args , ** kwargs ) \n    finally : \n        if should_execute : \n            client . delete ( self . resource_key , self . nodelist . nodelist_key , self . times_modified_key , self . refcount_key ) \n        self . conn . remove_from_registry ( self . resource_key ) \n    return should_execute "}
{"10340": "\ndef close ( self ) : \n    if self . closed : \n        raise ValueError ( \"Cannot close a closed state\" ) \n    if self . call is not None : \n        self . call . cancel ( ) \n    self . closed = 1 "}
{"10341": "\ndef check ( self ) : \n    if self . closed : \n        raise ValueError ( \"Cannot check a closed state\" ) \n    self . _maybeReset ( ) \n    if self . url is None : \n        return 0 \n    return self . _maybeCheck ( ) "}
{"10344": "\ndef freeze_from_checkpoint ( input_checkpoint , output_file_path , output_node_names ) : \n    check_input_checkpoint ( input_checkpoint ) \n    output_node_names = output_node_names_string_as_list ( output_node_names ) \n    with tf . Session ( ) as sess : \n        restore_from_checkpoint ( sess , input_checkpoint ) \n        freeze_graph . freeze_graph_with_def_protos ( input_graph_def = sess . graph_def , input_saver_def = None , input_checkpoint = input_checkpoint , output_node_names = ',' . join ( output_node_names ) , restore_op_name = 'save/restore_all' , filename_tensor_name = 'save/Const:0' , output_graph = output_file_path , clear_devices = 1 , initializer_nodes = '' ) "}
{"10346": "\ndef save_graph_only ( sess , output_file_path , output_node_names , as_text = 0 ) : \n    for node in sess . graph_def . node : \n        node . device = '' \n    graph_def = graph_util . extract_sub_graph ( sess . graph_def , output_node_names ) \n    output_dir , output_filename = os . path . split ( output_file_path ) \n    graph_io . write_graph ( graph_def , output_dir , output_filename , as_text = as_text ) "}
{"10347": "\ndef save_graph_only_from_checkpoint ( input_checkpoint , output_file_path , output_node_names , as_text = 0 ) : \n    check_input_checkpoint ( input_checkpoint ) \n    output_node_names = output_node_names_string_as_list ( output_node_names ) \n    with tf . Session ( ) as sess : \n        restore_from_checkpoint ( sess , input_checkpoint ) \n        save_graph_only ( sess , output_file_path , output_node_names , as_text = as_text ) "}
{"10356": "\ndef caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = 'Graph' , conversion_out_dir_path = None , use_padding_same = 0 ) : \n    try : \n        from caffeflow import convert \n    except ImportError : \n        raise Exception ( \"caffeflow package needs to be installed to freeze Caffe models. Check out the README file.\" ) \n    with ( dummy_context_mgr ( conversion_out_dir_path ) or util . TemporaryDirectory ( ) ) as dir_path : \n        params_values_output_path = os . path . join ( dir_path , 'params_values.npy' ) \n        network_output_path = os . path . join ( dir_path , 'network.py' ) \n        convert . convert ( caffe_def_path , caffemodel_path , params_values_output_path , network_output_path , 0 , use_padding_same = use_padding_same ) \n        network_module = imp . load_source ( 'module.name' , network_output_path ) \n        network_class = getattr ( network_module , graph_name ) \n        network = network_class ( inputs ) \n        sess = tf . Session ( ) \n        network . load ( params_values_output_path , sess ) \n        return sess "}
{"10357": "\ndef freeze ( caffe_def_path , caffemodel_path , inputs , output_file_path , output_node_names , graph_name = 'Graph' , conversion_out_dir_path = None , checkpoint_out_path = None , use_padding_same = 0 ) : \n    with caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = graph_name , conversion_out_dir_path = conversion_out_dir_path , use_padding_same = use_padding_same ) as sess : \n        saver = tf . train . Saver ( ) \n        with ( dummy_context_mgr ( checkpoint_out_path ) or util . TemporaryDirectory ( ) ) as temp_dir_path : \n            checkpoint_path = checkpoint_out_path or os . path . join ( temp_dir_path , 'pose.ckpt' ) \n            saver . save ( sess , checkpoint_path ) \n            output_node_names = util . output_node_names_string_as_list ( output_node_names ) \n            tf_freeze . freeze_from_checkpoint ( checkpoint_path , output_file_path , output_node_names ) "}
{"10358": "\ndef save_graph_only ( caffe_def_path , caffemodel_path , inputs , output_file_path , output_node_names , graph_name = 'Graph' , use_padding_same = 0 ) : \n    with caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = graph_name , use_padding_same = use_padding_same ) as sess : \n        tf_freeze . save_graph_only ( sess , output_file_path , output_node_names ) "}
{"10361": "\ndef every_other ( iterable ) : \n    items = iter ( iterable ) \n    while 1 : \n        try : \n            yield next ( items ) \n            next ( items ) \n        except StopIteration : \n            return "}
{"10364": "\ndef takewhile_peek ( predicate , iterable ) : \n    while 1 : \n        try : \n            if not predicate ( iterable . peek ( ) ) : \n                break \n            yield next ( iterable ) \n        except StopIteration : \n            break "}
{"10379": "\ndef select ( selector , obj ) : \n    parser = Parser ( obj ) \n    try : \n        return parser . parse ( selector ) \n    except SelectorSyntaxError as e : \n        log . exception ( e ) \n        return 0 "}
{"10383": "\ndef ancestors ( self , lhs , rhs ) : \n    def _search ( node ) : \n        if node in lhs : \n            return 1 \n        if not node . parent : \n            return 0 \n        return _search ( node . parent ) \n    return [ node for node in rhs if _search ( node ) ] "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = 1 if pat . group ( 5 ) == 'odd' else 0 \n    elif pat . group ( 6 ) : \n        a = 0 \n        b = int ( pat . group ( 6 ) ) \n    else : \n        sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n        coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 \n    reverse = 0 \n    if lexeme == 'nth-last-child' : \n        reverse = 1 \n    def validate ( node ) : \n        if not node . siblings : \n            return 0 \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += 1 \n        if a == 0 : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and ( idx * a + b ) >= 0 \n        return m \n    return validate "}
{"10393": "\ndef chain_check ( cls , timestamp : int ) -> bool : \n    record = cls . get_record ( timestamp ) \n    if isinstance ( record , NistBeaconValue ) is 0 : \n        return 0 \n    prev_record = cls . get_previous ( record . timestamp ) \n    next_record = cls . get_next ( record . timestamp ) \n    if prev_record is None and next_record is None : \n        return 0 \n    if ( isinstance ( prev_record , NistBeaconValue ) and isinstance ( next_record , NistBeaconValue ) ) : \n        return ( record . valid_signature and prev_record . valid_signature and next_record . valid_signature and record . previous_output_value == prev_record . output_value and next_record . previous_output_value == record . output_value ) \n    if ( prev_record is None and isinstance ( next_record , NistBeaconValue ) ) : \n        return ( record . valid_signature and next_record . valid_signature and cls . _INIT_RECORD == record and next_record . previous_output_value == record . output_value ) \n    if ( isinstance ( prev_record , NistBeaconValue ) and next_record is None ) : \n        return ( record . valid_signature and prev_record . valid_signature and record . previous_output_value == prev_record . output_value ) "}
{"10409": "\ndef sourceWatchdog ( self ) : \n    for i , source in enumerate ( self . sources ) : \n        if not source . config . get ( 'watchdog' , 0 ) : \n            continue \n        sn = repr ( source ) \n        last = self . lastEvents . get ( source , None ) \n        if last : \n            try : \n                if last < ( time . time ( ) - ( source . inter * 10 ) ) : \n                    log . msg ( \"Trying to restart stale source %s: %ss\" % ( sn , int ( time . time ( ) - last ) ) ) \n                    s = self . sources . pop ( i ) \n                    try : \n                        s . t . stop ( ) \n                    except Exception as e : \n                        log . msg ( \"Could not stop timer for %s: %s\" % ( sn , e ) ) \n                    config = copy . deepcopy ( s . config ) \n                    del self . lastEvents [ source ] \n                    del s , source \n                    source = self . createSource ( config ) \n                    reactor . callLater ( 0 , self . _startSource , source ) \n            except Exception as e : \n                log . msg ( \"Could not reset source %s: %s\" % ( sn , e ) ) "}
{"10421": "\ndef send_email_validation ( request ) : \n    token = EmailConfirmationSerializer ( ) . create_token ( request . id , dict ( email = request . sender_email ) ) \n    pid , record = get_record ( request . recid ) \n    _send_notification ( request . sender_email , _ ( \"Access request verification\" ) , \"zenodo_accessrequests/emails/validate_email.tpl\" , request = request , record = record , pid = pid , days = timedelta ( seconds = current_app . config [ \"ACCESSREQUESTS_CONFIRMLINK_EXPIRES_IN\" ] ) . days , confirm_link = url_for ( \"invenio_records_ui.recid_access_request_email_confirm\" , pid_value = request . recid , token = token , _external = 1 , ) ) "}
{"10425": "\ndef validate_token ( cls , token , expected_data ) : \n    data = SecretLinkFactory . validate_token ( token , expected_data = expected_data ) \n    if data : \n        link = cls . query . get ( data [ 'id' ] ) \n        if link and link . is_valid ( ) : \n            return 1 \n    return 0 "}
{"10426": "\ndef revoke ( self ) : \n    if self . revoked_at is None : \n        with db . session . begin_nested ( ) : \n            self . revoked_at = datetime . utcnow ( ) \n        link_revoked . send ( self ) \n        return 1 \n    return 0 "}
{"10434": "\ndef verify ( cls , timestamp : int , message_hash : SHA512Hash , signature : bytes , ) -> bool : \n    if timestamp < 1496176860 : \n        verifier = cls . _VERIFIER_20130905 \n    elif timestamp < 1502202360 : \n        verifier = None \n    else : \n        verifier = cls . _VERIFIER_20170808 \n    if verifier : \n        result = verifier . verify ( message_hash , signature , ) \n    else : \n        result = 0 \n    if isinstance ( result , int ) : \n        result = 1 if result == 1 else 0 \n    return result "}
{"10444": "\ndef _init_ssh ( self ) : \n    self . ssh_host = self . config . get ( 'ssh_host' , self . hostname ) \n    self . known_hosts = self . config . get ( 'ssh_knownhosts_file' , self . tensor . config . get ( 'ssh_knownhosts_file' , None ) ) \n    self . ssh_keyfile = self . config . get ( 'ssh_keyfile' , self . tensor . config . get ( 'ssh_keyfile' , None ) ) \n    self . ssh_key = self . config . get ( 'ssh_key' , self . tensor . config . get ( 'ssh_key' , None ) ) \n    self . ssh_keypass = self . config . get ( 'ssh_keypass' , self . tensor . config . get ( 'ssh_keypass' , None ) ) \n    self . ssh_user = self . config . get ( 'ssh_username' , self . tensor . config . get ( 'ssh_username' , None ) ) \n    self . ssh_password = self . config . get ( 'ssh_password' , self . tensor . config . get ( 'ssh_password' , None ) ) \n    self . ssh_port = self . config . get ( 'ssh_port' , self . tensor . config . get ( 'ssh_port' , 22 ) ) \n    if not ( self . ssh_key or self . ssh_keyfile or self . ssh_password ) : \n        raise Exception ( \"To use SSH you must specify *one* of ssh_key,\" \" ssh_keyfile or ssh_password for this source\" \" check or globally\" ) \n    if not self . ssh_user : \n        raise Exception ( \"ssh_username must be set\" ) \n    self . ssh_keydb = [ ] \n    cHash = hashlib . sha1 ( ':' . join ( ( self . ssh_host , self . ssh_user , str ( self . ssh_port ) , str ( self . ssh_password ) , str ( self . ssh_key ) , str ( self . ssh_keyfile ) ) ) . encode ( ) ) . hexdigest ( ) \n    if cHash in self . tensor . hostConnectorCache : \n        self . ssh_client = self . tensor . hostConnectorCache . get ( cHash ) \n        self . ssh_connector = 0 \n    else : \n        self . ssh_connector = 1 \n        self . ssh_client = ssh . SSHClient ( self . ssh_host , self . ssh_user , self . ssh_port , password = self . ssh_password , knownhosts = self . known_hosts ) \n        if self . ssh_keyfile : \n            self . ssh_client . addKeyFile ( self . ssh_keyfile , self . ssh_keypass ) \n        if self . ssh_key : \n            self . ssh_client . addKeyString ( self . ssh_key , self . ssh_keypass ) \n        self . tensor . hostConnectorCache [ cHash ] = self . ssh_client "}
{"10446": "\ndef tick ( self ) : \n    if self . sync : \n        if self . running : \n            defer . returnValue ( None ) \n    self . running = 1 \n    try : \n        event = yield self . _get ( ) \n        if event : \n            self . queueBack ( event ) \n    except Exception as e : \n        log . msg ( \"[%s] Unhandled error: %s\" % ( self . service , e ) ) \n    self . running = 0 "}
{"10448": "\ndef createClient ( self ) : \n    server = self . config . get ( 'server' , 'localhost' ) \n    port = self . config . get ( 'port' , 5555 ) \n    failover = self . config . get ( 'failover' , 0 ) \n    self . factory = riemann . RiemannClientFactory ( server , failover = failover ) \n    if failover : \n        initial = random . choice ( server ) \n    else : \n        initial = server \n    log . msg ( 'Connecting to Riemann on %s:%s' % ( initial , port ) ) \n    if self . tls : \n        if SSL : \n            self . connector = reactor . connectSSL ( initial , port , self . factory , ClientTLSContext ( self . key , self . cert ) ) \n        else : \n            log . msg ( '[FATAL] SSL support not available!' ' Please install PyOpenSSL. Exiting now' ) \n            reactor . stop ( ) \n    else : \n        self . connector = reactor . connectTCP ( initial , port , self . factory ) \n    d = defer . Deferred ( ) \n    def cb ( ) : \n        if hasattr ( self . factory , 'proto' ) and self . factory . proto : \n            self . t . start ( self . inter ) \n            d . callback ( None ) \n        else : \n            reactor . callLater ( 0.01 , cb ) \n    cb ( ) \n    return d "}
{"10461": "\ndef message_loop ( self , t_q , r_q ) : \n    t_msg = { } \n    while t_msg . get ( \"state\" , \"\" ) != \"__DIE__\" : \n        try : \n            t_msg = t_q . get ( 1 , self . cycle_sleep ) \n            self . task = t_msg . get ( \"task\" , \"\" ) \n            if self . task != \"\" : \n                self . task . task_start = time . time ( ) \n                self . r_q_send ( { \"w_id\" : self . w_id , \"task\" : self . task , \"state\" : \"__ACK__\" } ) \n                self . cycle_sleep = self . task . worker_loop_delay \n                self . task . result = self . task . run ( ) \n                self . task . task_stop = time . time ( ) \n                self . r_q_send ( { \"w_id\" : self . w_id , \"task\" : self . task , \"state\" : \"__FINISHED__\" } ) \n                self . task = None \n        except Empty : \n            pass \n        except Full : \n            time . sleep ( 0.1 ) \n        except : \n            if self . task is not None : \n                self . task . task_stop = time . time ( ) \n            tb_str = \"\" . join ( tb . format_exception ( * ( sys . exc_info ( ) ) ) ) \n            self . r_q_send ( { \"w_id\" : self . w_id , \"task\" : self . task , \"error\" : tb_str , \"state\" : \"__ERROR__\" , } ) \n    return "}
{"10462": "\ndef log_time ( self ) : \n    if self . hot_loop and self . time_delta >= self . log_interval : \n        return 1 \n    return 0 "}
{"10467": "\ndef build_payment_parameters ( amount : Money , client_ref : str ) -> PaymentParameters : \n    merchant_id = web_merchant_id \n    amount , currency = money_to_amount_and_currency ( amount ) \n    refno = client_ref \n    sign = sign_web ( merchant_id , amount , currency , refno ) \n    parameters = PaymentParameters ( merchant_id = merchant_id , amount = amount , currency = currency , refno = refno , sign = sign , use_alias = 0 , ) \n    logger . info ( 'build-payment-parameters' , parameters = parameters ) \n    return parameters "}
{"10468": "\ndef build_register_credit_card_parameters ( client_ref : str ) -> PaymentParameters : \n    amount = 0 \n    currency = 'CHF' \n    merchant_id = web_merchant_id \n    refno = client_ref \n    sign = sign_web ( merchant_id , amount , currency , refno ) \n    parameters = PaymentParameters ( merchant_id = merchant_id , amount = amount , currency = currency , refno = refno , sign = sign , use_alias = 1 , ) \n    logger . info ( 'building-payment-parameters' , parameters = parameters ) \n    return parameters "}
{"10471": "\ndef _construct ( self ) : \n    self . setLayout ( QtGui . QVBoxLayout ( ) ) \n    self . _headerLayout = QtGui . QHBoxLayout ( ) \n    self . _locationWidget = QtGui . QComboBox ( ) \n    self . _headerLayout . addWidget ( self . _locationWidget , stretch = 1 ) \n    self . _upButton = QtGui . QToolButton ( ) \n    self . _upButton . setIcon ( QtGui . QIcon ( ':riffle/icon/up' ) ) \n    self . _headerLayout . addWidget ( self . _upButton ) \n    self . layout ( ) . addLayout ( self . _headerLayout ) \n    self . _contentSplitter = QtGui . QSplitter ( ) \n    self . _bookmarksWidget = QtGui . QListView ( ) \n    self . _contentSplitter . addWidget ( self . _bookmarksWidget ) \n    self . _filesystemWidget = QtGui . QTableView ( ) \n    self . _filesystemWidget . setSelectionBehavior ( self . _filesystemWidget . SelectRows ) \n    self . _filesystemWidget . setSelectionMode ( self . _filesystemWidget . SingleSelection ) \n    self . _filesystemWidget . verticalHeader ( ) . hide ( ) \n    self . _contentSplitter . addWidget ( self . _filesystemWidget ) \n    proxy = riffle . model . FilesystemSortProxy ( self ) \n    model = riffle . model . Filesystem ( path = self . _root , parent = self , iconFactory = self . _iconFactory ) \n    proxy . setSourceModel ( model ) \n    proxy . setDynamicSortFilter ( 1 ) \n    self . _filesystemWidget . setModel ( proxy ) \n    self . _filesystemWidget . setSortingEnabled ( 1 ) \n    self . _contentSplitter . setStretchFactor ( 1 , 1 ) \n    self . layout ( ) . addWidget ( self . _contentSplitter ) \n    self . _footerLayout = QtGui . QHBoxLayout ( ) \n    self . _footerLayout . addStretch ( 1 ) \n    self . _cancelButton = QtGui . QPushButton ( 'Cancel' ) \n    self . _footerLayout . addWidget ( self . _cancelButton ) \n    self . _acceptButton = QtGui . QPushButton ( 'Choose' ) \n    self . _footerLayout . addWidget ( self . _acceptButton ) \n    self . layout ( ) . addLayout ( self . _footerLayout ) "}
{"10472": "\ndef _postConstruction ( self ) : \n    self . setWindowTitle ( 'Filesystem Browser' ) \n    self . _filesystemWidget . sortByColumn ( 0 , QtCore . Qt . AscendingOrder ) \n    self . _bookmarksWidget . hide ( ) \n    self . _acceptButton . setDefault ( 1 ) \n    self . _acceptButton . setDisabled ( 1 ) \n    self . _acceptButton . clicked . connect ( self . accept ) \n    self . _cancelButton . clicked . connect ( self . reject ) \n    self . _configureShortcuts ( ) \n    self . setLocation ( self . _root ) \n    self . _filesystemWidget . horizontalHeader ( ) . setResizeMode ( QtGui . QHeaderView . ResizeToContents ) \n    self . _filesystemWidget . horizontalHeader ( ) . setResizeMode ( 0 , QtGui . QHeaderView . Stretch ) \n    self . _upButton . clicked . connect ( self . _onNavigateUpButtonClicked ) \n    self . _locationWidget . currentIndexChanged . connect ( self . _onNavigate ) \n    self . _filesystemWidget . activated . connect ( self . _onActivateItem ) \n    selectionModel = self . _filesystemWidget . selectionModel ( ) \n    selectionModel . currentRowChanged . connect ( self . _onSelectItem ) "}
{"10473": "\ndef _configureShortcuts ( self ) : \n    self . _upShortcut = QtGui . QShortcut ( QtGui . QKeySequence ( 'Backspace' ) , self ) \n    self . _upShortcut . setAutoRepeat ( 0 ) \n    self . _upShortcut . activated . connect ( self . _onNavigateUpButtonClicked ) "}
{"10474": "\ndef _onActivateItem ( self , index ) : \n    item = self . _filesystemWidget . model ( ) . item ( index ) \n    if not isinstance ( item , riffle . model . File ) : \n        self . _acceptButton . setDisabled ( 1 ) \n        self . setLocation ( item . path , interactive = 1 ) "}
{"10475": "\ndef _onSelectItem ( self , selection , previousSelection ) : \n    self . _acceptButton . setEnabled ( 1 ) \n    del self . _selected [ : ] \n    item = self . _filesystemWidget . model ( ) . item ( selection ) \n    self . _selected . append ( item . path ) "}
{"10476": "\ndef _onNavigate ( self , index ) : \n    if index > 0 : \n        self . setLocation ( self . _locationWidget . itemData ( index ) , interactive = 1 ) "}
{"10480": "\ndef fetchChildren ( self ) : \n    if not self . canFetchMore ( ) : \n        return [ ] \n    children = self . _fetchChildren ( ) \n    self . _fetched = 1 \n    return children "}
{"10481": "\ndef refetch ( self ) : \n    for child in self . children [ : ] : \n        self . removeChild ( child ) \n    self . _fetched = 0 "}
{"10483": "\ndef call ( args , stdout = None , stderr = None , stdin = None , daemonize = 0 , preexec_fn = None , shell = 0 , cwd = None , env = None ) : \n    stream = lambda s , m : s is None and os . open ( os . devnull , m ) or s \n    stdout = stream ( stdout , os . O_WRONLY ) \n    stderr = stream ( stderr , os . O_WRONLY ) \n    stdin = stream ( stdin , os . O_RDONLY ) \n    shared_pid = Value ( 'i' , 0 ) \n    pid = os . fork ( ) \n    if pid > 0 : \n        os . waitpid ( pid , 0 ) \n        child_pid = shared_pid . value \n        del shared_pid \n        if daemonize : \n            sys . exit ( 0 ) \n        return child_pid \n    else : \n        os . setsid ( ) \n        proc = subprocess . Popen ( args , stdout = stdout , stderr = stderr , stdin = stdin , close_fds = 1 , preexec_fn = preexec_fn , shell = shell , cwd = cwd , env = env ) \n        shared_pid . value = proc . pid \n        os . _exit ( 0 ) "}
{"10491": "\ndef include_ ( parser , token ) : \n    bits = token . split_contents ( ) \n    dynamic = 0 \n    if len ( bits ) >= 2 : \n        dynamic = '{{' in bits [ 1 ] \n        if dynamic : \n            fallback = None \n            bits_new = [ ] \n            for bit in bits : \n                if fallback is 1 : \n                    fallback = bit \n                    continue \n                if bit == 'fallback' : \n                    fallback = 1 \n                else : \n                    bits_new . append ( bit ) \n            if fallback : \n                fallback = parser . compile_filter ( construct_relative_path_ ( parser , fallback ) ) \n            token . contents = ' ' . join ( bits_new ) \n    token . contents = token . contents . replace ( 'include_' , 'include' ) \n    include_node = do_include ( parser , token ) \n    if dynamic : \n        include_node = DynamicIncludeNode ( include_node . template , extra_context = include_node . extra_context , isolated_context = include_node . isolated_context , fallback = fallback or None , ) \n    return include_node "}
{"10494": "\ndef is_valid_filesys ( path ) : \n    if os . path . isabs ( path ) and os . path . isdir ( path ) and not os . path . isfile ( path ) : \n        return 1 \n    else : \n        raise LocalPortValidationError ( 'Port value %s is not a valid filesystem location' % path ) "}
{"10495": "\ndef is_valid_s3_url ( url ) : \n    if url . startswith ( 'source:' ) : \n        return 1 \n    scheme , netloc , path , _ , _ , _ = urlparse ( url ) \n    port_except = RemotePortValidationError ( 'Port value %s is not a valid s3 location' % url ) \n    if len ( scheme ) < 2 : \n        raise port_except \n    if 's3' in scheme or 's3' in netloc or 's3' in path : \n        return 1 \n    else : \n        raise port_except "}
{"10497": "\ndef list ( self , s3_folder = '' , full_key_data = 0 ) : \n    if not s3_folder . startswith ( '/' ) : \n        s3_folder = '/' + s3_folder \n    s3_prefix = self . prefix + s3_folder \n    bucket_data = self . client . list_objects ( Bucket = self . bucket , Prefix = s3_prefix ) \n    if full_key_data : \n        return bucket_data [ 'Contents' ] \n    else : \n        return [ k [ 'Key' ] for k in bucket_data [ 'Contents' ] ] "}
{"10498": "\ndef _build_worklfow_json ( self ) : \n    wf_json = { 'tasks' : [ ] , 'name' : 'cloud-harness_%s' % str ( uuid . uuid4 ( ) ) } \n    task_def = json . loads ( self . task_template . json ( ) ) \n    d = { \"name\" : task_def [ 'name' ] , \"outputs\" : [ ] , \"inputs\" : [ ] , \"taskType\" : task_def [ 'taskType' ] } \n    for port in self . task_template . input_ports : \n        port_value = port . value \n        if port_value is 0 : \n            port_value = 'false' \n        if port_value is 1 : \n            port_value = 'true' \n        d [ 'inputs' ] . append ( { \"name\" : port . _name , \"value\" : port_value } ) \n    for port in self . task_template . output_ports : \n        d [ 'outputs' ] . append ( { \"name\" : port . _name } ) \n    wf_json [ 'tasks' ] . append ( d ) \n    for port in self . task_template . output_ports : \n        if hasattr ( port , 'stageToS3' ) and port . stageToS3 : \n            save_location = '{customer_storage}/{run_name}/{port}' . format ( customer_storage = self . storage . location , run_name = self . task_template . run_name , port = port . name ) \n            new_task = dict ( ** self . STAGE_TO_S3 ) \n            new_task [ 'inputs' ] = [ { 'name' : 'data' , 'source' : '%s:%s' % ( task_def [ 'name' ] , port . _name ) } , { 'name' : 'destination' , 'value' : save_location } ] \n            wf_json [ 'tasks' ] . append ( new_task ) \n    return wf_json "}
{"10500": "\ndef archive ( folder , dry_run = 0 ) : \n    for f in folder : \n        if not os . path . exists ( f ) : \n            bail ( 'folder does not exist: ' + f ) \n    _archive_safe ( folder , PROJ_ARCHIVE , dry_run = dry_run ) "}
{"10505": "\ndef list ( self , path ) : \n    self . __validate_storage_path ( path ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] not in self . __BROWSABLE_TYPES : \n        raise StorageArgumentException ( 'The entity type \"{0}\" cannot be' 'listed' . format ( entity [ 'entity_type' ] ) ) \n    entity_uuid = entity [ 'uuid' ] \n    file_names = [ ] \n    more_pages = 1 \n    page_number = 1 \n    while more_pages : \n        response = self . api_client . list_folder_content ( entity_uuid , page = page_number , ordering = 'name' ) \n        more_pages = response [ 'next' ] is not None \n        page_number += 1 \n        for child in response [ 'results' ] : \n            pattern = '/{name}' if child [ 'entity_type' ] == 'folder' else '{name}' \n            file_names . append ( pattern . format ( name = child [ 'name' ] ) ) \n    return file_names "}
{"10507": "\ndef exists ( self , path ) : \n    self . __validate_storage_path ( path ) \n    try : \n        metadata = self . api_client . get_entity_by_query ( path = path ) \n    except StorageNotFoundException : \n        return 0 \n    return metadata and 'uuid' in metadata "}
{"10508": "\ndef get_parent ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = 0 ) \n    path_steps = [ step for step in path . split ( '/' ) if step ] \n    del path_steps [ - 1 ] \n    parent_path = '/{0}' . format ( '/' . join ( path_steps ) ) \n    return self . api_client . get_entity_by_query ( path = parent_path ) "}
{"10509": "\ndef mkdir ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = 0 ) \n    parent_metadata = self . get_parent ( path ) \n    self . api_client . create_folder ( path . split ( '/' ) [ - 1 ] , parent_metadata [ 'uuid' ] ) "}
{"10511": "\ndef delete ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = 0 ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] in self . __BROWSABLE_TYPES : \n        contents = self . api_client . list_folder_content ( entity [ 'uuid' ] ) \n        if contents [ 'count' ] > 0 : \n            raise StorageArgumentException ( 'This method cannot delete non-empty folder. Please empty the folder first.' ) \n        self . api_client . delete_folder ( entity [ 'uuid' ] ) \n    elif entity [ 'entity_type' ] == 'file' : \n        self . api_client . delete_file ( entity [ 'uuid' ] ) "}
{"10512": "\ndef __validate_storage_path ( cls , path , projects_allowed = 1 ) : \n    if not path or not isinstance ( path , str ) or path [ 0 ] != '/' or path == '/' : \n        raise StorageArgumentException ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) \n    if not projects_allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == 1 : \n        raise StorageArgumentException ( 'This method does not accept projects in the path.' ) "}
{"10538": "\ndef gatk_genotype_gvcfs ( job , gvcfs , ref , fai , ref_dict , annotations = None , emit_threshold = 10.0 , call_threshold = 30.0 , unsafe_mode = 0 ) : \n    inputs = { 'genome.fa' : ref , 'genome.fa.fai' : fai , 'genome.dict' : ref_dict } \n    inputs . update ( gvcfs ) \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    command = [ '-T' , 'GenotypeGVCFs' , '-R' , '/data/genome.fa' , '--out' , 'genotyped.vcf' , '-stand_emit_conf' , str ( emit_threshold ) , '-stand_call_conf' , str ( call_threshold ) ] \n    if annotations : \n        for annotation in annotations : \n            command . extend ( [ '-A' , annotation ] ) \n    for uuid in gvcfs . keys ( ) : \n        command . extend ( [ '--variant' , os . path . join ( '/data' , uuid ) ] ) \n    if unsafe_mode : \n        command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    job . fileStore . logToMaster ( 'Running GATK GenotypeGVCFs\\n' 'Emit threshold: {emit_threshold}\\n' 'Call threshold: {call_threshold}\\n\\n' 'Annotations:\\n{annotations}\\n\\n' 'Samples:\\n{samples}\\n' . format ( emit_threshold = emit_threshold , call_threshold = call_threshold , annotations = '\\n' . join ( annotations ) if annotations else '' , samples = '\\n' . join ( gvcfs . keys ( ) ) ) ) \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , dockerParameters = docker_parameters ) \n    return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'genotyped.vcf' ) ) "}
{"10560": "\ndef create ( self , public = 0 , ** kwargs ) : \n    kwargs [ \"public\" ] = public \n    self . metadata = self . db . create ( self . path , kwargs ) . json ( ) "}
{"10573": "\ndef sync ( self ) : \n    logging . debug ( \"Logger: Syncing...\" ) \n    failed = 0 \n    try : \n        cdb = self . connectordb \n        cdb . ping ( ) \n        with self . synclock : \n            c = self . database . cursor ( ) \n            for stream in self . streams : \n                s = cdb [ stream ] \n                c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) \n                datapointArray = [ ] \n                for dp in c . fetchall ( ) : \n                    datapointArray . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2 ] ) } ) \n                if len ( s ) > 0 : \n                    newtime = s [ - 1 ] [ \"t\" ] \n                    while ( len ( datapointArray ) > 0 and datapointArray [ 0 ] [ \"t\" ] < newtime ) : \n                        logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) \n                        datapointArray = datapointArray [ 1 : ] \n                if len ( datapointArray ) > 0 : \n                    logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapointArray ) ) ) \n                    while ( len ( datapointArray ) > DATAPOINT_INSERT_LIMIT ) : \n                        s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) \n                        datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] \n                        c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapointArray [ 0 ] [ \"t\" ] ) ) \n                    s . insert_array ( datapointArray ) \n                    c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapointArray [ - 1 ] [ \"t\" ] ) ) \n            self . lastsynctime = time . time ( ) \n            if self . onsync is not None : \n                self . onsync ( ) \n    except Exception as e : \n        falied = 1 \n        reraise = self . syncraise \n        if self . onsyncfail is not None : \n            reraise = self . onsyncfail ( e ) \n        if reraise : \n            raise "}
{"10582": "\ndef run_star ( job , r1_id , r2_id , star_index_url , wiggle = 0 , sort = 1 ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = star_index_url , name = 'starIndex.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'starIndex.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'starIndex.tar.gz' ) ) \n    star_index = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if len ( os . listdir ( work_dir ) ) == 1 else '/data' \n    parameters = [ '--runThreadN' , str ( job . cores ) , '--genomeDir' , star_index , '--outFileNamePrefix' , 'rna' , '--outSAMunmapped' , 'Within' , '--quantMode' , 'TranscriptomeSAM' , '--outSAMattributes' , 'NH' , 'HI' , 'AS' , 'NM' , 'MD' , '--outFilterType' , 'BySJout' , '--outFilterMultimapNmax' , '20' , '--outFilterMismatchNmax' , '999' , '--outFilterMismatchNoverReadLmax' , '0.04' , '--alignIntronMin' , '20' , '--alignIntronMax' , '1000000' , '--alignMatesGapMax' , '1000000' , '--alignSJoverhangMin' , '8' , '--alignSJDBoverhangMin' , '1' , '--sjdbScore' , '1' , '--limitBAMsortRAM' , '49268954168' ] \n    if sort : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'SortedByCoordinate' ] ) \n        aligned_bam = 'rnaAligned.sortedByCoord.out.bam' \n    else : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'Unsorted' ] ) \n        aligned_bam = 'rnaAligned.out.bam' \n    if wiggle : \n        parameters . extend ( [ '--outWigType' , 'bedGraph' , '--outWigStrand' , 'Unstranded' , '--outWigReferencesPrefix' , 'chr' ] ) \n    if r1_id and r2_id : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        job . fileStore . readGlobalFile ( r2_id , os . path . join ( work_dir , 'R2.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' , '/data/R2.fastq' ] ) \n    else : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' ] ) \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/star:2.4.2a--bcbd5122b69ff6ac4ef61958e47bde94001cfe80' , workDir = work_dir , parameters = parameters ) \n    aligned_bam_path = os . path . join ( work_dir , aligned_bam ) \n    if sort : \n        assert ( os . stat ( aligned_bam_path ) . st_size > 0 , 'Aligned bam failed to sort. Ensure sufficient memory is free.' ) \n    aligned_id = job . fileStore . writeGlobalFile ( aligned_bam_path ) \n    transcriptome_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaAligned.toTranscriptome.out.bam' ) ) \n    log_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaLog.final.out' ) ) \n    sj_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSJ.out.tab' ) ) \n    if wiggle : \n        wiggle_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSignal.UniqueMultiple.str1.out.bg' ) ) \n        return transcriptome_id , aligned_id , wiggle_id , log_id , sj_id \n    else : \n        return transcriptome_id , aligned_id , log_id , sj_id "}
{"10584": "\ndef export ( self , directory ) : \n    if os . path . exists ( directory ) : \n        raise FileExistsError ( \"The stream export directory already exists\" ) \n    os . mkdir ( directory ) \n    with open ( os . path . join ( directory , \"stream.json\" ) , \"w\" ) as f : \n        json . dump ( self . data , f ) \n    self [ : ] . sort ( ) . writeJSON ( os . path . join ( directory , \"data.json\" ) ) \n    if self . downlink : \n        self ( i1 = 0 , i2 = 0 , downlink = 1 ) . sort ( ) . writeJSON ( os . path . join ( directory , \"downlink.json\" ) ) "}
{"10602": "\ndef create ( self , email , password , role = \"user\" , public = 1 , ** kwargs ) : \n    kwargs [ \"email\" ] = email \n    kwargs [ \"password\" ] = password \n    kwargs [ \"role\" ] = role \n    kwargs [ \"public\" ] = public \n    self . metadata = self . db . create ( self . path , kwargs ) . json ( ) "}
{"10610": "\ndef run_picard_sort ( job , bam , sort_by_name = 0 ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    job . fileStore . readGlobalFile ( bam , os . path . join ( work_dir , 'input.bam' ) ) \n    command = [ 'SortSam' , 'O=/data/output.bam' , 'I=/data/input.bam' ] \n    docker_parameters = [ '--rm' , '--log-driver' , 'none' , '-e' , 'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) , '-v' , '{}:/data' . format ( work_dir ) ] \n    if sort_by_name : \n        command . append ( 'SO=queryname' ) \n    else : \n        command . append ( 'SO=coordinate' ) \n    start_time = time . time ( ) \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e' , dockerParameters = docker_parameters ) \n    end_time = time . time ( ) \n    _log_runtime ( job , start_time , end_time , \"Picard SortSam\" ) \n    return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.bam' ) ) "}
{"10611": "\ndef run_base_recalibration ( job , bam , bai , ref , ref_dict , fai , dbsnp , mills , unsafe = 0 ) : \n    inputs = { 'ref.fasta' : ref , 'ref.fasta.fai' : fai , 'ref.dict' : ref_dict , 'input.bam' : bam , 'input.bai' : bai , 'dbsnp.vcf' : dbsnp , 'mills.vcf' : mills } \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    parameters = [ '-T' , 'BaseRecalibrator' , '-nct' , str ( int ( job . cores ) ) , '-R' , '/data/ref.fasta' , '-I' , '/data/input.bam' , '-knownSites' , '/data/dbsnp.vcf' , '-knownSites' , '/data/mills.vcf' , '-o' , '/data/recal_data.table' ] \n    if unsafe : \n        parameters . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    docker_parameters = [ '--rm' , '--log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) , '-v' , '{}:/data' . format ( work_dir ) ] \n    start_time = time . time ( ) \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , workDir = work_dir , parameters = parameters , dockerParameters = docker_parameters ) \n    end_time = time . time ( ) \n    _log_runtime ( job , start_time , end_time , \"GATK3 BaseRecalibrator\" ) \n    return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'recal_data.table' ) ) "}
{"10613": "\ndef run_rsem ( job , bam_id , rsem_ref_url , paired = 1 ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = rsem_ref_url , name = 'rsem_ref.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'rsem_ref.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'rsem_ref.tar.gz' ) ) \n    rsem_files = [ ] \n    for root , directories , files in os . walk ( work_dir ) : \n        rsem_files . extend ( [ os . path . join ( root , x ) for x in files ] ) \n    ref_prefix = [ os . path . basename ( os . path . splitext ( x ) [ 0 ] ) for x in rsem_files if 'grp' in x ] [ 0 ] \n    ref_folder = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if len ( os . listdir ( work_dir ) ) == 1 else '/data' \n    job . fileStore . readGlobalFile ( bam_id , os . path . join ( work_dir , 'transcriptome.bam' ) ) \n    output_prefix = 'rsem' \n    parameters = [ '--quiet' , '--no-qualities' , '-p' , str ( job . cores ) , '--forward-prob' , '0.5' , '--seed-length' , '25' , '--fragment-length-mean' , '-1.0' , '--bam' , '/data/transcriptome.bam' , os . path . join ( ref_folder , ref_prefix ) , output_prefix ] \n    if paired : \n        parameters = [ '--paired-end' ] + parameters \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21' , parameters = parameters , workDir = work_dir ) \n    gene_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.genes.results' ) ) \n    isoform_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.isoforms.results' ) ) \n    return gene_id , isoform_id "}
{"10616": "\ndef subscribe ( self , stream , callback , transform = \"\" ) : \n    if self . status == \"disconnected\" or self . status == \"disconnecting\" or self . status == \"connecting\" : \n        self . connect ( ) \n    if self . status is not \"connected\" : \n        return 0 \n    logging . debug ( \"Subscribing to %s\" , stream ) \n    self . send ( { \"cmd\" : \"subscribe\" , \"arg\" : stream , \"transform\" : transform } ) \n    with self . subscription_lock : \n        self . subscriptions [ stream + \":\" + transform ] = callback \n    return 1 "}
{"10617": "\ndef connect ( self ) : \n    self . ws_openlock . acquire ( ) \n    self . ws_openlock . release ( ) \n    if self . status == \"connected\" : \n        return 1 \n    if self . status == \"disconnecting\" : \n        time . sleep ( 0.1 ) \n        return self . connect ( ) \n    if self . status == \"disconnected\" or self . status == \"reconnecting\" : \n        self . ws = websocket . WebSocketApp ( self . ws_url , header = self . headers , on_message = self . __on_message , on_ping = self . __on_ping , on_open = self . __on_open , on_close = self . __on_close , on_error = self . __on_error ) \n        self . ws_thread = threading . Thread ( target = self . ws . run_forever ) \n        self . ws_thread . daemon = 1 \n        self . status = \"connecting\" \n        self . ws_openlock . acquire ( ) \n        self . ws_thread . start ( ) \n    self . ws_openlock . acquire ( ) \n    self . ws_openlock . release ( ) \n    return self . status == \"connected\" "}
{"10618": "\ndef __reconnect ( self ) : \n    self . status = \"reconnecting\" \n    if self . disconnected_time - self . connected_time > 15 * 60 : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    else : \n        self . reconnect_time *= self . reconnect_time_backoff_multiplier \n    if self . reconnect_time > self . reconnect_time_max_seconds : \n        self . reconnect_time = self . reconnect_time_max_seconds \n    self . reconnect_time *= 1 + random . uniform ( - 0.2 , 0.2 ) \n    if self . reconnect_time < self . reconnect_time_starting_seconds : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    logging . warn ( \"ConnectorDB:WS: Attempting to reconnect in %fs\" , self . reconnect_time ) \n    self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) \n    self . reconnector . daemon = 1 \n    self . reconnector . start ( ) "}
{"10623": "\ndef __on_message ( self , ws , msg ) : \n    msg = json . loads ( msg ) \n    logging . debug ( \"ConnectorDB:WS: Msg '%s'\" , msg [ \"stream\" ] ) \n    stream_key = msg [ \"stream\" ] + \":\" \n    if \"transform\" in msg : \n        stream_key += msg [ \"transform\" ] \n    self . subscription_lock . acquire ( ) \n    if stream_key in self . subscriptions : \n        subscription_function = self . subscriptions [ stream_key ] \n        self . subscription_lock . release ( ) \n        fresult = subscription_function ( msg [ \"stream\" ] , msg [ \"data\" ] ) \n        if fresult is 1 : \n            fresult = msg [ \"data\" ] \n        if fresult is not 0 and fresult is not None and msg [ \"stream\" ] . endswith ( \"/downlink\" ) and msg [ \"stream\" ] . count ( \"/\" ) == 3 : \n            self . insert ( msg [ \"stream\" ] [ : - 9 ] , fresult ) \n    else : \n        self . subscription_lock . release ( ) \n        logging . warn ( \"ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s\" , msg [ \"stream\" ] , list ( self . subscriptions . keys ( ) ) ) "}
{"10624": "\ndef __ensure_ping ( self ) : \n    logging . debug ( \"ConnectorDB:WS: pingcheck\" ) \n    if ( time . time ( ) - self . lastpingtime > self . connection_ping_timeout ) : \n        logging . warn ( \"ConnectorDB:WS: Websocket ping timed out!\" ) \n        if self . ws is not None : \n            self . ws . close ( ) \n            self . __on_close ( self . ws ) \n    else : \n        self . pingtimer = threading . Timer ( self . connection_ping_timeout , self . __ensure_ping ) \n        self . pingtimer . daemon = 1 \n        self . pingtimer . start ( ) "}
{"10627": "\ndef gatk_variant_recalibrator ( job , mode , vcf , ref_fasta , ref_fai , ref_dict , annotations , hapmap = None , omni = None , phase = None , dbsnp = None , mills = None , max_gaussians = 4 , unsafe_mode = 0 ) : \n    mode = mode . upper ( ) \n    inputs = { 'genome.fa' : ref_fasta , 'genome.fa.fai' : ref_fai , 'genome.dict' : ref_dict , 'input.vcf' : vcf } \n    command = [ '-T' , 'VariantRecalibrator' , '-R' , 'genome.fa' , '-input' , 'input.vcf' , '-tranche' , '100.0' , '-tranche' , '99.9' , '-tranche' , '99.0' , '-tranche' , '90.0' , '--maxGaussians' , str ( max_gaussians ) , '-recalFile' , 'output.recal' , '-tranchesFile' , 'output.tranches' , '-rscriptFile' , 'output.plots.R' ] \n    if mode == 'SNP' : \n        command . extend ( [ '-resource:hapmap,known=false,training=true,truth=true,prior=15.0' , 'hapmap.vcf' , '-resource:omni,known=false,training=true,truth=true,prior=12.0' , 'omni.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-resource:1000G,known=false,training=true,truth=false,prior=10.0' , '1000G.vcf' , '-mode' , 'SNP' ] ) \n        inputs [ 'hapmap.vcf' ] = hapmap \n        inputs [ 'omni.vcf' ] = omni \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n        inputs [ '1000G.vcf' ] = phase \n    elif mode == 'INDEL' : \n        command . extend ( [ '-resource:mills,known=false,training=true,truth=true,prior=12.0' , 'mills.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-mode' , 'INDEL' ] ) \n        inputs [ 'mills.vcf' ] = mills \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n    else : \n        raise ValueError ( 'Variant filter modes can be SNP or INDEL, got %s' % mode ) \n    for annotation in annotations : \n        command . extend ( [ '-an' , annotation ] ) \n    if unsafe_mode : \n        command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    job . fileStore . logToMaster ( 'Running GATK VariantRecalibrator on {mode}s using the following annotations:\\n' '{annotations}' . format ( mode = mode , annotations = '\\n' . join ( annotations ) ) ) \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , dockerParameters = docker_parameters ) \n    recal_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.recal' ) ) \n    tranches_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.tranches' ) ) \n    plots_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.plots.R' ) ) \n    return recal_id , tranches_id , plots_id "}
{"10628": "\ndef gatk_apply_variant_recalibration ( job , mode , vcf , recal_table , tranches , ref_fasta , ref_fai , ref_dict , ts_filter_level = 99.0 , unsafe_mode = 0 ) : \n    inputs = { 'genome.fa' : ref_fasta , 'genome.fa.fai' : ref_fai , 'genome.dict' : ref_dict , 'input.vcf' : vcf , 'recal' : recal_table , 'tranches' : tranches } \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    mode = mode . upper ( ) \n    command = [ '-T' , 'ApplyRecalibration' , '-mode' , mode , '-R' , 'genome.fa' , '-input' , 'input.vcf' , '-o' , 'vqsr.vcf' , '-ts_filter_level' , str ( ts_filter_level ) , '-recalFile' , 'recal' , '-tranchesFile' , 'tranches' ] \n    if unsafe_mode : \n        command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    job . fileStore . logToMaster ( 'Running GATK ApplyRecalibration on {mode}s ' 'with a sensitivity of {sensitivity}%' . format ( mode = mode , sensitivity = ts_filter_level ) ) \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , dockerParameters = docker_parameters ) \n    return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'vqsr.vcf' ) ) "}
{"10630": "\ndef bam_quickcheck ( bam_path ) : \n    directory , bam_name = os . path . split ( bam_path ) \n    exit_code = subprocess . call ( [ 'docker' , 'run' , '-v' , directory + ':/data' , 'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c' , 'quickcheck' , '-vv' , '/data/' + bam_name ] ) \n    if exit_code != 0 : \n        return 0 \n    return 1 "}
{"10632": "\ndef write_config ( configuration ) : \n    with open ( CONFIG_PATH , 'w' ) as f : \n        json . dump ( configuration , f , indent = 2 , sort_keys = 1 ) "}
{"10645": "\ndef start ( self , job ) : \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-worker:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"\\\"SPARK_MASTER_IP=\" + self . masterIP + \":\" + _SPARK_MASTER_PORT + \"\\\"\" , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . masterIP + \":\" + _SPARK_MASTER_PORT ] ) [ : - 1 ] \n    self . __start_datanode ( job ) \n    hdfs_down = 1 \n    retries = 0 \n    while hdfs_down and ( retries < 5 ) : \n        _log . info ( \"Sleeping 30 seconds before checking HDFS startup.\" ) \n        time . sleep ( 30 ) \n        clusterID = \"\" \n        try : \n            clusterID = subprocess . check_output ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"grep\" , \"clusterID\" , \"-R\" , \"/opt/apache-hadoop/logs\" ] ) \n        except : \n            pass \n        if \"Incompatible\" in clusterID : \n            _log . warning ( \"Hadoop Datanode failed to start with: %s\" , clusterID ) \n            _log . warning ( \"Retrying container startup, retry #%d.\" , retries ) \n            retries += 1 \n            _log . warning ( \"Removing ephemeral hdfs directory.\" ) \n            subprocess . check_call ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"rm\" , \"-rf\" , \"/ephemeral/hdfs\" ] ) \n            _log . warning ( \"Killing container %s.\" , self . hdfsContainerID ) \n            subprocess . check_call ( [ \"docker\" , \"kill\" , self . hdfsContainerID ] ) \n            _log . info ( \"Restarting datanode.\" ) \n            self . __start_datanode ( job ) \n        else : \n            _log . info ( \"HDFS datanode started up OK!\" ) \n            hdfs_down = 0 \n    if retries >= 5 : \n        raise RuntimeError ( \"Failed %d times trying to start HDFS datanode.\" % retries ) \n    return "}
{"10651": "\ndef fetch_config ( zone , conn ) : \n    more_to_fetch = 1 \n    cfg_chunks = [ ] \n    next_name = None \n    next_type = None \n    next_identifier = None \n    while more_to_fetch == 1 : \n        more_to_fetch = 0 \n        getstr = '/%s/hostedzone/%s/rrset' % ( R53_API_VERSION , zone ) \n        if next_name is not None : \n            getstr += '?name=%s&type=%s' % ( next_name , next_type ) \n            if next_identifier is not None : \n                getstr += '&identifier=%s' % next_identifier \n        log . debug ( 'requesting %s' % getstr ) \n        resp = conn . make_request ( 'GET' , getstr ) \n        etree = lxml . etree . parse ( resp ) \n        cfg_chunks . append ( etree ) \n        root = etree . getroot ( ) \n        truncated = root . find ( '{%s}IsTruncated' % R53_XMLNS ) \n        if truncated is not None and truncated . text == 'true' : \n            more_to_fetch = 1 \n            next_name = root . find ( '{%s}NextRecordName' % R53_XMLNS ) . text \n            next_type = root . find ( '{%s}NextRecordType' % R53_XMLNS ) . text \n            try : \n                next_identifier = root . find ( '{%s}NextRecordIdentifier' % R53_XMLNS ) . text \n            except AttributeError : \n                next_identifier = None \n    return cfg_chunks "}
{"10665": "\ndef transform_hits ( hits ) : \n    packages = { } \n    for hit in hits : \n        name = hit [ 'name' ] \n        summary = hit [ 'summary' ] \n        version = hit [ 'version' ] \n        score = hit [ '_pypi_ordering' ] \n        if score is None : \n            score = 0 \n        if name not in packages . keys ( ) : \n            packages [ name ] = { 'name' : name , 'summary' : summary , 'versions' : [ version ] , 'score' : score , } \n        else : \n            packages [ name ] [ 'versions' ] . append ( version ) \n            if version == highest_version ( packages [ name ] [ 'versions' ] ) : \n                packages [ name ] [ 'summary' ] = summary \n                packages [ name ] [ 'score' ] = score \n    package_list = sorted ( packages . values ( ) , key = lambda x : x [ 'score' ] , reverse = 1 , ) \n    return package_list "}
{"10669": "\ndef tostring ( doc , pretty_print = 0 , include_meta_content_type = 0 , encoding = None , method = \"html\" , with_tail = 1 , doctype = None ) : \n    html = etree . tostring ( doc , method = method , pretty_print = pretty_print , encoding = encoding , with_tail = with_tail , doctype = doctype ) \n    if method == 'html' and not include_meta_content_type : \n        if isinstance ( html , str ) : \n            html = __str_replace_meta_content_type ( '' , html ) \n        else : \n            html = __bytes_replace_meta_content_type ( bytes ( ) , html ) \n    return html "}
{"10677": "\ndef is_single_class ( ) : \n    ret = 0 \n    counts = get_counts ( ) \n    if counts [ \"classes\" ] < 1 and counts [ \"modules\" ] < 1 : \n        ret = counts [ \"tests\" ] > 0 \n    else : \n        ret = counts [ \"classes\" ] <= 1 and counts [ \"modules\" ] <= 1 \n    return ret "}
{"10678": "\ndef is_single_module ( ) : \n    ret = 0 \n    counts = get_counts ( ) \n    if counts [ \"modules\" ] == 1 : \n        ret = 1 \n    elif counts [ \"modules\" ] < 1 : \n        ret = is_single_class ( ) \n    return ret "}
{"10690": "\ndef get_current_traceback ( ignore_system_exceptions = 0 , show_hidden_frames = 0 , skip = 0 ) : \n    exc_type , exc_value , tb = sys . exc_info ( ) \n    if ignore_system_exceptions and exc_type in system_exceptions : \n        raise \n    for x in range_type ( skip ) : \n        if tb . tb_next is None : \n            break \n        tb = tb . tb_next \n    tb = Traceback ( exc_type , exc_value , tb ) \n    if not show_hidden_frames : \n        tb . filter_hidden_frames ( ) \n    return tb "}
{"10692": "\ndef render_summary ( self , include_title = 1 ) : \n    title = '' \n    frames = [ ] \n    classes = [ 'traceback' ] \n    if not self . frames : \n        classes . append ( 'noframe-traceback' ) \n    if include_title : \n        if self . is_syntax_error : \n            title = u'Syntax Error' \n        else : \n            title = u'Traceback <em>(most recent call last)</em>:' \n    for frame in self . frames : \n        frames . append ( u'<li%s>%s' % ( frame . info and u' title=\"%s\"' % escape ( frame . info ) or u'' , frame . render ( ) ) ) \n    if self . is_syntax_error : \n        description_wrapper = u'<pre class=syntaxerror>%s</pre>' \n    else : \n        description_wrapper = u'<blockquote>%s</blockquote>' \n    return SUMMARY_HTML % { 'classes' : u' ' . join ( classes ) , 'title' : title and u'<h3>%s</h3>' % title or u'' , 'frames' : u'\\n' . join ( frames ) , 'description' : description_wrapper % escape ( self . exception ) } "}
{"10694": "\ndef get_annotated_lines ( self ) : \n    lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] \n    if hasattr ( self . code , 'co_firstlineno' ) : \n        lineno = self . code . co_firstlineno - 1 \n        while lineno > 0 : \n            if _funcdef_re . match ( lines [ lineno ] . code ) : \n                break \n            lineno -= 1 \n        try : \n            offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) \n        except TokenError : \n            offset = 0 \n        for line in lines [ lineno : lineno + offset ] : \n            line . in_frame = 1 \n    try : \n        lines [ self . lineno - 1 ] . current = 1 \n    except IndexError : \n        pass \n    return lines "}
{"10697": "\ndef _get_index_urls_locations ( self , project_name ) : \n    def mkurl_pypi_url ( url ) : \n        loc = posixpath . join ( url , project_url_name ) \n        if not loc . endswith ( '/' ) : \n            loc = loc + '/' \n        return loc \n    project_url_name = urllib_parse . quote ( project_name . lower ( ) ) \n    if self . index_urls : \n        main_index_url = Link ( mkurl_pypi_url ( self . index_urls [ 0 ] ) , trusted = 1 , ) \n        page = self . _get_page ( main_index_url ) \n        if page is None and PyPI . netloc not in str ( main_index_url ) : \n            warnings . warn ( \"Failed to find %r at %s. It is suggested to upgrade \" \"your index to support normalized names as the name in \" \"/simple/{name}.\" % ( project_name , main_index_url ) , RemovedInPip8Warning , ) \n            project_url_name = self . _find_url_name ( Link ( self . index_urls [ 0 ] , trusted = 1 ) , project_url_name , ) or project_url_name \n    if project_url_name is not None : \n        return [ mkurl_pypi_url ( url ) for url in self . index_urls ] \n    return [ ] "}
{"10698": "\ndef _find_all_versions ( self , project_name ) : \n    index_locations = self . _get_index_urls_locations ( project_name ) \n    index_file_loc , index_url_loc = self . _sort_locations ( index_locations ) \n    fl_file_loc , fl_url_loc = self . _sort_locations ( self . find_links , expand_dir = 1 ) \n    dep_file_loc , dep_url_loc = self . _sort_locations ( self . dependency_links ) \n    file_locations = ( Link ( url ) for url in itertools . chain ( index_file_loc , fl_file_loc , dep_file_loc ) ) \n    url_locations = [ link for link in itertools . chain ( ( Link ( url , trusted = 1 ) for url in index_url_loc ) , ( Link ( url , trusted = 1 ) for url in fl_url_loc ) , ( Link ( url ) for url in dep_url_loc ) , ) if self . _validate_secure_origin ( logger , link ) ] \n    logger . debug ( '%d location(s) to search for versions of %s:' , len ( url_locations ) , project_name ) \n    for location in url_locations : \n        logger . debug ( '* %s' , location ) \n    canonical_name = pkg_resources . safe_name ( project_name ) . lower ( ) \n    formats = fmt_ctl_formats ( self . format_control , canonical_name ) \n    search = Search ( project_name . lower ( ) , canonical_name , formats ) \n    find_links_versions = self . _package_versions ( ( Link ( url , '-f' , trusted = 1 ) for url in self . find_links ) , search ) \n    page_versions = [ ] \n    for page in self . _get_pages ( url_locations , project_name ) : \n        logger . debug ( 'Analyzing links from page %s' , page . url ) \n        with indent_log ( ) : \n            page_versions . extend ( self . _package_versions ( page . links , search ) ) \n    dependency_versions = self . _package_versions ( ( Link ( url ) for url in self . dependency_links ) , search ) \n    if dependency_versions : \n        logger . debug ( 'dependency_links found: %s' , ', ' . join ( [ version . location . url for version in dependency_versions ] ) ) \n    file_versions = self . _package_versions ( file_locations , search ) \n    if file_versions : \n        file_versions . sort ( reverse = 1 ) \n        logger . debug ( 'Local files found: %s' , ', ' . join ( [ url_to_path ( candidate . location . url ) for candidate in file_versions ] ) ) \n    return ( file_versions + find_links_versions + page_versions + dependency_versions ) "}
{"10699": "\ndef find_requirement ( self , req , upgrade ) : \n    all_versions = self . _find_all_versions ( req . name ) \n    _versions = set ( req . specifier . filter ( [ x . version for x in all_versions ] , prereleases = ( self . allow_all_prereleases if self . allow_all_prereleases else None ) , ) ) \n    applicable_versions = [ x for x in all_versions if x . version in _versions ] \n    if req . satisfied_by is not None : \n        applicable_versions . insert ( 0 , InstallationCandidate ( req . name , req . satisfied_by . version , INSTALLED_VERSION , ) ) \n        existing_applicable = 1 \n    else : \n        existing_applicable = 0 \n    applicable_versions = self . _sort_versions ( applicable_versions ) \n    if not upgrade and existing_applicable : \n        if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n            logger . debug ( 'Existing installed version (%s) is most up-to-date and ' 'satisfies requirement' , req . satisfied_by . version , ) \n        else : \n            logger . debug ( 'Existing installed version (%s) satisfies requirement ' '(most up-to-date version is %s)' , req . satisfied_by . version , applicable_versions [ 0 ] [ 2 ] , ) \n        return None \n    if not applicable_versions : \n        logger . critical ( 'Could not find a version that satisfies the requirement %s ' '(from versions: %s)' , req , ', ' . join ( sorted ( set ( str ( i . version ) for i in all_versions ) , key = parse_version , ) ) ) \n        if self . need_warn_external : \n            logger . warning ( \"Some externally hosted files were ignored as access to \" \"them may be unreliable (use --allow-external %s to \" \"allow).\" , req . name , ) \n        if self . need_warn_unverified : \n            logger . warning ( \"Some insecure and unverifiable files were ignored\" \" (use --allow-unverified %s to allow).\" , req . name , ) \n        raise DistributionNotFound ( 'No matching distribution found for %s' % req ) \n    if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n        logger . debug ( 'Installed version (%s) is most up-to-date (past versions: ' '%s)' , req . satisfied_by . version , ', ' . join ( str ( i . version ) for i in applicable_versions [ 1 : ] ) or \"none\" , ) \n        raise BestVersionAlreadyInstalled \n    if len ( applicable_versions ) > 1 : \n        logger . debug ( 'Using version %s (newest of versions: %s)' , applicable_versions [ 0 ] . version , ', ' . join ( str ( i . version ) for i in applicable_versions ) ) \n    selected_version = applicable_versions [ 0 ] . location \n    if ( selected_version . verifiable is not None and not selected_version . verifiable ) : \n        logger . warning ( \"%s is potentially insecure and unverifiable.\" , req . name , ) \n    return selected_version "}
{"10701": "\ndef _get_content_type ( url , session ) : \n    scheme , netloc , path , query , fragment = urllib_parse . urlsplit ( url ) \n    if scheme not in ( 'http' , 'https' ) : \n        return '' \n    resp = session . head ( url , allow_redirects = 1 ) \n    resp . raise_for_status ( ) \n    return resp . headers . get ( \"Content-Type\" , \"\" ) "}
{"10703": "\ndef verifiable ( self ) : \n    trusted = self . trusted or getattr ( self . comes_from , \"trusted\" , None ) \n    if trusted is not None and trusted : \n        try : \n            api_version = getattr ( self . comes_from , \"api_version\" , None ) \n            api_version = int ( api_version ) \n        except ( ValueError , TypeError ) : \n            api_version = None \n        if api_version is None or api_version <= 1 : \n            return \n        if self . hash : \n            return 1 \n        else : \n            return 0 \n    elif trusted is not None : \n        return 0 "}
{"10709": "\ndef compile ( marker ) : \n    try : \n        return _cache [ marker ] \n    except KeyError : \n        pass \n    if not marker . strip ( ) : \n        def marker_fn ( environment = None , override = None ) : \n            return 1 \n    else : \n        compiled_marker = compile_marker ( parse_marker ( marker ) ) \n        def marker_fn ( environment = None , override = None ) : \n            if override is None : \n                override = { } \n            if environment is None : \n                environment = default_environment ( ) \n            environment . update ( override ) \n            return eval ( compiled_marker , environment ) \n    marker_fn . __doc__ = marker \n    _cache [ marker ] = marker_fn \n    return _cache [ marker ] "}
{"10717": "\ndef match_request ( self ) : \n    try : \n        url_rule , self . request . view_args = self . url_adapter . match ( return_rule = 1 ) \n        self . request . url_rule = url_rule \n    except HTTPException as e : \n        self . request . routing_exception = e "}
{"10726": "\ndef url_for ( endpoint , ** values ) : \n    appctx = _app_ctx_stack . top \n    reqctx = _request_ctx_stack . top \n    if appctx is None : \n        raise RuntimeError ( 'Attempted to generate a URL without the ' 'application context being pushed. This has to be ' 'executed when application context is available.' ) \n    if reqctx is not None : \n        url_adapter = reqctx . url_adapter \n        blueprint_name = request . blueprint \n        if not reqctx . request . _is_old_module : \n            if endpoint [ : 1 ] == '.' : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + endpoint \n                else : \n                    endpoint = endpoint [ 1 : ] \n        else : \n            if '.' not in endpoint : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + '.' + endpoint \n            elif endpoint . startswith ( '.' ) : \n                endpoint = endpoint [ 1 : ] \n        external = values . pop ( '_external' , 0 ) \n    else : \n        url_adapter = appctx . url_adapter \n        if url_adapter is None : \n            raise RuntimeError ( 'Application was not able to create a URL ' 'adapter for request independent URL generation. ' 'You might be able to fix this by setting ' 'the SERVER_NAME config variable.' ) \n        external = values . pop ( '_external' , 1 ) \n    anchor = values . pop ( '_anchor' , None ) \n    method = values . pop ( '_method' , None ) \n    scheme = values . pop ( '_scheme' , None ) \n    appctx . app . inject_url_defaults ( endpoint , values ) \n    if scheme is not None : \n        if not external : \n            raise ValueError ( 'When specifying _scheme, _external must be True' ) \n        url_adapter . url_scheme = scheme \n    try : \n        rv = url_adapter . build ( endpoint , values , method = method , force_external = external ) \n    except BuildError as error : \n        values [ '_external' ] = external \n        values [ '_anchor' ] = anchor \n        values [ '_method' ] = method \n        return appctx . app . handle_url_build_error ( error , endpoint , values ) \n    if anchor is not None : \n        rv += '#' + url_quote ( anchor ) \n    return rv "}
{"10733": "\ndef root_is_purelib ( name , wheeldir ) : \n    name_folded = name . replace ( \"-\" , \"_\" ) \n    for item in os . listdir ( wheeldir ) : \n        match = dist_info_re . match ( item ) \n        if match and match . group ( 'name' ) == name_folded : \n            with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : \n                for line in wheel : \n                    line = line . lower ( ) . rstrip ( ) \n                    if line == \"root-is-purelib: true\" : \n                        return 1 \n    return 0 "}
{"10741": "\ndef running_under_virtualenv ( ) : \n    if hasattr ( sys , 'real_prefix' ) : \n        return 1 \n    elif sys . prefix != getattr ( sys , \"base_prefix\" , sys . prefix ) : \n        return 1 \n    return 0 "}
{"10743": "\ndef distutils_scheme ( dist_name , user = 0 , home = None , root = None , isolated = 0 ) : \n    from distutils . dist import Distribution \n    scheme = { } \n    if isolated : \n        extra_dist_args = { \"script_args\" : [ \"--no-user-cfg\" ] } \n    else : \n        extra_dist_args = { } \n    dist_args = { 'name' : dist_name } \n    dist_args . update ( extra_dist_args ) \n    d = Distribution ( dist_args ) \n    d . parse_config_files ( ) \n    i = d . get_command_obj ( 'install' , create = 1 ) \n    i . user = user or i . user \n    i . home = home or i . home \n    i . root = root or i . root \n    i . finalize_options ( ) \n    for key in SCHEME_KEYS : \n        scheme [ key ] = getattr ( i , 'install_' + key ) \n    if i . install_lib is not None : \n        scheme . update ( dict ( purelib = i . install_lib , platlib = i . install_lib ) ) \n    if running_under_virtualenv ( ) : \n        scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3 ] , dist_name , ) \n        if root is not None : \n            scheme [ \"headers\" ] = os . path . join ( root , os . path . abspath ( scheme [ \"headers\" ] ) [ 1 : ] , ) \n    return scheme "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = 1 if 'no-cache' in cc else 0 \n    if 'max-age' in cc and cc [ 'max-age' ] == 0 : \n        no_cache = 1 \n    if no_cache : \n        return 0 \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return 0 \n    if resp . status == 301 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return 0 \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age += min_fresh \n    fresh = ( freshness_lifetime > current_age ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return 0 "}
{"10749": "\ndef install_site_py ( self ) : \n    if self . sitepy_installed : \n        return \n    sitepy = os . path . join ( self . install_dir , \"site.py\" ) \n    source = resource_string ( \"setuptools\" , \"site-patch.py\" ) \n    current = \"\" \n    if os . path . exists ( sitepy ) : \n        log . debug ( \"Checking existing site.py in %s\" , self . install_dir ) \n        f = open ( sitepy , 'rb' ) \n        current = f . read ( ) \n        if PY3 : \n            current = current . decode ( ) \n        f . close ( ) \n        if not current . startswith ( 'def __boot():' ) : \n            raise DistutilsError ( \"%s is not a setuptools-generated site.py; please\" \" remove it.\" % sitepy ) \n    if current != source : \n        log . info ( \"Creating %s\" , sitepy ) \n        if not self . dry_run : \n            ensure_directory ( sitepy ) \n            f = open ( sitepy , 'wb' ) \n            f . write ( source ) \n            f . close ( ) \n        self . byte_compile ( [ sitepy ] ) \n    self . sitepy_installed = 1 "}
{"10750": "\ndef save ( self ) : \n    if not self . dirty : \n        return \n    data = '\\n' . join ( map ( self . make_relative , self . paths ) ) \n    if data : \n        log . debug ( \"Saving %s\" , self . filename ) \n        data = ( \"import sys; sys.__plen = len(sys.path)\\n\" \"%s\\n\" \"import sys; new=sys.path[sys.__plen:];\" \" del sys.path[sys.__plen:];\" \" p=getattr(sys,'__egginsert',0); sys.path[p:p]=new;\" \" sys.__egginsert = p+len(new)\\n\" ) % data \n        if os . path . islink ( self . filename ) : \n            os . unlink ( self . filename ) \n        f = open ( self . filename , 'wt' ) \n        f . write ( data ) \n        f . close ( ) \n    elif os . path . exists ( self . filename ) : \n        log . debug ( \"Deleting empty %s\" , self . filename ) \n        os . unlink ( self . filename ) \n    self . dirty = 0 "}
{"10755": "\ndef common_logger_config ( self , logger , config , incremental = 0 ) : \n    level = config . get ( 'level' , None ) \n    if level is not None : \n        logger . setLevel ( _checkLevel ( level ) ) \n    if not incremental : \n        for h in logger . handlers [ : ] : \n            logger . removeHandler ( h ) \n        handlers = config . get ( 'handlers' , None ) \n        if handlers : \n            self . add_handlers ( logger , handlers ) \n        filters = config . get ( 'filters' , None ) \n        if filters : \n            self . add_filters ( logger , filters ) "}
{"10760": "\ndef _generate ( self , source , name , filename , defer_init = 0 ) : \n    return generate ( source , self , name , filename , defer_init = defer_init ) "}
{"10761": "\ndef compile_templates ( self , target , extensions = None , filter_func = None , zip = 'deflated' , log_function = None , ignore_errors = 1 , py_compile = 0 ) : \n    from jinja2 . loaders import ModuleLoader \n    if log_function is None : \n        log_function = lambda x : None \n    if py_compile : \n        if not PY2 or PYPY : \n            from warnings import warn \n            warn ( Warning ( 'py_compile has no effect on pypy or Python 3' ) ) \n            py_compile = 0 \n        else : \n            import imp , marshal \n            py_header = imp . get_magic ( ) + u'\\xff\\xff\\xff\\xff' . encode ( 'iso-8859-15' ) \n            if sys . version_info >= ( 3 , 3 ) : \n                py_header += u'\\x00\\x00\\x00\\x00' . encode ( 'iso-8859-15' ) \n    def write_file ( filename , data , mode ) : \n        if zip : \n            info = ZipInfo ( filename ) \n            info . external_attr = 0o755 << 16 \n            zip_file . writestr ( info , data ) \n        else : \n            f = open ( os . path . join ( target , filename ) , mode ) \n            try : \n                f . write ( data ) \n            finally : \n                f . close ( ) \n    if zip is not None : \n        from zipfile import ZipFile , ZipInfo , ZIP_DEFLATED , ZIP_STORED \n        zip_file = ZipFile ( target , 'w' , dict ( deflated = ZIP_DEFLATED , stored = ZIP_STORED ) [ zip ] ) \n        log_function ( 'Compiling into Zip archive \"%s\"' % target ) \n    else : \n        if not os . path . isdir ( target ) : \n            os . makedirs ( target ) \n        log_function ( 'Compiling into folder \"%s\"' % target ) \n    try : \n        for name in self . list_templates ( extensions , filter_func ) : \n            source , filename , _ = self . loader . get_source ( self , name ) \n            try : \n                code = self . compile ( source , name , filename , 1 , 1 ) \n            except TemplateSyntaxError as e : \n                if not ignore_errors : \n                    raise \n                log_function ( 'Could not compile \"%s\": %s' % ( name , e ) ) \n                continue \n            filename = ModuleLoader . get_module_filename ( name ) \n            if py_compile : \n                c = self . _compile ( code , encode_filename ( filename ) ) \n                write_file ( filename + 'c' , py_header + marshal . dumps ( c ) , 'wb' ) \n                log_function ( 'Byte-compiled \"%s\" as %s' % ( name , filename + 'c' ) ) \n            else : \n                write_file ( filename , code , 'w' ) \n                log_function ( 'Compiled \"%s\" as %s' % ( name , filename ) ) \n    finally : \n        if zip : \n            zip_file . close ( ) \n    log_function ( 'Finished compiling templates' ) "}
{"10763": "\ndef find_eggs_in_zip ( importer , path_item , only = 0 ) : \n    if importer . archive . endswith ( '.whl' ) : \n        return \n    metadata = EggMetadata ( importer ) \n    if metadata . has_metadata ( 'PKG-INFO' ) : \n        yield Distribution . from_filename ( path_item , metadata = metadata ) \n    if only : \n        return \n    for subitem in metadata . resource_listdir ( '/' ) : \n        if subitem . endswith ( '.egg' ) : \n            subpath = os . path . join ( path_item , subitem ) \n            for dist in find_eggs_in_zip ( zipimport . zipimporter ( subpath ) , subpath ) : \n                yield dist "}
{"10764": "\ndef find_on_path ( importer , path_item , only = 0 ) : \n    path_item = _normalize_cached ( path_item ) \n    if os . path . isdir ( path_item ) and os . access ( path_item , os . R_OK ) : \n        if path_item . lower ( ) . endswith ( '.egg' ) : \n            yield Distribution . from_filename ( path_item , metadata = PathMetadata ( path_item , os . path . join ( path_item , 'EGG-INFO' ) ) ) \n        else : \n            for entry in os . listdir ( path_item ) : \n                lower = entry . lower ( ) \n                if lower . endswith ( '.egg-info' ) or lower . endswith ( '.dist-info' ) : \n                    fullpath = os . path . join ( path_item , entry ) \n                    if os . path . isdir ( fullpath ) : \n                        metadata = PathMetadata ( path_item , fullpath ) \n                    else : \n                        metadata = FileMetadata ( fullpath ) \n                    yield Distribution . from_location ( path_item , entry , metadata , precedence = DEVELOP_DIST ) \n                elif not only and lower . endswith ( '.egg' ) : \n                    dists = find_distributions ( os . path . join ( path_item , entry ) ) \n                    for dist in dists : \n                        yield dist \n                elif not only and lower . endswith ( '.egg-link' ) : \n                    with open ( os . path . join ( path_item , entry ) ) as entry_file : \n                        entry_lines = entry_file . readlines ( ) \n                    for line in entry_lines : \n                        if not line . strip ( ) : \n                            continue \n                        path = os . path . join ( path_item , line . rstrip ( ) ) \n                        dists = find_distributions ( path ) \n                        for item in dists : \n                            yield item \n                        break "}
{"10774": "\ndef format ( self , record ) : \n    formatted = logging . Formatter . format ( self , record ) \n    formatted = \"\" . join ( [ ( \" \" * get_indentation ( ) ) + line for line in formatted . splitlines ( 1 ) ] ) \n    return formatted "}
{"10775": "\ndef format_currency ( number , currency , format = None , locale = LC_NUMERIC , currency_digits = 1 , format_type = 'standard' , decimal_quantization = 1 ) : \n    locale = Locale . parse ( locale ) \n    if format : \n        pattern = parse_pattern ( format ) \n    else : \n        try : \n            p = locale . currency_formats [ format_type ] \n            pattern = NumberPattern ( p . pattern , p . prefix , p . suffix , p . grouping , p . int_prec , p . frac_prec , p . exp_prec , p . exp_plus ) \n        except KeyError : \n            raise UnknownCurrencyFormatError ( \"%r is not a known currency format type\" % format_type ) \n    return pattern . apply ( number , locale , currency = currency , currency_digits = currency_digits , decimal_quantization = decimal_quantization ) "}
{"10784": "\ndef fetch_build_egg ( self , req ) : \n    try : \n        cmd = self . _egg_fetcher \n        cmd . package_index . to_scan = [ ] \n    except AttributeError : \n        from setuptools . command . easy_install import easy_install \n        dist = self . __class__ ( { 'script_args' : [ 'easy_install' ] } ) \n        dist . parse_config_files ( ) \n        opts = dist . get_option_dict ( 'easy_install' ) \n        keep = ( 'find_links' , 'site_dirs' , 'index_url' , 'optimize' , 'site_dirs' , 'allow_hosts' ) \n        for key in list ( opts ) : \n            if key not in keep : \n                del opts [ key ] \n        if self . dependency_links : \n            links = self . dependency_links [ : ] \n            if 'find_links' in opts : \n                links = opts [ 'find_links' ] [ 1 ] . split ( ) + links \n            opts [ 'find_links' ] = ( 'setup' , links ) \n        install_dir = self . get_egg_cache_dir ( ) \n        cmd = easy_install ( dist , args = [ \"x\" ] , install_dir = install_dir , exclude_scripts = 1 , always_copy = 0 , build_directory = None , editable = 0 , upgrade = 0 , multi_version = 1 , no_report = 1 , user = 0 ) \n        cmd . ensure_finalized ( ) \n        self . _egg_fetcher = cmd \n    return cmd . easy_install ( req ) "}
{"10797": "\ndef _check_skip_installed ( self , req_to_install , finder ) : \n    req_to_install . check_if_exists ( ) \n    if req_to_install . satisfied_by : \n        skip_reason = 'satisfied (use --upgrade to upgrade)' \n        if self . upgrade : \n            best_installed = 0 \n            if not ( self . force_reinstall or req_to_install . link ) : \n                try : \n                    finder . find_requirement ( req_to_install , self . upgrade ) \n                except BestVersionAlreadyInstalled : \n                    skip_reason = 'up-to-date' \n                    best_installed = 1 \n                except DistributionNotFound : \n                    pass \n            if not best_installed : \n                if not ( self . use_user_site and not dist_in_usersite ( req_to_install . satisfied_by ) ) : \n                    req_to_install . conflicts_with = req_to_install . satisfied_by \n                req_to_install . satisfied_by = None \n        return skip_reason \n    else : \n        return None "}
{"10801": "\ndef tokenize_annotated ( doc , annotation ) : \n    tokens = tokenize ( doc , include_hrefs = 0 ) \n    for tok in tokens : \n        tok . annotation = annotation \n    return tokens "}
{"10806": "\ndef expand_tokens ( tokens , equal = 0 ) : \n    for token in tokens : \n        for pre in token . pre_tags : \n            yield pre \n        if not equal or not token . hide_when_equal : \n            if token . trailing_whitespace : \n                yield token . html ( ) + token . trailing_whitespace \n            else : \n                yield token . html ( ) \n        for post in token . post_tags : \n            yield post "}
{"10809": "\ndef flatten_el ( el , include_hrefs , skip_tag = 0 ) : \n    if not skip_tag : \n        if el . tag == 'img' : \n            yield ( 'img' , el . get ( 'src' ) , start_tag ( el ) ) \n        else : \n            yield start_tag ( el ) \n    if el . tag in empty_tags and not el . text and not len ( el ) and not el . tail : \n        return \n    start_words = split_words ( el . text ) \n    for word in start_words : \n        yield html_escape ( word ) \n    for child in el : \n        for item in flatten_el ( child , include_hrefs = include_hrefs ) : \n            yield item \n    if el . tag == 'a' and el . get ( 'href' ) and include_hrefs : \n        yield ( 'href' , el . get ( 'href' ) ) \n    if not skip_tag : \n        yield end_tag ( el ) \n        end_words = split_words ( el . tail ) \n        for word in end_words : \n            yield html_escape ( word ) "}
{"10811": "\ndef start_tag ( el ) : \n    return '<%s%s>' % ( el . tag , '' . join ( [ ' %s=\"%s\"' % ( name , html_escape ( value , 1 ) ) for name , value in el . attrib . items ( ) ] ) ) "}
{"10813": "\ndef serialize_html_fragment ( el , skip_outer = 0 ) : \n    assert not isinstance ( el , basestring ) , ( \"You should pass in an element, not a string like %r\" % el ) \n    html = etree . tostring ( el , method = \"html\" , encoding = _unicode ) \n    if skip_outer : \n        html = html [ html . find ( '>' ) + 1 : ] \n        html = html [ : html . rfind ( '<' ) ] \n        return html . strip ( ) \n    else : \n        return html "}
{"10819": "\ndef document_fromstring ( html , guess_charset = 1 , parser = None ) : \n    if not isinstance ( html , _strings ) : \n        raise TypeError ( 'string required' ) \n    if parser is None : \n        parser = html_parser \n    return parser . parse ( html , useChardet = guess_charset ) . getroot ( ) "}
{"10822": "\ndef export ( self , location ) : \n    url , rev = self . get_url_rev ( ) \n    rev_options = get_rev_options ( url , rev ) \n    logger . info ( 'Exporting svn repository %s to %s' , url , location ) \n    with indent_log ( ) : \n        if os . path . exists ( location ) : \n            rmtree ( location ) \n        self . run_command ( [ 'export' ] + rev_options + [ url , location ] , show_stdout = 0 ) "}
{"10830": "\ndef trap_http_exception ( self , e ) : \n    if self . config [ 'TRAP_HTTP_EXCEPTIONS' ] : \n        return 1 \n    if self . config [ 'TRAP_BAD_REQUEST_ERRORS' ] : \n        return isinstance ( e , BadRequest ) \n    return 0 "}
{"10841": "\ndef modules ( self ) : \n    sys . path . insert ( 0 , self . basedir ) \n    for p in self . paths ( ) : \n        try : \n            module_name = self . module_path ( p ) \n            logger . debug ( \"Importing {} from path {}\" . format ( module_name , p ) ) \n            m = importlib . import_module ( module_name ) \n            yield m \n        except Exception as e : \n            logger . warning ( 'Caught exception while importing {}: {}' . format ( p , e ) ) \n            logger . warning ( e , exc_info = 1 ) \n            error_info = getattr ( self , 'error_info' , None ) \n            if not error_info : \n                exc_info = sys . exc_info ( ) \n                self . error_info = exc_info \n            continue \n    sys . path . pop ( 0 ) "}
{"10842": "\ndef classes ( self ) : \n    for module in self . modules ( ) : \n        cs = inspect . getmembers ( module , inspect . isclass ) \n        class_name = getattr ( self , 'class_name' , '' ) \n        class_regex = '' \n        if class_name : \n            if class_name . startswith ( \"*\" ) : \n                class_name = class_name . strip ( \"*\" ) \n                class_regex = re . compile ( r'.*?{}' . format ( class_name ) , re . I ) \n            else : \n                class_regex = re . compile ( r'^{}' . format ( class_name ) , re . I ) \n        for c_name , c in cs : \n            can_yield = 1 \n            if class_regex and not class_regex . match ( c_name ) : \n                can_yield = 0 \n            if can_yield and issubclass ( c , unittest . TestCase ) : \n                if c is not unittest . TestCase : \n                    logger . debug ( 'class: {} matches {}' . format ( c_name , class_name ) ) \n                    yield c "}
{"10843": "\ndef method_names ( self ) : \n    for c in self . classes ( ) : \n        ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) \n        method_name = getattr ( self , 'method_name' , '' ) \n        method_regex = '' \n        if method_name : \n            if method_name . startswith ( self . method_prefix ) : \n                method_regex = re . compile ( r'^{}' . format ( method_name ) , flags = re . I ) \n            else : \n                if method_name . startswith ( \"*\" ) : \n                    method_name = method_name . strip ( \"*\" ) \n                    method_regex = re . compile ( r'^{}[_]{{0,1}}.*?{}' . format ( self . method_prefix , method_name ) , flags = re . I ) \n                else : \n                    method_regex = re . compile ( r'^{}[_]{{0,1}}{}' . format ( self . method_prefix , method_name ) , flags = re . I ) \n        for m_name , m in ms : \n            if not m_name . startswith ( self . method_prefix ) : \n                continue \n            can_yield = 1 \n            if method_regex and not method_regex . match ( m_name ) : \n                can_yield = 0 \n            if can_yield : \n                logger . debug ( 'method: {} matches {}' . format ( m_name , method_name ) ) \n                yield c , m_name "}
{"10844": "\ndef _find_basename ( self , name , basenames , is_prefix = 0 ) : \n    ret = \"\" \n    fileroots = [ ( os . path . splitext ( n ) [ 0 ] , n ) for n in basenames ] \n    glob = 0 \n    if name . startswith ( \"*\" ) : \n        glob = 1 \n    name = name . strip ( \"*\" ) \n    for fileroot , basename in fileroots : \n        if name in fileroot or fileroot in name : \n            for pf in self . module_postfixes : \n                logger . debug ( 'Checking if basename {} starts with {} and ends with {}' . format ( basename , name , pf ) ) \n                if glob : \n                    if name in fileroot and fileroot . endswith ( pf ) : \n                        ret = basename \n                        break \n                else : \n                    if fileroot . startswith ( name ) and fileroot . endswith ( pf ) : \n                        ret = basename \n                        break \n            if not ret : \n                for pf in self . module_prefixes : \n                    n = pf + name \n                    logger . debug ( 'Checking if basename {} starts with {}' . format ( basename , n ) ) \n                    if glob : \n                        if fileroot . startswith ( pf ) and name in fileroot : \n                            ret = basename \n                            break \n                    else : \n                        if fileroot . startswith ( n ) : \n                            ret = basename \n                            break \n            if not ret : \n                if is_prefix : \n                    logger . debug ( 'Checking if basename {} starts with {}' . format ( basename , name ) ) \n                    if basename . startswith ( name ) or ( glob and name in basename ) : \n                        ret = basename \n                    else : \n                        logger . debug ( 'Checking if basename {} starts with {} and is a test module' . format ( basename , name ) ) \n                        if glob : \n                            if name in basename and self . _is_module_path ( basename ) : \n                                ret = basename \n                        else : \n                            if basename . startswith ( name ) and self . _is_module_path ( basename ) : \n                                ret = basename \n            if ret : \n                logger . debug ( 'Found basename {}' . format ( ret ) ) \n                break \n    return ret "}
{"10845": "\ndef _is_module_path ( self , path ) : \n    ret = 0 \n    basename = os . path . basename ( path ) \n    fileroot = os . path . splitext ( basename ) [ 0 ] \n    for pf in self . module_postfixes : \n        if fileroot . endswith ( pf ) : \n            ret = 1 \n            break \n    if not ret : \n        for pf in self . module_prefixes : \n            if fileroot . startswith ( pf ) : \n                ret = 1 \n                break \n    return ret "}
{"10846": "\ndef walk ( self , basedir ) : \n    system_d = SitePackagesDir ( ) \n    filter_system_d = system_d and os . path . commonprefix ( [ system_d , basedir ] ) != system_d \n    for root , dirs , files in os . walk ( basedir , topdown = 1 ) : \n        dirs [ : ] = [ d for d in dirs if d [ 0 ] != '.' and d [ 0 ] != \"_\" ] \n        if filter_system_d : \n            dirs [ : ] = [ d for d in dirs if not d . startswith ( system_d ) ] \n        yield root , dirs , files "}
{"10847": "\ndef paths ( self ) : \n    module_name = getattr ( self , 'module_name' , '' ) \n    module_prefix = getattr ( self , 'prefix' , '' ) \n    filepath = getattr ( self , 'filepath' , '' ) \n    if filepath : \n        if os . path . isabs ( filepath ) : \n            yield filepath \n        else : \n            yield os . path . join ( self . basedir , filepath ) \n    else : \n        if module_prefix : \n            basedirs = self . _find_prefix_paths ( self . basedir , module_prefix ) \n        else : \n            basedirs = [ self . basedir ] \n        for basedir in basedirs : \n            try : \n                if module_name : \n                    path = self . _find_module_path ( basedir , module_name ) \n                else : \n                    path = basedir \n                if os . path . isfile ( path ) : \n                    logger . debug ( 'Module path: {}' . format ( path ) ) \n                    yield path \n                else : \n                    seen_paths = set ( ) \n                    for root , dirs , files in self . walk ( path ) : \n                        for basename in files : \n                            if basename . startswith ( \"__init__\" ) : \n                                if self . _is_module_path ( root ) : \n                                    filepath = os . path . join ( root , basename ) \n                                    if filepath not in seen_paths : \n                                        logger . debug ( 'Module package path: {}' . format ( filepath ) ) \n                                        seen_paths . add ( filepath ) \n                                        yield filepath \n                            else : \n                                fileroot = os . path . splitext ( basename ) [ 0 ] \n                                for pf in self . module_postfixes : \n                                    if fileroot . endswith ( pf ) : \n                                        filepath = os . path . join ( root , basename ) \n                                        if filepath not in seen_paths : \n                                            logger . debug ( 'Module postfix path: {}' . format ( filepath ) ) \n                                            seen_paths . add ( filepath ) \n                                            yield filepath \n                                for pf in self . module_prefixes : \n                                    if fileroot . startswith ( pf ) : \n                                        filepath = os . path . join ( root , basename ) \n                                        if filepath not in seen_paths : \n                                            logger . debug ( 'Module prefix path: {}' . format ( filepath ) ) \n                                            seen_paths . add ( filepath ) \n                                            yield filepath \n            except IOError as e : \n                logger . warning ( e , exc_info = 1 ) \n                pass "}
{"10848": "\ndef _dump_arg_defaults ( kwargs ) : \n    if current_app : \n        kwargs . setdefault ( 'cls' , current_app . json_encoder ) \n        if not current_app . config [ 'JSON_AS_ASCII' ] : \n            kwargs . setdefault ( 'ensure_ascii' , 0 ) \n        kwargs . setdefault ( 'sort_keys' , current_app . config [ 'JSON_SORT_KEYS' ] ) \n    else : \n        kwargs . setdefault ( 'sort_keys' , 1 ) \n        kwargs . setdefault ( 'cls' , JSONEncoder ) "}
{"10850": "\ndef set_many ( self , mapping , timeout = None ) : \n    rv = 1 \n    for key , value in _items ( mapping ) : \n        if not self . set ( key , value , timeout ) : \n            rv = 0 \n    return rv "}
{"10858": "\ndef get_data ( self , cache = 1 , as_text = 0 , parse_form_data = 0 ) : \n    rv = getattr ( self , '_cached_data' , None ) \n    if rv is None : \n        if parse_form_data : \n            self . _load_form_data ( ) \n        rv = self . stream . read ( ) \n        if cache : \n            self . _cached_data = rv \n    if as_text : \n        rv = rv . decode ( self . charset , self . encoding_errors ) \n    return rv "}
{"10859": "\ndef get_wsgi_headers ( self , environ ) : \n    headers = Headers ( self . headers ) \n    location = None \n    content_location = None \n    content_length = None \n    status = self . status_code \n    for key , value in headers : \n        ikey = key . lower ( ) \n        if ikey == u'location' : \n            location = value \n        elif ikey == u'content-location' : \n            content_location = value \n        elif ikey == u'content-length' : \n            content_length = value \n    if location is not None : \n        old_location = location \n        if isinstance ( location , text_type ) : \n            location = iri_to_uri ( location , safe_conversion = 1 ) \n        if self . autocorrect_location_header : \n            current_url = get_current_url ( environ , root_only = 1 ) \n            if isinstance ( current_url , text_type ) : \n                current_url = iri_to_uri ( current_url ) \n            location = url_join ( current_url , location ) \n        if location != old_location : \n            headers [ 'Location' ] = location \n    if content_location is not None and isinstance ( content_location , text_type ) : \n        headers [ 'Content-Location' ] = iri_to_uri ( content_location ) \n    if 100 <= status < 200 or status == 204 : \n        headers [ 'Content-Length' ] = content_length = u'0' \n    elif status == 304 : \n        remove_entity_headers ( headers ) \n    if self . automatically_set_content_length and self . is_sequence and content_length is None and status != 304 : \n        try : \n            content_length = sum ( len ( to_bytes ( x , 'ascii' ) ) for x in self . response ) \n        except UnicodeError : \n            pass \n        else : \n            headers [ 'Content-Length' ] = str ( content_length ) \n    return headers "}
{"10860": "\ndef iri_to_uri ( iri , charset = 'utf-8' , errors = 'strict' , safe_conversion = 0 ) : \n    if isinstance ( iri , tuple ) : \n        iri = url_unparse ( iri ) \n    if safe_conversion : \n        try : \n            native_iri = to_native ( iri ) \n            ascii_iri = to_native ( iri ) . encode ( 'ascii' ) \n            if ascii_iri . split ( ) == [ ascii_iri ] : \n                return native_iri \n        except UnicodeError : \n            pass \n    iri = url_parse ( to_unicode ( iri , charset , errors ) ) \n    netloc = iri . encode_netloc ( ) \n    path = url_quote ( iri . path , charset , errors , '/:~+%' ) \n    query = url_quote ( iri . query , charset , errors , '%&[]:;$*()+,!?*/=' ) \n    fragment = url_quote ( iri . fragment , charset , errors , '=%&[]:;$()+,!?*/' ) \n    return to_native ( url_unparse ( ( iri . scheme , netloc , path , query , fragment ) ) ) "}
{"10862": "\ndef user_data_dir ( appname , roaming = 0 ) : \n    if WINDOWS : \n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\" \n        path = os . path . join ( os . path . normpath ( _get_win_folder ( const ) ) , appname ) \n    elif sys . platform == \"darwin\" : \n        path = os . path . join ( os . path . expanduser ( '~/Library/Application Support/' ) , appname , ) \n    else : \n        path = os . path . join ( os . getenv ( 'XDG_DATA_HOME' , os . path . expanduser ( \"~/.local/share\" ) ) , appname , ) \n    return path "}
{"10864": "\ndef user_config_dir ( appname , roaming = 1 ) : \n    if WINDOWS : \n        path = user_data_dir ( appname , roaming = roaming ) \n    elif sys . platform == \"darwin\" : \n        path = user_data_dir ( appname ) \n    else : \n        path = os . getenv ( 'XDG_CONFIG_HOME' , os . path . expanduser ( \"~/.config\" ) ) \n        path = os . path . join ( path , appname ) \n    return path "}
{"10868": "\ndef to_text ( s , blank_if_none = 1 ) : \n    if s is None : \n        if blank_if_none : \n            return \"\" \n        else : \n            return None \n    elif isinstance ( s , text_type ) : \n        return s \n    else : \n        return text_type ( s ) "}
{"10870": "\ndef parse ( doc , treebuilder = \"etree\" , encoding = None , namespaceHTMLElements = 1 ) : \n    tb = treebuilders . getTreeBuilder ( treebuilder ) \n    p = HTMLParser ( tb , namespaceHTMLElements = namespaceHTMLElements ) \n    return p . parse ( doc , encoding = encoding ) "}
{"10871": "\ndef parse ( self , stream , encoding = None , parseMeta = 1 , useChardet = 1 ) : \n    self . _parse ( stream , innerHTML = 0 , encoding = encoding , parseMeta = parseMeta , useChardet = useChardet ) \n    return self . tree . getDocument ( ) "}
{"10872": "\ndef parseFragment ( self , stream , container = \"div\" , encoding = None , parseMeta = 0 , useChardet = 1 ) : \n    self . _parse ( stream , 1 , container = container , encoding = encoding ) \n    return self . tree . getFragment ( ) "}
{"10873": "\ndef translate ( self , word ) : \n    if ( word not in self . transmissions ) : \n        raise NoMatchError ( 'no matches found' ) \n    else : \n        trans = self . transmissions [ word ] \n        return sorted ( ( ( k , v ) for k , v in trans . iteritems ( ) if v != 0 ) , reverse = 1 ) "}
{"10879": "\ndef run_command ( self , cmd , show_stdout = 1 , cwd = None , raise_on_returncode = 1 , command_level = logging . DEBUG , command_desc = None , extra_environ = None ) : \n    cmd = [ self . name ] + cmd \n    try : \n        return call_subprocess ( cmd , show_stdout , cwd , raise_on_returncode , command_level , command_desc , extra_environ ) \n    except OSError as e : \n        if e . errno == errno . ENOENT : \n            raise BadCommand ( 'Cannot find command %r' % self . name ) \n        else : \n            raise "}
{"10884": "\ndef process_url ( self , url , retrieve = 0 ) : \n    if url in self . scanned_urls and not retrieve : \n        return \n    self . scanned_urls [ url ] = 1 \n    if not URL_SCHEME ( url ) : \n        self . process_filename ( url ) \n        return \n    else : \n        dists = list ( distros_for_url ( url ) ) \n        if dists : \n            if not self . url_ok ( url ) : \n                return \n            self . debug ( \"Found link: %s\" , url ) \n    if dists or not retrieve or url in self . fetched_urls : \n        list ( map ( self . add , dists ) ) \n        return \n    if not self . url_ok ( url ) : \n        self . fetched_urls [ url ] = 1 \n        return \n    self . info ( \"Reading %s\" , url ) \n    self . fetched_urls [ url ] = 1 \n    f = self . open_url ( url , \"Download error on %s: %%s -- Some packages may not be found!\" % url ) \n    if f is None : \n        return \n    self . fetched_urls [ f . url ] = 1 \n    if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : \n        f . close ( ) \n        return \n    base = f . url \n    page = f . read ( ) \n    if not isinstance ( page , str ) : \n        if isinstance ( f , HTTPError ) : \n            charset = 'latin-1' \n        else : \n            charset = f . headers . get_param ( 'charset' ) or 'latin-1' \n        page = page . decode ( charset , \"ignore\" ) \n    f . close ( ) \n    for match in HREF . finditer ( page ) : \n        link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) \n        self . process_url ( link ) \n    if url . startswith ( self . index_url ) and getattr ( f , 'code' , None ) != 404 : \n        page = self . process_index ( url , page ) "}
{"10889": "\ndef check_enableusersite ( ) : \n    if hasattr ( sys , 'flags' ) and getattr ( sys . flags , 'no_user_site' , 0 ) : \n        return 0 \n    if hasattr ( os , \"getuid\" ) and hasattr ( os , \"geteuid\" ) : \n        if os . geteuid ( ) != os . getuid ( ) : \n            return None \n    if hasattr ( os , \"getgid\" ) and hasattr ( os , \"getegid\" ) : \n        if os . getegid ( ) != os . getgid ( ) : \n            return None \n    return 1 "}
{"10896": "\ndef Popen_nonblocking ( * args , ** kwargs ) : \n    kwargs . setdefault ( 'close_fds' , 'posix' in sys . builtin_module_names ) \n    kwargs . setdefault ( 'bufsize' , 1 ) \n    proc = subprocess . Popen ( * args , ** kwargs ) \n    if proc . stdout : \n        q = queue . Queue ( ) \n        t = threading . Thread ( target = enqueue_lines , args = ( proc . stdout , q ) ) \n        proc . stdout = q \n        t . daemon = 1 \n        t . start ( ) \n    if proc . stderr : \n        q = queue . Queue ( ) \n        t = threading . Thread ( target = enqueue_lines , args = ( proc . stderr , q ) ) \n        proc . stderr = q \n        t . daemon = 1 \n        t . start ( ) \n    return proc "}
{"10897": "\ndef have_pyrex ( ) : \n    pyrex_impls = 'Cython.Distutils.build_ext' , 'Pyrex.Distutils.build_ext' \n    for pyrex_impl in pyrex_impls : \n        try : \n            __import__ ( pyrex_impl , fromlist = [ 'build_ext' ] ) . build_ext \n            return 1 \n        except Exception : \n            pass \n    return 0 "}
{"10899": "\ndef debug_application ( self , environ , start_response ) : \n    app_iter = None \n    try : \n        app_iter = self . app ( environ , start_response ) \n        for item in app_iter : \n            yield item \n        if hasattr ( app_iter , 'close' ) : \n            app_iter . close ( ) \n    except Exception : \n        if hasattr ( app_iter , 'close' ) : \n            app_iter . close ( ) \n        traceback = get_current_traceback ( skip = 1 , show_hidden_frames = self . show_hidden_frames , ignore_system_exceptions = 1 ) \n        for frame in traceback . frames : \n            self . frames [ frame . id ] = frame \n        self . tracebacks [ traceback . id ] = traceback \n        try : \n            start_response ( '500 INTERNAL SERVER ERROR' , [ ( 'Content-Type' , 'text/html; charset=utf-8' ) , ( 'X-XSS-Protection' , '0' ) , ] ) \n        except Exception : \n            environ [ 'wsgi.errors' ] . write ( 'Debugging middleware caught exception in streamed ' 'response at a point where response headers were already ' 'sent.\\n' ) \n        else : \n            yield traceback . render_full ( evalex = self . evalex , secret = self . secret ) . encode ( 'utf-8' , 'replace' ) \n        traceback . log ( environ [ 'wsgi.errors' ] ) "}
{"10901": "\ndef user_agent ( ) : \n    data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . __version__ } , \"python\" : platform . python_version ( ) , \"implementation\" : { \"name\" : platform . python_implementation ( ) , } , } \n    if data [ \"implementation\" ] [ \"name\" ] == 'CPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'PyPy' : \n        if sys . pypy_version_info . releaselevel == 'final' : \n            pypy_version_info = sys . pypy_version_info [ : 3 ] \n        else : \n            pypy_version_info = sys . pypy_version_info \n        data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy_version_info ] ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'Jython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'IronPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    if sys . platform . startswith ( \"linux\" ) : \n        distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux_distribution ( ) ) , ) ) \n        libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"lib\" , \"version\" ] , platform . libc_ver ( ) ) , ) ) \n        if libc : \n            distro [ \"libc\" ] = libc \n        if distro : \n            data [ \"distro\" ] = distro \n    if sys . platform . startswith ( \"darwin\" ) and platform . mac_ver ( ) [ 0 ] : \n        data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac_ver ( ) [ 0 ] } \n    if platform . system ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) \n    if platform . release ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) \n    if platform . machine ( ) : \n        data [ \"cpu\" ] = platform . machine ( ) \n    return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort_keys = 1 ) , ) "}
{"10902": "\ndef is_url ( name ) : \n    if ':' not in name : \n        return 0 \n    scheme = name . split ( ':' , 1 ) [ 0 ] . lower ( ) \n    return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all_schemes "}
{"10903": "\ndef unpack_file_url ( link , location , download_dir = None ) : \n    link_path = url_to_path ( link . url_without_fragment ) \n    if os . path . isdir ( link_path ) : \n        if os . path . isdir ( location ) : \n            rmtree ( location ) \n        shutil . copytree ( link_path , location , symlinks = 1 ) \n        if download_dir : \n            logger . info ( 'Link is a directory, ignoring download_dir' ) \n        return \n    if link . hash : \n        link_path_hash = _get_hash_from_file ( link_path , link ) \n        _check_hash ( link_path_hash , link ) \n    already_downloaded_path = None \n    if download_dir : \n        already_downloaded_path = _check_download_dir ( link , download_dir ) \n    if already_downloaded_path : \n        from_path = already_downloaded_path \n    else : \n        from_path = link_path \n    content_type = mimetypes . guess_type ( from_path ) [ 0 ] \n    unpack_file ( from_path , location , content_type , link ) \n    if download_dir and not already_downloaded_path : \n        _copy_file ( from_path , download_dir , content_type , link ) "}
{"10904": "\ndef _download_http_url ( link , session , temp_dir ) : \n    target_url = link . url . split ( '#' , 1 ) [ 0 ] \n    try : \n        resp = session . get ( target_url , headers = { \"Accept-Encoding\" : \"identity\" } , stream = 1 , ) \n        resp . raise_for_status ( ) \n    except requests . HTTPError as exc : \n        logger . critical ( \"HTTP error %s while getting %s\" , exc . response . status_code , link , ) \n        raise \n    content_type = resp . headers . get ( 'content-type' , '' ) \n    filename = link . filename \n    content_disposition = resp . headers . get ( 'content-disposition' ) \n    if content_disposition : \n        type , params = cgi . parse_header ( content_disposition ) \n        filename = params . get ( 'filename' ) or filename \n    ext = splitext ( filename ) [ 1 ] \n    if not ext : \n        ext = mimetypes . guess_extension ( content_type ) \n        if ext : \n            filename += ext \n    if not ext and link . url != resp . url : \n        ext = os . path . splitext ( resp . url ) [ 1 ] \n        if ext : \n            filename += ext \n    file_path = os . path . join ( temp_dir , filename ) \n    with open ( file_path , 'wb' ) as content_file : \n        _download_url ( resp , link , content_file ) \n    return file_path , content_type "}
{"10906": "\ndef currencyFormat ( _context , code , symbol , format , currency_digits = 1 , decimal_quantization = 1 , name = '' ) : \n    _context . action ( discriminator = ( 'currency' , name , code ) , callable = _register_currency , args = ( name , code , symbol , format , currency_digits , decimal_quantization ) ) "}
{"10914": "\ndef is_declared ( self , name ) : \n    if name in self . declared_locally or name in self . declared_parameter : \n        return 1 \n    return name in self . declared "}
{"10923": "\ndef populate_requirement_set ( requirement_set , args , options , finder , session , name , wheel_cache ) : \n    for req in args : \n        requirement_set . add_requirement ( InstallRequirement . from_line ( req , None , isolated = options . isolated_mode , wheel_cache = wheel_cache ) ) \n    for req in options . editables : \n        requirement_set . add_requirement ( InstallRequirement . from_editable ( req , default_vcs = options . default_vcs , isolated = options . isolated_mode , wheel_cache = wheel_cache ) ) \n    found_req_in_file = 0 \n    for filename in options . requirements : \n        for req in parse_requirements ( filename , finder = finder , options = options , session = session , wheel_cache = wheel_cache ) : \n            found_req_in_file = 1 \n            requirement_set . add_requirement ( req ) \n    if not ( args or options . editables or found_req_in_file ) : \n        opts = { 'name' : name } \n        if options . find_links : \n            msg = ( 'You must give at least one requirement to ' '%(name)s (maybe you meant \"pip %(name)s ' '%(links)s\"?)' % dict ( opts , links = ' ' . join ( options . find_links ) ) ) \n        else : \n            msg = ( 'You must give at least one requirement ' 'to %(name)s (see \"pip help %(name)s\")' % opts ) \n        logger . warning ( msg ) "}
{"10924": "\ndef export ( self , location ) : \n    temp_dir = tempfile . mkdtemp ( '-export' , 'pip-' ) \n    self . unpack ( temp_dir ) \n    if os . path . exists ( location ) : \n        rmtree ( location ) \n    try : \n        self . run_command ( [ 'export' , location ] , cwd = temp_dir , show_stdout = 0 ) \n    finally : \n        rmtree ( temp_dir ) "}
{"10925": "\ndef lookup ( self , ResponseGroup = \"Large\" , ** kwargs ) : \n    response = self . api . ItemLookup ( ResponseGroup = ResponseGroup , ** kwargs ) \n    root = objectify . fromstring ( response ) \n    if root . Items . Request . IsValid == 'False' : \n        code = root . Items . Request . Errors . Error . Code \n        msg = root . Items . Request . Errors . Error . Message \n        raise LookupException ( \"Amazon Product Lookup Error: '{0}', '{1}'\" . format ( code , msg ) ) \n    if not hasattr ( root . Items , 'Item' ) : \n        raise AsinNotFound ( \"ASIN(s) not found: '{0}'\" . format ( etree . tostring ( root , pretty_print = 1 ) ) ) \n    if len ( root . Items . Item ) > 1 : \n        return [ AmazonProduct ( item , self . aws_associate_tag , self , region = self . region ) for item in root . Items . Item ] \n    else : \n        return AmazonProduct ( root . Items . Item , self . aws_associate_tag , self , region = self . region ) "}
{"10926": "\ndef iterate_pages ( self ) : \n    try : \n        while 1 : \n            yield self . _query ( ItemPage = self . current_page , ** self . kwargs ) \n            self . current_page += 1 \n    except NoMorePages : \n        pass "}
{"10934": "\ndef send ( self , request , ** kw ) : \n    if request . method == 'GET' : \n        cached_response = self . controller . cached_request ( request ) \n        if cached_response : \n            return self . build_response ( request , cached_response , from_cache = 1 ) \n        request . headers . update ( self . controller . conditional_headers ( request ) ) \n    resp = super ( CacheControlAdapter , self ) . send ( request , ** kw ) \n    return resp "}
{"10935": "\ndef build_response ( self , request , response , from_cache = 0 ) : \n    if not from_cache and request . method == 'GET' : \n        if response . status == 304 : \n            cached_response = self . controller . update_cached_response ( request , response ) \n            if cached_response is not response : \n                from_cache = 1 \n            response . read ( decode_content = 0 ) \n            response . release_conn ( ) \n            response = cached_response \n        elif response . status == 301 : \n            self . controller . cache_response ( request , response ) \n        else : \n            if self . heuristic : \n                response = self . heuristic . apply ( response ) \n            response . _fp = CallbackFileWrapper ( response . _fp , functools . partial ( self . controller . cache_response , request , response , ) ) \n    resp = super ( CacheControlAdapter , self ) . build_response ( request , response ) \n    if request . method in self . invalidating_methods and resp . ok : \n        cache_url = self . controller . cache_url ( request . url ) \n        self . cache . delete ( cache_url ) \n    resp . from_cache = from_cache \n    return resp "}
{"10938": "\ndef do_sort ( environment , value , reverse = 0 , case_sensitive = 0 , attribute = None ) : \n    if not case_sensitive : \n        def sort_func ( item ) : \n            if isinstance ( item , string_types ) : \n                item = item . lower ( ) \n            return item \n    else : \n        sort_func = None \n    if attribute is not None : \n        getter = make_attrgetter ( environment , attribute ) \n        def sort_func ( item , processor = sort_func or ( lambda x : x ) ) : \n            return processor ( getter ( item ) ) \n    return sorted ( value , key = sort_func , reverse = reverse ) "}
{"10947": "\ndef verify_signature ( self , value , sig ) : \n    key = self . derive_key ( ) \n    try : \n        sig = base64_decode ( sig ) \n    except Exception : \n        return 0 \n    return self . algorithm . verify_signature ( key , value , sig ) "}
{"10950": "\ndef validate ( self , signed_value , max_age = None ) : \n    try : \n        self . unsign ( signed_value , max_age = max_age ) \n        return 1 \n    except BadSignature : \n        return 0 "}
{"10955": "\ndef _all_dirs ( base_path ) : \n    for root , dirs , files in os . walk ( base_path , followlinks = 1 ) : \n        for dir in dirs : \n            yield os . path . relpath ( os . path . join ( root , dir ) , base_path ) "}
{"10956": "\ndef prepare_response ( self , request , cached ) : \n    if \"*\" in cached . get ( \"vary\" , { } ) : \n        return \n    for header , value in cached . get ( \"vary\" , { } ) . items ( ) : \n        if request . headers . get ( header , None ) != value : \n            return \n    body_raw = cached [ \"response\" ] . pop ( \"body\" ) \n    try : \n        body = io . BytesIO ( body_raw ) \n    except TypeError : \n        body = io . BytesIO ( body_raw . encode ( 'utf8' ) ) \n    return HTTPResponse ( body = body , preload_content = 0 , ** cached [ \"response\" ] ) "}
{"10966": "\ndef normal_left_dclick ( self , event ) : \n    x = event . x \n    y = event . y \n    component = self . component \n    if hasattr ( component , \"element\" ) : \n        if component . element is not None : \n            component . active_tool = self \n            component . element . edit_traits ( kind = \"livemodal\" ) \n            event . handled = 1 \n            component . active_tool = None \n            component . request_redraw ( ) \n    return "}
{"10971": "\ndef unmap_model ( self , old ) : \n    for node_mapping in self . nodes : \n        ct = node_mapping . containment_trait \n        if hasattr ( old , ct ) : \n            old_elements = getattr ( old , ct ) \n            for old_element in old_elements : \n                old . on_trait_change ( self . map_element , ct + \"_items\" , remove = 1 ) "}
{"10981": "\ndef render_grid_file ( context , f ) : \n    f . seek ( 0 ) \n    response = context . response \n    if __debug__ : \n        response . headers [ 'Grid-ID' ] = str ( f . _id ) \n        log . debug ( \"Serving GridFS file.\" , extra = dict ( identifier = str ( f . _id ) , filename = f . filename , length = f . length , mimetype = f . content_type ) ) \n    response . conditional_response = 1 \n    response . accept_ranges = 'bytes' \n    response . content_type = f . content_type \n    response . content_length = f . length \n    response . content_md5 = response . etag = f . md5 \n    response . last_modified = f . metadata . get ( 'modified' , None ) \n    response . content_disposition = 'attachment; filename=' + f . name \n    if context . request . if_range . match_response ( response ) : \n        response . body_file = f \n    else : \n        response . app_iter = iter ( f ) \n    return 1 "}
{"10986": "\ndef perform ( self , event ) : \n    wizard = NewDotGraphWizard ( parent = self . window . control , window = self . window , title = \"New Graph\" ) \n    if wizard . open ( ) == OK : \n        wizard . finished = 1 "}
{"11017": "\ndef create ( self , prog = None , format = None ) : \n    prog = self . program if prog is None else prog \n    format = self . format if format is None else format \n    tmp_fd , tmp_name = tempfile . mkstemp ( ) \n    os . close ( tmp_fd ) \n    dot_fd = file ( tmp_name , \"w+b\" ) \n    self . save_dot ( dot_fd ) \n    dot_fd . close ( ) \n    tmp_dir = os . path . dirname ( tmp_name ) \n    p = subprocess . Popen ( ( self . programs [ prog ] , '-T' + format , tmp_name ) , cwd = tmp_dir , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    stderr = p . stderr \n    stdout = p . stdout \n    stdout_output = list ( ) \n    while 1 : \n        data = stdout . read ( ) \n        if not data : \n            break \n        stdout_output . append ( data ) \n    stdout . close ( ) \n    if stdout_output : \n        stdout_output = '' . join ( stdout_output ) \n    if not stderr . closed : \n        stderr_output = list ( ) \n        while 1 : \n            data = stderr . read ( ) \n            if not data : \n                break \n            stderr_output . append ( data ) \n        stderr . close ( ) \n        if stderr_output : \n            stderr_output = '' . join ( stderr_output ) \n    status = p . wait ( ) \n    if status != 0 : \n        logger . error ( \"Program terminated with status: %d. stderr \" \"follows: %s\" % ( status , stderr_output ) ) \n    elif stderr_output : \n        logger . error ( \"%s\" , stderr_output ) \n    os . unlink ( tmp_name ) \n    return stdout_output "}
{"11022": "\ndef add_edge ( self , tail_node_or_ID , head_node_or_ID , ** kwds ) : \n    tail_node = self . add_node ( tail_node_or_ID ) \n    head_node = self . add_node ( head_node_or_ID ) \n    if \"directed\" in self . trait_names ( ) : \n        directed = self . directed \n    else : \n        directed = 0 \n    if self . default_edge is not None : \n        edge = self . default_edge . clone_traits ( copy = \"deep\" ) \n        edge . tail_node = tail_node \n        edge . head_node = head_node \n        edge . conn = \"->\" if directed else \"--\" \n        edge . set ( ** kwds ) \n    else : \n        edge = Edge ( tail_node , head_node , directed , ** kwds ) \n    if \"strict\" in self . trait_names ( ) : \n        if not self . strict : \n            self . edges . append ( edge ) \n        else : \n            self . edges . append ( edge ) \n    else : \n        self . edges . append ( edge ) "}
{"11033": "\ndef create_ui ( self , parent ) : \n    self . graph = self . editor_input . load ( ) \n    view = View ( Item ( name = \"graph\" , editor = graph_tree_editor , show_label = 0 ) , id = \"godot.graph_editor\" , kind = \"live\" , resizable = 1 ) \n    ui = self . edit_traits ( view = view , parent = parent , kind = \"subpanel\" ) \n    return ui "}
{"11035": "\ndef windows ( iterable , length = 2 , overlap = 0 , padding = 1 ) : \n    it = iter ( iterable ) \n    results = list ( itertools . islice ( it , length ) ) \n    while len ( results ) == length : \n        yield results \n        results = results [ length - overlap : ] \n        results . extend ( itertools . islice ( it , length - overlap ) ) \n    if padding and results : \n        results . extend ( itertools . repeat ( None , length - len ( results ) ) ) \n        yield results "}
{"11060": "\ndef parse_xdot_drawing_directive ( self , new ) : \n    components = XdotAttrParser ( ) . parse_xdot_data ( new ) \n    max_x = max ( [ c . bounds [ 0 ] for c in components ] + [ 1 ] ) \n    max_y = max ( [ c . bounds [ 1 ] for c in components ] + [ 1 ] ) \n    pos_x = min ( [ c . x for c in components ] ) \n    pos_y = min ( [ c . y for c in components ] ) \n    move_to_origin ( components ) \n    container = Container ( auto_size = 1 , position = [ pos_x - self . pos [ 0 ] , pos_y - self . pos [ 1 ] ] , bgcolor = \"blue\" ) \n    container . add ( * components ) \n    self . drawing = container "}
{"11061": "\ndef parse_xdot_label_directive ( self , new ) : \n    components = XdotAttrParser ( ) . parse_xdot_data ( new ) \n    pos_x = min ( [ c . x for c in components ] ) \n    pos_y = min ( [ c . y for c in components ] ) \n    move_to_origin ( components ) \n    container = Container ( auto_size = 1 , position = [ pos_x - self . pos [ 0 ] , pos_y - self . pos [ 1 ] ] , bgcolor = \"red\" ) \n    container . add ( * components ) \n    self . label_drawing = container "}
{"11065": "\ndef normal_right_down ( self , event ) : \n    x = event . x \n    y = event . y \n    component = self . component \n    for tool in component . tools : \n        component . active_tool = self \n        event . handled = 1 \n        component . active_tool = None \n        component . request_redraw ( ) \n    return "}
{"11072": "\ndef get_full_page_url ( self , page_number , scheme = None ) : \n    args = dict ( request . view_args , _external = 1 , ) \n    if scheme is not None : \n        args [ '_scheme' ] = scheme \n    if page_number != 1 : \n        args [ 'page' ] = page_number \n    return url_for ( request . endpoint , ** args ) "}
{"11076": "\ndef select_content_type ( requested , available ) : \n    class Match ( object ) : \n        WILDCARD , PARTIAL , FULL_TYPE , = 2 , 1 , 0 \n        def __init__ ( self , candidate , pattern ) : \n            self . candidate = candidate \n            self . pattern = pattern \n            if pattern . content_type == pattern . content_subtype == '*' : \n                self . match_type = self . WILDCARD \n            elif pattern . content_subtype == '*' : \n                self . match_type = self . PARTIAL \n            else : \n                self . match_type = self . FULL_TYPE \n            self . parameter_distance = len ( self . candidate . parameters ) \n            for key , value in candidate . parameters . items ( ) : \n                if key in pattern . parameters : \n                    if pattern . parameters [ key ] == value : \n                        self . parameter_distance -= 1 \n                    else : \n                        self . parameter_distance += 1 \n    def extract_quality ( obj ) : \n        return getattr ( obj , 'quality' , 1.0 ) \n    matches = [ ] \n    for pattern in sorted ( requested , key = extract_quality , reverse = 1 ) : \n        for candidate in sorted ( available ) : \n            if _content_type_matches ( candidate , pattern ) : \n                if candidate == pattern : \n                    if extract_quality ( pattern ) == 0.0 : \n                        raise errors . NoMatch \n                    return candidate , pattern \n                matches . append ( Match ( candidate , pattern ) ) \n    if not matches : \n        raise errors . NoMatch \n    matches = sorted ( matches , key = attrgetter ( 'match_type' , 'parameter_distance' ) ) \n    return matches [ 0 ] . candidate , matches [ 0 ] . pattern "}
{"11077": "\ndef rewrite_url ( input_url , ** kwargs ) : \n    scheme , netloc , path , query , fragment = parse . urlsplit ( input_url ) \n    if 'scheme' in kwargs : \n        scheme = kwargs [ 'scheme' ] \n    ident , host_n_port = parse . splituser ( netloc ) \n    user , password = parse . splitpasswd ( ident ) if ident else ( None , None ) \n    if 'user' in kwargs : \n        user = kwargs [ 'user' ] \n    elif user is not None : \n        user = parse . unquote_to_bytes ( user ) . decode ( 'utf-8' ) \n    if 'password' in kwargs : \n        password = kwargs [ 'password' ] \n    elif password is not None : \n        password = parse . unquote_to_bytes ( password ) . decode ( 'utf-8' ) \n    ident = _create_url_identifier ( user , password ) \n    host , port = parse . splitnport ( host_n_port , defport = None ) \n    if 'host' in kwargs : \n        host = kwargs [ 'host' ] \n        if host is not None : \n            host = _normalize_host ( host , enable_long_host = kwargs . get ( 'enable_long_host' , 0 ) , encode_with_idna = kwargs . get ( 'encode_with_idna' , None ) , scheme = scheme , ) \n    if 'port' in kwargs : \n        port = kwargs [ 'port' ] \n        if port is not None : \n            port = int ( kwargs [ 'port' ] ) \n            if port < 0 : \n                raise ValueError ( 'port is required to be non-negative' ) \n    if host is None or host == '' : \n        host_n_port = None \n    elif port is None : \n        host_n_port = host \n    else : \n        host_n_port = '{0}:{1}' . format ( host , port ) \n    if 'path' in kwargs : \n        path = kwargs [ 'path' ] \n        if path is None : \n            path = '/' \n        else : \n            path = parse . quote ( path . encode ( 'utf-8' ) , safe = PATH_SAFE_CHARS ) \n    netloc = '{0}@{1}' . format ( ident , host_n_port ) if ident else host_n_port \n    if 'query' in kwargs : \n        new_query = kwargs [ 'query' ] \n        if new_query is None : \n            query = None \n        else : \n            params = [ ] \n            try : \n                for param in sorted ( new_query . keys ( ) ) : \n                    params . append ( ( param , new_query [ param ] ) ) \n            except AttributeError : \n                pass \n            if not params : \n                try : \n                    params = [ ( param , value ) for param , value in new_query ] \n                except ValueError : \n                    pass \n            if params : \n                query = parse . urlencode ( params ) \n            else : \n                query = new_query \n    if 'fragment' in kwargs : \n        fragment = kwargs [ 'fragment' ] \n        if fragment is not None : \n            fragment = parse . quote ( fragment . encode ( 'utf-8' ) , safe = FRAGMENT_SAFE_CHARS ) \n    if scheme is None : \n        scheme = '' \n    return parse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) "}
{"11080": "\ndef _normalize_host ( host , enable_long_host = 0 , encode_with_idna = None , scheme = None ) : \n    if encode_with_idna is not None : \n        enable_idna = encode_with_idna \n    else : \n        enable_idna = scheme . lower ( ) in IDNA_SCHEMES if scheme else 0 \n    if enable_idna : \n        try : \n            host = '.' . join ( segment . encode ( 'idna' ) . decode ( ) for segment in host . split ( '.' ) ) \n        except UnicodeError as exc : \n            raise ValueError ( 'host is invalid - {0}' . format ( exc ) ) \n    else : \n        host = parse . quote ( host . encode ( 'utf-8' ) , safe = HOST_SAFE_CHARS ) \n    if len ( host ) > 255 and not enable_long_host : \n        raise ValueError ( 'host too long' ) \n    return host "}
{"11082": "\ndef rdiscover_modules ( directory ) : \n    found = list ( ) \n    if os . path . isdir ( directory ) : \n        for entry in os . listdir ( directory ) : \n            next_dir = os . path . join ( directory , entry ) \n            if os . path . isfile ( os . path . join ( next_dir , MODULE_INIT_FILE ) ) : \n                modules = _search_for_modules ( next_dir , 1 , entry ) \n                found . extend ( modules ) \n    return found "}
{"11083": "\ndef rlist_modules ( mname ) : \n    module = import_module ( mname ) \n    if not module : \n        raise ImportError ( 'Unable to load module {}' . format ( mname ) ) \n    found = list ( ) \n    if _should_use_module_path ( module ) : \n        mpath = module . __path__ [ 0 ] \n    else : \n        mpaths = sys . path \n        mpath = _scan_paths_for ( mname , mpaths ) \n    if mpath : \n        for pmname in _search_for_modules ( mpath , recursive = 1 ) : \n            found_mod = MODULE_PATH_SEP . join ( ( mname , pmname ) ) \n            found . append ( found_mod ) \n    return found "}
{"11092": "\ndef find_module ( self , module_name , path = None ) : \n    module_path = os . path . join ( * module_name . split ( MODULE_PATH_SEP ) ) \n    for search_root in self . paths : \n        target_path = os . path . join ( search_root , module_path ) \n        is_pkg = 0 \n        if os . path . isdir ( target_path ) : \n            target_file = os . path . join ( target_path , '__init__.py' ) \n            is_pkg = 1 \n        else : \n            target_file = '{}.py' . format ( target_path ) \n        if os . path . exists ( target_file ) : \n            return ModuleLoader ( target_path , module_name , target_file , is_pkg ) \n    return None "}
{"11094": "\ndef remove_namespaces ( root ) : \n    for elem in root . getiterator ( ) : \n        if not hasattr ( elem . tag , 'find' ) : \n            continue \n        i = elem . tag . find ( '}' ) \n        if i >= 0 : \n            elem . tag = elem . tag [ i + 1 : ] \n    objectify . deannotate ( root , cleanup_namespaces = 1 ) "}
{"11095": "\ndef consistency ( self , desired_version = None , include_package = 0 , strictness = None ) : \n    keys_to_check = list ( self . versions . keys ( ) ) \n    if not include_package and 'package' in keys_to_check : \n        keys_to_check . remove ( 'package' ) \n    if desired_version is None : \n        try : \n            desired_version = self . versions [ 'setup.py' ] \n        except KeyError : \n            desired_version = self . versions [ keys_to_check [ 0 ] ] \n    if strictness is None : \n        strictness = self . strictness \n    desired = self . _version ( desired_version , strictness ) \n    error_keys = [ ] \n    for key in keys_to_check : \n        test = self . _version ( self . versions [ key ] , strictness ) \n        if test != desired : \n            error_keys += [ key ] \n    msg = \"\" \n    for key in error_keys : \n        msg += \"Error: desired {d} != {v} ({k})\\n\" . format ( d = str ( desired ) , v = str ( self . versions [ key ] ) , k = str ( key ) ) \n    return msg "}
{"11098": "\ndef execute_actions ( self , cwd ) : \n    self . _execute_globals ( cwd ) \n    for action in self . actions : \n        logger . info ( \"executing {}\" . format ( action ) ) \n        p = subprocess . Popen ( action , shell = 1 , cwd = cwd ) \n        p . wait ( ) "}
{"11106": "\ndef set ( self , k , v ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    r = requests . put ( url , data = str ( v ) ) \n    if r . status_code != 200 or r . json ( ) is not 1 : \n        raise KVStoreError ( 'PUT returned {}' . format ( r . status_code ) ) "}
{"11107": "\ndef get ( self , k , wait = 0 , wait_index = 0 , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if wait : \n        params [ 'index' ] = wait_index \n        params [ 'wait' ] = timeout \n    r = requests . get ( url , params = params ) \n    if r . status_code == 404 : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if r . status_code != 200 : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    try : \n        return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) \n    except TypeError as e : \n        return \"\" "}
{"11108": "\ndef recurse ( self , k , wait = 0 , wait_index = None , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    params [ 'recurse' ] = 'true' \n    if wait : \n        params [ 'wait' ] = timeout \n        if not wait_index : \n            params [ 'index' ] = self . index ( k , recursive = 1 ) \n        else : \n            params [ 'index' ] = wait_index \n    r = requests . get ( url , params = params ) \n    if r . status_code == 404 : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if r . status_code != 200 : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    entries = { } \n    for e in r . json ( ) : \n        if e [ 'Value' ] : \n            entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) \n        else : \n            entries [ e [ 'Key' ] ] = '' \n    return entries "}
{"11109": "\ndef index ( self , k , recursive = 0 ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if recursive : \n        params [ 'recurse' ] = '' \n    r = requests . get ( url , params = params ) \n    return r . headers [ 'X-Consul-Index' ] "}
{"11110": "\ndef delete ( self , k , recursive = 0 ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if recursive : \n        params [ 'recurse' ] = '' \n    r = requests . delete ( url , params = params ) \n    if r . status_code != 200 : \n        raise KVStoreError ( 'DELETE returned {}' . format ( r . status_code ) ) "}
{"11111": "\ndef plot_heatmap ( X , y , top_n = 10 , metric = 'correlation' , method = 'complete' ) : \n    sns . set ( color_codes = 1 ) \n    df = feature_importance_report ( X , y ) \n    df_sns = pd . DataFrame ( ) . from_records ( X ) [ df [ : top_n ] . index ] . T \n    df_sns . columns = y \n    color_mapping = dict ( zip ( set ( y ) , sns . mpl_palette ( \"Set2\" , len ( set ( y ) ) ) ) ) \n    return sns . clustermap ( df_sns , figsize = ( 22 , 22 ) , z_score = 0 , metric = metric , method = method , col_colors = [ color_mapping [ i ] for i in y ] ) "}
{"11114": "\ndef is_christmas_period ( ) : \n    now = datetime . date . today ( ) \n    if now . month != 12 : \n        return 0 \n    if now . day < 15 : \n        return 0 \n    if now . day > 27 : \n        return 0 \n    return 1 "}
{"11120": "\ndef filter_by_label ( X , y , ref_label , reverse = 0 ) : \n    check_reference_label ( y , ref_label ) \n    return list ( zip ( * filter ( lambda t : ( not reverse ) == ( t [ 1 ] == ref_label ) , zip ( X , y ) ) ) ) "}
{"11122": "\ndef feature_importance_report ( X , y , threshold = 0.001 , correcting_multiple_hypotesis = 1 , method = 'fdr_bh' , alpha = 0.1 , sort_by = 'pval' ) : \n    df = variance_threshold_on_df ( pd . DataFrame . from_records ( X ) , threshold = threshold ) \n    F , pvals = f_classif ( df . values , y ) \n    if correcting_multiple_hypotesis : \n        _ , pvals , _ , _ = multipletests ( pvals , alpha = alpha , method = method ) \n    df [ 'labels' ] = y \n    df_mean = df . groupby ( 'labels' ) . mean ( ) . T \n    df_mean [ 'F' ] = F \n    df_mean [ 'pval' ] = pvals \n    return df_mean . sort_values ( sort_by , ascending = 1 ) "}
{"11132": "\ndef reasonable_desired_version ( self , desired_version , allow_equal = 0 , allow_patch_skip = 0 ) : \n    try : \n        desired_version = desired_version . base_version \n    except : \n        pass \n    ( new_major , new_minor , new_patch ) = map ( int , desired_version . split ( '.' ) ) \n    tag_versions = self . _versions_from_tags ( ) \n    if not tag_versions : \n        return \"\" \n    max_version = max ( self . _versions_from_tags ( ) ) . base_version \n    ( old_major , old_minor , old_patch ) = map ( int , str ( max_version ) . split ( '.' ) ) \n    update_str = str ( max_version ) + \" -> \" + str ( desired_version ) \n    v_desired = vers . Version ( desired_version ) \n    v_max = vers . Version ( max_version ) \n    if allow_equal and v_desired == v_max : \n        return \"\" \n    if v_desired < v_max : \n        return ( \"Bad update: New version doesn't increase on last tag: \" + update_str + \"\\n\" ) \n    bad_update = skipped_version ( ( old_major , old_minor , old_patch ) , ( new_major , new_minor , new_patch ) , allow_patch_skip ) \n    msg = \"\" \n    if bad_update : \n        msg = ( \"Bad update: Did you skip a version from \" + update_str + \"?\\n\" ) \n    return msg "}
{"11133": "\ndef handle_ssl_redirect ( ) : \n    if request . endpoint and request . endpoint not in [ 'static' , 'filemanager.static' ] : \n        needs_ssl = 0 \n        ssl_enabled = 0 \n        view_function = current_app . view_functions [ request . endpoint ] \n        if request . endpoint . startswith ( 'admin.' ) or ( hasattr ( view_function , 'ssl_required' ) and view_function . ssl_required ) : \n            needs_ssl = 1 \n            ssl_enabled = 1 \n        if hasattr ( view_function , 'ssl_allowed' ) and view_function . ssl_allowed : \n            ssl_enabled = 1 \n        if ( hasattr ( view_function , 'ssl_disabled' ) and view_function . ssl_disabled ) : \n            needs_ssl = 0 \n            ssl_enabled = 0 \n        if current_app . config [ 'SSL_ENABLED' ] : \n            if needs_ssl and not request . is_secure : \n                log . debug ( 'Redirecting to https: %s' % request . endpoint ) \n                return redirect ( request . url . replace ( \"http://\" , \"https://\" ) ) \n            elif not ssl_enabled and request . is_secure : \n                log . debug ( 'Redirecting to http: %s' % request . endpoint ) \n                return redirect ( request . url . replace ( \"https://\" , \"http://\" ) ) \n        elif needs_ssl : \n            log . info ( 'Not redirecting to HTTPS for endpoint %s as SSL_ENABLED is set to False' % request . endpoint ) "}
{"11134": "\ndef init_celery ( app , celery ) : \n    celery . conf . update ( app . config ) \n    TaskBase = celery . Task \n    class ContextTask ( TaskBase ) : \n        abstract = 1 \n        def __call__ ( self , * args , ** kwargs ) : \n            with app . app_context ( ) : \n                return TaskBase . __call__ ( self , * args , ** kwargs ) \n    celery . Task = ContextTask \n    return celery "}
{"11135": "\ndef queue_email ( to_addresses , from_address , subject , body , commit = 1 , html = 1 , session = None ) : \n    from models import QueuedEmail \n    if session is None : \n        session = _db . session \n    log . info ( 'Queuing mail to %s: %s' % ( to_addresses , subject ) ) \n    queued_email = QueuedEmail ( html , to_addresses , from_address , subject , body , STATUS_QUEUED ) \n    session . add ( queued_email ) \n    session . commit ( ) \n    return queued_email "}
{"11137": "\ndef parse_cache_control ( header_value ) : \n    directives = { } \n    for segment in parse_list ( header_value ) : \n        name , sep , value = segment . partition ( '=' ) \n        if sep != '=' : \n            directives [ name ] = None \n        elif sep and value : \n            value = _dequote ( value . strip ( ) ) \n            try : \n                directives [ name ] = int ( value ) \n            except ValueError : \n                directives [ name ] = value \n    for name in _CACHE_CONTROL_BOOL_DIRECTIVES : \n        if directives . get ( name , '' ) is None : \n            directives [ name ] = 1 \n    return directives "}
{"11138": "\ndef parse_content_type ( content_type , normalize_parameter_values = 1 ) : \n    parts = _remove_comments ( content_type ) . split ( ';' ) \n    content_type , content_subtype = parts . pop ( 0 ) . split ( '/' ) \n    if '+' in content_subtype : \n        content_subtype , content_suffix = content_subtype . split ( '+' ) \n    else : \n        content_suffix = None \n    parameters = _parse_parameter_list ( parts , normalize_parameter_values = normalize_parameter_values ) \n    return datastructures . ContentType ( content_type , content_subtype , dict ( parameters ) , content_suffix ) "}
{"11139": "\ndef parse_forwarded ( header_value , only_standard_parameters = 0 ) : \n    result = [ ] \n    for entry in parse_list ( header_value ) : \n        param_tuples = _parse_parameter_list ( entry . split ( ';' ) , normalize_parameter_names = 1 , normalize_parameter_values = 0 ) \n        if only_standard_parameters : \n            for name , _ in param_tuples : \n                if name not in ( 'for' , 'proto' , 'by' , 'host' ) : \n                    raise errors . StrictHeaderParsingFailure ( 'Forwarded' , header_value ) \n        result . append ( dict ( param_tuples ) ) \n    return result "}
{"11141": "\ndef _parse_parameter_list ( parameter_list , normalized_parameter_values = _DEF_PARAM_VALUE , normalize_parameter_names = 0 , normalize_parameter_values = 1 ) : \n    if normalized_parameter_values is not _DEF_PARAM_VALUE : \n        warnings . warn ( 'normalized_parameter_values keyword to ' '_parse_parameter_list is deprecated, use ' 'normalize_parameter_values instead' , DeprecationWarning ) \n        normalize_parameter_values = normalized_parameter_values \n    parameters = [ ] \n    for param in parameter_list : \n        param = param . strip ( ) \n        if param : \n            name , value = param . split ( '=' ) \n            if normalize_parameter_names : \n                name = name . lower ( ) \n            if normalize_parameter_values : \n                value = value . lower ( ) \n            parameters . append ( ( name , _dequote ( value . strip ( ) ) ) ) \n    return parameters "}
{"11163": "\ndef read_pr_report ( self , filename ) : \n    done = 0 \n    f = open ( filename ) \n    while f : \n        line = f . readline ( ) \n        if not line : \n            done = 1 \n            break \n        if \"# Quad solid angle mean point theta table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_theta' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"# Quad solid angle mean point phi table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_phi' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"L_w band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_w_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n        if \"L_it band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_it_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n    return self . data_dictionary "}
{"11167": "\ndef pause ( self , signum , seconds = 0 , callback_function = None ) : \n    if callback_function is None : \n        callback_function = self . default_handler \n    if seconds > 0 : \n        self . log . info ( \"Signal handler pausing for {0} seconds or until it receives SIGALRM or SIGCONT\" . format ( seconds ) ) \n        signal . signal ( signal . SIGALRM , callback_function ) \n        signal . alarm ( seconds ) \n    else : \n        self . log . info ( 'Signal handler pausing until it receives SIGALRM or SIGCONT' ) \n    signal . signal ( signal . SIGCONT , callback_function ) \n    signal . pause ( ) \n    self . log . info ( 'Signal handler resuming from pause' ) \n    if signum == signal . SIGALRM : \n        return 1 \n    else : \n        return 0 "}
{"11171": "\ndef fetch_metric ( self , metric , start , end , tags = { } , aggregator = \"sum\" , downsample = None , ms_resolution = 1 ) : \n    query = \"{aggregator}:{downsample}{metric}{{{tags}}}\" . format ( aggregator = aggregator , downsample = downsample + \"-avg:\" if downsample else \"\" , metric = metric , tags = ',' . join ( \"%s=%s\" % ( k , v ) for k , v in tags . items ( ) ) ) \n    params = { 'ms' : ms_resolution , 'start' : '{0:.3f}' . format ( start . timestamp ( ) ) , 'end' : '{0:.3f}' . format ( end . timestamp ( ) ) , 'm' : query } \n    response = self . __request ( \"/query\" , params ) \n    if response . status_code == 200 : \n        try : \n            return response . json ( ) [ 0 ] [ 'dps' ] \n        except IndexError : \n            return { } \n    raise QueryError ( response . json ( ) ) "}
{"11179": "\ndef ignore_certain_metainf_files ( filename ) : \n    ignore = ( \"META-INF/manifest.mf\" , \"META-INF/*.sf\" , \"META-INF/*.rsa\" , \"META-INF/*.dsa\" , \"META-INF/ids.json\" ) \n    for glob in ignore : \n        if fnmatch . fnmatchcase ( filename . upper ( ) , glob . upper ( ) ) : \n            return 1 \n    return 0 "}
{"11187": "\ndef search_file_result ( self ) : \n    if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n        self . result_file = self . file_dialog . getOpenFileName ( caption = str ( \"Open Report File\" ) , directory = \"./outputs\" ) \n        if not self . result_file == '' : \n            self . ui . show_all_curves . setDisabled ( 0 ) \n            self . ui . show_grid . setDisabled ( 0 ) \n            self . data_processing ( ) \n            self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) \n            self . authorized_display = 1 "}
{"11189": "\ndef data_processing ( self ) : \n    the_file_name = str ( self . result_file ) \n    the_file = open ( the_file_name , 'r' ) \n    lines = the_file . readlines ( ) \n    lines_array = [ ] \n    for line in lines : \n        line = line . split ( ',' ) \n        lines_array . append ( line ) \n    labels_line = lines_array [ 0 ] \n    cell_labels_line = 0 \n    flag = 1 \n    try : \n        while flag : \n            if \"wave length (nm)\" in labels_line [ cell_labels_line ] : \n                index = labels_line . index ( labels_line [ cell_labels_line ] ) \n                flag = 0 \n            else : \n                cell_labels_line += 1 \n    except IndexError : \n        raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) \n    self . information = [ ] \n    data_wavelength = [ ] \n    self . num_line = 0 \n    for line in lines_array : \n        cell_line = 0 \n        self . information . append ( [ ] ) \n        data_wavelength . append ( [ ] ) \n        while cell_line < len ( line ) : \n            if cell_line < index : \n                self . information [ self . num_line ] . append ( line [ cell_line ] ) \n            elif cell_line > index : \n                data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) \n            cell_line += 1 \n        self . num_line += 1 \n    line_wavelength = 0 \n    for row_data_wavelength in data_wavelength : \n        row_data_wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row_data_wavelength ] \n        data_wavelength [ line_wavelength ] = row_data_wavelength \n        line_wavelength += 1 \n    self . wavelength = data_wavelength [ 0 ] \n    self . data_wanted = data_wavelength [ 1 : ] \n    the_file . close ( ) "}
{"11192": "\ndef display_error_message ( self ) : \n    self . ui . error_label . setScaledContents ( 1 ) \n    self . ui . error_text_label . show ( ) \n    self . ui . error_text_label . setStyleSheet ( 'color: red' ) "}
{"11193": "\ndef hide_error_message ( self ) : \n    self . ui . error_label . setScaledContents ( 0 ) \n    self . ui . error_text_label . hide ( ) "}
{"11194": "\ndef run ( self ) : \n    print ( 'Executing planarrad' ) \n    if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n        self . data ( ) \n        self . check_values ( ) \n        if self . without_error == 0 : \n            self . display_error_message ( ) \n        elif self . without_error == 1 : \n            self . is_running = 1 \n            self . hide_error_message ( ) \n            self . write_to_file ( ) \n            os . chdir ( './' ) \n            self . progress_bar ( ) \n            this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) \n            batch_file = os . path . join ( this_dir , \"inputs/batch_files/\" + str ( self . batch_name_value ) + \"_batch.txt\" ) \n            print ( batch_file ) \n            self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch_file ] , shell = 1 ) \n            if self . ui . progressBar . value ( ) == 100 : \n                self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) "}
{"11195": "\ndef cancel_planarrad ( self ) : \n    if ( self . is_running == 1 ) & ( self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE ) : \n        cancel = QtGui . QMessageBox . question ( self . ui . cancel , 'Cancel PlanarRad' , \"Are you sure to cancel ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if cancel == QtGui . QMessageBox . Yes : \n            self . is_running = 0 \n            os . kill ( self . p . pid , signal . SIGTERM ) \n            print ( \"Necessary to check if cancel_planarrad works well !\" ) \n            self . ui . progressBar . reset ( ) \n        else : \n            pass "}
{"11196": "\ndef quit ( self ) : \n    if self . is_running == 1 : \n        warning_planarrad_running = QtGui . QMessageBox . warning ( self . ui . quit , 'Warning !' , \"PlanarRad is running. Stop it before quit !\" , QtGui . QMessageBox . Ok ) \n    else : \n        quit = QtGui . QMessageBox . question ( self . ui . quit , 'Quit PlanarRad' , \"Are you sure to quit ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if quit == QtGui . QMessageBox . Yes : \n            QtGui . qApp . quit ( ) "}
{"11200": "\ndef prerequisite_actions ( self ) : \n    self . hide_error_message ( ) \n    self . ui . show_all_curves . setDisabled ( 1 ) \n    self . ui . sens . setDisabled ( 1 ) \n    self . ui . show_grid . setDisabled ( 1 ) \n    pathname = os . path . dirname ( sys . argv [ 0 ] ) \n    path = os . path . abspath ( pathname ) \n    self . verbose_value = self . ui . verbose_value . setText ( \"6\" ) \n    self . report_parameter_value = self . ui . report_parameter_value . setText ( \"Rrs\" ) \n    self . ui . progressBar . reset ( ) "}
{"11203": "\ndef graphic_target ( self , x , y ) : \n    if self . authorized_display == 1 : \n        try : \n            self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) \n            self . ui . mouse_coordinate . setText ( \"(%0.3f, %0.3f)\" % ( x , y ) ) \n        except : \n            pass "}
{"11205": "\ndef sign ( self , privkey ) : \n    if self . v : \n        raise InvalidSignature ( \"already signed\" ) \n    if privkey in ( 0 , '' , '\\x00' * 32 ) : \n        raise InvalidSignature ( \"Zero privkey cannot sign\" ) \n    rawhash = sha3 ( rlp . encode ( self , self . __class__ . exclude ( [ 'v' , 'r' , 's' ] ) ) ) \n    if len ( privkey ) == 64 : \n        privkey = encode_privkey ( privkey , 'bin' ) \n    pk = PrivateKey ( privkey , raw = 1 ) \n    signature = pk . ecdsa_recoverable_serialize ( pk . ecdsa_sign_recoverable ( rawhash , raw = 1 ) ) \n    signature = signature [ 0 ] + chr ( signature [ 1 ] ) \n    self . v = ord ( signature [ 64 ] ) + 27 \n    self . r = big_endian_to_int ( signature [ 0 : 32 ] ) \n    self . s = big_endian_to_int ( signature [ 32 : 64 ] ) \n    self . _sender = None \n    return self "}
{"11207": "\ndef check ( self ) : \n    if not self . is_valid : \n        return 1 \n    test = ( self . has_quorum , self . has_quorum_possible , self . has_noquorum ) \n    assert 1 == len ( [ x for x in test if x is not None ] ) \n    return 1 "}
{"11214": "\ndef mk_privkeys ( num ) : \n    privkeys = [ ] \n    assert num <= num_colors \n    for i in range ( num ) : \n        j = 0 \n        while 1 : \n            k = sha3 ( str ( j ) ) \n            a = privtoaddr ( k ) \n            an = big_endian_to_int ( a ) \n            if an % num_colors == i : \n                break \n            j += 1 \n        privkeys . append ( k ) \n    return privkeys "}
{"11220": "\ndef update ( self , data ) : \n    if data not in self . filter : \n        self . filter . append ( data ) \n        if len ( self . filter ) > self . max_items : \n            self . filter . pop ( 0 ) \n        return 1 \n    else : \n        self . filter . append ( self . filter . pop ( 0 ) ) \n        return 0 "}
{"11228": "\ndef finish ( self ) : \n    if self . finished : \n        return self . exit_code \n    checkpoint_status = self . checkpoint ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != 0 : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    if self . applied_count > 0 : \n        self . _end_acquisition ( ) \n        self . _apply_rows ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != 0 : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    self . finished = 1 \n    return self . exit_code "}
{"11229": "\ndef from_file ( self , filename , table = None , delimiter = '|' , null = 'NULL' , panic = 1 , quotechar = '\"' , parse_dates = 0 ) : \n    if not self . table : \n        if not table : \n            raise GiraffeError ( \"Table must be set or specified to load a file.\" ) \n        self . table = table \n    if not isinstance ( null , basestring ) : \n        raise GiraffeError ( \"Expected 'null' to be str, received {}\" . format ( type ( null ) ) ) \n    with Reader ( filename , delimiter = delimiter , quotechar = quotechar ) as f : \n        if not isinstance ( f . delimiter , basestring ) : \n            raise GiraffeError ( \"Expected 'delimiter' to be str, received {}\" . format ( type ( delimiter ) ) ) \n        self . columns = f . header \n        if isinstance ( f , ArchiveFileReader ) : \n            self . mload . set_encoding ( ROW_ENCODING_RAW ) \n            self . preprocessor = lambda s : s \n        if parse_dates : \n            self . preprocessor = DateHandler ( self . columns ) \n        self . _initiate ( ) \n        self . mload . set_null ( null ) \n        self . mload . set_delimiter ( delimiter ) \n        i = 0 \n        for i , line in enumerate ( f , 1 ) : \n            self . put ( line , panic = panic ) \n            if i % self . checkpoint_interval == 1 : \n                log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) , console = 1 ) \n                checkpoint_status = self . checkpoint ( ) \n                self . exit_code = self . _exit_code ( ) \n                if self . exit_code != 0 : \n                    return self . exit_code \n        log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) ) \n        return self . finish ( ) "}
{"11230": "\ndef put ( self , items , panic = 1 ) : \n    if not self . initiated : \n        self . _initiate ( ) \n    try : \n        row_status = self . mload . put_row ( self . preprocessor ( items ) ) \n        self . applied_count += 1 \n    except ( TeradataPTError , EncoderError ) as error : \n        self . error_count += 1 \n        if panic : \n            raise error \n        log . info ( \"BulkLoad\" , error ) "}
{"11237": "\ndef do_table ( self , line ) : \n    if len ( line ) > 0 : \n        if line . strip ( ) . lower ( ) == \"on\" : \n            log . write ( \"Table ON\" ) \n            self . table_output = 1 \n            return \n        elif line . strip ( ) . lower ( ) == \"off\" : \n            log . write ( \"Table OFF\" ) \n            self . table_output = 0 \n            return \n    log . write ( \"Table output: {}\" . format ( \"ON\" if self . table_output else \"OFF\" ) ) "}
{"11238": "\ndef execute ( self , command , coerce_floats = 1 , parse_dates = 0 , header = 0 , sanitize = 1 , silent = 0 , panic = None , multi_statement = 0 , prepare_only = 0 ) : \n    if panic is None : \n        panic = self . panic \n    self . options ( \"panic\" , panic ) \n    self . options ( \"multi-statement mode\" , multi_statement , 3 ) \n    if isfile ( command ) : \n        self . options ( \"file\" , command , 2 ) \n        with open ( command , 'r' ) as f : \n            command = f . read ( ) \n    else : \n        if log . level >= VERBOSE : \n            self . options ( \"query\" , command , 2 ) \n        else : \n            self . options ( \"query\" , truncate ( command ) , 2 ) \n    if not silent and not self . silent : \n        log . info ( \"Command\" , \"Executing ...\" ) \n        log . info ( self . options ) \n    if sanitize : \n        command = prepare_statement ( command ) \n        log . debug ( \"Debug[2]\" , \"Command (sanitized): {!r}\" . format ( command ) ) \n    self . cmd . set_encoding ( ENCODER_SETTINGS_DEFAULT ) \n    return Cursor ( self . cmd , command , multi_statement = multi_statement , header = header , prepare_only = prepare_only , coerce_floats = coerce_floats , parse_dates = parse_dates , panic = panic ) "}
{"11239": "\ndef get_value ( self , key , default = { } , nested = 1 , decrypt = 1 ) : \n    key = key . lstrip ( ) \n    if key . endswith ( \".\" ) : \n        key = key [ : - 1 ] \n    if nested : \n        path = key . split ( \".\" ) \n        curr = self . settings \n        for p in path [ : - 1 ] : \n            curr = curr . get ( p , { } ) \n        try : \n            value = curr [ path [ - 1 ] ] \n        except KeyError : \n            return default \n        value = self . decrypt ( value , path ) \n        return value \n    else : \n        return self . settings . get ( key , default ) "}
{"11240": "\ndef write_default ( self , conf = None ) : \n    if conf is None : \n        conf = home_file ( \".girafferc\" ) \n    contents = yaml . dump ( default_config , default_flow_style = 0 ) \n    with open ( conf , \"w\" ) as f : \n        f . write ( contents ) \n    os . chmod ( conf , 0o600 ) \n    return contents "}
{"11243": "\ndef to_str ( self , delimiter = '|' , null = 'NULL' ) : \n    self . export . set_null ( null ) \n    self . export . set_delimiter ( delimiter ) \n    self . options ( \"delimiter\" , escape_string ( delimiter ) , 2 ) \n    self . options ( \"null\" , null , 3 ) \n    return self . _fetchall ( ENCODER_SETTINGS_STRING , coerce_floats = 0 ) "}
{"11248": "\ndef detect_devices ( soapy_args = '' ) : \n    devices = simplesoapy . detect_devices ( soapy_args , as_string = 1 ) \n    text = [ ] \n    text . append ( 'Detected SoapySDR devices:' ) \n    if devices : \n        for i , d in enumerate ( devices ) : \n            text . append ( '  {}' . format ( d ) ) \n    else : \n        text . append ( '  No devices found!' ) \n    return ( devices , '\\n' . join ( text ) ) "}
{"11257": "\ndef freq_plan ( self , min_freq , max_freq , bins , overlap = 0 , quiet = 0 ) : \n    bin_size = self . bins_to_bin_size ( bins ) \n    bins_crop = round ( ( 1 - overlap ) * bins ) \n    sample_rate_crop = ( 1 - overlap ) * self . device . sample_rate \n    freq_range = max_freq - min_freq \n    hopping = 1 if freq_range >= sample_rate_crop else 0 \n    hop_size = self . nearest_freq ( sample_rate_crop , bin_size ) \n    hops = math . ceil ( freq_range / hop_size ) if hopping else 1 \n    min_center_freq = min_freq + ( hop_size / 2 ) if hopping else min_freq + ( freq_range / 2 ) \n    max_center_freq = min_center_freq + ( ( hops - 1 ) * hop_size ) \n    freq_list = [ min_center_freq + ( i * hop_size ) for i in range ( hops ) ] \n    if not quiet : \n        logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) \n        logger . info ( 'bin_size: {:.2f} Hz' . format ( bin_size ) ) \n        logger . info ( 'bins: {}' . format ( bins ) ) \n        logger . info ( 'bins (after crop): {}' . format ( bins_crop ) ) \n        logger . info ( 'sample_rate: {:.3f} MHz' . format ( self . device . sample_rate / 1e6 ) ) \n        logger . info ( 'sample_rate (after crop): {:.3f} MHz' . format ( sample_rate_crop / 1e6 ) ) \n        logger . info ( 'freq_range: {:.3f} MHz' . format ( freq_range / 1e6 ) ) \n        logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) \n        logger . info ( 'hop_size: {:.3f} MHz' . format ( hop_size / 1e6 ) ) \n        logger . info ( 'hops: {}' . format ( hops ) ) \n        logger . info ( 'min_center_freq: {:.3f} MHz' . format ( min_center_freq / 1e6 ) ) \n        logger . info ( 'max_center_freq: {:.3f} MHz' . format ( max_center_freq / 1e6 ) ) \n        logger . info ( 'min_freq (after crop): {:.3f} MHz' . format ( ( min_center_freq - ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . info ( 'max_freq (after crop): {:.3f} MHz' . format ( ( max_center_freq + ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . debug ( 'Frequency hops table:' ) \n        logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) \n        for f in freq_list : \n            logger . debug ( '  {:8.3f} MHz  {:8.3f} MHz  {:8.3f} MHz' . format ( ( f - ( self . device . sample_rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample_rate / 2 ) ) / 1e6 , ) ) \n    return freq_list "}
{"11259": "\ndef setup ( self , bins , repeats , base_buffer_size = 0 , max_buffer_size = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop_factor = 0 , log_scale = 1 , remove_dc = 0 , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = 0 , max_threads = 0 , max_queue_size = 0 ) : \n    if self . device . is_streaming : \n        self . device . stop_stream ( ) \n    base_buffer = self . device . start_stream ( buffer_size = base_buffer_size ) \n    self . _bins = bins \n    self . _repeats = repeats \n    self . _base_buffer_size = len ( base_buffer ) \n    self . _max_buffer_size = max_buffer_size \n    self . _buffer_repeats , self . _buffer = self . create_buffer ( bins , repeats , self . _base_buffer_size , self . _max_buffer_size ) \n    self . _tune_delay = tune_delay \n    self . _reset_stream = reset_stream \n    self . _psd = psd . PSD ( bins , self . device . sample_rate , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = crop_factor , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , max_threads = max_threads , max_queue_size = max_queue_size ) \n    self . _writer = writer . formats [ self . _output_format ] ( self . _output ) "}
{"11261": "\ndef psd ( self , freq ) : \n    if not self . device . is_streaming : \n        raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) \n    logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) \n    t_freq = time . time ( ) \n    if self . device . freq != freq : \n        if self . _reset_stream : \n            self . device . device . deactivateStream ( self . device . stream ) \n        self . device . freq = freq \n        if self . _reset_stream : \n            self . device . device . activateStream ( self . device . stream ) \n        if self . _tune_delay : \n            t_delay = time . time ( ) \n            while 1 : \n                self . device . read_stream ( ) \n                t_delay_end = time . time ( ) \n                if t_delay_end - t_delay >= self . _tune_delay : \n                    break \n            logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) \n    else : \n        logger . debug ( '    Same frequency as before, tuning skipped' ) \n    psd_state = self . _psd . set_center_freq ( freq ) \n    t_freq_end = time . time ( ) \n    logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) \n    for repeat in range ( self . _buffer_repeats ) : \n        logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) \n        t_acq = time . time ( ) \n        acq_time_start = datetime . datetime . utcnow ( ) \n        self . device . read_stream_into_buffer ( self . _buffer ) \n        acq_time_stop = datetime . datetime . utcnow ( ) \n        t_acq_end = time . time ( ) \n        logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) \n        self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) \n        t_final = time . time ( ) \n        if _shutdown : \n            break \n    psd_future = self . _psd . result_async ( psd_state ) \n    logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) \n    return ( psd_future , acq_time_start , acq_time_stop ) "}
{"11262": "\ndef sweep ( self , min_freq , max_freq , bins , repeats , runs = 0 , time_limit = 0 , overlap = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop = 0 , log_scale = 1 , remove_dc = 0 , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = 0 , base_buffer_size = 0 , max_buffer_size = 0 , max_threads = 0 , max_queue_size = 0 ) : \n    self . setup ( bins , repeats , base_buffer_size , max_buffer_size , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = overlap if crop else 0 , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , tune_delay = tune_delay , reset_stream = reset_stream , max_threads = max_threads , max_queue_size = max_queue_size ) \n    try : \n        freq_list = self . freq_plan ( min_freq - lnb_lo , max_freq - lnb_lo , bins , overlap ) \n        t_start = time . time ( ) \n        run = 0 \n        while not _shutdown and ( runs == 0 or run < runs ) : \n            run += 1 \n            t_run_start = time . time ( ) \n            logger . debug ( 'Run: {}' . format ( run ) ) \n            for freq in freq_list : \n                psd_future , acq_time_start , acq_time_stop = self . psd ( freq ) \n                self . _writer . write_async ( psd_future , acq_time_start , acq_time_stop , len ( self . _buffer ) * self . _buffer_repeats ) \n                if _shutdown : \n                    break \n            write_next_future = self . _writer . write_next_async ( ) \n            t_run = time . time ( ) \n            logger . debug ( '  Total run time: {:.3f} s' . format ( t_run - t_run_start ) ) \n            if time_limit and ( time . time ( ) - t_start ) >= time_limit : \n                logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time_limit , run ) ) \n                break \n        write_next_future . result ( ) \n        logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer_overflow_count ) ) \n        logging . debug ( 'PSD worker threads: {}' . format ( self . _psd . _executor . _max_workers ) ) \n        logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . _psd . _executor . max_queue_size_reached , self . _psd . _executor . max_queue_size ) ) \n        logging . debug ( 'Writer worker threads: {}' . format ( self . _writer . _executor . _max_workers ) ) \n        logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . _writer . _executor . max_queue_size_reached , self . _writer . _executor . max_queue_size ) ) \n    finally : \n        self . stop ( ) \n        t_stop = time . time ( ) \n        logger . info ( 'Total time: {:.3f} s' . format ( t_stop - t_start ) ) "}
{"11276": "\ndef watch_port_events ( port , chip , pin_function_maps , event_queue , return_after_kbdint = 0 ) : \n    gpio25 = open ( GPIO_INTERRUPT_DEVICE_VALUE , 'r' ) \n    epoll = select . epoll ( ) \n    epoll . register ( gpio25 , select . EPOLLIN | select . EPOLLET ) \n    while 1 : \n        try : \n            events = epoll . poll ( ) \n        except KeyboardInterrupt as e : \n            if return_after_kbdint : \n                return \n            else : \n                raise e \n        except IOError as e : \n            if e . errno != errno . EINTR : \n                raise \n        if port == pifacecommon . mcp23s17 . GPIOA : \n            interrupt_flag = chip . intfa . value \n        else : \n            interrupt_flag = chip . intfb . value \n        if interrupt_flag == 0 : \n            continue \n        else : \n            if port == pifacecommon . mcp23s17 . GPIOA : \n                interrupt_capture = chip . intcapa . value \n            else : \n                interrupt_capture = chip . intcapb . value \n            event_queue . add_event ( InterruptEvent ( interrupt_flag , interrupt_capture , chip , time . time ( ) ) ) \n    epoll . close ( ) "}
{"11277": "\ndef handle_events ( function_maps , event_queue , event_matches_function_map , terminate_signal ) : \n    while 1 : \n        event = event_queue . get ( ) \n        if event == terminate_signal : \n            return \n        functions = map ( lambda fm : fm . callback if event_matches_function_map ( event , fm ) else None , function_maps ) \n        functions = filter ( lambda f : f is not None , functions ) \n        for function in functions : \n            function ( event ) "}
{"11285": "\ndef render ( self , form , form_style , context , template_pack = TEMPLATE_PACK ) : \n    links , content = '' , '' \n    if not self . css_id : \n        self . css_id = \"-\" . join ( [ \"tabsholder\" , text_type ( randint ( 1000 , 9999 ) ) ] ) \n    for tab in self . fields : \n        tab . active = 0 \n    self . open_target_group_for_form ( form ) \n    for tab in self . fields : \n        content += render_field ( tab , form , form_style , context , template_pack = template_pack ) \n        links += tab . render_link ( form , template_pack ) \n    context . update ( { 'tabs' : self , 'links' : links , 'content' : content } ) \n    template = self . get_template_name ( template_pack ) \n    return render_to_string ( template , context . flatten ( ) ) "}
{"11293": "\ndef upload_link ( self , folder_id = None , sha1 = None , httponly = 0 ) : \n    kwargs = { 'folder' : folder_id , 'sha1' : sha1 , 'httponly' : httponly } \n    params = { key : value for key , value in kwargs . items ( ) if value } \n    return self . _get ( 'file/ul' , params = params ) "}
{"11294": "\ndef upload_file ( self , file_path , folder_id = None , sha1 = None , httponly = 0 ) : \n    upload_url_response_json = self . upload_link ( folder_id = folder_id , sha1 = sha1 , httponly = httponly ) \n    upload_url = upload_url_response_json [ 'url' ] \n    with open ( file_path , 'rb' ) as f : \n        response_json = requests . post ( upload_url , files = { 'upload_file' : f } ) . json ( ) \n    self . _check_status ( response_json ) \n    return response_json [ 'result' ] "}
{"11303": "\ndef verify ( data ) : \n    if len ( data ) == 0 : \n        return 0 \n    crc = VProCRC . get ( data ) \n    if crc : \n        log . info ( \"CRC Bad\" ) \n    else : \n        log . debug ( \"CRC OK\" ) \n    return not crc "}
{"11305": "\ndef _use_rev_b_archive ( self , records , offset ) : \n    if type ( self . _ARCHIVE_REV_B ) is bool : \n        return self . _ARCHIVE_REV_B \n    data = ArchiveBStruct . unpack_from ( records , offset ) \n    if data [ 'RecType' ] == 0 : \n        log . info ( 'detected archive rev. B' ) \n        self . _ARCHIVE_REV_B = 1 \n    else : \n        log . info ( 'detected archive rev. A' ) \n        self . _ARCHIVE_REV_B = 0 \n    return self . _ARCHIVE_REV_B "}
{"11307": "\ndef _cmd ( self , cmd , * args , ** kw ) : \n    ok = kw . setdefault ( 'ok' , 0 ) \n    self . _wakeup ( ) \n    if args : \n        cmd = \"%s %s\" % ( cmd , ' ' . join ( str ( a ) for a in args ) ) \n    for i in xrange ( 3 ) : \n        log . info ( \"send: \" + cmd ) \n        self . port . write ( cmd + '\\n' ) \n        if ok : \n            ack = self . port . read ( len ( self . OK ) ) \n            log_raw ( 'read' , ack ) \n            if ack == self . OK : \n                return \n        else : \n            ack = self . port . read ( len ( self . ACK ) ) \n            log_raw ( 'read' , ack ) \n            if ack == self . ACK : \n                return \n    raise NoDeviceException ( 'Can not access weather station' ) "}
{"11321": "\ndef push ( self , override , use_parent = 0 ) : \n    current = self . current \n    if use_parent and current : \n        override = current + override \n    _override_ctx_stack . push ( ( self , override ) ) "}
{"11323": "\ndef override ( self , override , use_parent = 0 ) : \n    self . push ( override , use_parent ) \n    yield self . current \n    self . pop ( ) "}
{"11324": "\ndef push ( self , additional , use_parent = 0 ) : \n    current = self . current \n    if use_parent and current : \n        additional = current + additional \n    _additional_ctx_stack . push ( ( self , additional ) ) "}
{"11326": "\ndef additional ( self , additional , use_parent = 0 ) : \n    self . push ( additional , use_parent ) \n    yield self . current \n    self . pop ( ) "}
{"11328": "\ndef interpret_stats ( results ) : \n    stats = results . stats \n    contains_updates = stats . pop ( \"contains_updates\" , 0 ) if stats else 0 \n    if not contains_updates : \n        result = '{} rows affected.' . format ( len ( results ) ) \n    else : \n        result = '' \n        for stat , value in stats . items ( ) : \n            if value : \n                result = \"{}\\n{} {}.\" . format ( result , value , stat . replace ( \"_\" , \" \" ) ) \n    return result . strip ( ) "}
{"11332": "\ndef get_graph ( self , directed = 1 ) : \n    if nx is None : \n        raise ImportError ( \"Try installing NetworkX first.\" ) \n    if directed : \n        graph = nx . MultiDiGraph ( ) \n    else : \n        graph = nx . MultiGraph ( ) \n    for item in self . _results . graph : \n        for node in item [ 'nodes' ] : \n            properties = copy . deepcopy ( node [ 'properties' ] ) \n            properties [ 'labels' ] = node [ 'labels' ] \n            graph . add_node ( node [ 'id' ] , ** properties ) \n        for rel in item [ 'relationships' ] : \n            properties = copy . deepcopy ( rel [ 'properties' ] ) \n            properties . update ( id = rel [ 'id' ] , type = rel [ 'type' ] ) \n            graph . add_edge ( rel [ 'startNode' ] , rel [ 'endNode' ] , key = rel . get ( 'type' ) , ** properties ) \n    return graph "}
{"11337": "\ndef permission_required ( perm , login_url = None , raise_exception = 0 ) : \n    def check_perms ( user ) : \n        if not getattr ( settings , 'DASHBOARD_REQUIRE_LOGIN' , app_settings . REQUIRE_LOGIN ) : \n            return 1 \n        if user . has_perm ( perm ) : \n            return 1 \n        if raise_exception : \n            raise PermissionDenied \n        return 0 \n    return user_passes_test ( check_perms , login_url = login_url ) "}
{"11338": "\ndef get_context_data ( self , ** kwargs ) : \n    ctx = super ( RenderWidgetMixin , self ) . get_context_data ( ** kwargs ) \n    ctx . update ( { 'is_rendered' : 1 , 'widget' : self . widget , } ) \n    ctx . update ( self . widget . get_context_data ( ) ) \n    return ctx "}
{"11346": "\ndef should_update ( self ) : \n    last_update = self . get_last_update ( ) \n    time_since = now ( ) - last_update . last_update \n    if time_since . seconds < self . update_interval : \n        return 0 \n    return 1 "}
{"11350": "\ndef _argcheck ( * args , ** kwargs ) : \n    try : \n        from pyspark import SparkContext \n    except ImportError : \n        return 0 \n    cond1 = any ( [ isinstance ( arg , SparkContext ) for arg in args ] ) \n    cond2 = isinstance ( kwargs . get ( 'context' , None ) , SparkContext ) \n    cond3 = any ( [ isinstance ( arg , BoltArraySpark ) for arg in args ] ) \n    cond4 = any ( [ any ( [ isinstance ( sub , BoltArraySpark ) for sub in arg ] ) if isinstance ( arg , ( tuple , list ) ) else 0 for arg in args ] ) \n    return cond1 or cond2 or cond3 or cond4 "}
{"11357": "\ndef map ( self , func ) : \n    vshape = self . shape [ self . split : ] \n    x = self . _rdd . values ( ) . first ( ) \n    if x . shape == vshape : \n        a , b = asarray ( [ x ] ) , asarray ( [ x , x ] ) \n    else : \n        a , b = x , concatenate ( ( x , x ) ) \n    try : \n        atest = func ( a ) \n        btest = func ( b ) \n    except Exception as e : \n        raise RuntimeError ( \"Error evaluating function on test array, got error:\\n %s\" % e ) \n    if not ( isinstance ( atest , ndarray ) and isinstance ( btest , ndarray ) ) : \n        raise ValueError ( \"Function must return ndarray\" ) \n    elif atest . shape == btest . shape : \n        if self . _rekeyed is 1 : \n            rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] , func ( kv [ 1 ] ) ) ) \n            shape = ( self . shape [ 0 ] , ) + atest . shape \n        else : \n            count , rdd = zip_with_index ( self . _rdd . values ( ) ) \n            rdd = rdd . map ( lambda kv : ( ( kv [ 1 ] , ) , func ( kv [ 0 ] ) ) ) \n            shape = ( count , ) + atest . shape \n        split = 1 \n        rekeyed = 1 \n    elif atest . shape [ 0 ] == a . shape [ 0 ] and btest . shape [ 0 ] == b . shape [ 0 ] : \n        shape = self . shape [ 0 : self . split ] + atest . shape [ 1 : ] \n        split = self . split \n        rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] , func ( kv [ 1 ] ) ) ) \n        rekeyed = self . _rekeyed \n    else : \n        raise ValueError ( \"Cannot infer effect of function on shape\" ) \n    return self . _constructor ( rdd , rekeyed = rekeyed , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11362": "\ndef removepad ( idx , value , number , padding , axes = None ) : \n    if axes is None : \n        axes = range ( len ( number ) ) \n    mask = len ( number ) * [ 0 , ] \n    for i in range ( len ( mask ) ) : \n        if i in axes and padding [ i ] != 0 : \n            mask [ i ] = 1 \n    starts = [ 0 if ( i == 0 or not m ) else p for ( i , m , p ) in zip ( idx , mask , padding ) ] \n    stops = [ None if ( i == n - 1 or not m ) else - p for ( i , m , p , n ) in zip ( idx , mask , padding , number ) ] \n    slices = [ slice ( i1 , i2 ) for ( i1 , i2 ) in zip ( starts , stops ) ] \n    return value [ slices ] "}
{"11365": "\ndef getmask ( inds , n ) : \n    inds = asarray ( inds , 'int' ) \n    mask = zeros ( n , dtype = bool ) \n    mask [ inds ] = 1 \n    return mask "}
{"11366": "\ndef repartition ( self , npartitions ) : \n    rdd = self . _rdd . repartition ( npartitions ) \n    return self . _constructor ( rdd , ordered = 0 ) . __finalize__ ( self ) "}
{"11370": "\ndef _stat ( self , axis = None , func = None , name = None , keepdims = 0 ) : \n    if axis is None : \n        axis = list ( range ( len ( self . shape ) ) ) \n    axis = tupleize ( axis ) \n    if func and not name : \n        return self . reduce ( func , axis , keepdims ) \n    if name and not func : \n        from bolt . local . array import BoltArrayLocal \n        swapped = self . _align ( axis ) \n        def reducer ( left , right ) : \n            return left . combine ( right ) \n        counter = swapped . _rdd . values ( ) . mapPartitions ( lambda i : [ StatCounter ( values = i , stats = name ) ] ) . treeReduce ( reducer , depth = 3 ) \n        arr = getattr ( counter , name ) \n        if keepdims : \n            for i in axis : \n                arr = expand_dims ( arr , axis = i ) \n        return BoltArrayLocal ( arr ) . toscalar ( ) \n    else : \n        raise ValueError ( 'Must specify either a function or a statistic name.' ) "}
{"11371": "\ndef mean ( self , axis = None , keepdims = 0 ) : \n    return self . _stat ( axis , name = 'mean' , keepdims = keepdims ) "}
{"11372": "\ndef var ( self , axis = None , keepdims = 0 ) : \n    return self . _stat ( axis , name = 'variance' , keepdims = keepdims ) "}
{"11373": "\ndef std ( self , axis = None , keepdims = 0 ) : \n    return self . _stat ( axis , name = 'stdev' , keepdims = keepdims ) "}
{"11374": "\ndef sum ( self , axis = None , keepdims = 0 ) : \n    from operator import add \n    return self . _stat ( axis , func = add , keepdims = keepdims ) "}
{"11375": "\ndef max ( self , axis = None , keepdims = 0 ) : \n    from numpy import maximum \n    return self . _stat ( axis , func = maximum , keepdims = keepdims ) "}
{"11376": "\ndef min ( self , axis = None , keepdims = 0 ) : \n    from numpy import minimum \n    return self . _stat ( axis , func = minimum , keepdims = keepdims ) "}
{"11401": "\ndef transpose ( self , * axes ) : \n    new = argpack ( axes ) \n    old = range ( self . ndim ) \n    istransposeable ( new , old ) \n    if new == old : \n        return self . _barray \n    def f ( k ) : \n        return tuple ( k [ i ] for i in new ) \n    newrdd = self . _barray . _rdd . map ( lambda kv : ( f ( kv [ 0 ] ) , kv [ 1 ] ) ) \n    newshape = tuple ( self . shape [ i ] for i in new ) + self . _barray . values . shape \n    return BoltArraySpark ( newrdd , shape = newshape , ordered = 0 ) . __finalize__ ( self . _barray ) "}
{"11410": "\ndef discrete_best_alpha ( data , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = 1 , verbose = 1 ) : \n    xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] \n    best_index = argmin ( ksvalues ) \n    best_alpha = alpha_of_xmin [ best_index ] \n    best_xmin = xmins [ best_index ] \n    best_ks = ksvalues [ best_index ] \n    best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( data < best_xmin ) . sum ( ) , ( data >= best_xmin ) . sum ( ) ) ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11411": "\ndef discrete_best_alpha ( self , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = 1 , verbose = 1 , finite = 1 ) : \n    data = self . data \n    self . _xmins = xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = np . array ( [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] ) \n    self . _alpha_values = np . array ( alpha_of_xmin ) \n    self . _xmin_kstest = ksvalues \n    ksvalues [ np . isnan ( ksvalues ) ] = np . inf \n    best_index = argmin ( ksvalues ) \n    self . _alpha = best_alpha = alpha_of_xmin [ best_index ] \n    self . _xmin = best_xmin = xmins [ best_index ] \n    self . _ks = best_ks = ksvalues [ best_index ] \n    self . _likelihood = best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if finite : \n        self . _alpha = self . _alpha * ( n - 1. ) / n + 1. / n \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( data < best_xmin ) . sum ( ) , ( data >= best_xmin ) . sum ( ) ) ) \n    self . _ngtx = n = ( self . data >= self . _xmin ) . sum ( ) \n    self . _alphaerr = ( self . _alpha - 1.0 ) / np . sqrt ( n ) \n    if scipyOK : \n        self . _ks_prob = scipy . stats . ksone . sf ( self . _ks , n ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11412": "\ndef plotppf ( self , x = None , xmin = None , alpha = None , dolog = 1 , ** kwargs ) : \n    if not ( xmin ) : \n        xmin = self . _xmin \n    if not ( alpha ) : \n        alpha = self . _alpha \n    if not ( x ) : \n        x = np . sort ( self . data [ self . data > xmin ] ) \n    else : \n        x = np . sort ( x [ x > xmin ] ) \n    m0 = min ( x ) \n    N = ( 1.0 + np . arange ( len ( x ) ) ) [ : : - 1 ] \n    xmodel = m0 * N ** ( 1 / ( 1 - alpha ) ) / max ( N ) ** ( 1 / ( 1 - alpha ) ) \n    if dolog : \n        pylab . loglog ( x , xmodel , '.' , ** kwargs ) \n        pylab . gca ( ) . set_xlim ( min ( x ) , max ( x ) ) \n        pylab . gca ( ) . set_ylim ( min ( x ) , max ( x ) ) \n    else : \n        pylab . plot ( x , xmodel , '.' , ** kwargs ) \n    pylab . plot ( [ min ( x ) , max ( x ) ] , [ min ( x ) , max ( x ) ] , 'k--' ) \n    pylab . xlabel ( \"Real Value\" ) \n    pylab . ylabel ( \"Power-Law Model Value\" ) "}
{"11413": "\ndef lognormal ( self , doprint = 1 ) : \n    if scipyOK : \n        fitpars = scipy . stats . lognorm . fit ( self . data ) \n        self . lognormal_dist = scipy . stats . lognorm ( * fitpars ) \n        self . lognormal_ksD , self . lognormal_ksP = scipy . stats . kstest ( self . data , self . lognormal_dist . cdf ) \n        self . lognormal_likelihood = - 1 * scipy . stats . lognorm . nnlf ( fitpars , self . data ) \n        self . power_lognorm_likelihood = ( self . _likelihood + self . lognormal_likelihood ) \n        self . likelihood_ratio_D = - 2 * ( log ( self . _likelihood / self . lognormal_likelihood ) ) \n        if doprint : \n            print ( \"Lognormal KS D: %g  p(D): %g\" % ( self . lognormal_ksD , self . lognormal_ksP ) , end = ' ' ) \n            print ( \"  Likelihood Ratio Statistic (powerlaw/lognormal): %g\" % self . likelihood_ratio_D ) \n            print ( \"At this point, have a look at Clauset et al 2009 Appendix C: determining sigma(likelihood_ratio)\" ) "}
{"11414": "\ndef sanitize_turbo ( html , allowed_tags = TURBO_ALLOWED_TAGS , allowed_attrs = TURBO_ALLOWED_ATTRS ) : \n    return clean ( html , tags = allowed_tags , attributes = allowed_attrs , strip = 1 ) "}
{"11422": "\ndef parse_options ( self , options ) : \n    quote_open = 0 \n    parsed_options = { } \n    def parse_add_single_option ( opt ) : \n        if \"=\" in opt : \n            opt_name , opt_value = opt . split ( \"=\" , 1 ) \n            opt_value = opt_value . replace ( '\"' , '' ) \n        else : \n            opt_name = opt \n            opt_value = 1 \n        if \" \" in opt_name or not self . OPTION_NAME_RE . match ( opt_name ) : \n            raise InvalidOptionNameError ( \"%s is not valid option name.\" % opt_name ) \n        if self . strict_mode : \n            for valid_opt_name , value_required in self . OPTIONS_SPEC : \n                if opt_name . lower ( ) == valid_opt_name : \n                    if value_required and opt_value is 1 : \n                        raise MissingMandatoryOptionValueError ( \"%s is missing mandatory value.\" % opt_name ) \n                    break \n            else : \n                raise UnknownOptionNameError ( \"%s is unrecognized option name.\" % opt_name ) \n        if opt_name not in parsed_options : \n            parsed_options [ opt_name ] = [ ] \n        parsed_options [ opt_name ] . append ( opt_value ) \n    start_of_current_opt = 0 \n    i = 1 \n    for i , character in enumerate ( options ) : \n        if character == '\"' : \n            quote_open = not quote_open \n        if quote_open : \n            continue \n        if character == \",\" : \n            opt = options [ start_of_current_opt : i ] \n            parse_add_single_option ( opt ) \n            start_of_current_opt = i + 1 \n    if start_of_current_opt + 1 != i : \n        opt = options [ start_of_current_opt : ] \n        parse_add_single_option ( opt ) \n    if quote_open : \n        raise InvalidOptionsError ( \"Unbalanced quotes.\" ) \n    return parsed_options "}
{"11430": "\ndef mechs ( self ) : \n    if not self . _mechs : \n        self . _mechs = self . _inquire ( 0 , 0 , 0 , 1 ) [ 3 ] \n    return self . _mechs "}
{"11431": "\ndef store ( self , usage = None , mech = None , overwrite = 0 , default = 0 , cred_store = None ) : \n    if usage is None : \n        usage = self . usage \n    if isinstance ( mech , OID ) : \n        oid_ptr = ffi . addressof ( mech . _oid ) \n    else : \n        oid_ptr = ffi . cast ( 'gss_OID' , C . GSS_C_NO_OID ) \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    elements_stored = ffi . new ( 'gss_OID_set[1]' ) \n    usage_stored = ffi . new ( 'gss_cred_usage_t[1]' ) \n    if cred_store is None : \n        if not hasattr ( C , 'gss_store_cred' ) : \n            raise NotImplementedError ( \"The GSSAPI implementation does not support \" \"gss_store_cred\" ) \n        retval = C . gss_store_cred ( minor_status , self . _cred [ 0 ] , ffi . cast ( 'gss_cred_usage_t' , usage ) , oid_ptr , ffi . cast ( 'OM_uint32' , overwrite ) , ffi . cast ( 'OM_uint32' , default ) , elements_stored , usage_stored ) \n    else : \n        if not hasattr ( C , 'gss_store_cred_into' ) : \n            raise NotImplementedError ( \"The GSSAPI implementation does not support \" \"gss_store_cred_into\" ) \n        c_strings , elements , cred_store_kv_set = _make_kv_set ( cred_store ) \n        retval = C . gss_store_cred_into ( minor_status , self . _cred [ 0 ] , ffi . cast ( 'gss_cred_usage_t' , usage ) , oid_ptr , ffi . cast ( 'OM_uint32' , overwrite ) , ffi . cast ( 'OM_uint32' , default ) , cred_store_kv_set , elements_stored , usage_stored ) \n    try : \n        if GSS_ERROR ( retval ) : \n            if oid_ptr : \n                raise _exception_for_status ( retval , minor_status [ 0 ] , oid_ptr ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ 0 ] ) \n    except : \n        if elements_stored [ 0 ] : \n            C . gss_release_oid_set ( minor_status , elements_stored ) \n        raise \n    return ( OIDSet ( elements_stored ) , usage_stored [ 0 ] ) "}
{"11433": "\ndef init ( dist = 'dist' , minver = None , maxver = None , use_markdown_readme = 1 , use_stdeb = 0 , use_distribute = 0 , ) : \n    if not minver == maxver == None : \n        import sys \n        if not minver <= sys . version < ( maxver or 'Any' ) : \n            sys . stderr . write ( '%s: requires python version in <%s, %s), not %s\\n' % ( sys . argv [ 0 ] , minver or 'any' , maxver or 'any' , sys . version . split ( ) [ 0 ] ) ) \n            sys . exit ( 1 ) \n    if use_distribute : \n        from distribute_setup import use_setuptools \n        use_setuptools ( to_dir = dist ) \n        from setuptools import setup \n    else : \n        try : \n            from setuptools import setup \n        except ImportError : \n            from distutils . core import setup \n    if use_markdown_readme : \n        try : \n            import setuptools . command . sdist \n            setuptools . command . sdist . READMES = tuple ( list ( getattr ( setuptools . command . sdist , 'READMES' , ( ) ) ) + [ 'README.md' ] ) \n        except ImportError : \n            pass \n    if use_stdeb : \n        import platform \n        if 'debian' in platform . dist ( ) : \n            try : \n                import stdeb \n            except ImportError : \n                pass \n    return setup "}
{"11438": "\ndef djfrontend_modernizr ( version = None ) : \n    if version is None : \n        version = getattr ( settings , 'DJFRONTEND_MODERNIZR' , DJFRONTEND_MODERNIZR_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        template = '<script src=\"{static}djfrontend/js/modernizr/{v}/modernizr.js\"></script>' \n    else : \n        template = ( '<script src=\"//cdnjs.cloudflare.com/ajax/libs/modernizr/{v}/modernizr.min.js\"></script>\\n' '<script>window.Modernizr || document.write(\\'<script src=\"{static}djfrontend/js/modernizr/{v}/modernizr.min.js\"><\\/script>\\')</script>' ) \n    return format_html ( template , static = _static_url , v = version ) "}
{"11439": "\ndef djfrontend_jquery ( version = None ) : \n    if version is None : \n        version = getattr ( settings , 'DJFRONTEND_JQUERY' , DJFRONTEND_JQUERY_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        template = '<script src=\"{static}djfrontend/js/jquery/{v}/jquery.js\"></script>' \n    else : \n        template = ( '<script src=\"//ajax.googleapis.com/ajax/libs/jquery/{v}/jquery.min.js\"></script>' '<script>window.jQuery || document.write(\\'<script src=\"{static}djfrontend/js/jquery/{v}/jquery.min.js\"><\\/script>\\')</script>' ) \n    return format_html ( template , static = _static_url , v = version ) "}
{"11440": "\ndef djfrontend_jqueryui ( version = None ) : \n    if version is None : \n        version = getattr ( settings , 'DJFRONTEND_JQUERYUI' , DJFRONTEND_JQUERYUI_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        return format_html ( '<script src=\"{0}djfrontend/js/jquery/jqueryui/{1}/jquery-ui.js\"></script>' , settings . STATIC_URL , version ) \n    else : \n        return format_html ( '<script src=\"//ajax.googleapis.com/ajax/libs/jqueryui/{v}/jquery-ui.min.js\"></script>' '<script>window.jQuery.ui || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jqueryui/{v}/jquery-ui.min.js\"><\\/script>\\')</script>' , static = _static_url , v = version ) "}
{"11441": "\ndef djfrontend_jquery_datatables ( version = None ) : \n    if version is None : \n        if not getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES' , 0 ) : \n            version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_VERSION' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) \n        else : \n            version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        template = '<script src=\"{static}djfrontend/js/jquery/jquery.dataTables/{v}/jquery.dataTables.js\"></script>' \n    else : \n        template = ( '<script src=\"//cdnjs.cloudflare.com/ajax/libs/datatables/{v}/jquery.dataTables.min.js\"></script>' '<script>window.jQuery.fn.DataTable || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.dataTables/{v}/jquery.dataTables.min.js\"><\\/script>\\')</script>' ) \n    return format_html ( template , static = _static_url , v = version ) "}
{"11442": "\ndef djfrontend_jquery_datatables_css ( version = None ) : \n    if version is None : \n        if not getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_CSS' , 0 ) : \n            version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_VERSION' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) \n        else : \n            version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_CSS' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) \n    return format_html ( '<link rel=\"stylesheet\" href=\"{static}djfrontend/css/jquery/jquery.dataTables/{v}/jquery.dataTables{min}.css\">' , static = _static_url , v = version , min = _min ) "}
{"11443": "\ndef djfrontend_jquery_datatables_themeroller ( version = None ) : \n    if version is None : \n        if not getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_THEMEROLLER' , 0 ) : \n            version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_VERSION' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) \n        else : \n            version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_THEMEROLLER' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) \n    return format_html ( '<link rel=\"stylesheet\" href=\"href=\"{static}djfrontend/css/jquery/jquery.dataTables/{v}/jquery.dataTables_themeroller.min.css\">' , static = _static_url , v = version ) "}
{"11444": "\ndef djfrontend_jquery_formset ( version = None ) : \n    if version is None : \n        version = getattr ( settings , 'DJFRONTEND_JQUERY_FORMSET' , DJFRONTEND_JQUERY_FORMSET_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        template = '<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.js\"></script>' \n    else : \n        template = ( '<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery.formset/{v}/jquery.formset.min.js\"></script>\\n' '<script>window.jQuery.fn.formset || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.formset/{v}/jquery.formset.min.js\"><\\/script>\\')</script>' ) \n    return format_html ( template , static = _static_url , v = version ) "}
{"11445": "\ndef djfrontend_jquery_scrollto ( version = None ) : \n    if version is None : \n        version = getattr ( settings , 'DJFRONTEND_JQUERY_SCROLLTO' , DJFRONTEND_JQUERY_SCROLLTO_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        template = '<script src=\"{static}djfrontend/js/jquery/jquery.scrollTo/{v}/jquery.scrollTo.js\"></script>' \n    else : \n        template = ( '<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery-scrollTo/{v}/jquery.scrollTo.min.js\"></script>' '<script>window.jQuery.fn.scrollTo || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.scrollTo/{v}/jquery.scrollTo.min.js\"><\\/script>\\')</script>' ) \n    return format_html ( template , static = _static_url , v = version ) "}
{"11446": "\ndef djfrontend_jquery_smoothscroll ( version = None ) : \n    if version is None : \n        version = getattr ( settings , 'DJFRONTEND_JQUERY_SMOOTHSCROLL' , DJFRONTEND_JQUERY_SMOOTHSCROLL_DEFAULT ) \n    if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n        template = '<script src=\"{static}djfrontend/js/jquery/jquery.smooth-scroll/{v}/jquery.smooth-scroll.js\"></script>' \n    else : \n        template = ( '<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery-smooth-scroll/{v}/jquery.smooth-scroll.min.js\"></script>' '<script>window.jQuery.fn.smoothScroll || document.write(\\'<script src=\"{static}djfrontend/js/jquery/jquery.smooth-scroll/{v}/jquery.smooth-scroll.min.js\"><\\/script>\\')</script>' ) \n    return format_html ( template , static = _static_url , v = version ) "}
{"11447": "\ndef djfrontend_twbs_css ( version = None ) : \n    if version is None : \n        if not getattr ( settings , 'DJFRONTEND_TWBS_CSS' , 0 ) : \n            version = getattr ( settings , 'DJFRONTEND_TWBS_VERSION' , DJFRONTEND_TWBS_VERSION_DEFAULT ) \n        else : \n            version = getattr ( settings , 'DJFRONTEND_TWBS_CSS' , DJFRONTEND_TWBS_VERSION_DEFAULT ) \n    return format_html ( '<link rel=\"stylesheet\" href=\"{static}djfrontend/css/twbs/{v}/bootstrap{min}.css\">' , static = _static_url , v = version , min = _min ) "}
{"11448": "\ndef djfrontend_ga ( account = None ) : \n    if account is None : \n        account = getattr ( settings , 'DJFRONTEND_GA' , 0 ) \n    if account : \n        if getattr ( settings , 'TEMPLATE_DEBUG' , 0 ) : \n            return '' \n        else : \n            if getattr ( settings , 'DJFRONTEND_GA_SETDOMAINNAME' , 0 ) : \n                if getattr ( settings , 'DJFRONTEND_GA_SETALLOWLINKER' , 0 ) : \n                    return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"require\", \"linker\");ga(\"linker:autoLink\", [\"%s\"]);ga(\"create\", \"%s\", \"auto\", {\"allowLinker\": true});ga(\"send\", \"pageview\");</script>' % ( settings . DJFRONTEND_GA_SETDOMAINNAME , account ) ) \n                else : \n                    return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"create\", \"%s\", \"%s\");ga(\"send\", \"pageview\");</script>' % ( account , settings . DJFRONTEND_GA_SETDOMAINNAME ) ) \n            else : \n                return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"create\", \"%s\", \"auto\");ga(\"send\", \"pageview\");</script>' % account ) \n    else : \n        return '' "}
{"11453": "\ndef serialize ( self , obj , * args , ** kwargs ) : \n    data = super ( Users , self ) . serialize ( obj , * args , ** kwargs ) \n    profile = data . pop ( 'fields' ) \n    profile . setdefault ( 'name' , obj . get_full_name ( ) ) \n    fields = data [ 'fields' ] = { 'username' : obj . get_username ( ) , 'emails' : [ ] , 'profile' : profile , 'permissions' : sorted ( self . model . get_all_permissions ( obj ) ) , } \n    for sensitive in [ 'password' , 'user_permissions_ids' , 'is_active' , 'is_staff' , 'is_superuser' , 'groups_ids' , ] : \n        profile . pop ( sensitive , None ) \n    try : \n        fields [ 'createdAt' ] = profile . pop ( 'date_joined' ) \n    except KeyError : \n        date_joined = getattr ( obj , 'get_date_joined' , lambda : getattr ( obj , 'date_joined' , None ) ) ( ) \n        if date_joined : \n            fields [ 'createdAt' ] = date_joined \n    try : \n        email = profile . pop ( 'email' ) \n    except KeyError : \n        email = getattr ( obj , 'get_email' , lambda : getattr ( obj , 'email' , None ) ) ( ) \n    if email : \n        fields [ 'emails' ] . append ( { 'address' : email , 'verified' : 1 } ) \n    return data "}
{"11454": "\ndef deserialize_profile ( profile , key_prefix = '' , pop = 0 ) : \n    result = { } \n    if pop : \n        getter = profile . pop \n    else : \n        getter = profile . get \n    def prefixed ( name ) : \n        return '%s%s' % ( key_prefix , name ) \n    for key in profile . keys ( ) : \n        val = getter ( key ) \n        if key == prefixed ( 'name' ) : \n            result [ 'full_name' ] = val \n        else : \n            raise MeteorError ( 400 , 'Bad profile key: %r' % key ) \n    return result "}
{"11455": "\ndef update ( self , selector , update , options = None ) : \n    del options \n    user = get_object ( self . model , selector [ '_id' ] , pk = this . user_id , ) \n    profile_update = self . deserialize_profile ( update [ '$set' ] , key_prefix = 'profile.' , pop = 1 , ) \n    if len ( update [ '$set' ] ) != 0 : \n        raise MeteorError ( 400 , 'Invalid update fields: %r' ) \n    for key , val in profile_update . items ( ) : \n        setattr ( user , key , val ) \n    user . save ( ) "}
{"11457": "\ndef validated_user ( cls , token , purpose , minutes_valid ) : \n    try : \n        username , auth_hash = loads ( token . decode ( 'base64' ) ) \n    except ( ValueError , Error ) : \n        cls . auth_failed ( token = token ) \n    try : \n        user = cls . user_model . objects . get ( ** { cls . user_model . USERNAME_FIELD : username , 'is_active' : 1 , } ) \n        user . backend = 'django.contrib.auth.backends.ModelBackend' \n    except cls . user_model . DoesNotExist : \n        cls . auth_failed ( username = username , token = token ) \n    if auth_hash not in iter_auth_hashes ( user , purpose , minutes_valid ) : \n        cls . auth_failed ( username = username , token = token ) \n    return user "}
{"11458": "\ndef check_secure ( ) : \n    if this . request . is_secure ( ) : \n        return 1 \n    elif this . request . META [ 'REMOTE_ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : \n        return 1 \n    raise MeteorError ( 403 , 'Authentication refused without SSL.' ) "}
{"11459": "\ndef get_username ( self , user ) : \n    if isinstance ( user , basestring ) : \n        return user \n    elif isinstance ( user , dict ) and len ( user ) == 1 : \n        [ ( key , val ) ] = user . items ( ) \n        if key == 'username' or ( key == self . user_model . USERNAME_FIELD ) : \n            return val \n        elif key in ( 'email' , 'emails.address' ) : \n            email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) \n            if self . user_model . USERNAME_FIELD == email_field : \n                return val \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = 1 , ) . get ( ** { email_field : val } ) \n        elif key in ( 'id' , 'pk' ) : \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = 1 , ) . get ( pk = val , ) \n        else : \n            raise MeteorError ( 400 , 'Invalid user lookup: %r' % key ) \n    else : \n        raise MeteorError ( 400 , 'Invalid user expression: %r' % user ) "}
{"11461": "\ndef do_login ( self , user ) : \n    this . user_id = user . pk \n    this . user_ddp_id = get_meteor_id ( user ) \n    this . user_sub_id = meteor_random_id ( ) \n    API . do_sub ( this . user_sub_id , 'LoggedInUser' , silent = 1 ) \n    self . update_subs ( user . pk ) \n    user_logged_in . send ( sender = user . __class__ , request = this . request , user = user , ) "}
{"11462": "\ndef do_logout ( self ) : \n    API . do_unsub ( this . user_sub_id , silent = 1 ) \n    del this . user_sub_id \n    self . update_subs ( None ) \n    user_logged_out . send ( sender = self . user_model , request = this . request , user = this . user , ) \n    this . user_id = None \n    this . user_ddp_id = None "}
{"11466": "\ndef change_password ( self , old_password , new_password ) : \n    try : \n        user = this . user \n    except self . user_model . DoesNotExist : \n        self . auth_failed ( ) \n    user = auth . authenticate ( username = user . get_username ( ) , password = self . get_password ( old_password ) , ) \n    if user is None : \n        self . auth_failed ( ) \n    else : \n        user . set_password ( self . get_password ( new_password ) ) \n        user . save ( ) \n        password_changed . send ( sender = __name__ , request = this . request , user = user , ) \n        return { \"passwordChanged\" : 1 } "}
{"11471": "\ndef get_meteor_id ( obj_or_model , obj_pk = None ) : \n    if obj_or_model is None : \n        return None \n    meta = obj_or_model . _meta \n    model = meta . model \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( obj_or_model , model ) : \n        if isinstance ( meta . pk , AleaIdField ) : \n            return obj_or_model . pk \n        if obj_pk is None : \n            obj_pk = str ( obj_or_model . pk ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if len ( alea_unique_fields ) == 1 : \n        aid = alea_unique_fields [ 0 ] . attname \n        if isinstance ( obj_or_model , model ) : \n            val = getattr ( obj_or_model , aid ) \n        elif obj_pk is None : \n            val = None \n        else : \n            val = model . objects . values_list ( aid , flat = 1 ) . get ( pk = obj_pk , ) \n        if val : \n            return val \n    if obj_pk is None : \n        return None \n    content_type = ContentType . objects . get_for_model ( model ) \n    try : \n        return ObjectMapping . objects . values_list ( 'meteor_id' , flat = 1 , ) . get ( content_type = content_type , object_id = obj_pk , ) \n    except ObjectDoesNotExist : \n        return ObjectMapping . objects . create ( content_type = content_type , object_id = obj_pk , meteor_id = meteor_random_id ( '/collection/%s' % meta ) , ) . meteor_id "}
{"11473": "\ndef get_object_id ( model , meteor_id ) : \n    if meteor_id is None : \n        return None \n    meta = model . _meta \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( meta . pk , AleaIdField ) : \n        return meteor_id \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if len ( alea_unique_fields ) == 1 : \n        val = model . objects . values_list ( 'pk' , flat = 1 , ) . get ( ** { alea_unique_fields [ 0 ] . attname : meteor_id , } ) \n        if val : \n            return val \n    content_type = ContentType . objects . get_for_model ( model ) \n    return ObjectMapping . objects . filter ( content_type = content_type , meteor_id = meteor_id , ) . values_list ( 'object_id' , flat = 1 ) . get ( ) "}
{"11476": "\ndef set_default_forwards ( app_name , operation , apps , schema_editor ) : \n    model = apps . get_model ( app_name , operation . model_name ) \n    for obj_pk in model . objects . values_list ( 'pk' , flat = 1 ) : \n        model . objects . filter ( pk = obj_pk ) . update ( ** { operation . name : get_meteor_id ( model , obj_pk ) , } ) "}
{"11477": "\ndef set_default_reverse ( app_name , operation , apps , schema_editor ) : \n    model = apps . get_model ( app_name , operation . model_name ) \n    for obj_pk in model . objects . values_list ( 'pk' , flat = 1 ) : \n        get_meteor_id ( model , obj_pk ) "}
{"11481": "\ndef initialize_options ( self ) : \n    setuptools . command . build_py . build_py . initialize_options ( self ) \n    self . meteor = 'meteor' \n    self . meteor_debug = 0 \n    self . build_lib = None \n    self . package_dir = None \n    self . meteor_builds = [ ] \n    self . no_prune_npm = None \n    self . inplace = 1 "}
{"11483": "\ndef run ( self ) : \n    for ( package , source , target , extra_args ) in self . meteor_builds : \n        src_dir = self . get_package_dir ( package ) \n        project_dir = self . path_to_dir ( src_dir , source ) \n        target_dir = self . path_to_dir ( src_dir , target ) \n        output_dir = self . path_to_dir ( os . path . abspath ( SETUP_DIR if self . inplace else self . build_lib ) , target_dir , ) \n        cmdline = [ self . meteor , 'build' , '--directory' , output_dir ] \n        no_prune_npm = self . no_prune_npm \n        if extra_args [ : 1 ] == [ '--no-prune-npm' ] : \n            no_prune_npm = 1 \n            extra_args [ : 1 ] = [ ] \n        if self . meteor_debug and '--debug' not in cmdline : \n            cmdline . append ( '--debug' ) \n        cmdline . extend ( extra_args ) \n        log . info ( 'building meteor app %r (%s)' , project_dir , ' ' . join ( cmdline ) , ) \n        subprocess . check_call ( cmdline , cwd = project_dir ) \n        if not no_prune_npm : \n            npm_build_dir = os . path . join ( output_dir , 'bundle' , 'programs' , 'server' , 'npm' , ) \n            log . info ( 'pruning meteor npm build %r' , npm_build_dir ) \n            shutil . rmtree ( npm_build_dir ) "}
{"11488": "\ndef api_endpoint ( path_or_func = None , decorate = 1 ) : \n    def maybe_decorated ( func ) : \n        if decorate : \n            for decorator in API_ENDPOINT_DECORATORS : \n                func = decorator ( ) ( func ) \n        return func \n    if callable ( path_or_func ) : \n        path_or_func . api_path = path_or_func . __name__ \n        return maybe_decorated ( path_or_func ) \n    else : \n        def _api_endpoint ( func ) : \n            if path_or_func is None : \n                func . api_path = func . __name__ \n            else : \n                func . api_path = path_or_func \n            return maybe_decorated ( func ) \n        return _api_endpoint "}
{"11504": "\ndef ddpp_sockjs_info ( environ , start_response ) : \n    import random \n    import ejson \n    start_response ( '200 OK' , [ ( 'Content-Type' , 'application/json; charset=UTF-8' ) , ] + common_headers ( environ ) , ) \n    yield ejson . dumps ( collections . OrderedDict ( [ ( 'websocket' , 1 ) , ( 'origins' , [ '*:*' , ] ) , ( 'cookie_needed' , 0 ) , ( 'entropy' , random . getrandbits ( 32 ) ) , ] ) ) "}
{"11512": "\ndef greenify ( ) : \n    if _GREEN : \n        return \n    _GREEN [ 1 ] = 1 \n    from gevent . monkey import patch_all , saved \n    if ( 'threading' in sys . modules ) and ( 'threading' not in saved ) : \n        import warnings \n        warnings . warn ( 'threading module loaded before patching!' ) \n    patch_all ( ) \n    try : \n        import psycopg2 \n        del psycopg2 \n    except ImportError : \n        from psycopg2cffi import compat \n        compat . register ( ) \n    from psycogreen . gevent import patch_psycopg \n    patch_psycopg ( ) "}
{"11516": "\ndef get ( self , name , factory , * factory_args , ** factory_kwargs ) : \n    update_thread_local = getattr ( factory , 'update_thread_local' , 1 ) \n    if ( not update_thread_local ) or ( name not in self . __dict__ ) : \n        obj = factory ( * factory_args , ** factory_kwargs ) \n        if update_thread_local : \n            setattr ( self , name , obj ) \n        return obj \n    return getattr ( self , name ) "}
{"11517": "\ndef emit ( self , record ) : \n    if getattr ( this , 'subs' , { } ) . get ( LOGS_NAME , 0 ) : \n        self . format ( record ) \n        this . send ( { 'msg' : ADDED , 'collection' : LOGS_NAME , 'id' : meteor_random_id ( '/collection/%s' % LOGS_NAME ) , 'fields' : { attr : { 'args' : lambda args : [ repr ( arg ) for arg in args ] , 'created' : datetime . datetime . fromtimestamp , 'exc_info' : stacklines_or_none , } . get ( attr , lambda val : val ) ( getattr ( record , attr , None ) ) for attr in ( 'args' , 'asctime' , 'created' , 'exc_info' , 'filename' , 'funcName' , 'levelname' , 'levelno' , 'lineno' , 'module' , 'msecs' , 'message' , 'name' , 'pathname' , 'process' , 'processName' , 'relativeCreated' , 'thread' , 'threadName' , ) } , } ) "}
{"11527": "\ndef send_json ( self , ids = None ) : \n    items = ids or self . _registration_id \n    values = { \"registration_ids\" : items } \n    if self . _data is not None : \n        values [ \"data\" ] = self . _data \n    for key , val in self . _kwargs . items ( ) : \n        if val : \n            values [ key ] = val \n    data = json . dumps ( values , separators = ( \",\" , \":\" ) , sort_keys = 1 ) . encode ( self . encoding ) \n    result = json . loads ( self . _send ( data , \"application/json\" ) ) \n    if ( \"failure\" in result ) and ( result [ \"failure\" ] ) : \n        unregistered = [ ] \n        throw_error = 0 \n        for index , error in enumerate ( result . get ( \"results\" , [ ] ) ) : \n            error = error . get ( \"error\" , \"\" ) \n            if error in ( \"NotRegistered\" , \"InvalidRegistration\" ) : \n                unregistered . append ( items [ index ] ) \n            elif error != \"\" : \n                throw_error = 1 \n        self . deactivate_unregistered_devices ( unregistered ) \n        if throw_error : \n            raise GCMPushError ( result ) \n    return result "}
{"11531": "\ndef search ( term = None , phrase = None , limit = DEFAULT_SEARCH_LIMIT , api_key = GIPHY_PUBLIC_KEY , strict = 0 , rating = None ) : \n    return Giphy ( api_key = api_key , strict = strict ) . search ( term = term , phrase = phrase , limit = limit , rating = rating ) "}
{"11532": "\ndef translate ( term = None , phrase = None , api_key = GIPHY_PUBLIC_KEY , strict = 0 , rating = None ) : \n    return Giphy ( api_key = api_key , strict = strict ) . translate ( term = term , phrase = phrase , rating = rating ) "}
{"11533": "\ndef trending ( limit = DEFAULT_SEARCH_LIMIT , api_key = GIPHY_PUBLIC_KEY , strict = 0 , rating = None ) : \n    return Giphy ( api_key = api_key , strict = strict ) . trending ( limit = limit , rating = rating ) "}
{"11534": "\ndef gif ( gif_id , api_key = GIPHY_PUBLIC_KEY , strict = 0 ) : \n    return Giphy ( api_key = api_key , strict = strict ) . gif ( gif_id ) "}
{"11535": "\ndef screensaver ( tag = None , api_key = GIPHY_PUBLIC_KEY , strict = 0 ) : \n    return Giphy ( api_key = api_key , strict = strict ) . screensaver ( tag = tag ) "}
{"11536": "\ndef upload ( tags , file_path , username = None , api_key = GIPHY_PUBLIC_KEY , strict = 0 ) : \n    return Giphy ( api_key = api_key , strict = strict ) . upload ( tags , file_path , username ) "}
{"11539": "\ndef translate ( self , term = None , phrase = None , strict = 0 , rating = None ) : \n    assert any ( ( term , phrase ) ) , 'You must supply a term or phrase to search' \n    if phrase : \n        phrase = phrase . replace ( ' ' , '-' ) \n    params = { 's' : ( term or phrase ) } \n    if rating : \n        params . update ( { 'rating' : rating } ) \n    resp = self . _fetch ( 'translate' , ** params ) \n    if resp [ 'data' ] : \n        return GiphyImage ( resp [ 'data' ] ) \n    elif strict or self . strict : \n        raise GiphyApiException ( \"Term/Phrase '%s' could not be translated into a GIF\" % ( term or phrase ) ) "}
{"11540": "\ndef trending ( self , rating = None , limit = DEFAULT_SEARCH_LIMIT ) : \n    results_yielded = 0 \n    page , per_page = 0 , 25 \n    params = { 'rating' : rating } if rating else { } \n    fetch = partial ( self . _fetch , 'trending' , ** params ) \n    while 1 : \n        data = fetch ( offset = page , limit = per_page ) \n        page += per_page \n        if not data [ 'data' ] : \n            raise StopIteration \n        for item in data [ 'data' ] : \n            results_yielded += 1 \n            yield GiphyImage ( item ) \n            if limit is not None and results_yielded >= limit : \n                raise StopIteration \n        if ( page >= data [ 'pagination' ] [ 'total_count' ] or ( limit is not None and results_yielded >= limit ) ) : \n            raise StopIteration "}
{"11541": "\ndef gif ( self , gif_id , strict = 0 ) : \n    resp = self . _fetch ( gif_id ) \n    if resp [ 'data' ] : \n        return GiphyImage ( resp [ 'data' ] ) \n    elif strict or self . strict : \n        raise GiphyApiException ( \"GIF with ID '%s' could not be found\" % gif_id ) "}
{"11544": "\ndef authenticate ( self , email = None , password = None , source = None ) : \n    from gdata . service import BadAuthentication \n    Api . yt_service . email = email if email else settings . YOUTUBE_AUTH_EMAIL \n    Api . yt_service . password = password if password else settings . YOUTUBE_AUTH_PASSWORD \n    Api . yt_service . source = source if source else settings . YOUTUBE_CLIENT_ID \n    try : \n        Api . yt_service . ProgrammaticLogin ( ) \n        self . authenticated = 1 \n    except BadAuthentication : \n        raise ApiError ( _ ( \"Incorrect username or password\" ) ) "}
{"11546": "\ndef check_upload_status ( self , video_id ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    entry = self . fetch_video ( video_id ) \n    upload_status = Api . yt_service . CheckUploadStatus ( entry ) \n    if upload_status is not None : \n        video_upload_state = upload_status [ 0 ] \n        detailed_message = upload_status [ 1 ] \n        return { \"upload_state\" : video_upload_state , \"detailed_message\" : detailed_message } \n    else : \n        return 1 "}
{"11548": "\ndef delete_video ( self , video_id ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    entry = self . fetch_video ( video_id ) \n    response = Api . yt_service . DeleteVideoEntry ( entry ) \n    if not response : \n        raise OperationError ( _ ( \"Cannot be deleted from Youtube\" ) ) \n    return 1 "}
{"11549": "\ndef check_video_availability ( request , video_id ) : \n    api = Api ( ) \n    api . authenticate ( ) \n    availability = api . check_upload_status ( video_id ) \n    if availability is not 1 : \n        data = { 'success' : 0 } \n    else : \n        data = { 'success' : 1 } \n    return HttpResponse ( json . dumps ( data ) , content_type = \"application/json\" ) "}
{"11550": "\ndef video ( request , video_id ) : \n    api = Api ( ) \n    api . authenticate ( ) \n    availability = api . check_upload_status ( video_id ) \n    if availability is not 1 : \n        video = Video . objects . filter ( video_id = video_id ) . get ( ) \n        state = availability [ \"upload_state\" ] \n        if state == \"failed\" or state == \"rejected\" : \n            return render_to_response ( \"django_youtube/video_failed.html\" , { \"video\" : video , \"video_id\" : video_id , \"message\" : _ ( \"Invalid video.\" ) , \"availability\" : availability } , context_instance = RequestContext ( request ) ) \n        else : \n            return render_to_response ( \"django_youtube/video_unavailable.html\" , { \"video\" : video , \"video_id\" : video_id , \"message\" : _ ( \"This video is currently being processed\" ) , \"availability\" : availability } , context_instance = RequestContext ( request ) ) \n    video_params = _video_params ( request , video_id ) \n    return render_to_response ( \"django_youtube/video.html\" , video_params , context_instance = RequestContext ( request ) ) "}
{"11562": "\ndef loads ( s , strip_comments = 0 , ** kw ) : \n    kw [ 'strip_comments' ] = strip_comments \n    return [ parse_node ( ss . strip ( ) , ** kw ) for ss in s . split ( ';' ) if ss . strip ( ) ] "}
{"11564": "\ndef load ( fp , strip_comments = 0 , ** kw ) : \n    kw [ 'strip_comments' ] = strip_comments \n    return loads ( fp . read ( ) , ** kw ) "}
{"11565": "\ndef read ( fname , encoding = 'utf8' , strip_comments = 0 , ** kw ) : \n    kw [ 'strip_comments' ] = strip_comments \n    with io . open ( fname , encoding = encoding ) as fp : \n        return load ( fp , ** kw ) "}
{"11566": "\ndef parse_node ( s , strip_comments = 0 , ** kw ) : \n    if strip_comments : \n        s = COMMENT . sub ( '' , s ) \n    s = s . strip ( ) \n    parts = s . split ( ')' ) \n    if len ( parts ) == 1 : \n        descendants , label = [ ] , s \n    else : \n        if not parts [ 0 ] . startswith ( '(' ) : \n            raise ValueError ( 'unmatched braces %s' % parts [ 0 ] [ : 100 ] ) \n        descendants = list ( _parse_siblings ( ')' . join ( parts [ : - 1 ] ) [ 1 : ] , ** kw ) ) \n        label = parts [ - 1 ] \n    name , length = _parse_name_and_length ( label ) \n    return Node . create ( name = name , length = length , descendants = descendants , ** kw ) "}
{"11569": "\ndef ascii_art ( self , strict = 0 , show_internal = 1 ) : \n    cmap = { '\\u2500' : '-' , '\\u2502' : '|' , '\\u250c' : '/' , '\\u2514' : '\\\\' , '\\u251c' : '|' , '\\u2524' : '|' , '\\u253c' : '+' , } \n    def normalize ( line ) : \n        m = re . compile ( '(?<=\\u2502)(?P<s>\\s+)(?=[\\u250c\\u2514\\u2502])' ) \n        line = m . sub ( lambda m : m . group ( 's' ) [ 1 : ] , line ) \n        line = re . sub ( '\\u2500\\u2502' , '\\u2500\\u2524' , line ) \n        line = re . sub ( '\\u2502\\u2500' , '\\u251c' , line ) \n        line = re . sub ( '\\u2524\\u2500' , '\\u253c' , line ) \n        if strict : \n            for u , a in cmap . items ( ) : \n                line = line . replace ( u , a ) \n        return line \n    return '\\n' . join ( normalize ( l ) for l in self . _ascii_art ( show_internal = show_internal ) [ 0 ] if set ( l ) != { ' ' , '\\u2502' } ) "}
{"11571": "\ndef prune ( self , leaves , inverse = 0 ) : \n    self . visit ( lambda n : n . ancestor . descendants . remove ( n ) , lambda n : ( ( not inverse and n in leaves ) or ( inverse and n . is_leaf and n not in leaves ) ) and n . ancestor , mode = \"postorder\" ) "}
{"11576": "\ndef dispose ( json_str ) : \n    result_str = list ( json_str ) \n    escaped = 0 \n    normal = 1 \n    sl_comment = 0 \n    ml_comment = 0 \n    quoted = 0 \n    a_step_from_comment = 0 \n    a_step_from_comment_away = 0 \n    former_index = None \n    for index , char in enumerate ( json_str ) : \n        if escaped : \n            escaped = 0 \n            continue \n        if a_step_from_comment : \n            if char != '/' and char != '*' : \n                a_step_from_comment = 0 \n                normal = 1 \n                continue \n        if a_step_from_comment_away : \n            if char != '/' : \n                a_step_from_comment_away = 0 \n        if char == '\"' : \n            if normal and not escaped : \n                quoted = 1 \n                normal = 0 \n            elif quoted and not escaped : \n                quoted = 0 \n                normal = 1 \n        elif char == '\\\\' : \n            if normal or quoted : \n                escaped = 1 \n        elif char == '/' : \n            if a_step_from_comment : \n                a_step_from_comment = 0 \n                sl_comment = 1 \n                normal = 0 \n                former_index = index - 1 \n            elif a_step_from_comment_away : \n                a_step_from_comment_away = 0 \n                normal = 1 \n                ml_comment = 0 \n                for i in range ( former_index , index + 1 ) : \n                    result_str [ i ] = \"\" \n            elif normal : \n                a_step_from_comment = 1 \n                normal = 0 \n        elif char == '*' : \n            if a_step_from_comment : \n                a_step_from_comment = 0 \n                ml_comment = 1 \n                normal = 0 \n                former_index = index - 1 \n            elif ml_comment : \n                a_step_from_comment_away = 1 \n        elif char == '\\n' : \n            if sl_comment : \n                sl_comment = 0 \n                normal = 1 \n                for i in range ( former_index , index + 1 ) : \n                    result_str [ i ] = \"\" \n        elif char == ']' or char == '}' : \n            if normal : \n                _remove_last_comma ( result_str , index ) \n    return ( \"\" if isinstance ( json_str , str ) else u\"\" ) . join ( result_str ) "}
{"11578": "\ndef get_argument ( self , name , default = _ARG_DEFAULT , strip = 1 ) : \n    args = self . get_arguments ( name , strip = strip ) \n    if not args : \n        if default is self . _ARG_DEFAULT : \n            raise HTTPError ( 400 , \"Missing argument %s\" % name ) \n        return default \n    return args [ - 1 ] "}
{"11579": "\ndef get_arguments ( self , name , strip = 1 ) : \n    values = [ ] \n    for v in self . request . params . getall ( name ) : \n        v = self . decode_argument ( v , name = name ) \n        if isinstance ( v , unicode ) : \n            v = re . sub ( r\"[\\x00-\\x08\\x0e-\\x1f]\" , \" \" , v ) \n        if strip : \n            v = v . strip ( ) \n        values . append ( v ) \n    return values "}
{"11602": "\ndef selectPolicy ( self , origin , request_method = None ) : \n    ret_origin = None \n    policyname = None \n    if self . matchstrategy in ( \"firstmatch\" , \"verbmatch\" ) : \n        for pol in self . activepolicies : \n            policy = self . policies [ pol ] \n            ret_origin = None \n            policyname = policy . name \n            if policyname == \"deny\" : \n                break \n            if self . matchstrategy == \"verbmatch\" : \n                if policy . methods != \"*\" and not CORS . matchlist ( request_method , policy . methods , case_sensitive = 1 ) : \n                    continue \n            if origin and policy . match : \n                if CORS . matchlist ( origin , policy . match ) : \n                    ret_origin = origin \n            elif policy . origin == \"copy\" : \n                ret_origin = origin \n            elif policy . origin : \n                ret_origin = policy . origin \n            if ret_origin : \n                break \n    return policyname , ret_origin "}
{"11629": "\ndef send ( msg_type , send_async = 0 , * args , ** kwargs ) : \n    message = message_factory ( msg_type , * args , ** kwargs ) \n    try : \n        if send_async : \n            message . send_async ( ) \n        else : \n            message . send ( ) \n    except MessageSendError as e : \n        err_exit ( \"Unable to send message: \" , e ) "}
{"11637": "\ndef _send_coroutine ( ) : \n    with PoolExecutor ( ) as executor : \n        while 1 : \n            msg = yield \n            future = executor . submit ( msg . send ) \n            future . add_done_callback ( _exception_handler ) "}
{"11641": "\ndef send_message ( msg_type , kwds ) : \n    if kwds [ \"file\" ] : \n        get_body_from_file ( kwds ) \n    kwargs = trim_args ( kwds ) \n    send ( msg_type , send_async = 0 , ** kwargs ) "}
{"11657": "\ndef validate ( self ) : \n    if PY3 : \n        if not isinstance ( self . vendor , text_type ) : \n            raise ValueError \n        for key , value in self : \n            if not isinstance ( key , text_type ) : \n                raise ValueError \n            if not isinstance ( value , text_type ) : \n                raise ValueError \n    if not isinstance ( self . vendor , text_type ) : \n        try : \n            self . vendor . decode ( 'utf-8' ) \n        except UnicodeDecodeError : \n            raise ValueError \n    for key , value in self . _internal : \n        try : \n            if not is_valid_key ( key ) : \n                raise ValueError \n        except : \n            raise ValueError ( \"%r is not a valid key\" % key ) \n        if not isinstance ( value , text_type ) : \n            try : \n                value . decode ( \"utf-8\" ) \n            except : \n                raise ValueError ( \"%r is not a valid value\" % value ) \n    else : \n        return 1 "}
{"11659": "\ndef write ( self , framing = 1 ) : \n    self . validate ( ) \n    def _encode ( value ) : \n        if not isinstance ( value , bytes ) : \n            return value . encode ( 'utf-8' ) \n        return value \n    f = BytesIO ( ) \n    vendor = _encode ( self . vendor ) \n    f . write ( cdata . to_uint_le ( len ( vendor ) ) ) \n    f . write ( vendor ) \n    f . write ( cdata . to_uint_le ( len ( self ) ) ) \n    for tag , value in self . _internal : \n        tag = _encode ( tag ) \n        value = _encode ( value ) \n        comment = tag + b\"=\" + value \n        f . write ( cdata . to_uint_le ( len ( comment ) ) ) \n        f . write ( comment ) \n    if framing : \n        f . write ( b\"\\x01\" ) \n    return f . getvalue ( ) "}
{"11690": "\ndef _do_autopaginating_api_call ( self , path , params , method , parser_func , next_marker_xpath , next_marker_param_name , next_type_xpath = None , parser_kwargs = None ) : \n    if not parser_kwargs : \n        parser_kwargs = { } \n    while 1 : \n        root = self . _send_request ( path , params , method ) \n        for record in parser_func ( root , connection = self , ** parser_kwargs ) : \n            yield record \n        next_marker = root . find ( next_marker_xpath ) \n        if next_marker is None : \n            break \n        params [ next_marker_param_name ] = next_marker . text \n        if next_type_xpath : \n            next_type = root . find ( next_type_xpath ) \n            params [ 'type' ] = next_type . text "}
{"11704": "\ndef create_hosted_zone_writer ( connection , name , caller_reference , comment ) : \n    if not caller_reference : \n        caller_reference = str ( uuid . uuid4 ( ) ) \n    e_root = etree . Element ( \"CreateHostedZoneRequest\" , xmlns = connection . _xml_namespace ) \n    e_name = etree . SubElement ( e_root , \"Name\" ) \n    e_name . text = name \n    e_caller_reference = etree . SubElement ( e_root , \"CallerReference\" ) \n    e_caller_reference . text = caller_reference \n    if comment : \n        e_config = etree . SubElement ( e_root , \"HostedZoneConfig\" ) \n        e_comment = etree . SubElement ( e_config , \"Comment\" ) \n        e_comment . text = comment \n    e_tree = etree . ElementTree ( element = e_root ) \n    fobj = BytesIO ( ) \n    e_tree . write ( fobj , xml_declaration = 1 , encoding = 'utf-8' , method = \"xml\" ) \n    return fobj . getvalue ( ) . decode ( 'utf-8' ) "}
{"11705": "\ndef lock ( fileobj ) : \n    try : \n        import fcntl \n    except ImportError : \n        return 0 \n    else : \n        try : \n            fcntl . lockf ( fileobj , fcntl . LOCK_EX ) \n        except IOError : \n            return 0 \n        else : \n            return 1 "}
{"11706": "\ndef insert_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    assert 0 < size \n    assert 0 <= offset \n    locked = 0 \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset \n    fobj . write ( b'\\x00' * size ) \n    fobj . flush ( ) \n    try : \n        try : \n            import mmap \n            file_map = mmap . mmap ( fobj . fileno ( ) , filesize + size ) \n            try : \n                file_map . move ( offset + size , offset , movesize ) \n            finally : \n                file_map . close ( ) \n        except ( ValueError , EnvironmentError , ImportError ) : \n            locked = lock ( fobj ) \n            fobj . truncate ( filesize ) \n            fobj . seek ( 0 , 2 ) \n            padsize = size \n            while padsize : \n                addsize = min ( BUFFER_SIZE , padsize ) \n                fobj . write ( b\"\\x00\" * addsize ) \n                padsize -= addsize \n            fobj . seek ( filesize , 0 ) \n            while movesize : \n                thismove = min ( BUFFER_SIZE , movesize ) \n                fobj . seek ( - thismove , 1 ) \n                nextpos = fobj . tell ( ) \n                data = fobj . read ( thismove ) \n                fobj . seek ( - thismove + size , 1 ) \n                fobj . write ( data ) \n                fobj . seek ( nextpos ) \n                movesize -= thismove \n            fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11707": "\ndef delete_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    locked = 0 \n    assert 0 < size \n    assert 0 <= offset \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset - size \n    assert 0 <= movesize \n    try : \n        if movesize > 0 : \n            fobj . flush ( ) \n            try : \n                import mmap \n                file_map = mmap . mmap ( fobj . fileno ( ) , filesize ) \n                try : \n                    file_map . move ( offset , offset + size , movesize ) \n                finally : \n                    file_map . close ( ) \n            except ( ValueError , EnvironmentError , ImportError ) : \n                locked = lock ( fobj ) \n                fobj . seek ( offset + size ) \n                buf = fobj . read ( BUFFER_SIZE ) \n                while buf : \n                    fobj . seek ( offset ) \n                    fobj . write ( buf ) \n                    offset += len ( buf ) \n                    fobj . seek ( offset + size ) \n                    buf = fobj . read ( BUFFER_SIZE ) \n        fobj . truncate ( filesize - size ) \n        fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11712": "\ndef is_modified ( self ) : \n    for key , val in self . _initial_vals . items ( ) : \n        if getattr ( self , key ) != val : \n            return 1 \n    return 0 "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = 0 ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and header > 4 : \n            available += header - 4 \n            header = 4 \n        if len ( data ) > available : \n            padding = self . metadata_blocks [ - 1 ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if newlength > 0 : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        elif len ( data ) < available : \n            self . metadata_blocks [ - 1 ] . length += ( available - len ( data ) ) \n            data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n            assert len ( data ) == available \n        if len ( data ) != available : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128 , 2 ) \n            except IOError : \n                pass \n            else : \n                if f . read ( 3 ) == b\"TAG\" : \n                    f . seek ( - 128 , 2 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11741": "\ndef delete ( self , force = 0 ) : \n    self . _halt_if_already_deleted ( ) \n    if force : \n        cset = ChangeSet ( connection = self . connection , hosted_zone_id = self . id ) \n        for rrset in self . record_sets : \n            if rrset . rrset_type not in [ 'SOA' , 'NS' ] : \n                cset . add_change ( 'DELETE' , rrset ) \n        if cset . deletions or cset . creations : \n            self . connection . _change_resource_record_sets ( cset ) \n    retval = self . connection . delete_hosted_zone_by_id ( self . id ) \n    self . _is_deleted = 1 \n    return retval "}
{"11755": "\ndef change_resource_record_set_writer ( connection , change_set , comment = None ) : \n    e_root = etree . Element ( \"ChangeResourceRecordSetsRequest\" , xmlns = connection . _xml_namespace ) \n    e_change_batch = etree . SubElement ( e_root , \"ChangeBatch\" ) \n    if comment : \n        e_comment = etree . SubElement ( e_change_batch , \"Comment\" ) \n        e_comment . text = comment \n    e_changes = etree . SubElement ( e_change_batch , \"Changes\" ) \n    for change in change_set . deletions + change_set . creations : \n        e_changes . append ( write_change ( change ) ) \n    e_tree = etree . ElementTree ( element = e_root ) \n    fobj = BytesIO ( ) \n    e_tree . write ( fobj , xml_declaration = 1 , encoding = 'utf-8' , method = \"xml\" ) \n    return fobj . getvalue ( ) . decode ( 'utf-8' ) "}
{"11765": "\ndef is_current ( self ) : \n    if not self . is_internal : \n        return 0 \n    has_same_endpoint = ( request . endpoint == self . endpoint ) \n    has_same_args = ( request . view_args == self . args ) \n    return has_same_endpoint and has_same_args "}
{"11771": "\ndef handle ( self , * args , ** kwargs ) : \n    frequency = kwargs [ 'frequency' ] \n    frequencies = settings . STATISTIC_FREQUENCY_ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) \n    if kwargs [ 'list' ] : \n        maintenance . list_statistics ( ) \n    elif kwargs [ 'calculate' ] : \n        maintenance . calculate_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'calculate' ] ) , frequencies ) \n    elif kwargs [ 'reset' ] : \n        maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset_cumulative' ] ) \n    elif kwargs [ 'recalculate' ] : \n        maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset_cumulative' ] , 1 ) "}
{"11773": "\ndef get_GET_bool ( request , var_name , default = 1 ) : \n    val = request . GET . get ( var_name , default ) \n    if isinstance ( val , str ) or isinstance ( val , unicode ) : \n        val = 1 if val [ 0 ] == 't' else 0 \n    return val "}
{"11775": "\ndef get_gecko_params ( request , uid = None , days_back = 0 , cumulative = 1 , frequency = settings . STATISTIC_FREQUENCY_DAILY , min_val = 0 , max_val = 100 , chart_type = 'standard' , percentage = 'show' , sort = 0 ) : \n    return { 'days_back' : int ( request . GET . get ( 'daysback' , days_back ) ) , 'uid' : request . GET . get ( 'uid' , uid ) , 'uids' : get_GET_array ( request , 'uids[]' ) , 'cumulative' : get_GET_bool ( request , 'cumulative' , cumulative ) , 'frequency' : request . GET . get ( 'frequency' , frequency ) , 'min' : request . GET . get ( 'min' , min_val ) , 'max' : request . GET . get ( 'max' , max_val ) , 'type' : request . GET . get ( 'type' , chart_type ) , 'percentage' : request . GET . get ( 'percentage' , percentage ) , 'sort' : get_GET_bool ( request , 'sort' , sort ) , } "}
{"11779": "\ndef geckoboard_geckometer ( request ) : \n    params = get_gecko_params ( request , cumulative = 1 ) \n    metric = Metric . objects . get ( uid = params [ 'uid' ] ) \n    return ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] ) "}
{"11780": "\ndef geckoboard_funnel ( request , frequency = settings . STATISTIC_FREQUENCY_DAILY ) : \n    params = get_gecko_params ( request , cumulative = 1 ) \n    metrics = Metric . objects . filter ( uid__in = params [ 'uids' ] ) \n    items = [ ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] \n    return { 'items' : items , 'type' : params [ 'type' ] , 'percentage' : params [ 'percentage' ] , 'sort' : params [ 'sort' ] , } "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    lnum = 1 \n    try : \n        while 1 : \n            if maxlines : \n                if lnum > maxlines : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    inum = 1 \n    try : \n        while 1 : \n            if maxitems : \n                if inum > maxitems : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11788": "\ndef outfile_input ( extension = None ) : \n    fileok = 0 \n    while not fileok : \n        filename = string_input ( 'File name? ' ) \n        if extension : \n            if not filename . endswith ( extension ) : \n                if extension . startswith ( '.' ) : \n                    filename = filename + extension \n                else : \n                    filename = filename + '.' + extension \n        if os . path . isfile ( filename ) : \n            choice = choice_input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) \n            if choice == 'y' : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'a' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = 1 \n                except IOError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except PermissionError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except FileNotFoundError : \n                    print ( filename + ': directory not found. Try again.' ) \n        else : \n            choice = choice_input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) \n            if choice == 'y' : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'w' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = 1 \n                except IOError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except PermissionError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except FileNotFoundError : \n                    print ( filename + ': directory not found. Try again.' ) \n    return filename "}
{"11797": "\ndef schedule ( self , kind = 'R' ) : \n    kind = kind . upper ( ) [ 0 ] \n    dfs = [ ] \n    for month in ( 'october' , 'november' , 'december' , 'january' , 'february' , 'march' , 'april' , 'may' , 'june' ) : \n        try : \n            doc = self . get_sub_doc ( 'games-{}' . format ( month ) ) \n        except ValueError : \n            continue \n        table = doc ( 'table#schedule' ) \n        df = sportsref . utils . parse_table ( table ) \n        dfs . append ( df ) \n    df = pd . concat ( dfs ) . reset_index ( drop = 1 ) \n    try : \n        sportsref . utils . get_html ( '{}/playoffs/NBA_{}.html' . format ( sportsref . nba . BASE_URL , self . yr ) ) \n        is_past_season = 1 \n    except ValueError : \n        is_past_season = 0 \n    if is_past_season : \n        team_per_game = self . team_stats_per_game ( ) \n        n_reg_games = int ( team_per_game . g . sum ( ) // 2 ) \n    else : \n        n_reg_games = len ( df ) \n    if kind == 'P' : \n        return df . iloc [ n_reg_games : ] \n    else : \n        return df . iloc [ : n_reg_games ] "}
{"11798": "\ndef standings ( self ) : \n    doc = self . get_sub_doc ( 'standings' ) \n    east_table = doc ( 'table#divs_standings_E' ) \n    east_df = pd . DataFrame ( sportsref . utils . parse_table ( east_table ) ) \n    east_df . sort_values ( 'wins' , ascending = 0 , inplace = 1 ) \n    east_df [ 'seed' ] = range ( 1 , len ( east_df ) + 1 ) \n    east_df [ 'conference' ] = 'E' \n    west_table = doc ( 'table#divs_standings_W' ) \n    west_df = sportsref . utils . parse_table ( west_table ) \n    west_df . sort_values ( 'wins' , ascending = 0 , inplace = 1 ) \n    west_df [ 'seed' ] = range ( 1 , len ( west_df ) + 1 ) \n    west_df [ 'conference' ] = 'W' \n    full_df = pd . concat ( [ east_df , west_df ] , axis = 0 ) . reset_index ( drop = 1 ) \n    full_df [ 'team_id' ] = full_df . team_id . str . extract ( r'(\\w+)\\W*\\(\\d+\\)' , expand = 0 ) \n    full_df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else 0 for gb in full_df [ 'gb' ] ] \n    full_df = full_df . drop ( 'has_class_full_table' , axis = 1 ) \n    expanded_table = doc ( 'table#expanded_standings' ) \n    expanded_df = sportsref . utils . parse_table ( expanded_table ) \n    full_df = pd . merge ( full_df , expanded_df , on = 'team_id' ) \n    return full_df "}
{"11799": "\ndef _get_team_stats_table ( self , selector ) : \n    doc = self . get_main_doc ( ) \n    table = doc ( selector ) \n    df = sportsref . utils . parse_table ( table ) \n    df . set_index ( 'team_id' , inplace = 1 ) \n    return df "}
{"11803": "\ndef _get_player_stats ( self , table_id_fmt ) : \n    doc = self . get_main_doc ( ) \n    tms = self . away ( ) , self . home ( ) \n    tm_ids = [ table_id_fmt . format ( tm ) for tm in tms ] \n    tables = [ doc ( 'table#{}' . format ( tm_id ) . lower ( ) ) for tm_id in tm_ids ] \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables ] \n    for i , ( tm , df ) in enumerate ( zip ( tms , dfs ) ) : \n        no_time = df [ 'mp' ] == 0 \n        stat_cols = [ col for col , dtype in df . dtypes . items ( ) if dtype != 'object' ] \n        df . loc [ no_time , stat_cols ] = 0 \n        df [ 'team_id' ] = tm \n        df [ 'is_home' ] = i == 1 \n        df [ 'is_starter' ] = [ p < 5 for p in range ( df . shape [ 0 ] ) ] \n        df . drop_duplicates ( subset = 'player_id' , keep = 'first' , inplace = 1 ) \n    return pd . concat ( dfs ) "}
{"11805": "\ndef cache ( func ) : \n    CACHE_DIR = appdirs . user_cache_dir ( 'sportsref' , getpass . getuser ( ) ) \n    if not os . path . isdir ( CACHE_DIR ) : \n        os . makedirs ( CACHE_DIR ) \n    \n    @ funcutils . wraps ( func ) \n    def wrapper ( url ) : \n        file_hash = hashlib . md5 ( ) \n        encoded_url = url . encode ( errors = 'replace' ) \n        file_hash . update ( encoded_url ) \n        file_hash = file_hash . hexdigest ( ) \n        filename = '{}/{}' . format ( CACHE_DIR , file_hash ) \n        sport_id = None \n        for a_base_url , a_sport_id in sportsref . SITE_ABBREV . items ( ) : \n            if url . startswith ( a_base_url ) : \n                sport_id = a_sport_id \n                break \n        else : \n            print ( 'No sport ID found for {}, not able to check cache' . format ( url ) ) \n        file_exists = os . path . isfile ( filename ) \n        if sport_id and file_exists : \n            cur_time = int ( time . time ( ) ) \n            mod_time = int ( os . path . getmtime ( filename ) ) \n            days_since_mod = datetime . timedelta ( seconds = ( cur_time - mod_time ) ) . days \n            days_cache_valid = globals ( ) [ '_days_valid_{}' . format ( sport_id ) ] ( url ) \n            cache_is_valid = days_since_mod < days_cache_valid \n        else : \n            cache_is_valid = 0 \n        allow_caching = sportsref . get_option ( 'cache' ) \n        if file_exists and cache_is_valid and allow_caching : \n            with codecs . open ( filename , 'r' , encoding = 'utf-8' , errors = 'replace' ) as f : \n                text = f . read ( ) \n        else : \n            text = func ( url ) \n            with codecs . open ( filename , 'w+' , encoding = 'utf-8' ) as f : \n                f . write ( text ) \n        return text \n    return wrapper "}
{"11809": "\ndef _get_stats_table ( self , table_id , kind = 'R' , summary = 0 ) : \n    doc = self . get_main_doc ( ) \n    table_id = 'table#{}{}' . format ( 'playoffs_' if kind == 'P' else '' , table_id ) \n    table = doc ( table_id ) \n    df = sportsref . utils . parse_table ( table , flatten = ( not summary ) , footer = summary ) \n    return df "}
{"11810": "\ndef stats_per_game ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'per_game' , kind = kind , summary = summary ) "}
{"11811": "\ndef stats_totals ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'totals' , kind = kind , summary = summary ) "}
{"11812": "\ndef stats_per36 ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'per_minute' , kind = kind , summary = summary ) "}
{"11813": "\ndef stats_per100 ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'per_poss' , kind = kind , summary = summary ) "}
{"11814": "\ndef stats_advanced ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'advanced' , kind = kind , summary = summary ) "}
{"11815": "\ndef stats_shooting ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'shooting' , kind = kind , summary = summary ) "}
{"11816": "\ndef stats_pbp ( self , kind = 'R' , summary = 0 ) : \n    return self . _get_stats_table ( 'advanced_pbp' , kind = kind , summary = summary ) "}
{"11819": "\ndef expand_details ( df , detailCol = 'detail' ) : \n    df = copy . deepcopy ( df ) \n    df [ 'detail' ] = df [ detailCol ] \n    dicts = [ sportsref . nfl . pbp . parse_play_details ( detail ) for detail in df [ 'detail' ] . values ] \n    cols = { c for d in dicts if d for c in d . keys ( ) } \n    blankEntry = { c : np . nan for c in cols } \n    newDicts = [ d if d else blankEntry for d in dicts ] \n    details = pd . DataFrame ( newDicts ) \n    df = pd . merge ( df , details , left_index = 1 , right_index = 1 ) \n    errors = [ i for i , d in enumerate ( dicts ) if d is None ] \n    df [ 'isError' ] = 0 \n    df . loc [ errors , 'isError' ] = 1 \n    df . loc [ 0 , 'qtr_time_remain' ] = '15:00' \n    df . qtr_time_remain . fillna ( method = 'bfill' , inplace = 1 ) \n    df . qtr_time_remain . fillna ( pd . Series ( np . where ( df . quarter == 4 , '0:00' , '15:00' ) ) , inplace = 1 ) \n    new_df = df . apply ( _clean_features , axis = 1 ) \n    return new_df "}
{"11820": "\ndef _add_team_columns ( features ) : \n    features = features . to_dict ( 'records' ) \n    curTm = curOpp = None \n    playAfterKickoff = 0 \n    for row in features : \n        if row [ 'isKickoff' ] or playAfterKickoff : \n            curTm , curOpp = _team_and_opp ( row ) \n        else : \n            curTm , curOpp = _team_and_opp ( row , curTm , curOpp ) \n        row [ 'team' ] , row [ 'opp' ] = curTm , curOpp \n        playAfterKickoff = row [ 'isKickoff' ] \n    features = pd . DataFrame ( features ) \n    features . team . fillna ( method = 'bfill' , inplace = 1 ) \n    features . opp . fillna ( method = 'bfill' , inplace = 1 ) \n    features . team . fillna ( method = 'ffill' , inplace = 1 ) \n    features . opp . fillna ( method = 'ffill' , inplace = 1 ) \n    return features "}
{"11828": "\ndef head_coaches_by_game ( self , year ) : \n    coach_str = self . _year_info_pq ( year , 'Coach' ) . text ( ) \n    regex = r'(\\S+?) \\((\\d+)-(\\d+)-(\\d+)\\)' \n    coachAndTenure = [ ] \n    m = 1 \n    while m : \n        m = re . search ( regex , coach_str ) \n        coachID , wins , losses , ties = m . groups ( ) \n        nextIndex = m . end ( 4 ) + 1 \n        coachStr = coachStr [ nextIndex : ] \n        tenure = int ( wins ) + int ( losses ) + int ( ties ) \n        coachAndTenure . append ( ( coachID , tenure ) ) \n    coachIDs = [ cID for cID , games in coachAndTenure for _ in range ( games ) ] \n    return np . array ( coachIDs [ : : - 1 ] ) "}
{"11835": "\ndef off_splits ( self , year ) : \n    doc = self . get_year_doc ( '{}_splits' . format ( year ) ) \n    tables = doc ( 'table.stats_table' ) \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables . items ( ) ] \n    dfs = [ df . assign ( split = df . columns [ 0 ] ) . rename ( columns = { df . columns [ 0 ] : 'split_value' } ) for df in dfs ] \n    if not dfs : \n        return pd . DataFrame ( ) \n    return pd . concat ( dfs ) . reset_index ( drop = 1 ) "}
{"11837": "\ndef flatten_links ( td , _recurse = 0 ) : \n    def _flatten_node ( c ) : \n        if isinstance ( c , basestring ) : \n            return c . strip ( ) \n        elif 'href' in c . attrib : \n            c_id = rel_url_to_id ( c . attrib [ 'href' ] ) \n            return c_id if c_id else c . text_content ( ) . strip ( ) \n        else : \n            return flatten_links ( pq ( c ) , _recurse = 1 ) \n    if td is None or not td . text ( ) : \n        return '' if _recurse else None \n    td . remove ( 'span.note' ) \n    return '' . join ( _flatten_node ( c ) for c in td . contents ( ) ) "}
{"11844": "\ndef __get_batch ( self , path , length , last = 0 ) : \n    import tables \n    h5_file = tables . open_file ( self . filename , 'r' ) \n    h5_node = h5_file . get_node ( path ) \n    if len ( h5_node ) == 0 : \n        raise Exception ( \"Cannot read from empty dataset.\" ) \n    if length is None : \n        chunkshape = h5_node . chunkshape \n        if chunkshape is None : \n            default_length = 128 * 2 ** 10 // h5_node [ 0 ] . nbytes \n            length = min ( h5_node . shape [ 0 ] , default_length ) \n        else : \n            length = chunkshape [ 0 ] \n    if last : \n        example = h5_node [ length * ( len ( h5_node ) // length ) : ] . copy ( ) \n    else : \n        example = h5_node [ : length ] . copy ( ) \n    h5_file . close ( ) \n    return example "}
{"11845": "\ndef get_remainder ( self , path , block_size ) : \n    return self . __get_batch ( path , length = block_size , last = 1 ) "}
{"11846": "\ndef get_queue ( self , path , n_procs = 4 , read_ahead = None , cyclic = 0 , block_size = None , ordered = 0 ) : \n    example = self . __get_batch ( path , block_size ) \n    block_size = example . shape [ 0 ] \n    if read_ahead is None : \n        read_ahead = 2 * n_procs + 1 \n    cbuf = SharedCircBuf ( read_ahead , example ) \n    stop = multiprocessing . Event ( ) \n    barrier = Barrier ( n_procs ) \n    sync = GuardSynchronizer ( ) if ordered else None \n    procs = [ ] \n    for i in range ( n_procs ) : \n        process = multiprocessing . Process ( target = _Streamer__read_process , args = ( self , path , block_size , cbuf , stop , barrier , cyclic , i * block_size , n_procs * block_size , sync ) ) \n        process . daemon = 1 \n        process . start ( ) \n        procs . append ( process ) \n    if not cyclic : \n        def monitor ( ) : \n            for p in procs : \n                p . join ( ) \n            cbuf . close ( ) \n        monitor_thread = threading . Thread ( target = monitor ) \n        monitor_thread . daemon = 1 \n        monitor_thread . start ( ) \n    return Streamer . Queue ( cbuf , stop , block_size ) "}
{"11851": "\ndef _get_objs ( self ) : \n    while 1 : \n        count = self . _read_varint ( ) \n        if count == 0 : \n            break \n        for _ in range ( count ) : \n            size = self . _read_varint ( ) \n            if size == 0 : \n                raise EOFError ( 'unexpected EOF.' ) \n            yield self . _fd . read ( size ) \n        if self . _group_delim : \n            yield self . _delimiter ( ) if self . _delimiter is not None else None "}
{"11854": "\ndef flush ( self ) : \n    if not self . is_output ( ) : \n        return \n    count = len ( self . _write_buff ) \n    if count == 0 : \n        return \n    encodeVarint ( self . _fd . write , count , 1 ) \n    for obj in self . _write_buff : \n        obj_str = obj . SerializeToString ( ) \n        encodeVarint ( self . _fd . write , len ( obj_str ) , 1 ) \n        self . _fd . write ( obj_str ) \n    self . _write_buff = [ ] "}
{"11855": "\ndef get_game_dir ( self , username = 0 ) : \n    if not self . common and not username : \n        raise RuntimeError ( \"Can't determine this game's directory without username\" ) \n    if self . common : \n        subdir = \"common\" \n    else : \n        subdir = \"username\" \n    subsubdir = self . dir \n    if WIN32 or CYGWIN : \n        subsubdir = subsubdir . lower ( ) \n    return os . path . join ( subdir , subsubdir ) "}
{"11859": "\ndef until_condition ( self , condition , condition_description ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    while 1 : \n        try : \n            if not hasattr ( condition , '__call__' ) : \n                raise TypeError ( \"condition is not callable\" ) \n            value = condition ( ) \n            if type ( value ) is bool and value is not 0 : \n                return value \n            elif type ( value ) is not bool and value is not None : \n                return value \n            else : \n                logger . debug ( \"#\" + str ( count ) + \" - wait until \" + condition_description ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0} : {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n        time . sleep ( self . _poll ) \n        count += 1 \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"condition <\" + condition_description + \"> was not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11860": "\ndef until_traits_are_present ( self , element_with_traits ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    missing_traits_descriptions = None \n    while 1 : \n        missing_traits_descriptions = [ ] \n        try : \n            missing_traits_descriptions = element_with_traits . evaluate_traits ( ) \n            if len ( missing_traits_descriptions ) == 0 : \n                return 1 \n            else : \n                logger . debug ( \"#{0} - wait until all traits are present: <{1}>\" . format ( str ( count ) , '> <' . join ( missing_traits_descriptions ) ) ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0}: {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n            pass \n        time . sleep ( self . _poll ) \n        count += 1 \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"conditions \" + '<' + '> <' . join ( missing_traits_descriptions ) + '>' + \" not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11864": "\ndef _send ( self , message , read_reply = 0 ) : \n    sock = None \n    for tries in range ( 0 , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"11865": "\ndef status ( self ) : \n    nad_reply = self . _send ( self . POLL_VOLUME + self . POLL_POWER + self . POLL_MUTED + self . POLL_SOURCE , read_reply = 1 ) \n    if nad_reply is None : \n        return \n    num_chars = 10 \n    nad_status = [ nad_reply [ i : i + num_chars ] for i in range ( 0 , len ( nad_reply ) , num_chars ) ] \n    return { 'volume' : int ( nad_status [ 0 ] [ - 2 : ] , 16 ) , 'power' : nad_status [ 1 ] [ - 2 : ] == '01' , 'muted' : nad_status [ 2 ] [ - 2 : ] == '01' , 'source' : self . SOURCES_REVERSED [ nad_status [ 3 ] [ - 2 : ] ] } "}
{"11867": "\ndef power_on ( self ) : \n    status = self . status ( ) \n    if not status [ 'power' ] : \n        self . _send ( self . CMD_ON , read_reply = 1 ) \n        sleep ( 0.5 ) "}
{"11869": "\ndef select_source ( self , source ) : \n    status = self . status ( ) \n    if status [ 'power' ] : \n        if status [ 'source' ] != source : \n            if source in self . SOURCES : \n                self . _send ( self . CMD_SOURCE + self . SOURCES [ source ] , read_reply = 1 ) "}
{"11870": "\ndef deobfuscate ( request , key , juice = None ) : \n    try : \n        url = decrypt ( str ( key ) , settings . UNFRIENDLY_SECRET , settings . UNFRIENDLY_IV , checksum = settings . UNFRIENDLY_ENFORCE_CHECKSUM ) \n    except ( CheckSumError , InvalidKeyError ) : \n        return HttpResponseNotFound ( ) \n    try : \n        url = url . decode ( 'utf-8' ) \n    except UnicodeDecodeError : \n        return HttpResponseNotFound ( ) \n    url_parts = urlparse ( unquote ( url ) ) \n    path = url_parts . path \n    query = url_parts . query \n    try : \n        view , args , kwargs = resolve ( path ) \n    except Resolver404 : \n        return HttpResponseNotFound ( ) \n    environ = request . environ . copy ( ) \n    environ [ 'PATH_INFO' ] = path [ len ( environ [ 'SCRIPT_NAME' ] ) : ] \n    environ [ 'QUERY_STRING' ] = query \n    patched_request = request . __class__ ( environ ) \n    missing_items = set ( dir ( request ) ) - set ( dir ( patched_request ) ) \n    while missing_items : \n        missing_item = missing_items . pop ( ) \n        patched_request . __setattr__ ( missing_item , request . __getattribute__ ( missing_item ) ) \n    patched_request . META [ 'obfuscated' ] = 1 \n    response = view ( patched_request , * args , ** kwargs ) \n    if juice and not response . has_header ( 'Content-Disposition' ) : \n        response [ 'Content-Disposition' ] = 'inline; filename=%s' % juice \n    return response "}
{"11875": "\ndef check_if_song_name ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    a_list = soup . findAll ( 'a' , 'touch' ) \n    text = [ str ( x ) for x in a_list ] \n    text = '' . join ( text ) \n    text = text . lower ( ) \n    string1 = 'download in 48 kbps' \n    string2 = 'download in 128 kbps' \n    string3 = 'download in 320 kbps' \n    href = '' \n    if string3 in text : \n        href = a_list [ 2 ] . get ( 'href' ) \n    elif string2 in text : \n        href = a_list [ 1 ] . get ( 'href' ) \n    elif string1 in text : \n        href = a_list [ 0 ] . get ( 'href' ) \n    else : \n        return ( 1 , 'nothing' ) \n    return ( 0 , href ) "}
{"11896": "\ndef import_qtcore ( ) : \n    has_ida = 0 \n    try : \n        import idaapi \n        has_ida = 1 \n    except ImportError : \n        has_ida = 0 \n    if has_ida : \n        old_path = sys . path [ : ] \n        try : \n            ida_python_path = os . path . dirname ( idaapi . __file__ ) \n            sys . path . insert ( 0 , ida_python_path ) \n            if idaapi . IDA_SDK_VERSION >= 690 : \n                from PyQt5 import QtCore \n                return QtCore \n            else : \n                from PySide import QtCore \n                return QtCore \n        finally : \n            sys . path = old_path \n    else : \n        try : \n            from PyQt5 import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        try : \n            from PySide import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        raise ImportError ( \"No module named PySide or PyQt\" ) "}
{"11905": "\ndef table ( name , auth = None , eager = 1 ) : \n    auth = auth or [ ] \n    dynamodb = boto . connect_dynamodb ( * auth ) \n    table = dynamodb . get_table ( name ) \n    return Table ( table = table , eager = eager ) "}
{"11906": "\ndef tables ( auth = None , eager = 1 ) : \n    auth = auth or [ ] \n    dynamodb = boto . connect_dynamodb ( * auth ) \n    return [ table ( t , auth , eager = eager ) for t in dynamodb . list_tables ( ) ] "}
{"11919": "\ndef __fetch_items ( self , path , page = 1 ) : \n    fetch_data = 1 \n    parsed_crates = 0 \n    total_crates = 0 \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            payload = { 'sort' : 'alphabetical' , 'page' : page } \n            raw_content = self . fetch ( path , payload = payload ) \n            content = json . loads ( raw_content ) \n            parsed_crates += len ( content [ 'crates' ] ) \n            if not total_crates : \n                total_crates = content [ 'meta' ] [ 'total' ] \n        except requests . exceptions . HTTPError as e : \n            logger . error ( \"HTTP exception raised - %s\" , e . response . text ) \n            raise e \n        yield raw_content \n        page += 1 \n        if parsed_crates >= total_crates : \n            fetch_data = 0 "}
{"11921": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Looking for questions at url '%s' using offset %s\" , self . url , str ( offset ) ) \n    nquestions = 0 \n    tquestions = 0 \n    equestions = 0 \n    page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    page_offset = page * KitsuneClient . ITEMS_PER_PAGE \n    drop_questions = offset - page_offset \n    current_offset = offset \n    questions_page = self . client . get_questions ( offset ) \n    while 1 : \n        try : \n            raw_questions = next ( questions_page ) \n        except StopIteration : \n            break \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code == 500 : \n                logger . exception ( e ) \n                logger . error ( \"Problem getting Kitsune questions. \" \"Loosing %i questions. Going to the next page.\" , KitsuneClient . ITEMS_PER_PAGE ) \n                equestions += KitsuneClient . ITEMS_PER_PAGE \n                current_offset += KitsuneClient . ITEMS_PER_PAGE \n                questions_page = self . client . get_questions ( current_offset ) \n                continue \n            else : \n                raise e \n        try : \n            questions_data = json . loads ( raw_questions ) \n            tquestions = questions_data [ 'count' ] \n            questions = questions_data [ 'results' ] \n        except ( ValueError , KeyError ) as ex : \n            logger . error ( ex ) \n            cause = ( \"Bad JSON format for mozilla_questions: %s\" % ( raw_questions ) ) \n            raise ParseError ( cause = cause ) \n        for question in questions : \n            if drop_questions > 0 : \n                drop_questions -= 1 \n                continue \n            question [ 'offset' ] = current_offset \n            current_offset += 1 \n            question [ 'answers_data' ] = [ ] \n            for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : \n                answers = json . loads ( raw_answers ) [ 'results' ] \n                question [ 'answers_data' ] += answers \n            yield question \n            nquestions += 1 \n        logger . debug ( \"Questions: %i/%i\" , nquestions + offset , tquestions ) \n    logger . info ( \"Total number of questions: %i (%i total)\" , nquestions , tquestions ) \n    logger . info ( \"Questions with errors dropped: %i\" , equestions ) "}
{"11922": "\ndef get_questions ( self , offset = None ) : \n    page = KitsuneClient . FIRST_PAGE \n    if offset : \n        page += int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    while 1 : \n        api_questions_url = urijoin ( self . base_url , '/question' ) + '/' \n        params = { \"page\" : page , \"ordering\" : \"updated\" } \n        questions = self . fetch ( api_questions_url , params ) \n        yield questions \n        questions_json = json . loads ( questions ) \n        next_uri = questions_json [ 'next' ] \n        if not next_uri : \n            break \n        page += 1 "}
{"11926": "\ndef get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : \n    more = 1 \n    next_uri = None \n    page = ReMoClient . FIRST_PAGE \n    page += int ( offset / ReMoClient . ITEMS_PER_PAGE ) \n    if category == CATEGORY_EVENT : \n        api = self . api_events_url \n    elif category == CATEGORY_ACTIVITY : \n        api = self . api_activities_url \n    elif category == CATEGORY_USER : \n        api = self . api_users_url \n    else : \n        raise ValueError ( category + ' not supported in ReMo' ) \n    while more : \n        params = { \"page\" : page , \"orderby\" : \"ASC\" } \n        logger . debug ( \"ReMo client calls APIv2: %s params: %s\" , api , str ( params ) ) \n        raw_items = self . fetch ( api , payload = params ) \n        yield raw_items \n        items_data = json . loads ( raw_items ) \n        next_uri = items_data [ 'next' ] \n        if not next_uri : \n            more = 0 \n        else : \n            parsed_uri = urllib . parse . urlparse ( next_uri ) \n            parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) \n            page = parsed_params [ 'page' ] [ 0 ] "}
{"11943": "\ndef export ( pid , record , template = None , ** kwargs ) : \n    formats = current_app . config . get ( 'RECORDS_UI_EXPORT_FORMATS' , { } ) . get ( pid . pid_type ) \n    fmt = formats . get ( request . view_args . get ( 'format' ) ) \n    if fmt is 0 : \n        abort ( 410 ) \n    elif fmt is None : \n        abort ( 404 ) \n    else : \n        serializer = obj_or_import_string ( fmt [ 'serializer' ] ) \n        data = serializer . serialize ( pid , record ) \n        if isinstance ( data , six . binary_type ) : \n            data = data . decode ( 'utf8' ) \n        return render_template ( template , pid = pid , record = record , data = data , format_title = fmt [ 'title' ] , ) "}
{"11945": "\ndef close ( self ) : \n    if self . _closed : \n        return \n    self . _socket . close ( ) \n    self . _closed = 1 "}
{"11961": "\ndef any_user ( password = None , permissions = [ ] , groups = [ ] , ** kwargs ) : \n    is_active = kwargs . pop ( 'is_active' , 1 ) \n    is_superuser = kwargs . pop ( 'is_superuser' , 0 ) \n    is_staff = kwargs . pop ( 'is_staff' , 0 ) \n    user = any_model ( User , is_active = is_active , is_superuser = is_superuser , is_staff = is_staff , ** kwargs ) \n    for group_name in groups : \n        group = Group . objects . get ( name = group_name ) \n        user . groups . add ( group ) \n    for permission_name in permissions : \n        app_label , codename = permission_name . split ( '.' ) \n        permission = Permission . objects . get ( content_type__app_label = app_label , codename = codename ) \n        user . user_permissions . add ( permission ) \n    if password : \n        user . set_password ( password ) \n    user . save ( ) \n    return user "}
{"11989": "\ndef any_field_blank ( function ) : \n    def wrapper ( field , ** kwargs ) : \n        if kwargs . get ( 'isnull' , 0 ) : \n            return None \n        if field . blank and random . random < 0.1 : \n            return None \n        return function ( field , ** kwargs ) \n    return wrapper "}
{"12005": "\ndef repository_exists ( self , workspace , repo ) : \n    if not self . exists ( workspace ) : \n        return 0 \n    workspaces = self . list ( ) \n    return repo in workspaces [ workspace ] [ \"repositories\" ] "}
{"12013": "\ndef set_console_handler ( self , debug = 0 ) : \n    console = logging . StreamHandler ( ) \n    console . setFormatter ( Formatter ( LFORMAT ) ) \n    if not debug : \n        console . setLevel ( logging . INFO ) \n    self . addHandler ( console ) "}
{"12014": "\ndef execute ( self , command , path = None ) : \n    logger = logging . getLogger ( __name__ ) \n    self . check_executable ( ) \n    logger . debug ( \"Executing command `%s` (cwd: %s)\" % ( command , path ) ) \n    process = subprocess . Popen ( command , shell = 1 , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    stdout , stderr = process . communicate ( ) \n    exit_code = process . wait ( ) \n    if stdout : \n        logger . info ( stdout . decode ( \"utf-8\" ) ) \n    if stderr : \n        if exit_code != 0 : \n            logger . error ( stderr . decode ( \"utf-8\" ) ) \n        else : \n            logger . info ( stderr . decode ( \"utf-8\" ) ) \n    return process "}
{"12018": "\ndef print_workspace ( self , name ) : \n    path_list = find_path ( name , self . config ) \n    if len ( path_list ) == 0 : \n        self . logger . error ( \"No matches for `%s`\" % name ) \n        return 0 \n    for name , path in path_list . items ( ) : \n        self . print_status ( name , path ) "}
{"12021": "\ndef _post_cutout_no_chunking_blosc ( self , token , channel , x_start , y_start , z_start , data , resolution ) : \n    data = numpy . expand_dims ( data , axis = 0 ) \n    blosc_data = blosc . pack_array ( data ) \n    url = self . url ( \"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\" . format ( token , channel , resolution , x_start , x_start + data . shape [ 3 ] , y_start , y_start + data . shape [ 2 ] , z_start , z_start + data . shape [ 1 ] ) ) \n    req = self . remote_utils . post_url ( url , data = blosc_data , headers = { 'Content-Type' : 'application/octet-stream' } ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( req . text ) \n    else : \n        return 1 "}
{"12024": "\ndef load_tiff_multipage ( tiff_filename , dtype = 'float32' ) : \n    if not os . path . isfile ( tiff_filename ) : \n        raise RuntimeError ( 'could not find file \"%s\"' % tiff_filename ) \n    data = tiff . imread ( tiff_filename ) \n    im = [ ] \n    while 1 : \n        Xi = numpy . array ( data , dtype = dtype ) \n        if Xi . ndim == 2 : \n            Xi = Xi [ numpy . newaxis , ... ] \n        im . append ( Xi ) \n        try : \n            data . seek ( data . tell ( ) + 1 ) \n        except EOFError : \n            break \n    im = numpy . concatenate ( im , axis = 0 ) \n    im = numpy . rollaxis ( im , 1 ) \n    im = numpy . rollaxis ( im , 2 ) \n    return im "}
{"12025": "\ndef write ( self ) : \n    file = open ( self . config_file , \"w+\" ) \n    file . write ( yaml . dump ( dict ( self ) , default_flow_style = 0 ) ) \n    file . close ( ) "}
{"12031": "\ndef merge_ids ( self , token , channel , ids , delete = 0 ) : \n    url = self . url ( ) + \"/merge/{}/\" . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( 'Could not merge ids {}' . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) ) \n    if delete : \n        self . delete_ramon ( token , channel , ids [ 1 : ] ) \n    return 1 "}
{"12032": "\ndef propagate ( self , token , channel ) : \n    if self . get_propagate_status ( token , channel ) != u'0' : \n        return \n    url = self . url ( 'sd/{}/{}/setPropagate/1/' . format ( token , channel ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( 'Propagate fail: {}' . format ( req . text ) ) \n    return 1 "}
{"12036": "\ndef parse ( self ) : \n    parser = self . subparser . add_parser ( \"show\" , help = \"Show workspace details\" , description = \"Show workspace details.\" ) \n    group = parser . add_mutually_exclusive_group ( required = 1 ) \n    group . add_argument ( '--all' , action = 'store_true' , help = \"All workspaces\" ) \n    group . add_argument ( 'name' , type = str , help = \"Workspace name\" , nargs = '?' ) "}
{"12041": "\ndef _guess_format_from_extension ( ext ) : \n    ext = ext . strip ( '.' ) \n    formats = [ ] \n    for fmt in FILE_FORMATS : \n        if ext in FILE_FORMATS [ fmt ] : \n            formats . append ( fmt ) \n    if formats == [ ] or len ( formats ) > 1 : \n        return 0 \n    return formats [ 0 ] "}
{"12043": "\ndef convert ( in_file , out_file , in_fmt = \"\" , out_fmt = \"\" ) : \n    in_file = os . path . expanduser ( in_file ) \n    out_file = os . path . expanduser ( out_file ) \n    if not os . path . exists ( in_file ) : \n        raise IOError ( \"Input file {0} does not exist, stopping...\" . format ( in_file ) ) \n    in_fmt = in_fmt . lower ( ) or _guess_format_from_extension ( in_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    out_fmt = out_fmt . lower ( ) or _guess_format_from_extension ( out_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    if not in_fmt or not out_fmt : \n        raise ValueError ( \"Cannot determine conversion formats.\" ) \n        return 0 \n    if in_fmt is out_fmt : \n        shutil . copyfileobj ( in_file , out_file ) \n        return out_file \n    if in_fmt == 'hdf5' : \n        from . import hdf5 \n        data = hdf5 . load ( in_file ) \n    elif in_fmt == 'tiff' : \n        from . import tiff \n        data = tiff . load ( in_file ) \n    elif in_fmt == 'png' : \n        from . import png \n        data = png . load ( in_file ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    if out_fmt == 'hdf5' : \n        from . import hdf5 \n        return hdf5 . save ( out_file , data ) \n    elif out_fmt == 'tiff' : \n        from . import tiff \n        return tiff . save ( out_file , data ) \n    elif out_fmt == 'png' : \n        from . import png \n        return png . export_png ( out_file , data ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    return _fail_pair_conversion ( in_fmt , out_fmt ) "}
{"12044": "\ndef build_graph ( self , project , site , subject , session , scan , size , email = None , invariants = Invariants . ALL , fiber_file = DEFAULT_FIBER_FILE , atlas_file = None , use_threads = 0 , callback = None ) : \n    if email is None : \n        email = self . email \n    if not set ( invariants ) <= set ( Invariants . ALL ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != 1 : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if size not in [ self . BIG , self . SMALL ] : \n        raise ValueError ( \"size must be either grute.BIG or grute.SMALL.\" ) \n    url = \"buildgraph/{}/{}/{}/{}/{}/{}/{}/{}/\" . format ( project , site , subject , session , scan , size , email , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments must not contain spaces.\" ) \n    if use_threads : \n        download_thread = threading . Thread ( target = self . _run_build_graph , args = [ url , fiber_file , atlas_file , callback ] ) \n        download_thread . start ( ) \n    else : \n        return self . _run_build_graph ( url , fiber_file , atlas_file ) \n    return "}
{"12045": "\ndef compute_invariants ( self , graph_file , input_format , invariants = Invariants . ALL , email = None , use_threads = 0 , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format, {}.\" . format ( input_format ) ) \n    if not set ( invariants ) <= set ( Invariants . ALL ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != 1 : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    url = \"graphupload/{}/{}/{}/\" . format ( email , input_format , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments cannot have spaces in them.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"File {} does not exist.\" . format ( graph_file ) ) \n    if use_threads : \n        upload_thread = threading . Thread ( target = self . _run_compute_invariants , args = [ url , graph_file , callback ] ) \n        upload_thread . start ( ) \n    else : \n        return self . _run_compute_invariants ( url , graph_file ) \n    return "}
{"12046": "\ndef convert_graph ( self , graph_file , input_format , output_formats , email = None , use_threads = 0 , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format {}.\" . format ( input_format ) ) \n    if not set ( output_formats ) <= set ( GraphFormats . _any ) : \n        raise ValueError ( \"Output formats must be a GraphFormats.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != 1 : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"No such file, {}!\" . format ( graph_file ) ) \n    url = \"convert/{}/{}/{}/l\" . format ( email , input_format , ',' . join ( output_formats ) ) \n    if \" \" in url : \n        raise ValueError ( \"Spaces are not permitted in arguments.\" ) \n    if use_threads : \n        convert_thread = threading . Thread ( target = self . _run_convert_graph , args = [ url , graph_file , callback ] ) \n        convert_thread . start ( ) \n    else : \n        return self . _run_convert_graph ( url , graph_file ) \n    return "}
{"12047": "\ndef to_dict ( ramons , flatten = 0 ) : \n    if type ( ramons ) is not list : \n        ramons = [ ramons ] \n    out_ramons = { } \n    for r in ramons : \n        out_ramons [ r . id ] = { \"id\" : r . id , \"type\" : _reverse_ramon_types [ type ( r ) ] , \"metadata\" : vars ( r ) } \n    return out_ramons "}
{"12051": "\ndef nd_json ( self , dataset , project , channel_list , metadata ) : \n    nd_dict = { } \n    nd_dict [ 'dataset' ] = self . dataset_dict ( * dataset ) \n    nd_dict [ 'project' ] = self . project_dict ( * project ) \n    nd_dict [ 'metadata' ] = metadata \n    nd_dict [ 'channels' ] = { } \n    for channel_name , value in channel_list . items ( ) : \n        nd_dict [ 'channels' ] [ channel_name ] = self . channel_dict ( * value ) \n    return json . dumps ( nd_dict , sort_keys = 1 , indent = 4 ) "}
{"12057": "\ndef find_path ( name , config , wsonly = 0 ) : \n    workspace = Workspace ( config ) \n    config = config [ \"workspaces\" ] \n    path_list = { } \n    if name . find ( '/' ) != - 1 : \n        wsonly = 0 \n        try : \n            ws , repo = name . split ( '/' ) \n        except ValueError : \n            raise ValueError ( \"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\" ) \n        if ( workspace . exists ( ws ) ) : \n            if ( repo in config [ ws ] [ \"repositories\" ] ) : \n                path_name = \"%s/%s\" % ( ws , repo ) \n                path_list [ path_name ] = config [ ws ] [ \"repositories\" ] [ repo ] \n    for ws_name , ws in sorted ( config . items ( ) ) : \n        if ( name == ws_name ) : \n            if wsonly is 1 : \n                return { ws_name : ws [ \"path\" ] } \n            repositories = sorted ( config [ ws_name ] [ \"repositories\" ] . items ( ) ) \n            for name , path in repositories : \n                path_list [ \"%s/%s\" % ( ws_name , name ) ] = path \n            break \n        for repo_name , repo_path in sorted ( ws [ \"repositories\" ] . items ( ) ) : \n            if ( repo_name == name ) : \n                path_list [ \"%s/%s\" % ( ws_name , repo_name ) ] = repo_path \n    return path_list "}
{"12060": "\ndef set_metadata ( self , token , data ) : \n    req = requests . post ( self . meta_url ( \"metadata/ocp/set/\" + token ) , json = data , verify = 0 ) \n    if req . status_code != 200 : \n        raise RemoteDataUploadError ( \"Could not upload metadata: \" + req . json ( ) [ 'message' ] ) \n    return req . json ( ) "}
{"12061": "\ndef get_url ( self , url ) : \n    try : \n        req = requests . get ( url , headers = { 'Authorization' : 'Token {}' . format ( self . _user_token ) } , verify = 0 ) \n        if req . status_code is 403 : \n            raise ValueError ( \"Access Denied\" ) \n        else : \n            return req \n    except requests . exceptions . ConnectionError as e : \n        if str ( e ) == '403 Client Error: Forbidden' : \n            raise ValueError ( 'Access Denied' ) \n        else : \n            raise e "}
{"12062": "\ndef post_url ( self , url , token = '' , json = None , data = None , headers = None ) : \n    if ( token == '' ) : \n        token = self . _user_token \n    if headers : \n        headers . update ( { 'Authorization' : 'Token {}' . format ( token ) } ) \n    else : \n        headers = { 'Authorization' : 'Token {}' . format ( token ) } \n    if json : \n        return requests . post ( url , headers = headers , json = json , verify = 0 ) \n    if data : \n        return requests . post ( url , headers = headers , data = data , verify = 0 ) \n    return requests . post ( url , headers = headers , verify = 0 ) "}
{"12063": "\ndef delete_url ( self , url , token = '' ) : \n    if ( token == '' ) : \n        token = self . _user_token \n    return requests . delete ( url , headers = { 'Authorization' : 'Token {}' . format ( token ) } , verify = 0 , ) "}
{"12066": "\ndef infer_gaps_in_tree ( df_seq , tree , id_col = 'id' , sequence_col = 'sequence' ) : \n    taxa = tree . taxon_namespace \n    alignment = df_seq . phylo . to_fasta ( id_col = id_col , id_only = 1 , sequence_col = sequence_col ) \n    data = dendropy . ProteinCharacterMatrix . get ( data = alignment , schema = \"fasta\" , taxon_namespace = taxa ) \n    taxon_state_sets_map = data . taxon_state_sets_map ( gaps_as_missing = 0 ) \n    dendropy . model . parsimony . fitch_down_pass ( tree . postorder_node_iter ( ) , taxon_state_sets_map = taxon_state_sets_map ) \n    dendropy . model . parsimony . fitch_up_pass ( tree . preorder_node_iter ( ) ) \n    return tree "}
{"12085": "\ndef configure ( self ) : \n    handler = logging . FileHandler ( self . path , delay = 1 ) \n    if self . _format : \n        handler . setFormatter ( logging . Formatter ( self . _format ) ) \n    if type ( self . _formatter ) == str : \n        if self . _env and self . _env . config . logging . dict_config . formatters [ self . _formatter ] : \n            d = self . _env . config . logging . dict_config . formatters [ self . _formatter ] . to_dict ( ) \n            handler . setFormatter ( logging . Formatter ( ** d ) ) \n    elif type ( self . _formatter ) == dict : \n        handler . setFormatter ( logging . Formatter ( ** self . _formatter ) ) \n    if len ( self . _loggers ) : \n        for name in self . _loggers : \n            logging . getLogger ( name ) . addHandler ( handler ) \n    else : \n        logging . getLogger ( ) . addHandler ( handler ) "}
{"12089": "\ndef remove ( self , recursive = 1 , ignore_error = 1 ) : \n    try : \n        if recursive or self . _cleanup == 'recursive' : \n            shutil . rmtree ( self . path ) \n        else : \n            os . rmdir ( self . path ) \n    except Exception as e : \n        if not ignore_error : \n            raise e "}
{"12091": "\ndef cleanup ( self ) : \n    for k in self . _children : \n        self . _children [ k ] . cleanup ( ) \n    if self . _cleanup : \n        self . remove ( 1 ) "}
{"12103": "\ndef _resolve_path ( self , create = 0 ) : \n    if type ( self . _path ) == str : \n        key_path = self . _path . split ( '.' ) \n    else : \n        key_path = [ self . _path ] \n    node = self . _root . _data \n    nodes = [ self . _root . _data ] \n    while len ( key_path ) : \n        key = key_path . pop ( 0 ) \n        try : \n            key = int ( key ) \n        except : \n            pass \n        if create : \n            if type ( node ) == dict and key not in node : \n                node [ key ] = { } \n            elif type ( node ) == list and type ( key ) == int and len ( node ) < key : \n                node . append ( [ None for i in range ( key - len ( node ) ) ] ) \n        nodes . append ( node ) \n        try : \n            node = node [ key ] \n        except TypeError : \n            if type ( key ) == int : \n                raise IndexError ( key ) \n            else : \n                raise KeyError ( key ) \n    return ( nodes [ - 1 ] , key ) "}
{"12106": "\ndef load ( self , reload = 0 ) : \n    if reload or not self . _loaded : \n        if self . _defaults_file and type ( self . _defaults_file ) == str : \n            self . _defaults_file = File ( self . _defaults_file , parent = self . _parent ) \n        defaults = { } \n        if self . _defaults_file : \n            defaults = yaml . safe_load ( self . _defaults_file . read ( ) . replace ( '\\t' , '    ' ) ) \n        data = { } \n        if self . exists : \n            data = yaml . safe_load ( self . read ( ) . replace ( '\\t' , '    ' ) ) \n        self . _defaults = defaults \n        self . _data = copy . deepcopy ( self . _defaults ) \n        self . update ( data = data ) \n        if self . _apply_env : \n            self . update ( ConfigEnv ( self . _env_prefix ) ) \n        self . _loaded = 1 \n    return self "}
{"12108": "\ndef build_callback_url ( request , urlname , message ) : \n    location = reverse ( urlname , kwargs = { \"pk\" : message . pk } ) \n    callback_domain = getattr ( settings , \"TWILIO_CALLBACK_DOMAIN\" , None ) \n    if callback_domain : \n        url = \"{}://{}{}\" . format ( \"https\" if getattr ( settings , \"TWILIO_CALLBACK_USE_HTTPS\" , 0 ) else \"http\" , callback_domain , location ) \n    elif request is not None : \n        url = request . build_absolute_uri ( location ) \n    else : \n        raise ValueError ( \"Unable to build callback url. Configure TWILIO_CALLBACK_DOMAIN \" \"or pass request object to function call\" ) \n    return url "}
{"12112": "\ndef read_socket_input ( connection , socket_obj ) : \n    count = connection . needs_input \n    if count <= 0 : \n        return count \n    while 1 : \n        try : \n            sock_data = socket_obj . recv ( count ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if len ( sock_data ) > 0 : \n        count = connection . process_input ( sock_data ) \n    else : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_input ( ) \n        connection . close_output ( ) \n    return count "}
{"12113": "\ndef write_socket_output ( connection , socket_obj ) : \n    count = connection . has_output \n    if count <= 0 : \n        return count \n    data = connection . output_data ( ) \n    if not data : \n        return Connection . EOS \n    while 1 : \n        try : \n            count = socket_obj . send ( data ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if count > 0 : \n        connection . output_written ( count ) \n    elif data : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_output ( ) \n        connection . close_input ( ) \n    return count "}
{"12116": "\ndef configure ( self , target_address , source_address , handler , properties ) : \n    self . _handler = handler \n    self . _properties = properties \n    dynamic_props = None \n    if properties : \n        dynamic_props = properties . get ( \"dynamic-node-properties\" ) \n        mode = _dist_modes . get ( properties . get ( \"distribution-mode\" ) ) \n        if mode is not None : \n            self . _pn_link . source . distribution_mode = mode \n        mode = _snd_settle_modes . get ( properties . get ( \"snd-settle-mode\" ) ) \n        if mode is not None : \n            self . _pn_link . snd_settle_mode = mode \n        mode = _rcv_settle_modes . get ( properties . get ( \"rcv-settle-mode\" ) ) \n        if mode is not None : \n            self . _pn_link . rcv_settle_mode = mode \n    if target_address is None : \n        if not self . _pn_link . is_sender : \n            raise Exception ( \"Dynamic target not allowed\" ) \n        self . _pn_link . target . dynamic = 1 \n        if dynamic_props : \n            self . _pn_link . target . properties . clear ( ) \n            self . _pn_link . target . properties . put_dict ( dynamic_props ) \n    elif target_address : \n        self . _pn_link . target . address = target_address \n    if source_address is None : \n        if not self . _pn_link . is_receiver : \n            raise Exception ( \"Dynamic source not allowed\" ) \n        self . _pn_link . source . dynamic = 1 \n        if dynamic_props : \n            self . _pn_link . source . properties . clear ( ) \n            self . _pn_link . source . properties . put_dict ( dynamic_props ) \n    elif source_address : \n        self . _pn_link . source . address = source_address "}
{"12119": "\ndef _session_closed ( self ) : \n    if self . _endpoint_state & proton . Endpoint . REMOTE_ACTIVE : \n        self . _process_remote_state ( ) \n    elif self . _endpoint_state & proton . Endpoint . REMOTE_UNINIT : \n        self . _failed = 1 \n        self . _link_failed ( \"Parent session closed.\" ) "}
{"12128": "\ndef receiver_remote_closed ( self , receiver_link , pn_condition ) : \n    LOG . debug ( \"receiver_remote_closed condition=%s\" , pn_condition ) \n    receiver_link . close ( ) \n    self . done = 1 "}
{"12129": "\ndef receiver_failed ( self , receiver_link , error ) : \n    LOG . warn ( \"receiver_failed error=%s\" , error ) \n    receiver_link . close ( ) \n    self . done = 1 "}
{"12131": "\ndef connect_socket ( host , port , blocking = 1 ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) \n    if not blocking : \n        my_socket . setblocking ( 0 ) \n    try : \n        my_socket . connect ( addr [ 0 ] [ 4 ] ) \n    except socket . error as e : \n        if e . errno != errno . EINPROGRESS : \n            raise \n    return my_socket "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return 0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return 0 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( _PROTON_VERSION < ( 0 , 10 ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = 1 \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = 1 \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12158": "\ndef add_pie_chart ( self , data , cursor , width , height , title = None , data_type = \"raw\" , fill_colors = None , labels = 0 , background = None , legend = None ) : \n    save_draw_color = self . draw_color \n    save_fill_color = self . fill_color \n    chart = PDFPieChart ( self . session , self . page , data , cursor , width , height , title , data_type , fill_colors , labels , background , legend ) \n    self . set_draw_color ( save_draw_color ) \n    self . set_fill_color ( save_fill_color ) "}
{"12159": "\ndef _output_pages ( self ) : \n    if not self . orientation_changes : \n        self . _get_orientation_changes ( ) \n    for page in self . pages : \n        obj = self . session . _add_object ( ) \n        self . session . _out ( '<</Type /Page' ) \n        self . session . _out ( '/Parent 1 0 R' ) \n        if self . orientation_changes : \n            self . session . _out ( '/MediaBox [0 0 %.2f %.2f]' % ( page . width , page . height ) ) \n        self . session . _out ( '/Resources 2 0 R' ) \n        self . session . _out ( '/Group <</Type /Group /S /Transparency /CS /DeviceRGB>>' ) \n        self . session . _out ( '/Contents %s 0 R>>' % ( obj . id + 1 ) ) \n        self . session . _out ( 'endobj' ) \n        self . session . _add_object ( ) \n        if self . session . compression is 1 : \n            textfilter = ' /Filter /FlateDecode ' \n            page . _compress ( ) \n        else : \n            textfilter = '' \n        self . session . _out ( '<<%s/Length %s >>' % ( textfilter , len ( page . buffer ) ) ) \n        self . session . _put_stream ( page . buffer ) \n        self . session . _out ( 'endobj' ) "}
{"12160": "\ndef _get_orientation_changes ( self ) : \n    self . orientation_changes = [ ] \n    for page in self . pages : \n        if page . orientation_change is 1 : \n            self . orientation_changes . append ( page . index ) \n        else : \n            pass \n    return self . orientation_changes "}
{"12166": "\ndef _set_style ( self , style = None ) : \n    if style is None : \n        self . style = '' \n        self . underline = 0 \n    elif self . family == ( 'symbol' or 'zapfdingbats' ) : \n        self . style = '' \n        self . underline = 0 \n    self . style = style . upper ( ) \n    if 'U' in self . style or self . style == 'U' : \n        self . underline = 1 \n    else : \n        self . underline = 0 "}
{"12181": "\ndef x_fit ( self , test_length ) : \n    if ( self . x + test_length ) >= self . xmax : \n        return 0 \n    else : \n        return 1 "}
{"12182": "\ndef y_fit ( self , test_length ) : \n    if ( self . y + test_length ) >= self . ymax : \n        return 0 \n    else : \n        return 1 "}
{"12183": "\ndef x_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if self . x > test_ordinate . x : \n        return 1 \n    else : \n        return 0 "}
{"12184": "\ndef y_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if self . y > test_ordinate . y : \n        return 1 \n    else : \n        return 0 "}
{"12194": "\ndef create ( self , label_id ) : \n    data = { 'type' : 'tagit' , 'rate_count' : 0 , 'rate_range' : 'day' , 'limit_count' : 0 , 'limit_range' : 'day' , 'schedule' : [ ] , 'enabled' : 1 , 'args' : { 'sn' : label_id , 'tag_sn' : label_id } } \n    return self . _post ( request = ApiActions . CREATE . value , uri = ApiUri . ACTIONS . value , params = data ) "}
{"12200": "\ndef create ( self , alert_config , occurrence_frequency_count = None , occurrence_frequency_unit = None , alert_frequency_count = None , alert_frequency_unit = None ) : \n    data = { 'rate_count' : occurrence_frequency_count or 1 , 'rate_range' : occurrence_frequency_unit or 'hour' , 'limit_count' : alert_frequency_count or 1 , 'limit_range' : alert_frequency_unit or 'hour' , 'schedule' : [ ] , 'enabled' : 1 , } \n    data . update ( alert_config . args ( ) ) \n    return self . _post ( request = ApiActions . CREATE . value , uri = ApiUri . ACTIONS . value , params = data ) "}
{"12210": "\ndef update ( self , ** kwargs ) : \n    if kwargs . get ( 'verify_kwargs' , 1 ) : \n        valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . _fields_ ] \n        valid += [ 'b' , 'times' ] \n        for k in kwargs . keys ( ) : \n            if k not in valid : \n                raise Exception ( \"Invalid kwarg '%s'.\" % k ) \n    if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : KIPPING } ) \n    elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : NONLINEAR } ) \n    self . limbdark . update ( ** kwargs ) \n    self . transit . update ( ** kwargs ) \n    self . settings . update ( ** kwargs ) "}
{"12215": "\ndef __line_gen ( self ) : \n    while 1 : \n        line = self . __buffer . readline ( ) \n        if not line : \n            self . __recv ( ) \n            continue \n        yield line "}
{"12216": "\ndef __buf_gen ( self , length = 0 ) : \n    while 1 : \n        buf = self . __buffer . read ( length ) \n        if not buf : \n            self . __recv ( ) \n            continue \n        yield buf "}
{"12218": "\ndef info_gen ( self , code , message , compressed = 0 ) : \n    if \"COMPRESS=GZIP\" in message : \n        return self . __info_gzip_gen ( ) \n    if compressed : \n        return self . __info_yenczlib_gen ( ) \n    return self . __info_plain_gen ( ) "}
{"12219": "\ndef info ( self , code , message , compressed = 0 ) : \n    return \"\" . join ( [ x for x in self . info_gen ( code , message , compressed ) ] ) "}
{"12240": "\ndef body ( self , msgid_article = None , decode = 0 ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"BODY\" , args ) \n    if code != 222 : \n        raise NNTPReplyError ( code , message ) \n    escape = 0 \n    crc32 = 0 \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return \"\" . join ( body ) "}
{"12243": "\ndef xzhdr ( self , header , msgid_range = None ) : \n    args = header \n    if msgid_range is not None : \n        args += \" \" + utils . unparse_msgid_range ( msgid_range ) \n    code , message = self . command ( \"XZHDR\" , args ) \n    if code != 221 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message , compressed = 1 ) "}
{"12247": "\ndef xfeature_compress_gzip ( self , terminator = 0 ) : \n    args = \"TERMINATOR\" if terminator else None \n    code , message = self . command ( \"XFEATURE COMPRESS GZIP\" , args ) \n    if code != 290 : \n        raise NNTPReplyError ( code , message ) \n    return 1 "}
{"12248": "\ndef post ( self , headers = { } , body = \"\" ) : \n    code , message = self . command ( \"POST\" ) \n    if code != 340 : \n        raise NNTPReplyError ( code , message ) \n    hdrs = utils . unparse_headers ( headers ) \n    self . socket . sendall ( hdrs ) \n    if isinstance ( body , basestring ) : \n        body = cStringIO . StringIO ( body ) \n    illegal = 0 \n    for line in body : \n        if line . startswith ( \".\" ) : \n            line = \".\" + line \n        if line . endswith ( \"\\r\\n\" ) : \n            line = line [ : - 2 ] \n        elif line . endswith ( \"\\n\" ) : \n            line = line [ : - 1 ] \n        if any ( c in line for c in \"\\0\\r\" ) : \n            illegal = 1 \n            break \n        self . socket . sendall ( line + \"\\r\\n\" ) \n    self . socket . sendall ( \".\\r\\n\" ) \n    code , message = self . status ( ) \n    if illegal : \n        raise NNTPDataError ( \"Illegal characters found\" ) \n    if code != 240 : \n        raise NNTPReplyError ( code , message ) \n    message_id = message . split ( None , 1 ) [ 0 ] \n    if message_id . startswith ( \"<\" ) and message_id . endswith ( \">\" ) : \n        return message_id \n    return 1 "}
{"12258": "\ndef create ( self , name , patterns , logs , trigger_config , alert_reports ) : \n    data = { 'tag' : { 'actions' : [ alert_report . to_dict ( ) for alert_report in alert_reports ] , 'name' : name , 'patterns' : patterns , 'sources' : [ { 'id' : log } for log in logs ] , 'sub_type' : 'InactivityAlert' , 'type' : 'AlertNotify' } } \n    data [ 'tag' ] . update ( trigger_config . to_dict ( ) ) \n    return self . _api_post ( url = self . url_template . format ( account_id = self . account_id ) , data = json . dumps ( data , sort_keys = 1 ) ) "}
{"12260": "\ndef _create_scheduled_query ( self , query , change , scope_unit , scope_count ) : \n    query_data = { 'scheduled_query' : { 'name' : 'ForAnomalyReport' , 'query' : query , 'threshold_type' : '%' , 'threshold_value' : change , 'time_period' : scope_unit . title ( ) , 'time_value' : scope_count , } } \n    query_url = 'https://logentries.com/rest/{account_id}/api/scheduled_queries' \n    return self . _api_post ( url = query_url . format ( account_id = self . account_id ) , data = json . dumps ( query_data , sort_keys = 1 ) ) "}
{"12261": "\ndef create ( self , name , query , scope_count , scope_unit , increase_positive , percentage_change , trigger_config , logs , alert_reports ) : \n    change = '{pos}{change}' . format ( pos = '+' if increase_positive else '-' , change = str ( percentage_change ) ) \n    query_response = self . _create_scheduled_query ( query = query , change = change , scope_unit = scope_unit , scope_count = scope_count , ) \n    scheduled_query_id = query_response . get ( 'scheduled_query' , { } ) . get ( 'id' ) \n    tag_data = { 'tag' : { 'actions' : [ alert_report . to_dict ( ) for alert_report in alert_reports ] , 'name' : name , 'scheduled_query_id' : scheduled_query_id , 'sources' : [ { 'id' : log } for log in logs ] , 'sub_type' : 'AnomalyAlert' , 'type' : 'AlertNotify' } } \n    tag_data [ 'tag' ] . update ( trigger_config . to_dict ( ) ) \n    tag_url = 'https://logentries.com/rest/{account_id}/api/tags' . format ( account_id = self . account_id ) \n    return self . _api_post ( url = tag_url , data = json . dumps ( tag_data , sort_keys = 1 ) , ) "}
{"12302": "\ndef download ( self , bands = None , download_dir = None , metadata = 0 ) : \n    if not download_dir : \n        download_dir = DOWNLOAD_DIR \n    if bands is None : \n        bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] \n    else : \n        self . validate_bands ( bands ) \n    pattern = re . compile ( '^[^\\s]+_(.+)\\.tiff?' , re . I ) \n    band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] \n    image_list = [ ] \n    self . connect_earthexplorer ( ) \n    tgzname = self . sceneInfo . name + '.tgz' \n    dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) \n    downloaded = self . download_file ( self . url , dest_dir , tgzname ) \n    logger . debug ( 'Status downloaded %s' % downloaded ) \n    print ( '\\n Status downloaded %s' % downloaded ) \n    if downloaded [ 'sucess' ] : \n        print ( '\\n Downloaded sucess' ) \n        logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) \n        try : \n            tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) \n            folder_path = join ( download_dir , self . sceneInfo . name ) \n            tar . extractall ( folder_path ) \n            remove ( downloaded [ 'file_path' ] ) \n            images_path = listdir ( folder_path ) \n            for image_path in images_path : \n                matched = pattern . match ( image_path ) \n                file_path = join ( folder_path , image_path ) \n                if matched and matched . group ( 1 ) in band_list : \n                    image_list . append ( [ file_path , getsize ( file_path ) ] ) \n                elif matched : \n                    remove ( file_path ) \n        except tarfile . ReadError as error : \n            print ( '\\nError when extracting files. %s' % error ) \n            logger . error ( 'Error when extracting files. %s' % error ) \n        return image_list \n    else : \n        logger . debug ( 'Info downloaded: %s' % downloaded ) \n        print ( '\\n Info downloaded: %s' % downloaded ) \n        return downloaded "}
{"12307": "\ndef normalize ( self , dt , is_dst = 0 ) : \n    if dt . tzinfo is None : \n        raise ValueError ( 'Naive time - no tzinfo set' ) \n    return dt . replace ( tzinfo = self ) "}
{"12310": "\ndef point_to_source ( source , position , fmt = ( 2 , 1 , \"~~~~~\" , \"^\" ) ) : \n    surrounding_lines , show_line_numbers , tail_body , pointer_char = fmt \n    line_no , char_no = position \n    lines = source . split ( \"\\n\" ) \n    line = lines [ line_no ] \n    if char_no >= len ( tail_body ) : \n        tail = \" \" * ( char_no - len ( tail_body ) ) + tail_body + pointer_char \n    else : \n        tail = \" \" * char_no + pointer_char + tail_body \n    if show_line_numbers : \n        line_no_width = int ( math . ceil ( math . log10 ( max ( 1 , line_no + surrounding_lines ) ) ) + 1 ) \n        line_fmt = \"{0:\" + str ( line_no_width ) + \"}: {1}\" \n    else : \n        line_fmt = \"{1}\" \n    pivot = line_no + 1 \n    output_lines = [ ( pivot , line ) , ( \"\" , tail ) ] \n    for i in range ( surrounding_lines ) : \n        upper_ofst = i + 1 \n        upper_idx = line_no + upper_ofst \n        lower_ofst = - upper_ofst \n        lower_idx = line_no + lower_ofst \n        if lower_idx >= 0 : \n            output_lines . insert ( 0 , ( pivot + lower_ofst , lines [ lower_idx ] ) ) \n        if upper_idx < len ( lines ) : \n            output_lines . append ( ( pivot + upper_ofst , lines [ upper_idx ] ) ) \n    return \"\\n\" . join ( line_fmt . format ( n , c ) for n , c in output_lines ) "}
{"12313": "\ndef fromlist ( cls , files , equal = 0 , offensive = 0 , lang = None ) : \n    self = cls . __new__ ( cls ) \n    self . files = fortunes = [ ] \n    count = 0 \n    for file in files : \n        fortune = load_fortune ( file , offensive = offensive , lang = lang ) \n        if fortune is None : \n            logger . warn ( \"Can't load: %s\" , file ) \n            continue \n        count += 1 if equal else fortune . size \n        fortunes . append ( ( fortune , count ) ) \n    if not fortunes : \n        raise ValueError ( 'All fortune files specified are invalid' ) \n    self . count = count \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12314": "\ndef set_chance ( cls , files , equal = 0 , offensive = 0 , lang = None ) : \n    self = cls . __new__ ( cls ) \n    total = 0. \n    file = [ ] \n    leftover = [ ] \n    for name , chance in files : \n        if total >= 1 : \n            break \n        fortune = load_fortune ( name , offensive = offensive , lang = lang ) \n        if fortune is None or not fortune . size : \n            continue \n        if chance : \n            file . append ( ( fortune , chance ) ) \n            total += chance \n        else : \n            leftover . append ( fortune ) \n    if leftover and total < 1 : \n        left = 1 - total \n        if equal : \n            perfile = left / len ( leftover ) \n            for fortune in leftover : \n                file . append ( ( fortune , perfile ) ) \n        else : \n            entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) \n            logger . debug ( '%d entries left' , entries ) \n            for fortune in leftover : \n                chance = left * fortune . size / entries \n                file . append ( ( fortune , chance ) ) \n    self . count = count = 65536 \n    bound = 0 \n    self . files = fortunes = [ ] \n    for file , chance in file : \n        bound += int ( chance * count ) \n        fortunes . append ( ( file , bound ) ) \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12316": "\ndef rule ( self , text ) : \n    self . _attempting ( text ) \n    return concatenation ( [ self . identifier , \"=\" , self . expression , \";\" , ] , ignore_whitespace = 1 ) ( text ) . retyped ( TokenType . rule ) "}
{"12317": "\ndef special_handling ( self , text ) : \n    self . _attempting ( text ) \n    return concatenation ( [ \"?\" , self . identifier , \"?\" , ] , ignore_whitespace = 1 ) ( text ) . retyped ( TokenType . special_handling ) "}
{"12328": "\ndef _get_rule_definition ( self , rule ) : \n    fmt = \"\"\"def {rule_fxn_name}(self, text):             {indent}\\\"\\\"\\\"{rule_source}\\\"\\\"\\\"             {indent}self._attempting(text)             {indent}return {rule_definition}(text){transform}          \"\"\" \n    fmt = self . _clean_fmt ( fmt ) \n    source = self . _indent ( self . _ast_to_code ( rule . expression ) , skip_first_line = 1 ) \n    if self . use_terminal_shorthand and len ( source ) == 1 and source [ 0 ] . startswith ( ( \"'\" , '\"' ) ) : \n        source = [ \"terminal({})\" . format ( source [ 0 ] ) ] \n    rule_source = fmt . format ( rule_fxn_name = self . _get_rule_fxn_name ( rule . name ) , indent = self . indent , rule_source = self . _get_rule_source ( rule ) , rule_definition = \"\\n\" . join ( source ) , transform = self . _get_rule_transform ( rule ) ) \n    return self . _indent ( rule_source , 1 ) "}
{"12329": "\ndef _get_rule_source ( self , rule ) : \n    p = len ( self . input_source ) + rule . position \n    source = self . input_source [ p : p + rule . consumed ] . rstrip ( ) \n    return self . _indent ( source , depth = self . indent + \"   \" , skip_first_line = 1 ) "}
{"12336": "\ndef _ast_optree_node_to_code ( self , node , ** kwargs ) : \n    opnode = node . opnode \n    if opnode is None : \n        return self . _ast_to_code ( node . operands [ 0 ] ) \n    else : \n        operator = opnode . operator \n        if operator is OP_ALTERNATE : \n            return self . _ast_op_alternate_to_code ( node , ** kwargs ) \n        elif operator is OP_WS_CONCAT : \n            kwargs [ \"ignore_whitespace\" ] = 0 \n            return self . _ast_op_concat_to_code ( node , ** kwargs ) \n        elif operator is OP_CONCAT : \n            kwargs [ \"ignore_whitespace\" ] = 1 \n            return self . _ast_op_concat_to_code ( node , ** kwargs ) \n        elif operator is OP_EXCLUDE : \n            return self . _ast_op_exclude_to_code ( node , ** kwargs ) \n        elif operator is OP_MULTIPLY : \n            return self . _ast_op_multiply_to_code ( node , ** kwargs ) \n        elif operator is OP_REPEAT : \n            return self . _ast_op_repeat_to_code ( node , ** kwargs ) \n        else : \n            raise Exception ( \"Unhandled optree node: {0}\" . format ( node ) ) "}
{"12339": "\ndef _ast_repetition_group_to_code ( self , repetition_group , ignore_whitespace = 0 , ** kwargs ) : \n    lines = [ \"zero_or_more(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( repetition_group . expression ) ) ) \n    lines [ - 1 ] += \",\" \n    lines . append ( self . _indent ( \"ignore_whitespace={}\" . format ( bool ( ignore_whitespace ) ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12344": "\ndef _ast_op_multiply_to_code ( self , opr , ignore_whitespace = 0 , ** kwargs ) : \n    opl , opr = opr . operands \n    if isinstance ( opl , Number ) : \n        times = opl . value \n        subject = self . _ast_to_code ( opr ) \n    else : \n        times = opr . value \n        subject = self . _ast_to_code ( opl ) \n    lines = [ \"repeated(\" ] \n    lines . extend ( self . _indent ( subject ) ) \n    lines [ - 1 ] += \",\" \n    lines . append ( \"{0}times={1},\" . format ( self . indent , times ) ) \n    lines . append ( \"{0}ignore_whitespace={1}\" . format ( self . indent , bool ( ignore_whitespace ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12345": "\ndef _ast_op_repeat_to_code ( self , opr , ignore_whitespace = 0 , ** kwargs ) : \n    lines = [ \"one_or_more(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( opr . operands [ 0 ] ) ) ) \n    lines [ - 1 ] += \",\" \n    lines . append ( self . _indent ( \"ignore_whitespace={}\" . format ( bool ( ignore_whitespace ) ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12352": "\ndef add_arguments ( self ) : \n    self . add_logging_argument ( ) \n    self . parser . add_argument ( '-a' , '--api-host' , dest = 'api_host' , action = 'store' , metavar = \"api_host\" , help = '{0} API host endpoint' . format ( self . product_name ) ) \n    self . parser . add_argument ( '-e' , '--email' , dest = 'email' , action = 'store' , metavar = \"e_mail\" , help = 'e-mail that has access to the {0} account' . format ( self . product_name ) ) \n    self . parser . add_argument ( '-t' , '--api-token' , dest = 'api_token' , required = 0 , action = 'store' , metavar = \"api_token\" , help = 'API token for given e-mail that has access to the {0} account' . format ( self . product_name ) ) \n    self . parser . add_argument ( '-z' , '--curl' , dest = 'curl' , required = 0 , action = 'store_true' , default = 0 , help = 'Output the corresponding curl command line and exit' ) "}
{"12354": "\ndef _validate_arguments ( self ) : \n    if self . _email is None : \n        self . set_error_message ( \"E-mail for the account not provided\" ) \n        return 0 \n    if self . _api_token is None : \n        self . set_error_message ( \"API Token for the account not provided\" ) \n        return 0 \n    return 1 "}
{"12358": "\ndef add_arguments ( self ) : \n    MetricCommon . add_arguments ( self ) \n    self . parser . add_argument ( '-n' , '--metric-name' , dest = 'metricName' , action = 'store' , required = 1 , metavar = 'metric_name' , help = 'Metric identifier' ) \n    self . parser . add_argument ( '-d' , '--display-name' , dest = 'displayName' , action = 'store' , required = 1 , metavar = 'display_name' , help = 'Metric display name' ) \n    self . parser . add_argument ( '-s' , '--display-name-short' , dest = 'displayNameShort' , action = 'store' , required = 1 , metavar = 'display_short_name' , help = 'Metric short display name' ) \n    self . parser . add_argument ( '-i' , '--description' , dest = 'description' , action = 'store' , required = not self . update , metavar = 'description' , help = 'Metric description' ) \n    self . parser . add_argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = 1 , choices = [ 'avg' , 'max' , 'min' , 'sum' ] , help = 'Metric default aggregate' ) \n    self . parser . add_argument ( '-u' , '--unit' , dest = 'unit' , action = 'store' , required = 0 , choices = [ 'percent' , 'number' , 'bytecount' , 'duration' ] , help = 'Metric unit' ) \n    self . parser . add_argument ( '-r' , '--resolution' , dest = 'resolution' , action = 'store' , metavar = 'resolution' , required = 0 , help = 'Metric default resolution' ) \n    self . parser . add_argument ( '-y' , '--type' , dest = 'type' , action = 'store' , default = None , required = 0 , metavar = 'type' , help = 'Sets the type metadata field' ) \n    self . parser . add_argument ( '-x' , '--is-disabled' , dest = 'isDisabled' , action = 'store' , default = None , required = 0 , choices = [ 'true' , 'false' ] , help = 'Enable or disable the metric definition' ) "}
{"12369": "\ndef add_arguments ( self ) : \n    ApiCli . add_arguments ( self ) \n    self . parser . add_argument ( '-f' , '--format' , dest = 'format' , action = 'store' , required = 0 , choices = [ 'csv' , 'json' , 'raw' , 'xml' ] , help = 'Output format. Default is raw' ) \n    self . parser . add_argument ( '-n' , '--name' , dest = 'metric_name' , action = 'store' , required = 1 , metavar = \"metric_name\" , help = 'Metric identifier' ) \n    self . parser . add_argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = 0 , choices = [ 'sum' , 'avg' , 'max' , 'min' ] , help = 'Metric default aggregate' ) \n    self . parser . add_argument ( '-r' , '--sample' , dest = 'sample' , action = 'store' , type = int , metavar = \"sample\" , help = 'Down sample rate sample in seconds' ) \n    self . parser . add_argument ( '-s' , '--source' , dest = 'source' , action = 'store' , metavar = \"source\" , required = 1 , help = 'Source of measurement' ) \n    self . parser . add_argument ( '-b' , '--start' , dest = 'start' , action = 'store' , required = 1 , metavar = \"start\" , help = 'Start of time range as ISO 8601 string or epoch seconds' ) \n    self . parser . add_argument ( '-d' , '--end' , dest = 'end' , action = 'store' , metavar = \"end\" , required = 0 , help = 'End of time range as ISO 8601 string or epoch seconds' ) \n    self . parser . add_argument ( '-o' , '--date-format' , dest = 'date_format' , action = 'store' , metavar = \"format\" , required = 0 , help = 'For CSV, JSON, and XML output formats dates (see Python date.strftime). ' + 'Default format is %%s' ) "}
{"12373": "\ndef output_raw ( self , text ) : \n    payload = json . loads ( text ) \n    out = json . dumps ( payload , sort_keys = 1 , indent = self . _indent , separators = ( ',' , ': ' ) ) \n    print ( self . colorize_json ( out ) ) "}
{"12377": "\ndef repetition ( extractor , bounds , * , ignore_whitespace = 0 ) : \n    return partial ( _get_repetition , extractor , bounds = bounds , ignore_whitespace = ignore_whitespace ) "}
{"12379": "\ndef _get_repetition ( extractor , text , * , bounds = ( 0 , None ) , ignore_whitespace = 0 ) : \n    minr , maxr = bounds \n    children = [ ] \n    while maxr is None or len ( children ) <= maxr : \n        ignored_ws , use_text = _split_ignored ( text , ignore_whitespace ) \n        try : \n            child = _call_extractor ( extractor , use_text ) \n            child . add_ignored ( ignored_ws ) \n        except DeadEnd : \n            break \n        if child . is_empty : \n            break \n        children . append ( child ) \n        text = text [ child . consumed : ] \n    if len ( children ) >= minr : \n        return ParseNode ( ParseNodeType . repetition , children = children ) \n    else : \n        raise DeadEnd ( ) "}
{"12380": "\ndef _get_exclusion ( extractor , exclusion , text ) : \n    try : \n        _call_extractor ( exclusion , text ) \n        exclusion_matches = 1 \n    except DeadEnd : \n        exclusion_matches = 0 \n    if exclusion_matches : \n        raise DeadEnd ( ) \n    else : \n        return _call_extractor ( extractor , text ) "}
{"12386": "\ndef is_type ( self , value ) : \n    if isinstance ( value , tuple ) : \n        for opt in value : \n            if self . node_type == opt : \n                return 1 \n        return 0 \n    else : \n        return self . node_type == value "}
{"12391": "\ndef compressed ( self , new_type = None , * , include_ignored = 0 ) : \n    values = [ ] \n    consumed = 0 \n    ignored = None \n    for i , child in enumerate ( self . children ) : \n        consumed += child . consumed \n        if i == 0 and not include_ignored : \n            ignored = child . ignored \n        if child . is_value : \n            if include_ignored : \n                values . append ( \"{0}{1}\" . format ( child . ignored or \"\" , child . value ) ) \n            else : \n                values . append ( child . value ) \n        else : \n            values . append ( child . compressed ( include_ignored = include_ignored ) . value ) \n    return ParseNode ( new_type or self . node_type , children = [ \"\" . join ( values ) ] , consumed = consumed , ignored = ignored , position = self . position ) "}
{"12399": "\ndef save_context ( self ) -> bool : \n    self . _contexts . append ( self . _cursor . position ) \n    return 1 "}
{"12400": "\ndef restore_context ( self ) -> bool : \n    self . _cursor . position = self . _contexts . pop ( ) \n    return 0 "}
{"12401": "\ndef to_fmt ( self , with_from = 0 ) -> fmt . indentable : \n    txt = fmt . sep ( \"\\n\" , [ fmt . sep ( \" \" , [ self . _type_source , \"to\" , self . _type_target , '=' , self . _fun . to_fmt ( ) ] ) , self . _notify . get_content ( with_from ) ] ) \n    return txt "}
{"12414": "\ndef add ( self , it : Signature ) -> bool : \n    if isinstance ( it , Scope ) : \n        it . state = StateScope . EMBEDDED \n    txt = it . internal_name ( ) \n    it . set_parent ( self ) \n    if self . is_namespace : \n        txt = it . internal_name ( ) \n    if txt == \"\" : \n        txt = '_' + str ( len ( self . _hsig ) ) \n    if txt in self . _hsig : \n        raise KeyError ( \"Already exists %s\" % txt ) \n    self . _hsig [ txt ] = it \n    self . __update_count ( ) \n    return 1 "}
{"12415": "\ndef remove ( self , it : Signature ) -> bool : \n    txt = it . internal_name ( ) \n    if txt not in self . _hsig : \n        raise KeyError ( it . show_name ( ) + ' not in Set' ) \n    sig = self . _hsig [ txt ] \n    if isinstance ( sig , Scope ) : \n        sig . state = StateScope . LINKED \n    del self . _hsig [ txt ] \n    return 1 "}
{"12416": "\ndef discard ( self , it : Signature ) -> bool : \n    txt = it . internal_name ( ) \n    if txt in self . _hsig : \n        sig = self . _hsig [ txt ] \n        if isinstance ( sig , Scope ) : \n            sig . state = StateScope . LINKED \n        del self . _hsig [ txt ] \n        return 1 \n    return 0 "}
{"12421": "\ndef get_by_symbol_name ( self , name : str ) -> Scope : \n    lst = [ ] \n    for s in self . values ( ) : \n        if s . name == name : \n            lst . append ( EvalCtx . from_sig ( s ) ) \n    if len ( lst ) == 0 : \n        p = self . get_parent ( ) \n        if p is not None : \n            return p . get_by_symbol_name ( name ) \n    rscope = Scope ( sig = lst , state = StateScope . LINKED , is_namespace = 0 ) \n    rscope . set_parent ( self ) \n    return rscope "}
{"12423": "\ndef get_all_polymorphic_return ( self ) -> bool : \n    lst = [ ] \n    for s in self . values ( ) : \n        if hasattr ( s , 'tret' ) and s . tret . is_polymorphic : \n            lst . append ( EvalCtx . from_sig ( s ) ) \n    rscope = Scope ( sig = lst , state = StateScope . LINKED , is_namespace = 0 ) \n    rscope . set_parent ( self ) \n    return rscope "}
{"12428": "\ndef _hit_ok ( hit , min_hit_charge , max_hit_charge ) : \n    if hit [ 'charge' ] < min_hit_charge : \n        return 0 \n    if max_hit_charge != 0 and hit [ 'charge' ] > max_hit_charge : \n        return 0 \n    return 1 "}
{"12444": "\ndef hook ( cls , hookname = None , erase = 0 ) : \n    if not hasattr ( cls , '_hooks' ) : \n        raise TypeError ( \"%s didn't seems to be a BasicParser subsclasse\" % cls . __name__ ) \n    class_hook_list = cls . _hooks \n    class_rule_list = cls . _rules \n    def wrapper ( f ) : \n        nonlocal hookname \n        add_method ( cls ) ( f ) \n        if hookname is None : \n            hookname = f . __name__ \n        if not erase and ( hookname in class_hook_list or hookname in class_rule_list ) : \n            raise TypeError ( \"%s is already define has rule or hook\" % hookname ) \n        if '.' not in hookname : \n            hookname = '.' . join ( [ cls . __module__ , cls . __name__ , hookname ] ) \n        set_one ( class_hook_list , hookname , f ) \n        return f \n    return wrapper "}
{"12445": "\ndef rule ( cls , rulename = None , erase = 0 ) : \n    if not hasattr ( cls , '_rules' ) : \n        raise TypeError ( \"%s didn't seems to be a BasicParser subsclasse\" % cls . __name__ ) \n    class_hook_list = cls . _hooks \n    class_rule_list = cls . _rules \n    def wrapper ( f ) : \n        nonlocal rulename \n        add_method ( cls ) ( f ) \n        if rulename is None : \n            rulename = f . __name__ \n        if not erase and ( rulename in class_hook_list or rulename in class_rule_list ) : \n            raise TypeError ( \"%s is already define has rule or hook\" % rulename ) \n        if '.' not in rulename : \n            rulename = cls . __module__ + '.' + cls . __name__ + '.' + rulename \n        set_one ( class_rule_list , rulename , f ) \n        return f \n    return wrapper "}
{"12448": "\ndef bind ( self , dst : str , src : Node ) -> bool : \n    for m in self . rule_nodes . maps : \n        for k , v in m . items ( ) : \n            if k == dst : \n                m [ k ] = src \n                return 1 \n    raise Exception ( '%s not found' % dst ) "}
{"12449": "\ndef read_eol ( self ) -> bool : \n    if self . read_eof ( ) : \n        return 0 \n    self . _stream . save_context ( ) \n    self . read_char ( '\\r' ) \n    if self . read_char ( '\\n' ) : \n        return self . _stream . validate_context ( ) \n    return self . _stream . restore_context ( ) "}
{"12450": "\ndef push_rule_nodes ( self ) -> bool : \n    if self . rule_nodes is None : \n        self . rule_nodes = collections . ChainMap ( ) \n        self . tag_cache = collections . ChainMap ( ) \n        self . id_cache = collections . ChainMap ( ) \n    else : \n        self . rule_nodes = self . rule_nodes . new_child ( ) \n        self . tag_cache = self . tag_cache . new_child ( ) \n        self . id_cache = self . id_cache . new_child ( ) \n    return 1 "}
{"12451": "\ndef pop_rule_nodes ( self ) -> bool : \n    self . rule_nodes = self . rule_nodes . parents \n    self . tag_cache = self . tag_cache . parents \n    self . id_cache = self . id_cache . parents \n    return 1 "}
{"12454": "\ndef begin_tag ( self , name : str ) -> Node : \n    self . tag_cache [ name ] = Tag ( self . _stream , self . _stream . index ) \n    return 1 "}
{"12455": "\ndef end_tag ( self , name : str ) -> Node : \n    self . tag_cache [ name ] . set_end ( self . _stream . index ) \n    return 1 "}
{"12456": "\ndef set_rules ( cls , rules : dict ) -> bool : \n    cls . _rules = cls . _rules . new_child ( ) \n    for rule_name , rule_pt in rules . items ( ) : \n        if '.' not in rule_name : \n            rule_name = cls . __module__ + '.' + cls . __name__ + '.' + rule_name \n        meta . set_one ( cls . _rules , rule_name , rule_pt ) \n    return 1 "}
{"12457": "\ndef set_hooks ( cls , hooks : dict ) -> bool : \n    cls . _hooks = cls . _hooks . new_child ( ) \n    for hook_name , hook_pt in hooks . items ( ) : \n        if '.' not in hook_name : \n            hook_name = cls . __module__ + '.' + cls . __name__ + '.' + hook_name \n        meta . set_one ( cls . _hooks , hook_name , hook_pt ) \n    return 1 "}
{"12458": "\ndef set_directives ( cls , directives : dict ) -> bool : \n    meta . _directives = meta . _directives . new_child ( ) \n    for dir_name , dir_pt in directives . items ( ) : \n        meta . set_one ( meta . _directives , dir_name , dir_pt ) \n        dir_pt . ns_name = dir_name \n    return 1 "}
{"12459": "\ndef eval_rule ( self , name : str ) -> Node : \n    n = Node ( ) \n    id_n = id ( n ) \n    self . rule_nodes [ '_' ] = n \n    self . id_cache [ id_n ] = '_' \n    if name not in self . __class__ . _rules : \n        self . diagnostic . notify ( error . Severity . ERROR , \"Unknown rule : %s\" % name , error . LocationInfo . from_stream ( self . _stream , is_error = 1 ) ) \n        raise self . diagnostic \n    self . _lastRule = name \n    rule_to_eval = self . __class__ . _rules [ name ] \n    res = rule_to_eval ( self ) \n    if res : \n        res = self . rule_nodes [ '_' ] \n    return res "}
{"12460": "\ndef eval_hook ( self , name : str , ctx : list ) -> Node : \n    if name not in self . __class__ . _hooks : \n        self . diagnostic . notify ( error . Severity . ERROR , \"Unknown hook : %s\" % name , error . LocationInfo . from_stream ( self . _stream , is_error = 1 ) ) \n        raise self . diagnostic \n    self . _lastRule = '#' + name \n    res = self . __class__ . _hooks [ name ] ( self , * ctx ) \n    if type ( res ) is not bool : \n        raise TypeError ( \"Your hook %r didn't return a bool value\" % name ) \n    return res "}
{"12461": "\ndef peek_text ( self , text : str ) -> bool : \n    start = self . _stream . index \n    stop = start + len ( text ) \n    if stop > self . _stream . eos_index : \n        return 0 \n    return self . _stream [ self . _stream . index : stop ] == text "}
{"12462": "\ndef one_char ( self ) -> bool : \n    if self . read_eof ( ) : \n        return 0 \n    self . _stream . incpos ( ) \n    return 1 "}
{"12463": "\ndef read_char ( self , c : str ) -> bool : \n    if self . read_eof ( ) : \n        return 0 \n    self . _stream . save_context ( ) \n    if c == self . _stream . peek_char : \n        self . _stream . incpos ( ) \n        return self . _stream . validate_context ( ) \n    return self . _stream . restore_context ( ) "}
{"12464": "\ndef read_until_eof ( self ) -> bool : \n    if self . read_eof ( ) : \n        return 1 \n    self . _stream . save_context ( ) \n    while not self . read_eof ( ) : \n        self . _stream . incpos ( ) \n    return self . _stream . validate_context ( ) "}
{"12469": "\ndef add_ruleclause_name ( self , ns_name , rid ) -> bool : \n    ns_name . parser_tree = parsing . Rule ( self . value ( rid ) ) \n    return 1 "}
{"12470": "\ndef add_rules ( self , bnf , r ) -> bool : \n    bnf [ r . rulename ] = r . parser_tree \n    return 1 "}
{"12471": "\ndef add_rule ( self , rule , rn , alts ) -> bool : \n    rule . rulename = self . value ( rn ) \n    rule . parser_tree = alts . parser_tree \n    return 1 "}
{"12472": "\ndef add_sequences ( self , sequences , cla ) -> bool : \n    if not hasattr ( sequences , 'parser_tree' ) : \n        sequences . parser_tree = cla . parser_tree \n    else : \n        oldnode = sequences \n        if isinstance ( oldnode . parser_tree , parsing . Seq ) : \n            oldpt = list ( oldnode . parser_tree . ptlist ) \n        else : \n            oldpt = [ oldnode . parser_tree ] \n        oldpt . append ( cla . parser_tree ) \n        sequences . parser_tree = parsing . Seq ( * tuple ( oldpt ) ) \n    return 1 "}
{"12473": "\ndef add_alt ( self , alternatives , alt ) -> bool : \n    if not hasattr ( alternatives , 'parser_tree' ) : \n        if hasattr ( alt , 'parser_tree' ) : \n            alternatives . parser_tree = alt . parser_tree \n        else : \n            alternatives . parser_tree = alt \n    else : \n        oldnode = alternatives \n        if isinstance ( oldnode . parser_tree , parsing . Alt ) : \n            oldpt = list ( oldnode . parser_tree . ptlist ) \n        else : \n            oldpt = [ oldnode . parser_tree ] \n        oldpt . append ( alt . parser_tree ) \n        alternatives . parser_tree = parsing . Alt ( * tuple ( oldpt ) ) \n    return 1 "}
{"12474": "\ndef add_range ( self , sequence , begin , end ) : \n    sequence . parser_tree = parsing . Range ( self . value ( begin ) . strip ( \"'\" ) , self . value ( end ) . strip ( \"'\" ) ) \n    return 1 "}
{"12475": "\ndef add_rpt ( self , sequence , mod , pt ) : \n    modstr = self . value ( mod ) \n    if modstr == '!!' : \n        self . _stream . restore_context ( ) \n        self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a lookahead rule\" , error . LocationInfo . from_stream ( self . _stream , is_error = 1 ) ) \n        raise self . diagnostic \n    if modstr == '!' : \n        self . _stream . restore_context ( ) \n        self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a negated rule\" , error . LocationInfo . from_stream ( self . _stream , is_error = 1 ) ) \n        raise self . diagnostic \n    oldnode = sequence \n    sequence . parser_tree = pt . functor ( oldnode . parser_tree ) \n    return 1 "}
{"12476": "\ndef add_capture ( self , sequence , cpt ) : \n    cpt_value = self . value ( cpt ) \n    sequence . parser_tree = parsing . Capture ( cpt_value , sequence . parser_tree ) \n    return 1 "}
{"12477": "\ndef add_bind ( self , sequence , cpt ) : \n    cpt_value = self . value ( cpt ) \n    sequence . parser_tree = parsing . Bind ( cpt_value , sequence . parser_tree ) \n    return 1 "}
{"12478": "\ndef add_hook ( self , sequence , h ) : \n    sequence . parser_tree = parsing . Hook ( h . name , h . listparam ) \n    return 1 "}
{"12479": "\ndef param_num ( self , param , n ) : \n    param . pair = ( int ( self . value ( n ) ) , int ) \n    return 1 "}
{"12480": "\ndef param_str ( self , param , s ) : \n    param . pair = ( self . value ( s ) . strip ( '\"' ) , str ) \n    return 1 "}
{"12481": "\ndef param_char ( self , param , c ) : \n    param . pair = ( self . value ( c ) . strip ( \"'\" ) , str ) \n    return 1 "}
{"12482": "\ndef param_id ( self , param , i ) : \n    param . pair = ( self . value ( i ) , parsing . Node ) \n    return 1 "}
{"12483": "\ndef hook_name ( self , hook , n ) : \n    hook . name = self . value ( n ) \n    hook . listparam = [ ] \n    return 1 "}
{"12484": "\ndef hook_param ( self , hook , p ) : \n    hook . listparam . append ( p . pair ) \n    return 1 "}
{"12492": "\ndef nextstate ( self , newstate , treenode = None , user_data = None ) : \n    if newstate is None : \n        return self \n    if isinstance ( newstate , State ) and id ( newstate ) != id ( self ) : \n        return newstate \n    elif isinstance ( newstate , StateEvent ) : \n        self . state_register . named_events [ newstate . name ] = 1 \n        return newstate . st \n    elif isinstance ( newstate , StatePrecond ) : \n        return newstate . st \n    elif isinstance ( newstate , StateHook ) : \n        newstate . call ( treenode , user_data ) \n        return newstate . st \n    return self "}
{"12493": "\ndef resetLivingState ( self ) : \n    must_delete = [ ] \n    l = len ( self . ls ) \n    for idx , ls in zip ( range ( l ) , self . ls ) : \n        ids = id ( ls [ 1 ] . thestate ( ) ) \n        if ids == id ( ls [ 0 ] ) and ( ls [ 1 ] . have_finish or not ls [ 1 ] . alive ) : \n            must_delete . append ( idx ) \n        elif ls [ 1 ] . alive : \n            ls [ 1 ] . alive = 0 \n    for delete in reversed ( must_delete ) : \n        self . ls . pop ( delete ) \n    self . init_all ( ) "}
{"12498": "\ndef dump_nodes ( self ) : \n    print ( \"DUMP NODE LOCAL INFOS\" ) \n    try : \n        print ( \"map Id->node name\" ) \n        for k , v in self . id_cache . items ( ) : \n            print ( \"[%d]=%s\" % ( k , v ) ) \n        print ( \"map tag->capture infos\" ) \n        for k , v in self . tag_cache . items ( ) : \n            print ( \"[%s]=%s\" % ( k , v ) ) \n        print ( \"map nodes->tag resolution\" ) \n        for k , v in self . rule_nodes . items ( ) : \n            txt = \"['%s']=%d\" % ( k , id ( v ) ) \n            if k in self . tag_cache : \n                tag = self . tag_cache [ k ] \n                txt += \" tag <%s>\" % tag \n                k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n                if k in self . _stream . value_cache : \n                    txt += \" cache <%s>\" % self . _stream . value_cache [ k ] \n            print ( txt ) \n    except Exception as err : \n        print ( \"RECV Exception %s\" % err ) \n    import sys \n    sys . stdout . flush ( ) \n    return 1 "}
{"12516": "\ndef echo_nodes ( self , * rest ) : \n    txt = \"\" \n    for thing in rest : \n        if isinstance ( thing , Node ) : \n            txt += self . value ( thing ) \n        else : \n            txt += str ( thing ) \n    print ( txt ) \n    return 1 "}
{"12523": "\ndef parse ( self , source : str = None , entry : str = None ) -> parsing . Node : \n    self . from_string = 1 \n    if source is not None : \n        self . parsed_stream ( source ) \n    if entry is None : \n        entry = self . entry \n    if entry is None : \n        raise ValueError ( \"No entry rule name defined for {}\" . format ( self . __class__ . __name__ ) ) \n    return self . _do_parse ( entry ) "}
{"12524": "\ndef parse_file ( self , filename : str , entry : str = None ) -> parsing . Node : \n    self . from_string = 0 \n    import os . path \n    with open ( filename , 'r' ) as f : \n        self . parsed_stream ( f . read ( ) , os . path . abspath ( filename ) ) \n    if entry is None : \n        entry = self . entry \n    if entry is None : \n        raise ValueError ( \"No entry rule name defined for {}\" . format ( self . __class__ . __name__ ) ) \n    return self . _do_parse ( entry ) "}
{"12525": "\ndef set_node ( self , dst , src ) : \n    if not isinstance ( src , Node ) : \n        dst . value = src \n    else : \n        dst . set ( src ) \n        idsrc = id ( src ) \n        iddst = id ( dst ) \n        if iddst not in self . id_cache : \n            print ( \"DST: %s\" % repr ( dst ) ) \n            print ( \"RULE_NODES %s\" % repr ( self . rule_nodes ) ) \n            print ( \"IDCACHE %s\" % repr ( self . id_cache ) ) \n        if idsrc in self . id_cache : \n            k = self . id_cache [ idsrc ] \n            k2 = self . id_cache [ iddst ] \n            if k in self . rule_nodes : \n                self . tag_cache [ k2 ] = self . tag_cache [ k ] \n    return 1 "}
{"12526": "\ndef set_node_as_int ( self , dst , src ) : \n    dst . value = self . value ( src ) \n    return 1 "}
{"12527": "\ndef get_subnode ( self , dst , ast , expr ) : \n    dst . value = eval ( 'ast' + expr ) \n    return 1 "}
{"12530": "\ndef dump ( deposition , from_date , with_json = 1 , latest_only = 0 , ** kwargs ) : \n    dep_json = json . dumps ( deposition . __getstate__ ( ) , default = default_serializer ) \n    dep_dict = json . loads ( dep_json ) \n    dep_dict [ '_p' ] = { } \n    dep_dict [ '_p' ] [ 'id' ] = deposition . id \n    dep_dict [ '_p' ] [ 'created' ] = dt2utc_timestamp ( deposition . created ) \n    dep_dict [ '_p' ] [ 'modified' ] = dt2utc_timestamp ( deposition . modified ) \n    dep_dict [ '_p' ] [ 'user_id' ] = deposition . user_id \n    dep_dict [ '_p' ] [ 'state' ] = deposition . state \n    dep_dict [ '_p' ] [ 'has_sip' ] = deposition . has_sip ( ) \n    dep_dict [ '_p' ] [ 'submitted' ] = deposition . submitted \n    return dep_dict "}
{"12531": "\ndef _get_recids_invenio12 ( from_date ) : \n    from invenio . dbquery import run_sql \n    return ( id [ 0 ] for id in run_sql ( 'select id_bibrec from ' 'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id ' 'where d.modification_date >=%s' , ( from_date , ) , run_on_slave = 1 ) ) "}
{"12532": "\ndef _get_recids_invenio2 ( from_date ) : \n    from invenio . legacy . dbquery import run_sql \n    return ( id [ 0 ] for id in run_sql ( 'select id_bibrec from ' 'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id ' 'where d.modification_date >=%s' , ( from_date , ) , run_on_slave = 1 ) ) "}
{"12535": "\ndef get_check ( ) : \n    try : \n        from invenio . dbquery import run_sql \n    except ImportError : \n        from invenio . legacy . dbquery import run_sql \n    return ( run_sql ( 'select count(id) from bibdoc' , run_on_slave = 1 ) [ 0 ] [ 0 ] , [ id [ 0 ] for id in run_sql ( 'select id from bibdoc' , run_on_slave = 1 ) ] , ) "}
{"12537": "\ndef dump ( obj , from_date , with_json = 1 , latest_only = 0 , ** kwargs ) : \n    return dict ( id = obj . id , client_id = obj . client_id , user_id = obj . user_id , token_type = obj . token_type , access_token = obj . access_token , refresh_token = obj . refresh_token , expires = dt2iso_or_empty ( obj . expires ) , _scopes = obj . _scopes , is_personal = obj . is_personal , is_internal = obj . is_internal ) "}
{"12539": "\ndef dump ( u , from_date , with_json = 1 , latest_only = 0 , ** kwargs ) : \n    return dict ( id = u . id , method = u . method , id_user = u . id_user ) "}
{"12541": "\ndef _get_modified_recids_invenio12 ( from_date ) : \n    from invenio . search_engine import search_pattern \n    from invenio . dbquery import run_sql \n    return set ( ( id [ 0 ] for id in run_sql ( 'select id from bibrec where modification_date >= %s' , ( from_date , ) , run_on_slave = 1 ) ) ) , search_pattern "}
{"12543": "\ndef _get_collection_restrictions ( collection ) : \n    try : \n        from invenio . dbquery import run_sql \n        from invenio . access_control_firerole import compile_role_definition \n    except ImportError : \n        from invenio . modules . access . firerole import compile_role_definition \n        from invenio . legacy . dbquery import run_sql \n    res = run_sql ( 'SELECT r.firerole_def_src, email ' 'FROM accROLE as r ' 'JOIN accROLE_accACTION_accARGUMENT ON r.id=id_accROLE ' 'JOIN accARGUMENT AS a ON a.id=id_accARGUMENT ' 'JOIN user_accROLE AS u ON r.id=u.id_accROLE ' 'JOIN user ON user.id=u.id_user ' 'WHERE a.keyword=\"collection\" AND ' 'a.value=%s AND ' 'id_accACTION=(select id from accACTION where name=\"viewrestrcoll\")' , ( collection , ) , run_on_slave = 1 ) \n    fireroles = set ( ) \n    users = set ( ) \n    for f , u in res : \n        fireroles . add ( compile_role_definition ( f ) ) \n        users . add ( u ) \n    return { 'fireroles' : list ( fireroles ) , 'users' : users } "}
{"12544": "\ndef get_record_revisions ( recid , from_date ) : \n    try : \n        from invenio . dbquery import run_sql \n    except ImportError : \n        from invenio . legacy . dbquery import run_sql \n    return run_sql ( 'SELECT job_date, marcxml ' 'FROM hstRECORD WHERE id_bibrec = %s AND job_date >= %s ' 'ORDER BY job_date ASC' , ( recid , from_date ) , run_on_slave = 1 ) "}
{"12545": "\ndef get_record_collections ( recid ) : \n    try : \n        from invenio . search_engine import ( get_all_collections_of_a_record , get_restricted_collections_for_recid ) \n    except ImportError : \n        from invenio . legacy . search_engine import ( get_all_collections_of_a_record , get_restricted_collections_for_recid ) \n    collections = { 'all' : get_all_collections_of_a_record ( recid , recreate_cache_if_needed = 0 ) , } \n    collections [ 'restricted' ] = dict ( ( coll , _get_collection_restrictions ( coll ) ) for coll in get_restricted_collections_for_recid ( recid , recreate_cache_if_needed = 0 ) ) \n    return collections "}
{"12546": "\ndef dump_record_json ( marcxml ) : \n    try : \n        from invenio . modules . records . api import Record \n        d = Record . create ( marcxml , 'marc' ) \n        return d . dumps ( clean = 1 ) \n    except ImportError : \n        from invenio . bibfield import create_record \n        d = create_record ( marcxml , master_format = 'marc' ) \n        return d . dumps ( ) "}
{"12548": "\ndef dump ( recid , from_date , with_json = 0 , latest_only = 0 , with_collections = 0 , ** kwargs ) : \n    if latest_only : \n        revision_iter = [ get_record_revisions ( recid , from_date ) [ - 1 ] ] \n    else : \n        revision_iter = get_record_revisions ( recid , from_date ) \n    record_dump = dict ( record = [ ] , files = [ ] , recid = recid , collections = get_record_collections ( recid ) if with_collections else None , ) \n    for revision_date , revision_marcxml in revision_iter : \n        marcxml = zlib . decompress ( revision_marcxml ) \n        record_dump [ 'record' ] . append ( dict ( modification_datetime = datetime_toutc ( revision_date ) . isoformat ( ) , marcxml = marcxml , json = dump_record_json ( marcxml ) if with_json else None , ) ) \n    record_dump [ 'files' ] = dump_bibdoc ( recid , from_date ) \n    return record_dump "}
{"12549": "\ndef dump ( ra , from_date , with_json = 1 , latest_only = 0 , ** kwargs ) : \n    return dict ( id = ra . id , user_id = ra . user_id , client_id = ra . client_id , extra_data = ra . extra_data ) "}
{"12556": "\ndef get ( query , * args , ** kwargs ) : \n    run_sql = _get_run_sql ( ) \n    actions = [ dict ( id = row [ 0 ] , name = row [ 1 ] , allowedkeywords = row [ 2 ] , optional = row [ 3 ] ) for action in query . split ( ',' ) for row in run_sql ( 'select id, name, description, allowedkeywords, optional ' 'from accACTION where name like %s' , ( action , ) , run_on_slave = 1 ) ] \n    return len ( actions ) , actions "}
{"12557": "\ndef dump ( rt , from_date , with_json = 1 , latest_only = 0 , ** kwargs ) : \n    return dict ( id_remote_account = rt . id_remote_account , token_type = rt . token_type , access_token = rt . access_token , secret = rt . secret ) "}
{"12559": "\ndef import_record ( data , source_type = None , latest_only = 0 ) : \n    source_type = source_type or 'marcxml' \n    assert source_type in [ 'marcxml' , 'json' ] \n    recorddump = current_migrator . records_dump_cls ( data , source_type = source_type , pid_fetchers = current_migrator . records_pid_fetchers , ) \n    try : \n        current_migrator . records_dumploader_cls . create ( recorddump ) \n        db . session . commit ( ) \n    except Exception : \n        db . session . rollback ( ) \n        raise "}
{"12561": "\ndef dump ( obj , from_date , with_json = 1 , latest_only = 0 , ** kwargs ) : \n    return dict ( name = obj . name , description = obj . description , website = obj . website , user_id = obj . user_id , client_id = obj . client_id , client_secret = obj . client_secret , is_confidential = obj . is_confidential , is_internal = obj . is_internal , _redirect_uris = obj . _redirect_uris , _default_scopes = obj . _default_scopes ) "}
{"12562": "\ndef _get_users_invenio12 ( * args , ** kwargs ) : \n    from invenio . dbquery import run_sql , deserialize_via_marshal \n    User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password_salt' , 'note' , 'full_name' , 'settings' , 'nickname' , 'last_login' ] ) \n    users = run_sql ( 'SELECT id, email, password, note, settings, nickname, last_login' ' FROM user' , run_on_slave = 1 ) \n    return len ( users ) , [ User ( id = user [ 0 ] , email = user [ 1 ] , password = user [ 2 ] . decode ( 'latin1' ) , password_salt = user [ 1 ] , note = user [ 3 ] , full_name = user [ 5 ] , settings = deserialize_via_marshal ( user [ 4 ] ) if user [ 4 ] else { } , nickname = 'id_{0}' . format ( user [ 0 ] ) , last_login = user [ 6 ] ) for user in users ] "}
{"12567": "\ndef _loadrecord ( record_dump , source_type , eager = 0 ) : \n    if eager : \n        import_record . s ( record_dump , source_type = source_type ) . apply ( throw = 1 ) \n    elif current_migrator . records_post_task : \n        chain ( import_record . s ( record_dump , source_type = source_type ) , current_migrator . records_post_task . s ( ) ) ( ) \n    else : \n        import_record . delay ( record_dump , source_type = source_type ) "}
{"12568": "\ndef loadrecords ( sources , source_type , recid ) : \n    if recid is not None : \n        for source in sources : \n            records = json . load ( source ) \n            for item in records : \n                if str ( item [ 'recid' ] ) == str ( recid ) : \n                    _loadrecord ( item , source_type , eager = 1 ) \n                    click . echo ( \"Record '{recid}' loaded.\" . format ( recid = recid ) ) \n                    return \n        click . echo ( \"Record '{recid}' not found.\" . format ( recid = recid ) ) \n    else : \n        for idx , source in enumerate ( sources , 1 ) : \n            click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n            data = json . load ( source ) \n            with click . progressbar ( data ) as records : \n                for item in records : \n                    _loadrecord ( item , source_type ) "}
{"12570": "\ndef loadcommon ( sources , load_task , asynchronous = 1 , predicate = None , task_args = None , task_kwargs = None ) : \n    task_args = tuple ( ) if task_args is None else task_args \n    task_kwargs = dict ( ) if task_kwargs is None else task_kwargs \n    click . echo ( 'Loading dumps started.' ) \n    for idx , source in enumerate ( sources , 1 ) : \n        click . echo ( 'Opening dump file {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        with click . progressbar ( data ) as data_bar : \n            for d in data_bar : \n                if predicate is not None : \n                    if predicate ( d ) : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply ( throw = 1 ) \n                        click . echo ( \"Loaded a single record.\" ) \n                        return \n                else : \n                    if asynchronous : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply_async ( ) \n                    else : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply ( throw = 1 ) "}
{"12572": "\ndef loadusers ( sources ) : \n    from . tasks . users import load_user \n    loadcommon ( sources , load_user , asynchronous = 0 ) "}
{"12573": "\ndef loaddeposit ( sources , depid ) : \n    from . tasks . deposit import load_deposit \n    if depid is not None : \n        def pred ( dep ) : \n            return int ( dep [ \"_p\" ] [ \"id\" ] ) == depid \n        loadcommon ( sources , load_deposit , predicate = pred , asynchronous = 0 ) \n    else : \n        loadcommon ( sources , load_deposit ) "}
{"12574": "\ndef get_profiler_statistics ( sort = \"cum_time\" , count = 20 , strip_dirs = 1 ) : \n    json_stats = [ ] \n    pstats = yappi . convert2pstats ( yappi . get_func_stats ( ) ) \n    if strip_dirs : \n        pstats . strip_dirs ( ) \n    for func , func_stat in pstats . stats . iteritems ( ) : \n        path , line , func_name = func \n        cc , num_calls , total_time , cum_time , callers = func_stat \n        json_stats . append ( { \"path\" : path , \"line\" : line , \"func_name\" : func_name , \"num_calls\" : num_calls , \"total_time\" : total_time , \"total_time_per_call\" : total_time / num_calls if total_time else 0 , \"cum_time\" : cum_time , \"cum_time_per_call\" : cum_time / num_calls if cum_time else 0 } ) \n    return sorted ( json_stats , key = itemgetter ( sort ) , reverse = 1 ) [ : count ] "}
{"12578": "\ndef delete ( self ) : \n    CProfileWrapper . profiler . disable ( ) \n    self . running = 0 \n    self . set_status ( 204 ) \n    self . finish ( ) "}
{"12592": "\ndef delete_buckets ( cls , record ) : \n    files = record . get ( '_files' , [ ] ) \n    buckets = set ( ) \n    for f in files : \n        buckets . add ( f . get ( 'bucket' ) ) \n    for b_id in buckets : \n        b = Bucket . get ( b_id ) \n        b . deleted = 1 "}
{"12600": "\ndef dump ( thing , query , from_date , file_prefix , chunk_size , limit , thing_flags ) : \n    init_app_context ( ) \n    file_prefix = file_prefix if file_prefix else '{0}_dump' . format ( thing ) \n    kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , '_' ) , 1 ) for f in thing_flags ) \n    try : \n        thing_func = collect_things_entry_points ( ) [ thing ] \n    except KeyError : \n        click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) \n    click . echo ( \"Querying {0}...\" . format ( thing ) ) \n    count , items = thing_func . get ( query , from_date , limit = limit , ** kwargs ) \n    progress_i = 0 \n    click . echo ( \"Dumping {0}...\" . format ( thing ) ) \n    with click . progressbar ( length = count ) as bar : \n        for i , chunk_ids in enumerate ( grouper ( items , chunk_size ) ) : \n            with open ( '{0}_{1}.json' . format ( file_prefix , i ) , 'w' ) as fp : \n                fp . write ( \"[\\n\" ) \n                for _id in chunk_ids : \n                    try : \n                        json . dump ( thing_func . dump ( _id , from_date , ** kwargs ) , fp , default = set_serializer ) \n                        fp . write ( \",\" ) \n                    except Exception as e : \n                        click . secho ( \"Failed dump {0} {1} ({2})\" . format ( thing , _id , e . message ) , fg = 'red' ) \n                    progress_i += 1 \n                    bar . update ( progress_i ) \n                fp . seek ( fp . tell ( ) - 1 ) \n                fp . write ( \"\\n]\" ) "}
{"12602": "\ndef delete ( self ) : \n    del self . bg . widget \n    del self . bg \n    del self . _pos \n    del self . _size \n    self . actions = { } \n    for e_type , e_handlers in self . peng . eventHandlers . items ( ) : \n        if 1 or e_type in eh : \n            to_del = [ ] \n            for e_handler in e_handlers : \n                if isinstance ( e_handler , weakref . ref ) : \n                    if super ( weakref . WeakMethod , e_handler ) . __call__ ( ) is self : \n                        to_del . append ( e_handler ) \n                elif e_handler is self : \n                    to_del . append ( e_handler ) \n            for d in to_del : \n                try : \n                    del e_handlers [ e_handlers . index ( d ) ] \n                except Exception : \n                    import traceback ; \n                    traceback . print_exc ( ) "}
{"12615": "\ndef ensureModelData ( self , obj ) : \n    if not hasattr ( obj , \"_modeldata\" ) : \n        self . create ( obj , cache = 1 ) \n    if \"_modelcache\" not in obj . _modeldata : \n        self . create ( obj , cache = 1 ) "}
{"12617": "\ndef draw ( self , obj ) : \n    self . ensureModelData ( obj ) \n    data = obj . _modeldata \n    if data . get ( \"_manual_render\" , 0 ) : \n        obj . batch3d . draw ( ) "}
{"12624": "\ndef draw ( self ) : \n    self . window . set2d ( ) \n    if isinstance ( self . bg , Layer ) : \n        self . bg . _draw ( ) \n    elif hasattr ( self . bg , \"draw\" ) and callable ( self . bg . draw ) : \n        self . bg . draw ( ) \n    elif isinstance ( self . bg , list ) or isinstance ( self . bg , tuple ) : \n        self . bg_vlist . draw ( GL_QUADS ) \n    elif callable ( self . bg ) : \n        self . bg ( ) \n    elif isinstance ( self . bg , Background ) : \n        if not self . bg . initialized : \n            self . bg . init_bg ( ) \n            self . bg . redraw_bg ( ) \n            self . bg . initialized = 1 \n    elif self . bg == \"blank\" : \n        pass \n    else : \n        raise TypeError ( \"Unknown background type\" ) \n    self . window . set2d ( ) \n    for widget in self . widgets . values ( ) : \n        if widget . do_redraw : \n            widget . on_redraw ( ) \n            widget . do_redraw = 0 \n    self . batch2d . draw ( ) \n    for widget in self . widgets . values ( ) : \n        widget . draw ( ) "}
{"12628": "\ndef registerEventHandlers ( self ) : \n    self . peng . keybinds . add ( self . peng . cfg [ \"controls.controls.crouch\" ] , \"peng3d:actor.%s.player.controls.crouch\" % self . actor . uuid , self . on_crouch_down , 0 ) \n    self . peng . keybinds . add ( self . peng . cfg [ \"controls.controls.jump\" ] , \"peng3d:actor.%s.player.controls.jump\" % self . actor . uuid , self . on_jump_down , 0 ) \n    pyglet . clock . schedule_interval ( self . update , 1.0 / 60 ) "}
{"12647": "\ndef on_redraw ( self ) : \n    x , y = self . pos \n    sx , sy = self . size \n    self . bg_vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] \n    self . stencil_vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] \n    if isinstance ( self . bg , Background ) : \n        if not self . bg . initialized : \n            self . bg . init_bg ( ) \n            self . bg . initialized = 1 \n        self . bg . redraw_bg ( ) "}
{"12667": "\ndef check_elements ( self ) : \n    existing_types = set ( self . elements . type . argiope . values . flatten ( ) ) \n    allowed_types = set ( ELEMENTS . keys ( ) ) \n    if ( existing_types <= allowed_types ) == 0 : \n        raise ValueError ( \"Element types {0} not in know elements {1}\" . format ( existing_types - allowed_types , allowed_types ) ) \n    print ( \"<Elements: OK>\" ) "}
{"12669": "\ndef centroids_and_volumes ( self , sort_index = 1 ) : \n    elements = self . elements \n    out = [ ] \n    for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : \n        etype_info = ELEMENTS [ etype ] \n        simplices_info = etype_info . simplices \n        index = group . index \n        simplices_data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) \n        simplices = simplices_data . values . reshape ( index . size , simplices_info . shape [ 0 ] , simplices_info . shape [ 1 ] , 3 ) \n        edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] \n        simplices_centroids = simplices . mean ( axis = 2 ) \n        if etype_info . space == 2 : \n            simplices_volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. \n        elif etype_info . space == 3 : \n            simplices_volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. \n        elements_volumes = simplices_volumes . sum ( axis = 1 ) \n        elements_centroids = ( ( simplices_volumes . reshape ( * simplices_volumes . shape , 1 ) * simplices_centroids ) . sum ( axis = 1 ) / elements_volumes . reshape ( * elements_volumes . shape , 1 ) ) \n        volumes_df = pd . DataFrame ( index = index , data = elements_volumes , columns = pd . MultiIndex . from_product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) \n        centroids_df = pd . DataFrame ( index = index , data = elements_centroids , columns = pd . MultiIndex . from_product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) \n        out . append ( pd . concat ( [ volumes_df , centroids_df ] , axis = 1 ) ) \n    out = pd . concat ( out ) \n    if sort_index : \n        out . sort_index ( inplace = 1 ) \n    return out . sort_index ( axis = 1 ) "}
{"12673": "\ndef element_set_to_node_set ( self , tag ) : \n    nodes , elements = self . nodes , self . elements \n    loc = ( elements . conn [ elements [ ( \"sets\" , tag , \"\" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) \n    loc = loc [ loc != 0 ] \n    nodes [ ( \"sets\" , tag ) ] = 0 \n    nodes . loc [ loc , ( \"sets\" , tag ) ] = 1 "}
{"12674": "\ndef node_set_to_surface ( self , tag ) : \n    nodes = self . nodes . copy ( ) \n    dummy = nodes . iloc [ 0 ] . copy ( ) \n    dummy [ \"coords\" ] *= np . nan \n    dummy [ \"sets\" ] = 1 \n    nodes . loc [ 0 ] = dummy \n    element_surfaces = self . split ( \"surfaces\" ) . unstack ( ) \n    surf = pd . DataFrame ( nodes . sets [ tag ] . loc [ element_surfaces . values . flatten ( ) ] . values . reshape ( element_surfaces . shape ) . prod ( axis = 1 ) . astype ( np . bool ) , index = element_surfaces . index ) . unstack ( ) . fillna ( 0 ) \n    for k in surf . keys ( ) : \n        self . elements [ \"surfaces\" , tag , \"f{0}\" . format ( k [ 1 ] + 1 ) ] = surf . loc [ : , k ] "}
{"12678": "\ndef make_directories ( self ) : \n    if os . path . isdir ( self . workdir ) == 0 : \n        os . mkdir ( self . workdir ) "}
{"12679": "\ndef run_postproc ( self ) : \n    t0 = time . time ( ) \n    if self . verbose : \n        print ( '####\u00a0POST-PROCESSING \"{0}\" USING POST-PROCESSOR \"{1}\"' . format ( self . label , self . solver . upper ( ) ) ) \n    if self . solver == \"abaqus\" : \n        command = '{0} viewer noGUI={1}_abqpp.py' . format ( self . solver_path , self . label ) \n        process = subprocess . Popen ( command , cwd = self . workdir , shell = 1 , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n        for line in iter ( process . stdout . readline , b'' ) : \n            line = line . rstrip ( ) . decode ( 'utf8' ) \n            print ( \"    \" , line ) \n    t1 = time . time ( ) \n    if self . verbose : \n        print ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) ) "}
{"12681": "\ndef read_history_report ( path , steps , x_name = None ) : \n    data = pd . read_csv ( path , delim_whitespace = 1 ) \n    if x_name != None : \n        data [ x_name ] = data . X \n        del data [ \"X\" ] \n    data [ \"step\" ] = 0 \n    t = 0. \n    for i in range ( len ( steps ) ) : \n        dt = steps [ i ] . duration \n        loc = data [ data . t == t ] . index \n        if len ( loc ) == 2 : \n            data . loc [ loc [ 1 ] : , \"step\" ] = i \n        t += dt \n    return data "}
{"12692": "\ndef exc_thrown_by_descriptor ( ) : \n    traceback = sys . exc_info ( ) [ 2 ] \n    tb_locals = traceback . tb_frame . f_locals \n    if \"self\" in tb_locals : \n        if not isinstance ( tb_locals [ \"self\" ] , Descriptor ) : \n            return 0 \n        return 1 \n    return 0 "}
{"12693": "\ndef _set_data ( self ) : \n    if getattr ( self , 'data' , 0 ) and not getattr ( self , '_x' , 0 ) and not getattr ( self , '_y' , 0 ) : \n        _x = XVariable ( ) \n        _y = YVariable ( ) \n        _x . contribute_to_class ( self , 'X' , self . data ) \n        _y . contribute_to_class ( self , 'Y' , self . data ) \n        self [ 'data' ] = zip ( self . _x . points , self . _y . points ) \n    else : \n        for axis in ( '_x' , '_y' ) : \n            axis_obj = getattr ( self , axis , 0 ) \n            if not axis_obj : \n                raise exception . MissingAxisException ( \"%s missing\" % axis ) \n            if not getattr ( axis_obj , 'points' , 0 ) : \n                raise exception . MissingDataException ( ) \n        self [ 'data' ] = zip ( self . _x . points , self . _y . points ) "}
{"12701": "\ndef force_unicode ( raw ) : \n    converted = UnicodeDammit ( raw , isHTML = 1 ) \n    if not converted . unicode : \n        converted . unicode = unicode ( raw , 'utf8' , errors = 'ignore' ) \n    encoding_m = encoding_re . match ( converted . unicode ) \n    if encoding_m : \n        converted . unicode = encoding_m . group ( 'start_xml' ) + encoding_m . group ( 'remainder' ) \n    return converted . unicode "}
{"12703": "\ndef is_matching_mime_type ( self , mime_type ) : \n    if len ( self . include_mime_types ) == 0 : \n        return 1 \n    if mime_type is None : \n        return 0 \n    mime_type = mime_type . lower ( ) \n    return any ( mime_type . startswith ( mt ) for mt in self . include_mime_types ) "}
{"12721": "\ndef strip_prefix ( s , prefix , strict = 0 ) : \n    if s . startswith ( prefix ) : \n        return s [ len ( prefix ) : ] \n    elif strict : \n        raise WimpyError ( \"string doesn't start with prefix\" ) \n    return s "}
{"12722": "\ndef strip_suffix ( s , suffix , strict = 0 ) : \n    if s . endswith ( suffix ) : \n        return s [ : len ( s ) - len ( suffix ) ] \n    elif strict : \n        raise WimpyError ( \"string doesn't end with suffix\" ) \n    return s "}
{"12723": "\ndef is_subsequence ( needle , haystack ) : \n    it = iter ( haystack ) \n    for element in needle : \n        if element not in it : \n            return 0 \n    return 1 "}
{"12742": "\ndef get_open_fds ( verbose = 0 ) : \n    pid = os . getpid ( ) \n    procs = subprocess . check_output ( [ \"lsof\" , '-w' , '-Ff' , \"-p\" , str ( pid ) ] ) \n    if verbose : \n        oprocs = subprocess . check_output ( [ \"lsof\" , '-w' , \"-p\" , str ( pid ) ] ) \n        logger . info ( oprocs ) \n    open_files = filter ( lambda s : s and s [ 0 ] == 'f' and s [ 1 : ] . isdigit ( ) , procs . split ( '\\n' ) ) \n    return open_files "}
{"12748": "\ndef random_adjspecies_pair ( maxlen = None , prevent_stutter = 1 ) : \n    while 1 : \n        pair = _random_adjspecies_pair ( ) \n        if maxlen and len ( '' . join ( pair ) ) > maxlen : \n            continue \n        if prevent_stutter and pair [ 0 ] [ - 1 ] == pair [ 1 ] [ 0 ] : \n            continue \n        return pair "}
{"12765": "\ndef ALL_mentions ( target_mentions , chain_mentions ) : \n    found_all = 1 \n    for name in target_mentions : \n        found_one = 0 \n        for chain_ment in chain_mentions : \n            if name in chain_ment : \n                found_one = 1 \n                break \n        if not found_one : \n            found_all = 0 \n            break \n    return found_all "}
{"12766": "\ndef ANY_mentions ( target_mentions , chain_mentions ) : \n    for name in target_mentions : \n        for chain_ment in chain_mentions : \n            if name in chain_ment : \n                return 1 \n    return 0 "}
{"12777": "\ndef html_entities_to_unicode ( text , space_padding = 0 , safe_only = 0 ) : \n    def convert_entities ( match ) : \n        x = match . group ( 1 ) \n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD : \n            return u'&%s;' % x \n        if x in name2codepoint : \n            return unichr ( name2codepoint [ x ] ) \n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS : \n            return XML_ENTITIES_TO_SPECIAL_CHARS [ x ] \n        elif len ( x ) > 0 and x [ 0 ] == '#' : \n            if len ( x ) > 1 and x [ 1 ] == 'x' : \n                return unichr ( int ( x [ 2 : ] , 16 ) ) \n            else : \n                return unichr ( int ( x [ 1 : ] ) ) \n        else : \n            return u'&%s;' % x \n    def convert_to_padded_entitites ( match ) : \n        converted_string = convert_entities ( match ) \n        num_spaces_needed = len ( match . group ( 0 ) ) - len ( converted_string ) \n        assert num_spaces_needed >= 0 , 'len(%r) !<= len(%r)' % ( converted_string , match . group ( 0 ) ) \n        num_left = int ( num_spaces_needed / 2 ) \n        num_right = num_spaces_needed - num_left \n        return ( ' ' * num_left ) + converted_string + ( ' ' * num_right ) \n    if space_padding : \n        return tags . sub ( convert_to_padded_entitites , text ) \n    else : \n        return tags . sub ( convert_entities , text ) "}
{"12783": "\ndef instantiate_config ( config ) : \n    make_absolute_paths ( config ) \n    pipeline_config = config [ 'streamcorpus_pipeline' ] \n    pipeline_config [ 'config_hash' ] = make_hash ( config ) \n    pipeline_config [ 'config_json' ] = json . dumps ( config ) \n    logger . debug ( 'running config: {0} = {1!r}' . format ( pipeline_config [ 'config_hash' ] , config ) ) \n    die = 0 \n    for pathstr in pipeline_config . get ( 'pythonpath' , { } ) . itervalues ( ) : \n        if pathstr not in sys . path : \n            sys . path . append ( pathstr ) \n    for modname in pipeline_config . get ( 'setup_modules' , { } ) . itervalues ( ) : \n        try : \n            m = importlib . import_module ( modname ) \n            if not m : \n                logger . critical ( 'could not load module %r' , modname ) \n                die = 1 \n                continue \n            if hasattr ( m , 'setup' ) : \n                m . setup ( ) \n                logger . debug ( 'loaded and setup %r' , modname ) \n            else : \n                logger . debug ( 'loaded %r' , modname ) \n        except Exception : \n            logger . critical ( 'error loading and initting module %r' , modname , exc_info = 1 ) \n            die = 1 \n    if die : \n        sys . exit ( 1 ) "}
{"12784": "\ndef generate_john_smith_chunk ( path_to_original ) : \n    creation_time = '1998-12-31T23:59:59.999999Z' \n    correct_time = 915148799 \n    if not os . path . isabs ( path_to_original ) : \n        path_to_original = os . path . join ( os . getcwd ( ) , path_to_original ) \n    for label_id in range ( 35 ) : \n        dir_path = os . path . join ( path_to_original , str ( label_id ) ) \n        fnames = os . listdir ( dir_path ) \n        fnames . sort ( ) \n        for fname in fnames : \n            stream_item = streamcorpus . make_stream_item ( creation_time , os . path . join ( 'john-smith-corpus' , str ( label_id ) , fname ) ) \n            if int ( stream_item . stream_time . epoch_ticks ) != correct_time : \n                raise PipelineBaseException ( 'wrong stream_time construction: %r-->%r != %r' % ( creation_time , stream_item . stream_time . epoch_ticks , correct_time ) ) \n            stream_item . source = 'bagga-and-baldwin' \n            body = streamcorpus . ContentItem ( ) \n            raw_string = open ( os . path . join ( dir_path , fname ) ) . read ( ) \n            body . clean_visible = unicode ( raw_string ) . encode ( 'utf8' ) \n            stream_item . body = body \n            stream_item . body . language = streamcorpus . Language ( code = 'en' , name = 'ENGLISH' ) \n            anno = streamcorpus . Annotator ( ) \n            anno . annotator_id = 'bagga-and-baldwin' \n            anno . annotation_time = stream_item . stream_time \n            rating = streamcorpus . Rating ( ) \n            rating . annotator = anno \n            rating . target = streamcorpus . Target ( target_id = str ( label_id ) ) \n            rating . contains_mention = 1 \n            rating . mentions = [ 'john' , 'smith' ] \n            streamcorpus . add_annotation ( stream_item , rating ) \n            yield stream_item "}
{"12788": "\ndef cleanse ( span , lower = 1 ) : \n    assert isinstance ( span , unicode ) , 'got non-unicode string %r' % span \n    span = penn_treebank_brackets . sub ( ' ' , span ) \n    if lower : \n        span = span . lower ( ) \n    span = span . translate ( strip_punctuation ) \n    span = whitespace . sub ( ' ' , span ) \n    return span . strip ( ) "}
{"12800": "\ndef tokens ( self , sentence_dom ) : \n    self . sent_pos = 0 \n    mention_id = 0 \n    while len ( sentence_dom . childNodes ) > 0 : \n        node = sentence_dom . childNodes . pop ( 0 ) \n        if node . nodeType == node . TEXT_NODE : \n            for line in node . data . splitlines ( 1 ) : \n                self . _input_string = line \n                for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                    tok = self . _make_token ( start , end ) \n                    if tok : \n                        yield tok \n                if line . endswith ( '\\n' ) : \n                    self . line_idx += 1 \n                self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n        else : \n            assert node . nodeName == 'ENAMEX' , node . nodeName \n            chain_id = node . attributes . get ( 'ID' ) . value \n            entity_type = node . attributes . get ( 'TYPE' ) . value \n            for node in node . childNodes : \n                assert node . nodeType == node . TEXT_NODE , node . nodeType \n                for line in node . data . splitlines ( 1 ) : \n                    self . _input_string = line \n                    for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                        tok = self . _make_token ( start , end ) \n                        if tok : \n                            if entity_type in _PRONOUNS : \n                                tok . mention_type = MentionType . PRO \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                                attr = Attribute ( attribute_type = AttributeType . PER_GENDER , value = str ( _PRONOUNS [ entity_type ] ) ) \n                                self . attributes . append ( attr ) \n                            else : \n                                tok . mention_type = MentionType . NAME \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                            tok . equiv_id = int ( chain_id ) \n                            tok . mention_id = mention_id \n                            yield tok \n                    if line . endswith ( '\\n' ) : \n                        self . line_idx += 1 \n                    self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n            mention_id += 1 "}
{"12802": "\ndef _retry ( func ) : \n    def retry_func ( self , * args , ** kwargs ) : \n        tries = 1 \n        while 1 : \n            try : \n                return func ( self , * args , ** kwargs ) \n                break \n            except OSError as exc : \n                logger . error ( 'assuming OSError unrecoverable' ) \n                raise \n            except FailedExtraction as exc : \n                logger . error ( 'FAIL(%d)' , tries , exc_info = 1 ) \n                raise \n            except FailedVerification as exc : \n                logger . warn ( 'FAIL(%d)' , tries , exc_info = 1 ) \n                if tries >= self . config [ 'tries' ] : \n                    if self . config . get ( 'suppress_failures' ) : \n                        logger . warn ( 'suppressing failure and breaking out of this loop; data may be corrupt, downstream will have to cope' ) \n                        break \n                    else : \n                        raise \n            except Exception as exc : \n                logger . warn ( 'FAIL(%d): having I/O trouble with S3' , tries , exc_info = 1 ) \n                if tries >= self . config [ 'tries' ] : \n                    raise \n            logger . warn ( 'RETRYING (%d left)' , self . config [ 'tries' ] - tries ) \n            time . sleep ( 3 * tries ) \n            tries += 1 \n    return retry_func "}
{"12803": "\ndef verify_md5 ( md5_expected , data , other_errors = None ) : \n    md5_recv = hashlib . md5 ( data ) . hexdigest ( ) \n    if md5_expected != md5_recv : \n        if other_errors is not None : \n            logger . critical ( '\\n' . join ( other_errors ) ) \n        raise FailedVerification ( 'original md5 = %r != %r = received md5' % ( md5_expected , md5_recv ) ) \n    return 1 "}
{"12804": "\ndef get_bucket ( config , bucket_name = None ) : \n    if not bucket_name : \n        if 'bucket' not in config : \n            raise ConfigurationError ( 'The \"bucket\" parameter is required for the s3 stages.' ) \n        bucket_name = config [ 'bucket' ] \n    aws_access_key_id_path = config . get ( 'aws_access_key_id_path' ) \n    aws_secret_access_key_path = config . get ( 'aws_secret_access_key_path' ) \n    params = ( ) \n    if aws_access_key_id_path and aws_secret_access_key_path : \n        try : \n            access = open ( aws_access_key_id_path ) . read ( ) . strip ( ) \n            secret = open ( aws_secret_access_key_path ) . read ( ) . strip ( ) \n            params = ( access , secret ) \n        except : \n            logger . error ( 'failed reading aws credentials from configured file' , exc_info = 1 ) \n            raise \n    conn = S3Connection ( * params ) \n    bucket = conn . get_bucket ( bucket_name ) \n    return bucket "}
{"12815": "\ndef char_offsets_to_xpaths ( html , char_offsets ) : \n    html = uni ( html ) \n    parser = XpathTextCollector ( ) \n    prev_end = 0 \n    prev_progress = 1 \n    for start , end in char_offsets : \n        if start == end : \n            yield None \n            continue \n        if not prev_progress : \n            for i in xrange ( prev_end , start ) : \n                parser . feed ( html [ i ] ) \n                prev_end += 1 \n                if parser . made_progress : \n                    break \n            if not parser . made_progress : \n                yield None \n                continue \n        if prev_end < start : \n            parser . feed ( html [ prev_end : start ] ) \n            if not parser . made_progress : \n                parser . feed ( html [ start : end ] ) \n                prev_progress = parser . made_progress \n                prev_end = end \n                yield None \n                continue \n        xstart = parser . xpath_offset ( ) \n        parser . feed ( html [ start : end ] ) \n        xend = parser . xpath_offset ( ) \n        prev_end = end \n        if not parser . made_progress : \n            prev_progress = 0 \n            yield None \n        else : \n            prev_progress = 1 \n            yield XpathRange ( xstart [ 0 ] , xstart [ 1 ] , xend [ 0 ] , xend [ 1 ] ) \n    parser . feed ( html [ prev_end : ] ) \n    parser . close ( ) "}
{"12828": "\ndef svg2pdf ( svg_file_path , pdf_file_path , dpi = 150 , command_binpath = None , support_unicode = 0 ) : \n    if support_unicode : \n        return rsvg_export ( svg_file_path , pdf_file_path , dpi = dpi , rsvg_binpath = command_binpath ) \n    return inkscape_export ( svg_file_path , pdf_file_path , export_flag = \"-A\" , dpi = dpi , inkscape_binpath = command_binpath ) "}
{"12836": "\ndef render ( self , file_path , ** kwargs ) : \n    temp = get_tempfile ( suffix = '.svg' ) \n    self . save_content ( temp . name ) \n    file_type = kwargs . get ( 'file_type' , 'pdf' ) \n    dpi = kwargs . get ( 'dpi' , 150 ) \n    support_unicode = kwargs . get ( 'support_unicode' , 0 ) \n    try : \n        if file_type == 'svg' : \n            shutil . copyfile ( temp . name , file_path ) \n        elif file_type == 'png' : \n            svg2png ( temp . name , file_path , dpi = dpi ) \n        elif file_type == 'pdf' : \n            svg2pdf ( temp . name , file_path , dpi = dpi , support_unicode = support_unicode ) \n    except : \n        log . exception ( 'Error exporting file {} to {}' . format ( file_path , file_type ) ) \n        raise "}
{"12839": "\ndef parse ( source , prefixes = None , model = None , encoding = None , use_xhtml_ns = 0 ) : \n    def get_tree_instance ( namespaceHTMLElements , use_xhtml_ns = use_xhtml_ns ) : \n        return treebuilder ( use_xhtml_ns ) \n    parser = html5lib . HTMLParser ( tree = get_tree_instance ) \n    doc = parser . parse ( source ) \n    first_element = next ( ( e for e in doc . root_nodes if isinstance ( e , element ) ) , None ) \n    return first_element "}
{"12844": "\ndef execute ( option ) : \n    namelist_option = [ ] \n    makefile_option = [ ] \n    flags = \"\" \n    for entry in option : \n        key = entry . keys ( ) [ 0 ] \n        if key == \"Problem Size\" : \n            namelist_option . append ( { \"SIZE\" : entry [ key ] } ) \n        elif key == \"F90\" : \n            makefile_option . append ( entry ) \n        else : \n            flags += entry [ key ] + \" \" \n    makefile_option . append ( { \"F90FLAGS\" : flags } ) \n    namelist = create_input ( namelist_option , \"namelist\" , template_location = \"templates\" ) \n    makefile_include = create_input ( makefile_option , \"Makefile.include\" , template_location = \"templates\" ) \n    benchmark_base = \"shallow\" \n    location = benchmark_base + \"/original/namelist\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( namelist ) \n    my_file . flush ( ) \n    location = benchmark_base + \"/common/Makefile.include\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( makefile_include ) \n    my_file . flush ( ) \n    base_path = benchmark_base + \"/original\" \n    import subprocess \n    make_process = subprocess . Popen ( [ \"make\" , \"clean\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return 0 , [ ] \n    make_process = subprocess . Popen ( [ \"make\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return 0 , [ ] \n    make_process = subprocess . Popen ( [ \"./shallow_base\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return 0 , [ ] \n    stdout = make_process . stdout . read ( ) \n    for line in stdout . split ( \"\\n\" ) : \n        if \"Time-stepping\" in line : \n            total_time = line . split ( ) [ 2 ] \n    return 1 , total_time "}
{"12845": "\ndef strval ( node , outermost = 1 ) : \n    if not isinstance ( node , element ) : \n        return node . xml_value if outermost else [ node . xml_value ] \n    accumulator = [ ] \n    for child in node . xml_children : \n        if isinstance ( child , text ) : \n            accumulator . append ( child . xml_value ) \n        elif isinstance ( child , element ) : \n            accumulator . extend ( strval ( child , outermost = 0 ) ) \n    if outermost : \n        accumulator = '' . join ( accumulator ) \n    return accumulator "}
{"12852": "\ndef get_extension ( filepath , check_if_exists = 0 ) : \n    if check_if_exists : \n        if not os . path . exists ( filepath ) : \n            err = 'File not found: ' + filepath \n            log . error ( err ) \n            raise IOError ( err ) \n    try : \n        rest , ext = os . path . splitext ( filepath ) \n    except : \n        raise \n    else : \n        return ext "}
{"12853": "\ndef add_extension_if_needed ( filepath , ext , check_if_exists = 0 ) : \n    if not filepath . endswith ( ext ) : \n        filepath += ext \n    if check_if_exists : \n        if not os . path . exists ( filepath ) : \n            err = 'File not found: ' + filepath \n            log . error ( err ) \n            raise IOError ( err ) \n    return filepath "}
{"12856": "\ndef csv_to_json ( csv_filepath , json_filepath , fieldnames , ignore_first_line = 1 ) : \n    import csv \n    import json \n    csvfile = open ( csv_filepath , 'r' ) \n    jsonfile = open ( json_filepath , 'w' ) \n    reader = csv . DictReader ( csvfile , fieldnames ) \n    rows = [ ] \n    if ignore_first_line : \n        next ( reader ) \n    for row in reader : \n        rows . append ( row ) \n    json . dump ( rows , jsonfile ) \n    jsonfile . close ( ) \n    csvfile . close ( ) "}
{"12868": "\ndef to_json_str ( self ) : \n    adict = dict ( vars ( self ) , sort_keys = 1 ) \n    adict [ 'type' ] = self . __class__ . __name__ \n    return json . dumps ( adict ) "}
{"12882": "\ndef embed_font_to_svg ( filepath , outfile , font_files ) : \n    tree = _embed_font_to_svg ( filepath , font_files ) \n    tree . write ( outfile , encoding = 'utf-8' , pretty_print = 1 ) "}
{"12890": "\ndef to_boolean ( obj ) : \n    if isinstance ( obj , LiteralWrapper ) : \n        val = obj . obj \n    elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : \n        val = next ( obj , None ) \n    else : \n        val = obj \n    if val is None : \n        yield 0 \n    elif isinstance ( val , bool ) : \n        yield val \n    elif isinstance ( val , str ) : \n        yield bool ( str ) \n    elif isinstance ( val , node ) : \n        yield 1 \n    elif isinstance ( val , float ) or isinstance ( val , int ) : \n        yield bool ( val ) \n    else : \n        raise RuntimeError ( 'Unknown type for boolean conversion: {}' . format ( val ) ) "}
{"12893": "\ndef save_into_qrcode ( text , out_filepath , color = '' , box_size = 10 , pixel_size = 1850 ) : \n    try : \n        qr = qrcode . QRCode ( version = 1 , error_correction = qrcode . constants . ERROR_CORRECT_L , box_size = box_size , border = 0 , ) \n        qr . add_data ( text ) \n        qr . make ( fit = 1 ) \n    except Exception as exc : \n        raise Exception ( 'Error trying to generate QR code ' ' from `vcard_string`: {}' . format ( text ) ) from exc \n    else : \n        img = qr . make_image ( image_factory = qrcode . image . svg . SvgPathImage ) \n    _ = _qrcode_to_file ( img , out_filepath ) \n    if color : \n        replace_file_content ( out_filepath , 'fill:#000000' , 'fill:#{}' . format ( color ) ) "}
{"12894": "\ndef launch ( option ) : \n    from melody . inputs import create_input \n    _ = create_input ( option , template_name = \"input.mdp\" ) \n    success = 1 \n    results = None \n    if success : \n        results = { \"rate\" : { \"value\" : 35 , \"units\" : \"ns/day\" } , } \n    return success , results "}
{"12895": "\ndef call_command ( cmd_name , args_strings ) : \n    if not os . path . isabs ( cmd_name ) : \n        cmd_fullpath = which ( cmd_name ) \n    else : \n        cmd_fullpath = cmd_name \n    try : \n        cmd_line = [ cmd_fullpath ] + args_strings \n        log . debug ( 'Calling: `{}`.' . format ( ' ' . join ( cmd_line ) ) ) \n        retval = subprocess . call ( ' ' . join ( cmd_line ) , shell = 1 ) \n    except CalledProcessError as ce : \n        log . exception ( \"Error calling command with arguments: \" \"{} \\n With return code: {}\" . format ( cmd_line , ce . returncode ) ) \n        raise \n    else : \n        return retval "}
{"12925": "\ndef modify_domain ( self , domain_name , new_salt = 0 , username = None ) : \n    domain = self . _get_domain_from_db ( domain_name ) \n    if domain is None : \n        raise NoSuchDomainException \n    if new_salt : \n        _logger . info ( \"Generating new salt..\" ) \n        domain . new_salt ( ) \n    if username is not None : \n        domain . username = username \n    return domain "}
{"12931": "\ndef _init_logging ( verbose = 0 ) : \n    config = { 'version' : 1 , 'formatters' : { 'console' : { 'format' : '* %(message)s' , } } , 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'level' : 'DEBUG' , 'formatter' : 'console' , 'stream' : 'ext://sys.stdout' , } } , 'loggers' : { 'pwm' : { 'level' : 'DEBUG' if verbose else 'INFO' , 'handlers' : [ 'console' ] , 'propagate' : 1 , } , 'requests.packages.urllib3' : { 'level' : 'INFO' if verbose else 'WARNING' , 'handlers' : [ 'console' ] , 'propagate' : 1 , } } } \n    logging . config . dictConfig ( config ) \n    HTTPConnection . debuglevel = 1 if verbose else 0 "}
{"12937": "\ndef open ( path , mode = gdalconst . GA_ReadOnly ) : \n    path = getattr ( path , 'name' , path ) \n    try : \n        return Raster ( vsiprefix ( path ) , mode ) \n    except AttributeError : \n        try : \n            imgdata = path . read ( ) \n        except AttributeError : \n            raise TypeError ( 'Not a file-like object providing read()' ) \n        else : \n            imgio = MemFileIO ( delete = 0 ) \n            gdal . FileFromMemBuffer ( imgio . name , imgdata ) \n            return Raster ( imgio , mode ) \n    raise ValueError ( 'Failed to open raster from \"%r\"' % path ) "}
{"12939": "\ndef copy ( self , source , dest ) : \n    if not self . copyable : \n        raise IOError ( 'Driver does not support raster copying' ) \n    if not isinstance ( source , Raster ) : \n        source = Raster ( source ) \n        should_close = 1 \n    else : \n        should_close = 0 \n    if source . name == dest : \n        raise ValueError ( 'Input and output are the same location: %s' % source . name ) \n    settings = driverdict_tolist ( self . settings ) \n    ds = self . CreateCopy ( dest , source . ds , self . strictmode , options = settings ) \n    if should_close : \n        source . close ( ) \n    return Raster ( ds ) "}
{"12965": "\ndef emphasis ( obj , align = 1 ) : \n    if isinstance ( obj , dict ) : \n        if align : \n            pretty_msg = os . linesep . join ( [ \"%25s: %s\" % ( k , obj [ k ] ) for k in sorted ( obj . keys ( ) ) ] ) \n        else : \n            pretty_msg = json . dumps ( obj , indent = 4 , sort_keys = 1 ) \n    else : \n        return obj \n    return pretty_msg "}
{"12966": "\nasync def handle_jobs ( job_handler , host , port , * , loop ) : \n    try : \n        try : \n            reader , writer = await asyncio . open_connection ( host , port , loop = loop ) \n        except OSError : \n            logging . error ( \"worker could not connect to server\" ) \n            return \n        while 1 : \n            try : \n                call_encoded = await reader . readuntil ( b\"\\n\" ) \n            except ( asyncio . IncompleteReadError , ConnectionResetError ) : \n                break \n            logging . debug ( \"worker got call\" ) \n            call_json = call_encoded . decode ( \"utf-8\" ) \n            call = json . loads ( call_json ) \n            response = job_handler ( call ) \n            response_json = json . dumps ( response ) + \"\\n\" \n            response_encoded = response_json . encode ( \"utf-8\" ) \n            writer . write ( response_encoded ) \n            logging . debug ( \"worker returned response\" ) \n    except KeyboardInterrupt : \n        pass "}
{"12976": "\ndef _send_reliable_message ( self , msg ) : \n    result = 0 \n    max_retries = 15 \n    trans_id = next ( LWLink . transaction_id ) \n    msg = \"%d,%s\" % ( trans_id , msg ) \n    try : \n        with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as write_sock , socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as read_sock : \n            write_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n            read_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_BROADCAST , 1 ) \n            read_sock . settimeout ( self . SOCKET_TIMEOUT ) \n            read_sock . bind ( ( '0.0.0.0' , self . RX_PORT ) ) \n            while max_retries : \n                max_retries -= 1 \n                write_sock . sendto ( msg . encode ( 'UTF-8' ) , ( LWLink . link_ip , self . TX_PORT ) ) \n                result = 0 \n                while 1 : \n                    response , dummy = read_sock . recvfrom ( 1024 ) \n                    response = response . decode ( 'UTF-8' ) \n                    if \"Not yet registered.\" in response : \n                        _LOGGER . error ( \"Not yet registered\" ) \n                        self . register ( ) \n                        result = 1 \n                        break \n                    if response . startswith ( \"%d,OK\" % trans_id ) : \n                        result = 1 \n                        break \n                    if response . startswith ( \"%d,ERR\" % trans_id ) : \n                        _LOGGER . error ( response ) \n                        break \n                    _LOGGER . info ( response ) \n                if result : \n                    break \n                time . sleep ( 0.25 ) \n    except socket . timeout : \n        _LOGGER . error ( \"LW broker timeout!\" ) \n        return result \n    except Exception as ex : \n        _LOGGER . error ( ex ) \n        raise \n    if result : \n        _LOGGER . info ( \"LW broker OK!\" ) \n    else : \n        _LOGGER . error ( \"LW broker fail!\" ) \n    return result "}
{"12979": "\ndef update_ ( self , sct_dict , conf_arg = 1 ) : \n    for opt , val in sct_dict . items ( ) : \n        if opt not in self . def_ : \n            continue \n        if not conf_arg or self . def_ [ opt ] . conf_arg : \n            self [ opt ] = val "}
{"12984": "\ndef create_config_ ( self , index = 0 , update = 0 ) : \n    if not self . config_files_ [ index : ] : \n        return \n    path = self . config_files_ [ index ] \n    if not path . parent . exists ( ) : \n        path . parent . mkdir ( parents = 1 ) \n    conf_dict = { } \n    for section in self . sections_ ( ) : \n        conf_opts = [ o for o , m in self [ section ] . defaults_ ( ) if m . conf_arg ] \n        if not conf_opts : \n            continue \n        conf_dict [ section ] = { } \n        for opt in conf_opts : \n            conf_dict [ section ] [ opt ] = ( self [ section ] [ opt ] if update else self [ section ] . def_ [ opt ] . default ) \n    with path . open ( 'w' ) as cfile : \n        toml . dump ( conf_dict , cfile ) "}
{"12985": "\ndef update_ ( self , conf_dict , conf_arg = 1 ) : \n    for section , secdict in conf_dict . items ( ) : \n        self [ section ] . update_ ( secdict , conf_arg ) "}
{"12994": "\ndef _zsh_comp_command ( self , zcf , cmd , grouping , add_help = 1 ) : \n    if add_help : \n        if grouping : \n            print ( \"+ '(help)'\" , end = BLK , file = zcf ) \n        print ( \"'--help[show help message]'\" , end = BLK , file = zcf ) \n        print ( \"'-h[show help message]'\" , end = BLK , file = zcf ) \n    no_comp = ( 'store_true' , 'store_false' ) \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        if meta . cmd_kwargs . get ( 'action' ) == 'append' : \n            grpfmt , optfmt = \"+ '{}'\" , \"'*{}[{}]{}'\" \n            if meta . comprule is None : \n                meta . comprule = '' \n        else : \n            grpfmt , optfmt = \"+ '({})'\" , \"'{}[{}]{}'\" \n        if meta . cmd_kwargs . get ( 'action' ) in no_comp or meta . cmd_kwargs . get ( 'nargs' ) == 0 : \n            meta . comprule = None \n        if meta . comprule is None : \n            compstr = '' \n        elif meta . comprule == '' : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :( )' \n        else : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :{}' . format ( meta . comprule ) \n        if grouping : \n            print ( grpfmt . format ( opt ) , end = BLK , file = zcf ) \n        for name in _names ( self . _conf [ sct ] , opt ) : \n            print ( optfmt . format ( name , meta . help . replace ( \"'\" , \"'\\\"'\\\"'\" ) , compstr ) , end = BLK , file = zcf ) "}
{"12995": "\ndef zsh_complete ( self , path , cmd , * cmds , sourceable = 0 ) : \n    grouping = internal . zsh_version ( ) >= ( 5 , 4 ) \n    path = pathlib . Path ( path ) \n    firstline = [ '#compdef' , cmd ] \n    firstline . extend ( cmds ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as zcf : \n        print ( * firstline , end = '\\n\\n' , file = zcf ) \n        print ( 'function _{} {{' . format ( cmd ) , file = zcf ) \n        print ( 'local line' , file = zcf ) \n        print ( '_arguments -C' , end = BLK , file = zcf ) \n        if subcmds : \n            substrs = [ \"{}\\\\:'{}'\" . format ( sub , self . subcmds [ sub ] . help ) for sub in subcmds ] \n            print ( '\"1:Commands:(({}))\"' . format ( ' ' . join ( substrs ) ) , end = BLK , file = zcf ) \n        self . _zsh_comp_command ( zcf , None , grouping ) \n        if subcmds : \n            print ( \"'*::arg:->args'\" , file = zcf ) \n            print ( 'case $line[1] in' , file = zcf ) \n            for sub in subcmds : \n                print ( '{sub}) _{cmd}_{sub} ;;' . format ( sub = sub , cmd = cmd ) , file = zcf ) \n            print ( 'esac' , file = zcf ) \n        print ( '}' , file = zcf ) \n        for sub in subcmds : \n            print ( '\\nfunction _{}_{} {{' . format ( cmd , sub ) , file = zcf ) \n            print ( '_arguments' , end = BLK , file = zcf ) \n            self . _zsh_comp_command ( zcf , sub , grouping ) \n            print ( '}' , file = zcf ) \n        if sourceable : \n            print ( '\\ncompdef _{0} {0}' . format ( cmd ) , * cmds , file = zcf ) "}
{"12996": "\ndef _bash_comp_command ( self , cmd , add_help = 1 ) : \n    out = [ '-h' , '--help' ] if add_help else [ ] \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict : \n        out . extend ( _names ( self . _conf [ sct ] , opt ) ) \n    return out "}
{"13004": "\ndef close ( self ) : \n    if self . _closed : \n        return \n    self . _closed = 1 \n    if self . _job is not None : \n        self . _manager . return_job ( self . _job ) \n        self . _job = None "}
{"13006": "\ndef close ( self ) : \n    if self . _closed : \n        return \n    self . _closed = 1 \n    self . _server . close ( ) \n    self . _manager . close ( ) \n    for worker in self . _workers : \n        worker . close ( ) "}
{"13021": "\ndef close ( self ) : \n    if self . _closed : \n        return \n    self . _closed = 1 \n    if self . _active_js is not None : \n        self . _active_js . cancel ( ) \n    for js in self . _js_queue : \n        js . cancel ( ) "}
{"13023": "\ndef _match_regex ( regex , obj ) : \n    if isinstance ( obj , six . string_types ) : \n        return len ( regex . findall ( obj ) ) > 0 \n    elif isinstance ( obj , dict ) : \n        return _match_regex ( regex , obj . values ( ) ) \n    elif hasattr ( obj , '__iter__' ) : \n        return any ( _match_regex ( regex , s ) for s in obj if isinstance ( s , six . string_types ) ) \n    else : \n        return 0 "}
{"13024": "\ndef get_entries ( latest , filters , exclude , limit = None ) : \n    entry_list = _list_all_latest ( ) if latest is 1 or not _is_valid_cache ( ) else _list_all_cached ( ) \n    filtered = filter_entries ( entry_list , filters , exclude ) \n    if limit is not None : \n        return filtered [ : limit ] \n    else : \n        return filtered "}
{"13029": "\ndef _get_attrib ( self , attr , convert_to_str = 0 ) : \n    if attr . startswith ( 'tags.' ) : \n        tag = attr [ len ( 'tags.' ) : ] \n        if tag in self . tags and self . tags [ tag ] != '' : \n            return self . tags [ tag ] \n        elif convert_to_str is 1 : \n            return '<not set>' \n        else : \n            return self . tags . get ( tag ) \n    elif not hasattr ( self , attr ) : \n        raise AttributeError ( 'Invalid attribute: {0}. Perhaps you meant ' '{1}?' . format ( red ( attr ) , green ( 'tags.' + attr ) ) ) \n    else : \n        result = getattr ( self , attr ) \n        if convert_to_str is 1 and not result : \n            return '<none>' \n        elif convert_to_str is 1 and isinstance ( result , list ) : \n            return ', ' . join ( result ) \n        elif convert_to_str is 1 : \n            return str ( result ) \n        else : \n            return result "}
{"13030": "\ndef sort_by ( cls , entries , attribute ) : \n    def key ( entry ) : \n        return entry . _get_attrib ( attribute , convert_to_str = 1 ) \n    return sorted ( entries , key = key ) "}
{"13031": "\ndef repr_as_line ( self , additional_columns = None , only_show = None , sep = ',' ) : \n    additional_columns = additional_columns or [ ] \n    if only_show is not None : \n        columns = _uniquify ( only_show ) \n    else : \n        columns = _uniquify ( self . DEFAULT_COLUMNS + additional_columns ) \n    to_display = [ self . _get_attrib ( c , convert_to_str = 1 ) for c in columns ] \n    return sep . join ( to_display ) "}
{"13035": "\ndef render_entries ( cls , entries , additional_columns = None , only_show = None , numbers = 0 ) : \n    additional_columns = additional_columns or [ ] \n    if only_show is not None : \n        columns = _uniquify ( only_show ) \n    else : \n        columns = _uniquify ( cls . DEFAULT_COLUMNS + additional_columns ) \n    top_row = [ cls . prettyname ( col ) for col in columns ] \n    table = [ top_row ] if numbers is 0 else [ [ '' ] + top_row ] \n    for i , entry in enumerate ( entries ) : \n        row = [ entry . _get_attrib ( c , convert_to_str = 1 ) for c in columns ] \n        table . append ( row if numbers is 0 else [ i ] + row ) \n    cur_width = get_current_terminal_width ( ) \n    colors = [ get_color_hash ( c , MIN_COLOR_BRIGHT , MAX_COLOR_BRIGHT ) for c in columns ] \n    if cur_width >= get_table_width ( table ) : \n        return render_table ( table , column_colors = colors if numbers is 0 else [ green ] + colors ) \n    else : \n        result = [ ] \n        first_index = 1 if numbers is 1 else 0 \n        for row in table [ 1 : ] : \n            rep = [ green ( '%s:' % row [ 0 ] if numbers is 1 else '-----' ) ] \n            for i , val in enumerate ( row [ first_index : ] ) : \n                color = colors [ i - 1 if numbers is 1 else i ] \n                name = columns [ i ] \n                rep . append ( '  %s: %s' % ( name , color ( val ) ) ) \n            result . append ( '\\n' . join ( rep ) ) \n        return '\\n' . join ( result ) "}
{"13038": "\ndef logger ( name = __name__ , output = None , uuid = 0 , timestamp = 0 ) : \n    processors = [ ] \n    if output == 'json' : \n        processors . append ( structlog . processors . JSONRenderer ( ) ) \n    if uuid : \n        processors . append ( add_unique_id ) \n    if uuid : \n        processors . append ( add_timestamp ) \n    return structlog . wrap_logger ( logbook . Logger ( name ) , processors = processors ) "}
{"13039": "\ndef setup ( title , output = 'json' , timezone = None ) : \n    timezone = timezone or dna . time_utils . _detect_timezone ( ) \n    broker_url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER_HOST' , 'localhost' ) , os . environ . get ( 'BROKER_PORT' , 6379 ) , 0 ) \n    app = Celery ( title , broker = broker_url ) \n    app . conf . update ( CELERY_TASK_SERIALIZER = output , CELERY_ACCEPT_CONTENT = [ output ] , CELERY_RESULT_SERIALIZER = output , CELERY_RESULT_BACKEND = broker_url , CELERY_TIMEZONE = timezone , CELERYD_FORCE_EXECV = 1 , CELERY_ENABLE_UTC = 1 , CELERY_IGNORE_RESULT = 0 ) \n    return app "}
{"13041": "\ndef delete ( self , worker_id ) : \n    code = 200 \n    if worker_id in self . jobs : \n        self . jobs [ worker_id ] [ 'worker' ] . revoke ( terminate = 1 ) \n        report = { 'id' : worker_id , 'revoked' : 1 } \n        self . jobs . pop ( worker_id ) \n    else : \n        report = { 'error' : 'job {} unknown' . format ( worker_id ) } \n        code = 404 \n    return flask . jsonify ( report ) , code "}
{"13042": "\ndef switch_opt ( default , shortname , help_msg ) : \n    return ConfOpt ( bool ( default ) , 1 , shortname , dict ( action = internal . Switch ) , 1 , help_msg , None ) "}
{"13043": "\ndef config_conf_section ( ) : \n    config_dict = OrderedDict ( ( ( 'create' , ConfOpt ( None , 1 , None , { 'action' : 'store_true' } , 0 , 'create most global config file' ) ) , ( 'create_local' , ConfOpt ( None , 1 , None , { 'action' : 'store_true' } , 0 , 'create most local config file' ) ) , ( 'update' , ConfOpt ( None , 1 , None , { 'action' : 'store_true' } , 0 , 'add missing entries to config file' ) ) , ( 'edit' , ConfOpt ( None , 1 , None , { 'action' : 'store_true' } , 0 , 'open config file in a text editor' ) ) , ( 'editor' , ConfOpt ( 'vim' , 0 , None , { } , 1 , 'text editor' ) ) , ) ) \n    return config_dict "}
{"13046": "\ndef create_complete_files ( climan , path , cmd , * cmds , zsh_sourceable = 0 ) : \n    path = pathlib . Path ( path ) \n    zsh_dir = path / 'zsh' \n    if not zsh_dir . exists ( ) : \n        zsh_dir . mkdir ( parents = 1 ) \n    zsh_file = zsh_dir / '_{}.sh' . format ( cmd ) \n    bash_dir = path / 'bash' \n    if not bash_dir . exists ( ) : \n        bash_dir . mkdir ( parents = 1 ) \n    bash_file = bash_dir / '{}.sh' . format ( cmd ) \n    climan . zsh_complete ( zsh_file , cmd , * cmds , sourceable = zsh_sourceable ) \n    climan . bash_complete ( bash_file , cmd , * cmds ) "}
{"13047": "\ndef render_columns ( columns , write_borders = 1 , column_colors = None ) : \n    if column_colors is not None and len ( column_colors ) != len ( columns ) : \n        raise ValueError ( 'Wrong number of column colors' ) \n    widths = [ max ( len ( cell ) for cell in column ) for column in columns ] \n    max_column_length = max ( len ( column ) for column in columns ) \n    result = '\\n' . join ( render_row ( i , columns , widths , column_colors ) for i in range ( max_column_length ) ) \n    if write_borders : \n        border = '+%s+' % '|' . join ( '-' * ( w + 2 ) for w in widths ) \n        return '%s\\n%s\\n%s' % ( border , result , border ) \n    else : \n        return result "}
{"13049": "\ndef render_table ( table , write_borders = 1 , column_colors = None ) : \n    prepare_rows ( table ) \n    columns = transpose_table ( table ) \n    return render_columns ( columns , write_borders , column_colors ) "}
{"13058": "\ndef is_running ( process ) : \n    try : \n        pgrep = sh . Command ( '/usr/bin/pgrep' ) \n        pgrep ( process ) \n        flag = 1 \n    except sh . ErrorReturnCode_1 : \n        flag = 0 \n    return flag "}
{"13063": "\ndef serve ( self , app_docopt = DEFAULT_DOC , description = '' ) : \n    exit_status = 0 \n    if isinstance ( app_docopt , str ) : \n        args = docopt ( app_docopt , version = description ) \n    elif isinstance ( app_docopt , dict ) : \n        args = app_docopt \n    else : \n        raise ValueError ( 'unknown configuration object ({})' . format ( type ( app_docopt ) ) ) \n    log_level = args . get ( '--log' , 'debug' ) \n    is_debug = args . get ( '--debug' , 0 ) \n    log_output = 'stdout' if is_debug else 'apy.log' \n    safe_bind = args . get ( '--bind' , '127.0.0.1' ) \n    safe_port = int ( args . get ( '--port' , 5000 ) ) \n    log_setup = dna . logging . setup ( level = log_level , output = log_output ) \n    with log_setup . applicationbound ( ) : \n        try : \n            log . info ( 'server ready' , version = description , log = log_level , debug = is_debug , bind = '{}:{}' . format ( safe_bind , safe_port ) ) \n            self . app . run ( host = safe_bind , port = safe_port , debug = is_debug ) \n        except Exception as error : \n            if is_debug : \n                raise \n            log . error ( '{}: {}' . format ( type ( error ) . __name__ , str ( error ) ) ) \n            exit_status = 1 \n        finally : \n            log . info ( 'session ended with status {}' . format ( exit_status ) ) \n    return exit_status "}
{"13065": "\ndef stream_command ( command , formatter = None , write_stdin = None , ignore_empty = 0 ) : \n    command_list = shlex . split ( command ) \n    try : \n        proc = subprocess . Popen ( command_list , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , stdin = subprocess . PIPE ) \n    except Exception as e : \n        raise IOError ( 'Encountered error: {0} when running command {1}' . format ( e . message , ' ' . join ( command_list ) ) ) \n    if write_stdin is not None : \n        proc . stdin . write ( write_stdin ) \n        proc . stdin . flush ( ) \n    while proc . poll ( ) is None : \n        try : \n            line = proc . stdout . readline ( ) \n        except KeyboardInterrupt : \n            sys . exit ( 'Keyboard interrupt while running {}' . format ( command ) ) \n        if len ( line . strip ( ) ) == 0 and ignore_empty is 1 : \n            continue \n        elif 'killed by signal 1' in decode ( line ) . lower ( ) : \n            continue \n        elif 'to the list of known hosts' in decode ( line ) . lower ( ) : \n            continue \n        if formatter is not None : \n            line = formatter ( line ) \n        sys . stdout . write ( line ) \n    result = proc . poll ( ) \n    return result "}
{"13066": "\ndef stream_command_dicts ( commands , parallel = 0 ) : \n    if parallel is 1 : \n        threads = [ ] \n        for command in commands : \n            target = lambda : stream_command ( ** command ) \n            thread = Thread ( target = target ) \n            thread . start ( ) \n            threads . append ( thread ) \n        for t in threads : \n            t . join ( ) \n    else : \n        for command in commands : \n            stream_command ( ** command ) "}
{"13067": "\ndef stream_commands ( commands , hash_colors = 1 , parallel = 0 ) : \n    def _get_color ( string ) : \n        if hash_colors is 1 : \n            return get_color_hash ( string ) \n        else : \n            return DEFAULT_COLOR \n    fixed_commands = [ ] \n    for command in commands : \n        cmd_text = command [ 'command' ] \n        description = command . get ( 'description' ) \n        color = _get_color ( description or '' ) \n        write_stdin = command . get ( 'write_stdin' ) \n        description = color ( description ) if color is not None else description \n        formatter = _format_with_description ( description ) \n        fixed_commands . append ( { 'command' : cmd_text , 'formatter' : formatter , 'write_stdin' : write_stdin , 'ignore_empty' : 1 } ) \n    stream_command_dicts ( fixed_commands , parallel = parallel ) "}
{"13069": "\ndef _get_path ( cmd ) : \n    if cmd in _PATHS : \n        return _PATHS [ cmd ] \n    out = subprocess . check_output ( 'which {}' . format ( cmd ) , shell = 1 ) \n    _PATHS [ cmd ] = out . decode ( \"utf-8\" ) . strip ( ) \n    return _PATHS [ cmd ] "}
{"13072": "\ndef _copy_to ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = 0 , local_path = local_path , remote_path = remote_path ) \n        print ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    print ( green ( 'Finished copying' ) ) "}
{"13073": "\ndef _copy_from ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    paths = set ( ) \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        _local_path = entry . format_string ( local_path ) \n        if _local_path in paths : \n            raise ValueError ( 'Duplicate local paths: one or more paths ' 'had value {} after formatting.' . format ( local_path ) ) \n        paths . add ( _local_path ) \n        _folder = os . path . split ( _local_path ) [ 0 ] \n        if len ( _folder ) > 0 : \n            if not os . path . exists ( _folder ) : \n                print ( 'Creating directory ' + _folder ) \n                os . makedirs ( _folder ) \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = 1 , local_path = _local_path , remote_path = remote_path ) \n        print ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    print ( green ( 'Finished copying' ) ) "}
{"13074": "\ndef _run_ssh_command ( entries , username , idfile , command , tunnel , parallel = 0 ) : \n    if len ( entries ) == 0 : \n        print ( '(No hosts to run command on)' ) \n        return 1 \n    if command . strip ( ) == '' or command is None : \n        raise ValueError ( 'No command given' ) \n    print ( 'Running command {0} on {1} matching hosts' . format ( green ( repr ( command ) ) , len ( entries ) ) ) \n    shell_cmds = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_ssh_command ( hname , username , idfile , command , tunnel ) \n        shell_cmds . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( shell_cmds , parallel = parallel ) \n    print ( green ( 'All commands finished' ) ) "}
{"13075": "\ndef _connect_ssh ( entry , username , idfile , tunnel = None ) : \n    if entry . hostname != \"\" and entry . hostname is not None : \n        _host = entry . hostname \n    elif entry . public_ip != \"\" and entry . public_ip is not None : \n        _host = entry . public_ip \n    elif entry . private_ip != \"\" and entry . private_ip is not None : \n        if tunnel is None : \n            raise ValueError ( \"Entry does not have a hostname or public IP. \" \"You can connect via private IP if you use a \" \"tunnel.\" ) \n        _host = entry . private_ip \n    else : \n        raise ValueError ( \"No hostname, public IP or private IP information \" \"found on host entry. I don't know how to connect.\" ) \n    command = _build_ssh_command ( _host , username , idfile , None , tunnel ) \n    print ( 'Connecting to %s...' % cyan ( entry . display ( ) ) ) \n    print ( 'SSH command: %s' % green ( command ) ) \n    proc = subprocess . Popen ( command , shell = 1 ) \n    return proc . wait ( ) "}
{"13081": "\ndef add ( self , part , override = 1 ) : \n    ct_add_method = [ self . content_types . add_default , self . content_types . add_override , ] [ override ] \n    self [ part . name ] = part \n    ct_add_method ( part ) "}
{"13086": "\ndef build ( self , secret_key ) : \n    key = jwk . JWK ( kty = 'oct' , k = base64url_encode ( uuid . UUID ( secret_key ) . bytes ) , ) \n    header = { 'alg' : 'dir' , 'enc' : 'A128GCM' , 'zip' : 'DEF' , 'cty' : 'JWT' , 'kid' : self . _access_key , } \n    now = int ( time . time ( ) ) \n    payload = { 'iat' : now , 'nbf' : now , } \n    if self . _expiration is not None : \n        payload [ 'exp' ] = int ( calendar . timegm ( self . _expiration . utctimetuple ( ) ) ) \n    if len ( self . _view_identifiers ) > 0 : \n        payload [ VIEW_IDENTIFIERS_CLAIM_NAME ] = self . _view_identifiers \n    if len ( self . _parameters ) > 0 : \n        parameters = [ ] \n        for parameter in self . _parameters : \n            serialized = { 'field' : parameter . field , 'op' : parameter . op , } \n            if hasattr ( parameter , '__iter__' ) : \n                serialized [ 'any' ] = list ( parameter . value ) \n            else : \n                serialized [ 'value' ] = parameter . value \n            parameters . append ( serialized ) \n        payload [ PARAMETERS_CLAIM_NAME ] = parameters \n    if len ( self . _attributes ) > 0 : \n        payload [ ATTRIBUTES_CLAIM_NAME ] = self . _attributes \n    tok = jwe . JWE ( json_encode ( payload ) , protected = header ) \n    tok . add_recipient ( key ) \n    return tok . serialize ( compact = 1 ) "}
{"13087": "\ndef assign_force_field ( ampal_obj , ff ) : \n    if hasattr ( ampal_obj , 'ligands' ) : \n        atoms = ampal_obj . get_atoms ( ligands = 1 , inc_alt_states = 1 ) \n    else : \n        atoms = ampal_obj . get_atoms ( inc_alt_states = 1 ) \n    for atom in atoms : \n        w_str = None \n        a_ff_id = None \n        if atom . element == 'H' : \n            continue \n        elif atom . parent . mol_code . upper ( ) in ff : \n            if atom . res_label . upper ( ) in ff [ atom . parent . mol_code ] : \n                a_ff_id = ( atom . parent . mol_code . upper ( ) , atom . res_label . upper ( ) ) \n            elif atom . res_label . upper ( ) in ff [ 'WLD' ] : \n                a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) \n            else : \n                w_str = ( '{} atom is not parameterised in the selected ' 'force field for {} residues, this will be ' 'ignored.' ) . format ( atom . res_label , atom . parent . mol_code ) \n        elif atom . res_label . upper ( ) in ff [ 'WLD' ] : \n            a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) \n        else : \n            w_str = ( '{} ({}) atom is not parameterised in the selected' ' residue force field.' ) . format ( atom . res_label , atom . parent . mol_code ) \n        if w_str : \n            warnings . warn ( w_str , NotParameterisedWarning ) \n        atom . tags [ '_buff_ff_id' ] = a_ff_id \n    return "}
{"13092": "\ndef copy_dir ( bucket_name , src_path , dest_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None , surrogate_key = None , cache_control = None , surrogate_control = None , create_directory_redirect_object = 1 ) : \n    if not src_path . endswith ( '/' ) : \n        src_path += '/' \n    if not dest_path . endswith ( '/' ) : \n        dest_path += '/' \n    common_prefix = os . path . commonprefix ( [ src_path , dest_path ] ) \n    if common_prefix == src_path : \n        msg = 'Common prefix {0} is same as source dir {1}' . format ( common_prefix , src_path ) \n        raise RuntimeError ( msg ) \n    if common_prefix == dest_path : \n        msg = 'Common prefix {0} is same as dest dir {1}' . format ( common_prefix , dest_path ) \n        raise RuntimeError ( msg ) \n    delete_dir ( bucket_name , dest_path , aws_access_key_id , aws_secret_access_key ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key , profile_name = aws_profile ) \n    s3 = session . resource ( 's3' ) \n    bucket = s3 . Bucket ( bucket_name ) \n    for src_obj in bucket . objects . filter ( Prefix = src_path ) : \n        src_rel_path = os . path . relpath ( src_obj . key , start = src_path ) \n        dest_key_path = os . path . join ( dest_path , src_rel_path ) \n        head = s3 . meta . client . head_object ( Bucket = bucket_name , Key = src_obj . key ) \n        metadata = head [ 'Metadata' ] \n        content_type = head [ 'ContentType' ] \n        if cache_control is None and 'CacheControl' in head : \n            cache_control = head [ 'CacheControl' ] \n        if surrogate_control is not None : \n            metadata [ 'surrogate-control' ] = surrogate_control \n        if surrogate_key is not None : \n            metadata [ 'surrogate-key' ] = surrogate_key \n        s3 . meta . client . copy_object ( Bucket = bucket_name , Key = dest_key_path , CopySource = { 'Bucket' : bucket_name , 'Key' : src_obj . key } , MetadataDirective = 'REPLACE' , Metadata = metadata , ACL = 'public-read' , CacheControl = cache_control , ContentType = content_type ) \n    if create_directory_redirect_object : \n        dest_dirname = dest_path . rstrip ( '/' ) \n        obj = bucket . Object ( dest_dirname ) \n        metadata = { 'dir-redirect' : 'true' } \n        obj . put ( Body = '' , ACL = 'public-read' , Metadata = metadata , CacheControl = cache_control ) "}
{"13094": "\ndef upload_dir ( bucket_name , path_prefix , source_dir , upload_dir_redirect_objects = 1 , surrogate_key = None , surrogate_control = None , cache_control = None , acl = None , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None ) : \n    logger = logging . getLogger ( __name__ ) \n    logger . debug ( 's3upload.upload({0}, {1}, {2})' . format ( bucket_name , path_prefix , source_dir ) ) \n    session = boto3 . session . Session ( profile_name = aws_profile , aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key ) \n    s3 = session . resource ( 's3' ) \n    bucket = s3 . Bucket ( bucket_name ) \n    metadata = { } \n    if surrogate_key is not None : \n        metadata [ 'surrogate-key' ] = surrogate_key \n    if surrogate_control is not None : \n        metadata [ 'surrogate-control' ] = surrogate_control \n    manager = ObjectManager ( session , bucket_name , path_prefix ) \n    for ( rootdir , dirnames , filenames ) in os . walk ( source_dir ) : \n        bucket_root = os . path . relpath ( rootdir , start = source_dir ) \n        if bucket_root in ( '.' , '/' ) : \n            bucket_root = '' \n        bucket_dirnames = manager . list_dirnames_in_directory ( bucket_root ) \n        for bucket_dirname in bucket_dirnames : \n            if bucket_dirname not in dirnames : \n                logger . debug ( ( 'Deleting bucket directory {0}' . format ( bucket_dirname ) ) ) \n                manager . delete_directory ( bucket_dirname ) \n        bucket_filenames = manager . list_filenames_in_directory ( bucket_root ) \n        for bucket_filename in bucket_filenames : \n            if bucket_filename not in filenames : \n                bucket_filename = os . path . join ( bucket_root , bucket_filename ) \n                logger . debug ( 'Deleting bucket file {0}' . format ( bucket_filename ) ) \n                manager . delete_file ( bucket_filename ) \n        for filename in filenames : \n            local_path = os . path . join ( rootdir , filename ) \n            bucket_path = os . path . join ( path_prefix , bucket_root , filename ) \n            logger . debug ( 'Uploading to {0}' . format ( bucket_path ) ) \n            upload_file ( local_path , bucket_path , bucket , metadata = metadata , acl = acl , cache_control = cache_control ) \n        if upload_dir_redirect_objects is 1 : \n            bucket_dir_path = os . path . join ( path_prefix , bucket_root ) \n            create_dir_redirect_object ( bucket_dir_path , bucket , metadata = metadata , acl = acl , cache_control = cache_control ) "}
{"13095": "\ndef upload_file ( local_path , bucket_path , bucket , metadata = None , acl = None , cache_control = None ) : \n    logger = logging . getLogger ( __name__ ) \n    extra_args = { } \n    if acl is not None : \n        extra_args [ 'ACL' ] = acl \n    if metadata is not None and len ( metadata ) > 0 : \n        extra_args [ 'Metadata' ] = metadata \n    if cache_control is not None : \n        extra_args [ 'CacheControl' ] = cache_control \n    content_type , content_encoding = mimetypes . guess_type ( local_path , strict = 0 ) \n    if content_type is not None : \n        extra_args [ 'ContentType' ] = content_type \n    logger . debug ( str ( extra_args ) ) \n    obj = bucket . Object ( bucket_path ) \n    obj . upload_file ( local_path , ExtraArgs = extra_args ) "}
{"13108": "\ndef get_interaction_energy ( ampal_objs , ff = None , assign_ff = 1 ) : \n    if ff is None : \n        ff = FORCE_FIELDS [ 'bude_2016v1' ] \n    if assign_ff : \n        for ampal_obj in ampal_objs : \n            assign_force_field ( ampal_obj , ff ) \n    interactions = find_inter_ampal ( ampal_objs , ff . distance_cutoff ) \n    buff_score = score_interactions ( interactions , ff ) \n    return buff_score "}
{"13109": "\ndef get_internal_energy ( ampal_obj , ff = None , assign_ff = 1 ) : \n    if ff is None : \n        ff = FORCE_FIELDS [ 'bude_2016v1' ] \n    if assign_ff : \n        assign_force_field ( ampal_obj , ff ) \n    interactions = find_intra_ampal ( ampal_obj , ff . distance_cutoff ) \n    buff_score = score_interactions ( interactions , ff ) \n    return buff_score "}
{"13110": "\ndef hotspots ( self ) : \n    rooted_leaf_samples , _ = self . live_data_copy ( ) \n    line_samples = { } \n    for _ , counts in rooted_leaf_samples . items ( ) : \n        for key , count in counts . items ( ) : \n            line_samples . setdefault ( key , 0 ) \n            line_samples [ key ] += count \n    return sorted ( line_samples . items ( ) , key = lambda v : v [ 1 ] , reverse = 1 ) "}
{"13112": "\ndef upload ( ctx , product , git_ref , dirname , aws_id , aws_secret , ci_env , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron , skip_upload ) : \n    logger = logging . getLogger ( __name__ ) \n    if skip_upload : \n        click . echo ( 'Skipping ltd upload.' ) \n        sys . exit ( 0 ) \n    logger . debug ( 'CI environment: %s' , ci_env ) \n    logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) \n    if ci_env == 'travis' and _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n        sys . exit ( 0 ) \n    ensure_login ( ctx ) \n    git_refs = _get_git_refs ( ci_env , git_ref ) \n    build_resource = register_build ( ctx . obj [ 'keeper_hostname' ] , ctx . obj [ 'token' ] , product , git_refs ) \n    logger . debug ( 'Created build resource %r' , build_resource ) \n    upload_dir ( build_resource [ 'bucket_name' ] , build_resource [ 'bucket_root_dir' ] , dirname , aws_access_key_id = aws_id , aws_secret_access_key = aws_secret , surrogate_key = build_resource [ 'surrogate_key' ] , cache_control = 'max-age=31536000' , surrogate_control = None , upload_dir_redirect_objects = 1 ) \n    logger . debug ( 'Upload complete for %r' , build_resource [ 'self_url' ] ) \n    confirm_build ( build_resource [ 'self_url' ] , ctx . obj [ 'token' ] ) \n    logger . debug ( 'Build %r complete' , build_resource [ 'self_url' ] ) "}
{"13113": "\ndef _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n    travis_event = os . getenv ( 'TRAVIS_EVENT_TYPE' ) \n    if travis_event is None : \n        raise click . UsageError ( 'Using --travis but the TRAVIS_EVENT_TYPE ' 'environment variable is not detected.' ) \n    if travis_event == 'push' and on_travis_push is 0 : \n        click . echo ( 'Skipping upload on Travis push event.' ) \n        return 1 \n    elif travis_event == 'pull_request' and on_travis_pr is 0 : \n        click . echo ( 'Skipping upload on Travis pull request event.' ) \n        return 1 \n    elif travis_event == 'api' and on_travis_api is 0 : \n        click . echo ( 'Skipping upload on Travis pull request event.' ) \n        return 1 \n    elif travis_event == 'cron' and on_travis_cron is 0 : \n        click . echo ( 'Skipping upload on Travis cron event.' ) \n        return 1 \n    else : \n        return 0 "}
{"13116": "\ndef confirm_build ( build_url , keeper_token ) : \n    data = { 'uploaded' : 1 } \n    r = requests . patch ( build_url , auth = ( keeper_token , '' ) , json = data ) \n    if r . status_code != 200 : \n        raise KeeperError ( r ) "}
{"13125": "\ndef gen ( self , slug , name , dataobj , xfield , yfield , time_unit = None , chart_type = \"line\" , width = 800 , height = 300 , color = Color ( ) , size = Size ( ) , scale = Scale ( zero = 0 ) , shape = Shape ( ) , filepath = None , html_before = \"\" , html_after = \"\" ) : \n    chart_obj = self . serialize ( dataobj , xfield , yfield , time_unit , chart_type , width , height , color , size , scale , shape ) \n    html = self . html ( slug , name , chart_obj , filepath , html_before , html_after ) \n    return html "}
{"13127": "\ndef serialize ( self , dataobj , xfield , yfield , time_unit = None , chart_type = \"line\" , width = 800 , height = 300 , color = None , size = None , scale = Scale ( zero = 0 ) , shape = None , options = { } ) : \n    dataset = dataobj \n    if self . _is_dict ( dataobj ) is 1 : \n        dataset = self . _dict_to_df ( dataobj , xfield , yfield ) \n    elif isinstance ( dataobj , list ) : \n        dataset = Data ( values = dataobj ) \n    xencode , yencode = self . _encode_fields ( xfield , yfield , time_unit ) \n    opts = dict ( x = xencode , y = yencode ) \n    if color is not None : \n        opts [ \"color\" ] = color \n    if size is not None : \n        opts [ \"size\" ] = size \n    if shape is not None : \n        opts [ \"shape\" ] = shape \n    chart = self . _chart_class ( dataset , chart_type , ** options ) . encode ( ** opts ) . configure_cell ( width = width , height = height , ) \n    return chart "}
{"13133": "\ndef _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = 0 ) ) : \n    if scale is None : \n        scale = Scale ( ) \n    xfieldtype = xfield [ 1 ] \n    yfieldtype = yfield [ 1 ] \n    x_options = None \n    if len ( xfield ) > 2 : \n        x_options = xfield [ 2 ] \n    y_options = None \n    if len ( yfield ) > 2 : \n        y_options = yfield [ 2 ] \n    if time_unit is not None : \n        if x_options is None : \n            xencode = X ( xfieldtype , timeUnit = time_unit ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , timeUnit = time_unit , scale = scale ) \n    else : \n        if x_options is None : \n            xencode = X ( xfieldtype ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , scale = scale ) \n    if y_options is None : \n        yencode = Y ( yfieldtype , scale = scale ) \n    else : \n        yencode = Y ( yfieldtype , axis = Axis ( ** y_options ) , scale = scale ) \n    return xencode , yencode "}
{"13136": "\ndef up ( tarball_url , auth_token , env , app_name ) : \n    tarball_url = tarball_url or _infer_tarball_url ( ) \n    if not tarball_url : \n        click . echo ( 'No tarball URL found.' ) \n        sys . exit ( 1 ) \n    if env : \n        env = { arg . split ( '=' ) [ 0 ] : arg . split ( '=' ) [ 1 ] for arg in env } \n    happy = Happy ( auth_token = auth_token ) \n    click . echo ( 'Creating app... ' , nl = 0 ) \n    build_id , app_name = happy . create ( tarball_url = tarball_url , env = env , app_name = app_name , ) \n    click . echo ( app_name ) \n    click . echo ( 'Building... ' , nl = 0 ) \n    happy . wait ( build_id ) \n    _write_app_name ( app_name ) \n    click . echo ( 'done' ) \n    click . echo ( \"It's up! :) https://%s.herokuapp.com\" % app_name ) "}
{"13137": "\ndef down ( auth_token , force , app_name ) : \n    if not app_name : \n        click . echo ( 'WARNING: Inferring the app name when deleting is deprecated. ' 'Starting with happy 2.0, the app_name parameter will be required.' ) \n    app_name = app_name or _read_app_name ( ) \n    if not app_name : \n        click . echo ( 'No app name given.' ) \n        sys . exit ( 1 ) \n    if not force : \n        click . confirm ( 'Are you sure you want to delete %s?' % app_name , abort = 1 , ) \n    happy = Happy ( auth_token = auth_token ) \n    click . echo ( 'Destroying app %s... ' % app_name , nl = 0 ) \n    happy . delete ( app_name = app_name ) \n    _delete_app_name_file ( ) \n    click . echo ( 'done' ) \n    click . echo ( \"It's down. :(\" ) "}
{"13142": "\ndef _get_session ( self ) : \n    session = Session ( ) \n    session . headers = { 'Content-type' : 'application/json' , 'Accept' : 'application/vnd.heroku+json; version=3' , } \n    if self . _auth_token : \n        session . trust_env = 0 \n        session . headers [ 'Authorization' ] = 'Bearer %s' % self . _auth_token \n    return session "}
{"13145": "\ndef check_build_status ( self , build_id ) : \n    data = self . api_request ( 'GET' , '/app-setups/%s' % build_id ) \n    status = data . get ( 'status' ) \n    if status == 'pending' : \n        return 0 \n    elif status == 'succeeded' : \n        return 1 \n    else : \n        raise BuildError ( str ( data ) ) "}
{"13157": "\ndef _request_activity_list ( self , athlete ) : \n    response = self . _get_request ( self . _athlete_endpoint ( athlete ) ) \n    response_buffer = StringIO ( response . text ) \n    activity_list = pd . read_csv ( filepath_or_buffer = response_buffer , parse_dates = { 'datetime' : [ 'date' , 'time' ] } , sep = ',\\s*' , engine = 'python' ) \n    activity_list . rename ( columns = lambda x : x . lower ( ) , inplace = 1 ) \n    activity_list . rename ( columns = lambda x : '_' + x if x [ 0 ] . isdigit ( ) else x , inplace = 1 ) \n    activity_list [ 'has_hr' ] = activity_list . average_heart_rate . map ( bool ) \n    activity_list [ 'has_spd' ] = activity_list . average_speed . map ( bool ) \n    activity_list [ 'has_pwr' ] = activity_list . average_power . map ( bool ) \n    activity_list [ 'has_cad' ] = activity_list . average_heart_rate . map ( bool ) \n    activity_list [ 'data' ] = pd . Series ( dtype = np . dtype ( \"object\" ) ) \n    return activity_list "}
{"13158": "\ndef _request_activity_data ( self , athlete , filename ) : \n    response = self . _get_request ( self . _activity_endpoint ( athlete , filename ) ) . json ( ) \n    activity = pd . DataFrame ( response [ 'RIDE' ] [ 'SAMPLES' ] ) \n    activity = activity . rename ( columns = ACTIVITY_COLUMN_TRANSLATION ) \n    activity . index = pd . to_timedelta ( activity . time , unit = 's' ) \n    activity . drop ( 'time' , axis = 1 , inplace = 1 ) \n    return activity [ [ i for i in ACTIVITY_COLUMN_ORDER if i in activity . columns ] ] "}
{"13192": "\ndef anagrams_in_word ( word , sowpods = 0 , start = \"\" , end = \"\" ) : \n    input_letters , blanks , questions = blank_tiles ( word ) \n    for tile in start + end : \n        input_letters . append ( tile ) \n    for word in word_list ( sowpods , start , end ) : \n        lmap = _letter_map ( input_letters ) \n        used_blanks = 0 \n        for letter in word : \n            if letter in lmap : \n                lmap [ letter ] -= 1 \n                if lmap [ letter ] < 0 : \n                    used_blanks += 1 \n                    if used_blanks > ( blanks + questions ) : \n                        break \n            else : \n                used_blanks += 1 \n                if used_blanks > ( blanks + questions ) : \n                    break \n        else : \n            yield ( word , word_score ( word , input_letters , questions ) ) "}
{"13194": "\ndef transform_timeseries_data ( timeseries , start , end = None ) : \n    data = [ ] \n    include = 0 \n    for metric , points in timeseries . items ( ) : \n        for point in points : \n            if point [ 'x' ] == start : \n                include = 1 \n            if include : \n                data . append ( point [ 'y' ] ) \n            if end is not None and point [ 'x' ] == end : \n                return data \n    return data "}
{"13198": "\ndef chmod ( path , mode , recursive = 1 ) : \n    if recursive : \n        cmd = 'chmod -R %s %s' % ( mode , path ) \n    else : \n        cmd = 'chmod %s %s' % ( mode , path ) \n    return sh ( cmd ) "}
{"13201": "\ndef sign ( self , req , receiver = '' , iss = '' , lifetime = 0 , sign_alg = '' , aud = None ) : \n    if not sign_alg : \n        for key_type , s_alg in [ ( 'RSA' , 'RS256' ) , ( 'EC' , 'ES256' ) ] : \n            if self . keyjar . get_signing_key ( key_type = key_type ) : \n                sign_alg = s_alg \n                break \n    if not sign_alg : \n        raise NoSigningKeys ( 'Could not find any signing keys' ) \n    return self . pack ( req = req , receiver = receiver , iss = iss , lifetime = lifetime , sign = 1 , encrypt = 0 , sign_alg = sign_alg ) "}
{"13227": "\ndef create_all_tasks ( element ) : \n    prj = element . project \n    if isinstance ( element , Asset ) : \n        flag = 1 \n    else : \n        flag = 0 \n    deps = prj . department_set . filter ( assetflag = flag ) \n    for d in deps : \n        t = Task ( project = prj , department = d , element = element ) \n        t . full_clean ( ) \n        t . save ( ) "}
{"13228": "\ndef pre_connect ( self , peer ) : \n    if peer in self . _connections : \n        return defer . succeed ( peer ) \n    else : \n        d = self . _connect ( peer , exact_peer = 0 ) \n        def connected ( p ) : \n            return p . peer \n        d . addCallback ( connected ) \n        return d "}
{"13261": "\ndef pretty_print ( input_word , anagrams , by_length = 0 ) : \n    scores = { } \n    if by_length : \n        noun = \"tiles\" \n        for word , score in anagrams : \n            try : \n                scores [ len ( word ) ] . append ( \"{0} ({1:d})\" . format ( word , score ) ) \n            except KeyError : \n                scores [ len ( word ) ] = [ \"{0} ({1:d})\" . format ( word , score ) ] \n    else : \n        noun = \"points\" \n        for word , score in anagrams : \n            try : \n                scores [ score ] . append ( word ) \n            except KeyError : \n                scores [ score ] = [ word ] \n    print ( \"Anagrams for {0}{1}:\" . format ( input_word , \" (score)\" * by_length ) ) \n    if not valid_scrabble_word ( input_word ) : \n        print ( \"{0} is not possible in Scrabble.\" . format ( input_word ) ) \n    for key , value in sorted ( scores . items ( ) , reverse = 1 ) : \n        print ( \"{0:d} {1}: {2}\" . format ( key , noun , \", \" . join ( value ) ) ) "}
{"13262": "\ndef argument_parser ( args ) : \n    parser = argparse . ArgumentParser ( prog = \"nagaram\" , description = \"Finds Scabble anagrams.\" , formatter_class = argparse . RawDescriptionHelpFormatter , add_help = 0 , ) \n    parser . add_argument ( \"-h\" , \"--help\" , dest = \"help\" , action = \"store_true\" , default = 0 , ) \n    parser . add_argument ( \"--sowpods\" , dest = \"sowpods\" , action = \"store_true\" , default = 0 , ) \n    parser . add_argument ( \"--length\" , \"-l\" , dest = \"length\" , action = \"store_true\" , default = 0 , ) \n    parser . add_argument ( \"--starts-with\" , \"-s\" , dest = \"starts_with\" , metavar = \"chars\" , default = \"\" , nargs = 1 , type = str , ) \n    parser . add_argument ( \"--ends-with\" , \"-e\" , dest = \"ends_with\" , metavar = \"chars\" , default = \"\" , nargs = 1 , type = str , ) \n    parser . add_argument ( \"--version\" , \"-v\" , action = \"version\" , version = \"Nagaram {0} (Released: {1})\" . format ( nagaram . __version__ , nagaram . __release_date__ , ) ) \n    parser . add_argument ( dest = \"wordlist\" , metavar = \"letters to find anagrams with (? for anything, _ for blanks)\" , nargs = argparse . REMAINDER , ) \n    settings = parser . parse_args ( args ) \n    if settings . help : \n        raise SystemExit ( nagaram . __doc__ . strip ( ) ) \n    if not settings . wordlist : \n        raise SystemExit ( parser . print_usage ( ) ) \n    if settings . starts_with : \n        settings . starts_with = settings . starts_with [ 0 ] \n    if settings . ends_with : \n        settings . ends_with = settings . ends_with [ 0 ] \n    return ( settings . wordlist , settings . sowpods , settings . length , settings . starts_with , settings . ends_with ) "}
{"13264": "\ndef dataReceived ( self , data ) : \n    self . _unprocessed_data . enqueue ( data ) \n    while 1 : \n        if len ( self . _unprocessed_data ) < self . _header . size : \n            return \n        hdr_data = self . _unprocessed_data . peek ( self . _header . size ) \n        packet_length , typekey = self . _header . unpack ( hdr_data ) \n        total_length = self . _header . size + packet_length \n        if len ( self . _unprocessed_data ) < total_length : \n            return \n        self . _unprocessed_data . drop ( self . _header . size ) \n        packet = self . _unprocessed_data . dequeue ( packet_length ) \n        self . _start_receive = None \n        typename = self . _type_register . get ( typekey , None ) \n        if typename is None : \n            self . on_unregistered_type ( typekey , packet ) \n        else : \n            self . packet_received ( typename , packet ) "}
{"13271": "\ndef verify ( self , ** kwargs ) : \n    super ( MetadataStatement , self ) . verify ( ** kwargs ) \n    if \"signing_keys\" in self : \n        if 'signing_keys_uri' in self : \n            raise VerificationError ( 'You can only have one of \"signing_keys\" and ' '\"signing_keys_uri\" in a metadata statement' ) \n        else : \n            kj = KeyJar ( ) \n            try : \n                kj . import_jwks ( self [ 'signing_keys' ] , '' ) \n            except Exception : \n                raise VerificationError ( '\"signing_keys\" not a proper JWKS' ) \n    if \"metadata_statements\" in self and \"metadata_statement_uris\" in self : \n        s = set ( self [ 'metadata_statements' ] . keys ( ) ) \n        t = set ( self [ 'metadata_statement_uris' ] . keys ( ) ) \n        if s . intersection ( t ) : \n            raise VerificationError ( 'You should not have the same key in \"metadata_statements\" ' 'and in \"metadata_statement_uris\"' ) \n    return 1 "}
{"13288": "\ndef word_score ( word , input_letters , questions = 0 ) : \n    score = 0 \n    bingo = 0 \n    filled_by_blanks = [ ] \n    rack = list ( input_letters ) \n    for letter in word : \n        if letter in rack : \n            bingo += 1 \n            score += letter_score ( letter ) \n            rack . remove ( letter ) \n        else : \n            filled_by_blanks . append ( letter_score ( letter ) ) \n    for blank_score in sorted ( filled_by_blanks , reverse = 1 ) : \n        if questions > 0 : \n            score += blank_score \n            questions -= 1 \n    if bingo > 6 : \n        score += 50 \n    return score "}
{"13289": "\ndef word_list ( sowpods = 0 , start = \"\" , end = \"\" ) : \n    location = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \"wordlists\" , ) \n    if sowpods : \n        filename = \"sowpods.txt\" \n    else : \n        filename = \"twl.txt\" \n    filepath = os . path . join ( location , filename ) \n    with open ( filepath ) as wordfile : \n        for word in wordfile . readlines ( ) : \n            word = word . strip ( ) \n            if start and end and word . startswith ( start ) and word . endswith ( end ) : \n                yield word \n            elif start and word . startswith ( start ) and not end : \n                yield word \n            elif end and word . endswith ( end ) and not start : \n                yield word \n            elif not start and not end : \n                yield word "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9 , \"b\" : 2 , \"c\" : 2 , \"d\" : 4 , \"e\" : 12 , \"f\" : 2 , \"g\" : 3 , \"h\" : 2 , \"i\" : 9 , \"j\" : 1 , \"k\" : 1 , \"l\" : 4 , \"m\" : 2 , \"n\" : 6 , \"o\" : 8 , \"p\" : 2 , \"q\" : 1 , \"r\" : 6 , \"s\" : 4 , \"t\" : 6 , \"u\" : 4 , \"v\" : 2 , \"w\" : 2 , \"x\" : 1 , \"y\" : 2 , \"z\" : 1 , \"_\" : 2 , } \n    for letter in word : \n        if letter == \"?\" : \n            continue \n        try : \n            letters_in_bag [ letter ] -= 1 \n        except KeyError : \n            return 0 \n        if letters_in_bag [ letter ] < 0 : \n            letters_in_bag [ \"_\" ] -= 1 \n            if letters_in_bag [ \"_\" ] < 0 : \n                return 0 \n    return 1 "}
{"13302": "\ndef nova_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = nova_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in nova_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = nova_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13303": "\ndef cinder_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = cinder_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in cinder_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = cinder_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13304": "\ndef neutron_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = neutron_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in neutron_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = neutron_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13305": "\ndef glance_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = glance_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in glance_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = glance_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13306": "\ndef swift_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = swift_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in swift_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = swift_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13307": "\ndef keystone_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = keystone_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in keystone_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = keystone_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13308": "\ndef heat_process ( body , message ) : \n    event_type = body [ 'event_type' ] \n    process = heat_customer_process . get ( event_type ) \n    if process is not None : \n        process ( body , message ) \n    else : \n        matched = 0 \n        process_wildcard = None \n        for pattern in heat_customer_process_wildcard . keys ( ) : \n            if pattern . match ( event_type ) : \n                process_wildcard = heat_customer_process_wildcard . get ( pattern ) \n                matched = 1 \n                break \n        if matched : \n            process_wildcard ( body , message ) \n        else : \n            default_process ( body , message ) \n    message . ack ( ) "}
{"13336": "\ndef copytree ( src , dst , symlinks = 1 ) : \n    from shutil import copy2 , Error , copystat \n    names = os . listdir ( src ) \n    if not Path ( dst ) . exists ( ) : \n        os . makedirs ( dst ) \n    errors = [ ] \n    for name in names : \n        srcname = os . path . join ( src , name ) \n        dstname = os . path . join ( dst , name ) \n        try : \n            if symlinks and os . path . islink ( srcname ) : \n                linkto = os . readlink ( srcname ) \n                os . symlink ( linkto , dstname ) \n            elif os . path . isdir ( srcname ) : \n                copytree ( srcname , dstname , symlinks ) \n            else : \n                copy2 ( srcname , dstname ) \n        except OSError as why : \n            errors . append ( ( srcname , dstname , str ( why ) ) ) \n        except Error as err : \n            errors . extend ( err . args [ 0 ] ) \n    try : \n        copystat ( src , dst ) \n    except OSError as why : \n        if why . winerror is None : \n            errors . extend ( ( src , dst , str ( why ) ) ) \n    if errors : \n        raise Error ( errors ) "}
{"13339": "\ndef is_changed ( self , item ) : \n    fname = os . path . join ( self . fdir , item ) \n    if os . path . isfile ( fname ) : \n        mtime = self . get_mtime ( fname ) \n        try : \n            _ftime = self . fmtime [ item ] \n        except KeyError : \n            self . fmtime [ item ] = mtime \n            return 1 \n        if mtime > _ftime : \n            self . fmtime [ item ] = mtime \n            return 1 \n        else : \n            return 0 \n    else : \n        logger . error ( 'Could not access {}' . format ( fname ) ) \n        raise KeyError ( item ) "}
{"13341": "\ndef clear ( self ) : \n    if not os . path . isdir ( self . fdir ) : \n        os . makedirs ( self . fdir , exist_ok = 1 ) \n        return \n    for f in os . listdir ( self . fdir ) : \n        del self [ f ] "}
{"13342": "\ndef scrape ( ctx , url ) : \n    data = load_feed ( url ) \n    feed = data [ 'feed' ] \n    entries = data [ 'entries' ] \n    _type = 'community' \n    country = 'Czech Republic' \n    for entry in entries : \n        _id = sluggify ( entry [ 'id' ] ) \n        city = entry [ 'tags' ] [ 0 ] [ 'term' ] \n        landing = entry [ 'link' ] \n        start_time = dt_normalize ( entry [ 'published_parsed' ] , local_tz = 1 ) \n        title = entry [ 'title' ] \n        summary = entry [ 'summary' ] \n        link = entry [ 'link' ] \n        ipdb . set_trace ( ) "}
{"13344": "\ndef has_changed ( self ) : \n    request = urllib_request . Request ( self . url ) \n    request . get_method = lambda : 'HEAD' \n    response = urllib_request . urlopen ( request ) \n    information = response . info ( ) \n    if 'Last-Modified' in information : \n        last_modified = information [ 'Last-Modified' ] \n        if last_modified == self . image_last_modified : \n            return 0 \n    self . image_last_modified = last_modified \n    return 1 "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ 1 : ] \n    if takes_context : \n        if 'context' in params [ : 1 ] : \n            params = params [ 1 : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = 0 \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if len ( bits ) > 1 and bits [ - 2 ] == 'as' : \n        output_var = bits [ - 1 ] \n        if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > 0 : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            elif kw in handled_params : \n                raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                kwargs [ str ( kw ) ] = var \n                kwarg_found = 1 \n                handled_params . append ( kw ) \n        else : \n            if kwarg_found : \n                raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n            else : \n                args . append ( bit ) \n                try : \n                    handled_params . append ( unhandled_params . pop ( 0 ) ) \n                except IndexError : \n                    if not takes_var_args : \n                        raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if len ( unhandled_params ) == 1 : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ 0 ] ) ) \n    elif len ( unhandled_params ) > 1 : \n        raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13346": "\ndef findCaller ( self , stack_info = 0 ) : \n    f = logging . currentframe ( ) \n    if f is not None : \n        f = f . f_back \n    rv = \"(unknown file)\" , 0 , \"(unknown function)\" \n    while hasattr ( f , \"f_code\" ) : \n        co = f . f_code \n        filename = os . path . normcase ( co . co_filename ) \n        if filename == logging . _srcfile or filename == self . _srcfile : \n            f = f . f_back \n            continue \n        rv = ( co . co_filename , f . f_lineno , co . co_name ) \n        if stack_info : \n            sio = io . StringIO ( ) \n            sio . write ( 'Stack (most recent call last):\\n' ) \n            traceback . print_stack ( f , file = sio ) \n            sinfo = sio . getvalue ( ) \n            if sinfo [ - 1 ] == '\\n' : \n                sinfo = sinfo [ : - 1 ] \n            sio . close ( ) \n        break \n    return rv "}
{"13350": "\ndef is_contained_in ( pe_pe , root ) : \n    if not pe_pe : \n        return 0 \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    ep_pkg = one ( pe_pe ) . EP_PKG [ 8000 ] ( ) \n    c_c = one ( pe_pe ) . C_C [ 8003 ] ( ) \n    if root in [ ep_pkg , c_c ] : \n        return 1 \n    elif is_contained_in ( ep_pkg , root ) : \n        return 1 \n    elif is_contained_in ( c_c , root ) : \n        return 1 \n    else : \n        return 0 "}
{"13351": "\ndef is_global ( pe_pe ) : \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    if one ( pe_pe ) . C_C [ 8003 ] ( ) : \n        return 0 \n    pe_pe = one ( pe_pe ) . EP_PKG [ 8000 ] . PE_PE [ 8001 ] ( ) \n    if not pe_pe : \n        return 1 \n    return is_global ( pe_pe ) "}
{"13361": "\ndef mk_class ( m , o_obj , derived_attributes = 0 ) : \n    first_filter = lambda selected : not one ( selected ) . O_ATTR [ 103 , 'succeeds' ] ( ) \n    o_attr = one ( o_obj ) . O_ATTR [ 102 ] ( first_filter ) \n    attributes = list ( ) \n    while o_attr : \n        s_dt = get_attribute_type ( o_attr ) \n        ty = _get_data_type_name ( s_dt ) \n        if not derived_attributes and one ( o_attr ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n            pass \n        elif not ty : \n            logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o_obj . Key_Lett , o_attr . Name ) ) \n        else : \n            attributes . append ( ( o_attr . Name , ty ) ) \n        o_attr = one ( o_attr ) . O_ATTR [ 103 , 'precedes' ] ( ) \n    metaclass = m . define_class ( o_obj . Key_Lett , list ( attributes ) , o_obj . Descrip ) \n    for o_id in many ( o_obj ) . O_ID [ 104 ] ( ) : \n        o_oida = many ( o_id ) . O_OIDA [ 105 ] ( ) \n        o_attrs = many ( o_oida ) . O_ATTR [ 105 ] ( ) \n        if not derived_attributes and one ( o_attrs ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n            logger . warning ( 'Omitting unique identifier %s.I%d' % ( o_obj . Key_Lett , o_id . Oid_ID + 1 ) ) \n            continue \n        names = [ o_attr . Name for o_attr in o_attrs ] \n        m . define_unique_identifier ( o_obj . Key_Lett , o_id . Oid_ID + 1 , * names ) \n    for o_tfr in many ( o_obj ) . O_TFR [ 115 ] ( ) : \n        fn = mk_operation ( metaclass , o_tfr ) \n        setattr ( metaclass . clazz , o_tfr . Name , fn ) \n    for o_dbattr in many ( o_obj ) . O_ATTR [ 102 ] . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n        o_attr = one ( o_dbattr ) . O_BATTR [ 107 ] . O_ATTR [ 106 ] ( ) \n        fn = mk_derived_attribute ( metaclass , o_dbattr ) \n        setattr ( metaclass . clazz , o_attr . Name , fn ) \n    return metaclass "}
{"13363": "\ndef mk_linked_association ( m , r_assoc ) : \n    r_rel = one ( r_assoc ) . R_REL [ 206 ] ( ) \n    r_rgo = one ( r_assoc ) . R_ASSR [ 211 ] . R_RGO [ 205 ] ( ) \n    source_o_obj = one ( r_rgo ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) \n    def _mk_assoc ( side1 , side2 ) : \n        r_rto = one ( side1 ) . R_RTO [ 204 ] ( ) \n        target_o_obj = one ( r_rto ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) \n        source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) \n        if side1 . Obj_ID != side2 . Obj_ID : \n            source_phrase = target_phrase = '' \n        else : \n            source_phrase = side1 . Txt_Phrs \n            target_phrase = side2 . Txt_Phrs \n        m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = side2 . Cond , target_conditional = 0 , source_phrase = source_phrase , target_phrase = target_phrase , source_many = side2 . Mult , target_many = 0 ) \n    r_aone = one ( r_assoc ) . R_AONE [ 209 ] ( ) \n    r_aoth = one ( r_assoc ) . R_AOTH [ 210 ] ( ) \n    _mk_assoc ( r_aone , r_aoth ) \n    _mk_assoc ( r_aoth , r_aone ) "}
{"13365": "\ndef mk_component ( bp_model , c_c = None , derived_attributes = 0 ) : \n    target = Domain ( ) \n    c_c_filt = lambda sel : c_c is None or is_contained_in ( sel , c_c ) \n    for o_obj in bp_model . select_many ( 'O_OBJ' , c_c_filt ) : \n        mk_class ( target , o_obj , derived_attributes ) \n    for r_rel in bp_model . select_many ( 'R_REL' , c_c_filt ) : \n        mk_association ( target , r_rel ) \n    for s_sync in bp_model . select_many ( 'S_SYNC' , c_c_filt ) : \n        fn = mk_function ( target , s_sync ) \n        target . add_symbol ( s_sync . Name , fn ) \n    for s_dt in bp_model . select_many ( 'S_DT' , c_c_filt ) : \n        s_edt = one ( s_dt ) . S_EDT [ 17 ] ( ) \n        if s_edt : \n            enum = mk_enum ( s_edt ) \n            target . add_symbol ( s_dt . Name , enum ) \n    for cnst_csp in bp_model . select_many ( 'CNST_CSP' , c_c_filt ) : \n        for cnst_syc in many ( cnst_csp ) . CNST_SYC [ 1504 ] ( ) : \n            value = mk_constant ( cnst_syc ) \n            target . add_symbol ( cnst_syc . Name , value ) \n    for ass in target . associations : \n        ass . formalize ( ) \n    for s_ee in bp_model . select_many ( 'S_EE' , c_c_filt ) : \n        if s_ee . Key_Lett in [ 'LOG' , 'ARCH' , 'TIM' , 'NVS' , 'PERSIST' ] : \n            target . add_symbol ( s_ee . Key_Lett , getattr ( builtin_ee , s_ee . Key_Lett ) ) \n        else : \n            ee = mk_external_entity ( target , s_ee ) \n            target . add_symbol ( s_ee . Key_Lett , ee ) \n    return target "}
{"13366": "\ndef work ( self , socket , call , args , kwargs , topics = ( ) ) : \n    task_id = uuid4_bytes ( ) \n    reply_socket , topics = self . replier ( socket , topics , call . reply_to ) \n    if reply_socket : \n        channel = ( call . call_id , task_id , topics ) \n    else : \n        channel = ( None , None , None ) \n    f , rpc_spec = self . find_call_target ( call ) \n    if rpc_spec . reject_if . __get__ ( self . app ) ( call , topics ) : \n        reply_socket and self . reject ( reply_socket , call . call_id , topics ) \n        return \n    reply_socket and self . accept ( reply_socket , channel ) \n    success = 0 \n    with self . catch_exceptions ( ) : \n        try : \n            val = self . call ( call , args , kwargs , f , rpc_spec ) \n        except : \n            exc_info = sys . exc_info ( ) \n            self . raise_ ( reply_socket , channel , exc_info ) \n            reraise ( * exc_info ) \n        success = 1 \n    if not success : \n        return \n    if isinstance ( val , Iterator ) : \n        vals = val \n        with self . catch_exceptions ( ) : \n            try : \n                try : \n                    val = next ( vals ) \n                except StopIteration : \n                    pass \n                else : \n                    self . send_reply ( reply_socket , YIELD , val , * channel ) \n                    for val in vals : \n                        self . send_reply ( reply_socket , YIELD , val , * channel ) \n                self . send_reply ( reply_socket , BREAK , None , * channel ) \n            except : \n                exc_info = sys . exc_info ( ) \n                self . raise_ ( reply_socket , channel , exc_info ) \n                reraise ( * exc_info ) \n    else : \n        self . send_reply ( reply_socket , RETURN , val , * channel ) "}
{"13370": "\ndef _call_wait ( self , hints , name , args , kwargs , topics = ( ) , raw = 0 , limit = None , retry = 0 , max_retries = None ) : \n    col = self . collector \n    if not col . is_running ( ) : \n        col . start ( ) \n    call_id = uuid4_bytes ( ) \n    reply_to = ( DUPLEX if self . socket is col . socket else col . topic ) \n    header = self . _make_header ( name , call_id , reply_to , hints ) \n    payload = self . _pack ( args , kwargs , raw ) \n    def send_call ( ) : \n        try : \n            safe ( send , self . socket , header , payload , topics , zmq . NOBLOCK ) \n        except zmq . Again : \n            raise Undelivered ( 'emission was not delivered' ) \n    col . prepare ( call_id , self , name , args , kwargs ) \n    send_call ( ) \n    return col . establish ( call_id , self . timeout , limit , send_call if retry else None , max_retries = max_retries ) "}
{"13371": "\ndef establish ( self , call_id , timeout , limit = None , retry = None , max_retries = None ) : \n    rejected = 0 \n    retried = 0 \n    results = [ ] \n    result_queue = self . result_queues [ call_id ] \n    try : \n        with Timeout ( timeout , 0 ) : \n            while 1 : \n                result = result_queue . get ( ) \n                if result is None : \n                    rejected += 1 \n                    if retry is not None : \n                        if retried == max_retries : \n                            break \n                        retry ( ) \n                        retried += 1 \n                    continue \n                results . append ( result ) \n                if len ( results ) == limit : \n                    break \n    finally : \n        del result_queue \n        self . remove_result_queue ( call_id ) \n    if not results : \n        if rejected : \n            raise Rejected ( '%d workers rejected' % rejected if rejected != 1 else 'A worker rejected' ) \n        else : \n            raise WorkerNotFound ( 'failed to find worker' ) \n    return results "}
{"13374": "\ndef deserialize_value ( ty , value ) : \n    uty = ty . upper ( ) \n    if uty == 'BOOLEAN' : \n        if value . isdigit ( ) : \n            return bool ( int ( value ) ) \n        elif value . upper ( ) == 'FALSE' : \n            return 0 \n        elif value . upper ( ) == 'TRUE' : \n            return 1 \n        else : \n            return None \n    elif uty == 'INTEGER' : \n        if '\"' in value : \n            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n        else : \n            return int ( value ) \n    elif uty == 'REAL' : \n        return float ( value ) \n    elif uty == 'STRING' : \n        return value [ 1 : - 1 ] . replace ( \"''\" , \"'\" ) \n    elif uty == 'UNIQUE_ID' : \n        if '\"' in value : \n            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n        else : \n            return int ( value ) "}
{"13379": "\ndef put ( self , items , indexes = 1 ) : \n    actions = [ ] \n    for cid , fc in items : \n        idxs = defaultdict ( list ) \n        if indexes : \n            for fname in self . indexed_features : \n                if fname in fc : \n                    idxs [ fname_to_idx_name ( fname ) ] . extend ( fc [ fname ] ) \n            for fname in self . fulltext_indexed_features : \n                if fname not in fc : \n                    continue \n                if isinstance ( fc [ fname ] , basestring ) : \n                    idxs [ fname_to_full_idx_name ( fname ) ] = fc [ fname ] \n                else : \n                    idxs [ fname_to_full_idx_name ( fname ) ] . extend ( fc [ fname ] ) \n        actions . append ( { '_index' : self . index , '_type' : self . type , '_id' : eid ( cid ) , '_op_type' : 'index' , '_source' : dict ( idxs , ** { 'fc' : self . fc_to_dict ( fc ) , } ) , } ) \n    bulk ( self . conn , actions , timeout = 60 , request_timeout = 60 ) "}
{"13381": "\ndef delete_all ( self ) : \n    try : \n        self . conn . indices . delete_mapping ( index = self . index , doc_type = self . type ) \n    except TransportError : \n        logger . warn ( 'type %r in index %r already deleted' , self . index , self . type , exc_info = 1 ) "}
{"13384": "\ndef scan_ids ( self , * key_ranges , ** kwargs ) : \n    kwargs [ 'feature_names' ] = 0 \n    for hit in self . _scan ( * key_ranges , ** kwargs ) : \n        yield did ( hit [ '_id' ] ) "}
{"13386": "\ndef scan_prefix_ids ( self , prefix ) : \n    resp = self . _scan_prefix ( prefix , feature_names = 0 ) \n    for hit in resp : \n        yield did ( hit [ '_id' ] ) "}
{"13387": "\ndef fulltext_scan ( self , query_id = None , query_fc = None , feature_names = None , preserve_order = 1 , indexes = None ) : \n    it = self . _fulltext_scan ( query_id , query_fc , feature_names = feature_names , preserve_order = preserve_order , indexes = indexes ) \n    for hit in it : \n        fc = self . fc_from_dict ( hit [ '_source' ] [ 'fc' ] ) \n        yield hit [ '_score' ] , did ( hit [ '_id' ] ) , fc "}
{"13388": "\ndef fulltext_scan_ids ( self , query_id = None , query_fc = None , preserve_order = 1 , indexes = None ) : \n    it = self . _fulltext_scan ( query_id , query_fc , feature_names = 0 , preserve_order = preserve_order , indexes = indexes ) \n    for hit in it : \n        yield hit [ '_score' ] , did ( hit [ '_id' ] ) "}
{"13390": "\ndef keyword_scan_ids ( self , query_id = None , query_fc = None ) : \n    it = self . _keyword_scan ( query_id , query_fc , feature_names = 0 ) \n    for hit in it : \n        yield did ( hit [ '_id' ] ) "}
{"13391": "\ndef index_scan_ids ( self , fname , val ) : \n    disj = [ ] \n    for fname2 in self . indexes [ fname ] [ 'feature_names' ] : \n        disj . append ( { 'term' : { fname_to_idx_name ( fname2 ) : val } } ) \n    query = { 'constant_score' : { 'filter' : { 'or' : disj } , } , } \n    hits = scan ( self . conn , index = self . index , doc_type = self . type , query = { '_source' : 0 , 'query' : query , } ) \n    for hit in hits : \n        yield did ( hit [ '_id' ] ) "}
{"13392": "\ndef _source ( self , feature_names ) : \n    if feature_names is None : \n        return 1 \n    elif isinstance ( feature_names , bool ) : \n        return feature_names \n    else : \n        return map ( lambda n : 'fc.' + n , feature_names ) "}
{"13394": "\ndef _create_index ( self ) : \n    try : \n        self . conn . indices . create ( index = self . index , timeout = 60 , request_timeout = 60 , body = { 'settings' : { 'number_of_shards' : self . shards , 'number_of_replicas' : self . replicas , } , } ) \n    except TransportError : \n        logger . warn ( 'index already exists? OK' , exc_info = 1 ) \n        pass "}
{"13395": "\ndef _create_mappings ( self ) : \n    self . conn . indices . put_mapping ( index = self . index , doc_type = self . type , timeout = 60 , request_timeout = 60 , body = { self . type : { 'dynamic_templates' : [ { 'default_no_analyze_fc' : { 'match' : 'fc.*' , 'mapping' : { 'index' : 'no' } , } , } ] , '_all' : { 'enabled' : 0 , } , '_id' : { 'index' : 'not_analyzed' , } , 'properties' : self . _get_index_mappings ( ) , } , } ) \n    self . conn . cluster . health ( index = self . index , wait_for_status = 'yellow' ) "}
{"13396": "\ndef _get_index_mappings ( self ) : \n    maps = { } \n    for fname in self . indexed_features : \n        config = self . indexes . get ( fname , { } ) \n        print ( fname , config ) \n        maps [ fname_to_idx_name ( fname ) ] = { 'type' : config . get ( 'es_index_type' , 'integer' ) , 'store' : 0 , 'index' : 'not_analyzed' , } \n    for fname in self . fulltext_indexed_features : \n        maps [ fname_to_full_idx_name ( fname ) ] = { 'type' : 'string' , 'store' : 0 , 'index' : 'analyzed' , } \n    return maps "}
{"13424": "\ndef check_pypi_name ( pypi_package_name , pypi_registry_host = None ) : \n    if pypi_registry_host is None : \n        pypi_registry_host = 'pypi.python.org' \n    receive_buffer = bytearray ( b'------------' ) \n    context = ssl . create_default_context ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD /simple/\" , pypi_package_name . encode ( 'ascii' ) , b\"/ HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return 1 \n    elif b'HTTP/1.1 404' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return 0 \n    remaining_bytes = ssl_http_socket . recv ( 2048 ) \n    redirect_path_location_start = remaining_bytes . find ( b'Location:' ) + 10 \n    redirect_path_location_end = remaining_bytes . find ( b'\\r\\n' , redirect_path_location_start ) \n    redirect_path = remaining_bytes [ redirect_path_location_start : redirect_path_location_end ] + b'/' \n    ssl_http_socket . shutdown ( 1 ) \n    ssl_http_socket . close ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD \" , redirect_path , b\" HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        return 1 \n    elif b'HTTP/1.1 404' in receive_buffer : \n        return 0 \n    else : \n        NotImplementedError ( 'A definitive answer was not found by primary or secondary lookups.' ) "}
{"13453": "\ndef create_queue ( self , name , strict = 1 , auto_delete = 0 , auto_delete_timeout = 0 ) : \n    content = { \"_object_id\" : { \"_object_name\" : self . object_name } , \"_method_name\" : \"create\" , \"_arguments\" : { \"type\" : \"queue\" , \"name\" : name , \"strict\" : strict , \"properties\" : { \"auto-delete\" : auto_delete , \"qpid.auto_delete_timeout\" : auto_delete_timeout } } } \n    logger . debug ( \"Message content -> {0}\" . format ( content ) ) \n    return content , self . method_properties "}
{"13460": "\ndef text_visible ( self ) : \n    words = self . read ( ) . split ( ) \n    for word in words : \n        if word . lstrip ( '-' ) . replace ( '.' , '' , 1 ) . isdigit ( ) : \n            return 1 \n        if word . isalpha ( ) and ( len ( word ) > 1 or len ( word ) <= 20 ) : \n            return 1 \n    return 0 "}
{"13461": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , default = 1 , help = \"increase debug logging level\" ) \n    parser . add_option ( \"-f\" , \"--function\" , dest = 'function' , action = \"store\" , help = \"invoke function named NAME\" , metavar = 'NAME' ) \n    parser . add_option ( \"-c\" , \"--component\" , dest = 'component' , action = \"store\" , help = \"look for the function in a component named NAME\" , metavar = 'NAME' , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if len ( args ) == 0 or not opts . function : \n        parser . print_help ( ) \n        sys . exit ( 1 ) \n    levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    from bridgepoint import ooaofooa \n    mm = ooaofooa . load_metamodel ( args ) \n    c_c = mm . select_any ( 'C_C' , where ( Name = opts . component ) ) \n    domain = ooaofooa . mk_component ( mm , c_c , derived_attributes = 0 ) \n    func = domain . find_symbol ( opts . function ) \n    return func ( ) "}
{"13462": "\ndef serialize_value ( value , ty ) : \n    ty = ty . upper ( ) \n    null_value = { 'BOOLEAN' : 0 , 'INTEGER' : 0 , 'REAL' : 0.0 , 'STRING' : '' , 'UNIQUE_ID' : 0 } \n    transfer_fn = { 'BOOLEAN' : lambda v : '%d' % int ( v ) , 'INTEGER' : lambda v : '%d' % v , 'REAL' : lambda v : '%f' % v , 'STRING' : lambda v : \"'%s'\" % v . replace ( \"'\" , \"''\" ) , 'UNIQUE_ID' : lambda v : '\"%s\"' % uuid . UUID ( int = v ) } \n    if value is None : \n        value = null_value [ ty ] \n    return transfer_fn [ ty ] ( value ) "}
{"13465": "\ndef main ( ) : \n    parser = ArgumentParser ( description = \"search files using n-grams\" ) \n    parser . add_argument ( '--path' , dest = 'path' , help = \"where to search\" , nargs = 1 , action = \"store\" , default = getcwd ( ) ) \n    parser . add_argument ( '--update' , dest = 'update' , help = \"update the index\" , action = 'store_true' , default = 1 ) \n    parser . add_argument ( '--filetype' , dest = 'filetype' , help = \"any, images, documents, code, audio, video\" , nargs = 1 , action = \"store\" , default = [ \"any\" ] ) \n    parser . add_argument ( '--verbose' , dest = 'verbose' , help = \"extended output\" , action = 'store_true' , default = 0 ) \n    parser . add_argument ( '--results' , dest = 'results' , help = \"number of results to display\" , action = \"store\" , default = 10 ) \n    parser . add_argument ( 'query' , nargs = '+' , help = \"what to search\" , action = \"store\" ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        verbose = 2 \n        pprint ( args ) \n    else : \n        verbose = 0 \n    query = args . query [ 0 ] \n    for arg in args . query [ 1 : ] : \n        query = query + \" \" + arg \n    slb = min ( [ len ( w ) for w in query . split ( \" \" ) ] ) \n    files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) \n    index = Index ( files , slb = slb , verbose = verbose ) \n    results = index . search ( query , verbose = verbose ) \n    Handler ( results , results_number = int ( args . results ) ) "}
{"13466": "\ndef search ( self , query , verbose = 0 ) : \n    if verbose > 0 : \n        print ( \"searching \" + query ) \n    query = query . lower ( ) \n    qgram = ng ( query , self . slb ) \n    qocument = set ( ) \n    for q in qgram : \n        if q in self . ngrams . keys ( ) : \n            for i in self . ngrams [ q ] : \n                qocument . add ( i ) \n    self . qocument = qocument \n    results = { } \n    for i in qocument : \n        for j in self . D [ i ] . keys ( ) : \n            if not j in results . keys ( ) : \n                results [ j ] = 0 \n            results [ j ] = results [ j ] + self . D [ i ] [ j ] \n    sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( 1 ) , reverse = 1 ) \n    return [ self . elements [ f [ 0 ] ] for f in sorted_results ] "}
{"13495": "\ndef create_inputhook_qt4 ( mgr , app = None ) : \n    if app is None : \n        app = QtCore . QCoreApplication . instance ( ) \n        if app is None : \n            app = QtGui . QApplication ( [ \" \" ] ) \n    ip = InteractiveShell . instance ( ) \n    if hasattr ( ip , '_inputhook_qt4' ) : \n        return app , ip . _inputhook_qt4 \n    got_kbdint = [ 0 ] \n    def inputhook_qt4 ( ) : \n        try : \n            allow_CTRL_C ( ) \n            app = QtCore . QCoreApplication . instance ( ) \n            if not app : \n                return 0 \n            app . processEvents ( QtCore . QEventLoop . AllEvents , 300 ) \n            if not stdin_ready ( ) : \n                timer = QtCore . QTimer ( ) \n                timer . timeout . connect ( app . quit ) \n                while not stdin_ready ( ) : \n                    timer . start ( 50 ) \n                    app . exec_ ( ) \n                    timer . stop ( ) \n        except KeyboardInterrupt : \n            ignore_CTRL_C ( ) \n            got_kbdint [ 0 ] = 1 \n            print ( \"\\nKeyboardInterrupt - Ctrl-C again for new prompt\" ) \n            mgr . clear_inputhook ( ) \n        except : \n            ignore_CTRL_C ( ) \n            from traceback import print_exc \n            print_exc ( ) \n            print ( \"Got exception from inputhook_qt4, unregistering.\" ) \n            mgr . clear_inputhook ( ) \n        finally : \n            allow_CTRL_C ( ) \n        return 0 \n    def preprompthook_qt4 ( ishell ) : \n        if got_kbdint [ 0 ] : \n            mgr . set_inputhook ( inputhook_qt4 ) \n        got_kbdint [ 0 ] = 0 \n    ip . _inputhook_qt4 = inputhook_qt4 \n    ip . set_hook ( 'pre_prompt_hook' , preprompthook_qt4 ) \n    return app , inputhook_qt4 "}
{"13501": "\ndef call ( self , url , method = None , args = None ) : \n    if not args : \n        args = { } \n    if sys . version_info . major == 3 : \n        data = urllib . parse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urllib . parse . parse_qs ( data . query , keep_blank_values = 1 ) ) \n    elif sys . version_info . major == 2 : \n        data = urlparse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urlparse . parse_qs ( data . query , keep_blank_values = 1 ) ) \n    for elem in self . _data_store : \n        pattern = elem [ 'pattern' ] \n        function = elem [ 'function' ] \n        _method = elem [ 'method' ] \n        type_cast = elem [ 'type_cast' ] \n        result = re . match ( pattern , path ) \n        if result and _method == method : \n            _args = dict ( _args , ** result . groupdict ( ) ) \n            for key , val in _args . items ( ) : \n                if isinstance ( _args [ key ] , list ) and len ( _args [ key ] ) == 1 : \n                    _args [ key ] = _args [ key ] [ 0 ] \n            for key , val in type_cast . items ( ) : \n                if key not in _args : \n                    continue \n                if not _args [ key ] : \n                    continue \n                if isinstance ( _args [ key ] , list ) : \n                    for i , _val in enumerate ( _args [ key ] ) : \n                        _args [ key ] [ i ] = self . _cast ( _val , val ) \n                else : \n                    _args [ key ] = self . _cast ( _args [ key ] , val ) \n            requiered_args = self . _get_function_args ( function ) \n            for key , val in args . items ( ) : \n                if key in requiered_args : \n                    _args [ key ] = val \n            return function ( ** _args ) \n    return None "}
{"13502": "\ndef execute ( self , source = None , hidden = 0 , interactive = 0 ) : \n    if not hidden : \n        history = self . input_buffer if source is None else source \n    executed = super ( HistoryConsoleWidget , self ) . execute ( source , hidden , interactive ) \n    if executed and not hidden : \n        history = history . rstrip ( ) \n        if history and ( not self . _history or self . _history [ - 1 ] != history ) : \n            self . _history . append ( history ) \n        self . _history_edits = { } \n        self . _history_index = len ( self . _history ) \n    return executed "}
{"13503": "\ndef _up_pressed ( self , shift_modifier ) : \n    prompt_cursor = self . _get_prompt_cursor ( ) \n    if self . _get_cursor ( ) . blockNumber ( ) == prompt_cursor . blockNumber ( ) : \n        if self . _history_locked ( ) and not shift_modifier : \n            return 0 \n        col = self . _get_input_buffer_cursor_column ( ) \n        input_buffer = self . input_buffer \n        if self . _history_index == len ( self . _history ) or ( self . _history_prefix and col != len ( self . _history_prefix ) ) : \n            self . _history_index = len ( self . _history ) \n            self . _history_prefix = input_buffer [ : col ] \n        self . history_previous ( self . _history_prefix , as_prefix = not shift_modifier ) \n        cursor = self . _get_prompt_cursor ( ) \n        if self . _history_prefix : \n            cursor . movePosition ( QtGui . QTextCursor . Right , n = len ( self . _history_prefix ) ) \n        else : \n            cursor . movePosition ( QtGui . QTextCursor . EndOfLine ) \n        self . _set_cursor ( cursor ) \n        return 0 \n    return 1 "}
{"13504": "\ndef _down_pressed ( self , shift_modifier ) : \n    end_cursor = self . _get_end_cursor ( ) \n    if self . _get_cursor ( ) . blockNumber ( ) == end_cursor . blockNumber ( ) : \n        if self . _history_locked ( ) and not shift_modifier : \n            return 0 \n        replaced = self . history_next ( self . _history_prefix , as_prefix = not shift_modifier ) \n        if self . _history_prefix and replaced : \n            cursor = self . _get_prompt_cursor ( ) \n            cursor . movePosition ( QtGui . QTextCursor . Right , n = len ( self . _history_prefix ) ) \n            self . _set_cursor ( cursor ) \n        return 0 \n    return 1 "}
{"13505": "\ndef history_previous ( self , substring = '' , as_prefix = 1 ) : \n    index = self . _history_index \n    replace = 0 \n    while index > 0 : \n        index -= 1 \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = 1 \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13506": "\ndef history_next ( self , substring = '' , as_prefix = 1 ) : \n    index = self . _history_index \n    replace = 0 \n    while self . _history_index < len ( self . _history ) : \n        index += 1 \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = 1 \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13520": "\ndef remote_iterator ( view , name ) : \n    view . execute ( 'it%s=iter(%s)' % ( name , name ) , block = 1 ) \n    while 1 : \n        try : \n            result = view . apply_sync ( lambda x : x . next ( ) , Reference ( 'it' + name ) ) \n        except RemoteError as e : \n            if e . ename == 'StopIteration' : \n                raise StopIteration \n            else : \n                raise e \n        else : \n            yield result "}
{"13526": "\ndef _override_setuptools ( req ) : \n    if req . project_name == 'setuptools' : \n        if not len ( req . specs ) : \n            return 1 \n        for comparator , version in req . specs : \n            if comparator in [ '==' , '>=' , '>' ] : \n                if '0.7' in version : \n                    return 0 \n        return 1 \n    return 0 "}
{"13527": "\ndef add ( self , dist , entry = None , insert = 1 , replace = 0 ) : \n    if insert : \n        dist . insert_on ( self . entries , entry ) \n    if entry is None : \n        entry = dist . location \n    keys = self . entry_keys . setdefault ( entry , [ ] ) \n    keys2 = self . entry_keys . setdefault ( dist . location , [ ] ) \n    if not replace and dist . key in self . by_key : \n        return \n    self . by_key [ dist . key ] = dist \n    if dist . key not in keys : \n        keys . append ( dist . key ) \n    if dist . key not in keys2 : \n        keys2 . append ( dist . key ) \n    self . _added_new ( dist ) "}
{"13545": "\ndef update_tab_bar_visibility ( self ) : \n    if self . tab_widget . count ( ) <= 1 : \n        self . tab_widget . tabBar ( ) . setVisible ( 0 ) \n    else : \n        self . tab_widget . tabBar ( ) . setVisible ( 1 ) \n    if self . tab_widget . count ( ) == 0 : \n        self . close ( ) "}
{"13548": "\ndef add_menu_action ( self , menu , action , defer_shortcut = 0 ) : \n    menu . addAction ( action ) \n    self . addAction ( action ) \n    if defer_shortcut : \n        action . setShortcutContext ( QtCore . Qt . WidgetShortcut ) "}
{"13551": "\ndef closeEvent ( self , event ) : \n    if self . tab_widget . count ( ) == 0 : \n        event . accept ( ) \n        return \n    title = self . window ( ) . windowTitle ( ) \n    cancel = QtGui . QMessageBox . Cancel \n    okay = QtGui . QMessageBox . Ok \n    if self . confirm_exit : \n        if self . tab_widget . count ( ) > 1 : \n            msg = \"Close all tabs, stop all kernels, and Quit?\" \n        else : \n            msg = \"Close console, stop kernel, and Quit?\" \n        info = \"Kernels not started here (e.g. notebooks) will be left alone.\" \n        closeall = QtGui . QPushButton ( \"&Quit\" , self ) \n        closeall . setShortcut ( 'Q' ) \n        box = QtGui . QMessageBox ( QtGui . QMessageBox . Question , title , msg ) \n        box . setInformativeText ( info ) \n        box . addButton ( cancel ) \n        box . addButton ( closeall , QtGui . QMessageBox . YesRole ) \n        box . setDefaultButton ( closeall ) \n        box . setEscapeButton ( cancel ) \n        pixmap = QtGui . QPixmap ( self . _app . icon . pixmap ( QtCore . QSize ( 64 , 64 ) ) ) \n        box . setIconPixmap ( pixmap ) \n        reply = box . exec_ ( ) \n    else : \n        reply = okay \n    if reply == cancel : \n        event . ignore ( ) \n        return \n    if reply == okay : \n        while self . tab_widget . count ( ) >= 1 : \n            widget = self . active_frontend \n            widget . _confirm_exit = 0 \n            self . close_tab ( widget ) \n        event . accept ( ) "}
{"13553": "\ndef passwd_check ( hashed_passphrase , passphrase ) : \n    try : \n        algorithm , salt , pw_digest = hashed_passphrase . split ( ':' , 2 ) \n    except ( ValueError , TypeError ) : \n        return 0 \n    try : \n        h = hashlib . new ( algorithm ) \n    except ValueError : \n        return 0 \n    if len ( pw_digest ) == 0 : \n        return 0 \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return h . hexdigest ( ) == pw_digest "}
{"13555": "\ndef indented_short_title ( self , item ) : \n    r = \"\" \n    if hasattr ( item , 'get_absolute_url' ) : \n        r = '<input type=\"hidden\" class=\"medialibrary_file_path\" value=\"%s\" />' % item . get_absolute_url ( ) \n    editable_class = '' \n    if not getattr ( item , 'feincms_editable' , 1 ) : \n        editable_class = ' tree-item-not-editable' \n    r += '<span id=\"page_marker-%d\" class=\"page_marker%s\" style=\"width: %dpx;\">&nbsp;</span>&nbsp;' % ( item . id , editable_class , 14 + item . level * 18 ) \n    if hasattr ( item , 'short_title' ) : \n        r += item . short_title ( ) \n    else : \n        r += unicode ( item ) \n    return mark_safe ( r ) "}
{"13557": "\ndef _toggle_boolean ( self , request ) : \n    try : \n        item_id = int ( request . POST . get ( 'item_id' , None ) ) \n        attr = str ( request . POST . get ( 'attr' , None ) ) \n    except : \n        return HttpResponseBadRequest ( \"Malformed request\" ) \n    if not request . user . is_staff : \n        logging . warning ( \"Denied AJAX request by non-staff %s to toggle boolean %s for object #%s\" , request . user , attr , item_id ) \n        return HttpResponseForbidden ( \"You do not have permission to access this object\" ) \n    self . _collect_editable_booleans ( ) \n    if not self . _ajax_editable_booleans . has_key ( attr ) : \n        return HttpResponseBadRequest ( \"not a valid attribute %s\" % attr ) \n    try : \n        obj = self . model . _default_manager . get ( pk = item_id ) \n    except self . model . DoesNotExist : \n        return HttpResponseNotFound ( \"Object does not exist\" ) \n    can_change = 0 \n    if hasattr ( obj , \"user_can\" ) and obj . user_can ( request . user , change_page = 1 ) : \n        can_change = 1 \n    else : \n        can_change = self . has_change_permission ( request , obj = obj ) \n    if not can_change : \n        logging . warning ( \"Denied AJAX request by %s to toggle boolean %s for object %s\" , request . user , attr , item_id ) \n        return HttpResponseForbidden ( \"You do not have permission to access this object\" ) \n    logging . info ( \"Processing request by %s to toggle %s on %s\" , request . user , attr , obj ) \n    try : \n        before_data = self . _ajax_editable_booleans [ attr ] ( self , obj ) \n        setattr ( obj , attr , not getattr ( obj , attr ) ) \n        obj . save ( ) \n        self . _refresh_changelist_caches ( ) \n        data = self . _ajax_editable_booleans [ attr ] ( self , obj ) \n    except Exception : \n        logging . exception ( \"Unhandled exception while toggling %s on %s\" , attr , obj ) \n        return HttpResponseServerError ( \"Unable to toggle %s on %s\" % ( attr , obj ) ) \n    d = [ ] \n    for a , b in zip ( before_data , data ) : \n        if a != b : \n            d . append ( b ) \n    return HttpResponse ( json . dumps ( d ) , mimetype = \"application/json\" ) "}
{"13558": "\ndef has_change_permission ( self , request , obj = None ) : \n    if settings . TREE_EDITOR_OBJECT_PERMISSIONS : \n        opts = self . opts \n        r = request . user . has_perm ( opts . app_label + '.' + opts . get_change_permission ( ) , obj ) \n    else : \n        r = 1 \n    return r and super ( TreeEditor , self ) . has_change_permission ( request , obj ) "}
{"13559": "\ndef has_delete_permission ( self , request , obj = None ) : \n    if settings . TREE_EDITOR_OBJECT_PERMISSIONS : \n        opts = self . opts \n        r = request . user . has_perm ( opts . app_label + '.' + opts . get_delete_permission ( ) , obj ) \n    else : \n        r = 1 \n    return r and super ( TreeEditor , self ) . has_delete_permission ( request , obj ) "}
{"13571": "\ndef wait_for_kernel ( self , timeout = None ) : \n    tic = time . time ( ) \n    self . km . hb_channel . unpause ( ) \n    while 1 : \n        self . run_cell ( '1' , 0 ) \n        if self . km . hb_channel . is_beating ( ) : \n            break \n        else : \n            if timeout is not None and ( time . time ( ) - tic ) > timeout : \n                return 0 \n    return 1 "}
{"13575": "\ndef _get_format_from_style ( self , token , style ) : \n    result = QtGui . QTextCharFormat ( ) \n    for key , value in style . style_for_token ( token ) . items ( ) : \n        if value : \n            if key == 'color' : \n                result . setForeground ( self . _get_brush ( value ) ) \n            elif key == 'bgcolor' : \n                result . setBackground ( self . _get_brush ( value ) ) \n            elif key == 'bold' : \n                result . setFontWeight ( QtGui . QFont . Bold ) \n            elif key == 'italic' : \n                result . setFontItalic ( 1 ) \n            elif key == 'underline' : \n                result . setUnderlineStyle ( QtGui . QTextCharFormat . SingleUnderline ) \n            elif key == 'sans' : \n                result . setFontStyleHint ( QtGui . QFont . SansSerif ) \n            elif key == 'roman' : \n                result . setFontStyleHint ( QtGui . QFont . Times ) \n            elif key == 'mono' : \n                result . setFontStyleHint ( QtGui . QFont . TypeWriter ) \n    return result "}
{"13580": "\ndef last_blank ( src ) : \n    if not src : \n        return 0 \n    ll = src . splitlines ( ) [ - 1 ] \n    return ( ll == '' ) or ll . isspace ( ) "}
{"13581": "\ndef last_two_blanks ( src ) : \n    if not src : \n        return 0 \n    new_src = '\\n' . join ( [ '###\\n' ] + src . splitlines ( ) [ - 2 : ] ) \n    return ( bool ( last_two_blanks_re . match ( new_src ) ) or bool ( last_two_blanks_re2 . match ( new_src ) ) ) "}
{"13586": "\ndef push ( self , lines ) : \n    if self . input_mode == 'cell' : \n        self . reset ( ) \n    self . _store ( lines ) \n    source = self . source \n    self . code , self . _is_complete = None , None \n    if source . rstrip ( ) . endswith ( '\\\\' ) : \n        return 0 \n    self . _update_indent ( lines ) \n    try : \n        self . code = self . _compile ( source , symbol = \"exec\" ) \n    except ( SyntaxError , OverflowError , ValueError , TypeError , MemoryError ) : \n        self . _is_complete = 1 \n    else : \n        self . _is_complete = self . code is not None \n    return self . _is_complete "}
{"13587": "\ndef push_accepts_more ( self ) : \n    if not self . _is_complete : \n        return 1 \n    if self . indent_spaces == 0 : \n        if self . input_mode == 'line' : \n            if not self . _full_dedent : \n                return 0 \n        else : \n            try : \n                code_ast = ast . parse ( u'' . join ( self . _buffer ) ) \n            except Exception : \n                return 0 \n            else : \n                if len ( code_ast . body ) == 1 : \n                    return 0 \n    last_line = self . source . splitlines ( ) [ - 1 ] \n    return bool ( last_line and not last_line . isspace ( ) ) "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if inisp < indent_spaces : \n        indent_spaces = inisp \n        if indent_spaces <= 0 : \n            full_dedent = 1 \n    if line . rstrip ( ) [ - 1 ] == ':' : \n        indent_spaces += 4 \n    elif dedent_re . match ( line ) : \n        indent_spaces -= 4 \n        if indent_spaces <= 0 : \n            full_dedent = 1 \n    if indent_spaces < 0 : \n        indent_spaces = 0 \n    return indent_spaces , full_dedent "}
{"13591": "\ndef _handle_cell_magic ( self , lines ) : \n    self . processing_cell_magic = 1 \n    first , _ , body = lines . partition ( '\\n' ) \n    magic_name , _ , line = first . partition ( ' ' ) \n    magic_name = magic_name . lstrip ( ESC_MAGIC ) \n    self . cell_magic_parts = [ body ] \n    tpl = 'get_ipython()._run_cached_cell_magic(%r, %r)' \n    tlines = tpl % ( magic_name , line ) \n    self . _store ( tlines ) \n    self . _store ( lines , self . _buffer_raw , 'source_raw' ) \n    self . _is_complete = last_blank ( lines ) \n    return self . _is_complete "}
{"13594": "\ndef push ( self , lines ) : \n    if not lines : \n        return super ( IPythonInputSplitter , self ) . push ( lines ) \n    lines = cast_unicode ( lines , self . encoding ) \n    if lines . startswith ( '%%' ) and not ( len ( lines . splitlines ( ) ) == 1 and lines . strip ( ) . endswith ( '?' ) ) : \n        return self . _handle_cell_magic ( lines ) \n    if self . input_mode == 'line' and self . processing_cell_magic : \n        return self . _line_mode_cell_append ( lines ) \n    lines_list = lines . splitlines ( ) \n    transforms = [ transform_ipy_prompt , transform_classic_prompt , transform_help_end , transform_escaped , transform_assign_system , transform_assign_magic ] \n    changed_input_mode = 0 \n    if self . input_mode == 'cell' : \n        self . reset ( ) \n        changed_input_mode = 1 \n        saved_input_mode = 'cell' \n        self . input_mode = 'line' \n    self . _store ( lines , self . _buffer_raw , 'source_raw' ) \n    try : \n        push = super ( IPythonInputSplitter , self ) . push \n        buf = self . _buffer \n        for line in lines_list : \n            if self . _is_complete or not buf or ( buf and buf [ - 1 ] . rstrip ( ) . endswith ( ( ':' , ',' ) ) ) : \n                for f in transforms : \n                    line = f ( line ) \n            out = push ( line ) \n    finally : \n        if changed_input_mode : \n            self . input_mode = saved_input_mode \n    return out "}
{"13600": "\ndef _update_status ( self ) : \n    srun , scomp , sdead = self . _s_running , self . _s_completed , self . _s_dead \n    running , completed , dead = self . _running , self . _completed , self . _dead \n    for num , job in enumerate ( running ) : \n        stat = job . stat_code \n        if stat == srun : \n            continue \n        elif stat == scomp : \n            completed . append ( job ) \n            self . _comp_report . append ( job ) \n            running [ num ] = 0 \n        elif stat == sdead : \n            dead . append ( job ) \n            self . _dead_report . append ( job ) \n            running [ num ] = 0 \n    running [ : ] = filter ( None , running ) "}
{"13605": "\ndef _init ( self ) : \n    for attr in [ 'call' , 'strform' ] : \n        assert hasattr ( self , attr ) , \"Missing attribute <%s>\" % attr \n    self . num = None \n    self . status = BackgroundJobBase . stat_created \n    self . stat_code = BackgroundJobBase . stat_created_c \n    self . finished = 0 \n    self . result = '<BackgroundJob has not completed>' \n    try : \n        make_tb = get_ipython ( ) . InteractiveTB . text \n    except : \n        make_tb = AutoFormattedTB ( mode = 'Context' , color_scheme = 'NoColor' , tb_offset = 1 ) . text \n    self . _make_tb = lambda : make_tb ( None , None , None ) \n    self . _tb = None \n    threading . Thread . __init__ ( self ) "}
{"13615": "\ndef _check_table ( self ) : \n    cursor = self . _db . execute ( \"PRAGMA table_info(%s)\" % self . table ) \n    lines = cursor . fetchall ( ) \n    if not lines : \n        return 1 \n    types = { } \n    keys = [ ] \n    for line in lines : \n        keys . append ( line [ 1 ] ) \n        types [ line [ 1 ] ] = line [ 2 ] \n    if self . _keys != keys : \n        self . log . warn ( 'keys mismatch' ) \n        return 0 \n    for key in self . _keys : \n        if types [ key ] != self . _types [ key ] : \n            self . log . warn ( 'type mismatch: %s: %s != %s' % ( key , types [ key ] , self . _types [ key ] ) ) \n            return 0 \n    return 1 "}
{"13629": "\ndef model_verbose ( obj , capitalize = 1 ) : \n    if isinstance ( obj , ModelForm ) : \n        name = obj . _meta . model . _meta . verbose_name \n    elif isinstance ( obj , Model ) : \n        name = obj . _meta . verbose_name \n    else : \n        raise Exception ( 'Unhandled type: ' + type ( obj ) ) \n    return name . capitalize ( ) if capitalize else name "}
{"13631": "\ndef options ( self , parser , env ) : \n    parser . add_option ( \"--processes\" , action = \"store\" , default = env . get ( 'NOSE_PROCESSES' , 0 ) , dest = \"multiprocess_workers\" , metavar = \"NUM\" , help = \"Spread test run among this many processes. \" \"Set a number equal to the number of processors \" \"or cores in your machine for best results. \" \"[NOSE_PROCESSES]\" ) \n    parser . add_option ( \"--process-timeout\" , action = \"store\" , default = env . get ( 'NOSE_PROCESS_TIMEOUT' , 10 ) , dest = \"multiprocess_timeout\" , metavar = \"SECONDS\" , help = \"Set timeout for return of results from each \" \"test runner process. [NOSE_PROCESS_TIMEOUT]\" ) \n    parser . add_option ( \"--process-restartworker\" , action = \"store_true\" , default = env . get ( 'NOSE_PROCESS_RESTARTWORKER' , 0 ) , dest = \"multiprocess_restartworker\" , help = \"If set, will restart each worker process once\" \" their tests are done, this helps control memory \" \"leaks from killing the system. \" \"[NOSE_PROCESS_RESTARTWORKER]\" ) "}
{"13634": "\ndef deactivate ( self ) : \n    remove_builtin = self . remove_builtin \n    for key , val in self . _orig_builtins . iteritems ( ) : \n        remove_builtin ( key , val ) \n    self . _orig_builtins . clear ( ) \n    self . _builtins_added = 0 "}
{"13636": "\ndef explicit_rel_links ( self , rels = ( 'homepage' , 'download' ) ) : \n    rels = set ( rels ) \n    for anchor in self . parsed . findall ( \".//a\" ) : \n        if anchor . get ( \"rel\" ) and anchor . get ( \"href\" ) : \n            found_rels = set ( anchor . get ( \"rel\" ) . split ( ) ) \n            if found_rels & rels : \n                href = anchor . get ( \"href\" ) \n                url = self . clean_link ( urllib_parse . urljoin ( self . base_url , href ) ) \n                yield Link ( url , self , trusted = 0 ) "}
{"13643": "\ndef do_help ( self , options , args , parser ) : \n    if options . help : \n        if self . classic : \n            self . help_fn ( topic = 'help' ) \n        else : \n            self . help_fn ( parser = parser ) \n        return 1 \n    if \"help\" in options . actions : \n        if args : \n            for a in args : \n                parser = CMDS . get ( a ) \n                if parser : \n                    self . help_fn ( parser = parser ) \n                else : \n                    self . help_fn ( topic = a ) \n        else : \n            self . help_fn ( topic = 'help' ) \n        return 1 \n    if options . version : \n        self . help_fn ( topic = 'version' ) \n        return 1 \n    return 0 "}
{"13644": "\ndef args_ok ( self , options , args ) : \n    for i in [ 'erase' , 'execute' ] : \n        for j in [ 'annotate' , 'html' , 'report' , 'combine' ] : \n            if ( i in options . actions ) and ( j in options . actions ) : \n                self . help_fn ( \"You can't specify the '%s' and '%s' \" \"options at the same time.\" % ( i , j ) ) \n                return 0 \n    if not options . actions : \n        self . help_fn ( \"You must specify at least one of -e, -x, -c, -r, -a, or -b.\" ) \n        return 0 \n    args_allowed = ( 'execute' in options . actions or 'annotate' in options . actions or 'html' in options . actions or 'debug' in options . actions or 'report' in options . actions or 'xml' in options . actions ) \n    if not args_allowed and args : \n        self . help_fn ( \"Unexpected arguments: %s\" % \" \" . join ( args ) ) \n        return 0 \n    if 'execute' in options . actions and not args : \n        self . help_fn ( \"Nothing to do.\" ) \n        return 0 \n    return 1 "}
{"13645": "\ndef do_execute ( self , options , args ) : \n    old_path0 = sys . path [ 0 ] \n    self . coverage . start ( ) \n    code_ran = 1 \n    try : \n        try : \n            if options . module : \n                sys . path [ 0 ] = '' \n                self . run_python_module ( args [ 0 ] , args ) \n            else : \n                filename = args [ 0 ] \n                sys . path [ 0 ] = os . path . abspath ( os . path . dirname ( filename ) ) \n                self . run_python_file ( filename , args ) \n        except NoSource : \n            code_ran = 0 \n            raise \n    finally : \n        self . coverage . stop ( ) \n        if code_ran : \n            self . coverage . save ( ) \n        sys . path [ 0 ] = old_path0 "}
{"13646": "\ndef do_debug ( self , args ) : \n    if not args : \n        self . help_fn ( \"What information would you like: data, sys?\" ) \n        return ERR \n    for info in args : \n        if info == 'sys' : \n            print ( \"-- sys ----------------------------------------\" ) \n            for line in info_formatter ( self . coverage . sysinfo ( ) ) : \n                print ( \" %s\" % line ) \n        elif info == 'data' : \n            print ( \"-- data ---------------------------------------\" ) \n            self . coverage . load ( ) \n            print ( \"path: %s\" % self . coverage . data . filename ) \n            print ( \"has_arcs: %r\" % self . coverage . data . has_arcs ( ) ) \n            summary = self . coverage . data . summary ( fullpath = 1 ) \n            if summary : \n                filenames = sorted ( summary . keys ( ) ) \n                print ( \"\\n%d files:\" % len ( filenames ) ) \n                for f in filenames : \n                    print ( \"%s: %d lines\" % ( f , summary [ f ] ) ) \n            else : \n                print ( \"No data collected\" ) \n        else : \n            self . help_fn ( \"Don't know what you mean by %r\" % info ) \n            return ERR \n    return OK "}
{"13649": "\ndef log_errors ( f , self , * args , ** kwargs ) : \n    try : \n        return f ( self , * args , ** kwargs ) \n    except Exception : \n        self . log . error ( \"Uncaught exception in %r\" % f , exc_info = 1 ) "}
{"13650": "\ndef is_url ( url ) : \n    if '://' not in url : \n        return 0 \n    proto , addr = url . split ( '://' , 1 ) \n    if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : \n        return 0 \n    return 1 "}
{"13651": "\ndef validate_url ( url ) : \n    if not isinstance ( url , basestring ) : \n        raise TypeError ( \"url must be a string, not %r\" % type ( url ) ) \n    url = url . lower ( ) \n    proto_addr = url . split ( '://' ) \n    assert len ( proto_addr ) == 2 , 'Invalid url: %r' % url \n    proto , addr = proto_addr \n    assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , \"Invalid protocol: %r\" % proto \n    pat = re . compile ( r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$' ) \n    if proto == 'tcp' : \n        lis = addr . split ( ':' ) \n        assert len ( lis ) == 2 , 'Invalid url: %r' % url \n        addr , s_port = lis \n        try : \n            port = int ( s_port ) \n        except ValueError : \n            raise AssertionError ( \"Invalid port %r in url: %r\" % ( port , url ) ) \n        assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url \n    else : \n        pass \n    return 1 "}
{"13656": "\ndef parallel ( view , dist = 'b' , block = None , ordered = 1 , ** flags ) : \n    def parallel_function ( f ) : \n        return ParallelFunction ( view , f , dist = dist , block = block , ordered = ordered , ** flags ) \n    return parallel_function "}
{"13657": "\ndef map ( self , * sequences ) : \n    self . _map = 1 \n    try : \n        ret = self . __call__ ( * sequences ) \n    finally : \n        del self . _map \n    return ret "}
{"13663": "\ndef register_post_execute ( self , func ) : \n    if not callable ( func ) : \n        raise ValueError ( 'argument %s must be callable' % func ) \n    self . _post_execute [ func ] = 1 "}
{"13668": "\ndef reset ( self , new_session = 1 ) : \n    self . history_manager . reset ( new_session ) \n    if new_session : \n        self . execution_count = 1 \n    if self . displayhook . do_full_cache : \n        self . displayhook . flush ( ) \n    if self . user_ns is not self . user_global_ns : \n        self . user_ns . clear ( ) \n    ns = self . user_global_ns \n    drop_keys = set ( ns . keys ( ) ) \n    drop_keys . discard ( '__builtin__' ) \n    drop_keys . discard ( '__builtins__' ) \n    drop_keys . discard ( '__name__' ) \n    for k in drop_keys : \n        del ns [ k ] \n    self . user_ns_hidden . clear ( ) \n    self . init_user_ns ( ) \n    self . alias_manager . clear_aliases ( ) \n    self . alias_manager . init_aliases ( ) \n    self . clear_main_mod_cache ( ) \n    self . new_main_mod ( ) "}
{"13669": "\ndef del_var ( self , varname , by_name = 0 ) : \n    if varname in ( '__builtin__' , '__builtins__' ) : \n        raise ValueError ( \"Refusing to delete %s\" % varname ) \n    ns_refs = self . all_ns_refs \n    if by_name : \n        for ns in ns_refs : \n            try : \n                del ns [ varname ] \n            except KeyError : \n                pass \n    else : \n        try : \n            obj = self . user_ns [ varname ] \n        except KeyError : \n            raise NameError ( \"name '%s' is not defined\" % varname ) \n        ns_refs . append ( self . history_manager . output_hist ) \n        for ns in ns_refs : \n            to_delete = [ n for n , o in ns . iteritems ( ) if o is obj ] \n            for name in to_delete : \n                del ns [ name ] \n        for name in ( '_' , '__' , '___' ) : \n            if getattr ( self . displayhook , name ) is obj : \n                setattr ( self . displayhook , name , None ) "}
{"13671": "\ndef push ( self , variables , interactive = 1 ) : \n    vdict = None \n    if isinstance ( variables , dict ) : \n        vdict = variables \n    elif isinstance ( variables , ( basestring , list , tuple ) ) : \n        if isinstance ( variables , basestring ) : \n            vlist = variables . split ( ) \n        else : \n            vlist = variables \n        vdict = { } \n        cf = sys . _getframe ( 1 ) \n        for name in vlist : \n            try : \n                vdict [ name ] = eval ( name , cf . f_globals , cf . f_locals ) \n            except : \n                print ( 'Could not get variable %s from %s' % ( name , cf . f_code . co_name ) ) \n    else : \n        raise ValueError ( 'variables must be a dict/str/list/tuple' ) \n    self . user_ns . update ( vdict ) \n    user_ns_hidden = self . user_ns_hidden \n    if interactive : \n        user_ns_hidden . difference_update ( vdict ) \n    else : \n        user_ns_hidden . update ( vdict ) "}
{"13672": "\ndef _ofind ( self , oname , namespaces = None ) : \n    oname = oname . strip ( ) \n    if not oname . startswith ( ESC_MAGIC ) and not oname . startswith ( ESC_MAGIC2 ) and not py3compat . isidentifier ( oname , dotted = 1 ) : \n        return dict ( found = 0 ) \n    alias_ns = None \n    if namespaces is None : \n        namespaces = [ ( 'Interactive' , self . user_ns ) , ( 'Interactive (global)' , self . user_global_ns ) , ( 'Python builtin' , builtin_mod . __dict__ ) , ( 'Alias' , self . alias_manager . alias_table ) , ] \n        alias_ns = self . alias_manager . alias_table \n    found = 0 ; \n    obj = None ; \n    ospace = None ; \n    ds = None ; \n    ismagic = 0 ; \n    isalias = 0 ; \n    parent = None \n    if ( oname == 'print' and not py3compat . PY3 and not ( self . compile . compiler_flags & __future__ . CO_FUTURE_PRINT_FUNCTION ) ) : \n        return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n    oname_parts = oname . split ( '.' ) \n    oname_head , oname_rest = oname_parts [ 0 ] , oname_parts [ 1 : ] \n    for nsname , ns in namespaces : \n        try : \n            obj = ns [ oname_head ] \n        except KeyError : \n            continue \n        else : \n            for part in oname_rest : \n                try : \n                    parent = obj \n                    obj = getattr ( obj , part ) \n                except : \n                    break \n            else : \n                found = 1 \n                ospace = nsname \n                if ns == alias_ns : \n                    isalias = 1 \n                break \n    if not found : \n        obj = None \n        if oname . startswith ( ESC_MAGIC2 ) : \n            oname = oname . lstrip ( ESC_MAGIC2 ) \n            obj = self . find_cell_magic ( oname ) \n        elif oname . startswith ( ESC_MAGIC ) : \n            oname = oname . lstrip ( ESC_MAGIC ) \n            obj = self . find_line_magic ( oname ) \n        else : \n            obj = self . find_line_magic ( oname ) \n            if obj is None : \n                obj = self . find_cell_magic ( oname ) \n        if obj is not None : \n            found = 1 \n            ospace = 'IPython internal' \n            ismagic = 1 \n    if not found and oname_head in [ \"''\" , '\"\"' , '[]' , '{}' , '()' ] : \n        obj = eval ( oname_head ) \n        found = 1 \n        ospace = 'Interactive' \n    return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } "}
{"13678": "\ndef showtraceback ( self , exc_tuple = None , filename = None , tb_offset = None , exception_only = 0 ) : \n    try : \n        try : \n            etype , value , tb = self . _get_exc_info ( exc_tuple ) \n        except ValueError : \n            self . write_err ( 'No traceback available to show.\\n' ) \n            return \n        if etype is SyntaxError : \n            self . showsyntaxerror ( filename ) \n        elif etype is UsageError : \n            self . write_err ( \"UsageError: %s\" % value ) \n        else : \n            if exception_only : \n                stb = [ 'An exception has occurred, use %tb to see ' 'the full traceback.\\n' ] \n                stb . extend ( self . InteractiveTB . get_exception_only ( etype , value ) ) \n            else : \n                try : \n                    stb = value . _render_traceback_ ( ) \n                except Exception : \n                    stb = self . InteractiveTB . structured_traceback ( etype , value , tb , tb_offset = tb_offset ) \n                self . _showtraceback ( etype , value , stb ) \n                if self . call_pdb : \n                    self . debugger ( force = 1 ) \n                return \n            self . _showtraceback ( etype , value , stb ) \n    except KeyboardInterrupt : \n        self . write_err ( \"\\nKeyboardInterrupt\\n\" ) "}
{"13685": "\ndef run_line_magic ( self , magic_name , line ) : \n    fn = self . find_line_magic ( magic_name ) \n    if fn is None : \n        cm = self . find_cell_magic ( magic_name ) \n        etpl = \"Line magic function `%%%s` not found%s.\" \n        extra = '' if cm is None else ( ' (But cell magic `%%%%%s` exists, ' 'did you mean that instead?)' % magic_name ) \n        error ( etpl % ( magic_name , extra ) ) \n    else : \n        stack_depth = 2 \n        magic_arg_s = self . var_expand ( line , stack_depth ) \n        args = [ magic_arg_s ] \n        if getattr ( fn , \"needs_local_scope\" , 0 ) : \n            args . append ( sys . _getframe ( stack_depth ) . f_locals ) \n        with self . builtin_trap : \n            result = fn ( * args ) \n        return result "}
{"13693": "\ndef safe_execfile_ipy ( self , fname ) : \n    fname = os . path . abspath ( os . path . expanduser ( fname ) ) \n    try : \n        with open ( fname ) as thefile : \n            pass \n    except : \n        warn ( 'Could not open file <%s> for safe execution.' % fname ) \n        return \n    dname = os . path . dirname ( fname ) \n    with prepended_to_syspath ( dname ) : \n        try : \n            with open ( fname ) as thefile : \n                self . run_cell ( thefile . read ( ) , store_history = 0 ) \n        except : \n            self . showtraceback ( ) \n            warn ( 'Unknown failure executing file: <%s>' % fname ) "}
{"13695": "\ndef run_cell ( self , raw_cell , store_history = 0 , silent = 0 ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    if silent : \n        store_history = 0 \n    self . input_splitter . push ( raw_cell ) \n    if self . input_splitter . cell_magic_parts : \n        self . _current_cell_magic_body = '' . join ( self . input_splitter . cell_magic_parts ) \n    cell = self . input_splitter . source_reset ( ) \n    with self . builtin_trap : \n        prefilter_failed = 0 \n        if len ( cell . splitlines ( ) ) == 1 : \n            try : \n                cell = self . prefilter_manager . prefilter_lines ( cell ) + '\\n' \n            except AliasError as e : \n                error ( e ) \n                prefilter_failed = 1 \n            except Exception : \n                self . showtraceback ( ) \n                prefilter_failed = 1 \n        if store_history : \n            self . history_manager . store_inputs ( self . execution_count , cell , raw_cell ) \n        if not silent : \n            self . logger . log ( cell , raw_cell ) \n        if not prefilter_failed : \n            cell_name = self . compile . cache ( cell , self . execution_count ) \n            with self . display_trap : \n                try : \n                    code_ast = self . compile . ast_parse ( cell , filename = cell_name ) \n                except IndentationError : \n                    self . showindentationerror ( ) \n                    if store_history : \n                        self . execution_count += 1 \n                    return None \n                except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n                    self . showsyntaxerror ( ) \n                    if store_history : \n                        self . execution_count += 1 \n                    return None \n                interactivity = \"none\" if silent else self . ast_node_interactivity \n                self . run_ast_nodes ( code_ast . body , cell_name , interactivity = interactivity ) \n                post_exec = [ ] if silent else self . _post_execute . iteritems ( ) \n                for func , status in post_exec : \n                    if self . disable_failing_post_execute and not status : \n                        continue \n                    try : \n                        func ( ) \n                    except KeyboardInterrupt : \n                        print >> io . stderr , \"\\nKeyboardInterrupt\" \n                    except Exception : \n                        self . _post_execute [ func ] = 0 \n                        self . showtraceback ( ) \n                        print >> io . stderr , '\\n' . join ( [ \"post-execution function %r produced an error.\" % func , \"If this problem persists, you can disable failing post-exec functions with:\" , \"\" , \"    get_ipython().disable_failing_post_execute = True\" ] ) \n    if store_history : \n        self . history_manager . store_output ( self . execution_count ) \n        self . execution_count += 1 "}
{"13696": "\ndef run_ast_nodes ( self , nodelist , cell_name , interactivity = 'last_expr' ) : \n    if not nodelist : \n        return \n    if interactivity == 'last_expr' : \n        if isinstance ( nodelist [ - 1 ] , ast . Expr ) : \n            interactivity = \"last\" \n        else : \n            interactivity = \"none\" \n    if interactivity == 'none' : \n        to_run_exec , to_run_interactive = nodelist , [ ] \n    elif interactivity == 'last' : \n        to_run_exec , to_run_interactive = nodelist [ : - 1 ] , nodelist [ - 1 : ] \n    elif interactivity == 'all' : \n        to_run_exec , to_run_interactive = [ ] , nodelist \n    else : \n        raise ValueError ( \"Interactivity was %r\" % interactivity ) \n    exec_count = self . execution_count \n    try : \n        for i , node in enumerate ( to_run_exec ) : \n            mod = ast . Module ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"exec\" ) \n            if self . run_code ( code ) : \n                return 1 \n        for i , node in enumerate ( to_run_interactive ) : \n            mod = ast . Interactive ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"single\" ) \n            if self . run_code ( code ) : \n                return 1 \n        if softspace ( sys . stdout , 0 ) : \n            print \n    except : \n        self . showtraceback ( ) \n    return 0 "}
{"13697": "\ndef enable_pylab ( self , gui = None , import_all = 1 ) : \n    from IPython . core . pylabtools import mpl_runner \n    ns = { } \n    try : \n        gui = pylab_activate ( ns , gui , import_all , self ) \n    except KeyError : \n        error ( \"Backend %r not supported\" % gui ) \n        return \n    self . user_ns . update ( ns ) \n    self . user_ns_hidden . update ( ns ) \n    self . enable_gui ( gui ) \n    self . magics_manager . registry [ 'ExecutionMagics' ] . default_runner = mpl_runner ( self . safe_execfile ) "}
{"13700": "\ndef extract_input_lines ( self , range_str , raw = 0 ) : \n    lines = self . history_manager . get_range_by_str ( range_str , raw = raw ) \n    return \"\\n\" . join ( x for _ , _ , x in lines ) "}
{"13701": "\ndef find_user_code ( self , target , raw = 1 , py_only = 0 ) : \n    code = self . extract_input_lines ( target , raw = raw ) \n    if code : \n        return code \n    utarget = unquote_filename ( target ) \n    try : \n        if utarget . startswith ( ( 'http://' , 'https://' ) ) : \n            return openpy . read_py_url ( utarget , skip_encoding_cookie = 1 ) \n    except UnicodeDecodeError : \n        if not py_only : \n            response = urllib . urlopen ( target ) \n            return response . read ( ) . decode ( 'latin1' ) \n        raise ValueError ( ( \"'%s' seem to be unreadable.\" ) % utarget ) \n    potential_target = [ target ] \n    try : \n        potential_target . insert ( 0 , get_py_filename ( target ) ) \n    except IOError : \n        pass \n    for tgt in potential_target : \n        if os . path . isfile ( tgt ) : \n            try : \n                return openpy . read_py_file ( tgt , skip_encoding_cookie = 1 ) \n            except UnicodeDecodeError : \n                if not py_only : \n                    with io_open ( tgt , 'r' , encoding = 'latin1' ) as f : \n                        return f . read ( ) \n                raise ValueError ( ( \"'%s' seem to be unreadable.\" ) % target ) \n    try : \n        codeobj = eval ( target , self . user_ns ) \n    except Exception : \n        raise ValueError ( ( \"'%s' was not found in history, as a file, url, \" \"nor in the user namespace.\" ) % target ) \n    if isinstance ( codeobj , basestring ) : \n        return codeobj \n    elif isinstance ( codeobj , Macro ) : \n        return codeobj . value \n    raise TypeError ( \"%s is neither a string nor a macro.\" % target , codeobj ) "}
{"13702": "\ndef atexit_operations ( self ) : \n    self . history_manager . end_session ( ) \n    for tfile in self . tempfiles : \n        try : \n            os . unlink ( tfile ) \n        except OSError : \n            pass \n    self . reset ( new_session = 0 ) \n    self . hooks . shutdown_hook ( ) "}
{"13707": "\ndef deprecated ( conditional = 1 ) : \n    def deprecate_decorator ( f ) : \n        import nose \n        def _deprecated_imp ( * args , ** kwargs ) : \n            ctx = WarningManager ( record = 1 ) \n            l = ctx . __enter__ ( ) \n            warnings . simplefilter ( 'always' ) \n            try : \n                f ( * args , ** kwargs ) \n                if not len ( l ) > 0 : \n                    raise AssertionError ( \"No warning raised when calling %s\" % f . __name__ ) \n                if not l [ 0 ] . category is DeprecationWarning : \n                    raise AssertionError ( \"First warning for %s is not a \" \"DeprecationWarning( is %s)\" % ( f . __name__ , l [ 0 ] ) ) \n            finally : \n                ctx . __exit__ ( ) \n        if callable ( conditional ) : \n            cond = conditional ( ) \n        else : \n            cond = conditional \n        if cond : \n            return nose . tools . make_decorator ( f ) ( _deprecated_imp ) \n        else : \n            return f \n    return deprecate_decorator "}
{"13724": "\ndef emit ( self , msg , level = 1 , debug = 0 ) : \n    if debug : \n        if not self . debug : \n            return \n        stream = sys . stderr \n    else : \n        if self . verbose < level : \n            return \n        stream = sys . stdout \n    print ( msg , file = stream ) \n    stream . flush ( ) "}
{"13738": "\ndef highlight_text ( needles , haystack , cls_name = 'highlighted' , words = 0 , case = 0 ) : \n    if not needles : \n        return haystack \n    if not haystack : \n        return '' \n    if words : \n        pattern = r\"(%s)\" % \"|\" . join ( [ '\\\\b{}\\\\b' . format ( re . escape ( n ) ) for n in needles ] ) \n    else : \n        pattern = r\"(%s)\" % \"|\" . join ( [ re . escape ( n ) for n in needles ] ) \n    if case : \n        regex = re . compile ( pattern ) \n    else : \n        regex = re . compile ( pattern , re . I ) \n    i , out = 0 , \"\" \n    for m in regex . finditer ( haystack ) : \n        out += \"\" . join ( [ haystack [ i : m . start ( ) ] , '<span class=\"%s\">' % cls_name , haystack [ m . start ( ) : m . end ( ) ] , \"</span>\" ] ) \n        i = m . end ( ) \n    return mark_safe ( out + haystack [ i : ] ) "}
{"13740": "\ndef highlight_words ( string , keywords , cls_name = 'highlighted' ) : \n    if not keywords : \n        return string \n    if not string : \n        return '' \n    include , exclude = get_text_tokenizer ( keywords ) \n    highlighted = highlight_text ( include , string , cls_name , words = 1 ) \n    return highlighted "}
{"13741": "\ndef run ( self , func ) : \n    try : \n        self . _copy ( self ) \n        if _file : \n            __builtin__ . file = self . _file \n        __builtin__ . open = self . _open \n        self . _active = 1 \n        return func ( ) \n    finally : \n        self . _active = 0 \n        if _file : \n            __builtin__ . file = _file \n        __builtin__ . open = _open \n        self . _copy ( _os ) "}
{"13743": "\ndef indent ( instr , nspaces = 4 , ntabs = 0 , flatten = 0 ) : \n    if instr is None : \n        return \n    ind = '\\t' * ntabs + ' ' * nspaces \n    if flatten : \n        pat = re . compile ( r'^\\s*' , re . MULTILINE ) \n    else : \n        pat = re . compile ( r'^' , re . MULTILINE ) \n    outstr = re . sub ( pat , ind , instr ) \n    if outstr . endswith ( os . linesep + ind ) : \n        return outstr [ : - len ( ind ) ] \n    else : \n        return outstr "}
{"13753": "\ndef init_ssh ( self ) : \n    if not self . sshserver and not self . sshkey : \n        return \n    if self . sshkey and not self . sshserver : \n        self . sshserver = self . ip \n        self . ip = LOCALHOST \n    info = dict ( ip = self . ip , shell_port = self . shell_port , iopub_port = self . iopub_port , stdin_port = self . stdin_port , hb_port = self . hb_port ) \n    self . log . info ( \"Forwarding connections to %s via %s\" % ( self . ip , self . sshserver ) ) \n    self . ip = LOCALHOST \n    try : \n        newports = tunnel_to_kernel ( info , self . sshserver , self . sshkey ) \n    except : \n        self . log . error ( \"Could not setup tunnels\" , exc_info = 1 ) \n        self . exit ( 1 ) \n    self . shell_port , self . iopub_port , self . stdin_port , self . hb_port = newports \n    cf = self . connection_file \n    base , ext = os . path . splitext ( cf ) \n    base = os . path . basename ( base ) \n    self . connection_file = os . path . basename ( base ) + '-ssh' + ext \n    self . log . critical ( \"To connect another client via this tunnel, use:\" ) \n    self . log . critical ( \"--existing %s\" % self . connection_file ) "}
{"13754": "\ndef pretty ( obj , verbose = 0 , max_width = 79 , newline = '\\n' ) : \n    stream = StringIO ( ) \n    printer = RepresentationPrinter ( stream , verbose , max_width , newline ) \n    printer . pretty ( obj ) \n    printer . flush ( ) \n    return stream . getvalue ( ) "}
{"13755": "\ndef pprint ( obj , verbose = 0 , max_width = 79 , newline = '\\n' ) : \n    printer = RepresentationPrinter ( sys . stdout , verbose , max_width , newline ) \n    printer . pretty ( obj ) \n    printer . flush ( ) \n    sys . stdout . write ( newline ) \n    sys . stdout . flush ( ) "}
{"13757": "\ndef _default_pprint ( obj , p , cycle ) : \n    klass = getattr ( obj , '__class__' , None ) or type ( obj ) \n    if getattr ( klass , '__repr__' , None ) not in _baseclass_reprs : \n        p . text ( repr ( obj ) ) \n        return \n    p . begin_group ( 1 , '<' ) \n    p . pretty ( klass ) \n    p . text ( ' at 0x%x' % id ( obj ) ) \n    if cycle : \n        p . text ( ' ...' ) \n    elif p . verbose : \n        first = 1 \n        for key in dir ( obj ) : \n            if not key . startswith ( '_' ) : \n                try : \n                    value = getattr ( obj , key ) \n                except AttributeError : \n                    continue \n                if isinstance ( value , types . MethodType ) : \n                    continue \n                if not first : \n                    p . text ( ',' ) \n                p . breakable ( ) \n                p . text ( key ) \n                p . text ( '=' ) \n                step = len ( key ) + 1 \n                p . indentation += step \n                p . pretty ( value ) \n                p . indentation -= step \n                first = 0 \n    p . end_group ( 1 , '>' ) "}
{"13761": "\ndef _re_pattern_pprint ( obj , p , cycle ) : \n    p . text ( 're.compile(' ) \n    pattern = repr ( obj . pattern ) \n    if pattern [ : 1 ] in 'uU' : \n        pattern = pattern [ 1 : ] \n        prefix = 'ur' \n    else : \n        prefix = 'r' \n    pattern = prefix + pattern . replace ( '\\\\\\\\' , '\\\\' ) \n    p . text ( pattern ) \n    if obj . flags : \n        p . text ( ',' ) \n        p . breakable ( ) \n        done_one = 0 \n        for flag in ( 'TEMPLATE' , 'IGNORECASE' , 'LOCALE' , 'MULTILINE' , 'DOTALL' , 'UNICODE' , 'VERBOSE' , 'DEBUG' ) : \n            if obj . flags & getattr ( re , flag ) : \n                if done_one : \n                    p . text ( '|' ) \n                p . text ( 're.' + flag ) \n                done_one = 1 \n    p . text ( ')' ) "}
{"13783": "\ndef check_site_dir ( self ) : \n    instdir = normalize_path ( self . install_dir ) \n    pth_file = os . path . join ( instdir , 'easy-install.pth' ) \n    is_site_dir = instdir in self . all_site_dirs \n    if not is_site_dir and not self . multi_version : \n        is_site_dir = self . check_pth_processing ( ) \n    else : \n        testfile = self . pseudo_tempname ( ) + '.write-test' \n        test_exists = os . path . exists ( testfile ) \n        try : \n            if test_exists : \n                os . unlink ( testfile ) \n            open ( testfile , 'w' ) . close ( ) \n            os . unlink ( testfile ) \n        except ( OSError , IOError ) : \n            self . cant_write_to_target ( ) \n    if not is_site_dir and not self . multi_version : \n        raise DistutilsError ( self . no_default_version_msg ( ) ) \n    if is_site_dir : \n        if self . pth_file is None : \n            self . pth_file = PthDistributions ( pth_file , self . all_site_dirs ) \n    else : \n        self . pth_file = None \n    PYTHONPATH = os . environ . get ( 'PYTHONPATH' , '' ) . split ( os . pathsep ) \n    if instdir not in map ( normalize_path , [ _f for _f in PYTHONPATH if _f ] ) : \n        self . sitepy_installed = 1 \n    elif self . multi_version and not os . path . exists ( pth_file ) : \n        self . sitepy_installed = 1 \n        self . pth_file = None \n    self . install_dir = instdir "}
{"13788": "\ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    reg_url = cfg [ 'url' ] \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = 0 ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while 1 : \n        try : \n            idents , msg = session . recv ( sub , mode = 0 ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ 0 ] \n        if msg [ 'msg_type' ] == 'stream' : \n            print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) \n        elif msg [ 'msg_type' ] == 'pyerr' : \n            c = msg [ 'content' ] \n            print ( topic + ':' ) \n            for line in c [ 'traceback' ] : \n                print ( '    ' + line ) "}
{"13802": "\ndef load_config_file ( self , filename , path = None ) : \n    loader = PyFileConfigLoader ( filename , path = path ) \n    try : \n        config = loader . load_config ( ) \n    except ConfigFileNotFound : \n        raise \n    except Exception : \n        filename = loader . full_filename or filename \n        self . log . error ( \"Exception while loading config file %s\" , filename , exc_info = 1 ) \n    else : \n        self . log . debug ( \"Loaded config file: %s\" , loader . full_filename ) \n        self . update_config ( config ) "}
{"13807": "\ndef _config_changed ( self , name , old , new ) : \n    traits = self . traits ( config = 1 ) \n    section_names = [ cls . __name__ for cls in reversed ( self . __class__ . __mro__ ) if issubclass ( cls , Configurable ) and issubclass ( self . __class__ , cls ) ] \n    for sname in section_names : \n        if new . _has_section ( sname ) : \n            my_config = new [ sname ] \n            for k , v in traits . iteritems ( ) : \n                if k [ 0 ] . upper ( ) == k [ 0 ] and not k . startswith ( '_' ) : \n                    raise ConfigurableError ( 'Configurable traitlets with ' 'config=True must start with a lowercase so they are ' 'not confused with Config subsections: %s.%s' % ( self . __class__ . __name__ , k ) ) \n                try : \n                    config_value = my_config [ k ] \n                except KeyError : \n                    pass \n                else : \n                    setattr ( self , k , deepcopy ( config_value ) ) "}
{"13808": "\ndef class_get_help ( cls , inst = None ) : \n    assert inst is None or isinstance ( inst , cls ) \n    cls_traits = cls . class_traits ( config = 1 ) \n    final_help = [ ] \n    final_help . append ( u'%s options' % cls . __name__ ) \n    final_help . append ( len ( final_help [ 0 ] ) * u'-' ) \n    for k , v in sorted ( cls . class_traits ( config = 1 ) . iteritems ( ) ) : \n        help = cls . class_get_trait_help ( v , inst ) \n        final_help . append ( help ) \n    return '\\n' . join ( final_help ) "}
{"13810": "\ndef class_config_section ( cls ) : \n    def c ( s ) : \n        s = '\\n\\n' . join ( wrap_paragraphs ( s , 78 ) ) \n        return '# ' + s . replace ( '\\n' , '\\n# ' ) \n    breaker = '#' + '-' * 78 \n    s = \"# %s configuration\" % cls . __name__ \n    lines = [ breaker , s , breaker , '' ] \n    desc = cls . class_traits ( ) . get ( 'description' ) \n    if desc : \n        desc = desc . default_value \n    else : \n        desc = getattr ( cls , '__doc__' , '' ) \n    if desc : \n        lines . append ( c ( desc ) ) \n        lines . append ( '' ) \n    parents = [ ] \n    for parent in cls . mro ( ) : \n        if parent is not cls and issubclass ( parent , Configurable ) and parent . class_traits ( config = 1 ) : \n            parents . append ( parent ) \n    if parents : \n        pstr = ', ' . join ( [ p . __name__ for p in parents ] ) \n        lines . append ( c ( '%s will inherit config from: %s' % ( cls . __name__ , pstr ) ) ) \n        lines . append ( '' ) \n    for name , trait in cls . class_traits ( config = 1 ) . iteritems ( ) : \n        help = trait . get_metadata ( 'help' ) or '' \n        lines . append ( c ( help ) ) \n        lines . append ( '# c.%s.%s = %r' % ( cls . __name__ , name , trait . get_default_value ( ) ) ) \n        lines . append ( '' ) \n    return '\\n' . join ( lines ) "}
{"13819": "\ndef process_handler ( cmd , callback , stderr = subprocess . PIPE ) : \n    sys . stdout . flush ( ) \n    sys . stderr . flush ( ) \n    close_fds = sys . platform != 'win32' \n    p = subprocess . Popen ( cmd , shell = 1 , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = stderr , close_fds = close_fds ) \n    try : \n        out = callback ( p ) \n    except KeyboardInterrupt : \n        print ( '^C' ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        out = None \n    finally : \n        if p . returncode is None : \n            try : \n                p . terminate ( ) \n                p . poll ( ) \n            except OSError : \n                pass \n        if p . returncode is None : \n            try : \n                p . kill ( ) \n            except OSError : \n                pass \n    return out "}
{"13820": "\ndef arg_split ( s , posix = 0 , strict = 1 ) : \n    is_unicode = 0 \n    if ( not py3compat . PY3 ) and isinstance ( s , unicode ) : \n        is_unicode = 1 \n        s = s . encode ( 'utf-8' ) \n    lex = shlex . shlex ( s , posix = posix ) \n    lex . whitespace_split = 1 \n    lex . commenters = '' \n    tokens = [ ] \n    while 1 : \n        try : \n            tokens . append ( lex . next ( ) ) \n        except StopIteration : \n            break \n        except ValueError : \n            if strict : \n                raise \n            tokens . append ( lex . token ) \n            break \n    if is_unicode : \n        tokens = [ x . decode ( 'utf-8' ) for x in tokens ] \n    return tokens "}
{"13822": "\ndef magics_class ( cls ) : \n    cls . registered = 1 \n    cls . magics = dict ( line = magics [ 'line' ] , cell = magics [ 'cell' ] ) \n    magics [ 'line' ] = { } \n    magics [ 'cell' ] = { } \n    return cls "}
{"13826": "\ndef lsmagic_docs ( self , brief = 0 , missing = '' ) : \n    docs = { } \n    for m_type in self . magics : \n        m_docs = { } \n        for m_name , m_func in self . magics [ m_type ] . iteritems ( ) : \n            if m_func . __doc__ : \n                if brief : \n                    m_docs [ m_name ] = m_func . __doc__ . split ( '\\n' , 1 ) [ 0 ] \n                else : \n                    m_docs [ m_name ] = m_func . __doc__ . rstrip ( ) \n            else : \n                m_docs [ m_name ] = missing \n        docs [ m_type ] = m_docs \n    return docs "}
{"13832": "\ndef page_guiref ( arg_s = None ) : \n    from IPython . core import page \n    page . page ( gui_reference , auto_html = 1 ) "}
{"13836": "\ndef calc_next_run ( self ) : \n    base_time = self . last_run \n    if self . last_run == HAS_NOT_RUN : \n        if self . wait_for_schedule is 0 : \n            self . next_run = timezone . now ( ) \n            self . wait_for_schedule = 0 \n            self . save ( ) \n            return \n        else : \n            base_time = timezone . now ( ) \n    self . next_run = croniter ( self . schedule , base_time ) . get_next ( datetime ) \n    self . save ( ) "}
{"13838": "\ndef run ( self , message ) : \n    the_callable = self . func_from_info ( ) \n    try : \n        task_message = dict ( task = self , channel_message = message , ) \n        the_callable ( task_message ) \n    finally : \n        if self . end_running < self . next_run : \n            self . enabled = 0 \n            Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            return \n        if self . iterations == 0 : \n            return \n        else : \n            self . iterations -= 1 \n            if self . iterations == 0 : \n                self . enabled = 0 \n                Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            self . save ( ) "}
{"13840": "\ndef run_iterations ( cls , the_callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run_immediately = 0 , delay_until = None ) : \n    task = task_with_callable ( the_callable , label = label , schedule = schedule , userdata = userdata ) \n    task . iterations = iterations \n    if delay_until is not None : \n        if isinstance ( delay_until , datetime ) : \n            if delay_until > timezone . now ( ) : \n                task . start_running = delay_until \n            else : \n                raise ValueError ( \"Task cannot start running in the past\" ) \n        else : \n            raise ValueError ( \"delay_until must be a datetime.datetime instance\" ) \n    if run_immediately : \n        task . next_run = timezone . now ( ) \n    else : \n        task . calc_next_run ( ) \n    task . save ( ) "}
{"13841": "\ndef run_once ( cls , the_callable , userdata = None , delay_until = None ) : \n    cls . run_iterations ( the_callable , userdata = userdata , run_immediately = 1 , delay_until = delay_until ) "}
{"13844": "\ndef timid ( ctxt , test , key = None , check = 0 , exts = None ) : \n    if exts is None : \n        exts = extensions . ExtensionSet ( ) \n    ctxt . emit ( 'Reading test steps from %s%s...' % ( test , '[%s]' % key if key else '' ) , debug = 1 ) \n    ctxt . steps += exts . read_steps ( ctxt , steps . Step . parse_file ( ctxt , test , key ) ) \n    if check : \n        return None \n    for idx , step in enumerate ( ctxt . steps ) : \n        ctxt . emit ( '[Step %d]: %s . . .' % ( idx , step . name ) ) \n        if exts . pre_step ( ctxt , step , idx ) : \n            ctxt . emit ( '[Step %d]: `- Step %s' % ( idx , steps . states [ steps . SKIPPED ] ) ) \n            continue \n        result = step ( ctxt ) \n        exts . post_step ( ctxt , step , idx , result ) \n        ctxt . emit ( '[Step %d]: `- Step %s%s' % ( idx , steps . states [ result . state ] , ' (ignored)' if result . ignore else '' ) ) \n        if not result : \n            msg = 'Test step failure' \n            if result . msg : \n                msg += ': %s' % result . msg \n            return msg \n    return None "}
{"13845": "\ndef create_interrupt_event ( ) : \n    class SECURITY_ATTRIBUTES ( ctypes . Structure ) : \n        _fields_ = [ ( \"nLength\" , ctypes . c_int ) , ( \"lpSecurityDescriptor\" , ctypes . c_void_p ) , ( \"bInheritHandle\" , ctypes . c_int ) ] \n    sa = SECURITY_ATTRIBUTES ( ) \n    sa_p = ctypes . pointer ( sa ) \n    sa . nLength = ctypes . sizeof ( SECURITY_ATTRIBUTES ) \n    sa . lpSecurityDescriptor = 0 \n    sa . bInheritHandle = 1 \n    return ctypes . windll . kernel32 . CreateEventA ( sa_p , 0 , 0 , '' ) "}
{"13846": "\ndef run ( self ) : \n    try : \n        from _winapi import WAIT_OBJECT_0 , INFINITE \n    except ImportError : \n        from _subprocess import WAIT_OBJECT_0 , INFINITE \n    handles = [ ] \n    if self . interrupt_handle : \n        handles . append ( self . interrupt_handle ) \n    if self . parent_handle : \n        handles . append ( self . parent_handle ) \n    arch = platform . architecture ( ) [ 0 ] \n    c_int = ctypes . c_int64 if arch . startswith ( '64' ) else ctypes . c_int \n    while 1 : \n        result = ctypes . windll . kernel32 . WaitForMultipleObjects ( len ( handles ) , ( c_int * len ( handles ) ) ( * handles ) , 0 , INFINITE ) \n        if WAIT_OBJECT_0 <= result < len ( handles ) : \n            handle = handles [ result - WAIT_OBJECT_0 ] \n            if handle == self . interrupt_handle : \n                interrupt_main ( ) \n            elif handle == self . parent_handle : \n                os . _exit ( 1 ) \n        elif result < 0 : \n            warn ( \"\"\"Parent poll failed.  If the frontend dies,                the kernel may be left running.  Please let us know                about your system (bitness, Python, etc.) at                ipython-dev@scipy.org\"\"\" ) \n            return "}
{"13847": "\ndef filter_ns ( ns , name_pattern = \"*\" , type_pattern = \"all\" , ignore_case = 1 , show_all = 1 ) : \n    pattern = name_pattern . replace ( \"*\" , \".*\" ) . replace ( \"?\" , \".\" ) \n    if ignore_case : \n        reg = re . compile ( pattern + \"$\" , re . I ) \n    else : \n        reg = re . compile ( pattern + \"$\" ) \n    return dict ( ( key , obj ) for key , obj in ns . iteritems ( ) if reg . match ( key ) and show_hidden ( key , show_all ) and is_type ( obj , type_pattern ) ) "}
{"13848": "\ndef list_namespace ( namespace , type_pattern , filter , ignore_case = 0 , show_all = 0 ) : \n    pattern_list = filter . split ( \".\" ) \n    if len ( pattern_list ) == 1 : \n        return filter_ns ( namespace , name_pattern = pattern_list [ 0 ] , type_pattern = type_pattern , ignore_case = ignore_case , show_all = show_all ) \n    else : \n        filtered = filter_ns ( namespace , name_pattern = pattern_list [ 0 ] , type_pattern = \"all\" , ignore_case = ignore_case , show_all = show_all ) \n        results = { } \n        for name , obj in filtered . iteritems ( ) : \n            ns = list_namespace ( dict_dir ( obj ) , type_pattern , \".\" . join ( pattern_list [ 1 : ] ) , ignore_case = ignore_case , show_all = show_all ) \n            for inner_name , inner_obj in ns . iteritems ( ) : \n                results [ \"%s.%s\" % ( name , inner_name ) ] = inner_obj \n        return results "}
{"13850": "\ndef draw_if_interactive ( ) : \n    fig = Gcf . get_active ( ) . canvas . figure \n    if not hasattr ( fig , 'show' ) : \n        fig . show = lambda * a : send_figure ( fig ) \n    if not matplotlib . is_interactive ( ) : \n        return \n    try : \n        show . _to_draw . remove ( fig ) \n    except ValueError : \n        pass \n    show . _to_draw . append ( fig ) \n    show . _draw_called = 1 "}
{"13851": "\ndef flush_figures ( ) : \n    if not show . _draw_called : \n        return \n    if InlineBackend . instance ( ) . close_figures : \n        try : \n            return show ( 1 ) \n        except Exception as e : \n            try : \n                get_ipython \n            except NameError : \n                raise e \n            else : \n                get_ipython ( ) . showtraceback ( ) \n                return \n    try : \n        active = set ( [ fm . canvas . figure for fm in Gcf . get_all_fig_managers ( ) ] ) \n        for fig in [ fig for fig in show . _to_draw if fig in active ] : \n            try : \n                send_figure ( fig ) \n            except Exception as e : \n                try : \n                    get_ipython \n                except NameError : \n                    raise e \n                else : \n                    get_ipython ( ) . showtraceback ( ) \n                    break \n    finally : \n        show . _to_draw = [ ] \n        show . _draw_called = 0 "}
{"13857": "\ndef _handle_sigint ( self , sig , frame ) : \n    signal . signal ( signal . SIGINT , self . _signal_stop ) \n    thread = threading . Thread ( target = self . _confirm_exit ) \n    thread . daemon = 1 \n    thread . start ( ) "}
{"13862": "\ndef _render ( self , name , color = 1 , ** kwargs ) : \n    if name == 'rewrite' : \n        return self . _render_rewrite ( color = color ) \n    if color : \n        scheme = self . color_scheme_table . active_colors \n        if name == 'out' : \n            colors = color_lists [ 'normal' ] \n            colors . number , colors . prompt , colors . normal = scheme . out_number , scheme . out_prompt , scheme . normal \n        else : \n            colors = color_lists [ 'inp' ] \n            colors . number , colors . prompt , colors . normal = scheme . in_number , scheme . in_prompt , scheme . in_normal \n            if name == 'in2' : \n                colors . prompt = scheme . in_prompt2 \n    else : \n        colors = color_lists [ 'nocolor' ] \n        colors . number , colors . prompt , colors . normal = '' , '' , '' \n    count = self . shell . execution_count \n    fmtargs = dict ( color = colors , count = count , dots = \".\" * len ( str ( count ) ) , width = self . width , txtwidth = self . txtwidth ) \n    fmtargs . update ( self . lazy_evaluate_fields ) \n    fmtargs . update ( kwargs ) \n    prompt = colors . prompt + self . templates [ name ] + colors . normal \n    return self . _formatter . format ( prompt , ** fmtargs ) "}
{"13863": "\ndef base_launch_kernel ( code , fname , stdin = None , stdout = None , stderr = None , executable = None , independent = 0 , extra_arguments = [ ] , cwd = None ) : \n    if executable is None : \n        executable = sys . executable \n    arguments = [ executable , '-c' , code , '-f' , fname ] \n    arguments . extend ( extra_arguments ) \n    redirect_in = 1 \n    _stdin = PIPE if stdin is None else stdin \n    redirect_out = sys . executable . endswith ( 'pythonw.exe' ) \n    if redirect_out : \n        _stdout = PIPE if stdout is None else stdout \n        _stderr = PIPE if stderr is None else stderr \n    else : \n        _stdout , _stderr = stdout , stderr \n    if sys . platform == 'win32' : \n        interrupt_event = ParentPollerWindows . create_interrupt_event ( ) \n        arguments += [ '--interrupt=%i' % interrupt_event ] \n        if executable . endswith ( 'pythonw.exe' ) : \n            if stdout is None : \n                arguments . append ( '--no-stdout' ) \n            if stderr is None : \n                arguments . append ( '--no-stderr' ) \n        if independent : \n            proc = Popen ( arguments , creationflags = 512 , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        else : \n            try : \n                from _winapi import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            except : \n                from _subprocess import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            pid = GetCurrentProcess ( ) \n            handle = DuplicateHandle ( pid , pid , pid , 0 , 1 , DUPLICATE_SAME_ACCESS ) \n            proc = Popen ( arguments + [ '--parent=%i' % int ( handle ) ] , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        proc . win32_interrupt_event = interrupt_event \n    else : \n        if independent : \n            proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n        else : \n            proc = Popen ( arguments + [ '--parent=1' ] , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    if redirect_in : \n        if stdin is None : \n            proc . stdin . close ( ) \n    if redirect_out : \n        if stdout is None : \n            proc . stdout . close ( ) \n        if stderr is None : \n            proc . stderr . close ( ) \n    return proc "}
{"13866": "\ndef mappable ( obj ) : \n    if isinstance ( obj , ( tuple , list ) ) : \n        return 1 \n    for m in arrayModules : \n        if isinstance ( obj , m [ 'type' ] ) : \n            return 1 \n    return 0 "}
{"13869": "\ndef run_file ( self , fname , interact = 0 , get_output = 0 ) : \n    fobj = open ( fname , 'r' ) \n    try : \n        out = self . run_source ( fobj , interact , get_output ) \n    finally : \n        fobj . close ( ) \n    if get_output : \n        return out "}
{"13877": "\ndef one_digit_freqs ( digits , normalize = 0 ) : \n    freqs = np . zeros ( 10 , dtype = 'i4' ) \n    for d in digits : \n        freqs [ int ( d ) ] += 1 \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13878": "\ndef two_digit_freqs ( digits , normalize = 0 ) : \n    freqs = np . zeros ( 100 , dtype = 'i4' ) \n    last = digits . next ( ) \n    this = digits . next ( ) \n    for d in digits : \n        index = int ( last + this ) \n        freqs [ index ] += 1 \n        last = this \n        this = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13879": "\ndef n_digit_freqs ( digits , n , normalize = 0 ) : \n    freqs = np . zeros ( pow ( 10 , n ) , dtype = 'i4' ) \n    current = np . zeros ( n , dtype = int ) \n    for i in range ( n ) : \n        current [ i ] = digits . next ( ) \n    for d in digits : \n        index = int ( '' . join ( map ( str , current ) ) ) \n        freqs [ index ] += 1 \n        current [ 0 : - 1 ] = current [ 1 : ] \n        current [ - 1 ] = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13885": "\ndef DocFileSuite ( * paths , ** kw ) : \n    suite = unittest . TestSuite ( ) \n    if kw . get ( 'module_relative' , 1 ) : \n        kw [ 'package' ] = _normalize_module ( kw . get ( 'package' ) ) \n    for path in paths : \n        suite . addTest ( DocFileTest ( path , ** kw ) ) \n    return suite "}
{"13886": "\ndef debug_src ( src , pm = 0 , globs = None ) : \n    testsrc = script_from_examples ( src ) \n    debug_script ( testsrc , pm , globs ) "}
{"13888": "\ndef debug ( module , name , pm = 0 ) : \n    module = _normalize_module ( module ) \n    testsrc = testsource ( module , name ) \n    debug_script ( testsrc , pm , module . __dict__ ) "}
{"13892": "\ndef allow ( self , record ) : \n    if not self : \n        return 1 \n    return self . _allow ( record ) and not self . _deny ( record ) "}
{"13902": "\ndef init_parser ( ) : \n    usage = \"usage: %prog -u user -s secret -n name [-l label] \\[-t title] [-c callback] [TEXT]\" \n    parser = OptionParser ( usage , version = \"%prog \" + notifo . __version__ ) \n    parser . add_option ( \"-u\" , \"--user\" , action = \"store\" , dest = \"user\" , help = \"your notifo username\" ) \n    parser . add_option ( \"-s\" , \"--secret\" , action = \"store\" , dest = \"secret\" , help = \"your notifo API secret\" ) \n    parser . add_option ( \"-n\" , \"--name\" , action = \"store\" , dest = \"name\" , help = \"recipient for the notification\" ) \n    parser . add_option ( \"-l\" , \"--label\" , action = \"store\" , dest = \"label\" , help = \"label for the notification\" ) \n    parser . add_option ( \"-t\" , \"--title\" , action = \"store\" , dest = \"title\" , help = \"title of the notification\" ) \n    parser . add_option ( \"-c\" , \"--callback\" , action = \"store\" , dest = \"callback\" , help = \"callback URL to call\" ) \n    parser . add_option ( \"-m\" , \"--message\" , action = \"store_true\" , dest = \"message\" , default = 0 , help = \"send message instead of notification\" ) \n    ( options , args ) = parser . parse_args ( ) \n    return ( parser , options , args ) "}
{"13915": "\ndef _update_list ( self , hilight = 1 ) : \n    self . _sliding_interval . current = self . _index [ 0 ] \n    head = None \n    foot = None \n    if self . _sliding_interval . start > 0 : \n        head = '...' \n    if self . _sliding_interval . stop < self . _sliding_interval . _max : \n        foot = '...' \n    items_m = self . _justified_items [ self . _sliding_interval . start : self . _sliding_interval . stop + 1 ] \n    self . _console_widget . _clear_temporary_buffer ( ) \n    if ( hilight ) : \n        sel = ( self . _sliding_interval . nth , self . _index [ 1 ] ) \n    else : \n        sel = None \n    strng = html_tableify ( items_m , select = sel , header = head , footer = foot ) \n    self . _console_widget . _fill_temporary_buffer ( self . _old_cursor , strng , html = 1 ) "}
{"13916": "\ndef wordfreq ( text , is_filename = 0 ) : \n    if is_filename : \n        with open ( text ) as f : \n            text = f . read ( ) \n    freqs = { } \n    for word in text . split ( ) : \n        lword = word . lower ( ) \n        freqs [ lword ] = freqs . get ( lword , 0 ) + 1 \n    return freqs "}
{"13917": "\ndef print_wordfreq ( freqs , n = 10 ) : \n    words , counts = freqs . keys ( ) , freqs . values ( ) \n    items = zip ( counts , words ) \n    items . sort ( reverse = 1 ) \n    for ( count , word ) in items [ : n ] : \n        print ( word , count ) "}
{"13921": "\ndef send_shared_pin ( self , topics , pin , skip_validation = 0 ) : \n    if not self . api_key : \n        raise ValueError ( \"You need to specify an api_key.\" ) \n    if not skip_validation : \n        validate_pin ( pin ) \n    response = _request ( 'PUT' , url = self . url_v1 ( '/shared/pins/' + pin [ 'id' ] ) , user_agent = self . user_agent , api_key = self . api_key , topics_list = topics , json = pin , ) \n    _raise_for_status ( response ) "}
{"13923": "\ndef send_user_pin ( self , user_token , pin , skip_validation = 0 ) : \n    if not skip_validation : \n        validate_pin ( pin ) \n    response = _request ( 'PUT' , url = self . url_v1 ( '/user/pins/' + pin [ 'id' ] ) , user_agent = self . user_agent , user_token = user_token , json = pin , ) \n    _raise_for_status ( response ) "}
{"13934": "\ndef page ( strng , start = 0 , screen_lines = 0 , pager_cmd = None , html = None , auto_html = 0 ) : \n    start = max ( 0 , start ) \n    shell = InteractiveShell . instance ( ) \n    if auto_html : \n        try : \n            defaults = { 'file_insertion_enabled' : 0 , 'raw_enabled' : 0 , '_disable_config' : 1 } \n            html = publish_string ( strng , writer_name = 'html' , settings_overrides = defaults ) \n        except : \n            pass \n    payload = dict ( source = 'IPython.zmq.page.page' , text = strng , html = html , start_line_number = start ) \n    shell . payload_manager . write_payload ( payload ) "}
{"13962": "\ndef _append_jpg ( self , jpg , before_prompt = 0 ) : \n    self . _append_custom ( self . _insert_jpg , jpg , before_prompt ) "}
{"13963": "\ndef _append_png ( self , png , before_prompt = 0 ) : \n    self . _append_custom ( self . _insert_png , png , before_prompt ) "}
{"13964": "\ndef _append_svg ( self , svg , before_prompt = 0 ) : \n    self . _append_custom ( self . _insert_svg , svg , before_prompt ) "}
{"13974": "\ndef ask_exit ( self ) : \n    self . exit_now = 1 \n    payload = dict ( source = 'IPython.zmq.zmqshell.ZMQInteractiveShell.ask_exit' , exit = 1 , keepkernel = self . keepkernel_on_exit , ) \n    self . payload_manager . write_payload ( payload ) "}
{"13983": "\ndef expand_user ( path ) : \n    tilde_expand = 0 \n    tilde_val = '' \n    newpath = path \n    if path . startswith ( '~' ) : \n        tilde_expand = 1 \n        rest = len ( path ) - 1 \n        newpath = os . path . expanduser ( path ) \n        if rest : \n            tilde_val = newpath [ : - rest ] \n        else : \n            tilde_val = newpath \n    return newpath , tilde_expand , tilde_val "}
{"13989": "\ndef file_matches ( self , text ) : \n    if text . startswith ( '!' ) : \n        text = text [ 1 : ] \n        text_prefix = '!' \n    else : \n        text_prefix = '' \n    text_until_cursor = self . text_until_cursor \n    open_quotes = has_open_quotes ( text_until_cursor ) \n    if '(' in text_until_cursor or '[' in text_until_cursor : \n        lsplit = text \n    else : \n        try : \n            lsplit = arg_split ( text_until_cursor ) [ - 1 ] \n        except ValueError : \n            if open_quotes : \n                lsplit = text_until_cursor . split ( open_quotes ) [ - 1 ] \n            else : \n                return [ ] \n        except IndexError : \n            lsplit = \"\" \n    if not open_quotes and lsplit != protect_filename ( lsplit ) : \n        has_protectables = 1 \n        text0 , text = text , lsplit \n    else : \n        has_protectables = 0 \n        text = os . path . expanduser ( text ) \n    if text == \"\" : \n        return [ text_prefix + protect_filename ( f ) for f in self . glob ( \"*\" ) ] \n    m0 = self . clean_glob ( text . replace ( '\\\\' , '' ) ) \n    if has_protectables : \n        len_lsplit = len ( lsplit ) \n        matches = [ text_prefix + text0 + protect_filename ( f [ len_lsplit : ] ) for f in m0 ] \n    else : \n        if open_quotes : \n            matches = m0 \n        else : \n            matches = [ text_prefix + protect_filename ( f ) for f in m0 ] \n    matches = [ x + '/' if os . path . isdir ( x ) else x for x in matches ] \n    return matches "}
{"13994": "\ndef rlcomplete ( self , text , state ) : \n    if state == 0 : \n        self . line_buffer = line_buffer = self . readline . get_line_buffer ( ) \n        cursor_pos = self . readline . get_endidx ( ) \n        if not ( self . dumb_terminal or line_buffer . strip ( ) ) : \n            self . readline . insert_text ( '\\t' ) \n            sys . stdout . flush ( ) \n            return None \n        DEBUG = 0 \n        if DEBUG : \n            try : \n                self . complete ( text , line_buffer , cursor_pos ) \n            except : \n                import traceback ; \n                traceback . print_exc ( ) \n        else : \n            self . complete ( text , line_buffer , cursor_pos ) \n    try : \n        return self . matches [ state ] \n    except IndexError : \n        return None "}
{"13995": "\ndef _match_one ( self , rec , tests ) : \n    for key , test in tests . iteritems ( ) : \n        if not test ( rec . get ( key , None ) ) : \n            return 0 \n    return 1 "}
{"13998": "\ndef quiet ( self ) : \n    try : \n        cell = self . shell . history_manager . input_hist_parsed [ self . prompt_count ] \n        if cell . rstrip ( ) . endswith ( ';' ) : \n            return 1 \n    except IndexError : \n        pass \n    return 0 "}
{"14005": "\ndef dispatch_control ( self , msg ) : \n    idents , msg = self . session . feed_identities ( msg , copy = 0 ) \n    try : \n        msg = self . session . unserialize ( msg , content = 1 , copy = 0 ) \n    except : \n        self . log . error ( \"Invalid Control Message\" , exc_info = 1 ) \n        return \n    self . log . debug ( \"Control received: %s\" , msg ) \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    msg_type = header [ 'msg_type' ] \n    handler = self . control_handlers . get ( msg_type , None ) \n    if handler is None : \n        self . log . error ( \"UNKNOWN CONTROL MESSAGE TYPE: %r\" , msg_type ) \n    else : \n        try : \n            handler ( self . control_stream , idents , msg ) \n        except Exception : \n            self . log . error ( \"Exception in control handler:\" , exc_info = 1 ) "}
{"14006": "\ndef dispatch_shell ( self , stream , msg ) : \n    if self . control_stream : \n        self . control_stream . flush ( ) \n    idents , msg = self . session . feed_identities ( msg , copy = 0 ) \n    try : \n        msg = self . session . unserialize ( msg , content = 1 , copy = 0 ) \n    except : \n        self . log . error ( \"Invalid Message\" , exc_info = 1 ) \n        return \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    self . log . debug ( '\\n*** MESSAGE TYPE:%s***' , msg_type ) \n    self . log . debug ( '   Content: %s\\n   --->\\n   ' , msg [ 'content' ] ) \n    if msg_id in self . aborted : \n        self . aborted . remove ( msg_id ) \n        reply_type = msg_type . split ( '_' ) [ 0 ] + '_reply' \n        status = { 'status' : 'aborted' } \n        sub = { 'engine' : self . ident } \n        sub . update ( status ) \n        reply_msg = self . session . send ( stream , reply_type , subheader = sub , content = status , parent = msg , ident = idents ) \n        return \n    handler = self . shell_handlers . get ( msg_type , None ) \n    if handler is None : \n        self . log . error ( \"UNKNOWN MESSAGE TYPE: %r\" , msg_type ) \n    else : \n        sig = signal ( SIGINT , default_int_handler ) \n        try : \n            handler ( stream , idents , msg ) \n        except Exception : \n            self . log . error ( \"Exception in message handler:\" , exc_info = 1 ) \n        finally : \n            signal ( SIGINT , sig ) "}
{"14007": "\ndef start ( self ) : \n    self . shell . exit_now = 0 \n    if self . control_stream : \n        self . control_stream . on_recv ( self . dispatch_control , copy = 0 ) \n    def make_dispatcher ( stream ) : \n        def dispatcher ( msg ) : \n            return self . dispatch_shell ( stream , msg ) \n        return dispatcher \n    for s in self . shell_streams : \n        s . on_recv ( make_dispatcher ( s ) , copy = 0 ) "}
{"14011": "\ndef clear_request ( self , stream , idents , parent ) : \n    self . shell . reset ( 0 ) \n    msg = self . session . send ( stream , 'clear_reply' , ident = idents , parent = parent , content = dict ( status = 'ok' ) ) "}
{"14019": "\ndef ispackage ( path ) : \n    if os . path . isdir ( path ) : \n        end = os . path . basename ( path ) \n        if ident_re . match ( end ) : \n            for init in ( '__init__.py' , '__init__.pyc' , '__init__.pyo' ) : \n                if os . path . isfile ( os . path . join ( path , init ) ) : \n                    return 1 \n            if sys . platform . startswith ( 'java' ) and os . path . isfile ( os . path . join ( path , '__init__$py.class' ) ) : \n                return 1 \n    return 0 "}
{"14029": "\ndef user_has_group ( user , group , superuser_skip = 1 ) : \n    if user . is_superuser and superuser_skip : \n        return 1 \n    return user . groups . filter ( name = group ) . exists ( ) "}
{"14038": "\ndef _upload_file_to_gdoc ( self , file_path , content_type = 'application/x-vnd.oasis.opendocument.spreadsheet' ) : \n    try : \n        entry = self . gd_client . GetResourceById ( self . key ) \n        media = gdata . data . MediaSource ( file_path = file_path , content_type = content_type ) \n        self . gd_client . UpdateResource ( entry , media = media , update_metadata = 1 ) \n    except ( RequestError , IOError ) as e : \n        raise PODocsError ( e ) "}
{"14044": "\ndef check_url_accessibility ( url , timeout = 10 ) : \n    if ( url == 'localhost' ) : \n        url = 'http://127.0.0.1' \n    try : \n        req = urllib2 . urlopen ( url , timeout = timeout ) \n        if ( req . getcode ( ) == 200 ) : \n            return 1 \n    except Exception : \n        pass \n    fail ( \"URL '%s' is not accessible from this machine\" % url ) "}
{"14047": "\ndef compare_content_type ( url , content_type ) : \n    try : \n        response = urllib2 . urlopen ( url ) \n    except : \n        return 0 \n    return response . headers . type == content_type "}
{"14048": "\ndef compare_response_code ( url , code ) : \n    try : \n        response = urllib2 . urlopen ( url ) \n    except HTTPError as e : \n        return e . code == code \n    except : \n        return 0 \n    return response . code == code "}
{"14050": "\ndef clear_output ( self , stdout = 1 , stderr = 1 , other = 1 ) : \n    if stdout : \n        print ( '\\033[2K\\r' , file = io . stdout , end = '' ) \n        io . stdout . flush ( ) \n    if stderr : \n        print ( '\\033[2K\\r' , file = io . stderr , end = '' ) \n        io . stderr . flush ( ) "}
{"14055": "\ndef should_be_python ( self ) : \n    _ , ext = os . path . splitext ( self . filename ) \n    if ext . startswith ( '.py' ) : \n        return 1 \n    if not ext : \n        return 1 \n    return 0 "}
{"14060": "\ndef abort ( self ) : \n    assert not self . ready ( ) , \"Can't abort, I am already done!\" \n    return self . _client . abort ( self . msg_ids , targets = self . _targets , block = 1 ) "}
{"14073": "\ndef match ( self , fpath ) : \n    for d in self . dirs : \n        if fpath . startswith ( d ) : \n            if fpath == d : \n                return 1 \n            if fpath [ len ( d ) ] == os . sep : \n                return 1 \n    return 0 "}
{"14074": "\ndef match ( self , fpath ) : \n    for pat in self . pats : \n        if fnmatch . fnmatch ( fpath , pat ) : \n            return 1 \n    return 0 "}
{"14076": "\ndef loop_qt4 ( kernel ) : \n    from IPython . external . qt_for_kernel import QtCore \n    from IPython . lib . guisupport import get_app_qt4 , start_event_loop_qt4 \n    kernel . app = get_app_qt4 ( [ \" \" ] ) \n    kernel . app . setQuitOnLastWindowClosed ( 0 ) \n    kernel . timer = QtCore . QTimer ( ) \n    kernel . timer . timeout . connect ( kernel . do_one_iteration ) \n    kernel . timer . start ( 1000 * kernel . _poll_interval ) \n    start_event_loop_qt4 ( kernel . app ) "}
{"14077": "\ndef loop_wx ( kernel ) : \n    import wx \n    from IPython . lib . guisupport import start_event_loop_wx \n    doi = kernel . do_one_iteration \n    poll_interval = int ( 1000 * kernel . _poll_interval ) \n    class TimerFrame ( wx . Frame ) : \n        def __init__ ( self , func ) : \n            wx . Frame . __init__ ( self , None , - 1 ) \n            self . timer = wx . Timer ( self ) \n            self . timer . Start ( poll_interval ) \n            self . Bind ( wx . EVT_TIMER , self . on_timer ) \n            self . func = func \n        def on_timer ( self , event ) : \n            self . func ( ) \n    class IPWxApp ( wx . App ) : \n        def OnInit ( self ) : \n            self . frame = TimerFrame ( doi ) \n            self . frame . Show ( 0 ) \n            return 1 \n    kernel . app = IPWxApp ( redirect = 0 ) \n    import signal \n    if not callable ( signal . getsignal ( signal . SIGINT ) ) : \n        signal . signal ( signal . SIGINT , signal . default_int_handler ) \n    start_event_loop_wx ( kernel . app ) "}
{"14080": "\ndef loop_cocoa ( kernel ) : \n    import matplotlib \n    if matplotlib . __version__ < '1.1.0' : \n        kernel . log . warn ( \"MacOSX backend in matplotlib %s doesn't have a Timer, \" \"falling back on Tk for CFRunLoop integration.  Note that \" \"even this won't work if Tk is linked against X11 instead of \" \"Cocoa (e.g. EPD).  To use the MacOSX backend in the kernel, \" \"you must use matplotlib >= 1.1.0, or a native libtk.\" ) \n        return loop_tk ( kernel ) \n    from matplotlib . backends . backend_macosx import TimerMac , show \n    poll_interval = int ( 1000 * kernel . _poll_interval ) \n    real_excepthook = sys . excepthook \n    def handle_int ( etype , value , tb ) : \n        if etype is KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in CFRunLoop\" ) \n        else : \n            real_excepthook ( etype , value , tb ) \n    def doi ( ) : \n        sys . excepthook = real_excepthook \n        kernel . do_one_iteration ( ) \n        sys . excepthook = handle_int \n    t = TimerMac ( poll_interval ) \n    t . add_callback ( doi ) \n    t . start ( ) \n    poller = zmq . Poller ( ) \n    if kernel . control_stream : \n        poller . register ( kernel . control_stream . socket , zmq . POLLIN ) \n    for stream in kernel . shell_streams : \n        poller . register ( stream . socket , zmq . POLLIN ) \n    while 1 : \n        try : \n            try : \n                sys . excepthook = handle_int \n                show . mainloop ( ) \n                sys . excepthook = real_excepthook \n                poller . poll ( 10 * poll_interval ) \n                kernel . do_one_iteration ( ) \n            except : \n                raise \n        except KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in kernel\" ) \n        finally : \n            sys . excepthook = real_excepthook "}
{"14089": "\ndef load_config_file ( self , suppress_errors = 1 ) : \n    self . log . debug ( \"Searching path %s for config files\" , self . config_file_paths ) \n    base_config = 'ipython_config.py' \n    self . log . debug ( \"Attempting to load config file: %s\" % base_config ) \n    try : \n        Application . load_config_file ( self , base_config , path = self . config_file_paths ) \n    except ConfigFileNotFound : \n        self . log . debug ( \"Config file %s not found\" , base_config ) \n        pass \n    if self . config_file_name == base_config : \n        return \n    self . log . debug ( \"Attempting to load config file: %s\" % self . config_file_name ) \n    try : \n        Application . load_config_file ( self , self . config_file_name , path = self . config_file_paths ) \n    except ConfigFileNotFound : \n        if self . config_file_specified : \n            msg = self . log . warn \n        else : \n            msg = self . log . debug \n        msg ( \"Config file not found, skipping: %s\" , self . config_file_name ) \n    except : \n        if not suppress_errors : \n            raise \n        self . log . warn ( \"Error loading config file: %s\" % self . config_file_name , exc_info = 1 ) "}
{"14104": "\ndef summary ( self , fullpath = 0 ) : \n    summ = { } \n    if fullpath : \n        filename_fn = lambda f : f \n    else : \n        filename_fn = os . path . basename \n    for filename , lines in iitems ( self . lines ) : \n        summ [ filename_fn ( filename ) ] = len ( lines ) \n    return summ "}
{"14110": "\ndef _should_recompile ( self , e ) : \n    if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<BackgroundJob compilation>' , None ) : \n        return 0 \n    try : \n        if ( self . autoedit_syntax and not self . ask_yes_no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : \n            return 0 \n    except EOFError : \n        return 0 \n    def int0 ( x ) : \n        try : \n            return int ( x ) \n        except TypeError : \n            return 0 \n    try : \n        self . hooks . fix_error_editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) \n    except TryNext : \n        warn ( 'Could not open editor' ) \n        return 0 \n    return 1 "}
{"14113": "\ndef new_frontend_master ( self ) : \n    ip = self . ip if self . ip in LOCAL_IPS else LOCALHOST \n    kernel_manager = self . kernel_manager_class ( ip = ip , connection_file = self . _new_connection_file ( ) , config = self . config , ) \n    kwargs = dict ( ) \n    kwargs [ 'extra_arguments' ] = self . kernel_argv \n    kernel_manager . start_kernel ( ** kwargs ) \n    kernel_manager . start_channels ( ) \n    widget = self . widget_factory ( config = self . config , local_kernel = 1 ) \n    self . init_colors ( widget ) \n    widget . kernel_manager = kernel_manager \n    widget . _existing = 0 \n    widget . _may_close = 1 \n    widget . _confirm_exit = self . confirm_exit \n    return widget "}
{"14116": "\ndef Rconverter ( Robj , dataframe = 0 ) : \n    is_data_frame = ro . r ( 'is.data.frame' ) \n    colnames = ro . r ( 'colnames' ) \n    rownames = ro . r ( 'rownames' ) \n    names = ro . r ( 'names' ) \n    if dataframe : \n        as_data_frame = ro . r ( 'as.data.frame' ) \n        cols = colnames ( Robj ) \n        _names = names ( Robj ) \n        if cols != ri . NULL : \n            Robj = as_data_frame ( Robj ) \n            names = tuple ( np . array ( cols ) ) \n        elif _names != ri . NULL : \n            names = tuple ( np . array ( _names ) ) \n        else : \n            return np . asarray ( Robj ) \n        Robj = np . rec . fromarrays ( Robj , names = names ) \n    return np . asarray ( Robj ) "}
{"14123": "\ndef _format_exception_only ( self , etype , value ) : \n    have_filedata = 0 \n    Colors = self . Colors \n    list = [ ] \n    stype = Colors . excName + etype . __name__ + Colors . Normal \n    if value is None : \n        list . append ( str ( stype ) + '\\n' ) \n    else : \n        if etype is SyntaxError : \n            have_filedata = 1 \n            if not value . filename : \n                value . filename = \"<string>\" \n            list . append ( '%s  File %s\"%s\"%s, line %s%d%s\\n' % ( Colors . normalEm , Colors . filenameEm , value . filename , Colors . normalEm , Colors . linenoEm , value . lineno , Colors . Normal ) ) \n            if value . text is not None : \n                i = 0 \n                while i < len ( value . text ) and value . text [ i ] . isspace ( ) : \n                    i += 1 \n                list . append ( '%s    %s%s\\n' % ( Colors . line , value . text . strip ( ) , Colors . Normal ) ) \n                if value . offset is not None : \n                    s = '    ' \n                    for c in value . text [ i : value . offset - 1 ] : \n                        if c . isspace ( ) : \n                            s += c \n                        else : \n                            s += ' ' \n                    list . append ( '%s%s^%s\\n' % ( Colors . caret , s , Colors . Normal ) ) \n        try : \n            s = value . msg \n        except Exception : \n            s = self . _some_str ( value ) \n        if s : \n            list . append ( '%s%s:%s %s\\n' % ( str ( stype ) , Colors . excName , Colors . Normal , s ) ) \n        else : \n            list . append ( '%s\\n' % str ( stype ) ) \n    if have_filedata : \n        ipinst = ipapi . get ( ) \n        if ipinst is not None : \n            ipinst . hooks . synchronize_with_editor ( value . filename , value . lineno , 0 ) \n    return list "}
{"14125": "\ndef debugger ( self , force = 0 ) : \n    if force or self . call_pdb : \n        if self . pdb is None : \n            self . pdb = debugger . Pdb ( self . color_scheme_table . active_scheme_name ) \n        display_trap = DisplayTrap ( hook = sys . __displayhook__ ) \n        with display_trap : \n            self . pdb . reset ( ) \n            if hasattr ( self , 'tb' ) and self . tb is not None : \n                etb = self . tb \n            else : \n                etb = self . tb = sys . last_traceback \n            while self . tb is not None and self . tb . tb_next is not None : \n                self . tb = self . tb . tb_next \n            if etb and etb . tb_next : \n                etb = etb . tb_next \n            self . pdb . botframe = etb . tb_frame \n            self . pdb . interaction ( self . tb . tb_frame , self . tb ) \n    if hasattr ( self , 'tb' ) : \n        del self . tb "}
{"14127": "\ndef group_required ( group , login_url = None , redirect_field_name = REDIRECT_FIELD_NAME , skip_superuser = 1 ) : \n    def decorator ( view_func ) : \n        \n        @ login_required ( redirect_field_name = redirect_field_name , login_url = login_url ) \n        def _wrapped_view ( request , * args , ** kwargs ) : \n            if not ( request . user . is_superuser and skip_superuser ) : \n                if request . user . groups . filter ( name = group ) . count ( ) == 0 : \n                    raise PermissionDenied \n            return view_func ( request , * args , ** kwargs ) \n        return _wrapped_view \n    return decorator "}
{"14143": "\ndef configureWhere ( self , where ) : \n    from nose . importer import add_path \n    self . workingDir = None \n    where = tolist ( where ) \n    warned = 0 \n    for path in where : \n        if not self . workingDir : \n            abs_path = absdir ( path ) \n            if abs_path is None : \n                raise ValueError ( \"Working directory %s not found, or \" \"not a directory\" % path ) \n            log . info ( \"Set working dir to %s\" , abs_path ) \n            self . workingDir = abs_path \n            if self . addPaths and os . path . exists ( os . path . join ( abs_path , '__init__.py' ) ) : \n                log . info ( \"Working directory %s is a package; \" \"adding to sys.path\" % abs_path ) \n                add_path ( abs_path ) \n            continue \n        if not warned : \n            warn ( \"Use of multiple -w arguments is deprecated and \" \"support may be removed in a future release. You can \" \"get the same behavior by passing directories without \" \"the -w argument on the command line, or by using the \" \"--tests argument in a configuration file.\" , DeprecationWarning ) \n        self . testNames . append ( path ) "}
{"14150": "\ndef print_basic_unicode ( o , p , cycle ) : \n    if cycle : \n        return p . text ( 'Basic(...)' ) \n    out = pretty ( o , use_unicode = 1 ) \n    if '\\n' in out : \n        p . text ( u'\\n' ) \n    p . text ( out ) "}
{"14153": "\ndef can_print_latex ( o ) : \n    import sympy \n    if isinstance ( o , ( list , tuple , set , frozenset ) ) : \n        return all ( can_print_latex ( i ) for i in o ) \n    elif isinstance ( o , dict ) : \n        return all ( ( isinstance ( i , basestring ) or can_print_latex ( i ) ) and can_print_latex ( o [ i ] ) for i in o ) \n    elif isinstance ( o , ( sympy . Basic , sympy . matrices . Matrix , int , long , float ) ) : \n        return 1 \n    return 0 "}
{"14158": "\ndef _run_loop ( self ) : \n    while 1 : \n        try : \n            self . ioloop . start ( ) \n        except ZMQError as e : \n            if e . errno == errno . EINTR : \n                continue \n            else : \n                raise \n        except Exception : \n            if self . _exiting : \n                break \n            else : \n                raise \n        else : \n            break "}
{"14160": "\ndef execute ( self , code , silent = 0 , user_variables = None , user_expressions = None , allow_stdin = None ) : \n    if user_variables is None : \n        user_variables = [ ] \n    if user_expressions is None : \n        user_expressions = { } \n    if allow_stdin is None : \n        allow_stdin = self . allow_stdin \n    if not isinstance ( code , basestring ) : \n        raise ValueError ( 'code %r must be a string' % code ) \n    validate_string_list ( user_variables ) \n    validate_string_dict ( user_expressions ) \n    content = dict ( code = code , silent = silent , user_variables = user_variables , user_expressions = user_expressions , allow_stdin = allow_stdin , ) \n    msg = self . session . msg ( 'execute_request' , content ) \n    self . _queue_send ( msg ) \n    return msg [ 'header' ] [ 'msg_id' ] "}
{"14163": "\ndef history ( self , raw = 1 , output = 0 , hist_access_type = 'range' , ** kwargs ) : \n    content = dict ( raw = raw , output = output , hist_access_type = hist_access_type , ** kwargs ) \n    msg = self . session . msg ( 'history_request' , content ) \n    self . _queue_send ( msg ) \n    return msg [ 'header' ] [ 'msg_id' ] "}
{"14164": "\ndef shutdown ( self , restart = 0 ) : \n    msg = self . session . msg ( 'shutdown_request' , { 'restart' : restart } ) \n    self . _queue_send ( msg ) \n    return msg [ 'header' ] [ 'msg_id' ] "}
{"14165": "\ndef flush ( self , timeout = 1.0 ) : \n    stop_time = time . time ( ) + timeout \n    for i in xrange ( 2 ) : \n        self . _flushed = 0 \n        self . ioloop . add_callback ( self . _flush ) \n        while not self . _flushed and time . time ( ) < stop_time : \n            time . sleep ( 0.01 ) "}
{"14167": "\ndef start_channels ( self , shell = 1 , sub = 1 , stdin = 1 , hb = 1 ) : \n    if shell : \n        self . shell_channel . start ( ) \n    if sub : \n        self . sub_channel . start ( ) \n    if stdin : \n        self . stdin_channel . start ( ) \n        self . shell_channel . allow_stdin = 1 \n    else : \n        self . shell_channel . allow_stdin = 0 \n    if hb : \n        self . hb_channel . start ( ) "}
{"14171": "\ndef write_connection_file ( self ) : \n    if self . _connection_file_written : \n        return \n    self . connection_file , cfg = write_connection_file ( self . connection_file , ip = self . ip , key = self . session . key , stdin_port = self . stdin_port , iopub_port = self . iopub_port , shell_port = self . shell_port , hb_port = self . hb_port ) \n    self . shell_port = cfg [ 'shell_port' ] \n    self . stdin_port = cfg [ 'stdin_port' ] \n    self . iopub_port = cfg [ 'iopub_port' ] \n    self . hb_port = cfg [ 'hb_port' ] \n    self . _connection_file_written = 1 "}
{"14173": "\ndef shutdown_kernel ( self , restart = 0 ) : \n    if sys . platform == 'win32' : \n        self . kill_kernel ( ) \n        return \n    if self . _hb_channel is not None : \n        self . _hb_channel . pause ( ) \n    self . shell_channel . shutdown ( restart = restart ) \n    for i in range ( 10 ) : \n        if self . is_alive : \n            time . sleep ( 0.1 ) \n        else : \n            break \n    else : \n        if self . has_kernel : \n            self . kill_kernel ( ) \n    if not restart and self . _connection_file_written : \n        self . _connection_file_written = 0 \n        try : \n            os . remove ( self . connection_file ) \n        except IOError : \n            pass "}
{"14174": "\ndef restart_kernel ( self , now = 0 , ** kw ) : \n    if self . _launch_args is None : \n        raise RuntimeError ( \"Cannot restart the kernel. \" \"No previous call to 'start_kernel'.\" ) \n    else : \n        if self . has_kernel : \n            if now : \n                self . kill_kernel ( ) \n            else : \n                self . shutdown_kernel ( restart = 1 ) \n        self . _launch_args . update ( kw ) \n        self . start_kernel ( ** self . _launch_args ) \n        if sys . platform == 'win32' : \n            time . sleep ( 0.2 ) "}
{"14178": "\ndef is_alive ( self ) : \n    if self . has_kernel : \n        if self . kernel . poll ( ) is None : \n            return 1 \n        else : \n            return 0 \n    elif self . _hb_channel is not None : \n        return self . _hb_channel . is_beating ( ) \n    else : \n        return 1 "}
{"14185": "\ndef pre_step ( self , ctxt , step , idx ) : \n    debugger = ExtensionDebugger ( 'pre_step' ) \n    for ext in self . exts : \n        with debugger ( ext ) : \n            if ext . pre_step ( ctxt , step , idx ) : \n                debugger . debug ( 3 , 'Skipping step %d' % idx ) \n                return 1 \n    return 0 "}
{"14189": "\ndef scan_module ( egg_dir , base , name , stubs ) : \n    filename = os . path . join ( base , name ) \n    if filename [ : - 1 ] in stubs : \n        return 1 \n    pkg = base [ len ( egg_dir ) + 1 : ] . replace ( os . sep , '.' ) \n    module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] \n    if sys . version_info < ( 3 , 3 ) : \n        skip = 8 \n    else : \n        skip = 12 \n    f = open ( filename , 'rb' ) ; \n    f . read ( skip ) \n    code = marshal . load ( f ) ; \n    f . close ( ) \n    safe = 1 \n    symbols = dict . fromkeys ( iter_symbols ( code ) ) \n    for bad in [ '__file__' , '__path__' ] : \n        if bad in symbols : \n            log . warn ( \"%s: module references %s\" , module , bad ) \n            safe = 0 \n    if 'inspect' in symbols : \n        for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : \n            if bad in symbols : \n                log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) \n                safe = 0 \n    if '__name__' in symbols and '__main__' in symbols and '.' not in module : \n        if sys . version [ : 3 ] == \"2.4\" : \n            log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) \n            safe = 0 \n    return safe "}
{"14193": "\ndef load_secondary_config ( self ) : \n    if self . reuse_files : \n        try : \n            self . load_config_from_json ( ) \n        except ( AssertionError , IOError ) as e : \n            self . log . error ( \"Could not load config from JSON: %s\" % e ) \n        else : \n            self . write_connection_files = 0 \n    default_secure ( self . config ) \n    self . log . debug ( \"Config changed\" ) \n    self . log . debug ( repr ( self . config ) ) "}
{"14197": "\ndef pxrun_cell ( self , raw_cell , store_history = 0 , silent = 0 ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    ipself = self . shell \n    with ipself . builtin_trap : \n        cell = ipself . prefilter_manager . prefilter_lines ( raw_cell ) \n        if store_history : \n            ipself . history_manager . store_inputs ( ipself . execution_count , cell , raw_cell ) \n        cell_name = ipself . compile . cache ( cell , ipself . execution_count ) \n        try : \n            ast . parse ( cell , filename = cell_name ) \n        except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n            ipself . showsyntaxerror ( ) \n            ipself . execution_count += 1 \n            return None \n        except NameError : \n            pass \n    if store_history : \n        ipself . history_manager . store_output ( ipself . execution_count ) \n        ipself . execution_count += 1 \n    if re . search ( r'get_ipython\\(\\)\\.magic\\(u?[\"\\']%?autopx' , cell ) : \n        self . _disable_autopx ( ) \n        return 0 \n    else : \n        try : \n            result = self . view . execute ( cell , silent = 0 , block = 0 ) \n        except : \n            ipself . showtraceback ( ) \n            return 1 \n        else : \n            if self . view . block : \n                try : \n                    result . get ( ) \n                except : \n                    self . shell . showtraceback ( ) \n                    return 1 \n                else : \n                    with ipself . builtin_trap : \n                        result . display_outputs ( ) \n            return 0 "}
{"14199": "\ndef run_task ( message ) : \n    task = Task . objects . get ( pk = message [ 'id' ] ) \n    if task . allow_overlap : \n        task . run ( message ) \n    else : \n        if not task . running : \n            task . running = 1 \n            task . save ( ) \n            try : \n                task . run ( message ) \n            finally : \n                task . running = 0 \n                task . save ( ) "}
{"14201": "\ndef patch_protocol_for_agent ( protocol ) : \n    old_makeConnection = protocol . makeConnection \n    old_connectionLost = protocol . connectionLost \n    def new_makeConnection ( transport ) : \n        patch_transport_fake_push_producer ( transport ) \n        patch_transport_abortConnection ( transport , protocol ) \n        return old_makeConnection ( transport ) \n    def new_connectionLost ( reason ) : \n        if protocol . _fake_connection_aborted and reason . check ( ConnectionDone ) : \n            reason = Failure ( ConnectionAborted ( ) ) \n        return old_connectionLost ( reason ) \n    protocol . makeConnection = new_makeConnection \n    protocol . connectionLost = new_connectionLost \n    protocol . _fake_connection_aborted = 0 "}
{"14206": "\ndef form_valid ( self , form ) : \n    self . object = form . save ( commit = 0 ) \n    response = self . pre_save ( self . object ) \n    if response : \n        return response \n    self . object . save ( ) \n    form . save_m2m ( ) \n    self . post_save ( self . object ) \n    return HttpResponseRedirect ( self . get_success_url ( ) ) "}
{"14209": "\ndef report ( self , morfs , outfile = None ) : \n    self . find_code_units ( morfs ) \n    max_name = max ( [ len ( cu . name ) for cu in self . code_units ] + [ 5 ] ) \n    fmt_name = \"%%- %ds  \" % max_name \n    fmt_err = \"%s   %s: %s\\n\" \n    header = ( fmt_name % \"Name\" ) + \" Stmts   Miss\" \n    fmt_coverage = fmt_name + \"%6d %6d\" \n    if self . branches : \n        header += \" Branch BrMiss\" \n        fmt_coverage += \" %6d %6d\" \n    width100 = Numbers . pc_str_width ( ) \n    header += \"%*s\" % ( width100 + 4 , \"Cover\" ) \n    fmt_coverage += \"%%%ds%%%%\" % ( width100 + 3 , ) \n    if self . config . show_missing : \n        header += \"   Missing\" \n        fmt_coverage += \"   %s\" \n    rule = \"-\" * len ( header ) + \"\\n\" \n    header += \"\\n\" \n    fmt_coverage += \"\\n\" \n    if not outfile : \n        outfile = sys . stdout \n    outfile . write ( header ) \n    outfile . write ( rule ) \n    total = Numbers ( ) \n    for cu in self . code_units : \n        try : \n            analysis = self . coverage . _analyze ( cu ) \n            nums = analysis . numbers \n            args = ( cu . name , nums . n_statements , nums . n_missing ) \n            if self . branches : \n                args += ( nums . n_branches , nums . n_missing_branches ) \n            args += ( nums . pc_covered_str , ) \n            if self . config . show_missing : \n                args += ( analysis . missing_formatted ( ) , ) \n            outfile . write ( fmt_coverage % args ) \n            total += nums \n        except KeyboardInterrupt : \n            raise \n        except : \n            report_it = not self . config . ignore_errors \n            if report_it : \n                typ , msg = sys . exc_info ( ) [ : 2 ] \n                if typ is NotPython and not cu . should_be_python ( ) : \n                    report_it = 0 \n            if report_it : \n                outfile . write ( fmt_err % ( cu . name , typ . __name__ , msg ) ) \n    if total . n_files > 1 : \n        outfile . write ( rule ) \n        args = ( \"TOTAL\" , total . n_statements , total . n_missing ) \n        if self . branches : \n            args += ( total . n_branches , total . n_missing_branches ) \n        args += ( total . pc_covered_str , ) \n        if self . config . show_missing : \n            args += ( \"\" , ) \n        outfile . write ( fmt_coverage % args ) \n    return total . pc_covered "}
{"14210": "\ndef check ( self , check_all = 0 ) : \n    if not self . enabled and not check_all : \n        return \n    if check_all or self . check_all : \n        modules = sys . modules . keys ( ) \n    else : \n        modules = self . modules . keys ( ) \n    for modname in modules : \n        m = sys . modules . get ( modname , None ) \n        if modname in self . skip_modules : \n            continue \n        if not hasattr ( m , '__file__' ) : \n            continue \n        if m . __name__ == '__main__' : \n            continue \n        filename = m . __file__ \n        path , ext = os . path . splitext ( filename ) \n        if ext . lower ( ) == '.py' : \n            ext = PY_COMPILED_EXT \n            pyc_filename = pyfile . cache_from_source ( filename ) \n            py_filename = filename \n        else : \n            pyc_filename = filename \n            try : \n                py_filename = pyfile . source_from_cache ( filename ) \n            except ValueError : \n                continue \n        try : \n            pymtime = os . stat ( py_filename ) . st_mtime \n            if pymtime <= os . stat ( pyc_filename ) . st_mtime : \n                continue \n            if self . failed . get ( py_filename , None ) == pymtime : \n                continue \n        except OSError : \n            continue \n        try : \n            superreload ( m , reload , self . old_objects ) \n            if py_filename in self . failed : \n                del self . failed [ py_filename ] \n        except : \n            print >> sys . stderr , \"[autoreload of %s failed: %s]\" % ( modname , traceback . format_exc ( 1 ) ) \n            self . failed [ py_filename ] = pymtime "}
{"14211": "\ndef editor ( self , filename , linenum = None , wait = 1 ) : \n    editor = self . editor \n    if linenum is None or editor == 'notepad' : \n        linemark = '' \n    else : \n        linemark = '+%d' % int ( linenum ) \n    if ' ' in editor and os . path . isfile ( editor ) and editor [ 0 ] != '\"' : \n        editor = '\"%s\"' % editor \n    proc = subprocess . Popen ( '%s %s %s' % ( editor , linemark , filename ) , shell = 1 ) \n    if wait and proc . wait ( ) != 0 : \n        raise TryNext ( ) "}
{"14219": "\ndef _try_passwordless_openssh ( server , keyfile ) : \n    if pexpect is None : \n        raise ImportError ( \"pexpect unavailable, use paramiko\" ) \n    cmd = 'ssh -f ' + server \n    if keyfile : \n        cmd += ' -i ' + keyfile \n    cmd += ' exit' \n    p = pexpect . spawn ( cmd ) \n    while 1 : \n        try : \n            p . expect ( '[Pp]assword:' , timeout = .1 ) \n        except pexpect . TIMEOUT : \n            continue \n        except pexpect . EOF : \n            return 1 \n        else : \n            return 0 "}
{"14220": "\ndef _try_passwordless_paramiko ( server , keyfile ) : \n    if paramiko is None : \n        msg = \"Paramiko unavaliable, \" \n        if sys . platform == 'win32' : \n            msg += \"Paramiko is required for ssh tunneled connections on Windows.\" \n        else : \n            msg += \"use OpenSSH.\" \n        raise ImportError ( msg ) \n    username , server , port = _split_server ( server ) \n    client = paramiko . SSHClient ( ) \n    client . load_system_host_keys ( ) \n    client . set_missing_host_key_policy ( paramiko . WarningPolicy ( ) ) \n    try : \n        client . connect ( server , port , username = username , key_filename = keyfile , look_for_keys = 1 ) \n    except paramiko . AuthenticationException : \n        return 0 \n    else : \n        client . close ( ) \n        return 1 "}
{"14232": "\ndef _flush_iopub ( self , sock ) : \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        if self . debug : \n            pprint ( msg ) \n        parent = msg [ 'parent_header' ] \n        if not parent : \n            continue \n        msg_id = parent [ 'msg_id' ] \n        content = msg [ 'content' ] \n        header = msg [ 'header' ] \n        msg_type = msg [ 'header' ] [ 'msg_type' ] \n        md = self . metadata [ msg_id ] \n        if msg_type == 'stream' : \n            name = content [ 'name' ] \n            s = md [ name ] or '' \n            md [ name ] = s + content [ 'data' ] \n        elif msg_type == 'pyerr' : \n            md . update ( { 'pyerr' : self . _unwrap_exception ( content ) } ) \n        elif msg_type == 'pyin' : \n            md . update ( { 'pyin' : content [ 'code' ] } ) \n        elif msg_type == 'display_data' : \n            md [ 'outputs' ] . append ( content ) \n        elif msg_type == 'pyout' : \n            md [ 'pyout' ] = content \n        elif msg_type == 'status' : \n            if content [ 'execution_state' ] == 'idle' : \n                md [ 'outputs_ready' ] = 1 \n        else : \n            pass \n        self . metadata [ msg_id ] = md \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14233": "\ndef _spin_every ( self , interval = 1 ) : \n    while 1 : \n        if self . _stop_spinning . is_set ( ) : \n            return \n        time . sleep ( interval ) \n        self . spin ( ) "}
{"14236": "\ndef wait ( self , jobs = None , timeout = - 1 ) : \n    tic = time . time ( ) \n    if jobs is None : \n        theids = self . outstanding \n    else : \n        if isinstance ( jobs , ( int , basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        theids = set ( ) \n        for job in jobs : \n            if isinstance ( job , int ) : \n                job = self . history [ job ] \n            elif isinstance ( job , AsyncResult ) : \n                map ( theids . add , job . msg_ids ) \n                continue \n            theids . add ( job ) \n    if not theids . intersection ( self . outstanding ) : \n        return 1 \n    self . spin ( ) \n    while theids . intersection ( self . outstanding ) : \n        if timeout >= 0 and ( time . time ( ) - tic ) > timeout : \n            break \n        time . sleep ( 1e-3 ) \n        self . spin ( ) \n    return len ( theids . intersection ( self . outstanding ) ) == 0 "}
{"14237": "\ndef send_apply_request ( self , socket , f , args = None , kwargs = None , subheader = None , track = 0 , ident = None ) : \n    if self . _closed : \n        raise RuntimeError ( \"Client cannot be used after its sockets have been closed\" ) \n    args = args if args is not None else [ ] \n    kwargs = kwargs if kwargs is not None else { } \n    subheader = subheader if subheader is not None else { } \n    if not callable ( f ) and not isinstance ( f , Reference ) : \n        raise TypeError ( \"f must be callable, not %s\" % type ( f ) ) \n    if not isinstance ( args , ( tuple , list ) ) : \n        raise TypeError ( \"args must be tuple or list, not %s\" % type ( args ) ) \n    if not isinstance ( kwargs , dict ) : \n        raise TypeError ( \"kwargs must be dict, not %s\" % type ( kwargs ) ) \n    if not isinstance ( subheader , dict ) : \n        raise TypeError ( \"subheader must be dict, not %s\" % type ( subheader ) ) \n    bufs = util . pack_apply_message ( f , args , kwargs ) \n    msg = self . session . send ( socket , \"apply_request\" , buffers = bufs , ident = ident , subheader = subheader , track = track ) \n    msg_id = msg [ 'header' ] [ 'msg_id' ] \n    self . outstanding . add ( msg_id ) \n    if ident : \n        if isinstance ( ident , list ) : \n            ident = ident [ - 1 ] \n        if ident in self . _engines . values ( ) : \n            self . _outstanding_dict [ ident ] . add ( msg_id ) \n    self . history . append ( msg_id ) \n    self . metadata [ msg_id ] [ 'submitted' ] = datetime . now ( ) \n    return msg "}
{"14238": "\ndef send_execute_request ( self , socket , code , silent = 1 , subheader = None , ident = None ) : \n    if self . _closed : \n        raise RuntimeError ( \"Client cannot be used after its sockets have been closed\" ) \n    subheader = subheader if subheader is not None else { } \n    if not isinstance ( code , basestring ) : \n        raise TypeError ( \"code must be text, not %s\" % type ( code ) ) \n    if not isinstance ( subheader , dict ) : \n        raise TypeError ( \"subheader must be dict, not %s\" % type ( subheader ) ) \n    content = dict ( code = code , silent = bool ( silent ) , user_variables = [ ] , user_expressions = { } ) \n    msg = self . session . send ( socket , \"execute_request\" , content = content , ident = ident , subheader = subheader ) \n    msg_id = msg [ 'header' ] [ 'msg_id' ] \n    self . outstanding . add ( msg_id ) \n    if ident : \n        if isinstance ( ident , list ) : \n            ident = ident [ - 1 ] \n        if ident in self . _engines . values ( ) : \n            self . _outstanding_dict [ ident ] . add ( msg_id ) \n    self . history . append ( msg_id ) \n    self . metadata [ msg_id ] [ 'submitted' ] = datetime . now ( ) \n    return msg "}
{"14240": "\ndef queue_status ( self , targets = 'all' , verbose = 0 ) : \n    if targets == 'all' : \n        engine_ids = None \n    else : \n        engine_ids = self . _build_targets ( targets ) [ 1 ] \n    content = dict ( targets = engine_ids , verbose = verbose ) \n    self . session . send ( self . _query_socket , \"queue_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , 0 ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    status = content . pop ( 'status' ) \n    if status != 'ok' : \n        raise self . _unwrap_exception ( content ) \n    content = rekey ( content ) \n    if isinstance ( targets , int ) : \n        return content [ targets ] \n    else : \n        return content "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = 0 \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = 1 \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent += 1 \n        elif toktype == token . DEDENT : \n            indent -= 1 \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = 1 \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + 1 ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = 0 \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = 0 \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = 0 \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if bc . jump_to >= 0 : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = 0 \n    for bc in bytecodes : \n        start_new_chunk = 0 \n        first_chunk = 0 \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = 1 \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = 1 \n        elif bc . offset in jump_to : \n            start_new_chunk = 1 \n        elif bc . op in OPS_CHUNK_BEGIN : \n            start_new_chunk = 1 \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if bc . jump_to >= 0 and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch -= 1 \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if bc . op == OP_BREAK_LOOP : \n                chunk . exits . add ( block_stack [ - 1 ] [ 1 ] ) \n            chunk = None \n        if bc . op == OP_END_FINALLY : \n            for block in reversed ( block_stack ) : \n                if block [ 0 ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ 1 ] ) \n                    break \n        if bc . op == OP_COMPARE_OP and bc . arg == COMPARE_EXCEPTION : \n            ignore_branch += 1 \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if penult . op == OP_LOAD_CONST and ult . op == OP_RETURN_VALUE : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if chunks [ - 1 ] . byte != penult . offset : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - 1 ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , 0 ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - 1 ] . length = bc . next_offset - chunks [ - 1 ] . byte \n        for i in range ( len ( chunks ) - 1 ) : \n            chunks [ i ] . length = chunks [ i + 1 ] . byte - chunks [ i ] . byte \n    return chunks "}
{"14265": "\ndef wantFile ( self , file , package = None ) : \n    if self . coverInclusive : \n        if file . endswith ( \".py\" ) : \n            if package and self . coverPackages : \n                for want in self . coverPackages : \n                    if package . startswith ( want ) : \n                        return 1 \n            else : \n                return 1 \n    return None "}
{"14268": "\ndef fetch_distribution ( self , requirement , tmpdir , force_scan = 0 , source = 0 , develop_ok = 0 , local_index = None ) : \n    self . info ( \"Searching for %s\" , requirement ) \n    skipped = { } \n    dist = None \n    def find ( req , env = None ) : \n        if env is None : \n            env = self \n        for dist in env [ req . key ] : \n            if dist . precedence == DEVELOP_DIST and not develop_ok : \n                if dist not in skipped : \n                    self . warn ( \"Skipping development or system egg: %s\" , dist ) \n                    skipped [ dist ] = 1 \n                continue \n            if dist in req and ( dist . precedence <= SOURCE_DIST or not source ) : \n                self . info ( \"Best match: %s\" , dist ) \n                return dist . clone ( location = self . download ( dist . location , tmpdir ) ) \n    if force_scan : \n        self . prescan ( ) \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if local_index is not None : \n        dist = dist or find ( requirement , local_index ) \n    if dist is None and self . to_scan is not None : \n        self . prescan ( ) \n        dist = find ( requirement ) \n    if dist is None and not force_scan : \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if dist is None : \n        self . warn ( \"No local packages or download links found for %s%s\" , ( source and \"a source distribution of \" or \"\" ) , requirement , ) \n    return dist "}
{"14272": "\ndef configure ( self , options , conf ) : \n    self . conf = conf \n    if not options . capture : \n        self . enabled = 0 "}
{"14276": "\ndef hex_to_rgb ( color ) : \n    if color . startswith ( '#' ) : \n        color = color [ 1 : ] \n    if len ( color ) == 3 : \n        color = '' . join ( [ c * 2 for c in color ] ) \n    if len ( color ) != 6 : \n        return 0 \n    try : \n        r = int ( color [ : 2 ] , 16 ) \n        g = int ( color [ 2 : 4 ] , 16 ) \n        b = int ( color [ 4 : ] , 16 ) \n    except ValueError : \n        return 0 \n    else : \n        return r , g , b "}
{"14280": "\ndef _handle_history_reply ( self , msg ) : \n    content = msg [ 'content' ] \n    if 'history' not in content : \n        self . log . error ( \"History request failed: %r\" % content ) \n        if content . get ( 'status' , '' ) == 'aborted' and not self . _retrying_history_request : \n            self . log . error ( \"Retrying aborted history request\" ) \n            self . _retrying_history_request = 1 \n            time . sleep ( 0.25 ) \n            self . kernel_manager . shell_channel . history ( hist_access_type = 'tail' , n = 1000 ) \n        else : \n            self . _retrying_history_request = 0 \n        return \n    self . _retrying_history_request = 0 \n    history_items = content [ 'history' ] \n    self . log . debug ( \"Received history reply with %i entries\" , len ( history_items ) ) \n    items = [ ] \n    last_cell = u\"\" \n    for _ , _ , cell in history_items : \n        cell = cell . rstrip ( ) \n        if cell != last_cell : \n            items . append ( cell ) \n            last_cell = cell \n    self . _set_history ( items ) "}
{"14281": "\ndef _handle_pyout ( self , msg ) : \n    self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        content = msg [ 'content' ] \n        prompt_number = content . get ( 'execution_count' , 0 ) \n        data = content [ 'data' ] \n        if data . has_key ( 'text/html' ) : \n            self . _append_plain_text ( self . output_sep , 1 ) \n            self . _append_html ( self . _make_out_prompt ( prompt_number ) , 1 ) \n            html = data [ 'text/html' ] \n            self . _append_plain_text ( '\\n' , 1 ) \n            self . _append_html ( html + self . output_sep2 , 1 ) \n        elif data . has_key ( 'text/plain' ) : \n            self . _append_plain_text ( self . output_sep , 1 ) \n            self . _append_html ( self . _make_out_prompt ( prompt_number ) , 1 ) \n            text = data [ 'text/plain' ] \n            if \"\\n\" in text and not self . output_sep . endswith ( \"\\n\" ) : \n                self . _append_plain_text ( '\\n' , 1 ) \n            self . _append_plain_text ( text + self . output_sep2 , 1 ) "}
{"14282": "\ndef _handle_display_data ( self , msg ) : \n    self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        source = msg [ 'content' ] [ 'source' ] \n        data = msg [ 'content' ] [ 'data' ] \n        metadata = msg [ 'content' ] [ 'metadata' ] \n        if data . has_key ( 'text/html' ) : \n            html = data [ 'text/html' ] \n            self . _append_html ( html , 1 ) \n        elif data . has_key ( 'text/plain' ) : \n            text = data [ 'text/plain' ] \n            self . _append_plain_text ( text , 1 ) \n        self . _append_plain_text ( u'\\n' , 1 ) "}
{"14284": "\ndef execute_file ( self , path , hidden = 0 ) : \n    if sys . platform == 'win32' : \n        path = os . path . normpath ( path ) . replace ( '\\\\' , '/' ) \n    if ' ' in path or \"'\" in path or '\"' in path : \n        path = '\"%s\"' % path . replace ( '\"' , '\\\\\"' ) \n    self . execute ( '%%run %s' % path , hidden = hidden ) "}
{"14285": "\ndef _process_execute_error ( self , msg ) : \n    content = msg [ 'content' ] \n    traceback = '\\n' . join ( content [ 'traceback' ] ) + '\\n' \n    if 0 : \n        traceback = traceback . replace ( ' ' , '&nbsp;' ) \n        traceback = traceback . replace ( '\\n' , '<br/>' ) \n        ename = content [ 'ename' ] \n        ename_styled = '<span class=\"error\">%s</span>' % ename \n        traceback = traceback . replace ( ename , ename_styled ) \n        self . _append_html ( traceback ) \n    else : \n        self . _append_plain_text ( traceback ) "}
{"14286": "\ndef _process_execute_payload ( self , item ) : \n    handler = self . _payload_handlers . get ( item [ 'source' ] ) \n    if handler is None : \n        return 0 \n    else : \n        handler ( item ) \n        return 1 "}
{"14288": "\ndef _edit ( self , filename , line = None ) : \n    if self . custom_edit : \n        self . custom_edit_requested . emit ( filename , line ) \n    elif not self . editor : \n        self . _append_plain_text ( 'No default editor available.\\n' 'Specify a GUI text editor in the `IPythonWidget.editor` ' 'configurable to enable the %edit magic' ) \n    else : \n        try : \n            filename = '\"%s\"' % filename \n            if line and self . editor_line : \n                command = self . editor_line . format ( filename = filename , line = line ) \n            else : \n                try : \n                    command = self . editor . format ( ) \n                except KeyError : \n                    command = self . editor . format ( filename = filename ) \n                else : \n                    command += ' ' + filename \n        except KeyError : \n            self . _append_plain_text ( 'Invalid editor command.\\n' ) \n        else : \n            try : \n                Popen ( command , shell = 1 ) \n            except OSError : \n                msg = 'Opening editor with command \"%s\" failed.\\n' \n                self . _append_plain_text ( msg % command ) "}
{"14302": "\ndef pkg_commit_hash ( pkg_path ) : \n    if _sysinfo . commit : \n        return \"installation\" , _sysinfo . commit \n    proc = subprocess . Popen ( 'git rev-parse --short HEAD' , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = pkg_path , shell = 1 ) \n    repo_commit , _ = proc . communicate ( ) \n    if repo_commit : \n        return 'repository' , repo_commit . strip ( ) \n    return '(none found)' , '<not found>' "}
{"14322": "\ndef wantFile ( self , file ) : \n    base = op_basename ( file ) \n    ignore_matches = [ ignore_this for ignore_this in self . ignoreFiles if ignore_this . search ( base ) ] \n    if ignore_matches : \n        log . debug ( '%s matches ignoreFiles pattern; skipped' , base ) \n        return 0 \n    if not self . config . includeExe and os . access ( file , os . X_OK ) : \n        log . info ( '%s is executable; skipped' , file ) \n        return 0 \n    dummy , ext = op_splitext ( base ) \n    pysrc = ext == '.py' \n    wanted = pysrc and self . matches ( base ) \n    plug_wants = self . plugins . wantFile ( file ) \n    if plug_wants is not None : \n        log . debug ( \"plugin setting want %s to %s\" , file , plug_wants ) \n        wanted = plug_wants \n    log . debug ( \"wantFile %s? %s\" , file , wanted ) \n    return wanted "}
{"14323": "\ndef wantFunction ( self , function ) : \n    try : \n        if hasattr ( function , 'compat_func_name' ) : \n            funcname = function . compat_func_name \n        else : \n            funcname = function . __name__ \n    except AttributeError : \n        return 0 \n    declared = getattr ( function , '__test__' , None ) \n    if declared is not None : \n        wanted = declared \n    else : \n        wanted = not funcname . startswith ( '_' ) and self . matches ( funcname ) \n    plug_wants = self . plugins . wantFunction ( function ) \n    if plug_wants is not None : \n        wanted = plug_wants \n    log . debug ( \"wantFunction %s? %s\" , function , wanted ) \n    return wanted "}
{"14324": "\ndef wantMethod ( self , method ) : \n    try : \n        method_name = method . __name__ \n    except AttributeError : \n        return 0 \n    if method_name . startswith ( '_' ) : \n        return 0 \n    declared = getattr ( method , '__test__' , None ) \n    if declared is not None : \n        wanted = declared \n    else : \n        wanted = self . matches ( method_name ) \n    plug_wants = self . plugins . wantMethod ( method ) \n    if plug_wants is not None : \n        wanted = plug_wants \n    log . debug ( \"wantMethod %s? %s\" , method , wanted ) \n    return wanted "}
{"14328": "\ndef print_list_lines ( self , filename , first , last ) : \n    try : \n        Colors = self . color_scheme_table . active_colors \n        ColorsNormal = Colors . Normal \n        tpl_line = '%%s%s%%s %s%%s' % ( Colors . lineno , ColorsNormal ) \n        tpl_line_em = '%%s%s%%s %s%%s%s' % ( Colors . linenoEm , Colors . line , ColorsNormal ) \n        src = [ ] \n        for lineno in range ( first , last + 1 ) : \n            line = linecache . getline ( filename , lineno ) \n            if not line : \n                break \n            if lineno == self . curframe . f_lineno : \n                line = self . __format_line ( tpl_line_em , filename , lineno , line , arrow = 1 ) \n            else : \n                line = self . __format_line ( tpl_line , filename , lineno , line , arrow = 0 ) \n            src . append ( line ) \n            self . lineno = lineno \n        print >> io . stdout , '' . join ( src ) \n    except KeyboardInterrupt : \n        pass "}
{"14341": "\ndef init_connector ( self ) : \n    self . using_ssh = bool ( self . sshkey or self . sshserver ) \n    if self . sshkey and not self . sshserver : \n        self . sshserver = self . url . split ( '://' ) [ 1 ] . split ( ':' ) [ 0 ] \n    if self . using_ssh : \n        if tunnel . try_passwordless_ssh ( self . sshserver , self . sshkey , self . paramiko ) : \n            password = 0 \n        else : \n            password = getpass ( \"SSH Password for %s: \" % self . sshserver ) \n    else : \n        password = 0 \n    def connect ( s , url ) : \n        url = disambiguate_url ( url , self . location ) \n        if self . using_ssh : \n            self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) \n            return tunnel . tunnel_connection ( s , url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) \n        else : \n            return s . connect ( url ) \n    def maybe_tunnel ( url ) : \n        url = disambiguate_url ( url , self . location ) \n        if self . using_ssh : \n            self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) \n            url , tunnelobj = tunnel . open_tunnel ( url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) \n        return url \n    return connect , maybe_tunnel "}
{"14343": "\ndef html_to_text ( content ) : \n    text = None \n    h2t = html2text . HTML2Text ( ) \n    h2t . ignore_links = 0 \n    text = h2t . handle ( content ) \n    return text "}
{"14346": "\ndef options ( self , parser , env = os . environ ) : \n    super ( NoseExclude , self ) . options ( parser , env ) \n    env_dirs = [ ] \n    if 'NOSE_EXCLUDE_DIRS' in env : \n        exclude_dirs = env . get ( 'NOSE_EXCLUDE_DIRS' , '' ) \n        env_dirs . extend ( exclude_dirs . split ( ';' ) ) \n    parser . add_option ( \"--exclude-dir\" , action = \"append\" , dest = \"exclude_dirs\" , default = env_dirs , help = \"Directory to exclude from test discovery. \\                Path can be relative to current working directory \\                or an absolute path. May be specified multiple \\                times. [NOSE_EXCLUDE_DIRS]\" ) \n    parser . add_option ( \"--exclude-dir-file\" , type = \"string\" , dest = \"exclude_dir_file\" , default = env . get ( 'NOSE_EXCLUDE_DIRS_FILE' , 0 ) , help = \"A file containing a list of directories to exclude \\                from test discovery. Paths can be relative to current \\                working directory or an absolute path. \\                [NOSE_EXCLUDE_DIRS_FILE]\" ) "}
{"14347": "\ndef configure ( self , options , conf ) : \n    super ( NoseExclude , self ) . configure ( options , conf ) \n    self . exclude_dirs = { } \n    if options . exclude_dir_file : \n        if not options . exclude_dirs : \n            options . exclude_dirs = [ ] \n        new_dirs = self . _load_from_file ( options . exclude_dir_file ) \n        options . exclude_dirs . extend ( new_dirs ) \n    if not options . exclude_dirs : \n        self . enabled = 0 \n        return \n    self . enabled = 1 \n    root = os . getcwd ( ) \n    log . debug ( 'cwd: %s' % root ) \n    for exclude_param in options . exclude_dirs : \n        for d in exclude_param . split ( '\\n' ) : \n            d = d . strip ( ) \n            abs_d = self . _force_to_abspath ( d ) \n            if abs_d : \n                self . exclude_dirs [ abs_d ] = 1 \n    exclude_str = \"excluding dirs: %s\" % \",\" . join ( self . exclude_dirs . keys ( ) ) \n    log . debug ( exclude_str ) "}
{"14348": "\ndef wantDirectory ( self , dirname ) : \n    if dirname in self . exclude_dirs : \n        log . debug ( \"excluded: %s\" % dirname ) \n        return 0 \n    else : \n        return None "}
{"14349": "\ndef links_to_dynamic ( self , ext ) : \n    libnames = dict . fromkeys ( [ lib . _full_name for lib in self . shlibs ] ) \n    pkg = '.' . join ( ext . _full_name . split ( '.' ) [ : - 1 ] + [ '' ] ) \n    for libname in ext . libraries : \n        if pkg + libname in libnames : \n            return 1 \n    return 0 "}
{"14356": "\ndef write_pid_file ( self , overwrite = 0 ) : \n    pid_file = os . path . join ( self . profile_dir . pid_dir , self . name + u'.pid' ) \n    if os . path . isfile ( pid_file ) : \n        pid = self . get_pid_from_file ( ) \n        if not overwrite : \n            raise PIDFileError ( 'The pid file [%s] already exists. \\nThis could mean that this ' 'server is already running with [pid=%s].' % ( pid_file , pid ) ) \n    with open ( pid_file , 'w' ) as f : \n        self . log . info ( \"Creating pid file: %s\" % pid_file ) \n        f . write ( repr ( os . getpid ( ) ) + '\\n' ) "}
{"14362": "\ndef rehighlightBlock ( self , block ) : \n    old = self . highlighting_on \n    self . highlighting_on = 1 \n    super ( FrontendHighlighter , self ) . rehighlightBlock ( block ) \n    self . highlighting_on = old "}
{"14366": "\ndef _prompt_finished_hook ( self ) : \n    self . _input_splitter . reset ( ) \n    if not self . _reading : \n        self . _highlighter . highlighting_on = 0 "}
{"14367": "\ndef _tab_pressed ( self ) : \n    text = self . _get_input_buffer_cursor_line ( ) \n    if text is None : \n        return 0 \n    complete = bool ( text [ : self . _get_input_buffer_cursor_column ( ) ] . strip ( ) ) \n    if complete : \n        self . _complete ( ) \n    return not complete "}
{"14369": "\ndef _event_filter_console_keypress ( self , event ) : \n    key = event . key ( ) \n    if self . _control_key_down ( event . modifiers ( ) , include_command = 0 ) : \n        if key == QtCore . Qt . Key_C and self . _executing : \n            self . request_interrupt_kernel ( ) \n            return 1 \n        elif key == QtCore . Qt . Key_Period : \n            self . request_restart_kernel ( ) \n            return 1 \n    elif not event . modifiers ( ) & QtCore . Qt . AltModifier : \n        if key == QtCore . Qt . Key_Backspace : \n            col = self . _get_input_buffer_cursor_column ( ) \n            cursor = self . _control . textCursor ( ) \n            if col > 3 and not cursor . hasSelection ( ) : \n                text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n                if text . endswith ( '    ' ) and not text . strip ( ) : \n                    cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4 ) \n                    cursor . removeSelectedText ( ) \n                    return 1 \n    return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) "}
{"14372": "\ndef _silent_exec_callback ( self , expr , callback ) : \n    local_uuid = str ( uuid . uuid1 ( ) ) \n    msg_id = self . kernel_manager . shell_channel . execute ( '' , silent = 1 , user_expressions = { local_uuid : expr } ) \n    self . _callback_dict [ local_uuid ] = callback \n    self . _request_info [ 'execute' ] [ msg_id ] = self . _ExecutionRequest ( msg_id , 'silent_exec_callback' ) "}
{"14374": "\ndef _handle_execute_reply ( self , msg ) : \n    self . log . debug ( \"execute: %s\" , msg . get ( 'content' , '' ) ) \n    msg_id = msg [ 'parent_header' ] [ 'msg_id' ] \n    info = self . _request_info [ 'execute' ] . get ( msg_id ) \n    self . _reading = 0 \n    if info and info . kind == 'user' and not self . _hidden : \n        self . kernel_manager . sub_channel . flush ( ) \n        if self . ansi_codes : \n            self . _ansi_processor . reset_sgr ( ) \n        content = msg [ 'content' ] \n        status = content [ 'status' ] \n        if status == 'ok' : \n            self . _process_execute_ok ( msg ) \n        elif status == 'error' : \n            self . _process_execute_error ( msg ) \n        elif status == 'aborted' : \n            self . _process_execute_abort ( msg ) \n        self . _show_interpreter_prompt_for_reply ( msg ) \n        self . executed . emit ( msg ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    elif info and info . kind == 'silent_exec_callback' and not self . _hidden : \n        self . _handle_exec_callback ( msg ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    else : \n        super ( FrontendWidget , self ) . _handle_execute_reply ( msg ) "}
{"14375": "\ndef _handle_input_request ( self , msg ) : \n    self . log . debug ( \"input: %s\" , msg . get ( 'content' , '' ) ) \n    if self . _hidden : \n        raise RuntimeError ( 'Request for raw input during hidden execution.' ) \n    self . kernel_manager . sub_channel . flush ( ) \n    def callback ( line ) : \n        self . kernel_manager . stdin_channel . input ( line ) \n    if self . _reading : \n        self . log . debug ( \"Got second input request, assuming first was interrupted.\" ) \n        self . _reading = 0 \n    self . _readline ( msg [ 'content' ] [ 'prompt' ] , callback = callback ) "}
{"14376": "\ndef _handle_kernel_died ( self , since_last_heartbeat ) : \n    self . log . debug ( \"kernel died: %s\" , since_last_heartbeat ) \n    if self . custom_restart : \n        self . custom_restart_kernel_died . emit ( since_last_heartbeat ) \n    else : \n        message = 'The kernel heartbeat has been inactive for %.2f ' 'seconds. Do you want to restart the kernel? You may ' 'first want to check the network connection.' % since_last_heartbeat \n        self . restart_kernel ( message , now = 1 ) "}
{"14377": "\ndef _handle_object_info_reply ( self , rep ) : \n    self . log . debug ( \"oinfo: %s\" , rep . get ( 'content' , '' ) ) \n    cursor = self . _get_cursor ( ) \n    info = self . _request_info . get ( 'call_tip' ) \n    if info and info . id == rep [ 'parent_header' ] [ 'msg_id' ] and info . pos == cursor . position ( ) : \n        content = rep [ 'content' ] \n        if content . get ( 'ismagic' , 0 ) : \n            call_info , doc = None , None \n        else : \n            call_info , doc = call_tip ( content , format_call = 1 ) \n        if call_info or doc : \n            self . _call_tip_widget . show_call_info ( call_info , doc ) "}
{"14378": "\ndef _handle_pyout ( self , msg ) : \n    self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        text = msg [ 'content' ] [ 'data' ] \n        self . _append_plain_text ( text + '\\n' , before_prompt = 1 ) "}
{"14379": "\ndef _handle_stream ( self , msg ) : \n    self . log . debug ( \"stream: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        text = msg [ 'content' ] [ 'data' ] . expandtabs ( 8 ) \n        self . _append_plain_text ( text , before_prompt = 1 ) \n        self . _control . moveCursor ( QtGui . QTextCursor . End ) "}
{"14381": "\ndef execute_file ( self , path , hidden = 0 ) : \n    self . execute ( 'execfile(%r)' % path , hidden = hidden ) "}
{"14382": "\ndef interrupt_kernel ( self ) : \n    if self . custom_interrupt : \n        self . _reading = 0 \n        self . custom_interrupt_requested . emit ( ) \n    elif self . kernel_manager . has_kernel : \n        self . _reading = 0 \n        self . kernel_manager . interrupt_kernel ( ) \n    else : \n        self . _append_plain_text ( 'Kernel process is either remote or ' 'unspecified. Cannot interrupt.\\n' ) "}
{"14383": "\ndef reset ( self , clear = 0 ) : \n    if self . _executing : \n        self . _executing = 0 \n        self . _request_info [ 'execute' ] = { } \n    self . _reading = 0 \n    self . _highlighter . highlighting_on = 0 \n    if self . clear_on_kernel_restart or clear : \n        self . _control . clear ( ) \n        self . _append_plain_text ( self . banner ) \n    else : \n        self . _append_plain_text ( \"# restarting kernel...\" ) \n        self . _append_html ( \"<hr><br>\" ) \n    self . _append_before_prompt_pos = self . _get_cursor ( ) . position ( ) \n    self . _show_interpreter_prompt ( ) "}
{"14384": "\ndef restart_kernel ( self , message , now = 0 ) : \n    if self . custom_restart : \n        self . custom_restart_requested . emit ( ) \n    elif self . kernel_manager . has_kernel : \n        self . kernel_manager . hb_channel . pause ( ) \n        if self . confirm_restart : \n            buttons = QtGui . QMessageBox . Yes | QtGui . QMessageBox . No \n            result = QtGui . QMessageBox . question ( self , 'Restart kernel?' , message , buttons ) \n            do_restart = result == QtGui . QMessageBox . Yes \n        else : \n            do_restart = 1 \n        if do_restart : \n            try : \n                self . kernel_manager . restart_kernel ( now = now ) \n            except RuntimeError : \n                self . _append_plain_text ( 'Kernel started externally. ' 'Cannot restart.\\n' , before_prompt = 1 ) \n            else : \n                self . reset ( ) \n        else : \n            self . kernel_manager . hb_channel . unpause ( ) \n    else : \n        self . _append_plain_text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\\n' , before_prompt = 1 ) "}
{"14385": "\ndef _call_tip ( self ) : \n    if not self . enable_calltips : \n        return 0 \n    cursor = self . _get_cursor ( ) \n    cursor . movePosition ( QtGui . QTextCursor . Left ) \n    if cursor . document ( ) . characterAt ( cursor . position ( ) ) != '(' : \n        return 0 \n    context = self . _get_context ( cursor ) \n    if not context : \n        return 0 \n    name = '.' . join ( context ) \n    msg_id = self . kernel_manager . shell_channel . object_info ( name ) \n    pos = self . _get_cursor ( ) . position ( ) \n    self . _request_info [ 'call_tip' ] = self . _CallTipRequest ( msg_id , pos ) \n    return 1 "}
{"14397": "\ndef latex_to_png ( s , encode = 0 , backend = 'mpl' ) : \n    if backend == 'mpl' : \n        f = latex_to_png_mpl \n    elif backend == 'dvipng' : \n        f = latex_to_png_dvipng \n    else : \n        raise ValueError ( 'No such backend {0}' . format ( backend ) ) \n    bin_data = f ( s ) \n    if encode and bin_data : \n        bin_data = encodestring ( bin_data ) \n    return bin_data "}
{"14398": "\ndef latex_to_html ( s , alt = 'image' ) : \n    base64_data = latex_to_png ( s , encode = 1 ) \n    if base64_data : \n        return _data_uri_template_png % ( base64_data , alt ) "}
{"14400": "\ndef check_if_exists ( self ) : \n    if self . req is None : \n        return 0 \n    try : \n        self . satisfied_by = pkg_resources . get_distribution ( self . req ) \n    except pkg_resources . DistributionNotFound : \n        return 0 \n    except pkg_resources . VersionConflict : \n        existing_dist = pkg_resources . get_distribution ( self . req . project_name ) \n        if self . use_user_site : \n            if dist_in_usersite ( existing_dist ) : \n                self . conflicts_with = existing_dist \n            elif running_under_virtualenv ( ) and dist_in_site_packages ( existing_dist ) : \n                raise InstallationError ( \"Will not install to the user site because it will lack sys.path precedence to %s in %s\" % ( existing_dist . project_name , existing_dist . location ) ) \n        else : \n            self . conflicts_with = existing_dist \n    return 1 "}
{"14402": "\ndef cpu_percent ( interval = 0.1 , percpu = 0 ) : \n    global _last_cpu_times \n    global _last_per_cpu_times \n    blocking = interval is not None and interval > 0.0 \n    def calculate ( t1 , t2 ) : \n        t1_all = sum ( t1 ) \n        t1_busy = t1_all - t1 . idle \n        t2_all = sum ( t2 ) \n        t2_busy = t2_all - t2 . idle \n        if t2_busy <= t1_busy : \n            return 0.0 \n        busy_delta = t2_busy - t1_busy \n        all_delta = t2_all - t1_all \n        busy_perc = ( busy_delta / all_delta ) * 100 \n        return round ( busy_perc , 1 ) \n    if not percpu : \n        if blocking : \n            t1 = cpu_times ( ) \n            time . sleep ( interval ) \n        else : \n            t1 = _last_cpu_times \n        _last_cpu_times = cpu_times ( ) \n        return calculate ( t1 , _last_cpu_times ) \n    else : \n        ret = [ ] \n        if blocking : \n            tot1 = cpu_times ( percpu = 1 ) \n            time . sleep ( interval ) \n        else : \n            tot1 = _last_per_cpu_times \n        _last_per_cpu_times = cpu_times ( percpu = 1 ) \n        for t1 , t2 in zip ( tot1 , _last_per_cpu_times ) : \n            ret . append ( calculate ( t1 , t2 ) ) \n        return ret "}
{"14406": "\ndef get_children ( self , recursive = 0 ) : \n    if not self . is_running ( ) : \n        name = self . _platform_impl . _process_name \n        raise NoSuchProcess ( self . pid , name ) \n    ret = [ ] \n    if not recursive : \n        for p in process_iter ( ) : \n            try : \n                if p . ppid == self . pid : \n                    if self . create_time <= p . create_time : \n                        ret . append ( p ) \n            except NoSuchProcess : \n                pass \n    else : \n        table = defaultdict ( list ) \n        for p in process_iter ( ) : \n            try : \n                table [ p . ppid ] . append ( p ) \n            except NoSuchProcess : \n                pass \n        checkpids = [ self . pid ] \n        for pid in checkpids : \n            for child in table [ pid ] : \n                try : \n                    intime = self . create_time <= child . create_time \n                except NoSuchProcess : \n                    pass \n                else : \n                    if intime : \n                        ret . append ( child ) \n                        if child . pid not in checkpids : \n                            checkpids . append ( child . pid ) \n    return ret "}
{"14409": "\ndef get_memory_maps ( self , grouped = 1 ) : \n    it = self . _platform_impl . get_memory_maps ( ) \n    if grouped : \n        d = { } \n        for tupl in it : \n            path = tupl [ 2 ] \n            nums = tupl [ 3 : ] \n            try : \n                d [ path ] = map ( lambda x , y : x + y , d [ path ] , nums ) \n            except KeyError : \n                d [ path ] = nums \n        nt = self . _platform_impl . nt_mmap_grouped \n        return [ nt ( path , * d [ path ] ) for path in d ] \n    else : \n        nt = self . _platform_impl . nt_mmap_ext \n        return [ nt ( * x ) for x in it ] "}
{"14410": "\ndef is_running ( self ) : \n    if self . _gone : \n        return 0 \n    try : \n        return self . create_time == self . _platform_impl . get_process_create_time ( ) \n    except NoSuchProcess : \n        self . _gone = 1 \n        return 0 "}
{"14415": "\ndef _wire_kernel ( self ) : \n    self . gtk_main , self . gtk_main_quit = self . _hijack_gtk ( ) \n    gobject . timeout_add ( int ( 1000 * self . kernel . _poll_interval ) , self . iterate_kernel ) \n    return 0 "}
{"14430": "\ndef prefilter_line ( self , line , continue_prompt = 0 ) : \n    self . shell . _last_input_line = line \n    if not line : \n        return '' \n    if not continue_prompt or ( continue_prompt and self . multi_line_specials ) : \n        line = self . transform_line ( line , continue_prompt ) \n    line_info = LineInfo ( line , continue_prompt ) \n    stripped = line . strip ( ) \n    normal_handler = self . get_handler_by_name ( 'normal' ) \n    if not stripped : \n        if not continue_prompt : \n            self . shell . displayhook . prompt_count -= 1 \n        return normal_handler . handle ( line_info ) \n    if continue_prompt and not self . multi_line_specials : \n        return normal_handler . handle ( line_info ) \n    prefiltered = self . prefilter_line_info ( line_info ) \n    return prefiltered "}
{"14431": "\ndef prefilter_lines ( self , lines , continue_prompt = 0 ) : \n    llines = lines . rstrip ( '\\n' ) . split ( '\\n' ) \n    if len ( llines ) > 1 : \n        out = '\\n' . join ( [ self . prefilter_line ( line , lnum > 0 ) for lnum , line in enumerate ( llines ) ] ) \n    else : \n        out = self . prefilter_line ( llines [ 0 ] , continue_prompt ) \n    return out "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = 1 \n    if esc == ESC_QUOTE : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    elif esc == ESC_QUOTE2 : \n        newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n    elif esc == ESC_PAREN : \n        newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n    else : \n        if force_auto : \n            do_rewrite = not the_rest . startswith ( '(' ) \n        else : \n            if not the_rest : \n                do_rewrite = ( self . shell . autocall >= 2 ) \n            elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n                do_rewrite = 0 \n            else : \n                do_rewrite = 1 \n        if do_rewrite : \n            if the_rest . endswith ( ';' ) : \n                newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) \n            else : \n                newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n        else : \n            normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n            return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14442": "\ndef eventFilter ( self , obj , event ) : \n    if obj == self . _text_edit : \n        etype = event . type ( ) \n        if etype == QtCore . QEvent . KeyPress : \n            key = event . key ( ) \n            if key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return ) : \n                self . hide ( ) \n            elif key == QtCore . Qt . Key_Escape : \n                self . hide ( ) \n                return 1 \n        elif etype == QtCore . QEvent . FocusOut : \n            self . hide ( ) \n        elif etype == QtCore . QEvent . Enter : \n            self . _hide_timer . stop ( ) \n        elif etype == QtCore . QEvent . Leave : \n            self . _leave_event_hide ( ) \n    return super ( CallTipWidget , self ) . eventFilter ( obj , event ) "}
{"14446": "\ndef show_tip ( self , tip ) : \n    text_edit = self . _text_edit \n    document = text_edit . document ( ) \n    cursor = text_edit . textCursor ( ) \n    search_pos = cursor . position ( ) - 1 \n    self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = 0 ) \n    if self . _start_position == - 1 : \n        return 0 \n    self . setText ( tip ) \n    self . resize ( self . sizeHint ( ) ) \n    padding = 3 \n    cursor_rect = text_edit . cursorRect ( cursor ) \n    screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) \n    point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) \n    point . setY ( point . y ( ) + padding ) \n    tip_height = self . size ( ) . height ( ) \n    tip_width = self . size ( ) . width ( ) \n    vertical = 'bottom' \n    horizontal = 'Right' \n    if point . y ( ) + tip_height > screen_rect . height ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . y ( ) - tip_height < padding : \n            if 2 * point . y ( ) < screen_rect . height ( ) : \n                vertical = 'bottom' \n            else : \n                vertical = 'top' \n        else : \n            vertical = 'top' \n    if point . x ( ) + tip_width > screen_rect . width ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . x ( ) - tip_width < padding : \n            if 2 * point . x ( ) < screen_rect . width ( ) : \n                horizontal = 'Right' \n            else : \n                horizontal = 'Left' \n        else : \n            horizontal = 'Left' \n    pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) \n    point = text_edit . mapToGlobal ( pos ( ) ) \n    if vertical == 'top' : \n        point . setY ( point . y ( ) - tip_height - padding ) \n    if horizontal == 'Left' : \n        point . setX ( point . x ( ) - tip_width - padding ) \n    self . move ( point ) \n    self . show ( ) \n    return 1 "}
{"14452": "\ndef virtualenv_no_global ( ) : \n    site_mod_dir = os . path . dirname ( os . path . abspath ( site . __file__ ) ) \n    no_global_file = os . path . join ( site_mod_dir , 'no-global-site-packages.txt' ) \n    if running_under_virtualenv ( ) and os . path . isfile ( no_global_file ) : \n        return 1 "}
{"14453": "\ndef pwordfreq ( view , fnames ) : \n    assert len ( fnames ) == len ( view . targets ) \n    view . scatter ( 'fname' , fnames , flatten = 1 ) \n    ar = view . apply ( wordfreq , Reference ( 'fname' ) ) \n    freqs_list = ar . get ( ) \n    word_set = set ( ) \n    for f in freqs_list : \n        word_set . update ( f . keys ( ) ) \n    freqs = dict ( zip ( word_set , repeat ( 0 ) ) ) \n    for f in freqs_list : \n        for word , count in f . iteritems ( ) : \n            freqs [ word ] += count \n    return freqs "}
{"14463": "\ndef reset_sgr ( self ) : \n    self . intensity = 0 \n    self . italic = 0 \n    self . bold = 0 \n    self . underline = 0 \n    self . foreground_color = None \n    self . background_color = None "}
{"14468": "\ndef mutex ( func ) : \n    def wrapper ( * args , ** kwargs ) : \n        lock = args [ 0 ] . lock \n        lock . acquire ( 1 ) \n        try : \n            return func ( * args , ** kwargs ) \n        except : \n            raise \n        finally : \n            lock . release ( ) \n    return wrapper "}
{"14470": "\ndef already_used ( self , tok ) : \n    if tok in self . jwts : \n        return 1 \n    self . jwts [ tok ] = time . time ( ) \n    return 0 "}
{"14472": "\ndef semaphore ( count : int , bounded : bool = 0 ) : \n    lock_type = threading . BoundedSemaphore if bounded else threading . Semaphore \n    lock_obj = lock_type ( value = count ) \n    return with_it ( lock_obj ) "}
{"14474": "\ndef eventFilter ( self , obj , event ) : \n    etype = event . type ( ) \n    if etype == QtCore . QEvent . KeyPress : \n        key = event . key ( ) \n        if self . _control_key_down ( event . modifiers ( ) ) and key in self . _ctrl_down_remap : \n            new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , self . _ctrl_down_remap [ key ] , QtCore . Qt . NoModifier ) \n            QtGui . qApp . sendEvent ( obj , new_event ) \n            return 1 \n        elif obj == self . _control : \n            return self . _event_filter_console_keypress ( event ) \n        elif obj == self . _page_control : \n            return self . _event_filter_page_keypress ( event ) \n    elif etype == QtCore . QEvent . MouseButtonRelease and event . button ( ) == QtCore . Qt . MidButton and obj == self . _control . viewport ( ) : \n        cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n        self . _control . setTextCursor ( cursor ) \n        self . paste ( QtGui . QClipboard . Selection ) \n        return 1 \n    elif etype == QtCore . QEvent . Resize and not self . _filter_resize : \n        self . _filter_resize = 1 \n        QtGui . qApp . sendEvent ( obj , event ) \n        self . _adjust_scrollbars ( ) \n        self . _filter_resize = 0 \n        return 1 \n    elif etype == QtCore . QEvent . ShortcutOverride and self . override_shortcuts and self . _control_key_down ( event . modifiers ( ) ) and event . key ( ) in self . _shortcuts : \n        event . accept ( ) \n    elif etype == QtCore . QEvent . DragEnter and obj == self . _control . viewport ( ) and event . source ( ) == self . _control . viewport ( ) : \n        self . _filter_drag = 1 \n    elif etype == QtCore . QEvent . DragLeave and obj == self . _control . viewport ( ) and self . _filter_drag : \n        cursor = self . _control . textCursor ( ) \n        cursor . clearSelection ( ) \n        self . _control . setTextCursor ( cursor ) \n        self . _filter_drag = 0 \n    elif etype == QtCore . QEvent . Drop and obj == self . _control . viewport ( ) : \n        cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n        if self . _in_buffer ( cursor . position ( ) ) : \n            text = event . mimeData ( ) . text ( ) \n            self . _insert_plain_text_into_buffer ( cursor , text ) \n        QtGui . qApp . sendEvent ( obj , QtGui . QDragLeaveEvent ( ) ) \n        return 1 \n    elif etype in self . _pager_scroll_events and obj == self . _page_control : \n        self . _page_control . repaint ( ) \n        return 1 \n    return super ( ConsoleWidget , self ) . eventFilter ( obj , event ) "}
{"14477": "\ndef can_paste ( self ) : \n    if self . _control . textInteractionFlags ( ) & QtCore . Qt . TextEditable : \n        return bool ( QtGui . QApplication . clipboard ( ) . text ( ) ) \n    return 0 "}
{"14478": "\ndef clear ( self , keep_input = 1 ) : \n    if self . _executing : \n        self . _control . clear ( ) \n    else : \n        if keep_input : \n            input_buffer = self . input_buffer \n        self . _control . clear ( ) \n        self . _show_prompt ( ) \n        if keep_input : \n            self . input_buffer = input_buffer "}
{"14480": "\ndef execute ( self , source = None , hidden = 0 , interactive = 0 ) : \n    if source is None : \n        source = self . input_buffer \n        if not hidden : \n            source += '\\n' \n    elif not hidden : \n        self . input_buffer = source \n    complete = self . _is_complete ( source , interactive ) \n    if hidden : \n        if complete : \n            self . _execute ( source , hidden ) \n        else : \n            error = 'Incomplete noninteractive input: \"%s\"' \n            raise RuntimeError ( error % source ) \n    else : \n        if complete : \n            self . _append_plain_text ( '\\n' ) \n            self . _input_buffer_executing = self . input_buffer \n            self . _executing = 1 \n            self . _prompt_finished ( ) \n            self . _control . document ( ) . setMaximumBlockCount ( self . buffer_size ) \n            self . _control . setUndoRedoEnabled ( 0 ) \n            self . _execute ( source , hidden ) \n        else : \n            cursor = self . _get_end_cursor ( ) \n            cursor . beginEditBlock ( ) \n            cursor . insertText ( '\\n' ) \n            self . _insert_continuation_prompt ( cursor ) \n            cursor . endEditBlock ( ) \n            self . _control . moveCursor ( QtGui . QTextCursor . End ) \n    return complete "}
{"14481": "\ndef _get_input_buffer ( self , force = 0 ) : \n    if self . _executing and not force : \n        return self . _input_buffer_executing \n    cursor = self . _get_end_cursor ( ) \n    cursor . setPosition ( self . _prompt_pos , QtGui . QTextCursor . KeepAnchor ) \n    input_buffer = cursor . selection ( ) . toPlainText ( ) \n    return input_buffer . replace ( '\\n' + self . _continuation_prompt , '\\n' ) "}
{"14488": "\ndef _append_custom ( self , insert , input , before_prompt = 0 ) : \n    cursor = self . _control . textCursor ( ) \n    if before_prompt and ( self . _reading or not self . _executing ) : \n        cursor . setPosition ( self . _append_before_prompt_pos ) \n    else : \n        cursor . movePosition ( QtGui . QTextCursor . End ) \n    start_pos = cursor . position ( ) \n    result = insert ( cursor , input ) \n    if before_prompt and not self . _executing : \n        diff = cursor . position ( ) - start_pos \n        self . _append_before_prompt_pos += diff \n        self . _prompt_pos += diff \n    return result "}
{"14489": "\ndef _append_html ( self , html , before_prompt = 0 ) : \n    self . _append_custom ( self . _insert_html , html , before_prompt ) "}
{"14490": "\ndef _append_html_fetching_plain_text ( self , html , before_prompt = 0 ) : \n    return self . _append_custom ( self . _insert_html_fetching_plain_text , html , before_prompt ) "}
{"14491": "\ndef _append_plain_text ( self , text , before_prompt = 0 ) : \n    self . _append_custom ( self . _insert_plain_text , text , before_prompt ) "}
{"14492": "\ndef _clear_temporary_buffer ( self ) : \n    cursor = self . _get_prompt_cursor ( ) \n    prompt = self . _continuation_prompt . lstrip ( ) \n    if ( self . _temp_buffer_filled ) : \n        self . _temp_buffer_filled = 0 \n        while cursor . movePosition ( QtGui . QTextCursor . NextBlock ) : \n            temp_cursor = QtGui . QTextCursor ( cursor ) \n            temp_cursor . select ( QtGui . QTextCursor . BlockUnderCursor ) \n            text = temp_cursor . selection ( ) . toPlainText ( ) . lstrip ( ) \n            if not text . startswith ( prompt ) : \n                break \n    else : \n        return \n    cursor . movePosition ( QtGui . QTextCursor . Left ) \n    cursor . movePosition ( QtGui . QTextCursor . End , QtGui . QTextCursor . KeepAnchor ) \n    cursor . removeSelectedText ( ) \n    if self . _control . isUndoRedoEnabled ( ) : \n        self . _control . setUndoRedoEnabled ( 0 ) \n        self . _control . setUndoRedoEnabled ( 1 ) "}
{"14494": "\ndef _fill_temporary_buffer ( self , cursor , text , html = 0 ) : \n    current_pos = self . _control . textCursor ( ) . position ( ) \n    cursor . beginEditBlock ( ) \n    self . _append_plain_text ( '\\n' ) \n    self . _page ( text , html = html ) \n    cursor . endEditBlock ( ) \n    cursor . setPosition ( current_pos ) \n    self . _control . moveCursor ( QtGui . QTextCursor . End ) \n    self . _control . setTextCursor ( cursor ) \n    self . _temp_buffer_filled = 1 "}
{"14495": "\ndef _control_key_down ( self , modifiers , include_command = 0 ) : \n    if sys . platform == 'darwin' : \n        down = include_command and ( modifiers & QtCore . Qt . ControlModifier ) \n        return bool ( down ) ^ bool ( modifiers & QtCore . Qt . MetaModifier ) \n    else : \n        return bool ( modifiers & QtCore . Qt . ControlModifier ) "}
{"14496": "\ndef _create_control ( self ) : \n    if self . custom_control : \n        control = self . custom_control ( ) \n    elif self . kind == 'plain' : \n        control = QtGui . QPlainTextEdit ( ) \n    elif self . kind == 'rich' : \n        control = QtGui . QTextEdit ( ) \n        control . setAcceptRichText ( 0 ) \n    control . installEventFilter ( self ) \n    control . viewport ( ) . installEventFilter ( self ) \n    control . customContextMenuRequested . connect ( self . _custom_context_menu_requested ) \n    control . copyAvailable . connect ( self . copy_available ) \n    control . redoAvailable . connect ( self . redo_available ) \n    control . undoAvailable . connect ( self . undo_available ) \n    layout = control . document ( ) . documentLayout ( ) \n    layout . documentSizeChanged . disconnect ( ) \n    layout . documentSizeChanged . connect ( self . _adjust_scrollbars ) \n    control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , 1 ) \n    control . setContextMenuPolicy ( QtCore . Qt . CustomContextMenu ) \n    control . setReadOnly ( 1 ) \n    control . setUndoRedoEnabled ( 0 ) \n    control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) \n    return control "}
{"14497": "\ndef _create_page_control ( self ) : \n    if self . custom_page_control : \n        control = self . custom_page_control ( ) \n    elif self . kind == 'plain' : \n        control = QtGui . QPlainTextEdit ( ) \n    elif self . kind == 'rich' : \n        control = QtGui . QTextEdit ( ) \n    control . installEventFilter ( self ) \n    viewport = control . viewport ( ) \n    viewport . installEventFilter ( self ) \n    control . setReadOnly ( 1 ) \n    control . setUndoRedoEnabled ( 0 ) \n    control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) \n    return control "}
{"14498": "\ndef _event_filter_page_keypress ( self , event ) : \n    key = event . key ( ) \n    ctrl_down = self . _control_key_down ( event . modifiers ( ) ) \n    alt_down = event . modifiers ( ) & QtCore . Qt . AltModifier \n    if ctrl_down : \n        if key == QtCore . Qt . Key_O : \n            self . _control . setFocus ( ) \n            intercept = 1 \n    elif alt_down : \n        if key == QtCore . Qt . Key_Greater : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . End ) \n            intercepted = 1 \n        elif key == QtCore . Qt . Key_Less : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            intercepted = 1 \n    elif key in ( QtCore . Qt . Key_Q , QtCore . Qt . Key_Escape ) : \n        if self . _splitter : \n            self . _page_control . hide ( ) \n            self . _control . setFocus ( ) \n        else : \n            self . layout ( ) . setCurrentWidget ( self . _control ) \n        return 1 \n    elif key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return , QtCore . Qt . Key_Tab ) : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageDown , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return 1 \n    elif key == QtCore . Qt . Key_Backspace : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageUp , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return 1 \n    return 0 "}
{"14511": "\ndef _page ( self , text , html = 0 ) : \n    line_height = QtGui . QFontMetrics ( self . font ) . height ( ) \n    minlines = self . _control . viewport ( ) . height ( ) / line_height \n    if self . paging != 'none' and re . match ( \"(?:[^\\n]*\\n){%i}\" % minlines , text ) : \n        if self . paging == 'custom' : \n            self . custom_page_requested . emit ( text ) \n        else : \n            self . _page_control . clear ( ) \n            cursor = self . _page_control . textCursor ( ) \n            if html : \n                self . _insert_html ( cursor , text ) \n            else : \n                self . _insert_plain_text ( cursor , text ) \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            self . _page_control . viewport ( ) . resize ( self . _control . size ( ) ) \n            if self . _splitter : \n                self . _page_control . show ( ) \n                self . _page_control . setFocus ( ) \n            else : \n                self . layout ( ) . setCurrentWidget ( self . _page_control ) \n    elif html : \n        self . _append_html ( text ) \n    else : \n        self . _append_plain_text ( text ) "}
{"14512": "\ndef _prompt_started ( self ) : \n    self . _control . document ( ) . setMaximumBlockCount ( 0 ) \n    self . _control . setUndoRedoEnabled ( 1 ) \n    self . _control . setReadOnly ( 0 ) \n    self . _control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , 1 ) \n    if not self . _reading : \n        self . _executing = 0 \n    self . _prompt_started_hook ( ) \n    if self . _input_buffer_pending : \n        self . input_buffer = self . _input_buffer_pending \n        self . _input_buffer_pending = '' \n    self . _control . moveCursor ( QtGui . QTextCursor . End ) "}
{"14513": "\ndef _readline ( self , prompt = '' , callback = None ) : \n    if self . _reading : \n        raise RuntimeError ( 'Cannot read a line. Widget is already reading.' ) \n    if not callback and not self . isVisible ( ) : \n        raise RuntimeError ( 'Cannot synchronously read a line if the widget ' 'is not visible!' ) \n    self . _reading = 1 \n    self . _show_prompt ( prompt , newline = 0 ) \n    if callback is None : \n        self . _reading_callback = None \n        while self . _reading : \n            QtCore . QCoreApplication . processEvents ( ) \n        return self . _get_input_buffer ( force = 1 ) . rstrip ( '\\n' ) \n    else : \n        self . _reading_callback = lambda : callback ( self . _get_input_buffer ( force = 1 ) . rstrip ( '\\n' ) ) "}
{"14514": "\ndef _set_continuation_prompt ( self , prompt , html = 0 ) : \n    if html : \n        self . _continuation_prompt_html = prompt \n    else : \n        self . _continuation_prompt = prompt \n        self . _continuation_prompt_html = None "}
{"14516": "\ndef _show_prompt ( self , prompt = None , html = 0 , newline = 1 ) : \n    cursor = self . _get_end_cursor ( ) \n    self . _append_before_prompt_pos = cursor . position ( ) \n    if newline and cursor . position ( ) > 0 : \n        cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor ) \n        if cursor . selection ( ) . toPlainText ( ) != '\\n' : \n            self . _append_plain_text ( '\\n' ) \n    self . _append_plain_text ( self . _prompt_sep ) \n    if prompt is None : \n        if self . _prompt_html is None : \n            self . _append_plain_text ( self . _prompt ) \n        else : \n            self . _append_html ( self . _prompt_html ) \n    else : \n        if html : \n            self . _prompt = self . _append_html_fetching_plain_text ( prompt ) \n            self . _prompt_html = prompt \n        else : \n            self . _append_plain_text ( prompt ) \n            self . _prompt = prompt \n            self . _prompt_html = None \n    self . _prompt_pos = self . _get_end_cursor ( ) . position ( ) \n    self . _prompt_started ( ) "}
{"14519": "\ndef copy_config_file ( self , config_file , path = None , overwrite = 0 ) : \n    dst = os . path . join ( self . location , config_file ) \n    if os . path . isfile ( dst ) and not overwrite : \n        return 0 \n    if path is None : \n        path = os . path . join ( get_ipython_package_dir ( ) , u'config' , u'profile' , u'default' ) \n    src = os . path . join ( path , config_file ) \n    shutil . copy ( src , dst ) \n    return 1 "}
{"14526": "\ndef close ( self ) : \n    self . flush ( ) \n    setattr ( sys , self . channel , self . ostream ) \n    self . file . close ( ) \n    self . _closed = 1 "}
{"14531": "\ndef batch_list ( sequence , batch_size , mod = 0 , randomize = 0 ) : \n    if randomize : \n        sequence = random . sample ( sequence , len ( sequence ) ) \n    return [ sequence [ x : x + batch_size ] for x in xrange ( 0 , len ( sequence ) - mod , batch_size ) ] "}
{"14533": "\ndef Walk ( root = '.' , recurse = 1 , pattern = '*' ) : \n    for path , subdirs , files in os . walk ( root ) : \n        for name in files : \n            if fnmatch . fnmatch ( name , pattern ) : \n                yield os . path . join ( path , name ) \n        if not recurse : \n            break "}
{"14538": "\ndef nt_quote_arg ( arg ) : \n    result = [ ] \n    needquote = 0 \n    nb = 0 \n    needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) \n    if needquote : \n        result . append ( '\"' ) \n    for c in arg : \n        if c == '\\\\' : \n            nb += 1 \n        elif c == '\"' : \n            result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) \n            nb = 0 \n        else : \n            if nb : \n                result . append ( '\\\\' * nb ) \n                nb = 0 \n            result . append ( c ) \n    if nb : \n        result . append ( '\\\\' * nb ) \n    if needquote : \n        result . append ( '\\\\' * nb ) \n        result . append ( '\"' ) \n    return '' . join ( result ) "}
{"14542": "\ndef is_archive_file ( name ) : \n    archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) \n    ext = splitext ( name ) [ 1 ] . lower ( ) \n    if ext in archives : \n        return 1 \n    return 0 "}
{"14544": "\ndef readonly ( obj , * , error_on_set = 0 ) : \n    base_cls = type ( obj ) \n    class ReadonlyProxy ( base_cls ) : \n        def __getattribute__ ( self , name ) : \n            return getattr ( obj , name ) \n        def __setattr__ ( self , name , value ) : \n            if error_on_set : \n                raise AttributeError ( 'cannot set readonly object.' ) \n    update_wrapper ( ReadonlyProxy , base_cls , updated = ( ) ) \n    return ReadonlyProxy ( ) "}
{"14552": "\ndef get_home_dir ( require_writable = 0 ) : \n    if hasattr ( sys , \"frozen\" ) : \n        if '\\\\library.zip\\\\' in IPython . __file__ . lower ( ) : \n            root , rest = IPython . __file__ . lower ( ) . split ( 'library.zip' ) \n        else : \n            root = os . path . join ( os . path . split ( IPython . __file__ ) [ 0 ] , \"../../\" ) \n        root = os . path . abspath ( root ) . rstrip ( '\\\\' ) \n        if _writable_dir ( os . path . join ( root , '_ipython' ) ) : \n            os . environ [ \"IPYKITROOT\" ] = root \n        return py3compat . cast_unicode ( root , fs_encoding ) \n    homedir = os . path . expanduser ( '~' ) \n    homedir = os . path . realpath ( homedir ) \n    if not _writable_dir ( homedir ) and os . name == 'nt' : \n        try : \n            import _winreg as wreg \n            key = wreg . OpenKey ( wreg . HKEY_CURRENT_USER , \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\" ) \n            homedir = wreg . QueryValueEx ( key , 'Personal' ) [ 0 ] \n            key . Close ( ) \n        except : \n            pass \n    if ( not require_writable ) or _writable_dir ( homedir ) : \n        return py3compat . cast_unicode ( homedir , fs_encoding ) \n    else : \n        raise HomeDirError ( '%s is not a writable dir, ' 'set $HOME environment variable to override' % homedir ) "}
{"14559": "\ndef check_for_old_config ( ipython_dir = None ) : \n    if ipython_dir is None : \n        ipython_dir = get_ipython_dir ( ) \n    old_configs = [ 'ipy_user_conf.py' , 'ipythonrc' , 'ipython_config.py' ] \n    warned = 0 \n    for cfg in old_configs : \n        f = os . path . join ( ipython_dir , cfg ) \n        if os . path . exists ( f ) : \n            if filehash ( f ) == old_config_md5 . get ( cfg , '' ) : \n                os . unlink ( f ) \n            else : \n                warnings . warn ( \"Found old IPython config file %r (modified by user)\" % f ) \n                warned = 1 \n    if warned : \n        warnings . warn ( \"\"\"  The IPython configuration system has changed as of 0.11, and these files will  be ignored. See http://ipython.github.com/ipython-doc/dev/config for details  of the new config system.  To start configuring IPython, do `ipython profile create`, and edit  `ipython_config.py` in <ipython_dir>/profile_default.  If you need to leave the old config files in place for an older version of  IPython and want to suppress this warning message, set  `c.InteractiveShellApp.ignore_old_config=True` in the new config.\"\"\" ) "}
{"14560": "\ndef update_suggestions_dictionary ( request , object ) : \n    if request . user . is_authenticated ( ) : \n        user = request . user \n        content_type = ContentType . objects . get_for_model ( type ( object ) ) \n        try : \n            ObjectView . objects . get ( user = user , object_id = object . id , content_type = content_type ) \n        except : \n            ObjectView . objects . create ( user = user , content_object = object ) \n        viewed = ObjectView . objects . filter ( user = user ) \n    else : \n        update_dict_for_guests ( request , object , content_type ) \n        return \n    if viewed : \n        for obj in viewed : \n            if content_type == obj . content_type : \n                if not exists_in_dictionary ( request , object , content_type , obj , 1 ) : \n                    if object . id != obj . object_id : \n                        ObjectViewDictionary . objects . create ( current_object = object , visited_before_object = obj . content_object ) \n                        if not exists_in_dictionary ( request , obj , obj . content_type , object , 0 ) : \n                            ObjectViewDictionary . objects . create ( current_object = obj . content_object , visited_before_object = object ) \n    return "}
{"14565": "\ndef lines ( self , encoding = None , errors = 'strict' , retain = 1 ) : \n    if encoding is None and retain : \n        f = self . open ( 'U' ) \n        try : \n            return f . readlines ( ) \n        finally : \n            f . close ( ) \n    else : \n        return self . text ( encoding , errors ) . splitlines ( retain ) "}
{"14566": "\ndef read_md5 ( self ) : \n    f = self . open ( 'rb' ) \n    try : \n        m = md5 ( ) \n        while 1 : \n            d = f . read ( 8192 ) \n            if not d : \n                break \n            m . update ( d ) \n    finally : \n        f . close ( ) \n    return m . digest ( ) "}
{"14570": "\ndef handle ( self , * args , ** options ) : \n    try : \n        while 1 : \n            Channel ( HEARTBEAT_CHANNEL ) . send ( { 'time' : time . time ( ) } ) \n            time . sleep ( HEARTBEAT_FREQUENCY ) \n    except KeyboardInterrupt : \n        print ( \"Received keyboard interrupt, exiting...\" ) "}
{"14571": "\ndef enable_wx ( self , app = None ) : \n    import wx \n    wx_version = V ( wx . __version__ ) . version \n    if wx_version < [ 2 , 8 ] : \n        raise ValueError ( \"requires wxPython >= 2.8, but you have %s\" % wx . __version__ ) \n    from IPython . lib . inputhookwx import inputhook_wx \n    self . set_inputhook ( inputhook_wx ) \n    self . _current_gui = GUI_WX \n    import wx \n    if app is None : \n        app = wx . GetApp ( ) \n    if app is None : \n        app = wx . App ( redirect = 0 , clearSigInt = 0 ) \n    app . _in_event_loop = 1 \n    self . _apps [ GUI_WX ] = app \n    return app "}
{"14572": "\ndef disable_wx ( self ) : \n    if self . _apps . has_key ( GUI_WX ) : \n        self . _apps [ GUI_WX ] . _in_event_loop = 0 \n    self . clear_inputhook ( ) "}
{"14573": "\ndef disable_qt4 ( self ) : \n    if self . _apps . has_key ( GUI_QT4 ) : \n        self . _apps [ GUI_QT4 ] . _in_event_loop = 0 \n    self . clear_inputhook ( ) "}
{"14574": "\ndef enable_gtk ( self , app = None ) : \n    import gtk \n    try : \n        gtk . set_interactive ( 1 ) \n        self . _current_gui = GUI_GTK \n    except AttributeError : \n        from IPython . lib . inputhookgtk import inputhook_gtk \n        self . set_inputhook ( inputhook_gtk ) \n        self . _current_gui = GUI_GTK "}
{"14579": "\ndef _run_sql ( self , sql , params , raw = 1 , output = 0 ) : \n    toget = 'source_raw' if raw else 'source' \n    sqlfrom = \"history\" \n    if output : \n        sqlfrom = \"history LEFT JOIN output_history USING (session, line)\" \n        toget = \"history.%s, output_history.output\" % toget \n    cur = self . db . execute ( \"SELECT session, line, %s FROM %s \" % ( toget , sqlfrom ) + sql , params ) \n    if output : \n        return ( ( ses , lin , ( inp , out ) ) for ses , lin , inp , out in cur ) \n    return cur "}
{"14581": "\ndef get_tail ( self , n = 10 , raw = 1 , output = 0 , include_latest = 0 ) : \n    self . writeout_cache ( ) \n    if not include_latest : \n        n += 1 \n    cur = self . _run_sql ( \"ORDER BY session DESC, line DESC LIMIT ?\" , ( n , ) , raw = raw , output = output ) \n    if not include_latest : \n        return reversed ( list ( cur ) [ 1 : ] ) \n    return reversed ( list ( cur ) ) "}
{"14582": "\ndef get_range_by_str ( self , rangestr , raw = 1 , output = 0 ) : \n    for sess , s , e in extract_hist_ranges ( rangestr ) : \n        for line in self . get_range ( sess , s , e , raw = raw , output = output ) : \n            yield line "}
{"14585": "\ndef reset ( self , new_session = 1 ) : \n    self . output_hist . clear ( ) \n    self . dir_hist [ : ] = [ os . getcwdu ( ) ] \n    if new_session : \n        if self . session_number : \n            self . end_session ( ) \n        self . input_hist_parsed [ : ] = [ \"\" ] \n        self . input_hist_raw [ : ] = [ \"\" ] \n        self . new_session ( ) "}
{"14586": "\ndef _get_range_session ( self , start = 1 , stop = None , raw = 1 , output = 0 ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if start < 0 : \n        start += n \n    if not stop or ( stop > n ) : \n        stop = n \n    elif stop < 0 : \n        stop += n \n    for i in range ( start , stop ) : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( 0 , i , line ) "}
{"14589": "\ndef stop ( self ) : \n    self . stop_now = 1 \n    self . history_manager . save_flag . set ( ) \n    self . join ( ) "}
{"14592": "\ndef disk_partitions ( all = 0 ) : \n    phydevs = [ ] \n    f = open ( \"/proc/filesystems\" , \"r\" ) \n    try : \n        for line in f : \n            if not line . startswith ( \"nodev\" ) : \n                phydevs . append ( line . strip ( ) ) \n    finally : \n        f . close ( ) \n    retlist = [ ] \n    partitions = _psutil_linux . get_disk_partitions ( ) \n    for partition in partitions : \n        device , mountpoint , fstype , opts = partition \n        if device == 'none' : \n            device = '' \n        if not all : \n            if device == '' or fstype not in phydevs : \n                continue \n        ntuple = nt_partition ( device , mountpoint , fstype , opts ) \n        retlist . append ( ntuple ) \n    return retlist "}
{"14613": "\ndef _run_exec_lines ( self ) : \n    if not self . exec_lines : \n        return \n    try : \n        self . log . debug ( \"Running code from IPythonApp.exec_lines...\" ) \n        for line in self . exec_lines : \n            try : \n                self . log . info ( \"Running code in user namespace: %s\" % line ) \n                self . shell . run_cell ( line , store_history = 0 ) \n            except : \n                self . log . warn ( \"Error in executing line in user \" \"namespace: %s\" % line ) \n                self . shell . showtraceback ( ) \n    except : \n        self . log . warn ( \"Unknown error in handling IPythonApp.exec_lines:\" ) \n        self . shell . showtraceback ( ) "}
{"14616": "\ndef _run_cmd_line_code ( self ) : \n    if self . code_to_run : \n        line = self . code_to_run \n        try : \n            self . log . info ( \"Running code given at command line (c=): %s\" % line ) \n            self . shell . run_cell ( line , store_history = 0 ) \n        except : \n            self . log . warn ( \"Error in executing line in user namespace: %s\" % line ) \n            self . shell . showtraceback ( ) \n    elif self . file_to_run : \n        fname = self . file_to_run \n        try : \n            self . _exec_file ( fname ) \n        except : \n            self . log . warn ( \"Error in executing file in user namespace: %s\" % fname ) \n            self . shell . showtraceback ( ) "}
{"14627": "\ndef read ( self , directory ) : \n    usable = 0 \n    try : \n        status_file = os . path . join ( directory , self . STATUS_FILE ) \n        fstatus = open ( status_file , \"rb\" ) \n        try : \n            status = pickle . load ( fstatus ) \n        finally : \n            fstatus . close ( ) \n    except ( IOError , ValueError ) : \n        usable = 0 \n    else : \n        usable = 1 \n        if status [ 'format' ] != self . STATUS_FORMAT : \n            usable = 0 \n        elif status [ 'version' ] != coverage . __version__ : \n            usable = 0 \n    if usable : \n        self . files = status [ 'files' ] \n        self . settings = status [ 'settings' ] \n    else : \n        self . reset ( ) "}
{"14637": "\ndef start_kernel ( self , ** kwargs ) : \n    kernel_id = unicode ( uuid . uuid4 ( ) ) \n    km = self . kernel_manager_factory ( connection_file = os . path . join ( self . connection_dir , \"kernel-%s.json\" % kernel_id ) , config = self . config , ) \n    km . start_kernel ( ** kwargs ) \n    km . start_channels ( shell = 1 , sub = 0 , stdin = 0 , hb = 0 ) \n    self . _kernels [ kernel_id ] = km \n    return kernel_id "}
{"14651": "\ndef export_html ( html , filename , image_tag = None , inline = 1 ) : \n    if image_tag is None : \n        image_tag = default_image_tag \n    else : \n        image_tag = ensure_utf8 ( image_tag ) \n    if inline : \n        path = None \n    else : \n        root , ext = os . path . splitext ( filename ) \n        path = root + \"_files\" \n        if os . path . isfile ( path ) : \n            raise OSError ( \"%s exists, but is not a directory.\" % path ) \n    with open ( filename , 'w' ) as f : \n        html = fix_html ( html ) \n        f . write ( IMG_RE . sub ( lambda x : image_tag ( x , path = path , format = \"png\" ) , html ) ) "}
{"14664": "\ndef validateAttrib ( self , method , cls = None ) : \n    any = 0 \n    for group in self . attribs : \n        match = 1 \n        for key , value in group : \n            attr = get_method_attr ( method , cls , key ) \n            if callable ( value ) : \n                if not value ( key , method , cls ) : \n                    match = 0 \n                    break \n            elif value is 1 : \n                if not bool ( attr ) : \n                    match = 0 \n                    break \n            elif value is 0 : \n                if bool ( attr ) : \n                    match = 0 \n                    break \n            elif type ( attr ) in ( list , tuple ) : \n                if not str ( value ) . lower ( ) in [ str ( x ) . lower ( ) for x in attr ] : \n                    match = 0 \n                    break \n            else : \n                if ( value != attr and str ( value ) . lower ( ) != str ( attr ) . lower ( ) ) : \n                    match = 0 \n                    break \n        any = any or match \n    if any : \n        return None \n    return 0 "}
{"14665": "\ndef wantMethod ( self , method ) : \n    try : \n        cls = method . im_class \n    except AttributeError : \n        return 0 \n    return self . validateAttrib ( method , cls ) "}
{"14666": "\ndef rotate ( self ) : \n    if self . _prev_yank : \n        text = self . _ring . rotate ( ) \n        if text : \n            self . _skip_cursor = 1 \n            cursor = self . _text_edit . textCursor ( ) \n            cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , n = len ( self . _prev_yank ) ) \n            cursor . insertText ( text ) \n            self . _prev_yank = text "}
{"14668": "\ndef parser_from_schema ( schema_url , require_version = 1 ) : \n    schema_tree = etree . parse ( schema_url ) \n    def get_version ( element , getter ) : \n        try : \n            return getter ( element ) \n        except VersionNotFound : \n            if require_version : \n                raise \n            else : \n                return None \n    root = schema_tree . getroot ( ) \n    if root . tag == '{%s}definitions' % namespaces . WSDL : \n        schema_el = schema_tree . find ( 'wsdl:types/xs:schema' , namespaces = NS_MAP ) \n        version = get_version ( root , version_from_wsdl ) \n    else : \n        schema_el = root \n        version = get_version ( schema_el , version_from_schema ) \n    schema = etree . XMLSchema ( schema_el ) \n    return objectify . makeparser ( schema = schema ) , version "}
{"14671": "\ndef _inject_cookie_message ( self , msg ) : \n    if isinstance ( msg , unicode ) : \n        msg = msg . encode ( 'utf8' , 'replace' ) \n    try : \n        self . request . _cookies = Cookie . SimpleCookie ( msg ) \n    except : \n        logging . warn ( \"couldn't parse cookie string: %s\" , msg , exc_info = 1 ) "}
{"14672": "\ndef start_hb ( self , callback ) : \n    if not self . _beating : \n        self . _kernel_alive = 1 \n        def ping_or_dead ( ) : \n            self . hb_stream . flush ( ) \n            if self . _kernel_alive : \n                self . _kernel_alive = 0 \n                self . hb_stream . send ( b'ping' ) \n                self . hb_stream . flush ( ) \n            else : \n                try : \n                    callback ( ) \n                except : \n                    pass \n                finally : \n                    self . stop_hb ( ) \n        def beat_received ( msg ) : \n            self . _kernel_alive = 1 \n        self . hb_stream . on_recv ( beat_received ) \n        loop = ioloop . IOLoop . instance ( ) \n        self . _hb_periodic_callback = ioloop . PeriodicCallback ( ping_or_dead , self . time_to_dead * 1000 , loop ) \n        loop . add_timeout ( time . time ( ) + self . first_beat , self . _really_start_hb ) \n        self . _beating = 1 "}
{"14674": "\ndef stop_hb ( self ) : \n    if self . _beating : \n        self . _beating = 0 \n        self . _hb_periodic_callback . stop ( ) \n        if not self . hb_stream . closed ( ) : \n            self . hb_stream . on_recv ( None ) "}
{"14677": "\ndef seek ( self , index ) : \n    if index < 0 : \n        index = self . nblocks + index \n    self . _validate_index ( index ) \n    self . block_index = index \n    self . finished = 0 "}
{"14681": "\ndef series ( collection , method , prints = 15 , * args , ** kwargs ) : \n    if 'verbose' in kwargs . keys ( ) : \n        verbose = kwargs [ 'verbose' ] \n    else : \n        verbose = 1 \n    results = [ ] \n    timer = turntable . utils . Timer ( nLoops = len ( collection ) , numPrints = prints , verbose = verbose ) \n    for subject in collection : \n        results . append ( method ( subject , * args , ** kwargs ) ) \n        timer . loop ( ) \n    timer . fin ( ) \n    return results "}
{"14689": "\ndef countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = 0 , progressbar_inversed = 0 , showpct = 0 ) : \n    end_date = dateparse . parse_datetime ( date ) \n    end = dateformat . format ( end_date , 'U' ) \n    content = '<div class=\"name\">' + name + '</div>' \n    content += '<div class=\"description\">' + description + '</div>' \n    if progressbar : \n        if not end : \n            raise Exception ( 'For progressbar, start date is requried.' ) \n        parsed_date = datetime . datetime . combine ( dateparse . parse_date ( start ) , datetime . time ( ) ) \n        start_date = dateparse . parse_datetime ( start ) or parsed_date \n        now = datetime . datetime . now ( ) \n        pct = ( now - start_date ) . total_seconds ( ) / ( end_date - start_date ) . total_seconds ( ) \n        pct = int ( pct * 100 ) \n        if progressbar_inversed : \n            pct = 100 - pct \n        bar = '<div class=\"progress progress-striped active\">' \n        bar += '<div class=\"progress-bar\"  role=\"progressbar\" aria-valuenow=\"{pct}\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: {pct}%\">' \n        bar += '<span class=\"sr-only\">{pct}% Complete</span>' \n        bar += '</div>' \n        bar += '</div>' \n        if showpct : \n            bar += '<div class=\"percentage\">{pct}%</div>' \n        bar = bar . format ( pct = pct ) \n        content += bar \n    content += '<div class=\"counter\"></div>' \n    attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } \n    if id : \n        attr [ 'id' ] = id \n    return html . tag ( 'div' , content , attr ) "}
{"14696": "\ndef get_msgs ( self ) : \n    msgs = [ ] \n    while 1 : \n        try : \n            msgs . append ( self . get_msg ( block = 0 ) ) \n        except Empty : \n            break \n    return msgs "}
{"14697": "\ndef get_msg ( self , block = 1 , timeout = None ) : \n    return self . _in_queue . get ( block , timeout ) "}
{"14698": "\ndef prop ( func = None , * , field = _UNSET , get : bool = 1 , set : bool = 1 , del_ : bool = 0 , default = _UNSET , types : tuple = _UNSET ) : \n    def wrap ( func ) : \n        if not callable ( func ) : \n            raise TypeError \n        prop_name = func . __name__ \n        key = field \n        if key is _UNSET : \n            key = '_' + prop_name \n        fget , fset , fdel = None , None , None \n        if get : \n            def fget ( self ) : \n                try : \n                    return self . __dict__ [ key ] \n                except KeyError : \n                    if default is not _UNSET : \n                        return default \n                    raise AttributeError ( f\"'{type(self).__name__}' object has no attribute '{key}'\" ) \n        if set : \n            def fset ( self , val ) : \n                if types is not _UNSET and not isinstance ( val , types ) : \n                    if isinstance ( types , tuple ) : \n                        types_name = tuple ( x . __name__ for x in types ) \n                    else : \n                        types_name = types . __name__ \n                    raise TypeError ( f'type of {type(self).__name__}.{prop_name} must be {types_name}; ' f'got {type(val).__name__} instead' ) \n                self . __dict__ [ key ] = val \n        if del_ : \n            def fdel ( self ) : \n                del self . __dict__ [ key ] \n        return property ( fget , fset , fdel , func . __doc__ ) \n    return wrap ( func ) if func else wrap "}
{"14702": "\ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = 0 \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules += module_list ( path ) \n        if time ( ) - t >= TIMEOUT_STORAGE and not store : \n            store = 1 \n            print ( \"\\nCaching the list of root modules, please wait!\" ) \n            print ( \"(This will only be done once - type '%rehashx' to \" \"reset cache!)\\n\" ) \n            sys . stdout . flush ( ) \n        if time ( ) - t > TIMEOUT_GIVEUP : \n            print ( \"This is taking too long, we give up.\\n\" ) \n            ip . db [ 'rootmodules' ] = [ ] \n            return [ ] \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14704": "\ndef module_completion ( line ) : \n    words = line . split ( ' ' ) \n    nwords = len ( words ) \n    if nwords == 3 and words [ 0 ] == 'from' : \n        return [ 'import ' ] \n    if nwords < 3 and ( words [ 0 ] in [ 'import' , 'from' ] ) : \n        if nwords == 1 : \n            return get_root_modules ( ) \n        mod = words [ 1 ] . split ( '.' ) \n        if len ( mod ) < 2 : \n            return get_root_modules ( ) \n        completion_list = try_import ( '.' . join ( mod [ : - 1 ] ) , 1 ) \n        return [ '.' . join ( mod [ : - 1 ] + [ el ] ) for el in completion_list ] \n    if nwords >= 3 and words [ 0 ] == 'from' : \n        mod = words [ 1 ] \n        return try_import ( mod ) "}
{"14705": "\ndef magic_run_completer ( self , event ) : \n    comps = arg_split ( event . line , strict = 0 ) \n    relpath = ( len ( comps ) > 1 and comps [ - 1 ] or '' ) . strip ( \"'\\\"\" ) \n    lglob = glob . glob \n    isdir = os . path . isdir \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    dirs = [ f . replace ( '\\\\' , '/' ) + \"/\" for f in lglob ( relpath + '*' ) if isdir ( f ) ] \n    if filter ( magic_run_re . match , comps ) : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( '*' ) ] \n    else : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in dirs + pys ] "}
{"14717": "\ndef handle_stranded_tasks ( self , engine ) : \n    lost = self . pending [ engine ] \n    for msg_id in lost . keys ( ) : \n        if msg_id not in self . pending [ engine ] : \n            continue \n        raw_msg = lost [ msg_id ] . raw_msg \n        idents , msg = self . session . feed_identities ( raw_msg , copy = 0 ) \n        parent = self . session . unpack ( msg [ 1 ] . bytes ) \n        idents = [ engine , idents [ 0 ] ] \n        try : \n            raise error . EngineError ( \"Engine %r died while running task %r\" % ( engine , msg_id ) ) \n        except : \n            content = error . wrap_exception ( ) \n        header = dict ( status = 'error' , engine = engine , date = datetime . now ( ) , ) \n        msg = self . session . msg ( 'apply_reply' , content , parent = parent , subheader = header ) \n        raw_reply = map ( zmq . Message , self . session . serialize ( msg , ident = idents ) ) \n        self . dispatch_result ( raw_reply ) \n    self . completed . pop ( engine ) \n    self . failed . pop ( engine ) "}
{"14718": "\ndef dispatch_submission ( self , raw_msg ) : \n    self . notifier_stream . flush ( ) \n    try : \n        idents , msg = self . session . feed_identities ( raw_msg , copy = 0 ) \n        msg = self . session . unserialize ( msg , content = 0 , copy = 0 ) \n    except Exception : \n        self . log . error ( \"task::Invaid task msg: %r\" % raw_msg , exc_info = 1 ) \n        return \n    self . mon_stream . send_multipart ( [ b'intask' ] + raw_msg , copy = 0 ) \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    self . all_ids . add ( msg_id ) \n    targets = header . get ( 'targets' , [ ] ) \n    targets = map ( cast_bytes , targets ) \n    targets = set ( targets ) \n    retries = header . get ( 'retries' , 0 ) \n    self . retries [ msg_id ] = retries \n    after = header . get ( 'after' , None ) \n    if after : \n        after = Dependency ( after ) \n        if after . all : \n            if after . success : \n                after = Dependency ( after . difference ( self . all_completed ) , success = after . success , failure = after . failure , all = after . all , ) \n            if after . failure : \n                after = Dependency ( after . difference ( self . all_failed ) , success = after . success , failure = after . failure , all = after . all , ) \n        if after . check ( self . all_completed , self . all_failed ) : \n            after = MET \n    else : \n        after = MET \n    follow = Dependency ( header . get ( 'follow' , [ ] ) ) \n    timeout = header . get ( 'timeout' , None ) \n    if timeout : \n        timeout = datetime . now ( ) + timedelta ( 0 , float ( timeout ) , 0 ) \n    job = Job ( msg_id = msg_id , raw_msg = raw_msg , idents = idents , msg = msg , header = header , targets = targets , after = after , follow = follow , timeout = timeout , ) \n    for dep in after , follow : \n        if not dep : \n            continue \n        if msg_id in dep or dep . difference ( self . all_ids ) : \n            self . depending [ msg_id ] = job \n            return self . fail_unreachable ( msg_id , error . InvalidDependency ) \n        if dep . unreachable ( self . all_completed , self . all_failed ) : \n            self . depending [ msg_id ] = job \n            return self . fail_unreachable ( msg_id ) \n    if after . check ( self . all_completed , self . all_failed ) : \n        if not self . maybe_run ( job ) : \n            if msg_id not in self . all_failed : \n                self . save_unmet ( job ) \n    else : \n        self . save_unmet ( job ) "}
{"14720": "\ndef fail_unreachable ( self , msg_id , why = error . ImpossibleDependency ) : \n    if msg_id not in self . depending : \n        self . log . error ( \"msg %r already failed!\" , msg_id ) \n        return \n    job = self . depending . pop ( msg_id ) \n    for mid in job . dependents : \n        if mid in self . graph : \n            self . graph [ mid ] . remove ( msg_id ) \n    try : \n        raise why ( ) \n    except : \n        content = error . wrap_exception ( ) \n    self . all_done . add ( msg_id ) \n    self . all_failed . add ( msg_id ) \n    msg = self . session . send ( self . client_stream , 'apply_reply' , content , parent = job . header , ident = job . idents ) \n    self . session . send ( self . mon_stream , msg , ident = [ b'outtask' ] + job . idents ) \n    self . update_graph ( msg_id , success = 0 ) "}
{"14721": "\ndef maybe_run ( self , job ) : \n    msg_id = job . msg_id \n    self . log . debug ( \"Attempting to assign task %s\" , msg_id ) \n    if not self . targets : \n        return 0 \n    if job . follow or job . targets or job . blacklist or self . hwm : \n        def can_run ( idx ) : \n            if self . hwm and self . loads [ idx ] == self . hwm : \n                return 0 \n            target = self . targets [ idx ] \n            if target in job . blacklist : \n                return 0 \n            if job . targets and target not in job . targets : \n                return 0 \n            return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) \n        indices = filter ( can_run , range ( len ( self . targets ) ) ) \n        if not indices : \n            if job . follow . all : \n                dests = set ( ) \n                relevant = set ( ) \n                if job . follow . success : \n                    relevant = self . all_completed \n                if job . follow . failure : \n                    relevant = relevant . union ( self . all_failed ) \n                for m in job . follow . intersection ( relevant ) : \n                    dests . add ( self . destinations [ m ] ) \n                if len ( dests ) > 1 : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return 0 \n            if job . targets : \n                job . targets . difference_update ( job . blacklist ) \n                if not job . targets or not job . targets . intersection ( self . targets ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return 0 \n            return 0 \n    else : \n        indices = None \n    self . submit_task ( job , indices ) \n    return 1 "}
{"14723": "\ndef submit_task ( self , job , indices = None ) : \n    if indices : \n        loads = [ self . loads [ i ] for i in indices ] \n    else : \n        loads = self . loads \n    idx = self . scheme ( loads ) \n    if indices : \n        idx = indices [ idx ] \n    target = self . targets [ idx ] \n    self . engine_stream . send ( target , flags = zmq . SNDMORE , copy = 0 ) \n    self . engine_stream . send_multipart ( job . raw_msg , copy = 0 ) \n    self . add_job ( idx ) \n    self . pending [ target ] [ job . msg_id ] = job \n    content = dict ( msg_id = job . msg_id , engine_id = target . decode ( 'ascii' ) ) \n    self . session . send ( self . mon_stream , 'task_destination' , content = content , ident = [ b'tracktask' , self . ident ] ) "}
{"14724": "\ndef dispatch_result ( self , raw_msg ) : \n    try : \n        idents , msg = self . session . feed_identities ( raw_msg , copy = 0 ) \n        msg = self . session . unserialize ( msg , content = 0 , copy = 0 ) \n        engine = idents [ 0 ] \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            self . finish_job ( idx ) \n    except Exception : \n        self . log . error ( \"task::Invaid result: %r\" , raw_msg , exc_info = 1 ) \n        return \n    header = msg [ 'header' ] \n    parent = msg [ 'parent_header' ] \n    if header . get ( 'dependencies_met' , 1 ) : \n        success = ( header [ 'status' ] == 'ok' ) \n        msg_id = parent [ 'msg_id' ] \n        retries = self . retries [ msg_id ] \n        if not success and retries > 0 : \n            self . retries [ msg_id ] = retries - 1 \n            self . handle_unmet_dependency ( idents , parent ) \n        else : \n            del self . retries [ msg_id ] \n            self . handle_result ( idents , parent , raw_msg , success ) \n            self . mon_stream . send_multipart ( [ b'outtask' ] + raw_msg , copy = 0 ) \n    else : \n        self . handle_unmet_dependency ( idents , parent ) "}
{"14725": "\ndef handle_result ( self , idents , parent , raw_msg , success = 1 ) : \n    engine = idents [ 0 ] \n    client = idents [ 1 ] \n    raw_msg [ : 2 ] = [ client , engine ] \n    self . client_stream . send_multipart ( raw_msg , copy = 0 ) \n    msg_id = parent [ 'msg_id' ] \n    self . pending [ engine ] . pop ( msg_id ) \n    if success : \n        self . completed [ engine ] . add ( msg_id ) \n        self . all_completed . add ( msg_id ) \n    else : \n        self . failed [ engine ] . add ( msg_id ) \n        self . all_failed . add ( msg_id ) \n    self . all_done . add ( msg_id ) \n    self . destinations [ msg_id ] = engine \n    self . update_graph ( msg_id , success ) "}
{"14727": "\ndef update_graph ( self , dep_id = None , success = 1 ) : \n    jobs = self . graph . pop ( dep_id , [ ] ) \n    if dep_id is None or self . hwm and any ( [ load == self . hwm - 1 for load in self . loads ] ) : \n        jobs = self . depending . keys ( ) \n    for msg_id in sorted ( jobs , key = lambda msg_id : self . depending [ msg_id ] . timestamp ) : \n        job = self . depending [ msg_id ] \n        if job . after . unreachable ( self . all_completed , self . all_failed ) or job . follow . unreachable ( self . all_completed , self . all_failed ) : \n            self . fail_unreachable ( msg_id ) \n        elif job . after . check ( self . all_completed , self . all_failed ) : \n            if self . maybe_run ( job ) : \n                self . depending . pop ( msg_id ) \n                for mid in job . dependents : \n                    if mid in self . graph : \n                        self . graph [ mid ] . remove ( msg_id ) "}
{"14742": "\ndef notebook_exists ( self , notebook_id ) : \n    if notebook_id not in self . mapping : \n        return 0 \n    path = self . get_path_by_name ( self . mapping [ notebook_id ] ) \n    return os . path . isfile ( path ) "}
{"14745": "\ndef get_notebook ( self , notebook_id , format = u'json' ) : \n    format = unicode ( format ) \n    if format not in self . allowed_formats : \n        raise web . HTTPError ( 415 , u'Invalid notebook format: %s' % format ) \n    last_modified , nb = self . get_notebook_object ( notebook_id ) \n    kwargs = { } \n    if format == 'json' : \n        kwargs [ 'split_lines' ] = 0 \n    data = current . writes ( nb , format , ** kwargs ) \n    name = nb . metadata . get ( 'name' , 'notebook' ) \n    return last_modified , name , data "}
{"14753": "\ndef phys_tokens ( toks ) : \n    last_line = None \n    last_lineno = - 1 \n    last_ttype = None \n    for ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext in toks : \n        if last_lineno != elineno : \n            if last_line and last_line . endswith ( \"\\\\\\n\" ) : \n                inject_backslash = 1 \n                if last_ttype == tokenize . COMMENT : \n                    inject_backslash = 0 \n                elif ttype == token . STRING : \n                    if \"\\n\" in ttext and ttext . split ( '\\n' , 1 ) [ 0 ] [ - 1 ] == '\\\\' : \n                        inject_backslash = 0 \n                if inject_backslash : \n                    ccol = len ( last_line . split ( \"\\n\" ) [ - 2 ] ) - 1 \n                    yield ( 99999 , \"\\\\\\n\" , ( slineno , ccol ) , ( slineno , ccol + 2 ) , last_line ) \n            last_line = ltext \n            last_ttype = ttype \n        yield ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext \n        last_lineno = elineno "}
{"14754": "\ndef source_token_lines ( source ) : \n    ws_tokens = set ( [ token . INDENT , token . DEDENT , token . NEWLINE , tokenize . NL ] ) \n    line = [ ] \n    col = 0 \n    source = source . expandtabs ( 8 ) . replace ( '\\r\\n' , '\\n' ) \n    tokgen = generate_tokens ( source ) \n    for ttype , ttext , ( _ , scol ) , ( _ , ecol ) , _ in phys_tokens ( tokgen ) : \n        mark_start = 1 \n        for part in re . split ( '(\\n)' , ttext ) : \n            if part == '\\n' : \n                yield line \n                line = [ ] \n                col = 0 \n                mark_end = 0 \n            elif part == '' : \n                mark_end = 0 \n            elif ttype in ws_tokens : \n                mark_end = 0 \n            else : \n                if mark_start and scol > col : \n                    line . append ( ( \"ws\" , \" \" * ( scol - col ) ) ) \n                    mark_start = 0 \n                tok_class = tokenize . tok_name . get ( ttype , 'xx' ) . lower ( ) [ : 3 ] \n                if ttype == token . NAME and keyword . iskeyword ( ttext ) : \n                    tok_class = \"key\" \n                line . append ( ( tok_class , part ) ) \n                mark_end = 1 \n            scol = 0 \n        if mark_end : \n            col = ecol \n    if line : \n        yield line "}
{"14759": "\ndef init_shell ( self ) : \n    self . shell = TerminalInteractiveShell . instance ( config = self . config , display_banner = 0 , profile_dir = self . profile_dir , ipython_dir = self . ipython_dir ) \n    self . shell . configurables . append ( self ) "}
{"14764": "\ndef on_trait_change ( self , handler , name = None , remove = 0 ) : \n    if remove : \n        names = parse_notifier_name ( name ) \n        for n in names : \n            self . _remove_notifiers ( handler , n ) \n    else : \n        names = parse_notifier_name ( name ) \n        for n in names : \n            self . _add_notifiers ( handler , n ) "}
{"14769": "\ndef check ( self , completed , failed = None ) : \n    if len ( self ) == 0 : \n        return 1 \n    against = set ( ) \n    if self . success : \n        against = completed \n    if failed is not None and self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return self . issubset ( against ) \n    else : \n        return not self . isdisjoint ( against ) "}
{"14770": "\ndef unreachable ( self , completed , failed = None ) : \n    if len ( self ) == 0 : \n        return 0 \n    against = set ( ) \n    if not self . success : \n        against = completed \n    if failed is not None and not self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return not self . isdisjoint ( against ) \n    else : \n        return self . issubset ( against ) "}
{"14775": "\ndef allreduce ( self , f , value , flat = 1 ) : \n    return self . reduce ( f , value , flat = flat , all = 1 ) "}
{"14778": "\ndef dispatch_query ( self , msg ) : \n    try : \n        idents , msg = self . session . feed_identities ( msg ) \n    except ValueError : \n        idents = [ ] \n    if not idents : \n        self . log . error ( \"Bad Query Message: %r\" , msg ) \n        return \n    client_id = idents [ 0 ] \n    try : \n        msg = self . session . unserialize ( msg , content = 1 ) \n    except Exception : \n        content = error . wrap_exception ( ) \n        self . log . error ( \"Bad Query Message: %r\" , msg , exc_info = 1 ) \n        self . session . send ( self . query , \"hub_error\" , ident = client_id , content = content ) \n        return \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    self . log . info ( \"client::client %r requested %r\" , client_id , msg_type ) \n    handler = self . query_handlers . get ( msg_type , None ) \n    try : \n        assert handler is not None , \"Bad Message Type: %r\" % msg_type \n    except : \n        content = error . wrap_exception ( ) \n        self . log . error ( \"Bad Message Type: %r\" , msg_type , exc_info = 1 ) \n        self . session . send ( self . query , \"hub_error\" , ident = client_id , content = content ) \n        return \n    else : \n        handler ( idents , msg ) "}
{"14781": "\ndef save_task_request ( self , idents , msg ) : \n    client_id = idents [ 0 ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::client %r sent invalid task message: %r\" , client_id , msg , exc_info = 1 ) \n        return \n    record = init_record ( msg ) \n    record [ 'client_uuid' ] = client_id . decode ( 'ascii' ) \n    record [ 'queue' ] = 'task' \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    self . pending . add ( msg_id ) \n    self . unassigned . add ( msg_id ) \n    try : \n        existing = self . db . get_record ( msg_id ) \n        if existing [ 'resubmitted' ] : \n            for key in ( 'submitted' , 'client_uuid' , 'buffers' ) : \n                record . pop ( key ) \n        for key , evalue in existing . iteritems ( ) : \n            if key . endswith ( 'buffers' ) : \n                continue \n            rvalue = record . get ( key , None ) \n            if evalue and rvalue and evalue != rvalue : \n                self . log . warn ( \"conflicting initial state for record: %r:%r <%r> %r\" , msg_id , rvalue , key , evalue ) \n            elif evalue and not rvalue : \n                record [ key ] = evalue \n        try : \n            self . db . update_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error updating record %r\" , msg_id , exc_info = 1 ) \n    except KeyError : \n        try : \n            self . db . add_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error adding record %r\" , msg_id , exc_info = 1 ) \n    except Exception : \n        self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = 1 ) "}
{"14782": "\ndef save_task_result ( self , idents , msg ) : \n    client_id = idents [ 0 ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::invalid task result message send to %r: %r\" , client_id , msg , exc_info = 1 ) \n        return \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        self . log . warn ( \"Task %r had no parent!\" , msg ) \n        return \n    msg_id = parent [ 'msg_id' ] \n    if msg_id in self . unassigned : \n        self . unassigned . remove ( msg_id ) \n    header = msg [ 'header' ] \n    engine_uuid = header . get ( 'engine' , u'' ) \n    eid = self . by_ident . get ( cast_bytes ( engine_uuid ) , None ) \n    status = header . get ( 'status' , None ) \n    if msg_id in self . pending : \n        self . log . info ( \"task::task %r finished on %s\" , msg_id , eid ) \n        self . pending . remove ( msg_id ) \n        self . all_completed . add ( msg_id ) \n        if eid is not None : \n            if status != 'aborted' : \n                self . completed [ eid ] . append ( msg_id ) \n            if msg_id in self . tasks [ eid ] : \n                self . tasks [ eid ] . remove ( msg_id ) \n        completed = header [ 'date' ] \n        started = header . get ( 'started' , None ) \n        result = { 'result_header' : header , 'result_content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine_uuid' : engine_uuid , } \n        result [ 'result_buffers' ] = msg [ 'buffers' ] \n        try : \n            self . db . update_record ( msg_id , result ) \n        except Exception : \n            self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = 1 ) \n    else : \n        self . log . debug ( \"task::unknown task %r finished\" , msg_id ) "}
{"14783": "\ndef save_iopub_message ( self , topics , msg ) : \n    try : \n        msg = self . session . unserialize ( msg , content = 1 ) \n    except Exception : \n        self . log . error ( \"iopub::invalid IOPub message\" , exc_info = 1 ) \n        return \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        self . log . warn ( \"iopub::IOPub message lacks parent: %r\" , msg ) \n        return \n    msg_id = parent [ 'msg_id' ] \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    content = msg [ 'content' ] \n    try : \n        rec = self . db . get_record ( msg_id ) \n    except KeyError : \n        rec = empty_record ( ) \n        rec [ 'msg_id' ] = msg_id \n        self . db . add_record ( msg_id , rec ) \n    d = { } \n    if msg_type == 'stream' : \n        name = content [ 'name' ] \n        s = rec [ name ] or '' \n        d [ name ] = s + content [ 'data' ] \n    elif msg_type == 'pyerr' : \n        d [ 'pyerr' ] = content \n    elif msg_type == 'pyin' : \n        d [ 'pyin' ] = content [ 'code' ] \n    elif msg_type in ( 'display_data' , 'pyout' ) : \n        d [ msg_type ] = content \n    elif msg_type == 'status' : \n        pass \n    else : \n        self . log . warn ( \"unhandled iopub msg_type: %r\" , msg_type ) \n    if not d : \n        return \n    try : \n        self . db . update_record ( msg_id , d ) \n    except Exception : \n        self . log . error ( \"DB Error saving iopub message %r\" , msg_id , exc_info = 1 ) "}
{"14785": "\ndef register_engine ( self , reg , msg ) : \n    content = msg [ 'content' ] \n    try : \n        queue = cast_bytes ( content [ 'queue' ] ) \n    except KeyError : \n        self . log . error ( \"registration::queue not specified\" , exc_info = 1 ) \n        return \n    heart = content . get ( 'heartbeat' , None ) \n    if heart : \n        heart = cast_bytes ( heart ) \n    eid = self . _next_id \n    self . log . debug ( \"registration::register_engine(%i, %r, %r, %r)\" , eid , queue , reg , heart ) \n    content = dict ( id = eid , status = 'ok' ) \n    content . update ( self . engine_info ) \n    if queue in self . by_ident : \n        try : \n            raise KeyError ( \"queue_id %r in use\" % queue ) \n        except : \n            content = error . wrap_exception ( ) \n            self . log . error ( \"queue_id %r in use\" , queue , exc_info = 1 ) \n    elif heart in self . hearts : \n        try : \n            raise KeyError ( \"heart_id %r in use\" % heart ) \n        except : \n            self . log . error ( \"heart_id %r in use\" , heart , exc_info = 1 ) \n            content = error . wrap_exception ( ) \n    else : \n        for h , pack in self . incoming_registrations . iteritems ( ) : \n            if heart == h : \n                try : \n                    raise KeyError ( \"heart_id %r in use\" % heart ) \n                except : \n                    self . log . error ( \"heart_id %r in use\" , heart , exc_info = 1 ) \n                    content = error . wrap_exception ( ) \n                break \n            elif queue == pack [ 1 ] : \n                try : \n                    raise KeyError ( \"queue_id %r in use\" % queue ) \n                except : \n                    self . log . error ( \"queue_id %r in use\" , queue , exc_info = 1 ) \n                    content = error . wrap_exception ( ) \n                break \n    msg = self . session . send ( self . query , \"registration_reply\" , content = content , ident = reg ) \n    if content [ 'status' ] == 'ok' : \n        if heart in self . heartmonitor . hearts : \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) \n            self . finish_registration ( heart ) \n        else : \n            purge = lambda : self . _purge_stalled_registration ( heart ) \n            dc = ioloop . DelayedCallback ( purge , self . registration_timeout , self . loop ) \n            dc . start ( ) \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) \n    else : \n        self . log . error ( \"registration::registration %i failed: %r\" , eid , content [ 'evalue' ] ) \n    return eid "}
{"14786": "\ndef unregister_engine ( self , ident , msg ) : \n    try : \n        eid = msg [ 'content' ] [ 'id' ] \n    except : \n        self . log . error ( \"registration::bad engine id for unregistration: %r\" , ident , exc_info = 1 ) \n        return \n    self . log . info ( \"registration::unregister_engine(%r)\" , eid ) \n    uuid = self . keytable [ eid ] \n    content = dict ( id = eid , queue = uuid . decode ( 'ascii' ) ) \n    self . dead_engines . add ( uuid ) \n    handleit = lambda : self . _handle_stranded_msgs ( eid , uuid ) \n    dc = ioloop . DelayedCallback ( handleit , self . registration_timeout , self . loop ) \n    dc . start ( ) \n    if self . notifier : \n        self . session . send ( self . notifier , \"unregistration_notification\" , content = content ) "}
{"14787": "\ndef finish_registration ( self , heart ) : \n    try : \n        ( eid , queue , reg , purge ) = self . incoming_registrations . pop ( heart ) \n    except KeyError : \n        self . log . error ( \"registration::tried to finish nonexistant registration\" , exc_info = 1 ) \n        return \n    self . log . info ( \"registration::finished registering engine %i:%r\" , eid , queue ) \n    if purge is not None : \n        purge . stop ( ) \n    control = queue \n    self . ids . add ( eid ) \n    self . keytable [ eid ] = queue \n    self . engines [ eid ] = EngineConnector ( id = eid , queue = queue , registration = reg , control = control , heartbeat = heart ) \n    self . by_ident [ queue ] = eid \n    self . queues [ eid ] = list ( ) \n    self . tasks [ eid ] = list ( ) \n    self . completed [ eid ] = list ( ) \n    self . hearts [ heart ] = eid \n    content = dict ( id = eid , queue = self . engines [ eid ] . queue . decode ( 'ascii' ) ) \n    if self . notifier : \n        self . session . send ( self . notifier , \"registration_notification\" , content = content ) \n    self . log . info ( \"engine::Engine Connected: %i\" , eid ) "}
{"14791": "\ndef get_results ( self , client_id , msg ) : \n    content = msg [ 'content' ] \n    msg_ids = sorted ( set ( content [ 'msg_ids' ] ) ) \n    statusonly = content . get ( 'status_only' , 0 ) \n    pending = [ ] \n    completed = [ ] \n    content = dict ( status = 'ok' ) \n    content [ 'pending' ] = pending \n    content [ 'completed' ] = completed \n    buffers = [ ] \n    if not statusonly : \n        try : \n            matches = self . db . find_records ( dict ( msg_id = { '$in' : msg_ids } ) ) \n            records = { } \n            for rec in matches : \n                records [ rec [ 'msg_id' ] ] = rec \n        except Exception : \n            content = error . wrap_exception ( ) \n            self . session . send ( self . query , \"result_reply\" , content = content , parent = msg , ident = client_id ) \n            return \n    else : \n        records = { } \n    for msg_id in msg_ids : \n        if msg_id in self . pending : \n            pending . append ( msg_id ) \n        elif msg_id in self . all_completed : \n            completed . append ( msg_id ) \n            if not statusonly : \n                c , bufs = self . _extract_record ( records [ msg_id ] ) \n                content [ msg_id ] = c \n                buffers . extend ( bufs ) \n        elif msg_id in records : \n            if rec [ 'completed' ] : \n                completed . append ( msg_id ) \n                c , bufs = self . _extract_record ( records [ msg_id ] ) \n                content [ msg_id ] = c \n                buffers . extend ( bufs ) \n            else : \n                pending . append ( msg_id ) \n        else : \n            try : \n                raise KeyError ( 'No such message: ' + msg_id ) \n            except : \n                content = error . wrap_exception ( ) \n            break \n    self . session . send ( self . query , \"result_reply\" , content = content , parent = msg , ident = client_id , buffers = buffers ) "}
{"14796": "\ndef run_command_under_r_root ( self , cmd , catched = 1 ) : \n    RPATH = self . path \n    with self . cd ( newdir = RPATH ) : \n        if catched : \n            process = sp . run ( cmd , stdout = sp . PIPE , stderr = sp . PIPE ) \n        else : \n            process = sp . run ( cmd ) \n        return process "}
{"14799": "\ndef _is_from_this_session ( self , msg ) : \n    session = self . _kernel_manager . session . session \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        return 1 \n    else : \n        return parent . get ( 'session' ) == session "}
{"14801": "\ndef annotate_file ( self , cu , analysis ) : \n    if not cu . relative : \n        return \n    filename = cu . filename \n    source = cu . source_file ( ) \n    if self . directory : \n        dest_file = os . path . join ( self . directory , cu . flat_rootname ( ) ) \n        dest_file += \".py,cover\" \n    else : \n        dest_file = filename + \",cover\" \n    dest = open ( dest_file , 'w' ) \n    statements = sorted ( analysis . statements ) \n    missing = sorted ( analysis . missing ) \n    excluded = sorted ( analysis . excluded ) \n    lineno = 0 \n    i = 0 \n    j = 0 \n    covered = 1 \n    while 1 : \n        line = source . readline ( ) \n        if line == '' : \n            break \n        lineno += 1 \n        while i < len ( statements ) and statements [ i ] < lineno : \n            i += 1 \n        while j < len ( missing ) and missing [ j ] < lineno : \n            j += 1 \n        if i < len ( statements ) and statements [ i ] == lineno : \n            covered = j >= len ( missing ) or missing [ j ] > lineno \n        if self . blank_re . match ( line ) : \n            dest . write ( '  ' ) \n        elif self . else_re . match ( line ) : \n            if i >= len ( statements ) and j >= len ( missing ) : \n                dest . write ( '! ' ) \n            elif i >= len ( statements ) or j >= len ( missing ) : \n                dest . write ( '> ' ) \n            elif statements [ i ] == missing [ j ] : \n                dest . write ( '! ' ) \n            else : \n                dest . write ( '> ' ) \n        elif lineno in excluded : \n            dest . write ( '- ' ) \n        elif covered : \n            dest . write ( '> ' ) \n        else : \n            dest . write ( '! ' ) \n        dest . write ( line ) \n    source . close ( ) \n    dest . close ( ) "}
{"14809": "\ndef send ( self , stream , msg_or_type , content = None , parent = None , ident = None , buffers = None , subheader = None , track = 0 , header = None ) : \n    if not isinstance ( stream , ( zmq . Socket , ZMQStream ) ) : \n        raise TypeError ( \"stream must be Socket or ZMQStream, not %r\" % type ( stream ) ) \n    elif track and isinstance ( stream , ZMQStream ) : \n        raise TypeError ( \"ZMQStream cannot track messages\" ) \n    if isinstance ( msg_or_type , ( Message , dict ) ) : \n        msg = msg_or_type \n    else : \n        msg = self . msg ( msg_or_type , content = content , parent = parent , subheader = subheader , header = header ) \n    buffers = [ ] if buffers is None else buffers \n    to_send = self . serialize ( msg , ident ) \n    flag = 0 \n    if buffers : \n        flag = zmq . SNDMORE \n        _track = 0 \n    else : \n        _track = track \n    if track : \n        tracker = stream . send_multipart ( to_send , flag , copy = 0 , track = _track ) \n    else : \n        tracker = stream . send_multipart ( to_send , flag , copy = 0 ) \n    for b in buffers [ : - 1 ] : \n        stream . send ( b , flag , copy = 0 ) \n    if buffers : \n        if track : \n            tracker = stream . send ( buffers [ - 1 ] , copy = 0 , track = track ) \n        else : \n            tracker = stream . send ( buffers [ - 1 ] , copy = 0 ) \n    if self . debug : \n        pprint . pprint ( msg ) \n        pprint . pprint ( to_send ) \n        pprint . pprint ( buffers ) \n    msg [ 'tracker' ] = tracker \n    return msg "}
{"14810": "\ndef send_raw ( self , stream , msg_list , flags = 0 , copy = 1 , ident = None ) : \n    to_send = [ ] \n    if isinstance ( ident , bytes ) : \n        ident = [ ident ] \n    if ident is not None : \n        to_send . extend ( ident ) \n    to_send . append ( DELIM ) \n    to_send . append ( self . sign ( msg_list ) ) \n    to_send . extend ( msg_list ) \n    stream . send_multipart ( msg_list , flags , copy = copy ) "}
{"14811": "\ndef recv ( self , socket , mode = zmq . NOBLOCK , content = 1 , copy = 1 ) : \n    if isinstance ( socket , ZMQStream ) : \n        socket = socket . socket \n    try : \n        msg_list = socket . recv_multipart ( mode , copy = copy ) \n    except zmq . ZMQError as e : \n        if e . errno == zmq . EAGAIN : \n            return None , None \n        else : \n            raise \n    idents , msg_list = self . feed_identities ( msg_list , copy ) \n    try : \n        return idents , self . unserialize ( msg_list , content = content , copy = copy ) \n    except Exception as e : \n        raise e "}
{"14812": "\ndef feed_identities ( self , msg_list , copy = 1 ) : \n    if copy : \n        idx = msg_list . index ( DELIM ) \n        return msg_list [ : idx ] , msg_list [ idx + 1 : ] \n    else : \n        failed = 1 \n        for idx , m in enumerate ( msg_list ) : \n            if m . bytes == DELIM : \n                failed = 0 \n                break \n        if failed : \n            raise ValueError ( \"DELIM not in msg_list\" ) \n        idents , msg_list = msg_list [ : idx ] , msg_list [ idx + 1 : ] \n        return [ m . bytes for m in idents ] , msg_list "}
{"14813": "\ndef unserialize ( self , msg_list , content = 1 , copy = 1 ) : \n    minlen = 4 \n    message = { } \n    if not copy : \n        for i in range ( minlen ) : \n            msg_list [ i ] = msg_list [ i ] . bytes \n    if self . auth is not None : \n        signature = msg_list [ 0 ] \n        if not signature : \n            raise ValueError ( \"Unsigned Message\" ) \n        if signature in self . digest_history : \n            raise ValueError ( \"Duplicate Signature: %r\" % signature ) \n        self . digest_history . add ( signature ) \n        check = self . sign ( msg_list [ 1 : 4 ] ) \n        if not signature == check : \n            raise ValueError ( \"Invalid Signature: %r\" % signature ) \n    if not len ( msg_list ) >= minlen : \n        raise TypeError ( \"malformed message, must have at least %i elements\" % minlen ) \n    header = self . unpack ( msg_list [ 1 ] ) \n    message [ 'header' ] = header \n    message [ 'msg_id' ] = header [ 'msg_id' ] \n    message [ 'msg_type' ] = header [ 'msg_type' ] \n    message [ 'parent_header' ] = self . unpack ( msg_list [ 2 ] ) \n    if content : \n        message [ 'content' ] = self . unpack ( msg_list [ 3 ] ) \n    else : \n        message [ 'content' ] = msg_list [ 3 ] \n    message [ 'buffers' ] = msg_list [ 4 : ] \n    return message "}
{"14819": "\ndef getsource ( obj , is_binary = 0 ) : \n    if is_binary : \n        return None \n    else : \n        if hasattr ( obj , \"__wrapped__\" ) : \n            obj = obj . __wrapped__ \n        try : \n            src = inspect . getsource ( obj ) \n        except TypeError : \n            if hasattr ( obj , '__class__' ) : \n                src = inspect . getsource ( obj . __class__ ) \n        return src "}
{"14821": "\ndef call_tip ( oinfo , format_call = 1 ) : \n    argspec = oinfo . get ( 'argspec' ) \n    if argspec is None : \n        call_line = None \n    else : \n        try : \n            has_self = argspec [ 'args' ] [ 0 ] == 'self' \n        except ( KeyError , IndexError ) : \n            pass \n        else : \n            if has_self : \n                argspec [ 'args' ] = argspec [ 'args' ] [ 1 : ] \n        call_line = oinfo [ 'name' ] + format_argspec ( argspec ) \n    doc = oinfo . get ( 'call_docstring' ) \n    if doc is None : \n        doc = oinfo . get ( 'init_docstring' ) \n    if doc is None : \n        doc = oinfo . get ( 'docstring' , '' ) \n    return call_line , doc "}
{"14833": "\ndef psearch ( self , pattern , ns_table , ns_search = [ ] , ignore_case = 0 , show_all = 0 ) : \n    type_pattern = 'all' \n    filter = '' \n    cmds = pattern . split ( ) \n    len_cmds = len ( cmds ) \n    if len_cmds == 1 : \n        filter = cmds [ 0 ] \n    elif len_cmds == 2 : \n        filter , type_pattern = cmds \n    else : \n        raise ValueError ( 'invalid argument string for psearch: <%s>' % pattern ) \n    for name in ns_search : \n        if name not in ns_table : \n            raise ValueError ( 'invalid namespace <%s>. Valid names: %s' % ( name , ns_table . keys ( ) ) ) \n    search_result , namespaces_seen = set ( ) , set ( ) \n    for ns_name in ns_search : \n        ns = ns_table [ ns_name ] \n        if id ( ns ) in namespaces_seen : \n            continue \n        namespaces_seen . add ( id ( ns ) ) \n        tmp_res = list_namespace ( ns , type_pattern , filter , ignore_case = ignore_case , show_all = show_all ) \n        search_result . update ( tmp_res ) \n    page . page ( '\\n' . join ( sorted ( search_result ) ) ) "}
{"14834": "\ndef threaded_reactor ( ) : \n    global _twisted_thread \n    try : \n        from twisted . internet import reactor \n    except ImportError : \n        return None , None \n    if not _twisted_thread : \n        from twisted . python import threadable \n        from threading import Thread \n        _twisted_thread = Thread ( target = lambda : reactor . run ( installSignalHandlers = 0 ) ) \n        _twisted_thread . setDaemon ( 1 ) \n        _twisted_thread . start ( ) \n    return reactor , _twisted_thread "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = 0 ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2 : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14837": "\ndef to_string ( self , indent = 1 , declaration = 1 ) : \n    return etree . tostring ( self . to_xml ( ) , encoding = self . encoding , xml_declaration = declaration , pretty_print = indent ) "}
{"14843": "\ndef format2 ( self , raw , out = None , scheme = '' ) : \n    string_output = 0 \n    if out == 'str' or self . out == 'str' or isinstance ( self . out , StringIO . StringIO ) : \n        out_old = self . out \n        self . out = StringIO . StringIO ( ) \n        string_output = 1 \n    elif out is not None : \n        self . out = out \n    if scheme == 'NoColor' : \n        error = 0 \n        self . out . write ( raw ) \n        if string_output : \n            return raw , error \n        else : \n            return None , error \n    colors = self . color_table [ scheme ] . colors \n    self . colors = colors \n    self . raw = raw . expandtabs ( ) . rstrip ( ) \n    self . lines = [ 0 , 0 ] \n    pos = 0 \n    raw_find = self . raw . find \n    lines_append = self . lines . append \n    while 1 : \n        pos = raw_find ( '\\n' , pos ) + 1 \n        if not pos : \n            break \n        lines_append ( pos ) \n    lines_append ( len ( self . raw ) ) \n    self . pos = 0 \n    text = StringIO . StringIO ( self . raw ) \n    error = 0 \n    try : \n        for atoken in generate_tokens ( text . readline ) : \n            self ( * atoken ) \n    except tokenize . TokenError as ex : \n        msg = ex . args [ 0 ] \n        line = ex . args [ 1 ] [ 0 ] \n        self . out . write ( \"%s\\n\\n*** ERROR: %s%s%s\\n\" % ( colors [ token . ERRORTOKEN ] , msg , self . raw [ self . lines [ line ] : ] , colors . normal ) ) \n        error = 1 \n    self . out . write ( colors . normal + '\\n' ) \n    if string_output : \n        output = self . out . getvalue ( ) \n        self . out = out_old \n        return ( output , error ) \n    return ( None , error ) "}
{"14846": "\ndef mpl_runner ( safe_execfile ) : \n    def mpl_execfile ( fname , * where , ** kw ) : \n        import matplotlib \n        import matplotlib . pylab as pylab \n        is_interactive = matplotlib . rcParams [ 'interactive' ] \n        matplotlib . interactive ( 0 ) \n        safe_execfile ( fname , * where , ** kw ) \n        matplotlib . interactive ( is_interactive ) \n        if pylab . draw_if_interactive . called : \n            pylab . draw ( ) \n            pylab . draw_if_interactive . called = 0 \n    return mpl_execfile "}
{"14849": "\ndef activate_matplotlib ( backend ) : \n    import matplotlib \n    if backend . startswith ( 'module://' ) : \n        matplotlib . rcParams [ 'backend' ] = backend \n    else : \n        matplotlib . use ( backend ) \n    matplotlib . interactive ( 1 ) \n    import matplotlib . pylab as pylab \n    pylab . show . _needmain = 0 \n    pylab . draw_if_interactive = flag_calls ( pylab . draw_if_interactive ) "}
{"14854": "\ndef stop ( self ) : \n    self . stopped = 1 \n    if self . thread != threading . currentThread ( ) : \n        return \n    if hasattr ( sys , \"gettrace\" ) and self . warn : \n        if sys . gettrace ( ) != self . _trace : \n            msg = \"Trace function changed, measurement is likely wrong: %r\" \n            self . warn ( msg % ( sys . gettrace ( ) , ) ) \n    sys . settrace ( None ) "}
{"14864": "\ndef process_startup ( ) : \n    cps = os . environ . get ( \"COVERAGE_PROCESS_START\" ) \n    if cps : \n        cov = coverage ( config_file = cps , auto_data = 1 ) \n        cov . start ( ) \n        cov . _warn_no_data = 0 \n        cov . _warn_unimported_source = 0 "}
{"14871": "\ndef start ( self ) : \n    if self . run_suffix : \n        self . data_suffix = self . run_suffix \n    if self . auto_data : \n        self . load ( ) \n    if self . source or self . source_pkgs : \n        self . source_match = TreeMatcher ( self . source ) \n    else : \n        if self . cover_dir : \n            self . cover_match = TreeMatcher ( [ self . cover_dir ] ) \n        if self . pylib_dirs : \n            self . pylib_match = TreeMatcher ( self . pylib_dirs ) \n    if self . include : \n        self . include_match = FnmatchMatcher ( self . include ) \n    if self . omit : \n        self . omit_match = FnmatchMatcher ( self . omit ) \n    if self . debug . should ( 'config' ) : \n        self . debug . write ( \"Configuration values:\" ) \n        config_info = sorted ( self . config . __dict__ . items ( ) ) \n        self . debug . write_formatted_info ( config_info ) \n    if self . debug . should ( 'sys' ) : \n        self . debug . write ( \"Debugging info:\" ) \n        self . debug . write_formatted_info ( self . sysinfo ( ) ) \n    self . collector . start ( ) \n    self . _started = 1 \n    self . _measured = 1 "}
{"14875": "\ndef save ( self ) : \n    data_suffix = self . data_suffix \n    if data_suffix is 1 : \n        extra = \"\" \n        if _TEST_NAME_FILE : \n            f = open ( _TEST_NAME_FILE ) \n            test_name = f . read ( ) \n            f . close ( ) \n            extra = \".\" + test_name \n        data_suffix = \"%s%s.%s.%06d\" % ( socket . gethostname ( ) , extra , os . getpid ( ) , random . randint ( 0 , 999999 ) ) \n    self . _harvest_data ( ) \n    self . data . write ( suffix = data_suffix ) "}
{"14877": "\ndef _harvest_data ( self ) : \n    if not self . _measured : \n        return \n    self . data . add_line_data ( self . collector . get_line_data ( ) ) \n    self . data . add_arc_data ( self . collector . get_arc_data ( ) ) \n    self . collector . reset ( ) \n    if self . _warn_unimported_source : \n        for pkg in self . source_pkgs : \n            self . _warn ( \"Module %s was never imported.\" % pkg ) \n    summary = self . data . summary ( ) \n    if not summary and self . _warn_no_data : \n        self . _warn ( \"No data was collected.\" ) \n    for src in self . source : \n        for py_file in find_python_files ( src ) : \n            py_file = self . file_locator . canonical_filename ( py_file ) \n            if self . omit_match and self . omit_match . match ( py_file ) : \n                continue \n            self . data . touch_file ( py_file ) \n    self . _measured = 0 "}
{"14881": "\ndef report ( self , morfs = None , show_missing = 1 , ignore_errors = None , file = None , omit = None , include = None ) : \n    self . _harvest_data ( ) \n    self . config . from_args ( ignore_errors = ignore_errors , omit = omit , include = include , show_missing = show_missing , ) \n    reporter = SummaryReporter ( self , self . config ) \n    return reporter . report ( morfs , outfile = file ) "}
{"14884": "\ndef xml_report ( self , morfs = None , outfile = None , ignore_errors = None , omit = None , include = None ) : \n    self . _harvest_data ( ) \n    self . config . from_args ( ignore_errors = ignore_errors , omit = omit , include = include , xml_output = outfile , ) \n    file_to_close = None \n    delete_file = 0 \n    if self . config . xml_output : \n        if self . config . xml_output == '-' : \n            outfile = sys . stdout \n        else : \n            outfile = open ( self . config . xml_output , \"w\" ) \n            file_to_close = outfile \n    try : \n        try : \n            reporter = XmlReporter ( self , self . config ) \n            return reporter . report ( morfs , outfile = outfile ) \n        except CoverageException : \n            delete_file = 1 \n            raise \n    finally : \n        if file_to_close : \n            file_to_close . close ( ) \n            if delete_file : \n                file_be_gone ( self . config . xml_output ) "}
{"14886": "\ndef display_html ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_html ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'text/html' ] ) "}
{"14887": "\ndef display_svg ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_svg ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'image/svg+xml' ] ) "}
{"14888": "\ndef display_png ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_png ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'image/png' ] ) "}
{"14889": "\ndef display_jpeg ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_jpeg ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'image/jpeg' ] ) "}
{"14890": "\ndef display_latex ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_latex ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'text/latex' ] ) "}
{"14891": "\ndef display_json ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_json ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'application/json' ] ) "}
{"14892": "\ndef display_javascript ( * objs , ** kwargs ) : \n    raw = kwargs . pop ( 'raw' , 0 ) \n    if raw : \n        for obj in objs : \n            publish_javascript ( obj ) \n    else : \n        display ( * objs , include = [ 'text/plain' , 'application/javascript' ] ) "}
{"14895": "\ndef system ( self , cmd ) : \n    enc = DEFAULT_ENCODING \n    patterns = [ pexpect . TIMEOUT , pexpect . EOF ] \n    EOF_index = patterns . index ( pexpect . EOF ) \n    out_size = 0 \n    try : \n        if hasattr ( pexpect , 'spawnb' ) : \n            child = pexpect . spawnb ( self . sh , args = [ '-c' , cmd ] ) \n        else : \n            child = pexpect . spawn ( self . sh , args = [ '-c' , cmd ] ) \n        flush = sys . stdout . flush \n        while 1 : \n            res_idx = child . expect_list ( patterns , self . read_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            flush ( ) \n            if res_idx == EOF_index : \n                break \n            out_size = len ( child . before ) \n    except KeyboardInterrupt : \n        child . sendline ( chr ( 3 ) ) \n        try : \n            out_size = len ( child . before ) \n            child . expect_list ( patterns , self . terminate_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            sys . stdout . flush ( ) \n        except KeyboardInterrupt : \n            pass \n        finally : \n            child . terminate ( force = 1 ) \n    child . isalive ( ) \n    return child . exitstatus "}
{"14899": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        self . log . critical ( 'Could not read pid file, cluster is probably not running.' ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    if not self . check_pid ( pid ) : \n        self . log . critical ( 'Cluster [pid=%r] is not running.' % pid ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    elif os . name == 'posix' : \n        sig = self . signal \n        self . log . info ( \"Stopping cluster [pid=%r] with [signal=%r]\" % ( pid , sig ) ) \n        try : \n            os . kill ( pid , sig ) \n        except OSError : \n            self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = 1 ) \n            self . remove_pid_file ( ) \n    elif os . name == 'nt' : \n        try : \n            p = check_call ( [ 'taskkill' , '-pid' , str ( pid ) , '-t' , '-f' ] , stdout = PIPE , stderr = PIPE ) \n        except ( CalledProcessError , OSError ) : \n            self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = 1 ) \n        self . remove_pid_file ( ) "}
{"14903": "\ndef get_app_wx ( * args , ** kwargs ) : \n    import wx \n    app = wx . GetApp ( ) \n    if app is None : \n        if not kwargs . has_key ( 'redirect' ) : \n            kwargs [ 'redirect' ] = 0 \n        app = wx . PySimpleApp ( * args , ** kwargs ) \n    return app "}
{"14905": "\ndef start_event_loop_wx ( app = None ) : \n    if app is None : \n        app = get_app_wx ( ) \n    if not is_event_loop_running_wx ( app ) : \n        app . _in_event_loop = 1 \n        app . MainLoop ( ) \n        app . _in_event_loop = 0 \n    else : \n        app . _in_event_loop = 1 "}
{"14907": "\ndef is_event_loop_running_qt4 ( app = None ) : \n    if app is None : \n        app = get_app_qt4 ( [ '' ] ) \n    if hasattr ( app , '_in_event_loop' ) : \n        return app . _in_event_loop \n    else : \n        return 0 "}
{"14908": "\ndef start_event_loop_qt4 ( app = None ) : \n    if app is None : \n        app = get_app_qt4 ( [ '' ] ) \n    if not is_event_loop_running_qt4 ( app ) : \n        app . _in_event_loop = 1 \n        app . exec_ ( ) \n        app . _in_event_loop = 0 \n    else : \n        app . _in_event_loop = 1 "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255 , 255 , 255 ) , size = 12 , antialias = 0 , center = 0 ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2 ) \n        y = y - ( height // 2 ) \n    for ystep in range ( height ) : \n        for xstep in range ( width ) : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if normalisation != 0 : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            else : \n                if normalisation > .5 : \n                    set_color ( x + xstep , y + ystep , color ) "}
{"14913": "\ndef from_grayscale ( im , channels_on = ( 1 , 1 , 1 ) ) : \n    xdim , ydim = im . shape \n    canvas = np . zeros ( ( xdim , ydim , 3 ) , dtype = np . uint8 ) \n    for i , include in enumerate ( channels_on ) : \n        if include : \n            canvas [ : , : , i ] = im \n    return canvas . view ( AnnotatedImage ) "}
