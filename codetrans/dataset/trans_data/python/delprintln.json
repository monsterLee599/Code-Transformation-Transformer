{"14": "\ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    elif 'info_only' in kwargs and kwargs [ 'info_only' ] : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n        else : \n            if 'index' not in kwargs : \n                self . p ( [ ] ) \n            else : \n                stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n                self . p_i ( stream_id ) \n    else : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n        else : \n            stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n        if 'index' not in kwargs : \n            self . p ( stream_id ) \n        else : \n            self . p_i ( stream_id ) \n        if stream_id in self . streams : \n            urls = self . streams [ stream_id ] [ 'src' ] \n            ext = self . streams [ stream_id ] [ 'container' ] \n            total_size = self . streams [ stream_id ] [ 'size' ] \n        else : \n            urls = self . dash_streams [ stream_id ] [ 'src' ] \n            ext = self . dash_streams [ stream_id ] [ 'container' ] \n            total_size = self . dash_streams [ stream_id ] [ 'size' ] \n        if not urls : \n            log . wtf ( '[Failed] Cannot extract video source.' ) \n        download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) \n        if not kwargs [ 'caption' ] : \n            pass \n            return \n        for lang in self . caption_tracks : \n            filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n            pass \n            srt = self . caption_tracks [ lang ] \n            with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( srt ) \n            pass "}
{"15": "\ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if sourceType == 'sina' : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'youku' : \n        youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n    elif sourceType == 'tudou' : \n        tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'qq' : \n        qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'letv' : \n        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'zhuzhan' : \n        url = 'http://www.acfun.cn/v/ac' + vid \n        yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n        seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n        for t in seq : \n            if yk_streams . get ( t ) : \n                preferred = yk_streams [ t ] \n                break \n        size = 0 \n        for url in preferred [ 0 ] : \n            _ , _ , seg_size = url_info ( url ) \n            size += seg_size \n        if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : \n            ext = 'flv' \n        else : \n            ext = 'mp4' \n        print_info ( site_info , title , ext , size ) \n        if not info_only : \n            download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) \n    else : \n        raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            pass \n            return \n        try : \n            title = get_filename ( title ) \n            pass \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"35": "\ndef task_state ( args ) : \n    dag = get_dag ( args ) \n    task = dag . get_task ( task_id = args . task_id ) \n    ti = TaskInstance ( task , args . execution_date ) \n    pass "}
{"63": "\ndef print_stats ( self ) : \n    session = settings . Session ( ) \n    TI = TaskInstance \n    tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) \n    successful_tis = [ x for x in tis if x . state == State . SUCCESS ] \n    ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] \n    ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) \n    pass \n    pass \n    for dag_id in DAG_IDS : \n        pass \n        pass \n    pass \n    if len ( tis ) > len ( successful_tis ) : \n        pass \n        pass \n    session . commit ( ) "}
{"218": "\ndef conditionally_trigger ( context , dag_run_obj ) : \n    c_p = context [ 'params' ] [ 'condition_param' ] \n    pass \n    if context [ 'params' ] [ 'condition_param' ] : \n        dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } \n        pp . pprint ( dag_run_obj . payload ) \n        return dag_run_obj "}
{"592": "\ndef download_url ( url , root , filename = None , md5 = None ) : \n    from six . moves import urllib \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = os . path . basename ( url ) \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        pass \n    else : \n        try : \n            pass \n            urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) \n        except OSError : \n            if url [ : 5 ] == 'https' : \n                url = url . replace ( 'https:' , 'http:' ) \n                pass \n                urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) "}
{"595": "\ndef download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) : \n    import requests \n    url = \"https://docs.google.com/uc?export=download\" \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = file_id \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        pass \n    else : \n        session = requests . Session ( ) \n        response = session . get ( url , params = { 'id' : file_id } , stream = True ) \n        token = _get_confirm_token ( response ) \n        if token : \n            params = { 'id' : file_id , 'confirm' : token } \n            response = session . get ( url , params = params , stream = True ) \n        _save_response_content ( response , fpath ) "}
{"601": "\ndef download ( self ) : \n    import tarfile \n    if self . _check_integrity ( ) : \n        pass \n        return \n    download_url ( self . url , self . root , self . filename , self . md5_checksum ) \n    with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : \n        tar . extractall ( path = self . root ) \n    with open ( os . path . join ( self . root , 'dataset' , 'SBU_captioned_photo_dataset_urls.txt' ) ) as fh : \n        for line in fh : \n            url = line . rstrip ( ) \n            try : \n                download_url ( url , os . path . join ( self . root , 'dataset' ) ) \n            except OSError : \n                pass "}
{"602": "\ndef download ( self ) : \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    for url in self . urls : \n        filename = url . rpartition ( '/' ) [ 2 ] \n        file_path = os . path . join ( self . raw_folder , filename ) \n        download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) \n        self . extract_gzip ( gzip_path = file_path , remove_finished = True ) \n    pass \n    training_set = ( read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) ) \n    test_set = ( read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) ) \n    with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : \n        torch . save ( training_set , f ) \n    with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : \n        torch . save ( test_set , f ) \n    pass "}
{"603": "\ndef download ( self ) : \n    import shutil \n    import zipfile \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    filename = self . url . rpartition ( '/' ) [ 2 ] \n    file_path = os . path . join ( self . raw_folder , filename ) \n    download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) \n    pass \n    with zipfile . ZipFile ( file_path ) as zip_f : \n        zip_f . extractall ( self . raw_folder ) \n    os . unlink ( file_path ) \n    gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) \n    for gzip_file in os . listdir ( gzip_folder ) : \n        if gzip_file . endswith ( '.gz' ) : \n            self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) \n    for split in self . splits : \n        pass \n        training_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) \n        test_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) \n        with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : \n            torch . save ( training_set , f ) \n        with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : \n            torch . save ( test_set , f ) \n    shutil . rmtree ( gzip_folder ) \n    pass "}
{"632": "\ndef visualize_decision ( features , labels , true_w_b , candidate_w_bs , fname ) : \n    fig = figure . Figure ( figsize = ( 6 , 6 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( 1 , 1 , 1 ) \n    ax . scatter ( features [ : , 0 ] , features [ : , 1 ] , c = np . float32 ( labels [ : , 0 ] ) , cmap = cm . get_cmap ( \"binary\" ) , edgecolors = \"k\" ) \n    def plot_weights ( w , b , ** kwargs ) : \n        w1 , w2 = w \n        x1s = np . linspace ( - 1 , 1 , 100 ) \n        x2s = - ( w1 * x1s + b ) / w2 \n        ax . plot ( x1s , x2s , ** kwargs ) \n    for w , b in candidate_w_bs : \n        plot_weights ( w , b , alpha = 1. / np . sqrt ( len ( candidate_w_bs ) ) , lw = 1 , color = \"blue\" ) \n    if true_w_b is not None : \n        plot_weights ( * true_w_b , lw = 4 , color = \"green\" , label = \"true separator\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . set_ylim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pass "}
{"653": "\ndef _ensure_tf_install ( ) : \n    try : \n        import tensorflow as tf \n    except ImportError : \n        pass \n        raise \n    import distutils . version \n    required_tensorflow_version = \"1.13\" \n    if ( distutils . version . LooseVersion ( tf . __version__ ) < distutils . version . LooseVersion ( required_tensorflow_version ) ) : \n        raise ImportError ( \"This version of TensorFlow Probability requires TensorFlow \" \"version >= {required}; Detected an installation of version {present}. \" \"Please upgrade TensorFlow to proceed.\" . format ( required = required_tensorflow_version , present = tf . __version__ ) ) "}
{"819": "\ndef download ( directory , filename ) : \n    filepath = os . path . join ( directory , filename ) \n    if tf . io . gfile . exists ( filepath ) : \n        return filepath \n    if not tf . io . gfile . exists ( directory ) : \n        tf . io . gfile . makedirs ( directory ) \n    url = os . path . join ( ROOT_PATH , filename ) \n    pass \n    urllib . request . urlretrieve ( url , filepath ) \n    return filepath "}
{"846": "\ndef save_imgs ( x , fname ) : \n    n = x . shape [ 0 ] \n    fig = figure . Figure ( figsize = ( n , 1 ) , frameon = False ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( 1 , n , i + 1 ) \n        ax . imshow ( x [ i ] . squeeze ( ) , interpolation = \"none\" , cmap = cm . get_cmap ( \"binary\" ) ) \n        ax . axis ( \"off\" ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pass "}
{"870": "\ndef plot_weight_posteriors ( names , qm_vals , qs_vals , fname ) : \n    fig = figure . Figure ( figsize = ( 6 , 3 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( 1 , 2 , 1 ) \n    for n , qm in zip ( names , qm_vals ) : \n        sns . distplot ( qm . flatten ( ) , ax = ax , label = n ) \n    ax . set_title ( \"weight means\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    ax = fig . add_subplot ( 1 , 2 , 2 ) \n    for n , qs in zip ( names , qs_vals ) : \n        sns . distplot ( qs . flatten ( ) , ax = ax ) \n    ax . set_title ( \"weight stddevs\" ) \n    ax . set_xlim ( [ 0 , 1. ] ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pass "}
{"871": "\ndef plot_heldout_prediction ( input_vals , probs , fname , n = 10 , title = \"\" ) : \n    fig = figure . Figure ( figsize = ( 9 , 3 * n ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( n , 3 , 3 * i + 1 ) \n        ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - 1 ] ) , interpolation = \"None\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 2 ) \n        for prob_sample in probs : \n            sns . barplot ( np . arange ( 10 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) \n            ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"posterior samples\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 3 ) \n        sns . barplot ( np . arange ( 10 ) , np . mean ( probs [ : , i , : ] , axis = 0 ) , ax = ax ) \n        ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"predictive probs\" ) \n    fig . suptitle ( title ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pass "}
{"1019": "\ndef load_nips2011_papers ( path ) : \n    path = os . path . expanduser ( path ) \n    filename = \"NIPS_1987-2015.csv\" \n    filepath = os . path . join ( path , filename ) \n    if not os . path . exists ( filepath ) : \n        url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) \n        if not tf . io . gfile . exists ( path ) : \n            tf . io . gfile . makedirs ( path ) \n        pass \n        urllib . request . urlretrieve ( url , filepath ) \n    with open ( filepath ) as f : \n        iterator = csv . reader ( f ) \n        documents = next ( iterator ) [ 1 : ] \n        words = [ ] \n        x_train = [ ] \n        for row in iterator : \n            words . append ( row [ 0 ] ) \n            x_train . append ( row [ 1 : ] ) \n    x_train = np . array ( x_train , dtype = np . int ) \n    doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] \n    documents = [ documents [ doc ] for doc in doc_idx ] \n    x_train = x_train [ : , doc_idx ] \n    word_idx = np . logical_and ( np . sum ( x_train != 0 , 1 ) >= 2 , np . sum ( x_train , 1 ) >= 10 ) \n    words = [ word for word , idx in zip ( words , word_idx ) if idx ] \n    bag_of_words = x_train [ word_idx , : ] . T \n    return bag_of_words , words "}
{"1082": "\ndef compute_true_volumes ( det_bounds , dim , num_samples , error_rate = 1e-6 , seed = 42 ) : \n    bounds = { } \n    with tf . compat . v1 . Session ( ) as sess : \n        rej_weights , _ = correlation_matrix_volume_rejection_samples ( det_bounds , dim , [ num_samples , len ( det_bounds ) ] , np . float32 , seed = seed ) \n        rej_weights = sess . run ( rej_weights ) \n        for rw , det in zip ( np . rollaxis ( rej_weights , 1 ) , det_bounds ) : \n            template = ( \"Estimating volume of {}x{} correlation \" \"matrices with determinant >= {}.\" ) \n            pass \n            sys . stdout . flush ( ) \n            bounds [ det ] = _clopper_pearson_confidence_interval ( rw , error_rate = error_rate ) \n        return bounds "}
{"1151": "\ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = 0 \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id += 1 \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if sys . version_info < ( 3 , ) : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    pass \n    return texts "}
{"1182": "\ndef get_weights ( self ) : \n    tensorWeights = callBigDlFunc ( self . bigdl_type , \"getWeights\" , self . value ) \n    if tensorWeights is not None : \n        return [ tensor . to_ndarray ( ) for tensor in tensorWeights ] \n    else : \n        pass \n        return None "}
{"1194": "\ndef read_data_sets ( data_dir ) : \n    WHOLE_DATA = 'ml-1m.zip' \n    local_file = base . maybe_download ( WHOLE_DATA , data_dir , SOURCE_URL + WHOLE_DATA ) \n    zip_ref = zipfile . ZipFile ( local_file , 'r' ) \n    extracted_to = os . path . join ( data_dir , \"ml-1m\" ) \n    if not os . path . exists ( extracted_to ) : \n        pass \n        zip_ref . extractall ( data_dir ) \n        zip_ref . close ( ) \n    rating_files = os . path . join ( extracted_to , \"ratings.dat\" ) \n    rating_list = [ i . strip ( ) . split ( \"::\" ) for i in open ( rating_files , \"r\" ) . readlines ( ) ] \n    movielens_data = np . array ( rating_list ) . astype ( int ) \n    return movielens_data "}
{"1249": "\ndef main ( ) : \n    args = parser . parse_args ( ) \n    path = get_settings_path ( ) \n    if args . default : \n        if populate_settings_dir ( force = True ) : \n            pass \n        else : \n            pass \n    else : \n        pass "}
{"1275": "\ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while True : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if len ( model . out_params ) > 1 : \n            pred = zip ( * pred ) \n        pass "}
{"1276": "\ndef predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : \n    if file_path is None or file_path == '-' : \n        if sys . stdin . isatty ( ) : \n            raise RuntimeError ( 'To process data from terminal please use interact mode' ) \n        f = sys . stdin \n    else : \n        f = open ( file_path , encoding = 'utf8' ) \n    model : Chainer = build_model ( config ) \n    args_count = len ( model . in_x ) \n    while True : \n        batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) \n        if not batch : \n            break \n        args = [ ] \n        for i in range ( args_count ) : \n            args . append ( batch [ i : : args_count ] ) \n        res = model ( * args ) \n        if len ( model . out_params ) == 1 : \n            res = [ res ] \n        for res in zip ( * res ) : \n            res = json . dumps ( res , ensure_ascii = False ) \n            pass \n    if f is not sys . stdin : \n        f . close ( ) "}
{"1349": "\ndef signal_handler ( signum , stackframe ) : \n    global g_runner \n    global g_handling_signal \n    if g_handling_signal : \n        return \n    g_handling_signal = True \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    g_runner . terminate ( ) "}
{"1350": "\ndef wipe_output_dir ( ) : \n    pass \n    try : \n        if os . path . exists ( g_output_dir ) : \n            shutil . rmtree ( str ( g_output_dir ) ) \n    except OSError as e : \n        pass \n        pass \n        pass \n        sys . exit ( 1 ) "}
{"1351": "\ndef remove_sandbox ( parent_dir , dir_name ) : \n    if \"Rsandbox\" in dir_name : \n        rsandbox_dir = os . path . join ( parent_dir , dir_name ) \n        try : \n            if sys . platform == \"win32\" : \n                os . system ( r'C:/cygwin64/bin/rm.exe -r -f \"{0}\"' . format ( rsandbox_dir ) ) \n            else : \n                shutil . rmtree ( rsandbox_dir ) \n        except OSError as e : \n            pass \n            pass \n            pass \n            pass \n            sys . exit ( 1 ) "}
{"1352": "\ndef scrape_port_from_stdout ( self ) : \n    regex = re . compile ( r\"Open H2O Flow in your web browser: https?://([^:]+):(\\d+)\" ) \n    retries_left = 30 \n    while retries_left and not self . terminated : \n        with open ( self . output_file_name , \"r\" ) as f : \n            for line in f : \n                mm = re . search ( regex , line ) \n                if mm is not None : \n                    self . port = mm . group ( 2 ) \n                    pass \n                    return \n        if self . terminated : \n            break \n        retries_left -= 1 \n        time . sleep ( 1 ) \n    if self . terminated : \n        return \n    pass \n    sys . exit ( 1 ) "}
{"1353": "\ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60 \n    while retries > 0 : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while len ( s ) > 0 : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( 1 ) \n                if size is not None : \n                    size = int ( size ) \n                    if size == nodes_per_cloud : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries -= 1 \n        if self . terminated : \n            return \n        time . sleep ( 1 ) \n    pass \n    pass \n    pass \n    sys . exit ( 1 ) "}
{"1354": "\ndef stop ( self ) : \n    if self . pid > 0 : \n        pass \n        try : \n            self . child . terminate ( ) \n            self . child . wait ( ) \n        except OSError : \n            pass \n        self . pid = - 1 "}
{"1386": "\ndef set_encoding ( self , encoding ) : \n    self . _bar_ends = \"[]\" \n    self . _bar_symbols = \"#\" \n    if not encoding : \n        return \n    s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" \n    s2 = \"\\u258C\\u2588\" \n    s3 = \"\\u2588\" \n    if self . _file_mode : \n        s1 = s2 = None \n    assert len ( s3 ) == 1 \n    for s in ( s1 , s2 , s3 ) : \n        if s is None : \n            continue \n        try : \n            s . encode ( encoding ) \n            self . _bar_ends = \"||\" \n            self . _bar_symbols = s \n            return \n        except UnicodeEncodeError : \n            pass \n        except LookupError : \n            pass "}
{"1392": "\ndef summary ( self , return_data = False ) : \n    if not self . _has_content ( ) : \n        pass \n        return self . _ex . _cache . _data ; \n    if not self . _ex . _cache . is_valid ( ) : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    if not return_data : \n        if self . nrows == 0 : \n            pass \n        elif H2ODisplay . _in_ipy ( ) : \n            import IPython . display \n            IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , True ) , raw = True ) \n        else : \n            pass \n    else : \n        return self . _ex . _cache . _data "}
{"1393": "\ndef describe ( self , chunk_summary = False ) : \n    if self . _has_content ( ) : \n        res = h2o . api ( \"GET /3/Frames/%s\" % self . frame_id , data = { \"row_count\" : 10 } ) [ \"frames\" ] [ 0 ] \n        self . _ex . _cache . _fill_data ( res ) \n        pass \n        pass \n        if chunk_summary : \n            res [ \"chunk_summary\" ] . show ( ) \n            res [ \"distribution_summary\" ] . show ( ) \n        pass \n    self . summary ( ) "}
{"1406": "\ndef structure ( self ) : \n    df = self . as_data_frame ( use_pandas = False ) \n    cn = df . pop ( 0 ) \n    nr = self . nrow \n    nc = self . ncol \n    width = max ( [ len ( c ) for c in cn ] ) \n    isfactor = self . isfactor ( ) \n    numlevels = self . nlevels ( ) \n    lvls = self . levels ( ) \n    pass \n    for i in range ( nc ) : \n        pass \n        if isfactor [ i ] : \n            nl = numlevels [ i ] \n            pass \n        else : \n            pass "}
{"1431": "\ndef hist ( self , breaks = \"sturges\" , plot = True , ** kwargs ) : \n    server = kwargs . pop ( \"server\" ) if \"server\" in kwargs else False \n    assert_is_type ( breaks , int , [ numeric ] , Enum ( \"sturges\" , \"rice\" , \"sqrt\" , \"doane\" , \"fd\" , \"scott\" ) ) \n    assert_is_type ( plot , bool ) \n    assert_is_type ( server , bool ) \n    if kwargs : \n        raise H2OValueError ( \"Unknown parameters to hist(): %r\" % kwargs ) \n    hist = H2OFrame . _expr ( expr = ExprNode ( \"hist\" , self , breaks ) ) . _frame ( ) \n    if plot : \n        try : \n            import matplotlib \n            if server : \n                matplotlib . use ( \"Agg\" , warn = False ) \n            import matplotlib . pyplot as plt \n        except ImportError : \n            pass \n            return \n        hist [ \"widths\" ] = hist [ \"breaks\" ] . difflag1 ( ) \n        lefts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"breaks\" ] , use_pandas = False ) [ 2 : ] ] \n        widths = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"widths\" ] , use_pandas = False ) [ 2 : ] ] \n        counts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"counts\" ] , use_pandas = False ) [ 2 : ] ] \n        plt . xlabel ( self . names [ 0 ] ) \n        plt . ylabel ( \"Frequency\" ) \n        plt . title ( \"Histogram of %s\" % self . names [ 0 ] ) \n        plt . bar ( left = lefts , width = widths , height = counts , bottom = 0 ) \n        if not server : \n            plt . show ( ) \n    else : \n        hist [ \"density\" ] = hist [ \"counts\" ] / ( hist [ \"breaks\" ] . difflag1 ( ) * hist [ \"counts\" ] . sum ( ) ) \n        return hist "}
{"1454": "\ndef version_check ( ) : \n    from . __init__ import __version__ as ver_pkg \n    ci = h2oconn . cluster \n    if not ci : \n        raise H2OConnectionError ( \"Connection not initialized. Did you run h2o.connect()?\" ) \n    ver_h2o = ci . version \n    if ver_pkg == \"SUBST_PROJECT_VERSION\" : \n        ver_pkg = \"UNKNOWN\" \n    if str ( ver_h2o ) != str ( ver_pkg ) : \n        branch_name_h2o = ci . branch_name \n        build_number_h2o = ci . build_number \n        if build_number_h2o is None or build_number_h2o == \"unknown\" : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        elif build_number_h2o == \"99999\" : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        else : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) \n    if ci . build_too_old : \n        pass "}
{"1466": "\ndef download_pojo ( model , path = \"\" , get_jar = True , jar_name = \"\" ) : \n    assert_is_type ( model , ModelBase ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if not model . have_pojo : \n        raise H2OValueError ( \"Export to POJO not supported\" ) \n    if path == \"\" : \n        java_code = api ( \"GET /3/Models.java/%s\" % model . model_id ) \n        pass \n        return None \n    else : \n        filename = api ( \"GET /3/Models.java/%s\" % model . model_id , save_to = path ) \n        if get_jar : \n            if jar_name == \"\" : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) \n            else : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , jar_name ) ) \n        return filename "}
{"1468": "\ndef download_all_logs ( dirname = \".\" , filename = None ) : \n    assert_is_type ( dirname , str ) \n    assert_is_type ( filename , str , None ) \n    url = \"%s/3/Logs/download\" % h2oconn . base_url \n    opener = urlopen ( ) \n    response = opener ( url ) \n    if not os . path . exists ( dirname ) : \n        os . mkdir ( dirname ) \n    if filename is None : \n        if PY3 : \n            headers = [ h [ 1 ] for h in response . headers . _headers ] \n        else : \n            headers = response . headers . headers \n        for h in headers : \n            if \"filename=\" in h : \n                filename = h . split ( \"filename=\" ) [ 1 ] . strip ( ) \n                break \n    path = os . path . join ( dirname , filename ) \n    response = opener ( url ) . read ( ) \n    pass \n    with open ( path , \"wb\" ) as f : \n        f . write ( response ) \n    return path "}
{"1471": "\ndef demo ( funcname , interactive = True , echo = True , test = False ) : \n    import h2o . demos as h2odemo \n    assert_is_type ( funcname , str ) \n    assert_is_type ( interactive , bool ) \n    assert_is_type ( echo , bool ) \n    assert_is_type ( test , bool ) \n    demo_function = getattr ( h2odemo , funcname , None ) \n    if demo_function and type ( demo_function ) is type ( demo ) : \n        demo_function ( interactive , echo , test ) \n    else : \n        pass "}
{"1481": "\ndef mojo_predict_csv ( input_csv_path , mojo_zip_path , output_csv_path = None , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : \n    default_java_options = '-Xmx4g -XX:ReservedCodeCacheSize=256m' \n    prediction_output_file = 'prediction.csv' \n    java = H2OLocalServer . _find_java ( ) \n    H2OLocalServer . _check_java ( java = java , verbose = verbose ) \n    if verbose : \n        pass \n    if not os . path . isfile ( input_csv_path ) : \n        raise RuntimeError ( \"Input csv cannot be found at %s\" % input_csv_path ) \n    mojo_zip_path = os . path . abspath ( mojo_zip_path ) \n    if verbose : \n        pass \n    if not os . path . isfile ( mojo_zip_path ) : \n        raise RuntimeError ( \"MOJO zip cannot be found at %s\" % mojo_zip_path ) \n    parent_dir = os . path . dirname ( mojo_zip_path ) \n    if output_csv_path is None : \n        output_csv_path = os . path . join ( parent_dir , prediction_output_file ) \n    if genmodel_jar_path is None : \n        genmodel_jar_path = os . path . join ( parent_dir , gen_model_file_name ) \n    if verbose : \n        pass \n    if not os . path . isfile ( genmodel_jar_path ) : \n        raise RuntimeError ( \"Genmodel jar cannot be found at %s\" % genmodel_jar_path ) \n    if verbose and output_csv_path is not None : \n        pass \n    if classpath is None : \n        classpath = genmodel_jar_path \n    if verbose : \n        pass \n    if java_options is None : \n        java_options = default_java_options \n    if verbose : \n        pass \n    cmd = [ java ] \n    for option in java_options . split ( ' ' ) : \n        cmd += [ option ] \n    cmd += [ \"-cp\" , classpath , h2o_predictor_class , \"--mojo\" , mojo_zip_path , \"--input\" , input_csv_path , '--output' , output_csv_path , '--decimal' ] \n    if verbose : \n        cmd_str = \" \" . join ( cmd ) \n        pass \n    subprocess . check_call ( cmd , shell = False ) \n    with open ( output_csv_path ) as csv_file : \n        result = list ( csv . DictReader ( csv_file ) ) \n    return result "}
{"1482": "\ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack \n            pass \n            pass \n            pass \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = True \n        return decorator_invisible \n    return deprecated_decorator "}
{"1485": "\ndef summary ( self , header = True ) : \n    table = [ ] \n    for model in self . models : \n        model_summary = model . _model_json [ \"output\" ] [ \"model_summary\" ] \n        r_values = list ( model_summary . cell_values [ 0 ] ) \n        r_values [ 0 ] = model . model_id \n        table . append ( r_values ) \n    pass \n    if header : \n        pass \n    pass \n    H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ 1 : ] , numalign = \"left\" , stralign = \"left\" ) "}
{"1486": "\ndef show ( self ) : \n    hyper_combos = itertools . product ( * list ( self . hyper_params . values ( ) ) ) \n    if not self . models : \n        c_values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper_combos ) ] \n        pass \n    else : \n        pass "}
{"1487": "\ndef get_hyperparams ( self , id , display = True ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ 0 ] ) \n    res = [ model . params [ h ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ h ] [ 'actual' ] , list ) else model . params [ h ] [ 'actual' ] for h in self . hyper_params ] \n    if display : \n        pass \n    return res "}
{"1488": "\ndef get_hyperparams_dict ( self , id , display = True ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    model_params = dict ( ) \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ 0 ] ) \n    for param_name in self . hyper_names : \n        model_params [ param_name ] = model . params [ param_name ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ param_name ] [ 'actual' ] , list ) else model . params [ param_name ] [ 'actual' ] \n    if display : \n        pass \n    return model_params "}
{"1491": "\ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if \"importance\" in list ( model . keys ( ) ) and model [ \"importance\" ] : \n        vals = model [ \"importance\" ] . cell_values \n        header = model [ \"importance\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        pass "}
{"1493": "\ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        pass \n        return \n    variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) \n    if type == \"barplot\" : \n        plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) \n    elif type == \"lines\" : \n        plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1496": "\ndef extractRunInto ( javaLogText ) : \n    global g_initialXY \n    global g_reguarlize_Y \n    global g_regularize_X_objective \n    global g_updateX \n    global g_updateY \n    global g_objective \n    global g_stepsize \n    global g_history \n    if os . path . isfile ( javaLogText ) : \n        run_result = dict ( ) \n        run_result [ \"total time (ms)\" ] = [ ] \n        run_result [ \"initialXY (ms)\" ] = [ ] \n        run_result [ \"regularize Y (ms)\" ] = [ ] \n        run_result [ \"regularize X and objective (ms)\" ] = [ ] \n        run_result [ \"update X (ms)\" ] = [ ] \n        run_result [ \"update Y (ms)\" ] = [ ] \n        run_result [ \"objective (ms)\" ] = [ ] \n        run_result [ \"step size (ms)\" ] = [ ] \n        run_result [ \"update history (ms)\" ] = [ ] \n        total_run_time = - 1 \n        val = 0.0 \n        with open ( javaLogText , 'r' ) as thefile : \n            for each_line in thefile : \n                temp_string = each_line . split ( ) \n                if len ( temp_string ) > 0 : \n                    val = temp_string [ - 1 ] . replace ( '\\\\' , '' ) \n                if g_initialXY in each_line : \n                    if total_run_time > 0 : \n                        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n                        total_run_time = 0.0 \n                    else : \n                        total_run_time = 0.0 \n                    run_result [ \"initialXY (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_reguarlize_Y in each_line : \n                    run_result [ \"regularize Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_regularize_X_objective in each_line : \n                    run_result [ \"regularize X and objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateX in each_line : \n                    run_result [ \"update X (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateY in each_line : \n                    run_result [ \"update Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_objective in each_line : \n                    run_result [ \"objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_stepsize in each_line : \n                    run_result [ \"step size (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_history in each_line : \n                    run_result [ \"update history (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n        pass \n    else : \n        pass "}
{"1497": "\ndef main ( argv ) : \n    global g_test_root_dir \n    global g_temp_filename \n    if len ( argv ) < 2 : \n        pass \n        sys . exit ( 1 ) \n    else : \n        javaLogText = argv [ 1 ] \n        pass \n        extractRunInto ( javaLogText ) "}
{"1530": "\ndef to_pojo ( self , pojo_name = \"\" , path = \"\" , get_jar = True ) : \n    assert_is_type ( pojo_name , str ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if pojo_name == \"\" : \n        pojo_name = \"AssemblyPOJO_\" + str ( uuid . uuid4 ( ) ) \n    java = h2o . api ( \"GET /99/Assembly.java/%s/%s\" % ( self . id , pojo_name ) ) \n    file_path = path + \"/\" + pojo_name + \".java\" \n    if path == \"\" : \n        pass \n    else : \n        with open ( file_path , 'w' , encoding = \"utf-8\" ) as f : \n            f . write ( java ) \n    if get_jar and path != \"\" : \n        h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) "}
{"1536": "\ndef scoring_history ( self ) : \n    model = self . _model_json [ \"output\" ] \n    if \"scoring_history\" in model and model [ \"scoring_history\" ] is not None : \n        return model [ \"scoring_history\" ] . as_data_frame ( ) \n    pass "}
{"1537": "\ndef show ( self ) : \n    if self . _future : \n        self . _job . poll_once ( ) \n        return \n    if self . _model_json is None : \n        pass \n        return \n    if self . model_id is None : \n        pass \n        return \n    model = self . _model_json [ \"output\" ] \n    pass \n    pass \n    pass \n    pass \n    self . summary ( ) \n    pass \n    tm = model [ \"training_metrics\" ] \n    if tm : \n        tm . show ( ) \n    vm = model [ \"validation_metrics\" ] \n    if vm : \n        vm . show ( ) \n    xm = model [ \"cross_validation_metrics\" ] \n    if xm : \n        xm . show ( ) \n    xms = model [ \"cross_validation_metrics_summary\" ] \n    if xms : \n        xms . show ( ) \n    if \"scoring_history\" in model and model [ \"scoring_history\" ] : \n        model [ \"scoring_history\" ] . show ( ) \n    if \"variable_importances\" in model and model [ \"variable_importances\" ] : \n        model [ \"variable_importances\" ] . show ( ) "}
{"1538": "\ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if self . algo == 'glm' or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : \n        if self . algo == 'glm' : \n            tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values \n            maxVal = 0 \n            sum = 0 \n            for item in tempvals : \n                sum = sum + item [ 1 ] \n                if item [ 1 ] > maxVal : \n                    maxVal = item [ 1 ] \n            vals = [ ] \n            for item in tempvals : \n                tempT = ( item [ 0 ] , item [ 1 ] , item [ 1 ] / maxVal , item [ 1 ] / sum ) \n                vals . append ( tempT ) \n            header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] \n        else : \n            vals = model [ \"variable_importances\" ] . cell_values \n            header = model [ \"variable_importances\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        pass "}
{"1551": "\ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        pass \n        if self . _table_description : \n            pass \n    pass \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if nr > 20 : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if nr > 20 and can_use_pandas ( ) : \n        pass "}
{"1552": "\ndef start ( jar_path = None , nthreads = - 1 , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and min_mem_size > max_mem_size : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( port [ - 1 ] == \"+\" and port [ : - 1 ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - 1 ] ) \n            port = 0 \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        pass \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        pass \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1570": "\ndef print_dict ( ) : \n    global g_ok_java_messages \n    global g_java_messages_to_ignore_text_filename \n    allKeys = sorted ( g_ok_java_messages . keys ( ) ) \n    with open ( g_java_messages_to_ignore_text_filename , 'w' ) as ofile : \n        for key in allKeys : \n            for mess in g_ok_java_messages [ key ] : \n                ofile . write ( 'KeyName: ' + key + '\\n' ) \n                ofile . write ( 'IgnoredMessage: ' + mess + '\\n' ) \n            pass \n            pass \n            pass "}
{"1572": "\ndef usage ( ) : \n    global g_script_name \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    sys . exit ( 1 ) "}
{"1575": "\ndef main ( ) : \n    for filename in locate_files ( ROOT_DIR ) : \n        pass \n        with open ( filename , \"rt\" ) as f : \n            tokens = list ( tokenize . generate_tokens ( f . readline ) ) \n            text1 = tokenize . untokenize ( tokens ) \n            ntokens = normalize_tokens ( tokens ) \n            text2 = tokenize . untokenize ( ntokens ) \n            assert text1 == text2 "}
{"1578": "\ndef extractPrintSaveIntermittens ( ) : \n    global g_summary_dict_intermittents \n    localtz = time . tzname [ 0 ] \n    for ind in range ( len ( g_summary_dict_all [ \"TestName\" ] ) ) : \n        if g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] >= g_threshold_failure : \n            addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) \n    if len ( g_summary_dict_intermittents [ \"TestName\" ] ) > 0 : \n        json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) \n        with open ( g_summary_csv_filename , 'w' ) as summaryFile : \n            for ind in range ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) ) : \n                testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] \n                numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] \n                firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ 0 ] ) \n                summaryFile . write ( eachTest ) \n                pass "}
{"1579": "\ndef plot ( self , type = \"roc\" , server = False ) : \n    assert_is_type ( type , \"roc\" ) \n    try : \n        imp . find_module ( 'matplotlib' ) \n        import matplotlib \n        if server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        pass \n        return \n    if type == \"roc\" : \n        plt . xlabel ( 'False Positive Rate (FPR)' ) \n        plt . ylabel ( 'True Positive Rate (TPR)' ) \n        plt . title ( 'ROC Curve' ) \n        plt . text ( 0.5 , 0.5 , r'AUC={0:.4f}' . format ( self . _metric_json [ \"AUC\" ] ) ) \n        plt . plot ( self . fprs , self . tprs , 'b--' ) \n        plt . axis ( [ 0 , 1 , 0 , 1 ] ) \n        if not server : \n            plt . show ( ) "}
{"1581": "\ndef available ( ) : \n    builder_json = h2o . api ( \"GET /3/ModelBuilders\" , data = { \"algo\" : \"deepwater\" } ) \n    visibility = builder_json [ \"model_builders\" ] [ \"deepwater\" ] [ \"visibility\" ] \n    if visibility == \"Experimental\" : \n        pass \n        return False \n    else : \n        return True "}
{"1587": "\ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while True : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if login == username . strip ( ) : \n                    return login , password \n        pass \n        for ind , ( login , password ) in enumerate ( lines ) : \n            pass \n        pass \n        pass \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if ind == 0 : \n                add_credentials ( ) \n                continue \n            elif ind == - 1 : \n                delete_credentials ( ) \n                check_secret ( ) \n                continue \n            elif 0 <= ind - 1 < len ( lines ) : \n                return lines [ ind - 1 ] \n        except Exception : \n            pass "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if sys . version_info [ 0 ] < 3 : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        pass \n        return [ ] "}
{"1601": "\ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            pass \n            try : \n                pass \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        elif resp . length > 0 : \n            respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            pass \n            try : \n                pass \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if status == 307 : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if status >= 300 : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1739": "\ndef travis_build_package ( ) : \n    travis_tag = os . environ . get ( 'TRAVIS_TAG' ) \n    if not travis_tag : \n        pass \n        return \"TRAVIS_TAG environment variable is not present\" \n    try : \n        name , version = travis_tag . split ( \"_\" ) \n    except ValueError : \n        pass \n        return \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\" . format ( travis_tag ) \n    try : \n        version = Version ( version ) \n    except InvalidVersion : \n        pass \n        return \"Version must be a valid PEP440 version (version is: {})\" . format ( version ) \n    if name . lower ( ) in OMITTED_RELEASE_PACKAGES : \n        pass \n        return \n    abs_dist_path = Path ( os . environ [ 'TRAVIS_BUILD_DIR' ] , 'dist' ) \n    create_package ( name , str ( abs_dist_path ) ) \n    pass \n    pattern = \"*{}*\" . format ( version ) \n    packages = list ( abs_dist_path . glob ( pattern ) ) \n    if not packages : \n        return \"Package version does not match tag {}, abort\" . format ( version ) \n    pypi_server = os . environ . get ( \"PYPI_SERVER\" , \"default PyPI server\" ) \n    pass "}
{"1980": "\ndef showCode ( self , width = 80 ) : \n    symbolStrings = [ ( self . bitPattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] \n    leftColWidth , rightColWidth = map ( max , map ( map , repeat ( len ) , zip ( * symbolStrings ) ) ) \n    colwidth = leftColWidth + rightColWidth \n    columns = 81 // ( colwidth + 2 ) \n    rows = - ( - len ( symbolStrings ) // columns ) \n    def justify ( bs ) : \n        b , s = bs \n        return b . rjust ( leftColWidth ) + ':' + s . ljust ( rightColWidth ) \n    for i in range ( rows ) : \n        pass "}
{"1991": "\ndef processStream ( self ) : \n    pass \n    pass \n    self . windowSize = self . verboseRead ( WindowSizeAlphabet ( ) ) \n    pass \n    self . ISLAST = False \n    self . output = bytearray ( ) \n    while not self . ISLAST : \n        self . ISLAST = self . verboseRead ( BoolCode ( 'LAST' , description = \"Last block\" ) ) \n        if self . ISLAST : \n            if self . verboseRead ( BoolCode ( 'EMPTY' , description = \"Empty block\" ) ) : \n                break \n        if self . metablockLength ( ) : \n            continue \n        if not self . ISLAST and self . uncompressed ( ) : \n            continue \n        pass \n        self . numberOfBlockTypes = { } \n        self . currentBlockCounts = { } \n        self . blockTypeCodes = { } \n        self . blockCountCodes = { } \n        for blockType in ( L , I , D ) : \n            self . blockType ( blockType ) \n        pass \n        self . NPOSTFIX , self . NDIRECT = self . verboseRead ( DistanceParamAlphabet ( ) ) \n        self . readLiteralContextModes ( ) \n        pass \n        self . cmaps = { } \n        numberOfTrees = { I : self . numberOfBlockTypes [ I ] } \n        for blockType in ( L , D ) : \n            numberOfTrees [ blockType ] = self . contextMap ( blockType ) \n        pass \n        self . prefixCodes = { } \n        for blockType in ( L , I , D ) : \n            self . readPrefixArray ( blockType , numberOfTrees [ blockType ] ) \n        self . metablock ( ) "}
{"1992": "\ndef metablockLength ( self ) : \n    self . MLEN = self . verboseRead ( MetablockLengthAlphabet ( ) ) \n    if self . MLEN : \n        return False \n    self . verboseRead ( ReservedAlphabet ( ) ) \n    MSKIP = self . verboseRead ( SkipLengthAlphabet ( ) ) \n    self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n    self . stream . pos += 8 * MSKIP \n    pass \n    return True "}
{"1993": "\ndef uncompressed ( self ) : \n    ISUNCOMPRESSED = self . verboseRead ( BoolCode ( 'UNCMPR' , description = 'Is uncompressed?' ) ) \n    if ISUNCOMPRESSED : \n        self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n        pass \n        self . output += self . stream . readBytes ( self . MLEN ) \n        pass \n    return ISUNCOMPRESSED "}
{"2008": "\ndef from_samp ( username = None , password = None ) : \n    pass \n    import vaex . samp \n    t = vaex . samp . single_table ( username = username , password = password ) \n    return from_astropy_table ( t . to_table ( ) ) "}
{"2012": "\ndef from_pandas ( df , name = \"pandas\" , copy_index = True , index_name = \"index\" ) : \n    import six \n    vaex_df = vaex . dataframe . DataFrameArrays ( name ) \n    def add ( name , column ) : \n        values = column . values \n        try : \n            vaex_df . add_column ( name , values ) \n        except Exception as e : \n            pass \n            try : \n                values = values . astype ( \"S\" ) \n                vaex_df . add_column ( name , values ) \n            except Exception as e : \n                pass \n    for name in df . columns : \n        add ( name , df [ name ] ) \n    if copy_index : \n        add ( index_name , df . index ) \n    return vaex_df "}
{"2020": "\ndef delayed ( f ) : \n    def wrapped ( * args , ** kwargs ) : \n        key_promise = list ( [ ( key , promisify ( value ) ) for key , value in kwargs . items ( ) ] ) \n        arg_promises = list ( [ promisify ( value ) for value in args ] ) \n        kwarg_promises = list ( [ promise for key , promise in key_promise ] ) \n        promises = arg_promises + kwarg_promises \n        for promise in promises : \n            def echo_error ( exc , promise = promise ) : \n                pass \n            def echo ( value , promise = promise ) : \n                pass \n        allarguments = aplus . listPromise ( * promises ) \n        def call ( _ ) : \n            kwargs_real = { key : promise . get ( ) for key , promise in key_promise } \n            args_real = list ( [ promise . get ( ) for promise in arg_promises ] ) \n            return f ( * args_real , ** kwargs_real ) \n        def error ( exc ) : \n            pass \n            raise exc \n        return allarguments . then ( call , error ) \n    return wrapped "}
{"2023": "\ndef sort ( self , Ncol , order ) : \n    self . emit ( QtCore . SIGNAL ( \"layoutAboutToBeChanged()\" ) ) \n    if Ncol == 0 : \n        pass \n        sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) \n        pass \n        sortlist . sort ( key = operator . itemgetter ( 0 ) ) \n        pass \n        self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) \n        pass \n    if Ncol == 1 : \n        if None not in self . ranking : \n            sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) \n            sortlist . sort ( key = operator . itemgetter ( 0 ) ) \n            self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) \n        else : \n            self . indices = list ( range ( len ( self . pairs ) ) ) \n        pass \n    if order == QtCore . Qt . DescendingOrder : \n        self . indices . reverse ( ) \n    pass \n    self . emit ( QtCore . SIGNAL ( \"layoutChanged()\" ) ) "}
{"2075": "\ndef cat ( self , i1 , i2 , format = 'html' ) : \n    from IPython import display \n    if format == 'html' : \n        output = self . _as_html_table ( i1 , i2 ) \n        display . display ( display . HTML ( output ) ) \n    else : \n        output = self . _as_table ( i1 , i2 , format = format ) \n        pass "}
{"2152": "\ndef show_versions ( ) : \n    core_deps = [ 'audioread' , 'numpy' , 'scipy' , 'sklearn' , 'joblib' , 'decorator' , 'six' , 'soundfile' , 'resampy' , 'numba' ] \n    extra_deps = [ 'numpydoc' , 'sphinx' , 'sphinx_rtd_theme' , 'sphinxcontrib.versioning' , 'sphinx-gallery' , 'pytest' , 'pytest-mpl' , 'pytest-cov' , 'matplotlib' ] \n    pass \n    pass \n    pass \n    pass \n    for dep in core_deps : \n        pass \n    pass \n    for dep in extra_deps : \n        pass \n    pass "}
{"2155": "\ndef beat_track ( input_file , output_csv ) : \n    pass \n    y , sr = librosa . load ( input_file , sr = 22050 ) \n    hop_length = 512 \n    pass \n    tempo , beats = librosa . beat . beat_track ( y = y , sr = sr , hop_length = hop_length ) \n    pass \n    beat_times = librosa . frames_to_time ( beats , sr = sr , hop_length = hop_length ) \n    pass \n    librosa . output . times_csv ( output_csv , beat_times ) \n    pass "}
{"2156": "\ndef adjust_tuning ( input_file , output_file ) : \n    pass \n    y , sr = librosa . load ( input_file ) \n    pass \n    y_harm = librosa . effects . harmonic ( y ) \n    pass \n    tuning = librosa . estimate_tuning ( y = y_harm , sr = sr ) \n    pass \n    pass \n    y_tuned = librosa . effects . pitch_shift ( y , sr , - tuning ) \n    pass \n    librosa . output . write_wav ( output_file , y_tuned , sr ) "}
{"2216": "\ndef hpss_beats ( input_file , output_csv ) : \n    pass \n    y , sr = librosa . load ( input_file ) \n    pass \n    y = librosa . effects . percussive ( y ) \n    pass \n    onset_env = librosa . onset . onset_strength ( y = y , sr = sr , hop_length = HOP_LENGTH , n_fft = N_FFT , aggregate = np . median ) \n    tempo , beats = librosa . beat . beat_track ( onset_envelope = onset_env , sr = sr , hop_length = HOP_LENGTH ) \n    beat_times = librosa . frames_to_time ( beats , sr = sr , hop_length = HOP_LENGTH ) \n    pass \n    librosa . output . times_csv ( output_csv , beat_times ) "}
{"2260": "\ndef stretch_demo ( input_file , output_file , speed ) : \n    pass \n    y , sr = librosa . load ( input_file ) \n    pass \n    y_stretch = librosa . effects . time_stretch ( y , speed ) \n    pass \n    librosa . output . write_wav ( output_file , y_stretch , sr ) "}
{"2262": "\ndef hpss_demo ( input_file , output_harmonic , output_percussive ) : \n    pass \n    y , sr = librosa . load ( input_file ) \n    pass \n    y_harmonic , y_percussive = librosa . effects . hpss ( y ) \n    pass \n    librosa . output . write_wav ( output_harmonic , y_harmonic , sr ) \n    pass \n    librosa . output . write_wav ( output_percussive , y_percussive , sr ) "}
{"2274": "\ndef onset_detect ( input_file , output_csv ) : \n    pass \n    y , sr = librosa . load ( input_file , sr = 22050 ) \n    hop_length = 512 \n    pass \n    onsets = librosa . onset . onset_detect ( y = y , sr = sr , hop_length = hop_length ) \n    pass \n    onset_times = librosa . frames_to_time ( onsets , sr = sr , hop_length = hop_length ) \n    pass \n    librosa . output . times_csv ( output_csv , onset_times ) \n    pass "}
{"2315": "\ndef load_state_dict ( module , state_dict , strict = False , logger = None ) : \n    unexpected_keys = [ ] \n    own_state = module . state_dict ( ) \n    for name , param in state_dict . items ( ) : \n        if name not in own_state : \n            unexpected_keys . append ( name ) \n            continue \n        if isinstance ( param , torch . nn . Parameter ) : \n            param = param . data \n        try : \n            own_state [ name ] . copy_ ( param ) \n        except Exception : \n            raise RuntimeError ( 'While copying the parameter named {}, ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {}.' . format ( name , own_state [ name ] . size ( ) , param . size ( ) ) ) \n    missing_keys = set ( own_state . keys ( ) ) - set ( state_dict . keys ( ) ) \n    err_msg = [ ] \n    if unexpected_keys : \n        err_msg . append ( 'unexpected key in source state_dict: {}\\n' . format ( ', ' . join ( unexpected_keys ) ) ) \n    if missing_keys : \n        err_msg . append ( 'missing keys in source state_dict: {}\\n' . format ( ', ' . join ( missing_keys ) ) ) \n    err_msg = '\\n' . join ( err_msg ) \n    if err_msg : \n        if strict : \n            raise RuntimeError ( err_msg ) \n        elif logger is not None : \n            logger . warn ( err_msg ) \n        else : \n            pass "}
{"2325": "\ndef convert_video ( in_file , out_file , print_cmd = False , pre_options = '' , ** kwargs ) : \n    options = [ ] \n    for k , v in kwargs . items ( ) : \n        if isinstance ( v , bool ) : \n            if v : \n                options . append ( '-{}' . format ( k ) ) \n        elif k == 'log_level' : \n            assert v in [ 'quiet' , 'panic' , 'fatal' , 'error' , 'warning' , 'info' , 'verbose' , 'debug' , 'trace' ] \n            options . append ( '-loglevel {}' . format ( v ) ) \n        else : \n            options . append ( '-{} {}' . format ( k , v ) ) \n    cmd = 'ffmpeg -y {} -i {} {} {}' . format ( pre_options , in_file , ' ' . join ( options ) , out_file ) \n    if print_cmd : \n        pass \n    subprocess . call ( cmd , shell = True ) "}
{"2341": "\ndef check_prerequisites ( prerequisites , checker , msg_tmpl = 'Prerequisites \"{}\" are required in method \"{}\" but not ' 'found, please install them first.' ) : \n    def wrap ( func ) : \n        \n        @ functools . wraps ( func ) \n        def wrapped_func ( * args , ** kwargs ) : \n            requirements = [ prerequisites ] if isinstance ( prerequisites , str ) else prerequisites \n            missing = [ ] \n            for item in requirements : \n                if not checker ( item ) : \n                    missing . append ( item ) \n            if missing : \n                pass \n                raise RuntimeError ( 'Prerequisites not meet.' ) \n            else : \n                return func ( * args , ** kwargs ) \n        return wrapped_func \n    return wrap "}
{"2457": "\ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if state . ndim == 2 and len ( state ) == 4 and d0 == 2 and d1 == 2 : \n        return __eof_qubit ( state ) \n    elif state . ndim == 1 : \n        if d0 < d1 : \n            tr = [ 1 ] \n        else : \n            tr = [ 0 ] \n        state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n        return entropy ( state ) \n    else : \n        pass \n    return None "}
{"2688": "\ndef to_string ( self , indent ) : \n    ind = indent * ' ' \n    pass \n    self . children [ 0 ] . to_string ( indent + 3 ) "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        pass \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if len ( msg ) < msg_len : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif len ( msg ) > msg_len : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            pass \n            prev_msg = msg \n    if not quiet : \n        pass "}
{"3409": "\ndef complex_input_with_reference ( ) : \n    pass \n    wps = WebProcessingService ( 'http://localhost:8094/wps' , verbose = verbose ) \n    processid = 'wordcount' \n    textdoc = ComplexDataInput ( \"http://www.gutenberg.org/files/28885/28885-h/28885-h.htm\" ) \n    inputs = [ ( \"text\" , textdoc ) ] \n    outputs = [ ( \"output\" , True , 'some/mime-type' ) ] \n    execution = wps . execute ( processid , inputs , output = outputs ) \n    monitorExecution ( execution ) \n    pass \n    pass \n    for output in execution . processOutputs : \n        pass "}
{"3473": "\ndef echo ( self , s , file = sys . stdout , end = \"\\n\" ) : \n    p = getattr ( self , \"_progress_bar\" , None ) \n    if p is not None : \n        p . write ( s , file = file , end = \"\\n\" ) \n        return \n    pass "}
{"3578": "\ndef decompose ( hangul_letter ) : \n    from . import checker \n    if len ( hangul_letter ) < 1 : \n        raise NotLetterException ( '' ) \n    elif not checker . is_hangul ( hangul_letter ) : \n        raise NotHangulException ( '' ) \n    if hangul_letter in CHO : \n        return hangul_letter , '' , '' \n    if hangul_letter in JOONG : \n        return '' , hangul_letter , '' \n    if hangul_letter in JONG : \n        return '' , '' , hangul_letter \n    code = hangul_index ( hangul_letter ) \n    cho , joong , jong = decompose_index ( code ) \n    if cho < 0 : \n        cho = 0 \n    try : \n        return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] \n    except : \n        pass \n        pass \n        raise Exception ( ) "}
{"3632": "\ndef display_messages ( self , layout ) : \n    pass "}
{"3668": "\ndef _display ( self , layout ) : \n    pass \n    TextWriter ( ) . format ( layout , self . out ) "}
{"3687": "\ndef help_message ( self , msgids ) : \n    for msgid in msgids : \n        try : \n            for message_definition in self . get_message_definitions ( msgid ) : \n                pass \n                pass \n        except UnknownMessageError as ex : \n            pass \n            pass \n            continue "}
{"3688": "\ndef list_messages ( self ) : \n    messages = sorted ( self . _messages_definitions . values ( ) , key = lambda m : m . msgid ) \n    for message in messages : \n        if not message . may_be_emitted ( ) : \n            continue \n        pass \n    pass "}
{"3695": "\ndef set_option ( self , optname , value , action = None , optdict = None ) : \n    if optname in self . _options_methods or optname in self . _bw_options_methods : \n        if value : \n            try : \n                meth = self . _options_methods [ optname ] \n            except KeyError : \n                meth = self . _bw_options_methods [ optname ] \n                warnings . warn ( \"%s is deprecated, replace it by %s\" % ( optname , optname . split ( \"-\" ) [ 0 ] ) , DeprecationWarning , ) \n            value = utils . _check_csv ( value ) \n            if isinstance ( value , ( list , tuple ) ) : \n                for _id in value : \n                    meth ( _id , ignore_unknown = True ) \n            else : \n                meth ( value ) \n            return \n    elif optname == \"output-format\" : \n        self . _reporter_name = value \n        if self . _reporters : \n            self . _load_reporter ( ) \n    try : \n        checkers . BaseTokenChecker . set_option ( self , optname , value , action , optdict ) \n    except config . UnsupportedAction : \n        pass "}
{"3709": "\ndef cb_list_groups ( self , * args , ** kwargs ) : \n    for check in self . linter . get_checker_names ( ) : \n        pass \n    sys . exit ( 0 ) "}
{"3714": "\ndef register_plugins ( linter , directory ) : \n    imported = { } \n    for filename in listdir ( directory ) : \n        base , extension = splitext ( filename ) \n        if base in imported or base == \"__pycache__\" : \n            continue \n        if ( extension in PY_EXTS and base != \"__init__\" or ( not extension and isdir ( join ( directory , base ) ) ) ) : \n            try : \n                module = modutils . load_module_from_file ( join ( directory , filename ) ) \n            except ValueError : \n                continue \n            except ImportError as exc : \n                pass \n            else : \n                if hasattr ( module , \"register\" ) : \n                    module . register ( linter ) \n                    imported [ base ] = 1 "}
{"3717": "\ndef format_section ( stream , section , options , doc = None ) : \n    if doc : \n        pass \n    pass \n    _ini_format ( stream , options ) "}
{"3718": "\ndef _ini_format ( stream , options ) : \n    for optname , optdict , value in options : \n        value = _format_option_value ( optdict , value ) \n        help_opt = optdict . get ( \"help\" ) \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79 , indent = \"# \" ) \n            pass \n            pass \n        else : \n            pass \n        if value is None : \n            pass \n        else : \n            value = str ( value ) . strip ( ) \n            if re . match ( r\"^([\\w-]+,)+[\\w-]+$\" , str ( value ) ) : \n                separator = \"\\n \" + \" \" * len ( optname ) \n                value = separator . join ( x + \",\" for x in str ( value ) . split ( \",\" ) ) \n                value = value [ : - 1 ] \n            pass "}
{"3756": "\ndef _display_sims ( self , sims ) : \n    nb_lignes_dupliquees = 0 \n    for num , couples in sims : \n        pass \n        pass \n        couples = sorted ( couples ) \n        for lineset , idx in couples : \n            pass \n        for line in lineset . _real_lines [ idx : idx + num ] : \n            pass \n        nb_lignes_dupliquees += num * ( len ( couples ) - 1 ) \n    nb_total_lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) \n    pass "}
{"3807": "\ndef _check_graphviz_available ( output_format ) : \n    try : \n        subprocess . call ( [ \"dot\" , \"-V\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    except OSError : \n        pass \n        sys . exit ( 32 ) "}
{"3808": "\ndef run ( self , args ) : \n    if not args : \n        pass \n        return 1 \n    sys . path . insert ( 0 , os . getcwd ( ) ) \n    try : \n        project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) \n        linker = Linker ( project , tag = True ) \n        handler = DiadefsHandler ( self . config ) \n        diadefs = handler . get_diadefs ( project , linker ) \n    finally : \n        sys . path . pop ( 0 ) \n    if self . config . output_format == \"vcg\" : \n        writer . VCGWriter ( self . config ) . write ( diadefs ) \n    else : \n        writer . DotWriter ( self . config ) . write ( diadefs ) \n    return 0 "}
{"3816": "\ndef lint ( filename , options = ( ) ) : \n    full_path = osp . abspath ( filename ) \n    parent_path = osp . dirname ( full_path ) \n    child_path = osp . basename ( full_path ) \n    while parent_path != \"/\" and osp . exists ( osp . join ( parent_path , \"__init__.py\" ) ) : \n        child_path = osp . join ( osp . basename ( parent_path ) , child_path ) \n        parent_path = osp . dirname ( parent_path ) \n    run_cmd = \"import sys; from pylint.lint import Run; Run(sys.argv[1:])\" \n    cmd = ( [ sys . executable , \"-c\" , run_cmd ] + [ \"--msg-template\" , \"{path}:{line}: {category} ({msg_id}, {symbol}, {obj}) {msg}\" , \"-r\" , \"n\" , child_path , ] + list ( options ) ) \n    process = Popen ( cmd , stdout = PIPE , cwd = parent_path , env = _get_env ( ) , universal_newlines = True ) \n    for line in process . stdout : \n        if line . startswith ( \"No config file found\" ) : \n            continue \n        parts = line . split ( \":\" ) \n        if parts and parts [ 0 ] == child_path : \n            line = \":\" . join ( [ filename ] + parts [ 1 : ] ) \n        pass \n    process . wait ( ) \n    return process . returncode "}
{"3821": "\ndef _rest_format_section ( stream , section , options , doc = None ) : \n    if section : \n        pass \n    if doc : \n        pass \n        pass \n    for optname , optdict , value in options : \n        help_opt = optdict . get ( \"help\" ) \n        pass \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79 , indent = \"  \" ) \n            pass \n        if value : \n            value = str ( _format_option_value ( optdict , value ) ) \n            pass \n            pass "}
{"3828": "\ndef print_full_documentation ( self , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    pass \n    pass \n    pass \n    pass \n    pass \n    by_checker = { } \n    for checker in self . get_checkers ( ) : \n        if checker . name == \"master\" : \n            if checker . options : \n                for section , options in checker . options_by_section ( ) : \n                    if section is None : \n                        title = \"General options\" \n                    else : \n                        title = \"%s options\" % section . capitalize ( ) \n                    pass \n                    pass \n                    _rest_format_section ( stream , None , options ) \n                    pass \n        else : \n            name = checker . name \n            try : \n                by_checker [ name ] [ \"options\" ] += checker . options_and_values ( ) \n                by_checker [ name ] [ \"msgs\" ] . update ( checker . msgs ) \n                by_checker [ name ] [ \"reports\" ] += checker . reports \n            except KeyError : \n                by_checker [ name ] = { \"options\" : list ( checker . options_and_values ( ) ) , \"msgs\" : dict ( checker . msgs ) , \"reports\" : list ( checker . reports ) , } \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    for checker , info in sorted ( by_checker . items ( ) ) : \n        self . _print_checker_doc ( checker , info , stream = stream ) "}
{"3829": "\ndef _print_checker_doc ( checker_name , info , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    doc = info . get ( \"doc\" ) \n    module = info . get ( \"module\" ) \n    msgs = info . get ( \"msgs\" ) \n    options = info . get ( \"options\" ) \n    reports = info . get ( \"reports\" ) \n    checker_title = \"%s checker\" % ( checker_name . replace ( \"_\" , \" \" ) . title ( ) ) \n    if module : \n        pass \n    pass \n    pass \n    pass \n    if module : \n        pass \n    pass \n    pass \n    if doc : \n        title = \"{} Documentation\" . format ( checker_title ) \n        pass \n        pass \n        pass \n        pass \n    if options : \n        title = \"{} Options\" . format ( checker_title ) \n        pass \n        pass \n        _rest_format_section ( stream , None , options ) \n        pass \n    if msgs : \n        title = \"{} Messages\" . format ( checker_title ) \n        pass \n        pass \n        for msgid , msg in sorted ( msgs . items ( ) , key = lambda kv : ( _MSG_ORDER . index ( kv [ 0 ] [ 0 ] ) , kv [ 1 ] ) ) : \n            msg = build_message_definition ( checker_name , msgid , msg ) \n            pass \n        pass \n    if reports : \n        title = \"{} Reports\" . format ( checker_title ) \n        pass \n        pass \n        for report in reports : \n            pass \n        pass \n    pass "}
{"3861": "\ndef generate_config ( self , stream = None , skipsections = ( ) , encoding = None ) : \n    options_by_section = { } \n    sections = [ ] \n    for provider in self . options_providers : \n        for section , options in provider . options_by_section ( ) : \n            if section is None : \n                section = provider . name \n            if section in skipsections : \n                continue \n            options = [ ( n , d , v ) for ( n , d , v ) in options if d . get ( \"type\" ) is not None and not d . get ( \"deprecated\" ) ] \n            if not options : \n                continue \n            if section not in sections : \n                sections . append ( section ) \n            alloptions = options_by_section . setdefault ( section , [ ] ) \n            alloptions += options \n    stream = stream or sys . stdout \n    printed = False \n    for section in sections : \n        if printed : \n            pass \n        utils . format_section ( stream , section . upper ( ) , sorted ( options_by_section [ section ] ) ) \n        printed = True "}
{"4121": "\ndef _print_token_factory ( col ) : \n    def _helper ( msg ) : \n        style = style_from_dict ( { Token . Color : col , } ) \n        tokens = [ ( Token . Color , msg ) ] \n        print_tokens ( tokens , style = style ) \n    def _helper_no_terminal ( msg ) : \n        pass \n    if sys . stdout . isatty ( ) : \n        return _helper \n    else : \n        return _helper_no_terminal "}
{"4199": "\ndef color_values ( color ) : \n    this_dir = os . path . dirname ( os . path . realpath ( inspect . getsourcefile ( lambda : 0 ) ) ) \n    color_name_file = os . path . join ( this_dir , 'color_names.txt' ) \n    found = False \n    for line in open ( color_name_file , 'r' ) : \n        line = line . rstrip ( ) \n        if color . lower ( ) == line . split ( ) [ 0 ] : \n            red = line . split ( ) [ 2 ] \n            green = line . split ( ) [ 3 ] \n            blue = line . split ( ) [ 4 ] \n            found = True \n            break \n    if not found : \n        pass \n        red = 255 \n        green = 255 \n        blue = 255 \n    return red , green , blue "}
{"4200": "\ndef check_list ( var , num_terms ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n        for _ in range ( 1 , num_terms ) : \n            var . append ( var [ 0 ] ) \n    if len ( var ) != num_terms : \n        pass \n        sys . exit ( 1 ) \n    return var "}
{"4202": "\ndef write_filter ( script , filter_xml ) : \n    if isinstance ( script , mlx . FilterScript ) : \n        script . filters . append ( filter_xml ) \n    elif isinstance ( script , str ) : \n        script_file = open ( script , 'a' ) \n        script_file . write ( filter_xml ) \n        script_file . close ( ) \n    else : \n        pass \n    return None "}
{"4209": "\ndef rotate ( script , axis = 'z' , angle = 0.0 ) : \n    angle = math . radians ( angle ) \n    if axis . lower ( ) == 'x' : \n        vert_function ( script , x_func = 'x' , y_func = 'y*cos({angle})-z*sin({angle})' . format ( angle = angle ) , z_func = 'y*sin({angle})+z*cos({angle})' . format ( angle = angle ) ) \n    elif axis . lower ( ) == 'y' : \n        vert_function ( script , x_func = 'z*sin({angle})+x*cos({angle})' . format ( angle = angle ) , y_func = 'y' , z_func = 'z*cos({angle})-x*sin({angle})' . format ( angle = angle ) ) \n    elif axis . lower ( ) == 'z' : \n        vert_function ( script , x_func = 'x*cos({angle})-y*sin({angle})' . format ( angle = angle ) , y_func = 'x*sin({angle})+y*cos({angle})' . format ( angle = angle ) , z_func = 'z' ) \n    else : \n        pass \n        sys . exit ( 1 ) \n    return None "}
{"4231": "\ndef handle_error ( program_name , cmd , log = None ) : \n    pass \n    pass \n    if log is not None : \n        pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    while True : \n        choice = input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if choice == 'x' : \n        pass \n        sys . exit ( 1 ) \n    elif choice == 'xd' : \n        pass \n        util . delete_all ( 'TEMP3D*' ) \n        if log is not None : \n            os . remove ( log ) \n        sys . exit ( 1 ) \n    elif choice == 'c' : \n        pass \n        break_now = True \n    elif choice == 'r' : \n        pass \n        break_now = False \n    return break_now "}
{"4235": "\ndef save_to_file ( self , script_file ) : \n    if not self . filters : \n        pass \n    script_file_descriptor = open ( script_file , 'w' ) \n    script_file_descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) \n    script_file_descriptor . close ( ) "}
{"4246": "\ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : \n    topology = { 'manifold' : True , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = False \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2 ] \n                if topology [ 'genus' ] != 'undefined' : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2 ] \n                if topology [ 'hole_num' ] == 'a' : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        elif print_output : \n            pass \n    return topology "}
{"4247": "\ndef parse_hausdorff ( ml_log , log = None , print_output = False ) : \n    hausdorff_distance = { \"min_distance\" : 0.0 , \"max_distance\" : 0.0 , \"mean_distance\" : 0.0 , \"rms_distance\" : 0.0 , \"number_points\" : 0 } \n    with open ( ml_log ) as fread : \n        result = fread . readlines ( ) \n        data = \"\" \n        for idx , line in enumerate ( result ) : \n            m = re . match ( r\"\\s*Sampled (\\d+) pts.*\" , line ) \n            if m is not None : \n                hausdorff_distance [ \"number_points\" ] = int ( m . group ( 1 ) ) \n            if 'Hausdorff Distance computed' in line : \n                data = result [ idx + 2 ] \n        m = re . match ( r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\" , data ) \n        hausdorff_distance [ \"min_distance\" ] = float ( m . group ( 1 ) ) \n        hausdorff_distance [ \"max_distance\" ] = float ( m . group ( 2 ) ) \n        hausdorff_distance [ \"mean_distance\" ] = float ( m . group ( 3 ) ) \n        hausdorff_distance [ \"rms_distance\" ] = float ( m . group ( 4 ) ) \n        for key , value in hausdorff_distance . items ( ) : \n            if log is not None : \n                log_file = open ( log , 'a' ) \n                log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n                log_file . close ( ) \n            elif print_output : \n                pass \n        return hausdorff_distance "}
{"4259": "\ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) \n    if fext != 'obj' : \n        pass \n        sys . exit ( 1 ) \n    fread = open ( fbasename , 'r' ) \n    first = True \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if element == 'v' : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        elif element == 'l' : \n            p1 = x_co \n            p2 = y_co \n            line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4262": "\ndef measure_dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml_version = ml_version ) : \n    axis1 = axis1 . lower ( ) \n    axis2 = axis2 . lower ( ) \n    ml_script1_file = 'TEMP3D_measure_dimension.mlx' \n    file_out = 'TEMP3D_measure_dimension.xyz' \n    ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) \n    compute . section ( ml_script1 , axis1 , offset1 , surface = True ) \n    compute . section ( ml_script1 , axis2 , offset2 , surface = False ) \n    layers . delete_lower ( ml_script1 ) \n    ml_script1 . save_to_file ( ml_script1_file ) \n    ml_script1 . run_script ( log = log , script_file = ml_script1_file ) \n    for val in ( 'x' , 'y' , 'z' ) : \n        if val not in ( axis1 , axis2 ) : \n            axis = val \n    axis_num = ord ( axis ) - ord ( 'x' ) \n    aabb = measure_aabb ( file_out , log ) \n    dimension = { 'min' : aabb [ 'min' ] [ axis_num ] , 'max' : aabb [ 'max' ] [ axis_num ] , 'length' : aabb [ 'size' ] [ axis_num ] , 'axis' : axis } \n    if log is None : \n        pass \n        pass \n        pass \n    else : \n        log_file = open ( log , 'a' ) \n        log_file . write ( '\\nFor file \"%s\"\\n' % fbasename ) \n        log_file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) \n        log_file . write ( 'min = %s\\n' % dimension [ 'min' ] ) \n        log_file . write ( 'max = %s\\n' % dimension [ 'max' ] ) \n        log_file . write ( 'Total length = %s\\n' % dimension [ 'length' ] ) \n        log_file . close ( ) \n    return dimension "}
{"4307": "\ndef start ( host , port , profiler_stats , dont_start_browser , debug_mode ) : \n    stats_handler = functools . partial ( StatsHandler , profiler_stats ) \n    if not debug_mode : \n        sys . stderr = open ( os . devnull , 'w' ) \n    pass \n    if not dont_start_browser : \n        webbrowser . open ( 'http://{}:{}/' . format ( host , port ) ) \n    try : \n        StatsServer ( ( host , port ) , stats_handler ) . serve_forever ( ) \n    except KeyboardInterrupt : \n        pass \n        sys . exit ( 0 ) "}
{"4322": "\ndef run_profilers ( run_object , prof_config , verbose = False ) : \n    if len ( prof_config ) > len ( set ( prof_config ) ) : \n        raise AmbiguousConfigurationError ( 'Profiler configuration %s is ambiguous' % prof_config ) \n    available_profilers = { opt for opt , _ in _PROFILERS } \n    for option in prof_config : \n        if option not in available_profilers : \n            raise BadOptionError ( 'Unknown option: %s' % option ) \n    run_stats = OrderedDict ( ) \n    present_profilers = ( ( o , p ) for o , p in _PROFILERS if o in prof_config ) \n    for option , prof in present_profilers : \n        curr_profiler = prof ( run_object ) \n        if verbose : \n            pass \n        run_stats [ option ] = curr_profiler . run ( ) \n    return run_stats "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if self . verbose > 0 : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            pass \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( 0 , n_fits , n_folds ) : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4467": "\ndef get_studies_by_regions ( dataset , masks , threshold = 0.08 , remove_overlap = True , studies = None , features = None , regularization = \"scale\" ) : \n    import nibabel as nib \n    import os \n    try : \n        loaded_masks = [ nib . load ( os . path . relpath ( m ) ) for m in masks ] \n    except OSError : \n        pass \n    grouped_ids = [ dataset . get_studies ( mask = m , activation_threshold = threshold ) for m in loaded_masks ] \n    flat_ids = reduce ( lambda a , b : a + b , grouped_ids ) \n    if remove_overlap : \n        import collections \n        flat_ids = [ id for ( id , count ) in collections . Counter ( flat_ids ) . items ( ) if count == 1 ] \n        grouped_ids = [ [ x for x in m if x in flat_ids ] for m in grouped_ids ] \n    y = [ [ idx ] * len ( ids ) for ( idx , ids ) in enumerate ( grouped_ids ) ] \n    y = reduce ( lambda a , b : a + b , y ) \n    y = np . array ( y ) \n    X = [ dataset . get_feature_data ( ids = group_ids , features = features ) for group_ids in grouped_ids ] \n    X = np . vstack ( tuple ( X ) ) \n    if regularization : \n        X = regularize ( X , method = regularization ) \n    return ( X , y ) "}
{"4581": "\ndef stay_safe ( ) : \n    random = int ( choice ( str ( int ( time ( ) ) ) ) ) \n    if not CONFIGURATION [ \"quiet\" ] and random % 3 == 0 : \n        pass \n        pass \n        pass \n        pass "}
{"4584": "\ndef _print_header ( cls ) : \n    if ( not PyFunceble . CONFIGURATION [ \"quiet\" ] and not PyFunceble . CONFIGURATION [ \"header_printed\" ] ) : \n        pass \n        if PyFunceble . CONFIGURATION [ \"less\" ] : \n            Prints ( None , \"Less\" ) . header ( ) \n        else : \n            Prints ( None , \"Generic\" ) . header ( ) \n        PyFunceble . CONFIGURATION [ \"header_printed\" ] = True "}
{"4586": "\ndef domain ( self , domain = None , last_domain = None ) : \n    self . _print_header ( ) \n    if domain : \n        PyFunceble . INTERN [ \"to_test\" ] = self . _format_domain ( domain ) \n    else : \n        PyFunceble . INTERN [ \"to_test\" ] = None \n    if PyFunceble . INTERN [ \"to_test\" ] : \n        if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n            status = self . syntax_status . get ( ) \n        else : \n            status , _ = self . status . get ( ) \n        self . _file_decision ( PyFunceble . INTERN [ \"to_test\" ] , last_domain , status ) \n        if PyFunceble . CONFIGURATION [ \"simple\" ] : \n            pass \n        return PyFunceble . INTERN [ \"to_test\" ] , status \n    return None "}
{"4587": "\ndef url ( self , url_to_test = None , last_url = None ) : \n    self . _print_header ( ) \n    if url_to_test : \n        PyFunceble . INTERN [ \"to_test\" ] = url_to_test \n    else : \n        PyFunceble . INTERN [ \"to_test\" ] = None \n    if PyFunceble . INTERN [ \"to_test\" ] : \n        if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n            status = self . syntax_status . get ( ) \n        else : \n            status = self . url_status . get ( ) \n        self . _file_decision ( PyFunceble . INTERN [ \"to_test\" ] , last_url , status ) \n        if PyFunceble . CONFIGURATION [ \"simple\" ] : \n            pass \n        return PyFunceble . INTERN [ \"to_test\" ] , status \n    return None "}
{"4588": "\ndef colorify_logo ( cls , home = False ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        elif PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] >= 50 : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n        else : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        pass "}
{"4591": "\ndef file ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n        list_to_test = domain2idna ( list_to_test ) \n        if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . hierarchical ) \n        else : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . standard ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        _ = list_to_test [ - 1 ] \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . domain ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] \n    except IndexError : \n        pass "}
{"4592": "\ndef file_url ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        _ = list_to_test [ - 1 ] \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . url ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] \n    except IndexError : \n        pass "}
{"4606": "\ndef _load ( self ) : \n    if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in PyFunceble . environ : \n        while True : \n            response = input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + \"A configuration key is missing.\\n\" + PyFunceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) \n            if isinstance ( response , str ) : \n                if response . lower ( ) == \"y\" : \n                    self . _merge_values ( ) \n                    self . _save ( ) \n                    pass \n                    break \n                elif response . lower ( ) == \"n\" : \n                    raise Exception ( \"Configuration key still missing.\" ) \n    else : \n        self . _merge_values ( ) \n        self . _save ( ) "}
{"4622": "\ndef update ( self ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        pass \n    for extension , referer in self . _extensions ( ) : \n        if extension not in self . iana_db or self . iana_db [ extension ] != referer : \n            self . iana_db [ extension ] = referer \n            Dict ( self . iana_db ) . to_json ( self . destination ) \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        pass "}
{"4637": "\ndef header ( self , do_not_print = False ) : \n    if ( not PyFunceble . CONFIGURATION [ \"header_printed\" ] or self . template == \"Percentage\" or do_not_print ) : \n        if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template == \"Generic_File\" ) : \n            to_print = self . headers [ \"Generic\" ] \n            if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] and PyFunceble . HTTP_CODE [ \"active\" ] ) : \n                to_print = Dict ( to_print ) . remove_key ( \"Analyze Date\" ) \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"invalid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ] \n        elif ( self . template == \"Less\" or self . template == \"Percentage\" or self . template == \"HTTP\" ) : \n            to_print = self . headers [ self . template ] \n            if self . template == \"Less\" and not PyFunceble . HTTP_CODE [ \"active\" ] : \n                to_print [ \"Source\" ] = 10 \n        if not PyFunceble . HTTP_CODE [ \"active\" ] : \n            to_print = Dict ( to_print ) . remove_key ( \"HTTP Code\" ) \n        self . currently_used_header = to_print \n        if not do_not_print : \n            self . _before_header ( ) \n            for formatted_template in self . _header_constructor ( to_print ) : \n                if not self . only_on_file : \n                    pass \n                if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                    File ( self . output ) . write ( formatted_template + \"\\n\" ) "}
{"4642": "\ndef data ( self ) : \n    if isinstance ( self . data_to_print , list ) : \n        to_print = { } \n        to_print_size = [ ] \n        alone_cases = [ \"Percentage\" , \"HTTP\" ] \n        without_header = [ \"FullHosts\" , \"PlainDomain\" ] \n        if self . template . lower ( ) == \"json\" : \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                return self . _json_print ( ) \n            return None \n        if self . template not in alone_cases and self . template not in without_header : \n            self . header ( True ) \n            to_print_size = self . _size_from_header ( self . currently_used_header ) \n        elif self . template in without_header : \n            for data in self . data_to_print : \n                to_print_size . append ( str ( len ( data ) ) ) \n        else : \n            to_print_size = self . _size_from_header ( self . headers [ self . template ] ) \n        to_print = self . _data_constructor ( to_print_size ) \n        self . _before_header ( ) \n        for data in self . _header_constructor ( to_print , False ) : \n            if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template in [ \"Less\" , \"Percentage\" ] : \n                if not self . only_on_file : \n                    colorified_data = self . _colorify ( data ) \n                    pass \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                File ( self . output ) . write ( data + \"\\n\" ) \n    else : \n        raise Exception ( \"Please review Prints().data()\" ) "}
{"4643": "\ndef _save ( self , last = False ) : \n    if ( self . _authorization ( ) and PyFunceble . CONFIGURATION [ \"logs\" ] and \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . file = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"execution_time\" ] ) \n        if PyFunceble . path . isfile ( self . file ) : \n            content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) \n        else : \n            content = { } \n        if self . action == \"start\" : \n            if \"final_total\" in content and content [ \"final_total\" ] : \n                del content [ \"final_total\" ] \n            if \"data\" in content : \n                content [ \"data\" ] . append ( [ PyFunceble . INTERN [ \"start\" ] ] ) \n            else : \n                content [ \"data\" ] = [ [ PyFunceble . INTERN [ \"start\" ] ] ] \n        elif self . action == \"stop\" : \n            try : \n                content [ \"data\" ] [ - 1 ] . append ( PyFunceble . INTERN [ \"end\" ] ) \n                start = content [ \"data\" ] [ 0 ] [ 0 ] \n                end = content [ \"data\" ] [ - 1 ] [ - 1 ] \n                content [ \"current_total\" ] = self . format_execution_time ( start , end ) \n                if last : \n                    content [ \"final_total\" ] = content [ \"current_total\" ] \n                    pass \n            except KeyError : \n                pass \n        try : \n            Dict ( content ) . to_json ( self . file ) \n        except FileNotFoundError : \n            DirectoryStructure ( ) \n            Dict ( content ) . to_json ( self . file ) "}
{"4668": "\ndef log ( self ) : \n    if ( PyFunceble . CONFIGURATION [ \"show_percentage\" ] and PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 ) : \n        output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) \n        File ( output ) . delete ( ) \n        self . _calculate ( ) \n        if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n            pass \n            Prints ( None , \"Percentage\" , output ) . header ( ) \n            lines_to_print = [ [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] \n            if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n                lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] \n                del lines_to_print [ 1 ] \n            for to_print in lines_to_print : \n                Prints ( to_print , \"Percentage\" , output ) . data ( ) \n    elif PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 : \n        self . _calculate ( ) "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = False \n            try : \n                time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                pass \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"4722": "\ndef _main ( argv , standard_out , standard_error ) : \n    import argparse \n    parser = argparse . ArgumentParser ( description = __doc__ , prog = 'autoflake' ) \n    parser . add_argument ( '-c' , '--check' , action = 'store_true' , help = 'return error code if changes are needed' ) \n    parser . add_argument ( '-i' , '--in-place' , action = 'store_true' , help = 'make changes to files instead of printing diffs' ) \n    parser . add_argument ( '-r' , '--recursive' , action = 'store_true' , help = 'drill down directories recursively' ) \n    parser . add_argument ( '--exclude' , metavar = 'globs' , help = 'exclude file/directory names that match these ' 'comma-separated globs' ) \n    parser . add_argument ( '--imports' , help = 'by default, only unused standard library ' 'imports are removed; specify a comma-separated ' 'list of additional modules/packages' ) \n    parser . add_argument ( '--expand-star-imports' , action = 'store_true' , help = 'expand wildcard star imports with undefined ' 'names; this only triggers if there is only ' 'one star import in the file; this is skipped if ' 'there are any uses of `__all__` or `del` in the ' 'file' ) \n    parser . add_argument ( '--remove-all-unused-imports' , action = 'store_true' , help = 'remove all unused imports (not just those from ' 'the standard library)' ) \n    parser . add_argument ( '--ignore-init-module-imports' , action = 'store_true' , help = 'exclude __init__.py when removing unused ' 'imports' ) \n    parser . add_argument ( '--remove-duplicate-keys' , action = 'store_true' , help = 'remove all duplicate keys in objects' ) \n    parser . add_argument ( '--remove-unused-variables' , action = 'store_true' , help = 'remove unused variables' ) \n    parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s ' + __version__ ) \n    parser . add_argument ( 'files' , nargs = '+' , help = 'files to format' ) \n    args = parser . parse_args ( argv [ 1 : ] ) \n    if args . remove_all_unused_imports and args . imports : \n        pass \n        return 1 \n    if args . exclude : \n        args . exclude = _split_comma_separated ( args . exclude ) \n    else : \n        args . exclude = set ( [ ] ) \n    filenames = list ( set ( args . files ) ) \n    failure = False \n    for name in find_files ( filenames , args . recursive , args . exclude ) : \n        try : \n            fix_file ( name , args = args , standard_out = standard_out ) \n        except IOError as exception : \n            pass \n            failure = True \n    return 1 if failure else 0 "}
{"5084": "\ndef discover ( scope , loglevel , capture ) : \n    if loglevel : \n        level = getattr ( logging , loglevel , None ) \n        if not level : \n            pass \n            return \n        logger . setLevel ( level ) \n    run ( scope = scope , capture = capture ) "}
{"5112": "\ndef output_seed ( seed ) : \n    pass \n    compat . input ( '' ) \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    compat . input ( '' ) "}
{"5158": "\ndef _start_repl ( api ) : \n    banner = ( 'IOTA API client for {uri} ({testnet}) ' 'initialized as variable `api`.\\n' 'Type `help(api)` for list of API commands.' . format ( testnet = 'testnet' if api . testnet else 'mainnet' , uri = api . adapter . get_uri ( ) , ) ) \n    scope_vars = { 'api' : api } \n    try : \n        import IPython \n    except ImportError : \n        from code import InteractiveConsole \n        InteractiveConsole ( locals = scope_vars ) . interact ( banner , '' ) \n    else : \n        pass \n        IPython . start_ipython ( argv = [ ] , user_ns = scope_vars ) "}
{"5288": "\ndef start_monitor ( self , standalone = True ) : \n    try : \n        self . start ( ) \n        cmdline = shlex . split ( self . config . process_to_monitor ) \n        if standalone : \n            signal . signal ( signal . SIGINT , self . shutdown ) \n        self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        while self . process and not self . finished : \n            self . process . wait ( ) \n            if self . _is_sigsegv ( self . process . returncode ) : \n                if self . config . debug : \n                    pass \n                while not self . got_testcase ( ) : \n                    time . sleep ( 1 ) \n                self . save_testcase ( self . testcase [ - 10 : ] ) \n            if self . process : \n                self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n    except OSError : \n        self . shutdown ( ) \n        self . process = False \n        self . got_testcase = lambda : True \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmdline [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( \"Unknown error please send log to author\" ) "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == False : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = 0 \n    while total_gend < num : \n        if len ( self . _last_pref_keys ) > 0 and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            pass \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += 1 \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5641": "\ndef POST ( self ) : \n    json_data = web . data ( ) \n    pass \n    pass \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    pass \n    pass \n    pass \n    me = api . people . me ( ) \n    if message . personId == me . id : \n        return 'OK' \n    else : \n        if \"/CAT\" in message . text : \n            pass \n            cat_fact = get_catfact ( ) \n            pass \n            api . messages . create ( room . id , text = cat_fact ) \n    return 'OK' "}
{"5691": "\ndef get_ngrok_public_url ( ) : \n    try : \n        response = requests . get ( url = NGROK_CLIENT_API_BASE_URL + \"/tunnels\" , headers = { 'content-type' : 'application/json' } ) \n        response . raise_for_status ( ) \n    except requests . exceptions . RequestException : \n        pass \n        return None \n    else : \n        for tunnel in response . json ( ) [ \"tunnels\" ] : \n            if tunnel . get ( \"public_url\" , \"\" ) . startswith ( \"http://\" ) : \n                pass \n                return tunnel [ \"public_url\" ] "}
{"5692": "\ndef delete_webhooks_with_name ( api , name ) : \n    for webhook in api . webhooks . list ( ) : \n        if webhook . name == name : \n            pass \n            api . webhooks . delete ( webhook . id ) "}
{"5693": "\ndef create_ngrok_webhook ( api , ngrok_public_url ) : \n    pass \n    webhook = api . webhooks . create ( name = WEBHOOK_NAME , targetUrl = urljoin ( ngrok_public_url , WEBHOOK_URL_SUFFIX ) , resource = WEBHOOK_RESOURCE , event = WEBHOOK_EVENT , ) \n    pass \n    pass \n    return webhook "}
{"5695": "\ndef console ( ) : \n    parser = argparse . ArgumentParser ( description = console . __doc__ ) \n    parser . add_argument ( '--device' , default = '/dev/ttyUSB0' , help = 'port to read DSMR data from' ) \n    parser . add_argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) \n    parser . add_argument ( '--port' , default = None , help = 'TCP port to use for connection' ) \n    parser . add_argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) \n    parser . add_argument ( '--verbose' , '-v' , action = 'count' ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        level = logging . DEBUG \n    else : \n        level = logging . ERROR \n    logging . basicConfig ( level = level ) \n    loop = asyncio . get_event_loop ( ) \n    def print_callback ( telegram ) : \n        for obiref , obj in telegram . items ( ) : \n            if obj : \n                pass \n        pass \n    if args . host and args . port : \n        create_connection = partial ( create_tcp_dsmr_reader , args . host , args . port , args . version , print_callback , loop = loop ) \n    else : \n        create_connection = partial ( create_dsmr_reader , args . device , args . version , print_callback , loop = loop ) \n    try : \n        while True : \n            conn = create_connection ( ) \n            transport , protocol = loop . run_until_complete ( conn ) \n            loop . run_until_complete ( protocol . wait_closed ( ) ) \n            loop . run_until_complete ( asyncio . sleep ( 5 ) ) \n    except KeyboardInterrupt : \n        transport . close ( ) \n        loop . run_until_complete ( asyncio . sleep ( 0 ) ) \n    finally : \n        loop . close ( ) "}
{"5739": "\ndef run_benchmark ( ) : \n    stopping = threading . Event ( ) \n    workers = [ InsertWorker ( stopping ) for _ in range ( NUM_WORKERS ) ] \n    pass \n    [ worker . start ( ) for worker in workers ] \n    time . sleep ( WORKLOAD_TIME ) \n    pass \n    stopping . set ( ) \n    [ worker . join ( ) for worker in workers ] \n    with get_connection ( ) as conn : \n        count = conn . get ( \"SELECT COUNT(*) AS count FROM %s\" % TABLE ) . count \n    pass \n    pass "}
{"5741": "\ndef lookup_by_number ( errno ) : \n    for key , val in globals ( ) . items ( ) : \n        if errno == val : \n            pass "}
{"5820": "\ndef serve ( self , workers = None , ** kwargs ) : \n    if self . app . debug : \n        pass \n        self . app . run ( ) \n    else : \n        pass \n        server = GunicornServer ( self . app , workers = workers or number_of_gunicorn_workers ( ) , worker_class = 'egg:meinheld#gunicorn_worker' , ** kwargs ) \n        server . run ( ) "}
{"5841": "\ndef delete_matching_files_from_storage ( self , root_folder , regex ) : \n    if not self . name : \n        return \n    try : \n        directory_list , file_list = self . storage . listdir ( root_folder ) \n    except OSError : \n        pass \n    else : \n        folder , filename = os . path . split ( self . name ) \n        basename , ext = os . path . splitext ( filename ) \n        for f in file_list : \n            if not f . startswith ( basename ) or not f . endswith ( ext ) : \n                continue \n            tag = f [ len ( basename ) : - len ( ext ) ] \n            assert f == basename + tag + ext \n            if regex . match ( tag ) is not None : \n                file_location = os . path . join ( root_folder , f ) \n                self . storage . delete ( file_location ) \n                cache . delete ( self . storage . url ( file_location ) ) \n                pass "}
{"5907": "\ndef new_subcommand ( selected_address_books , input_from_stdin_or_file , open_editor ) : \n    selected_address_book = choose_address_book_from_list ( \"Select address book for new contact\" , selected_address_books ) \n    if selected_address_book is None : \n        pass \n        sys . exit ( 1 ) \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_user_input ( selected_address_book , input_from_stdin_or_file , config . get_supported_private_objects ( ) , config . get_preferred_vcard_version ( ) , config . localize_dates ( ) ) \n        except ValueError as err : \n            pass \n            sys . exit ( 1 ) \n        else : \n            new_contact . write_to_file ( ) \n        if open_editor : \n            modify_existing_contact ( new_contact ) \n        else : \n            pass \n    else : \n        create_new_contact ( selected_address_book ) "}
{"5908": "\ndef birthdays_subcommand ( vcard_list , parsable ) : \n    vcard_list = [ vcard for vcard in vcard_list if vcard . get_birthday ( ) is not None ] \n    vcard_list . sort ( key = lambda x : ( x . get_birthday ( ) . month , x . get_birthday ( ) . day ) if isinstance ( x . get_birthday ( ) , datetime . datetime ) else ( 0 , 0 , x . get_birthday ( ) ) ) \n    birthday_list = [ ] \n    for vcard in vcard_list : \n        date = vcard . get_birthday ( ) \n        if parsable : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_first_name_last_name ( ) ) ) \n            else : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_last_name_first_name ( ) ) ) \n        else : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_first_name_last_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n            else : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_last_name_first_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n    if birthday_list : \n        if parsable : \n            pass \n        else : \n            list_birthdays ( birthday_list ) \n    else : \n        if not parsable : \n            pass \n        sys . exit ( 1 ) "}
{"5909": "\ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ 0 ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                elif len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) >= 3 : \n                    if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                        matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            pass \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    elif all_phone_numbers_list : \n        if parsable : \n            pass \n        else : \n            list_phone_numbers ( all_phone_numbers_list ) \n    else : \n        if not parsable : \n            pass \n        sys . exit ( 1 ) "}
{"5910": "\ndef list_subcommand ( vcard_list , parsable ) : \n    if not vcard_list : \n        if not parsable : \n            pass \n        sys . exit ( 1 ) \n    elif parsable : \n        contact_line_list = [ ] \n        for vcard in vcard_list : \n            if config . display_by_name ( ) == \"first_name\" : \n                name = vcard . get_first_name_last_name ( ) \n            else : \n                name = vcard . get_last_name_first_name ( ) \n            contact_line_list . append ( '\\t' . join ( [ vcard . get_uid ( ) , name , vcard . address_book . name ] ) ) \n        pass \n    else : \n        list_contacts ( vcard_list ) "}
{"5911": "\ndef modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : \n    if selected_vcard . get_version ( ) not in config . supported_vcard_versions : \n        pass \n        while True : \n            input_string = input ( \"Do you want to proceed anyway (y/n)? \" ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                pass \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_existing_contact_with_new_user_input ( selected_vcard , input_from_stdin_or_file , config . localize_dates ( ) ) \n        except ValueError as err : \n            pass \n            sys . exit ( 1 ) \n        if selected_vcard == new_contact : \n            pass \n        else : \n            pass \n            while True : \n                input_string = input ( \"Do you want to proceed (y/n)? \" ) \n                if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                    pass \n                    break \n                if input_string . lower ( ) == \"y\" : \n                    new_contact . write_to_file ( overwrite = True ) \n                    if open_editor : \n                        modify_existing_contact ( new_contact ) \n                    else : \n                        pass \n                    break \n    else : \n        modify_existing_contact ( selected_vcard ) "}
{"5912": "\ndef remove_subcommand ( selected_vcard , force ) : \n    if not force : \n        while True : \n            input_string = input ( \"Deleting contact %s from address book %s. Are you sure? \" \"(y/n): \" % ( selected_vcard , selected_vcard . address_book ) ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                pass \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    selected_vcard . delete_vcard_file ( ) \n    pass "}
{"5914": "\ndef merge_subcommand ( vcard_list , selected_address_books , search_terms , target_uid ) : \n    if target_uid != \"\" and search_terms != \"\" : \n        pass \n        sys . exit ( 1 ) \n    if target_uid != \"\" : \n        target_vcards = get_contacts ( selected_address_books , target_uid , method = \"uid\" ) \n        if len ( target_vcards ) != 1 : \n            if not target_vcards : \n                pass \n            else : \n                pass \n                for vcard in target_vcards : \n                    pass \n            sys . exit ( 1 ) \n    else : \n        target_vcards = get_contact_list_by_user_selection ( selected_address_books , search_terms , False ) \n    source_vcard = choose_vcard_from_list ( \"Select contact from which to merge\" , vcard_list ) \n    if source_vcard is None : \n        pass \n        sys . exit ( 1 ) \n    else : \n        pass \n    target_vcard = choose_vcard_from_list ( \"Select contact into which to merge\" , target_vcards ) \n    if target_vcard is None : \n        pass \n        sys . exit ( 1 ) \n    else : \n        pass \n    if source_vcard == target_vcard : \n        pass \n    else : \n        merge_existing_contacts ( source_vcard , target_vcard , True ) "}
{"5915": "\ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        pass \n        sys . exit ( 1 ) \n    else : \n        pass \n    if len ( target_address_book_list ) == 1 and target_address_book_list [ 0 ] == source_vcard . address_book : \n        pass \n        sys . exit ( 1 ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if abook != source_vcard . address_book ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            pass \n            sys . exit ( 1 ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , True ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n    else : \n        if source_vcard == target_vcard : \n            pass \n            if action == \"move\" : \n                copy_contact ( source_vcard , selected_target_address_book , True ) \n            else : \n                pass \n        else : \n            pass \n            while True : \n                input_string = input ( \"Your choice: \" ) \n                if input_string . lower ( ) == \"a\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    break \n                if input_string . lower ( ) == \"o\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    target_vcard . delete_vcard_file ( ) \n                    break \n                if input_string . lower ( ) == \"m\" : \n                    merge_existing_contacts ( source_vcard , target_vcard , action == \"move\" ) \n                    break \n                if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                    pass \n                    break "}
{"5972": "\ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = True ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while True : \n        try : \n            call_item = call_queue . get ( block = True , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = False ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                pass \n            sys . exit ( 1 ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        else : \n            if ( ( _last_memory_leak_check is None ) or ( time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY ) ) : \n                gc . collect ( ) \n                _last_memory_leak_check = time ( ) "}
{"5989": "\ndef event_processor ( self , frame , event , arg ) : \n    out = self . debugger . intf [ - 1 ] . output \n    lineno = frame . f_lineno \n    filename = self . core . canonic_filename ( frame ) \n    filename = self . core . filename ( filename ) \n    if not out : \n        pass \n    else : \n        out . write ( \"%s - %s:%d\" % ( event , filename , lineno ) ) \n        if arg is not None : \n            out . writeline ( ', %s ' % repr ( arg ) ) \n        else : \n            out . writeline ( '' ) \n            pass \n        pass \n    return self . event_processor "}
{"6029": "\ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        pass \n        midpoint = 383 if term and term == 'xterm-256color' else 117963 \n    if ( ( 16 * 5 + 16 * g + 16 * b ) < midpoint ) : \n        return True \n    else : \n        return False "}
{"6055": "\ndef post_mortem ( exc = None , frameno = 1 , dbg = None ) : \n    if dbg is None : \n        if Mdebugger . debugger_obj is None : \n            Mdebugger . debugger_obj = Mdebugger . Trepan ( ) \n            pass \n        dbg = Mdebugger . debugger_obj \n        pass \n    re_bogus_file = re . compile ( \"^<.+>$\" ) \n    if exc [ 0 ] is None : \n        exc = get_last_or_frame_exception ( ) \n        if exc [ 0 ] is None : \n            pass \n            return \n        pass \n    exc_type , exc_value , exc_tb = exc \n    dbg . core . execution_status = ( 'Terminated with unhandled exception %s' % exc_type ) \n    if exc_tb is not None : \n        while exc_tb . tb_next is not None : \n            filename = exc_tb . tb_frame . f_code . co_filename \n            if ( dbg . mainpyfile and 0 == len ( dbg . mainpyfile ) and not re_bogus_file . match ( filename ) ) : \n                dbg . mainpyfile = filename \n                pass \n            exc_tb = exc_tb . tb_next \n            pass \n        dbg . core . processor . curframe = exc_tb . tb_frame \n        pass \n    if 0 == len ( dbg . program_sys_argv ) : \n        dbg . program_sys_argv = list ( sys . argv [ 1 : ] ) \n        dbg . program_sys_argv [ : 0 ] = [ dbg . mainpyfile ] \n    try : \n        f = exc_tb . tb_frame \n        if f and f . f_lineno != exc_tb . tb_lineno : \n            f = f . f_back \n        dbg . core . processor . event_processor ( f , 'exception' , exc , 'Trepan3k:pm' ) \n    except DebuggerRestart : \n        while True : \n            sys . argv = list ( dbg . _program_sys_argv ) \n            dbg . msg ( \"Restarting %s with arguments:\\n\\t%s\" % ( dbg . filename ( dbg . mainpyfile ) , \" \" . join ( dbg . _program_sys_argv [ 1 : ] ) ) ) \n            try : \n                dbg . run_script ( dbg . mainpyfile ) \n            except DebuggerRestart : \n                pass \n            pass \n    except DebuggerQuit : \n        pass \n    return "}
{"6059": "\ndef dbgr ( self , string ) : \n    pass \n    self . proc . cmd_queue . append ( string ) \n    self . proc . process_command ( ) \n    return "}
{"6111": "\ndef create_project_with_docs ( client , docs , language , name , account = None , progress = False ) : \n    description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) \n    if account is not None : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description , account_id = account , ) \n    else : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description ) \n    proj_id = proj_record [ 'project_id' ] \n    proj_client = client . client_for_path ( 'projects/' + proj_id ) \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Uploading documents' ) \n        else : \n            progress_bar = None \n        for batch in _batches ( docs , BATCH_SIZE ) : \n            docs_to_upload = [ _simplify_doc ( doc ) for doc in batch ] \n            proj_client . post ( 'upload' , docs = docs_to_upload ) \n            if progress : \n                progress_bar . update ( BATCH_SIZE ) \n    finally : \n        if progress : \n            progress_bar . close ( ) \n    pass \n    proj_client . post ( 'build' ) \n    while True : \n        time . sleep ( 10 ) \n        proj_status = proj_client . get ( ) \n        build_info = proj_status [ 'last_build_info' ] \n        if 'success' in build_info : \n            if not build_info [ 'success' ] : \n                raise LuminosoServerError ( build_info [ 'reason' ] ) \n            return proj_status "}
{"6113": "\ndef _main ( argv ) : \n    parser = argparse . ArgumentParser ( description = DESCRIPTION , formatter_class = argparse . RawDescriptionHelpFormatter , ) \n    parser . add_argument ( '-b' , '--base-url' , default = URL_BASE , help = 'API root url, default: %s' % URL_BASE , ) \n    parser . add_argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) \n    parser . add_argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) \n    parser . add_argument ( '-t' , '--token' , help = \"API authentication token\" ) \n    parser . add_argument ( '-s' , '--save-token' , action = 'store_true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) \n    parser . add_argument ( 'input_filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) \n    parser . add_argument ( 'project_name' , nargs = '?' , default = None , help = 'What the project should be called' , ) \n    args = parser . parse_args ( argv ) \n    if args . save_token : \n        if not args . token : \n            raise ValueError ( \"error: no token provided\" ) \n        LuminosoClient . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) \n    client = LuminosoClient . connect ( url = args . base_url , token = args . token ) \n    name = args . project_name \n    if name is None : \n        name = input ( 'Enter a name for the project: ' ) \n        if not name : \n            pass \n            return \n    result = upload_docs ( client , args . input_filename , args . language , name , account = args . account_id , progress = True , ) \n    pass "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        pass \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == 0 : \n            pass \n            return \n        if len ( projects ) > 1 : \n            pass \n        project_id = projects [ 0 ] [ 'project_id' ] \n        pass \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000 ) : \n        counter += 1 \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        pass \n    if not stage : \n        pass \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6222": "\ndef report ( self ) : \n    if not self . end_time : \n        self . end ( ) \n    pass "}
{"6335": "\ndef _update_index_url_from_configs ( self ) : \n    if 'VIRTUAL_ENV' in os . environ : \n        self . pip_config_locations . append ( os . path . join ( os . environ [ 'VIRTUAL_ENV' ] , 'pip.conf' ) ) \n        self . pip_config_locations . append ( os . path . join ( os . environ [ 'VIRTUAL_ENV' ] , 'pip.ini' ) ) \n    if site_config_files : \n        self . pip_config_locations . extend ( site_config_files ) \n    index_url = None \n    custom_config = None \n    if 'PIP_INDEX_URL' in os . environ and os . environ [ 'PIP_INDEX_URL' ] : \n        index_url = os . environ [ 'PIP_INDEX_URL' ] \n        custom_config = 'PIP_INDEX_URL environment variable' \n    else : \n        for pip_config_filename in self . pip_config_locations : \n            if pip_config_filename . startswith ( '~' ) : \n                pip_config_filename = os . path . expanduser ( pip_config_filename ) \n            if os . path . isfile ( pip_config_filename ) : \n                config = ConfigParser ( ) \n                config . read ( [ pip_config_filename ] ) \n                try : \n                    index_url = config . get ( 'global' , 'index-url' ) \n                    custom_config = pip_config_filename \n                    break \n                except ( NoOptionError , NoSectionError ) : \n                    pass \n    if index_url : \n        self . PYPI_API_URL = self . _prepare_api_url ( index_url ) \n        pass "}
{"6357": "\ndef pair ( cmd , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    key = pair_key ( word ) \n    tokens = [ t . decode ( ) for t in DB . smembers ( key ) ] \n    tokens . sort ( ) \n    pass \n    pass "}
{"6358": "\ndef do_AUTOCOMPLETE ( cmd , s ) : \n    s = list ( preprocess_query ( s ) ) [ 0 ] \n    keys = [ k . decode ( ) for k in DB . smembers ( edge_ngram_key ( s ) ) ] \n    pass \n    pass "}
{"6363": "\ndef do_fuzzy ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    pass "}
{"6364": "\ndef do_fuzzyindex ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    token = Token ( word ) \n    neighbors = make_fuzzy ( token ) \n    neighbors = [ ( n , DB . zcard ( dbkeys . token_key ( n ) ) ) for n in neighbors ] \n    neighbors . sort ( key = lambda n : n [ 1 ] , reverse = True ) \n    for token , freq in neighbors : \n        if freq == 0 : \n            break \n        pass "}
{"6366": "\ndef do_help ( self , command ) : \n    if command : \n        doc = getattr ( self , 'do_' + command ) . __doc__ \n        pass \n    else : \n        pass \n        pass \n        names = self . get_names ( ) \n        names . sort ( ) \n        for name in names : \n            if name [ : 3 ] != 'do_' : \n                continue \n            doc = getattr ( self , name ) . __doc__ \n            doc = doc . split ( '\\n' ) [ 0 ] \n            pass "}
{"6367": "\ndef do_DBINFO ( self , * args ) : \n    info = DB . info ( ) \n    keys = [ 'keyspace_misses' , 'keyspace_hits' , 'used_memory_human' , 'total_commands_processed' , 'total_connections_received' , 'connected_clients' ] \n    for key in keys : \n        pass \n    nb_of_redis_db = int ( DB . config_get ( 'databases' ) [ 'databases' ] ) \n    for db_index in range ( nb_of_redis_db - 1 ) : \n        db_name = 'db{}' . format ( db_index ) \n        if db_name in info : \n            label = white ( 'nb keys (db {})' . format ( db_index ) ) \n            pass "}
{"6368": "\ndef do_DBKEY ( self , key ) : \n    type_ = DB . type ( key ) . decode ( ) \n    if type_ == 'set' : \n        out = DB . smembers ( key ) \n    elif type_ == 'string' : \n        out = DB . get ( key ) \n    else : \n        out = 'Unsupported type {}' . format ( type_ ) \n    pass \n    pass "}
{"6369": "\ndef do_GEOHASH ( self , latlon ) : \n    try : \n        lat , lon = map ( float , latlon . split ( ) ) \n    except ValueError : \n        pass \n    else : \n        pass "}
{"6370": "\ndef do_GET ( self , _id ) : \n    doc = doc_by_id ( _id ) \n    if not doc : \n        return self . error ( 'id \"{}\" not found' . format ( _id ) ) \n    for key , value in doc . items ( ) : \n        if key == config . HOUSENUMBERS_FIELD : \n            continue \n        pass \n    if doc . get ( 'housenumbers' ) : \n        def sorter ( v ) : \n            try : \n                return int ( re . match ( r'^\\d+' , v [ 'raw' ] ) . group ( ) ) \n            except AttributeError : \n                return - 1 \n        housenumbers = sorted ( doc [ 'housenumbers' ] . values ( ) , key = sorter ) \n        pass "}
{"6372": "\ndef do_BESTSCORE ( self , word ) : \n    key = keys . token_key ( indexed_string ( word ) [ 0 ] ) \n    for _id , score in DB . zrevrange ( key , 0 , 20 , withscores = True ) : \n        result = Result ( _id ) \n        pass "}
{"6373": "\ndef do_STRDISTANCE ( self , s ) : \n    s = s . split ( '|' ) \n    if not len ( s ) == 2 : \n        pass \n        return \n    one , two = s \n    pass "}
{"6569": "\ndef _set_missing_to_none ( self , currency ) : \n    rates = self . _rates [ currency ] \n    first_date , last_date = self . bounds [ currency ] \n    for date in list_dates_between ( first_date , last_date ) : \n        if date not in rates : \n            rates [ date ] = None \n    if self . verbose : \n        missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) \n        if missing : \n            pass "}
{"6570": "\ndef _compute_missing_rates ( self , currency ) : \n    rates = self . _rates [ currency ] \n    tmp = defaultdict ( lambda : [ None , None ] ) \n    for date in sorted ( rates ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist += 1 \n            tmp [ date ] [ 0 ] = closest_rate , dist \n    for date in sorted ( rates , reverse = True ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist += 1 \n            tmp [ date ] [ 1 ] = closest_rate , dist \n    for date in sorted ( tmp ) : \n        ( r0 , d0 ) , ( r1 , d1 ) = tmp [ date ] \n        rates [ date ] = ( r0 * d1 + r1 * d0 ) / ( d0 + d1 ) \n        if self . verbose : \n            pass "}
{"6571": "\ndef _get_rate ( self , currency , date ) : \n    if currency == self . ref_currency : \n        return 1.0 \n    if date not in self . _rates [ currency ] : \n        first_date , last_date = self . bounds [ currency ] \n        if not self . fallback_on_wrong_date : \n            raise RateNotFoundError ( '{0} not in {1} bounds {2}/{3}' . format ( date , currency , first_date , last_date ) ) \n        if date < first_date : \n            fallback_date = first_date \n        elif date > last_date : \n            fallback_date = last_date \n        else : \n            raise AssertionError ( 'Should never happen, bug in the code!' ) \n        if self . verbose : \n            pass \n        date = fallback_date \n    rate = self . _rates [ currency ] [ date ] \n    if rate is None : \n        raise RateNotFoundError ( '{0} has no rate for {1}' . format ( currency , date ) ) \n    return rate "}
{"6612": "\ndef last ( symbol : str ) : \n    app = PriceDbApplication ( ) \n    if symbol : \n        symbol = symbol . upper ( ) \n        sec_symbol = SecuritySymbol ( \"\" , \"\" ) \n        sec_symbol . parse ( symbol ) \n        latest = app . get_latest_price ( sec_symbol ) \n        assert isinstance ( latest , PriceModel ) \n        pass \n    else : \n        latest = app . get_latest_prices ( ) \n        for price in latest : \n            pass "}
{"6613": "\ndef list_prices ( date , currency , last ) : \n    app = PriceDbApplication ( ) \n    app . logger = logger \n    if last : \n        prices = app . get_latest_prices ( ) \n    else : \n        prices = app . get_prices ( date , currency ) \n    for price in prices : \n        pass \n    pass "}
{"6615": "\ndef prune ( symbol : str , all : str ) : \n    app = PriceDbApplication ( ) \n    app . logger = logger \n    count = 0 \n    if symbol is not None : \n        sec_symbol = SecuritySymbol ( \"\" , \"\" ) \n        sec_symbol . parse ( symbol ) \n        deleted = app . prune ( sec_symbol ) \n        if deleted : \n            count = 1 \n    else : \n        count = app . prune_all ( ) \n    pass "}
{"6702": "\ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    pass \n    if name is not None : \n        for result in results : \n            matches = False \n            if name in result . name : \n                matches = True \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = True \n            if matches is True : \n                content = self . _print_log ( result . name ) \n    else : \n        if len ( results ) > 0 : \n            latest = results [ 0 ] \n            for result in results : \n                if result . time_created >= latest . time_created : \n                    latest = result \n            content = self . _print_log ( result . name ) \n    return content "}
{"6707": "\ndef status ( backend ) : \n    pass \n    settings = read_client_secrets ( ) \n    pass \n    if 'SREGISTRY_CLIENT' in settings : \n        pass \n        update_secrets ( settings ) \n    else : \n        pass "}
{"6708": "\ndef add ( backend , variable , value , force = False ) : \n    pass \n    settings = read_client_secrets ( ) \n    prefix = 'SREGISTRY_%s_' % backend . upper ( ) \n    if not variable . startswith ( prefix ) : \n        variable = '%s%s' % ( prefix , variable ) \n    variable = variable . upper ( ) \n    bot . info ( \"%s %s\" % ( variable , value ) ) \n    if backend in settings : \n        if variable in settings [ backend ] and force is False : \n            previous = settings [ backend ] [ variable ] \n            bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) \n            sys . exit ( 1 ) \n    if backend not in settings : \n        settings [ backend ] = { } \n    settings [ backend ] [ variable ] = value \n    update_secrets ( settings ) "}
{"6709": "\ndef remove ( backend , variable ) : \n    pass \n    settings = read_client_secrets ( ) \n    prefixed = variable \n    prefix = 'SREGISTRY_%s_' % backend . upper ( ) \n    if not variable . startswith ( prefix ) : \n        prefixed = '%s%s' % ( prefix , variable ) \n    variable = variable . upper ( ) \n    bot . info ( variable ) \n    if backend in settings : \n        if variable in settings [ backend ] : \n            del settings [ backend ] [ variable ] \n        if prefixed in settings [ backend ] : \n            del settings [ backend ] [ prefixed ] \n        update_secrets ( settings ) "}
{"6710": "\ndef activate ( backend ) : \n    settings = read_client_secrets ( ) \n    if backend is not None : \n        settings [ 'SREGISTRY_CLIENT' ] = backend \n        update_secrets ( settings ) \n        pass "}
{"6711": "\ndef delete_backend ( backend ) : \n    settings = read_client_secrets ( ) \n    if backend in settings : \n        del settings [ backend ] \n        if 'SREGISTRY_CLIENT' in settings : \n            if settings [ 'SREGISTRY_CLIENT' ] == backend : \n                del settings [ 'SREGISTRY_CLIENT' ] \n        update_secrets ( settings ) \n        pass \n    else : \n        if backend is not None : \n            pass \n        else : \n            pass "}
{"6756": "\ndef main ( args , parser , subparser ) : \n    from sregistry . main import get_client \n    images = args . image \n    if not isinstance ( images , list ) : \n        images = [ images ] \n    for image in images : \n        pass \n        cli = get_client ( image , quiet = args . quiet ) \n        cli . announce ( args . command ) \n        cli . share ( image , share_to = args . share_to ) "}
{"6771": "\ndef extract_tar ( archive , output_folder , handle_whiteout = False ) : \n    from . terminal import run_command \n    if handle_whiteout is True : \n        return _extract_tar ( archive , output_folder ) \n    args = '-xf' \n    if archive . endswith ( \".tar.gz\" ) : \n        args = '-xzf' \n    command = [ \"tar\" , args , archive , \"-C\" , output_folder , \"--exclude=dev/*\" ] \n    if not bot . is_quiet ( ) : \n        pass \n    return run_command ( command ) "}
{"6772": "\ndef _extract_tar ( archive , output_folder ) : \n    from . terminal import ( run_command , which ) \n    result = which ( 'blob2oci' ) \n    if result [ 'return_code' ] != 0 : \n        bot . error ( 'Cannot find blob2oci script on path, exiting.' ) \n        sys . exit ( 1 ) \n    script = result [ 'message' ] \n    command = [ 'exec' , script , '--layer' , archive , '--extract' , output_folder ] \n    if not bot . is_quiet ( ) : \n        pass \n    return run_command ( command ) "}
{"6782": "\ndef inspect ( self , name ) : \n    pass \n    container = self . get ( name ) \n    if container is not None : \n        collection = container . collection . name \n        fields = container . __dict__ . copy ( ) \n        fields [ 'collection' ] = collection \n        fields [ 'metrics' ] = json . loads ( fields [ 'metrics' ] ) \n        del fields [ '_sa_instance_state' ] \n        fields [ 'created_at' ] = str ( fields [ 'created_at' ] ) \n        pass \n        return fields "}
{"6787": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    self . require_secrets ( ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20 \n    if names [ 'registry' ] == None : \n        names [ 'registry' ] = self . base \n    names = self . _add_https ( names ) \n    url = '%s/push/' % names [ 'registry' ] \n    auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] \n    SREGISTRY_EVENT = self . authorize ( request_type = \"push\" , names = names ) \n    fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    r = requests . post ( auth_url , json = fields , headers = headers ) \n    message = self . _read_response ( r ) \n    pass \n    if r . status_code != 200 : \n        sys . exit ( 1 ) \n    url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) \n    bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) \n    cid = r . json ( ) [ 'cid' ] \n    upload_to = os . path . basename ( names [ 'storage' ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"upload\" , names = names ) \n    encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) \n    progress_callback = create_callback ( encoder , self . quiet ) \n    monitor = MultipartEncoderMonitor ( encoder , progress_callback ) \n    headers = { 'Content-Type' : monitor . content_type , 'Authorization' : SREGISTRY_EVENT } \n    try : \n        r = requests . post ( url , data = monitor , headers = headers ) \n        r . raise_for_status ( ) \n        message = r . json ( ) [ 'message' ] \n        pass \n    except requests . HTTPError as e : \n        pass \n    except KeyboardInterrupt : \n        pass \n    except Exception as e : \n        pass "}
{"6832": "\ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : \n    f , j = get_f_and_j ( mu ) \n    if nt > 1 : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    if verbose : \n        pass \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , 1 ] , 'g--' ) \n        plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) \n        if savefig == 'None' : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6884": "\ndef fileLoad ( self , filePath = None , updatePath = True ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        raise FileNotFoundError ( \"Data file '%s' does not exist.\" % ( filePath ) ) \n    else : \n        pass \n        with open ( filePath , \"r\" ) as q : \n            data_raw = q . read ( ) \n        pass \n        self . data = json . loads ( data_raw ) \n        if updatePath : \n            self . filePath = filePath "}
{"6885": "\ndef fileSave ( self , filePath = None , updatePath = False ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        pass \n        if not os . path . exists ( os . path . split ( filePath ) [ 0 ] ) : \n            os . makedirs ( os . path . split ( filePath ) [ 0 ] ) \n    dataJsonString = json . dumps ( self . data , indent = 4 , sort_keys = True ) \n    pass \n    with open ( filePath , \"w\" ) as fileout : \n        fileout . write ( dataJsonString ) \n    pass \n    if updatePath : \n        self . filePath = filePath "}
{"6954": "\ndef set_feature_transform ( self , mode = 'polynomial' , degree = 1 ) : \n    if self . status != 'load_train_data' : \n        pass \n        return self . train_X \n    self . feature_transform_mode = mode \n    self . feature_transform_degree = degree \n    self . train_X = self . train_X [ : , 1 : ] \n    self . train_X = utility . DatasetLoader . feature_transform ( self . train_X , self . feature_transform_mode , self . feature_transform_degree ) \n    return self . train_X "}
{"6955": "\ndef prediction ( self , input_data = '' , mode = 'test_data' ) : \n    prediction = { } \n    if ( self . status != 'train' ) : \n        pass \n        return prediction \n    if ( input_data == '' ) : \n        pass \n        return prediction \n    if mode == 'future_data' : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : None , \"prediction\" : prediction } \n    else : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data [ : - 1 ] ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        input_data_y = float ( data [ - 1 ] ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : input_data_y , \"prediction\" : prediction } "}
{"7047": "\ndef parse_log ( self , bowtie_log ) : \n    pass \n    regexes = { 'unpaired' : { 'unpaired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" , 'unpaired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'unpaired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" } , 'paired' : { 'paired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly 0 times\" , 'paired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly exactly 1 time\" , 'paired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly >1 times\" , 'paired_aligned_discord_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly 1 time\" , 'paired_aligned_discord_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly >1 times\" , 'paired_aligned_mate_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'paired_aligned_mate_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" , 'paired_aligned_mate_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" } } \n    with open ( bowtie_log , \"r\" ) as f : \n        for l in f : \n            pass \n            total = re . search ( r\"(\\\\d+) reads; of these:\" , l ) \n            pass \n            if total : \n                pass \n                self . set_n_reads ( total . group ( 1 ) ) \n            paired = re . search ( r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) were paired; of these:\" , l ) \n            if paired : \n                paired_total = int ( paired . group ( 1 ) ) \n                paired_numbers = { } \n                l = f . readline ( ) \n                while l . startswith ( '    ' ) : \n                    for k , r in regexes [ 'paired' ] . items ( ) : \n                        match = re . search ( r , l ) \n                        if match : \n                            paired_numbers [ k ] = int ( match . group ( 1 ) ) \n                    l = f . readline ( ) \n                align_zero_times = paired_numbers [ 'paired_aligned_none' ] + paired_numbers [ 'paired_aligned_mate_none' ] \n                if align_zero_times : \n                    self . set_align_0x ( align_zero_times ) \n                align_one_time = paired_numbers [ 'paired_aligned_one' ] + paired_numbers [ 'paired_aligned_mate_one' ] \n                if align_one_time : \n                    self . set_align_1x ( align_one_time ) \n                align_more_than_one_time = paired_numbers [ 'paired_aligned_multi' ] + paired_numbers [ 'paired_aligned_mate_multi' ] \n                if align_more_than_one_time : \n                    self . set_align_mt1x ( align_more_than_one_time ) \n            overall = re . search ( r\"([\\\\d\\\\.]+)% overall alignment rate\" , l ) \n            if overall : \n                self . overall_rate = float ( overall . group ( 1 ) ) "}
{"7098": "\ndef main ( newick ) : \n    logger . info ( \"Starting newick file processing\" ) \n    pass \n    tree = dendropy . Tree . get ( file = open ( newick , 'r' ) , schema = \"newick\" ) \n    tree . reroot_at_midpoint ( ) \n    to_write = tree . as_string ( \"newick\" ) . strip ( ) . replace ( \"[&R] \" , '' ) . replace ( ' ' , '_' ) . replace ( \"'\" , \"\" ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_dic = { \"treeData\" : [ { \"trees\" : [ to_write ] } ] , } \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        status_fh . write ( \"pass\" ) "}
{"7192": "\ndef print_table ( language ) : \n    table = translation_table ( language ) \n    for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( 0 ) ) : \n        pass \n    return None "}
{"7196": "\ndef set ( self , node_ids , variable = None , name = \"tmp\" ) : \n    if variable is None : \n        variable = pd . Series ( np . ones ( len ( node_ids ) ) , index = node_ids . index ) \n    df = pd . DataFrame ( { name : variable , \"node_idx\" : self . _node_indexes ( node_ids ) } ) \n    length = len ( df ) \n    df = df . dropna ( how = \"any\" ) \n    newl = len ( df ) \n    if length - newl > 0 : \n        pass \n    self . variable_names . add ( name ) \n    self . net . initialize_access_var ( name . encode ( 'utf-8' ) , df . node_idx . values . astype ( 'int' ) , df [ name ] . values . astype ( 'double' ) ) "}
{"7262": "\ndef find_best_root ( self , force_positive = True , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if n == self . tree . root : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( chisq < best_root [ \"chisq\" ] ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( 1 - x ) , var * ( 1 - x ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if reg [ \"slope\" ] >= 0 or ( force_positive == False ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        pass \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( 1 - y ) , var * ( 1 - y ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3 , 3 ) ) \n        estimator_hessian [ : 2 , : 2 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2 , 2 ] = ( deriv [ 0 ] [ 1 ] + deriv [ 1 ] [ 1 ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ 0 ] [ 0 ] - deriv [ 1 ] [ 0 ] ) ** 2 \n        estimator_hessian [ 0 , 2 ] = estimator_hessian [ 2 , 0 ] \n        estimator_hessian [ 1 , 2 ] = estimator_hessian [ 2 , 1 ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7287": "\ndef optimize_branch_length_global ( self , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length_global: running branch length optimization...\" , 1 ) \n    def neg_log ( s ) : \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            n . branch_length = si ** 2 \n        self . infer_ancestral_sequences ( marginal = True ) \n        gradient = [ ] \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            if n . up : \n                pp , pc = self . marginal_branch_profile ( n ) \n                Qtds = self . gtr . expQsds ( si ) . T \n                Qt = self . gtr . expQs ( si ) . T \n                res = pp . dot ( Qt ) \n                overlap = np . sum ( res * pc , axis = 1 ) \n                res_ds = pp . dot ( Qtds ) \n                overlap_ds = np . sum ( res_ds * pc , axis = 1 ) \n                logP = np . sum ( self . multiplicity * overlap_ds / overlap ) \n                gradient . append ( logP ) \n            else : \n                gradient . append ( 2 * ( si ** 2 - 0.001 ) ) \n        pass \n        return ( - self . tree . sequence_marginal_LH + ( s [ 0 ] ** 2 - 0.001 ) ** 2 , - 1.0 * np . array ( gradient ) ) \n    from scipy . optimize import minimize \n    x0 = np . sqrt ( [ n . branch_length for n in self . tree . find_clades ( order = 'preorder' ) ] ) \n    sol = minimize ( neg_log , x0 , jac = True ) \n    for new_len , node in zip ( sol [ 'x' ] , self . tree . find_clades ( ) ) : \n        self . logger ( \"Optimization results: old_len=%.4f, new_len=%.4f \" \" Updating branch length...\" % ( node . branch_length , new_len ) , 5 ) \n        node . branch_length = new_len ** 2 \n        node . mutation_length = new_len ** 2 \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    self . _prepare_nodes ( ) "}
{"7294": "\ndef _check_fix_Q ( self , fixed_mu = False ) : \n    self . Pi /= self . Pi . sum ( ) \n    self . W += self . break_degen + self . break_degen . T \n    np . fill_diagonal ( self . W , 0 ) \n    Wdiag = - ( self . Q ) . sum ( axis = 0 ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W /= scale_factor \n    if not fixed_mu : \n        self . mu *= scale_factor \n    if ( self . Q . sum ( axis = 0 ) < 1e-10 ) . sum ( ) < self . alphabet . shape [ 0 ] : \n        pass \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7297": "\ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = False , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2 \n        if 'success' not in opt : \n            opt [ 'success' ] = True \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4 , warn = True ) \n    except : \n        import scipy \n        pass \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2 \n        opt = { 'success' : True } \n    if new_len > .9 * ttconf . MAX_BRANCH_LENGTH : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4 , warn = True ) \n    if opt [ \"success\" ] != True : \n        new_len = np . sum ( multiplicity [ seq_pair [ : , 1 ] != seq_pair [ : , 0 ] ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7305": "\ndef print_lh ( self , joint = True ) : \n    try : \n        u_lh = self . tree . unconstrained_sequence_LH \n        if joint : \n            s_lh = self . tree . sequence_joint_LH \n            t_lh = self . tree . positional_joint_LH \n            c_lh = self . tree . coalescent_joint_LH \n        else : \n            s_lh = self . tree . sequence_marginal_LH \n            t_lh = self . tree . positional_marginal_LH \n            c_lh = 0 \n        pass \n    except : \n        pass "}
{"7308": "\ndef assure_tree ( params , tmp_dir = 'treetime_tmp' ) : \n    if params . tree is None : \n        params . tree = os . path . basename ( params . aln ) + '.nwk' \n        pass \n        utils . tree_inference ( params . aln , params . tree , tmp_dir = tmp_dir ) \n    if os . path . isdir ( tmp_dir ) : \n        shutil . rmtree ( tmp_dir ) \n    try : \n        tt = TreeAnc ( params . tree ) \n    except : \n        pass \n        return 1 \n    return 0 "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2 : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    elif keyval [ 0 ] not in [ 'alphabet' ] : \n                        keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                pass \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = False \n        except : \n            pass \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = False \n    return gtr "}
{"7310": "\ndef read_if_vcf ( params ) : \n    ref = None \n    aln = params . aln \n    fixed_pi = None \n    if hasattr ( params , 'aln' ) and params . aln is not None : \n        if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : \n            if not params . vcf_reference : \n                pass \n                return - 1 \n            compress_seq = read_vcf ( params . aln , params . vcf_reference ) \n            sequences = compress_seq [ 'sequences' ] \n            ref = compress_seq [ 'reference' ] \n            aln = sequences \n            if not hasattr ( params , 'gtr' ) or params . gtr == \"infer\" : \n                alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] \n                fixed_pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] \n                if fixed_pi [ - 1 ] == 0 : \n                    fixed_pi [ - 1 ] = 0.05 \n                    fixed_pi = [ v - 0.01 for v in fixed_pi ] \n    return aln , ref , fixed_pi "}
{"7311": "\ndef ancestral_reconstruction ( params ) : \n    if assure_tree ( params , tmp_dir = 'ancestral_tmp' ) : \n        return 1 \n    outdir = get_outdir ( params , '_ancestral' ) \n    basename = get_basename ( params , outdir ) \n    gtr = create_gtr ( params ) \n    aln , ref , fixed_pi = read_if_vcf ( params ) \n    is_vcf = True if ref is not None else False \n    treeanc = TreeAnc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = 1 , fill_overhangs = not params . keep_overhangs ) \n    ndiff = treeanc . infer_ancestral_sequences ( 'ml' , infer_gtr = params . gtr == 'infer' , marginal = params . marginal , fixed_pi = fixed_pi ) \n    if ndiff == ttconf . ERROR : \n        return 1 \n    if params . gtr == \"infer\" : \n        pass \n        pass \n    export_sequences_and_tree ( treeanc , basename , is_vcf , params . zero_based , report_ambiguous = params . report_ambiguous ) \n    return 0 "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( log_prob < 0.693147 ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if L < 2 : \n        pass \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if n_delta > 1 : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif n_delta == 1 : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n        peak = y_vals . min ( ) \n        ind = ( y_vals - peak ) < BIG_NUMBER / 1000 \n        n_points = ind . sum ( ) \n        if n_points == 0 : \n            pass \n            x_vals = [ 0 , 1 ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n        elif n_points == 1 : \n            res = Distribution . delta_function ( x_vals [ 0 ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7342": "\ndef interactive_login ( ) : \n    solvebio . access_token = None \n    solvebio . api_key = None \n    client . set_token ( ) \n    domain , email , password = _ask_for_credentials ( ) \n    if not all ( [ domain , email , password ] ) : \n        pass \n        return \n    try : \n        response = client . post ( '/v1/auth/token' , { 'domain' : domain . replace ( '.solvebio.com' , '' ) , 'email' : email , 'password' : password } ) \n    except SolveError as e : \n        pass \n    else : \n        solvebio . api_key = response [ 'token' ] \n        client . set_token ( ) "}
{"7343": "\ndef whoami ( * args , ** kwargs ) : \n    user = client . whoami ( ) \n    if user : \n        print_user ( user ) \n    else : \n        pass "}
{"7344": "\ndef print_user ( user ) : \n    email = user [ 'email' ] \n    domain = user [ 'account' ] [ 'domain' ] \n    role = user [ 'role' ] \n    pass "}
{"7354": "\ndef download_vault_folder ( remote_path , local_path , dry_run = False , force = False ) : \n    local_path = os . path . normpath ( os . path . expanduser ( local_path ) ) \n    if not os . access ( local_path , os . W_OK ) : \n        raise Exception ( 'Write access to local path ({}) is required' . format ( local_path ) ) \n    full_path , path_dict = solvebio . Object . validate_full_path ( remote_path ) \n    vault = solvebio . Vault . get_by_full_path ( path_dict [ 'vault' ] ) \n    pass \n    if path_dict [ 'path' ] == '/' : \n        parent_object_id = None \n    else : \n        parent_object = solvebio . Object . get_by_full_path ( remote_path , assert_type = 'folder' ) \n        parent_object_id = parent_object . id \n    pass \n    if not os . path . exists ( local_path ) : \n        if not dry_run : \n            os . makedirs ( local_path ) \n    folders = vault . folders ( parent_object_id = parent_object_id ) \n    for f in folders : \n        path = os . path . normpath ( local_path + f . path ) \n        if not os . path . exists ( path ) : \n            pass \n            if not dry_run : \n                os . makedirs ( path ) \n    files = vault . files ( parent_object_id = parent_object_id ) \n    for f in files : \n        path = os . path . normpath ( local_path + f . path ) \n        if os . path . exists ( path ) : \n            if force : \n                pass \n                if not dry_run : \n                    os . remove ( path ) \n            else : \n                pass \n                continue \n        pass \n        if not dry_run : \n            f . download ( path ) "}
{"7443": "\ndef cli ( infile ) : \n    lines = get_file_handle ( infile ) \n    cytobands = parse_cytoband ( lines ) \n    pass \n    pass \n    intervals = cytobands [ '1' ] [ 2 ] \n    for interval in intervals : \n        pass \n        pass \n        pass \n        pass \n    pass \n    pass \n    pass \n    pass \n    pass "}
{"7570": "\ndef migrate ( uri : str , archive_uri : str , case_id : str , dry : bool , force : bool ) : \n    scout_client = MongoClient ( uri ) \n    scout_database = scout_client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] \n    scout_adapter = MongoAdapter ( database = scout_database ) \n    scout_case = scout_adapter . case ( case_id ) \n    if not force and scout_case . get ( 'is_migrated' ) : \n        pass \n        return \n    archive_client = MongoClient ( archive_uri ) \n    archive_database = archive_client [ archive_uri . rsplit ( '/' , 1 ) [ - 1 ] ] \n    archive_case = archive_database . case . find_one ( { 'owner' : scout_case [ 'owner' ] , 'display_name' : scout_case [ 'display_name' ] } ) \n    archive_data = archive_info ( archive_database , archive_case ) \n    if dry : \n        pass \n    else : \n        pass "}
{"7573": "\ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while len ( term ) < 7 : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    elif description : \n        sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n        for term in sorted_terms : \n            term . pop ( 'genes' ) \n            pass \n        context . abort ( ) \n    else : \n        hpo_terms = adapter . hpo_terms ( ) \n    if hpo_terms . count ( ) == 0 : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7773": "\ndef cli ( context , morbid , genemap , mim2gene , mim_titles , phenotypes ) : \n    from scout . utils . handle import get_file_handle \n    from pprint import pprint as pp \n    pass \n    pass \n    pass \n    pass \n    if morbid : \n        morbid_handle = get_file_handle ( morbid ) \n    if genemap : \n        genemap_handle = get_file_handle ( genemap ) \n    if mim2gene : \n        mim2gene_handle = get_file_handle ( mim2gene ) \n    if mim_titles : \n        mimtitles_handle = get_file_handle ( mim_titles ) \n    mim_genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for entry in mim_genes : \n        if entry == 'C10orf11' : \n            pp ( mim_genes [ entry ] ) \n    context . abort ( ) \n    if phenotypes : \n        if not genemap : \n            click . echo ( \"Please provide the genemap file\" ) \n            context . abort ( ) \n        phenotypes = get_mim_phenotypes ( genemap_handle ) \n        for i , mim_term in enumerate ( phenotypes ) : \n            pass \n    pass \n    context . abort ( ) \n    genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for hgnc_symbol in genes : \n        if hgnc_symbol == 'OPA1' : \n            pass "}
{"7799": "\ndef read_hdf5 ( self , filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    pass \n    self . header = { } \n    self . filename = filename \n    self . h5 = h5py . File ( filename ) \n    for key , val in self . h5 [ b'data' ] . attrs . items ( ) : \n        if six . PY3 : \n            key = bytes ( key , 'ascii' ) \n        if key == b'src_raj' : \n            self . header [ key ] = Angle ( val , unit = 'hr' ) \n        elif key == b'src_dej' : \n            self . header [ key ] = Angle ( val , unit = 'deg' ) \n        else : \n            self . header [ key ] = val \n    self . n_ints_in_file = self . h5 [ b\"data\" ] . shape [ 0 ] \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    if load_data : \n        self . data = self . h5 [ b\"data\" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] \n        self . file_size_bytes = os . path . getsize ( self . filename ) \n    else : \n        pass \n        self . data = np . array ( [ 0 ] ) \n        self . n_ints_in_file = 0 \n        self . file_size_bytes = os . path . getsize ( self . filename ) "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            pass \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        pass \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7805": "\ndef info ( self ) : \n    for key , val in self . header . items ( ) : \n        if key == b'src_raj' : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if key == b'src_dej' : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        if key == b'tsamp' : \n            val *= u . second \n        if key in ( 'foff' , 'fch1' ) : \n            val *= u . MHz \n        if key == b'tstart' : \n            pass \n            key = \"tstart (MJD)\" \n        pass \n    pass \n    pass \n    pass \n    pass "}
{"7809": "\ndef write_to_filterbank ( self , filename_out ) : \n    pass \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    with open ( filename_out , \"wb\" ) as fileh : \n        fileh . write ( generate_sigproc_header ( self ) ) \n        j = self . data \n        if n_bytes == 4 : \n            np . float32 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 2 : \n            np . int16 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 1 : \n            np . int8 ( j . ravel ( ) ) . tofile ( fileh ) "}
{"7813": "\ndef calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = True , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( diode_cross , max_load = 150 ) \n    cross_dat = obs . data \n    tsamp = obs . header [ 'tsamp' ] \n    dio_ncoarse = obs . calc_n_coarse_chan ( ) \n    dio_nchans = obs . header [ 'nchans' ] \n    dio_chan_per_coarse = dio_nchans / dio_ncoarse \n    obs = None \n    Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) \n    cross_dat = None \n    pass \n    gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    Idat = None \n    Qdat = None \n    Udat = None \n    Vdat = None \n    pass \n    cross_obs = Waterfall ( cross_pols , max_load = 150 ) \n    obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) \n    obs_nchans = cross_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = obs_nchans / obs_ncoarse \n    pass \n    I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) \n    pass \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) \n    if onefile == True : \n        cross_obs . data [ : , 0 , : ] = np . squeeze ( I ) \n        cross_obs . data [ : , 1 , : ] = np . squeeze ( Q ) \n        cross_obs . data [ : , 2 , : ] = np . squeeze ( U ) \n        cross_obs . data [ : , 3 , : ] = np . squeeze ( V ) \n        cross_obs . write_to_fil ( cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        pass \n        return \n    obs = Waterfall ( obs_I , max_load = 150 ) \n    obs . data = I \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    pass \n    obs . data = Q \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    pass \n    obs . data = U \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    pass \n    obs . data = V \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.V.polcal.fil' ) \n    pass "}
{"7823": "\ndef open_file ( filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True , max_load = 1. ) : \n    if not os . path . isfile ( filename ) : \n        type ( filename ) \n        pass \n        raise IOError ( \"No such file or directory: \" + filename ) \n    filename = os . path . expandvars ( os . path . expanduser ( filename ) ) \n    ext = filename . split ( \".\" ) [ - 1 ] . strip ( ) . lower ( ) \n    if six . PY3 : \n        ext = bytes ( ext , 'ascii' ) \n    if h5py . is_hdf5 ( filename ) : \n        return H5Reader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    elif sigproc . is_filterbank ( filename ) : \n        return FilReader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    else : \n        raise NotImplementedError ( 'Cannot open this type of file with Waterfall' ) "}
{"7839": "\ndef info ( self ) : \n    pass \n    for key , val in self . file_header . items ( ) : \n        if key == 'src_raj' : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if key == 'src_dej' : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    pass "}
{"7849": "\ndef print_stats ( self ) : \n    header , data = self . read_next_data_block ( ) \n    data = data . view ( 'float32' ) \n    pass \n    pass \n    pass \n    pass \n    import pylab as plt "}
{"7853": "\ndef cmd_tool ( args = None ) : \n    if 'bl' in local_host : \n        header_loc = '/usr/local/sigproc/bin/header' \n    else : \n        raise IOError ( 'Script only able to run in BL systems.' ) \n    p = OptionParser ( ) \n    p . set_usage ( 'matchfils <FIL_FILE1> <FIL_FILE2>' ) \n    opts , args = p . parse_args ( sys . argv [ 1 : ] ) \n    file1 = args [ 0 ] \n    file2 = args [ 1 ] \n    make_batch_script ( ) \n    headersize1 = find_header_size ( file1 ) \n    file_size1 = os . path . getsize ( file1 ) \n    command = [ './tail_sum.sh' , file1 , str ( file_size1 - headersize1 ) ] \n    pass \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum1 = out . split ( ) [ 0 ] \n    pass \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file1 ] \n    pass \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header1 = out \n    pass \n    out , err = reset_outs ( ) \n    headersize2 = find_header_size ( file2 ) \n    file_size2 = os . path . getsize ( file2 ) \n    command = [ './tail_sum.sh' , file2 , str ( file_size2 - headersize2 ) ] \n    pass \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum2 = out . split ( ) [ 0 ] \n    pass \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file2 ] \n    pass \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header2 = out \n    pass \n    if check_sum1 != check_sum2 : \n        pass \n    else : \n        pass \n    os . remove ( 'tail_sum.sh' ) "}
{"7854": "\ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    if not HAS_BITSHUFFLE : \n        pass \n        exit ( ) \n    parser = ArgumentParser ( description = \"Command line utility for creating HDF5 Raw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) \n    args = parser . parse_args ( ) \n    fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] \n    filelist = glob . glob ( fileroot + '*.raw' ) \n    filelist = sorted ( filelist ) \n    r = GuppiRaw ( filelist [ 0 ] ) \n    header , data = r . read_next_data_block ( ) \n    dshape = data . shape \n    pass \n    n_blocks_total = 0 \n    for filename in filelist : \n        pass \n        r = GuppiRaw ( filename ) \n        n_blocks_total += r . n_blocks \n    pass \n    full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) \n    h5 = h5py . File ( fileroot + '.h5' , 'w' ) \n    h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' \n    block_size = 0 \n    dset = h5 . create_dataset ( 'data' , shape = full_dshape , dtype = data . dtype ) \n    h5_idx = 0 \n    for filename in filelist : \n        pass \n        r = GuppiRaw ( filename ) \n        h5 = h5py . File ( filename + '.h5' , 'w' ) \n        header , data = r . read_next_data_block ( ) \n        for ii in range ( 0 , r . n_blocks ) : \n            t0 = time . time ( ) \n            pass \n            header , data = r . read_next_data_block ( ) \n            t1 = time . time ( ) \n            t2 = time . time ( ) \n            pass \n            dset [ h5_idx , : ] = data \n            t3 = time . time ( ) \n            pass \n            h5_idx += 1 \n            for key , value in header . items ( ) : \n                dset . attrs [ key ] = value \n        h5 . close ( ) \n        t1 = time . time ( ) \n        pass "}
{"7862": "\ndef calibrate_fluxes ( main_obs_name , dio_name , dspec , Tsys , fullstokes = False , ** kwargs ) : \n    main_obs = Waterfall ( main_obs_name , max_load = 150 ) \n    ncoarse = main_obs . calc_n_coarse_chan ( ) \n    dio_obs = Waterfall ( dio_name , max_load = 150 ) \n    dio_chan_per_coarse = dio_obs . header [ 'nchans' ] / ncoarse \n    dOFF , dON = integrate_calib ( dio_name , dio_chan_per_coarse , fullstokes , ** kwargs ) \n    main_dat = main_obs . data \n    scale_facs = dspec / ( dON - dOFF ) \n    pass \n    nchans = main_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = nchans / ncoarse \n    ax0_size = np . size ( main_dat , 0 ) \n    ax1_size = np . size ( main_dat , 1 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , ncoarse , obs_chan_per_coarse ) ) \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = main_dat * scale_facs \n    main_dat = main_dat - Tsys \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , nchans ) ) \n    main_obs . data = main_dat \n    main_obs . write_to_filterbank ( main_obs_name [ : - 4 ] + '.fluxcal.fil' ) \n    pass "}
{"7904": "\ndef calcParallaxError ( args ) : \n    gmag = float ( args [ 'gmag' ] ) \n    vmini = float ( args [ 'vmini' ] ) \n    sigmaPar = parallaxErrorSkyAvg ( gmag , vmini ) \n    gminv = gminvFromVmini ( vmini ) \n    pass \n    pass \n    pass \n    pass \n    pass "}
{"7931": "\ndef report ( self , output_file = sys . stdout ) : \n    max_perf = self . results [ 'max_perf' ] \n    if self . _args and self . _args . verbose >= 3 : \n        pass \n    if self . _args and self . _args . verbose >= 1 : \n        pass \n        pass \n        pass \n        pass \n        pass \n        for b in self . results [ 'mem bottlenecks' ] : \n            pass \n        pass \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > max_perf [ 'FLOP/s' ] : \n        pass \n    else : \n        pass \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        pass \n        pass "}
{"7932": "\ndef report ( self , output_file = sys . stdout ) : \n    cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] \n    if self . verbose >= 3 : \n        pass \n    if self . verbose >= 1 : \n        pass \n        pass \n        pass \n        pass \n        for b in self . results [ 'mem bottlenecks' ] : \n            if b is None : \n                continue \n            pass \n        pass \n        pass \n        pass \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu_perf [ 'FLOP/s' ] : \n        pass \n    else : \n        pass \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        pass \n        pass "}
{"7933": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and self . _args . verbose > 2 : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        pass \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            pass \n            if lc_solution [ 'lt' ] is sympy . true : \n                pass \n            else : \n                if lc_solution [ 'eq' ] is None : \n                    pass \n                elif type ( lc_solution [ 'eq' ] ) is not list : \n                    pass \n                else : \n                    for solu in lc_solution [ 'eq' ] : \n                        for s , v in solu . items ( ) : \n                            pass "}
{"7940": "\ndef analyze ( self ) : \n    try : \n        incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = self . verbose > 2 ) \n    except RuntimeError as e : \n        pass \n        sys . exit ( 1 ) \n    block_throughput = incore_analysis [ 'throughput' ] \n    port_cycles = incore_analysis [ 'port cycles' ] \n    uops = incore_analysis [ 'uops' ] \n    elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) \n    block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] \n    try : \n        block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size \n    except ZeroDivisionError as e : \n        pass \n        sys . exit ( 1 ) \n    port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) \n    uops = uops * block_to_cl_ratio \n    cl_throughput = block_throughput * block_to_cl_ratio \n    T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) \n    T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) \n    if T_nOL < cl_throughput : \n        T_OL = cl_throughput \n    self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } \n    return self . results "}
{"7944": "\ndef userselect_increment ( block ) : \n    pass \n    pass \n    pass \n    increment = None \n    while increment is None : \n        increment = input ( \"Choose store pointer increment (number of bytes): \" ) \n        try : \n            increment = int ( increment ) \n        except ValueError : \n            increment = None \n    block [ 'pointer_increment' ] = increment \n    return increment "}
{"7945": "\ndef userselect_block ( blocks , default = None , debug = False ) : \n    pass \n    pass \n    for idx , b in blocks : \n        pass \n        if debug : \n            ln = b [ 'first_line' ] \n            pass \n            for l in b [ 'lines' ] : \n                pass \n                ln += 1 \n            pass \n            pass \n    block_idx = - 1 \n    while not ( 0 <= block_idx < len ( blocks ) ) : \n        block_idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - 1 \n    return block_idx "}
{"7975": "\ndef print_kernel_info ( self , output_file = sys . stdout ) : \n    table = ( '     idx |        min        max       step\\n' + '---------+---------------------------------\\n' ) \n    for l in self . _loop_stack : \n        table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\\n' . format ( * l ) \n    pass \n    table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) \n    for name , offsets in list ( self . sources . items ( ) ) : \n        prefix = '{:>8} | ' . format ( name ) \n        right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) \n        table += prefix_indent ( prefix , right_side , later_prefix = '         | ' ) \n    pass \n    table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) \n    for name , offsets in list ( self . destinations . items ( ) ) : \n        prefix = '{:>8} | ' . format ( name ) \n        right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) \n        table += prefix_indent ( prefix , right_side , later_prefix = '         | ' ) \n    pass \n    table = ( ' op | count \\n' + '----+-------\\n' ) \n    for op , count in list ( self . _flops . items ( ) ) : \n        table += '{:>3} | {:>4}\\n' . format ( op , count ) \n    table += '     =======\\n' \n    table += '      {:>4}' . format ( sum ( self . _flops . values ( ) ) ) \n    pass "}
{"7976": "\ndef print_variables_info ( self , output_file = sys . stdout ) : \n    table = ( '    name |   type size             \\n' + '---------+-------------------------\\n' ) \n    for name , var_info in list ( self . variables . items ( ) ) : \n        table += '{:>8} | {:>6} {!s:<10}\\n' . format ( name , var_info [ 0 ] , var_info [ 1 ] ) \n    pass "}
{"7977": "\ndef print_constants_info ( self , output_file = sys . stdout ) : \n    table = ( '    name | value     \\n' + '---------+-----------\\n' ) \n    for name , value in list ( self . constants . items ( ) ) : \n        table += '{!s:>8} | {:<10}\\n' . format ( name , value ) \n    pass "}
{"7978": "\ndef print_kernel_code ( self , output_file = sys . stdout ) : \n    pass "}
{"7996": "\ndef build_executable ( self , lflags = None , verbose = False , openmp = False ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = True ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            pass \n            sys . exit ( 1 ) \n        compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] \n        if os . environ . get ( 'LIKWID_LIB' ) == '' : \n            compiler_args = compiler_args [ : - 1 ] \n        if lflags is None : \n            lflags = [ ] \n        lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            pass \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            pass \n            sys . exit ( 1 ) \n    else : \n        if verbose : \n            pass \n    return out_filename "}
{"8015": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . verbose > 1 : \n        with pprint_nosort ( ) : \n            pprint . pprint ( self . results ) \n    if self . verbose > 0 : \n        pass \n    if self . verbose > 0 : \n        pass \n    pass \n    pass \n    pass \n    pass \n    pass \n    if self . verbose > 0 : \n        pass \n    pass \n    if not self . no_phenoecm : \n        pass \n        pass \n        for metrics in self . results [ 'data transfers' ] . values ( ) : \n            for metric_name in sorted ( metrics ) : \n                pass \n            pass \n            break \n        for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : \n            pass \n            for k , v in sorted ( metrics . items ( ) ) : \n                pass \n            pass \n        pass \n        pass \n        pass "}
{"8097": "\ndef calc ( pvalues , lamb ) : \n    m = len ( pvalues ) \n    pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) \n    pFDR = np . ones ( m ) \n    pass \n    for i in range ( m ) : \n        y = pvalues [ i ] \n        Pr = max ( 1 , m - i ) / float ( m ) \n        pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) \n        pass \n    num_null = pi0 * m \n    num_alt = m - num_null \n    num_negs = np . array ( range ( m ) ) \n    num_pos = m - num_negs \n    pp = num_pos / float ( m ) \n    qvalues = np . ones ( m ) \n    qvalues [ 0 ] = pFDR [ 0 ] \n    for i in range ( m - 1 ) : \n        qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) \n    sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt \n    sens [ sens > 1.0 ] = 1.0 \n    df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) \n    df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] \n    return df , num_null , m "}
{"8248": "\ndef start ( self ) : \n    if sys . stdout is not self : \n        self . _original_steam = sys . stdout \n        sys . stdout = self \n        self . _redirection = True \n    if self . _redirection : \n        pass "}
{"8259": "\ndef run_net ( traj ) : \n    eqs = traj . eqs \n    namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) \n    neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) \n    neuron . vm = traj . EL \n    neuron . w = traj . a * ( neuron . vm - traj . EL ) \n    neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) \n    pass \n    net = Network ( neuron ) \n    net . run ( 100 * ms , report = 'text' ) \n    MSpike = SpikeMonitor ( neuron ) \n    net . add ( MSpike ) \n    MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) \n    net . add ( MStateV ) \n    pass \n    net . run ( 500 * ms , report = 'text' ) \n    traj . v_standard_result = Brian2MonitorResult \n    traj . f_add_result ( 'SpikeMonitor' , MSpike ) \n    traj . f_add_result ( 'StateMonitorV' , MStateV ) "}
{"8267": "\ndef compact_hdf5_file ( filename , name = None , index = None , keep_backup = True ) : \n    if name is None and index is None : \n        index = - 1 \n    tmp_traj = load_trajectory ( name , index , as_new = False , load_all = pypetconstants . LOAD_NOTHING , force = True , filename = filename ) \n    service = tmp_traj . v_storage_service \n    complevel = service . complevel \n    complib = service . complib \n    shuffle = service . shuffle \n    fletcher32 = service . fletcher32 \n    name_wo_ext , ext = os . path . splitext ( filename ) \n    tmp_filename = name_wo_ext + '_tmp' + ext \n    abs_filename = os . path . abspath ( filename ) \n    abs_tmp_filename = os . path . abspath ( tmp_filename ) \n    command = [ 'ptrepack' , '-v' , '--complib' , complib , '--complevel' , str ( complevel ) , '--shuffle' , str ( int ( shuffle ) ) , '--fletcher32' , str ( int ( fletcher32 ) ) , abs_filename , abs_tmp_filename ] \n    str_command = ' ' . join ( command ) \n    pass \n    retcode = subprocess . call ( command ) \n    if retcode != 0 : \n        pass \n    else : \n        pass \n        pass \n        if keep_backup : \n            backup_file_name = name_wo_ext + '_backup' + ext \n            os . rename ( filename , backup_file_name ) \n        else : \n            os . remove ( filename ) \n        os . rename ( tmp_filename , filename ) \n        pass \n    return retcode "}
{"8278": "\ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if len ( subrun_list ) == 0 : \n        spikes_e = traj . results . monitors . spikes_e \n        time_window = traj . parameters . analysis . statistics . time_window \n        start_time = traj . parameters . simulation . durations . initial_run \n        end_time = start_time + traj . parameters . simulation . durations . measurement_run \n        neuron_ids = traj . parameters . analysis . statistics . neuron_ids \n        mean_ff = self . _compute_mean_fano_factor ( neuron_ids , spikes_e , time_window , start_time , end_time ) \n        traj . f_add_result ( 'statistics.mean_fano_factor' , mean_ff , comment = 'Average Fano ' 'Factor over all ' 'exc neurons' ) \n        pass "}
{"8283": "\ndef _print_graphs ( self , traj ) : \n    print_folder = self . _make_folder ( traj ) \n    plt . figure ( ) \n    plt . scatter ( self . spike_monitor . t , self . spike_monitor . i , s = 1 ) \n    plt . xlabel ( 't' ) \n    plt . ylabel ( 'Exc. Neurons' ) \n    plt . title ( 'Spike Raster Plot' ) \n    filename = os . path . join ( print_folder , 'spike.png' ) \n    pass \n    plt . savefig ( filename ) \n    plt . close ( ) \n    fig = plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.V' ) \n    filename = os . path . join ( print_folder , 'V.png' ) \n    pass \n    fig . savefig ( filename ) \n    plt . close ( ) \n    plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.I_syn_e' ) \n    filename = os . path . join ( print_folder , 'I_syn_e.png' ) \n    pass \n    plt . savefig ( filename ) \n    plt . close ( ) \n    plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.I_syn_i' ) \n    filename = os . path . join ( print_folder , 'I_syn_i.png' ) \n    pass \n    plt . savefig ( filename ) \n    plt . close ( ) \n    if not traj . analysis . show_plots : \n        plt . close ( 'all' ) \n    else : \n        plt . show ( ) "}
{"8284": "\ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if len ( subrun_list ) == 0 : \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.spikes_e' , self . spike_monitor , comment = 'The spiketimes of the excitatory population' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.V' , self . V_monitor , comment = 'Membrane voltage of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_e' , self . I_syn_e_monitor , comment = 'I_syn_e of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_i' , self . I_syn_i_monitor , comment = 'I_syn_i of four neurons from 2 clusters' ) \n        pass \n        if traj . parameters . analysis . make_plots : \n            self . _print_graphs ( traj ) "}
{"8285": "\ndef get_batch ( ) : \n    optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) \n    batch = 0 \n    for o , a in optlist : \n        if o == '--batch' : \n            batch = int ( a ) \n            pass \n    return batch "}
{"8351": "\ndef port_to_tcp ( port = None ) : \n    domain_name = socket . getfqdn ( ) \n    try : \n        addr_list = socket . getaddrinfo ( domain_name , None ) \n    except Exception : \n        addr_list = socket . getaddrinfo ( '127.0.0.1' , None ) \n    family , socktype , proto , canonname , sockaddr = addr_list [ 0 ] \n    host = convert_ipv6 ( sockaddr [ 0 ] ) \n    address = 'tcp://' + host \n    if port is None : \n        port = ( ) \n    if not isinstance ( port , int ) : \n        context = zmq . Context ( ) \n        try : \n            socket_ = context . socket ( zmq . REP ) \n            socket_ . ipv6 = is_ipv6 ( address ) \n            port = socket_ . bind_to_random_port ( address , * port ) \n        except Exception : \n            pass \n            pypet_root_logger = logging . getLogger ( 'pypet' ) \n            pypet_root_logger . exception ( 'Could not connect to {}' . format ( address ) ) \n            raise \n        socket_ . close ( ) \n        context . term ( ) \n    return address + ':' + str ( port ) "}
{"8391": "\ndef main ( ) : \n    rules_to_test = [ 10 , 30 , 90 , 110 , 184 ] \n    steps = 250 \n    ncells = 400 \n    seed = 100042 \n    initial_states = [ 'single' , 'random' ] \n    folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_original' ) \n    if not os . path . isdir ( folder ) : \n        os . makedirs ( folder ) \n    filename = os . path . join ( folder , 'all_patterns.p' ) \n    pass \n    all_patterns = [ ] \n    for idx , rule_number in enumerate ( rules_to_test ) : \n        for initial_name in initial_states : \n            initial_state = make_initial_state ( initial_name , ncells , seed = seed ) \n            pattern = cellular_automaton_1D ( initial_state , rule_number , steps ) \n            all_patterns . append ( ( rule_number , initial_name , pattern ) ) \n        progressbar ( idx , len ( rules_to_test ) , reprint = True ) \n    with open ( filename , 'wb' ) as file : \n        pickle . dump ( all_patterns , file = file ) \n    pass \n    for idx , pattern_tuple in enumerate ( all_patterns ) : \n        rule_number , initial_name , pattern = pattern_tuple \n        filename = os . path . join ( folder , 'rule_%s_%s.png' % ( str ( rule_number ) , initial_name ) ) \n        plot_pattern ( pattern , rule_number , filename ) \n        progressbar ( idx , len ( all_patterns ) , reprint = True ) "}
{"8553": "\ndef main ( ) : \n    folder = os . getcwd ( ) \n    pass \n    merge_all_in_folder ( folder , delete_other_files = True , dynamic_imports = FunctionParameter , backup = False ) \n    pass "}
{"8554": "\ndef upload_file ( filename , session ) : \n    pass \n    outfilesource = os . path . join ( os . getcwd ( ) , filename ) \n    outfiletarget = 'sftp://' + ADDRESS + WORKING_DIR \n    out = saga . filesystem . File ( outfilesource , session = session , flags = OVERWRITE ) \n    out . copy ( outfiletarget ) \n    pass "}
{"8555": "\ndef download_file ( filename , session ) : \n    pass \n    infilesource = os . path . join ( 'sftp://' + ADDRESS + WORKING_DIR , filename ) \n    infiletarget = os . path . join ( os . getcwd ( ) , filename ) \n    incoming = saga . filesystem . File ( infilesource , session = session , flags = OVERWRITE ) \n    incoming . copy ( infiletarget ) \n    pass "}
{"8557": "\ndef merge_trajectories ( session ) : \n    jd = saga . job . Description ( ) \n    jd . executable = 'python' \n    jd . arguments = [ 'merge_trajs.py' ] \n    jd . output = \"mysagajob_merge.stdout\" \n    jd . error = \"mysagajob_merge.stderr\" \n    jd . working_directory = WORKING_DIR \n    js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) \n    myjob = js . create_job ( jd ) \n    pass \n    myjob . run ( ) \n    pass \n    pass \n    pass \n    myjob . wait ( ) \n    pass \n    pass "}
{"8558": "\ndef start_jobs ( session ) : \n    js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) \n    batches = range ( 3 ) \n    jobs = [ ] \n    for batch in batches : \n        pass \n        jd = saga . job . Description ( ) \n        jd . executable = 'python' \n        jd . arguments = [ 'the_task.py --batch=' + str ( batch ) ] \n        jd . output = \"mysagajob.stdout\" + str ( batch ) \n        jd . error = \"mysagajob.stderr\" + str ( batch ) \n        jd . working_directory = WORKING_DIR \n        myjob = js . create_job ( jd ) \n        pass \n        pass \n        pass \n        myjob . run ( ) \n        jobs . append ( myjob ) \n    for myjob in jobs : \n        pass \n        pass \n        pass \n        myjob . wait ( ) \n        pass \n        pass "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    pass \n    for step in range ( 1 , steps ) : \n        if V_array [ step - 1 ] >= 1 : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        elif spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : \n            V_array [ step ] = 0 \n        else : \n            dV = - 1 / tau_V * V_array [ step - 1 ] + I \n            V_array [ step ] = V_array [ step - 1 ] + dV * dt \n    pass \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8562": "\ndef add_parameters ( traj ) : \n    pass \n    traj . f_add_parameter ( 'neuron.V_init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) \n    traj . f_add_parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) \n    traj . f_add_parameter ( 'neuron.tau_V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) \n    traj . f_add_parameter ( 'neuron.tau_ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) \n    traj . f_add_parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) \n    traj . f_add_parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' ) "}
{"8563": "\ndef add_exploration ( traj ) : \n    pass \n    explore_dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau_ref' : [ 5.0 , 7.5 , 10.0 ] } \n    explore_dict = cartesian_product ( explore_dict , ( 'neuron.tau_ref' , 'neuron.I' ) ) \n    traj . f_explore ( explore_dict ) "}
{"8724": "\ndef run_command ( self , args : List [ str ] , max_num_processes : int = None , max_stack_size : int = None , max_virtual_memory : int = None , as_root : bool = False , stdin : FileIO = None , timeout : int = None , check : bool = False , truncate_stdout : int = None , truncate_stderr : int = None ) -> 'CompletedCommand' : \n    cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] \n    if stdin is None : \n        cmd . append ( '--stdin_devnull' ) \n    if max_num_processes is not None : \n        cmd += [ '--max_num_processes' , str ( max_num_processes ) ] \n    if max_stack_size is not None : \n        cmd += [ '--max_stack_size' , str ( max_stack_size ) ] \n    if max_virtual_memory is not None : \n        cmd += [ '--max_virtual_memory' , str ( max_virtual_memory ) ] \n    if timeout is not None : \n        cmd += [ '--timeout' , str ( timeout ) ] \n    if truncate_stdout is not None : \n        cmd += [ '--truncate_stdout' , str ( truncate_stdout ) ] \n    if truncate_stderr is not None : \n        cmd += [ '--truncate_stderr' , str ( truncate_stderr ) ] \n    if not as_root : \n        cmd += [ '--linux_user_id' , str ( self . _linux_uid ) ] \n    cmd += args \n    if self . debug : \n        pass \n    with tempfile . TemporaryFile ( ) as f : \n        try : \n            subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = True ) \n            f . seek ( 0 ) \n            json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            results_json = json . loads ( f . read ( json_len ) . decode ( ) ) \n            stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stdout = tempfile . NamedTemporaryFile ( ) \n            stdout . write ( f . read ( stdout_len ) ) \n            stdout . seek ( 0 ) \n            stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stderr = tempfile . NamedTemporaryFile ( ) \n            stderr . write ( f . read ( stderr_len ) ) \n            stderr . seek ( 0 ) \n            result = CompletedCommand ( return_code = results_json [ 'return_code' ] , timed_out = results_json [ 'timed_out' ] , stdout = stdout , stderr = stderr , stdout_truncated = results_json [ 'stdout_truncated' ] , stderr_truncated = results_json [ 'stderr_truncated' ] ) \n            if ( result . return_code != 0 or results_json [ 'timed_out' ] ) and check : \n                raise subprocess . CalledProcessError ( result . return_code , cmd , output = result . stdout , stderr = result . stderr ) \n            return result \n        except subprocess . CalledProcessError as e : \n            f . seek ( 0 ) \n            pass \n            pass \n            raise "}
{"8861": "\ndef run ( file_or_code , code , in_ns , use_var_indirection , warn_on_shadowed_name , warn_on_shadowed_var , warn_on_var_indirection , ) : \n    basilisp . init ( ) \n    ctx = compiler . CompilerContext ( filename = CLI_INPUT_FILE_PATH if code else ( STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code ) , opts = { compiler . WARN_ON_SHADOWED_NAME : warn_on_shadowed_name , compiler . WARN_ON_SHADOWED_VAR : warn_on_shadowed_var , compiler . USE_VAR_INDIRECTION : use_var_indirection , compiler . WARN_ON_VAR_INDIRECTION : warn_on_var_indirection , } , ) \n    eof = object ( ) \n    with runtime . ns_bindings ( in_ns ) as ns : \n        if code : \n            pass \n        elif file_or_code == STDIN_FILE_NAME : \n            pass \n        else : \n            pass "}
{"9051": "\ndef validate_version ( ) : \n    import leicacam \n    version_string = leicacam . __version__ \n    versions = version_string . split ( '.' , 3 ) \n    try : \n        for ver in versions : \n            int ( ver ) \n    except ValueError : \n        pass \n        return None \n    return version_string "}
{"9052": "\ndef generate ( ) : \n    old_dir = os . getcwd ( ) \n    proj_dir = os . path . join ( os . path . dirname ( __file__ ) , os . pardir ) \n    os . chdir ( proj_dir ) \n    version = validate_version ( ) \n    if not version : \n        os . chdir ( old_dir ) \n        return \n    pass \n    options = [ '--user' , 'arve0' , '--project' , 'leicacam' , '-v' , '--with-unreleased' , '--future-release' , version ] \n    generator = ChangelogGenerator ( options ) \n    generator . run ( ) \n    os . chdir ( old_dir ) "}
{"9082": "\ndef run_trace ( mname , fname , module_prefix , callable_names , no_print , module_exclude = None , callable_exclude = None , debug = False , ) : \n    module_exclude = [ ] if module_exclude is None else module_exclude \n    callable_exclude = [ ] if callable_exclude is None else callable_exclude \n    par = trace_pars ( mname ) \n    start_time = datetime . datetime . now ( ) \n    with pexdoc . exdoc . ExDocCxt ( exclude = par . exclude + module_exclude , pickle_fname = par . pickle_fname , in_callables_fname = par . in_callables_fname , out_callables_fname = par . out_callables_fname , _no_print = no_print , ) as exdoc_obj : \n        fname = os . path . realpath ( os . path . join ( os . path . dirname ( __file__ ) , \"..\" , \"..\" , \"tests\" , \"test_{0}.py\" . format ( fname ) , ) ) \n        test_cmd = ( [ \"--color=yes\" ] + ( [ \"-s\" , \"-vv\" ] if debug else [ \"-q\" , \"-q\" , \"-q\" ] ) + [ \"--disable-warnings\" ] + [ \"-x\" ] + ( [ par . noption ] if par . noption else [ ] ) + [ \"-m \" + mname ] + [ fname ] ) \n        with warnings . catch_warnings ( ) : \n            warnings . filterwarnings ( \"ignore\" , category = PytestWarning ) \n            if pytest . main ( test_cmd ) : \n                raise RuntimeError ( \"Tracing did not complete successfully\" ) \n    stop_time = datetime . datetime . now ( ) \n    if not no_print : \n        pass \n        for callable_name in callable_names : \n            callable_name = module_prefix + callable_name \n            pass \n            pass \n            pass \n    return copy . copy ( exdoc_obj ) "}
{"9095": "\ndef quietinterrupt ( msg = None ) : \n    def handler ( ) : \n        if msg : \n            pass \n        sys . exit ( 1 ) \n    signal . signal ( signal . SIGINT , handler ) "}
{"9096": "\ndef printtsv ( table , sep = \"\\t\" , file = sys . stdout ) : \n    for record in table : \n        pass "}
{"9189": "\ndef wrap_and_format ( self , width = None , include_params = False , include_return = False , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        elif isinstance ( line , BlankLine ) : \n            out . write ( '\\n' ) \n        elif isinstance ( line , ListItem ) : \n            out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ 0 ] , subsequent_indent = \"   \" , width = width ) ) \n            out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if len ( included_params ) > 0 : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        pass \n        pass \n    return out . getvalue ( ) "}
{"9227": "\ndef load ( ) : \n    autodiscover_modules ( 'cron' ) \n    if PROJECT_MODULE : \n        if '.' in PROJECT_MODULE . __name__ : \n            try : \n                import_module ( '%s.cron' % '.' . join ( PROJECT_MODULE . __name__ . split ( '.' ) [ 0 : - 1 ] ) ) \n            except ImportError as e : \n                if 'No module named' not in str ( e ) : \n                    pass \n    for cmd , app in get_commands ( ) . items ( ) : \n        try : \n            load_command_class ( app , cmd ) \n        except django . core . exceptions . ImproperlyConfigured : \n            pass "}
{"9229": "\ndef printtasks ( ) : \n    load ( ) \n    tab = crontab . CronTab ( '' ) \n    for task in registry : \n        tab . new ( task . command , KRONOS_BREADCRUMB ) . setall ( task . schedule ) \n    pass "}
{"9253": "\ndef main ( ) : \n    state = GameState ( ) \n    pass \n    while state . running : \n        input = get_single_char ( ) \n        state , should_advance = state . handle_input ( input ) \n        if should_advance : \n            state = state . advance_robots ( ) \n            state = state . check_game_end ( ) \n        pass \n    pass "}
{"9258": "\ndef player_move ( board ) : \n    pass \n    x , y = input ( 'Enter move (e.g. 2b): ' ) \n    pass \n    return int ( x ) - 1 , ord ( y ) - ord ( 'a' ) "}
{"9259": "\ndef play ( ) : \n    ai = { 'X' : player_move , 'O' : random_move } \n    board = Board ( ) \n    while not board . winner : \n        x , y = ai [ board . player ] ( board ) \n        board = board . make_move ( x , y ) \n    pass \n    pass "}
{"9470": "\ndef v2_runner_on_ok ( self , result , ** kwargs ) : \n    failed = \"failed\" in result . _result \n    unreachable = \"unreachable\" in result . _result \n    if ( \"print_action\" in result . _task . tags or failed or unreachable or self . _display . verbosity > 1 ) : \n        self . _print_task ( ) \n        self . last_skipped = False \n        msg = unicode ( result . _result . get ( \"msg\" , \"\" ) ) or unicode ( result . _result . get ( \"reason\" , \"\" ) ) or unicode ( result . _result . get ( \"message\" , \"\" ) ) \n        stderr = [ result . _result . get ( \"exception\" , None ) , result . _result . get ( \"module_stderr\" , None ) , ] \n        stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n        self . _print_host_or_item ( result . _host , result . _result . get ( \"changed\" , False ) , msg , result . _result . get ( \"diff\" , None ) , is_host = True , error = failed or unreachable , stdout = result . _result . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n        if \"results\" in result . _result : \n            for r in result . _result [ \"results\" ] : \n                failed = \"failed\" in r \n                stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module_stderr\" , None ) ] \n                stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n                self . _print_host_or_item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is_host = False , error = failed , stdout = r . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n    else : \n        self . last_skipped = True \n        pass "}
{"9471": "\ndef v2_playbook_on_stats ( self , stats ) : \n    pass \n    self . printed_last_task = False \n    self . _print_task ( \"STATS\" ) \n    hosts = sorted ( stats . processed . keys ( ) ) \n    for host in hosts : \n        s = stats . summarize ( host ) \n        if s [ \"failures\" ] or s [ \"unreachable\" ] : \n            color = \"failed\" \n        elif s [ \"changed\" ] : \n            color = \"changed\" \n        else : \n            color = \"ok\" \n        msg = \"{}    : ok={}\\tchanged={}\\tfailed={}\\tunreachable={}\" . format ( host , s [ \"ok\" ] , s [ \"changed\" ] , s [ \"failures\" ] , s [ \"unreachable\" ] ) \n        pass "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if self . _display . verbosity > 1 : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if len ( reason ) < 50 : \n            line += \" -- {}\" . format ( reason ) \n            pass \n        else : \n            pass \n            pass \n            pass "}
{"9522": "\ndef _list ( api_list_class , arg_namespace , ** extra ) : \n    if arg_namespace . starting_point : \n        ordering_field = ( arg_namespace . ordering or '' ) . lstrip ( '-' ) \n        if ordering_field in ( '' , 'datetime_uploaded' , 'datetime_created' ) : \n            arg_namespace . starting_point = parser . parse ( arg_namespace . starting_point ) \n    items = api_list_class ( starting_point = arg_namespace . starting_point , ordering = arg_namespace . ordering , limit = arg_namespace . limit , request_limit = arg_namespace . request_limit , ** extra ) \n    items . constructor = lambda x : x \n    try : \n        pprint ( list ( items ) ) \n    except ValueError as e : \n        pass "}
{"9523": "\ndef bar ( iter_content , parts , title = '' ) : \n    parts = max ( float ( parts ) , 1.0 ) \n    cells = 10 \n    progress = 0 \n    step = cells / parts \n    draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) \n    for chunk in iter_content : \n        yield chunk \n        progress += step \n        draw ( progress ) \n        sys . stdout . flush ( ) \n    draw ( cells ) \n    pass "}
{"9529": "\ndef camera_disable ( self , camera_id , ** kwargs ) : \n    api = self . _api_info [ 'camera' ] \n    payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'Disable' , 'version' : 9 , 'idList' : camera_id , } , ** kwargs ) \n    pass \n    pass \n    response = self . _get ( api [ 'url' ] , payload ) \n    return response [ 'success' ] "}
{"9553": "\ndef find ( dataset , url ) : \n    fn = os . path . join ( DATASETS , dataset ) \n    dn = os . path . dirname ( fn ) \n    if not os . path . exists ( dn ) : \n        pass \n        os . makedirs ( dn ) \n    if not os . path . exists ( fn ) : \n        if sys . version_info < ( 3 , ) : \n            urllib . urlretrieve ( url , fn ) \n        else : \n            urllib . request . urlretrieve ( url , fn ) \n    return fn "}
{"9555": "\ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        pass \n        h = tar . extractfile ( name ) \n        if sys . version_info < ( 3 , ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( 1 , 6 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9791": "\ndef _get_new_access_information ( self ) : \n    if not self . r . has_oauth_app_info : \n        self . _log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) \n        raise AttributeError ( 'Reddit Session invalid, please check your designated config file.' ) \n    url = self . r . get_authorize_url ( 'UsingOAuth2Util' , self . _get_value ( CONFIGKEY_SCOPE , set , split_val = ',' ) , self . _get_value ( CONFIGKEY_REFRESHABLE , as_boolean = True ) ) \n    self . _start_webserver ( url ) \n    if not self . _get_value ( CONFIGKEY_SERVER_MODE , as_boolean = True ) : \n        webbrowser . open ( url ) \n    else : \n        pass \n    self . _wait_for_response ( ) \n    try : \n        access_information = self . r . get_access_information ( self . server . response_code ) \n    except praw . errors . OAuthException : \n        self . _log ( \"Can not authenticate, maybe the app infos (e.g. secret) are wrong.\" , logging . ERROR ) \n        raise \n    self . _change_value ( CONFIGKEY_TOKEN , access_information [ \"access_token\" ] ) \n    self . _change_value ( CONFIGKEY_REFRESH_TOKEN , access_information [ \"refresh_token\" ] ) \n    self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) "}
{"9838": "\ndef get_authorisation_url ( self , application_name , token_expire = '1day' ) : \n    query_params = { 'name' : application_name , 'expiration' : token_expire , 'response_type' : 'token' , 'scope' : 'read,write' } \n    authorisation_url = self . build_uri ( path = '/authorize' , query_params = self . add_authorisation ( query_params ) ) \n    pass \n    return authorisation_url "}
{"9896": "\ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    cli = CommandLineTool ( ) \n    try : \n        return cli . run ( argv ) \n    except KeyboardInterrupt : \n        pass \n        return 3 "}
{"9953": "\ndef print_all ( ) : \n    _ , conf = read_latoolscfg ( ) \n    default = conf [ 'DEFAULT' ] [ 'config' ] \n    pstr = '\\nCurrently defined LAtools configurations:\\n\\n' \n    for s in conf . sections ( ) : \n        if s == default : \n            pstr += s + ' [DEFAULT]\\n' \n        elif s == 'REPRODUCE' : \n            pstr += s + ' [DO NOT ALTER]\\n' \n        else : \n            pstr += s + '\\n' \n        for k , v in conf [ s ] . items ( ) : \n            if k != 'config' : \n                if v [ : 9 ] == 'resources' : \n                    v = pkgrs . resource_filename ( 'latools' , v ) \n                pstr += '   ' + k + ': ' + v + '\\n' \n        pstr += '\\n' \n    pass \n    return "}
{"9954": "\ndef copy_SRM_file ( destination = None , config = 'DEFAULT' ) : \n    conf = read_configuration ( ) \n    src = pkgrs . resource_filename ( 'latools' , conf [ 'srmfile' ] ) \n    if destination is None : \n        destination = './LAtools_' + conf [ 'config' ] + '_SRMTable.csv' \n    if os . path . isdir ( destination ) : \n        destination += 'LAtools_' + conf [ 'config' ] + '_SRMTable.csv' \n    copyfile ( src , destination ) \n    pass \n    return "}
{"9956": "\ndef change_default ( config ) : \n    config_file , cf = read_latoolscfg ( ) \n    if config not in cf . sections ( ) : \n        raise ValueError ( \"\\n'{:s}' is not a defined configuration.\" . format ( config ) ) \n    if config == 'REPRODUCE' : \n        pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\\n' + '     ... this is an odd thing to be doing.' ) \n    else : \n        pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) \n    response = input ( pstr + '\\n> [N/y]: ' ) \n    if response . lower ( ) == 'y' : \n        cf . set ( 'DEFAULT' , 'config' , config ) \n        with open ( config_file , 'w' ) as f : \n            cf . write ( f ) \n        pass \n    else : \n        pass "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n            if len ( self . subsets [ 'not_in_set' ] ) > 0 : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) \n        pass \n        return \n    elif sample is not None : \n        s += 'Sample: ' + sample + '\\n' \n        s += self . data [ sample ] . filt . __repr__ ( ) \n        pass \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s += 'Subset: ' + str ( n ) + '\\n' \n            s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n            s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n        pass \n        return "}
{"9999": "\ndef filter_nremoved ( self , filt = True , quiet = False ) : \n    rminfo = { } \n    for n in self . subsets [ 'All_Samples' ] : \n        s = self . data [ n ] \n        rminfo [ n ] = s . filt_nremoved ( filt ) \n    if not quiet : \n        maxL = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) \n        pass \n        for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : \n            pass \n    return rminfo "}
{"10012": "\ndef by_regex ( file , outdir = None , split_pattern = None , global_header_rows = 0 , fname_pattern = None , trim_tail_lines = 0 , trim_head_lines = 0 ) : \n    if outdir is None : \n        outdir = os . path . join ( os . path . dirname ( file ) , 'split' ) \n    if not os . path . exists ( outdir ) : \n        os . mkdir ( outdir ) \n    with open ( file , 'r' ) as f : \n        lines = f . readlines ( ) \n    extension = os . path . splitext ( file ) [ - 1 ] \n    global_header = lines [ : global_header_rows ] \n    starts = [ ] \n    for i , line in enumerate ( lines ) : \n        if re . search ( split_pattern , line ) : \n            starts . append ( i ) \n    starts . append ( len ( lines ) ) \n    splits = { } \n    for i in range ( len ( starts ) - 1 ) : \n        m = re . search ( fname_pattern , lines [ starts [ i ] ] ) \n        if m : \n            fname = m . groups ( ) [ 0 ] . strip ( ) \n        else : \n            fname = 'no_name_{:}' . format ( i ) \n        splits [ fname ] = global_header + lines [ starts [ i ] : starts [ i + 1 ] ] [ trim_head_lines : trim_tail_lines ] \n    pass \n    for k , v in splits . items ( ) : \n        fname = ( k + extension ) . replace ( ' ' , '_' ) \n        with open ( os . path . join ( outdir , fname ) , 'w' ) as f : \n            f . writelines ( v ) \n        pass \n    pass \n    return outdir "}
{"10025": "\ndef grab_filt ( self , filt , analyte = None ) : \n    if isinstance ( filt , str ) : \n        if filt in self . components : \n            if analyte is None : \n                return self . components [ filt ] \n            else : \n                if self . switches [ analyte ] [ filt ] : \n                    return self . components [ filt ] \n        else : \n            try : \n                ind = self . make_fromkey ( filt ) \n            except KeyError : \n                pass \n    elif isinstance ( filt , dict ) : \n        try : \n            ind = self . make_fromkey ( filt [ analyte ] ) \n        except ValueError : \n            pass \n    elif filt : \n        ind = self . make ( analyte ) \n    else : \n        ind = ~ np . zeros ( self . size , dtype = bool ) \n    return ind "}
{"10088": "\ndef initialize ( self ) : \n    try : \n        logger . info ( \"Authenticating...\" ) \n        self . backend = Backend ( self . backend_url ) \n        self . backend . login ( self . username , self . password ) \n    except BackendException as exp : \n        logger . exception ( \"Exception: %s\" , exp ) \n        logger . error ( \"Response: %s\" , exp . response ) \n    if self . backend . token is None : \n        pass \n        pass \n        pass \n        exit ( 1 ) \n    logger . info ( \"Authenticated.\" ) \n    users = self . backend . get_all ( 'user' , { 'where' : json . dumps ( { 'name' : self . username } ) } ) \n    self . logged_in_user = users [ '_items' ] [ 0 ] \n    self . default_realm = self . logged_in_user [ '_realm' ] \n    self . realm_all = None \n    realms = self . backend . get_all ( 'realm' ) \n    for r in realms [ '_items' ] : \n        if r [ 'name' ] == 'All' and r [ '_level' ] == 0 : \n            self . realm_all = r [ '_id' ] \n            logger . info ( \"Found realm 'All': %s\" , self . realm_all ) \n        if r [ '_id' ] == self . default_realm : \n            logger . info ( \"Found logged-in user realm: %s\" , r [ 'name' ] ) \n    self . tp_always = None \n    self . tp_never = None \n    timeperiods = self . backend . get_all ( 'timeperiod' ) \n    for tp in timeperiods [ '_items' ] : \n        if tp [ 'name' ] == '24x7' : \n            self . tp_always = tp [ '_id' ] \n            logger . info ( \"Found TP '24x7': %s\" , self . tp_always ) \n        if tp [ 'name' ] . lower ( ) == 'none' or tp [ 'name' ] . lower ( ) == 'never' : \n            self . tp_never = tp [ '_id' ] \n            logger . info ( \"Found TP 'Never': %s\" , self . tp_never ) "}
{"10210": "\ndef print_downloads ( self ) : \n    for path , ann in self . annotation . items ( ) : \n        if path . startswith ( 'output' ) and ann [ 'type' ] == 'basic:file:' : \n            pass "}
{"10214": "\ndef print_processor_inputs ( self , processor_name ) : \n    p = self . processors ( processor_name = processor_name ) \n    if len ( p ) == 1 : \n        p = p [ 0 ] \n    else : \n        Exception ( 'Invalid processor name' ) \n    for field_schema , _ , _ in iterate_schema ( { } , p [ 'input_schema' ] , 'input' ) : \n        name = field_schema [ 'name' ] \n        typ = field_schema [ 'type' ] \n        pass "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if i > 0 and response is not None : \n                    pass \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    pass \n    return session_id "}
{"10330": "\ndef runProcess ( args , timeout , grace , reactor ) : \n    deferred = defer . Deferred ( ) \n    protocol = ProcessProtocol ( deferred ) \n    process = reactor . spawnProcess ( protocol , args [ 0 ] , args , env = os . environ ) \n    def _logEnded ( err ) : \n        err . trap ( tierror . ProcessDone , tierror . ProcessTerminated ) \n        pass \n    deferred . addErrback ( _logEnded ) \n    def _cancelTermination ( dummy ) : \n        for termination in terminations : \n            if termination . active ( ) : \n                termination . cancel ( ) \n    deferred . addCallback ( _cancelTermination ) \n    terminations = [ ] \n    terminations . append ( reactor . callLater ( timeout , process . signalProcess , \"TERM\" ) ) \n    terminations . append ( reactor . callLater ( timeout + grace , process . signalProcess , \"KILL\" ) ) \n    return deferred "}
{"10443": "\ndef set_version ( self , new_version : str ) : \n    try : \n        f = open ( self . file_path , 'r' ) \n        lines = f . readlines ( ) \n        f . close ( ) \n    except Exception as e : \n        pass \n        return \n    for idx , line in enumerate ( lines ) : \n        if self . magic_line in line : \n            start = len ( self . magic_line ) \n            end = len ( line ) - self . strip_end_chars \n            start_str = line [ 0 : start ] \n            end_str = line [ end : ] \n            lines [ idx ] = start_str + new_version + end_str \n    try : \n        f = open ( self . file_path , 'w' ) \n        f . writelines ( lines ) \n        f . close ( ) \n    except Exception as e : \n        pass \n        return "}
{"10478": "\ndef run ( self ) : \n    if ON_READ_THE_DOCS : \n        return \n    try : \n        pyside_rcc_command = 'pyside-rcc' \n        if sys . platform == 'win32' : \n            import PySide \n            pyside_rcc_command = os . path . join ( os . path . dirname ( PySide . __file__ ) , 'pyside-rcc.exe' ) \n        subprocess . check_call ( [ pyside_rcc_command , '-o' , self . resource_target_path , self . resource_source_path ] ) \n    except ( subprocess . CalledProcessError , OSError ) : \n        pass \n        raise SystemExit ( ) "}
{"10499": "\ndef execute ( self , override_wf_json = None ) : \n    r = self . gbdx . post ( self . URL , json = self . json if override_wf_json is None else override_wf_json ) \n    try : \n        r . raise_for_status ( ) \n    except : \n        pass \n        pass \n        self . id = None \n        return \n    self . id = r . json ( ) [ 'id' ] \n    self . _refresh_status ( ) "}
{"10502": "\ndef list ( pattern = ( ) ) : \n    globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] \n    matches = [ ] \n    offset = len ( PROJ_ARCHIVE ) + 1 \n    for suffix in globs : \n        glob_pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , suffix ) \n        matches . append ( set ( f [ offset : ] for f in glob . glob ( glob_pattern ) ) ) \n    matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) \n    for m in sorted ( matches ) : \n        pass "}
{"10503": "\ndef restore ( folder ) : \n    if os . path . isdir ( folder ) : \n        bail ( 'a folder of the same name already exists!' ) \n    pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) \n    matches = glob . glob ( pattern ) \n    if not matches : \n        bail ( 'no project matches: ' + folder ) \n    if len ( matches ) > 1 : \n        pass \n    source = sorted ( matches ) [ - 1 ] \n    pass \n    shutil . move ( source , '.' ) "}
{"10545": "\ndef rfxcom ( device ) : \n    if device is None : \n        device = app . config . get ( 'DEVICE' ) \n    if device is None : \n        pass \n        return \n    rfxcom_collect ( device ) "}
{"10588": "\ndef run ( cls , name , desc ) : \n    wrapper = cls ( name , desc ) \n    mount_path = wrapper . _get_mount_path ( ) \n    arg_parser = wrapper . _create_argument_parser ( ) \n    wrapper . _extend_argument_parser ( arg_parser ) \n    empty_config = wrapper . __get_empty_config ( ) \n    config_yaml = ruamel . yaml . load ( empty_config ) \n    wrapper . __populate_parser_from_config ( arg_parser , config_yaml ) \n    args = arg_parser . parse_args ( ) \n    for k , v in vars ( args ) . items ( ) : \n        k = k . replace ( '_' , '-' ) \n        if k in config_yaml : \n            config_yaml [ k ] = v \n    config_path = wrapper . _get_config_path ( ) \n    with open ( config_path , 'w' ) as writable : \n        ruamel . yaml . dump ( config_yaml , stream = writable ) \n    workdir_path = os . path . join ( mount_path , 'Toil-' + wrapper . _name ) \n    if os . path . exists ( workdir_path ) : \n        if args . restart : \n            log . info ( 'Reusing temporary directory: %s' , workdir_path ) \n        else : \n            raise UserError ( 'Temporary directory {} already exists. Run with --restart ' 'option or remove directory.' . format ( workdir_path ) ) \n    else : \n        os . makedirs ( workdir_path ) \n        log . info ( 'Temporary directory created: %s' , workdir_path ) \n    command = wrapper . _create_pipeline_command ( args , workdir_path , config_path ) \n    wrapper . _extend_pipeline_command ( command , args ) \n    try : \n        subprocess . check_call ( command ) \n    except subprocess . CalledProcessError as e : \n        pass \n    finally : \n        stat = os . stat ( mount_path ) \n        log . info ( 'Pipeline terminated, changing ownership of output files in %s from root to ' 'uid %s and gid %s.' , mount_path , stat . st_uid , stat . st_gid ) \n        chown_command = [ 'chown' , '-R' , '%s:%s' % ( stat . st_uid , stat . st_gid ) , mount_path ] \n        subprocess . check_call ( chown_command ) \n        if args . no_clean : \n            log . info ( 'Flag \"--no-clean\" was used, therefore %s was not deleted.' , workdir_path ) \n        else : \n            log . info ( 'Cleaning up temporary directory: %s' , workdir_path ) \n            shutil . rmtree ( workdir_path ) "}
{"10670": "\ndef open_in_browser ( doc , encoding = None ) : \n    import os \n    import webbrowser \n    import tempfile \n    if not isinstance ( doc , etree . _ElementTree ) : \n        doc = etree . ElementTree ( doc ) \n    handle , fn = tempfile . mkstemp ( suffix = '.html' ) \n    f = os . fdopen ( handle , 'wb' ) \n    try : \n        doc . write ( f , method = \"html\" , encoding = encoding or doc . docinfo . encoding or \"UTF-8\" ) \n    finally : \n        f . close ( ) \n    url = 'file://' + fn . replace ( os . path . sep , '/' ) \n    pass \n    webbrowser . open ( url ) "}
{"10730": "\ndef run ( self , options , args ) : \n    shells = COMPLETION_SCRIPTS . keys ( ) \n    shell_options = [ '--' + shell for shell in sorted ( shells ) ] \n    if options . shell in shells : \n        script = COMPLETION_SCRIPTS . get ( options . shell , '' ) \n        pass \n    else : \n        sys . stderr . write ( 'ERROR: You must pass %s\\n' % ' or ' . join ( shell_options ) ) "}
{"10785": "\ndef do_dice_roll ( ) : \n    options = get_options ( ) \n    dice = Dice ( options . sides ) \n    rolls = [ dice . roll ( ) for n in range ( options . number ) ] \n    for roll in rolls : \n        pass \n    if options . number > 1 : \n        pass "}
{"10876": "\ndef report ( self ) : \n    pass \n    sys . stdout . flush ( ) "}
{"11174": "\ndef pfprint ( item , end = '\\n' , file = None ) : \n    if file is None : \n        file = sys . stdout \n    pass "}
{"11194": "\ndef run ( self ) : \n    pass \n    if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n        self . data ( ) \n        self . check_values ( ) \n        if self . without_error == False : \n            self . display_error_message ( ) \n        elif self . without_error == True : \n            self . is_running = True \n            self . hide_error_message ( ) \n            self . write_to_file ( ) \n            os . chdir ( './' ) \n            self . progress_bar ( ) \n            this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) \n            batch_file = os . path . join ( this_dir , \"inputs/batch_files/\" + str ( self . batch_name_value ) + \"_batch.txt\" ) \n            pass \n            self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch_file ] , shell = True ) \n            if self . ui . progressBar . value ( ) == 100 : \n                self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) "}
{"11195": "\ndef cancel_planarrad ( self ) : \n    if ( self . is_running == True ) & ( self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE ) : \n        cancel = QtGui . QMessageBox . question ( self . ui . cancel , 'Cancel PlanarRad' , \"Are you sure to cancel ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if cancel == QtGui . QMessageBox . Yes : \n            self . is_running = False \n            os . kill ( self . p . pid , signal . SIGTERM ) \n            pass \n            self . ui . progressBar . reset ( ) \n        else : \n            pass "}
{"11330": "\ndef run ( query , params = None , config = None , conn = None , ** kwargs ) : \n    if params is None : \n        params = { } \n    if conn is None : \n        conn = Connection . get ( DEFAULT_CONFIGURABLE [ \"uri\" ] ) \n    elif isinstance ( conn , string_types ) : \n        conn = Connection . get ( conn ) \n    if config is None : \n        default_config = DEFAULT_CONFIGURABLE . copy ( ) \n        kwargs . update ( default_config ) \n        config = DefaultConfigurable ( ** kwargs ) \n    if query . strip ( ) : \n        params = extract_params_from_query ( query , params ) \n        result = conn . session . query ( query , params , data_contents = config . data_contents ) \n        if config . feedback : \n            pass \n        resultset = ResultSet ( result , query , config ) \n        if config . auto_pandas : \n            return resultset . get_dataframe ( ) \n        elif config . auto_networkx : \n            graph = resultset . get_graph ( ) \n            resultset . draw ( ) \n            return graph \n        else : \n            return resultset \n    else : \n        return 'Connected: %s' % conn . name "}
{"11410": "\ndef discrete_best_alpha ( data , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True ) : \n    xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] \n    best_index = argmin ( ksvalues ) \n    best_alpha = alpha_of_xmin [ best_index ] \n    best_xmin = xmins [ best_index ] \n    best_ks = ksvalues [ best_index ] \n    best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if verbose : \n        pass \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11411": "\ndef discrete_best_alpha ( self , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True , finite = True ) : \n    data = self . data \n    self . _xmins = xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = np . array ( [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] ) \n    self . _alpha_values = np . array ( alpha_of_xmin ) \n    self . _xmin_kstest = ksvalues \n    ksvalues [ np . isnan ( ksvalues ) ] = np . inf \n    best_index = argmin ( ksvalues ) \n    self . _alpha = best_alpha = alpha_of_xmin [ best_index ] \n    self . _xmin = best_xmin = xmins [ best_index ] \n    self . _ks = best_ks = ksvalues [ best_index ] \n    self . _likelihood = best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if finite : \n        self . _alpha = self . _alpha * ( n - 1. ) / n + 1. / n \n    if verbose : \n        pass \n    self . _ngtx = n = ( self . data >= self . _xmin ) . sum ( ) \n    self . _alphaerr = ( self . _alpha - 1.0 ) / np . sqrt ( n ) \n    if scipyOK : \n        self . _ks_prob = scipy . stats . ksone . sf ( self . _ks , n ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11413": "\ndef lognormal ( self , doprint = True ) : \n    if scipyOK : \n        fitpars = scipy . stats . lognorm . fit ( self . data ) \n        self . lognormal_dist = scipy . stats . lognorm ( * fitpars ) \n        self . lognormal_ksD , self . lognormal_ksP = scipy . stats . kstest ( self . data , self . lognormal_dist . cdf ) \n        self . lognormal_likelihood = - 1 * scipy . stats . lognorm . nnlf ( fitpars , self . data ) \n        self . power_lognorm_likelihood = ( self . _likelihood + self . lognormal_likelihood ) \n        self . likelihood_ratio_D = - 2 * ( log ( self . _likelihood / self . lognormal_likelihood ) ) \n        if doprint : \n            pass \n            pass \n            pass "}
{"11491": "\ndef dprint ( name , val ) : \n    from pprint import pformat \n    pass "}
{"11497": "\ndef process_ddp ( self , data ) : \n    msg_id = data . get ( 'id' , None ) \n    try : \n        msg = data . pop ( 'msg' ) \n    except KeyError : \n        self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) \n        return \n    try : \n        self . dispatch ( msg , data ) \n    except Exception as err : \n        kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } \n        if msg_id is not None : \n            kwargs [ 'id' ] = msg_id \n        if isinstance ( err , MeteorError ) : \n            error = err . as_dict ( ) \n        else : \n            error = { 'error' : 500 , 'reason' : 'Internal server error' , } \n        if kwargs [ 'msg' ] == 'error' : \n            kwargs . update ( error ) \n        else : \n            kwargs [ 'error' ] = error \n        if not isinstance ( err , MeteorError ) : \n            stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = 1 , ) \n            if stack is not None : \n                traceback . print_exc ( file = sys . stderr ) \n                sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) \n                sys . stderr . write ( stack ) \n        elif settings . DEBUG : \n            pass \n            dprint ( 'msg' , msg ) \n            dprint ( 'data' , data ) \n            error . setdefault ( 'details' , traceback . format_exc ( ) ) \n            pass \n        self . reply ( ** kwargs ) \n        if msg_id and msg == 'method' : \n            self . reply ( 'updated' , methods = [ msg_id ] ) "}
{"11507": "\ndef print ( self , msg , * args , ** kwargs ) : \n    if self . verbosity >= 1 : \n        pass "}
{"11530": "\ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    pass \n    number_of_scenarios = scenarios . shape [ 1 ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ 1 / number_of_scenarios for i in range ( 0 , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( 0 , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( 0 , number_of_scenarios ) \n    if number_of_reduced_scenarios >= number_of_scenarios : \n        return ( scenarios , probability , J ) \n    for scenario_k in range ( 0 , number_of_scenarios ) : \n        for scenario_u in range ( 0 , number_of_scenarios ) : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n    for scenario_u in range ( 0 , number_of_scenarios ) : \n        summation = 0 \n        for scenario_k in range ( 0 , number_of_scenarios ) : \n            if scenario_k != scenario_u : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n        z [ scenario_u ] = summation \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    for _ in range ( 0 , number_of_scenarios - number_of_reduced_scenarios - 1 ) : \n        pass \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = 0 \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if c [ deleted_scenario_number , scenario_j ] < lowest_value : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11604": "\ndef write_gro ( outfile , title , atoms , box ) : \n    pass \n    pass \n    atom_template = \"{:5d}{:<5s}{:>5s}{:5d}{:8.3f}{:8.3f}{:8.3f}\" \n    for idx , atname , resname , resid , x , y , z in atoms : \n        pass \n    grobox = ( box [ 0 ] [ 0 ] , box [ 1 ] [ 1 ] , box [ 2 ] [ 2 ] , box [ 0 ] [ 1 ] , box [ 0 ] [ 2 ] , box [ 1 ] [ 0 ] , box [ 1 ] [ 2 ] , box [ 2 ] [ 0 ] , box [ 2 ] [ 1 ] ) \n    box_template = '{:10.5f}' * 9 \n    pass "}
{"11605": "\ndef write_pdb ( outfile , title , atoms , box ) : \n    pass \n    pass \n    for idx , atname , resname , resid , x , y , z in atoms : \n        pass "}
{"11608": "\ndef write_top ( outpath , molecules , title ) : \n    topmolecules = [ ] \n    for i in molecules : \n        if i [ 0 ] . endswith ( '.o' ) : \n            topmolecules . append ( tuple ( [ i [ 0 ] [ : - 2 ] ] + list ( i [ 1 : ] ) ) ) \n        else : \n            topmolecules . append ( i ) \n    if outpath : \n        with open ( outpath , \"w\" ) as top : \n            pass \n            pass \n            pass \n            pass \n            pass \n            pass \n            pass \n            pass \n    else : \n        added_molecules = ( molecule for molecule in topmolecules if molecule [ 0 ] != 'Protein' ) \n        pass "}
{"11621": "\ndef display_required_items ( msg_type ) : \n    pass \n    pass \n    for k , v in CONFIG [ msg_type ] [ \"settings\" ] . items ( ) : \n        pass \n    pass \n    for k , v in CONFIG [ msg_type ] [ \"auth\" ] . items ( ) : \n        pass "}
{"11624": "\ndef configure_profile ( msg_type , profile_name , data , auth ) : \n    with jsonconfig . Config ( \"messages\" , indent = 4 ) as cfg : \n        write_data ( msg_type , profile_name , data , cfg ) \n        write_auth ( msg_type , profile_name , auth , cfg ) \n    pass \n    pass "}
{"11628": "\ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        pass \n    if encoding == \"json\" : \n        resp = requests . post ( self . url , json = self . message ) \n    elif encoding == \"url\" : \n        resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and resp . history [ 0 ] . status_code >= 300 : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        elif \"invalid_auth\" in resp . text : \n            raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        pass \n    pass "}
{"11643": "\ndef _send_content ( self , method = \"/sendMessage\" ) : \n    url = self . base_url + method \n    try : \n        resp = requests . post ( url , json = self . message ) \n        resp . raise_for_status ( ) \n    except requests . exceptions . HTTPError as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        if method == \"/sendMessage\" : \n            content_type = \"Message body\" \n        elif method == \"/sendDocument\" : \n            content_type = \"Attachment: \" + self . message [ \"document\" ] \n        pass "}
{"11644": "\ndef send ( self ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        pass \n    self . _send_content ( \"/sendMessage\" ) \n    if self . attachments : \n        if isinstance ( self . attachments , str ) : \n            self . attachments = [ self . attachments ] \n        for a in self . attachments : \n            self . message [ \"document\" ] = a \n            self . _send_content ( method = \"/sendDocument\" ) \n    if self . verbose : \n        pass \n    pass "}
{"11653": "\ndef send ( self ) : \n    self . _generate_email ( ) \n    if self . verbose : \n        pass \n    recipients = [ ] \n    for i in ( self . to , self . cc , self . bcc ) : \n        if i : \n            if isinstance ( i , MutableSequence ) : \n                recipients += i \n            else : \n                recipients . append ( i ) \n    session = self . _get_session ( ) \n    if self . verbose : \n        pass \n    session . sendmail ( self . from_ , recipients , self . message . as_string ( ) ) \n    session . quit ( ) \n    if self . verbose : \n        pass \n    if self . verbose : \n        pass \n    pass "}
{"11784": "\ndef error ( self , message , code = 1 ) : \n    pass \n    sys . exit ( code ) "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    pass \n    lnum = 1 \n    try : \n        while True : \n            if maxlines : \n                if lnum > maxlines : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    pass \n    inum = 1 \n    try : \n        while True : \n            if maxitems : \n                if inum > maxitems : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11788": "\ndef outfile_input ( extension = None ) : \n    fileok = False \n    while not fileok : \n        filename = string_input ( 'File name? ' ) \n        if extension : \n            if not filename . endswith ( extension ) : \n                if extension . startswith ( '.' ) : \n                    filename = filename + extension \n                else : \n                    filename = filename + '.' + extension \n        if os . path . isfile ( filename ) : \n            choice = choice_input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) \n            if choice == 'y' : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'a' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = True \n                except IOError : \n                    pass \n                except PermissionError : \n                    pass \n                except FileNotFoundError : \n                    pass \n        else : \n            choice = choice_input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) \n            if choice == 'y' : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'w' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = True \n                except IOError : \n                    pass \n                except PermissionError : \n                    pass \n                except FileNotFoundError : \n                    pass \n    return filename "}
{"11805": "\ndef cache ( func ) : \n    CACHE_DIR = appdirs . user_cache_dir ( 'sportsref' , getpass . getuser ( ) ) \n    if not os . path . isdir ( CACHE_DIR ) : \n        os . makedirs ( CACHE_DIR ) \n    \n    @ funcutils . wraps ( func ) \n    def wrapper ( url ) : \n        file_hash = hashlib . md5 ( ) \n        encoded_url = url . encode ( errors = 'replace' ) \n        file_hash . update ( encoded_url ) \n        file_hash = file_hash . hexdigest ( ) \n        filename = '{}/{}' . format ( CACHE_DIR , file_hash ) \n        sport_id = None \n        for a_base_url , a_sport_id in sportsref . SITE_ABBREV . items ( ) : \n            if url . startswith ( a_base_url ) : \n                sport_id = a_sport_id \n                break \n        else : \n            pass \n        file_exists = os . path . isfile ( filename ) \n        if sport_id and file_exists : \n            cur_time = int ( time . time ( ) ) \n            mod_time = int ( os . path . getmtime ( filename ) ) \n            days_since_mod = datetime . timedelta ( seconds = ( cur_time - mod_time ) ) . days \n            days_cache_valid = globals ( ) [ '_days_valid_{}' . format ( sport_id ) ] ( url ) \n            cache_is_valid = days_since_mod < days_cache_valid \n        else : \n            cache_is_valid = False \n        allow_caching = sportsref . get_option ( 'cache' ) \n        if file_exists and cache_is_valid and allow_caching : \n            with codecs . open ( filename , 'r' , encoding = 'utf-8' , errors = 'replace' ) as f : \n                text = f . read ( ) \n        else : \n            text = func ( url ) \n            with codecs . open ( filename , 'w+' , encoding = 'utf-8' ) as f : \n                f . write ( text ) \n        return text \n    return wrapper "}
{"11807": "\ndef memoize ( fun ) : \n    \n    @ funcutils . wraps ( fun ) \n    def wrapper ( * args , ** kwargs ) : \n        do_memoization = sportsref . get_option ( 'memoize' ) \n        if not do_memoization : \n            return fun ( * args , ** kwargs ) \n        hash_args = tuple ( args ) \n        hash_kwargs = frozenset ( sorted ( kwargs . items ( ) ) ) \n        key = ( hash_args , hash_kwargs ) \n        def _copy ( v ) : \n            if isinstance ( v , pq ) : \n                return v . clone ( ) \n            else : \n                return copy . deepcopy ( v ) \n        try : \n            ret = _copy ( cache [ key ] ) \n            return ret \n        except KeyError : \n            cache [ key ] = fun ( * args , ** kwargs ) \n            ret = _copy ( cache [ key ] ) \n            return ret \n        except TypeError : \n            pass \n            raise \n    cache = { } \n    return wrapper "}
{"11838": "\ndef rel_url_to_id ( url ) : \n    yearRegex = r'.*/years/(\\d{4}).*|.*/gamelog/(\\d{4}).*' \n    playerRegex = r'.*/players/(?:\\w/)?(.+?)(?:/|\\.html?)' \n    boxscoresRegex = r'.*/boxscores/(.+?)\\.html?' \n    teamRegex = r'.*/teams/(\\w{3})/.*' \n    coachRegex = r'.*/coaches/(.+?)\\.html?' \n    stadiumRegex = r'.*/stadiums/(.+?)\\.html?' \n    refRegex = r'.*/officials/(.+?r)\\.html?' \n    collegeRegex = r'.*/schools/(\\S+?)/.*|.*college=([^&]+)' \n    hsRegex = r'.*/schools/high_schools\\.cgi\\?id=([^\\&]{8})' \n    bsDateRegex = r'.*/boxscores/index\\.f?cgi\\?(month=\\d+&day=\\d+&year=\\d+)' \n    leagueRegex = r'.*/leagues/(.*_\\d{4}).*' \n    awardRegex = r'.*/awards/(.+)\\.htm' \n    regexes = [ yearRegex , playerRegex , boxscoresRegex , teamRegex , coachRegex , stadiumRegex , refRegex , collegeRegex , hsRegex , bsDateRegex , leagueRegex , awardRegex , ] \n    for regex in regexes : \n        match = re . match ( regex , url , re . I ) \n        if match : \n            return [ _f for _f in match . groups ( ) if _f ] [ 0 ] \n    if any ( url . startswith ( s ) for s in ( '/play-index/' , ) ) : \n        return url \n    pass \n    return url "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( 0 , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                pass \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"12008": "\ndef check_version ( ) : \n    import requests \n    r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) \n    r = r [ 'info' ] [ 'version' ] \n    if r != version : \n        pass \n    return r "}
{"12012": "\ndef print_update ( self , repo_name , repo_path ) : \n    color = Color ( ) \n    self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo_name , repo_path ) , \"green\" ) ) \n    try : \n        repo = Repository ( repo_path ) \n        repo . update ( ) \n    except RepositoryError as e : \n        self . logger . error ( e ) \n        pass \n    pass "}
{"12019": "\ndef print_status ( self , repo_name , repo_path ) : \n    color = Color ( ) \n    self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo_name , repo_path ) , \"green\" ) ) \n    try : \n        repo = Repository ( repo_path ) \n        repo . status ( ) \n    except RepositoryError as e : \n        self . logger . error ( e ) \n        pass \n    pass "}
{"12039": "\ndef show_all ( self ) : \n    for ws in self . workspace . list ( ) . keys ( ) : \n        self . show_workspace ( ws ) \n        pass "}
{"12111": "\ndef _send_request ( self ) : \n    msg = Message ( ) \n    msg . subject = \"An RPC call!\" \n    msg . address = self . _to \n    msg . reply_to = self . _reply_to \n    msg . body = self . _method \n    msg . correlation_id = 5 \n    pass \n    self . _sender . send ( msg , self ) "}
{"12267": "\ndef do_POST ( self ) : \n    self . send_response ( urllib2 . httplib . OK ) \n    self . end_headers ( ) \n    content_length = int ( self . headers [ 'Content-Length' ] ) \n    body = self . rfile . read ( content_length ) \n    pass \n    pass \n    pass \n    pass "}
{"12282": "\ndef _call_api ( self ) : \n    sockobj = socket ( AF_INET , SOCK_STREAM ) \n    sockobj . connect ( ( self . rpc_host , self . rpc_port ) ) \n    self . get_json ( ) \n    message = [ self . rpc_message . encode ( 'utf-8' ) ] \n    for line in message : \n        sockobj . send ( line ) \n        data = sockobj . recv ( self . MAX_LINE ) \n        pass \n        self . rpc_data . append ( data ) \n    sockobj . close ( ) "}
{"12301": "\ndef get_remote_file_size ( self , url ) : \n    try : \n        req = urllib . request . urlopen ( url ) \n        return int ( req . getheader ( 'Content-Length' ) . strip ( ) ) \n    except urllib . error . HTTPError as error : \n        logger . error ( 'Error retrieving size of the remote file %s' % error ) \n        pass \n        self . connect_earthexplorer ( ) \n        self . get_remote_file_size ( url ) "}
{"12302": "\ndef download ( self , bands = None , download_dir = None , metadata = False ) : \n    if not download_dir : \n        download_dir = DOWNLOAD_DIR \n    if bands is None : \n        bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] \n    else : \n        self . validate_bands ( bands ) \n    pattern = re . compile ( '^[^\\s]+_(.+)\\.tiff?' , re . I ) \n    band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] \n    image_list = [ ] \n    self . connect_earthexplorer ( ) \n    tgzname = self . sceneInfo . name + '.tgz' \n    dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) \n    downloaded = self . download_file ( self . url , dest_dir , tgzname ) \n    logger . debug ( 'Status downloaded %s' % downloaded ) \n    pass \n    if downloaded [ 'sucess' ] : \n        pass \n        logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) \n        try : \n            tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) \n            folder_path = join ( download_dir , self . sceneInfo . name ) \n            tar . extractall ( folder_path ) \n            remove ( downloaded [ 'file_path' ] ) \n            images_path = listdir ( folder_path ) \n            for image_path in images_path : \n                matched = pattern . match ( image_path ) \n                file_path = join ( folder_path , image_path ) \n                if matched and matched . group ( 1 ) in band_list : \n                    image_list . append ( [ file_path , getsize ( file_path ) ] ) \n                elif matched : \n                    remove ( file_path ) \n        except tarfile . ReadError as error : \n            pass \n            logger . error ( 'Error when extracting files. %s' % error ) \n        return image_list \n    else : \n        logger . debug ( 'Info downloaded: %s' % downloaded ) \n        pass \n        return downloaded "}
{"12304": "\ndef connect_earthexplorer ( self ) : \n    logger . info ( \"Establishing connection to Earthexplorer\" ) \n    pass \n    try : \n        opener = urllib . request . build_opener ( urllib . request . HTTPCookieProcessor ( ) ) \n        urllib . request . install_opener ( opener ) \n        params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) \n        params = params . encode ( 'utf-8' ) \n        f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) \n        data = f . read ( ) . decode ( 'utf-8' ) \n        f . close ( ) \n        if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > 0 : \n            pass \n            logger . error ( \"Authentification failed\" ) \n            raise AutenticationUSGSFailed ( 'Authentification USGS failed' ) \n        pass \n        logger . debug ( 'User %s connected with USGS' % self . user ) \n        return \n    except Exception as e : \n        pass \n        raise logger . error ( 'Error when trying to connect USGS: %s' % e ) "}
{"12311": "\ndef _dump_text ( self ) : \n    results = self . _relay_output [ 'result' ] ; \n    for l in results : \n        dt = time . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" , time . gmtime ( int ( l [ 1 ] [ 'ts' ] ) ) ) \n        pass "}
{"12349": "\ndef _handle_results ( self ) : \n    if self . _api_result . status_code != requests . codes . ok : \n        pass "}
{"12371": "\ndef output_csv ( self , text ) : \n    payload = json . loads ( text ) \n    pass \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) \n        for s in r [ 1 ] : \n            pass "}
{"12372": "\ndef output_json ( self , text ) : \n    payload = json . loads ( text ) \n    data = [ ] \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) \n        for s in r [ 1 ] : \n            data . append ( { \"timestamp\" : timestamp , \"metric\" : metric_name , \"aggregate\" : self . aggregate , \"source\" : s [ 0 ] , \"value\" : s [ 1 ] , } ) \n    payload = { \"data\" : data } \n    out = json . dumps ( payload , indent = self . _indent , separators = ( ',' , ': ' ) ) \n    pass "}
{"12373": "\ndef output_raw ( self , text ) : \n    payload = json . loads ( text ) \n    out = json . dumps ( payload , sort_keys = True , indent = self . _indent , separators = ( ',' , ': ' ) ) \n    pass "}
{"12374": "\ndef output_xml ( self , text ) : \n    document = Element ( 'results' ) \n    comment = Comment ( 'Generated by TrueSight Pulse measurement-get CLI' ) \n    document . append ( comment ) \n    aggregates = SubElement ( document , 'aggregates' ) \n    aggregate = SubElement ( aggregates , 'aggregate' ) \n    measurements = SubElement ( aggregate , 'measurements' ) \n    payload = json . loads ( text ) \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) \n        for s in r [ 1 ] : \n            measure_node = SubElement ( measurements , 'measure' ) \n            source = s [ 0 ] \n            value = str ( s [ 1 ] ) \n            ts_node = SubElement ( measure_node , 'timestamp' ) \n            ts_node . text = str ( timestamp ) \n            metric_node = SubElement ( measure_node , 'metric' ) \n            metric_node . text = metric_name \n            metric_node = SubElement ( measure_node , 'aggregate' ) \n            metric_node . text = self . aggregate \n            source_node = SubElement ( measure_node , 'source' ) \n            source_node . text = source \n            value_node = SubElement ( measure_node , 'value' ) \n            value_node . text = value \n    rough_string = ElementTree . tostring ( document , 'utf-8' ) \n    reparse = minidom . parseString ( rough_string ) \n    output = reparse . toprettyxml ( indent = \" \" ) \n    pass "}
{"12376": "\ndef pprint ( root , depth = 0 , space_unit = \"    \" , * , source_len = 0 , file = None ) : \n    spacing = space_unit * depth \n    if isinstance ( root , str ) : \n        pass \n    else : \n        if root . position is None : \n            position = - 1 \n        elif root . position < 0 : \n            position = source_len + root . position \n        else : \n            position = root . position \n        if root . is_value : \n            pass \n        else : \n            pass \n            for child in root . children : \n                pprint ( child , depth + 1 , source_len = source_len , file = file ) "}
{"12498": "\ndef dump_nodes ( self ) : \n    pass \n    try : \n        pass \n        for k , v in self . id_cache . items ( ) : \n            pass \n        pass \n        for k , v in self . tag_cache . items ( ) : \n            pass \n        pass \n        for k , v in self . rule_nodes . items ( ) : \n            txt = \"['%s']=%d\" % ( k , id ( v ) ) \n            if k in self . tag_cache : \n                tag = self . tag_cache [ k ] \n                txt += \" tag <%s>\" % tag \n                k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n                if k in self . _stream . value_cache : \n                    txt += \" cache <%s>\" % self . _stream . value_cache [ k ] \n            pass \n    except Exception as err : \n        pass \n    import sys \n    sys . stdout . flush ( ) \n    return True "}
{"12516": "\ndef echo_nodes ( self , * rest ) : \n    txt = \"\" \n    for thing in rest : \n        if isinstance ( thing , Node ) : \n            txt += self . value ( thing ) \n        else : \n            txt += str ( thing ) \n    pass \n    return True "}
{"12525": "\ndef set_node ( self , dst , src ) : \n    if not isinstance ( src , Node ) : \n        dst . value = src \n    else : \n        dst . set ( src ) \n        idsrc = id ( src ) \n        iddst = id ( dst ) \n        if iddst not in self . id_cache : \n            pass \n            pass \n            pass \n        if idsrc in self . id_cache : \n            k = self . id_cache [ idsrc ] \n            k2 = self . id_cache [ iddst ] \n            if k in self . rule_nodes : \n                self . tag_cache [ k2 ] = self . tag_cache [ k ] \n    return True "}
{"12644": "\ndef loadModelData ( self , name ) : \n    path = self . resourceNameToPath ( name , \".json\" ) \n    try : \n        data = json . load ( open ( path , \"r\" ) ) \n    except Exception : \n        pass \n        import traceback ; \n        traceback . print_exc ( ) \n        return { } \n    out = { } \n    if data . get ( \"version\" , 1 ) == 1 : \n        out [ \"materials\" ] = { } \n        for name , matdata in data . get ( \"materials\" , { } ) . items ( ) : \n            m = model . Material ( self , name , matdata ) \n            out [ \"materials\" ] [ name ] = m \n        out [ \"default_material\" ] = out [ \"materials\" ] [ data . get ( \"default_material\" , list ( out [ \"materials\" ] . keys ( ) ) [ 0 ] ) ] \n        out [ \"bones\" ] = { \"__root__\" : model . RootBone ( self , \"__root__\" , { \"start_rot\" : [ 0 , 0 ] , \"length\" : 0 } ) } \n        for name , bonedata in data . get ( \"bones\" , { } ) . items ( ) : \n            b = model . Bone ( self , name , bonedata ) \n            out [ \"bones\" ] [ name ] = b \n        for name , bone in out [ \"bones\" ] . items ( ) : \n            if name == \"__root__\" : \n                continue \n            bone . setParent ( out [ \"bones\" ] [ bone . bonedata [ \"parent\" ] ] ) \n        out [ \"regions\" ] = { } \n        for name , regdata in data . get ( \"regions\" , { } ) . items ( ) : \n            r = model . Region ( self , name , regdata ) \n            r . material = out [ \"materials\" ] [ regdata . get ( \"material\" , out [ \"default_material\" ] ) ] \n            r . bone = out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] \n            out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] . addRegion ( r ) \n            out [ \"regions\" ] [ name ] = r \n        out [ \"animations\" ] = { } \n        out [ \"animations\" ] [ \"static\" ] = model . Animation ( self , \"static\" , { \"type\" : \"static\" , \"bones\" : { } } ) \n        for name , anidata in data . get ( \"animations\" , { } ) . items ( ) : \n            a = model . Animation ( self , name , anidata ) \n            a . setBones ( out [ \"bones\" ] ) \n            out [ \"animations\" ] [ name ] = a \n        out [ \"default_animation\" ] = out [ \"animations\" ] [ data . get ( \"default_animation\" , out [ \"animations\" ] [ \"static\" ] ) ] \n    else : \n        raise ValueError ( \"Unknown version %s of model '%s'\" % ( data . get ( \"version\" , 1 ) , name ) ) \n    self . modelcache [ name ] = out \n    return out "}
{"12667": "\ndef check_elements ( self ) : \n    existing_types = set ( self . elements . type . argiope . values . flatten ( ) ) \n    allowed_types = set ( ELEMENTS . keys ( ) ) \n    if ( existing_types <= allowed_types ) == False : \n        raise ValueError ( \"Element types {0} not in know elements {1}\" . format ( existing_types - allowed_types , allowed_types ) ) \n    pass "}
{"12679": "\ndef run_postproc ( self ) : \n    t0 = time . time ( ) \n    if self . verbose : \n        pass \n    if self . solver == \"abaqus\" : \n        command = '{0} viewer noGUI={1}_abqpp.py' . format ( self . solver_path , self . label ) \n        process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n        for line in iter ( process . stdout . readline , b'' ) : \n            line = line . rstrip ( ) . decode ( 'utf8' ) \n            pass \n    t1 = time . time ( ) \n    if self . verbose : \n        pass "}
{"12763": "\ndef make_app ( ) : \n    env = Environment ( ) \n    args = parser . parse_args ( args = [ '/' , '--ignore-stdin' ] , env = env ) \n    args . output_options = 'HB' \n    server = 'HTTPony/{0}' . format ( __version__ ) \n    def application ( environ , start_response ) : \n        if environ . get ( 'CONTENT_LENGTH' ) == '' : \n            del environ [ 'CONTENT_LENGTH' ] \n        if environ . get ( 'CONTENT_TYPE' ) == '' : \n            del environ [ 'CONTENT_TYPE' ] \n        wrequest = WerkzeugRequest ( environ ) \n        data = wrequest . get_data ( ) \n        request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) \n        prepared = request . prepare ( ) \n        stream = streams . build_output_stream ( args , env , prepared , response = None , output_options = args . output_options ) \n        streams . write_stream ( stream , env . stdout , env . stdout_isatty ) \n        if data : \n            pass \n        response = Response ( headers = { 'Server' : server } ) \n        return response ( environ , start_response ) \n    return application "}
{"12810": "\ndef main ( argv = sys . argv ) : \n    args = parse ( argv ) \n    hostname = args . listen \n    port = args . port \n    pass \n    logging . getLogger ( 'werkzeug' ) . setLevel ( logging . CRITICAL ) \n    plugin_manager . load_installed_plugins ( ) \n    app = make_app ( ) \n    run_simple ( hostname , port , app ) "}
{"12932": "\ndef update_file ( url , filename ) : \n    resp = urlopen ( url ) \n    if resp . code != 200 : \n        raise Exception ( 'GET {} failed.' . format ( url ) ) \n    with open ( _get_package_path ( filename ) , 'w' ) as fp : \n        for l in resp : \n            if not l . startswith ( b'#' ) : \n                fp . write ( l . decode ( 'utf8' ) ) \n    pass "}
{"12994": "\ndef _zsh_comp_command ( self , zcf , cmd , grouping , add_help = True ) : \n    if add_help : \n        if grouping : \n            pass \n        pass \n        pass \n    no_comp = ( 'store_true' , 'store_false' ) \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        if meta . cmd_kwargs . get ( 'action' ) == 'append' : \n            grpfmt , optfmt = \"+ '{}'\" , \"'*{}[{}]{}'\" \n            if meta . comprule is None : \n                meta . comprule = '' \n        else : \n            grpfmt , optfmt = \"+ '({})'\" , \"'{}[{}]{}'\" \n        if meta . cmd_kwargs . get ( 'action' ) in no_comp or meta . cmd_kwargs . get ( 'nargs' ) == 0 : \n            meta . comprule = None \n        if meta . comprule is None : \n            compstr = '' \n        elif meta . comprule == '' : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :( )' \n        else : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :{}' . format ( meta . comprule ) \n        if grouping : \n            pass \n        for name in _names ( self . _conf [ sct ] , opt ) : \n            pass "}
{"12995": "\ndef zsh_complete ( self , path , cmd , * cmds , sourceable = False ) : \n    grouping = internal . zsh_version ( ) >= ( 5 , 4 ) \n    path = pathlib . Path ( path ) \n    firstline = [ '#compdef' , cmd ] \n    firstline . extend ( cmds ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as zcf : \n        pass \n        pass \n        pass \n        pass \n        if subcmds : \n            substrs = [ \"{}\\\\:'{}'\" . format ( sub , self . subcmds [ sub ] . help ) for sub in subcmds ] \n            pass \n        self . _zsh_comp_command ( zcf , None , grouping ) \n        if subcmds : \n            pass \n            pass \n            for sub in subcmds : \n                pass \n            pass \n        pass \n        for sub in subcmds : \n            pass \n            pass \n            self . _zsh_comp_command ( zcf , sub , grouping ) \n            pass \n        if sourceable : \n            pass "}
{"12997": "\ndef bash_complete ( self , path , cmd , * cmds ) : \n    path = pathlib . Path ( path ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as bcf : \n        pass \n        pass \n        pass \n        optstr = ' ' . join ( self . _bash_comp_command ( None ) ) \n        pass \n        if subcmds : \n            pass \n            pass \n        for sub in subcmds : \n            optstr = ' ' . join ( self . _bash_comp_command ( sub ) ) \n            pass \n        condstr = 'if' \n        for sub in subcmds : \n            pass \n            pass \n            condstr = 'elif' \n        pass \n        pass \n        if subcmds : \n            pass \n            pass \n        pass \n        pass \n        pass "}
{"13027": "\ndef get_host ( name ) : \n    f = { 'instance-state-name' : 'running' , 'tag:Name' : name } \n    ec2 = boto . connect_ec2 ( region = get_region ( ) ) \n    rs = ec2 . get_all_instances ( filters = f ) \n    if len ( rs ) == 0 : \n        raise Exception ( 'Host \"%s\" not found' % name ) \n    pass "}
{"13054": "\ndef get_input ( prompt , default = None , exit_msg = 'bye!' ) : \n    try : \n        response = six . moves . input ( prompt ) \n    except ( KeyboardInterrupt , EOFError ) : \n        pass \n        pass \n        exit ( ) \n    try : \n        return int ( response ) \n    except ValueError : \n        if response . strip ( ) == \"\" and default is not None : \n            return default \n        else : \n            return response "}
{"13072": "\ndef _copy_to ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = False , local_path = local_path , remote_path = remote_path ) \n        pass \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    pass "}
{"13073": "\ndef _copy_from ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    paths = set ( ) \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        _local_path = entry . format_string ( local_path ) \n        if _local_path in paths : \n            raise ValueError ( 'Duplicate local paths: one or more paths ' 'had value {} after formatting.' . format ( local_path ) ) \n        paths . add ( _local_path ) \n        _folder = os . path . split ( _local_path ) [ 0 ] \n        if len ( _folder ) > 0 : \n            if not os . path . exists ( _folder ) : \n                pass \n                os . makedirs ( _folder ) \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = True , local_path = _local_path , remote_path = remote_path ) \n        pass \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    pass "}
{"13074": "\ndef _run_ssh_command ( entries , username , idfile , command , tunnel , parallel = False ) : \n    if len ( entries ) == 0 : \n        pass \n        return 1 \n    if command . strip ( ) == '' or command is None : \n        raise ValueError ( 'No command given' ) \n    pass \n    shell_cmds = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_ssh_command ( hname , username , idfile , command , tunnel ) \n        shell_cmds . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( shell_cmds , parallel = parallel ) \n    pass "}
{"13075": "\ndef _connect_ssh ( entry , username , idfile , tunnel = None ) : \n    if entry . hostname != \"\" and entry . hostname is not None : \n        _host = entry . hostname \n    elif entry . public_ip != \"\" and entry . public_ip is not None : \n        _host = entry . public_ip \n    elif entry . private_ip != \"\" and entry . private_ip is not None : \n        if tunnel is None : \n            raise ValueError ( \"Entry does not have a hostname or public IP. \" \"You can connect via private IP if you use a \" \"tunnel.\" ) \n        _host = entry . private_ip \n    else : \n        raise ValueError ( \"No hostname, public IP or private IP information \" \"found on host entry. I don't know how to connect.\" ) \n    command = _build_ssh_command ( _host , username , idfile , None , tunnel ) \n    pass \n    pass \n    proc = subprocess . Popen ( command , shell = True ) \n    return proc . wait ( ) "}
{"13120": "\ndef pack_dir_cmd ( ) : \n    parser = argparse . ArgumentParser ( description = inspect . getdoc ( part_edit_cmd ) ) \n    parser . add_argument ( 'path' , help = ( 'Path to list (including path to zip file, ' 'i.e. ./file.zipx or ./file.zipx/subdir)' ) , ) \n    args = parser . parse_args ( ) \n    for item , is_file in sorted ( list_contents ( args . path ) ) : \n        prefix = 'd ' if not is_file else '  ' \n        msg = prefix + item \n        pass "}
{"13253": "\ndef clock_on_right ( mystring ) : \n    taken = length_no_ansi ( mystring ) \n    padding = ( get_terminal_size ( ) . columns - 1 ) - taken - 5 \n    clock = time . strftime ( \"%I:%M\" , time . localtime ( ) ) \n    pass "}
{"13261": "\ndef pretty_print ( input_word , anagrams , by_length = False ) : \n    scores = { } \n    if by_length : \n        noun = \"tiles\" \n        for word , score in anagrams : \n            try : \n                scores [ len ( word ) ] . append ( \"{0} ({1:d})\" . format ( word , score ) ) \n            except KeyError : \n                scores [ len ( word ) ] = [ \"{0} ({1:d})\" . format ( word , score ) ] \n    else : \n        noun = \"points\" \n        for word , score in anagrams : \n            try : \n                scores [ score ] . append ( word ) \n            except KeyError : \n                scores [ score ] = [ word ] \n    pass \n    if not valid_scrabble_word ( input_word ) : \n        pass \n    for key , value in sorted ( scores . items ( ) , reverse = True ) : \n        pass "}
{"13286": "\ndef main ( clargs = None ) : \n    from argparse import ArgumentParser \n    from librarian . library import Library \n    import sys \n    parser = ArgumentParser ( description = \"A test runner for each card in a librarian library.\" ) \n    parser . add_argument ( \"library\" , help = \"Library database\" ) \n    parser . add_argument ( \"-t\" , \"--tests\" , default = \"test/\" , help = \"Test directory\" ) \n    args = parser . parse_args ( clargs ) \n    descovery ( args . tests ) \n    library = Library ( args . library ) \n    cardcount , passes , failures = execute_tests ( library ) \n    pass \n    sys . exit ( failures ) "}
{"13309": "\ndef serve ( self , server = None ) : \n    if server is None : \n        from wsgiref . simple_server import make_server \n        server = lambda app : make_server ( '' , 8000 , app ) . serve_forever ( ) \n        pass \n    try : \n        server ( self ) \n    finally : \n        server . socket . close ( ) "}
{"13334": "\ndef run ( self , args_list = None ) : \n    self . log . debug ( \"Application.run: {args_list}\" . format ( ** locals ( ) ) ) \n    retval = None \n    try : \n        retval = self . _run ( args_list = args_list ) \n    except KeyboardInterrupt : \n        self . log . verbose ( \"Interrupted\" ) \n    except SystemExit as exit : \n        self . log . verbose ( \"Exited\" ) \n        retval = exit . code \n    except Exception : \n        pass \n        traceback . print_exc ( ) \n        if \"debug_pdb\" in self . args and self . args . debug_pdb : \n            debugger ( ) \n        retval = Application . UNCAUGHT_EXCEPTION_EXIT \n        raise \n    finally : \n        try : \n            self . _atexit ( ) \n        finally : \n            sys . stderr . flush ( ) \n            sys . stdout . flush ( ) \n            sys . exit ( retval ) "}
{"13396": "\ndef _get_index_mappings ( self ) : \n    maps = { } \n    for fname in self . indexed_features : \n        config = self . indexes . get ( fname , { } ) \n        pass \n        maps [ fname_to_idx_name ( fname ) ] = { 'type' : config . get ( 'es_index_type' , 'integer' ) , 'store' : False , 'index' : 'not_analyzed' , } \n    for fname in self . fulltext_indexed_features : \n        maps [ fname_to_full_idx_name ( fname ) ] = { 'type' : 'string' , 'store' : False , 'index' : 'analyzed' , } \n    return maps "}
{"13466": "\ndef search ( self , query , verbose = 0 ) : \n    if verbose > 0 : \n        pass \n    query = query . lower ( ) \n    qgram = ng ( query , self . slb ) \n    qocument = set ( ) \n    for q in qgram : \n        if q in self . ngrams . keys ( ) : \n            for i in self . ngrams [ q ] : \n                qocument . add ( i ) \n    self . qocument = qocument \n    results = { } \n    for i in qocument : \n        for j in self . D [ i ] . keys ( ) : \n            if not j in results . keys ( ) : \n                results [ j ] = 0 \n            results [ j ] = results [ j ] + self . D [ i ] [ j ] \n    sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n    return [ self . elements [ f [ 0 ] ] for f in sorted_results ] "}
{"13480": "\ndef dead_code ( ) : \n    with safe_cd ( SRC ) : \n        if IS_TRAVIS : \n            command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) \n        else : \n            command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) \n        output_file_name = \"dead_code.txt\" \n        with open ( output_file_name , \"w\" ) as outfile : \n            env = config_pythonpath ( ) \n            subprocess . call ( command , stdout = outfile , env = env ) \n        cutoff = 20 \n        num_lines = sum ( 1 for line in open ( output_file_name ) if line ) \n        if num_lines > cutoff : \n            pass \n            exit ( - 1 ) "}
{"13495": "\ndef create_inputhook_qt4 ( mgr , app = None ) : \n    if app is None : \n        app = QtCore . QCoreApplication . instance ( ) \n        if app is None : \n            app = QtGui . QApplication ( [ \" \" ] ) \n    ip = InteractiveShell . instance ( ) \n    if hasattr ( ip , '_inputhook_qt4' ) : \n        return app , ip . _inputhook_qt4 \n    got_kbdint = [ False ] \n    def inputhook_qt4 ( ) : \n        try : \n            allow_CTRL_C ( ) \n            app = QtCore . QCoreApplication . instance ( ) \n            if not app : \n                return 0 \n            app . processEvents ( QtCore . QEventLoop . AllEvents , 300 ) \n            if not stdin_ready ( ) : \n                timer = QtCore . QTimer ( ) \n                timer . timeout . connect ( app . quit ) \n                while not stdin_ready ( ) : \n                    timer . start ( 50 ) \n                    app . exec_ ( ) \n                    timer . stop ( ) \n        except KeyboardInterrupt : \n            ignore_CTRL_C ( ) \n            got_kbdint [ 0 ] = True \n            pass \n            mgr . clear_inputhook ( ) \n        except : \n            ignore_CTRL_C ( ) \n            from traceback import print_exc \n            print_exc ( ) \n            pass \n            mgr . clear_inputhook ( ) \n        finally : \n            allow_CTRL_C ( ) \n        return 0 \n    def preprompthook_qt4 ( ishell ) : \n        if got_kbdint [ 0 ] : \n            mgr . set_inputhook ( inputhook_qt4 ) \n        got_kbdint [ 0 ] = False \n    ip . _inputhook_qt4 = inputhook_qt4 \n    ip . set_hook ( 'pre_prompt_hook' , preprompthook_qt4 ) \n    return app , inputhook_qt4 "}
{"13512": "\ndef OnTimeToClose ( self , evt ) : \n    pass \n    sys . stdout . flush ( ) \n    self . cleanup_consoles ( evt ) \n    self . Close ( ) \n    sys . exit ( ) "}
{"13543": "\ndef _stdin_raw_nonblock ( self ) : \n    handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) \n    result = WaitForSingleObject ( handle , 100 ) \n    if result == WAIT_FAILED : \n        raise ctypes . WinError ( ) \n    elif result == WAIT_TIMEOUT : \n        pass \n        return None \n    else : \n        data = ctypes . create_string_buffer ( 256 ) \n        bytesRead = DWORD ( 0 ) \n        pass \n        if not ReadFile ( handle , data , 256 , ctypes . byref ( bytesRead ) , None ) : \n            raise ctypes . WinError ( ) \n        FlushConsoleInputBuffer ( handle ) \n        data = data . value \n        data = data . replace ( '\\r\\n' , '\\n' ) \n        data = data . replace ( '\\r' , '\\n' ) \n        pass \n        return data "}
{"13552": "\ndef passwd ( passphrase = None , algorithm = 'sha1' ) : \n    if passphrase is None : \n        for i in range ( 3 ) : \n            p0 = getpass . getpass ( 'Enter password: ' ) \n            p1 = getpass . getpass ( 'Verify password: ' ) \n            if p0 == p1 : \n                passphrase = p0 \n                break \n            else : \n                pass \n        else : \n            raise UsageError ( 'No matching passwords found. Giving up.' ) \n    h = hashlib . new ( algorithm ) \n    salt = ( '%0' + str ( salt_len ) + 'x' ) % random . getrandbits ( 4 * salt_len ) \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return ':' . join ( ( algorithm , salt , h . hexdigest ( ) ) ) "}
{"13569": "\ndef handle_iopub ( self ) : \n    while self . km . sub_channel . msg_ready ( ) : \n        sub_msg = self . km . sub_channel . get_msg ( ) \n        msg_type = sub_msg [ 'header' ] [ 'msg_type' ] \n        parent = sub_msg [ \"parent_header\" ] \n        if ( not parent ) or self . session_id == parent [ 'session' ] : \n            if msg_type == 'status' : \n                if sub_msg [ \"content\" ] [ \"execution_state\" ] == \"busy\" : \n                    pass \n            elif msg_type == 'stream' : \n                if sub_msg [ \"content\" ] [ \"name\" ] == \"stdout\" : \n                    pass \n                    io . stdout . flush ( ) \n                elif sub_msg [ \"content\" ] [ \"name\" ] == \"stderr\" : \n                    pass \n                    io . stderr . flush ( ) \n            elif msg_type == 'pyout' : \n                self . execution_count = int ( sub_msg [ \"content\" ] [ \"execution_count\" ] ) \n                format_dict = sub_msg [ \"content\" ] [ \"data\" ] \n                hook = self . displayhook \n                hook . start_displayhook ( ) \n                hook . write_output_prompt ( ) \n                hook . write_format_data ( format_dict ) \n                hook . log_output ( format_dict ) \n                hook . finish_displayhook ( ) "}
{"13618": "\ndef warn ( msg , level = 2 , exit_val = 1 ) : \n    if level > 0 : \n        header = [ '' , '' , 'WARNING: ' , 'ERROR: ' , 'FATAL ERROR: ' ] \n        io . stderr . write ( '%s%s' % ( header [ level ] , msg ) ) \n        if level == 4 : \n            pass \n            sys . exit ( exit_val ) "}
{"13638": "\ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    try : \n        start = time . clock ( ) \n        status = CoverageScript ( ) . command_line ( argv ) \n        end = time . clock ( ) \n        if 0 : \n            pass \n    except ExceptionDuringRun : \n        _ , err , _ = sys . exc_info ( ) \n        traceback . print_exception ( * err . args ) \n        status = ERR \n    except CoverageException : \n        _ , err , _ = sys . exc_info ( ) \n        pass \n        status = ERR \n    except SystemExit : \n        _ , err , _ = sys . exc_info ( ) \n        if err . args : \n            status = err . args [ 0 ] \n        else : \n            status = None \n    return status "}
{"13642": "\ndef help ( self , error = None , topic = None , parser = None ) : \n    assert error or topic or parser \n    if error : \n        pass \n        pass \n    elif parser : \n        pass \n    else : \n        help_msg = HELP_TOPICS . get ( topic , '' ) . strip ( ) \n        if help_msg : \n            pass \n        else : \n            pass "}
{"13646": "\ndef do_debug ( self , args ) : \n    if not args : \n        self . help_fn ( \"What information would you like: data, sys?\" ) \n        return ERR \n    for info in args : \n        if info == 'sys' : \n            pass \n            for line in info_formatter ( self . coverage . sysinfo ( ) ) : \n                pass \n        elif info == 'data' : \n            pass \n            self . coverage . load ( ) \n            pass \n            pass \n            summary = self . coverage . data . summary ( fullpath = True ) \n            if summary : \n                filenames = sorted ( summary . keys ( ) ) \n                pass \n                for f in filenames : \n                    pass \n            else : \n                pass \n        else : \n            self . help_fn ( \"Don't know what you mean by %r\" % info ) \n            return ERR \n    return OK "}
{"13671": "\ndef push ( self , variables , interactive = True ) : \n    vdict = None \n    if isinstance ( variables , dict ) : \n        vdict = variables \n    elif isinstance ( variables , ( basestring , list , tuple ) ) : \n        if isinstance ( variables , basestring ) : \n            vlist = variables . split ( ) \n        else : \n            vlist = variables \n        vdict = { } \n        cf = sys . _getframe ( 1 ) \n        for name in vlist : \n            try : \n                vdict [ name ] = eval ( name , cf . f_globals , cf . f_locals ) \n            except : \n                pass \n    else : \n        raise ValueError ( 'variables must be a dict/str/list/tuple' ) \n    self . user_ns . update ( vdict ) \n    user_ns_hidden = self . user_ns_hidden \n    if interactive : \n        user_ns_hidden . difference_update ( vdict ) \n    else : \n        user_ns_hidden . update ( vdict ) "}
{"13679": "\ndef _showtraceback ( self , etype , evalue , stb ) : \n    pass "}
{"13695": "\ndef run_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    if silent : \n        store_history = False \n    self . input_splitter . push ( raw_cell ) \n    if self . input_splitter . cell_magic_parts : \n        self . _current_cell_magic_body = '' . join ( self . input_splitter . cell_magic_parts ) \n    cell = self . input_splitter . source_reset ( ) \n    with self . builtin_trap : \n        prefilter_failed = False \n        if len ( cell . splitlines ( ) ) == 1 : \n            try : \n                cell = self . prefilter_manager . prefilter_lines ( cell ) + '\\n' \n            except AliasError as e : \n                error ( e ) \n                prefilter_failed = True \n            except Exception : \n                self . showtraceback ( ) \n                prefilter_failed = True \n        if store_history : \n            self . history_manager . store_inputs ( self . execution_count , cell , raw_cell ) \n        if not silent : \n            self . logger . log ( cell , raw_cell ) \n        if not prefilter_failed : \n            cell_name = self . compile . cache ( cell , self . execution_count ) \n            with self . display_trap : \n                try : \n                    code_ast = self . compile . ast_parse ( cell , filename = cell_name ) \n                except IndentationError : \n                    self . showindentationerror ( ) \n                    if store_history : \n                        self . execution_count += 1 \n                    return None \n                except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n                    self . showsyntaxerror ( ) \n                    if store_history : \n                        self . execution_count += 1 \n                    return None \n                interactivity = \"none\" if silent else self . ast_node_interactivity \n                self . run_ast_nodes ( code_ast . body , cell_name , interactivity = interactivity ) \n                post_exec = [ ] if silent else self . _post_execute . iteritems ( ) \n                for func , status in post_exec : \n                    if self . disable_failing_post_execute and not status : \n                        continue \n                    try : \n                        func ( ) \n                    except KeyboardInterrupt : \n                        pass \n                    except Exception : \n                        self . _post_execute [ func ] = False \n                        self . showtraceback ( ) \n                        pass \n    if store_history : \n        self . history_manager . store_output ( self . execution_count ) \n        self . execution_count += 1 "}
{"13724": "\ndef emit ( self , msg , level = 1 , debug = False ) : \n    if debug : \n        if not self . debug : \n            return \n        stream = sys . stderr \n    else : \n        if self . verbose < level : \n            return \n        stream = sys . stdout \n    pass \n    stream . flush ( ) "}
{"13777": "\ndef _get_build_prefix ( ) : \n    path = os . path . join ( tempfile . gettempdir ( ) , 'pip_build_%s' % __get_username ( ) . replace ( ' ' , '_' ) ) \n    if WINDOWS : \n        return path \n    try : \n        os . mkdir ( path ) \n        write_delete_marker_file ( path ) \n    except OSError : \n        file_uid = None \n        try : \n            file_uid = get_path_uid ( path ) \n        except OSError : \n            file_uid = None \n        if file_uid != os . geteuid ( ) : \n            msg = ( \"The temporary folder for building (%s) is either not owned by\" \" you, or is a symlink.\" % path ) \n            pass \n            pass \n            raise exceptions . InstallationError ( msg ) \n    return path "}
{"13785": "\ndef sleep_here ( count , t ) : \n    import time , sys \n    pass \n    sys . stdout . flush ( ) \n    time . sleep ( t ) \n    return count , t "}
{"13788": "\ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    reg_url = cfg [ 'url' ] \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = 0 ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while True : \n        try : \n            idents , msg = session . recv ( sub , mode = 0 ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ 0 ] \n        if msg [ 'msg_type' ] == 'stream' : \n            pass \n        elif msg [ 'msg_type' ] == 'pyerr' : \n            c = msg [ 'content' ] \n            pass \n            for line in c [ 'traceback' ] : \n                pass "}
{"13814": "\ndef crash_handler_lite ( etype , evalue , tb ) : \n    traceback . print_exception ( etype , evalue , tb ) \n    from IPython . core . interactiveshell import InteractiveShell \n    if InteractiveShell . initialized ( ) : \n        config = \"%config \" \n    else : \n        config = \"c.\" \n    pass "}
{"13819": "\ndef process_handler ( cmd , callback , stderr = subprocess . PIPE ) : \n    sys . stdout . flush ( ) \n    sys . stderr . flush ( ) \n    close_fds = sys . platform != 'win32' \n    p = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = stderr , close_fds = close_fds ) \n    try : \n        out = callback ( p ) \n    except KeyboardInterrupt : \n        pass \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        out = None \n    finally : \n        if p . returncode is None : \n            try : \n                p . terminate ( ) \n                p . poll ( ) \n            except OSError : \n                pass \n        if p . returncode is None : \n            try : \n                p . kill ( ) \n            except OSError : \n                pass \n    return out "}
{"13864": "\ndef create_zipfile ( context ) : \n    if not prerequisites_ok ( ) : \n        return \n    subprocess . call ( [ 'make' , 'zip' ] ) \n    for zipfile in glob . glob ( '*.zip' ) : \n        first_part = zipfile . split ( '.' ) [ 0 ] \n        new_name = \"%s.%s.zip\" % ( first_part , context [ 'version' ] ) \n        target = os . path . join ( context [ 'workingdir' ] , new_name ) \n        shutil . copy ( zipfile , target ) \n        pass "}
{"13917": "\ndef print_wordfreq ( freqs , n = 10 ) : \n    words , counts = freqs . keys ( ) , freqs . values ( ) \n    items = zip ( counts , words ) \n    items . sort ( reverse = True ) \n    for ( count , word ) in items [ : n ] : \n        pass "}
{"14000": "\ndef write_format_data ( self , format_dict ) : \n    result_repr = format_dict [ 'text/plain' ] \n    if '\\n' in result_repr : \n        prompt_template = self . shell . prompt_manager . out_template \n        if prompt_template and not prompt_template . endswith ( '\\n' ) : \n            result_repr = '\\n' + result_repr \n    pass "}
{"14050": "\ndef clear_output ( self , stdout = True , stderr = True , other = True ) : \n    if stdout : \n        pass \n        io . stdout . flush ( ) \n    if stderr : \n        pass \n        io . stderr . flush ( ) "}
{"14062": "\ndef wait_interactive ( self , interval = 1. , timeout = None ) : \n    N = len ( self ) \n    tic = time . time ( ) \n    while not self . ready ( ) and ( timeout is None or time . time ( ) - tic <= timeout ) : \n        self . wait ( interval ) \n        clear_output ( ) \n        pass \n        sys . stdout . flush ( ) \n    pass \n    pass "}
{"14144": "\ndef page_dumb ( strng , start = 0 , screen_lines = 25 ) : \n    out_ln = strng . splitlines ( ) [ start : ] \n    screens = chop ( out_ln , screen_lines - 1 ) \n    if len ( screens ) == 1 : \n        pass \n    else : \n        last_escape = \"\" \n        for scr in screens [ 0 : - 1 ] : \n            hunk = os . linesep . join ( scr ) \n            pass \n            if not page_more ( ) : \n                return \n            esc_list = esc_re . findall ( hunk ) \n            if len ( esc_list ) > 0 : \n                last_escape = esc_list [ - 1 ] \n        pass "}
{"14183": "\ndef debug ( self , level , message ) : \n    if self . _debug >= level : \n        pass "}
{"14210": "\ndef check ( self , check_all = False ) : \n    if not self . enabled and not check_all : \n        return \n    if check_all or self . check_all : \n        modules = sys . modules . keys ( ) \n    else : \n        modules = self . modules . keys ( ) \n    for modname in modules : \n        m = sys . modules . get ( modname , None ) \n        if modname in self . skip_modules : \n            continue \n        if not hasattr ( m , '__file__' ) : \n            continue \n        if m . __name__ == '__main__' : \n            continue \n        filename = m . __file__ \n        path , ext = os . path . splitext ( filename ) \n        if ext . lower ( ) == '.py' : \n            ext = PY_COMPILED_EXT \n            pyc_filename = pyfile . cache_from_source ( filename ) \n            py_filename = filename \n        else : \n            pyc_filename = filename \n            try : \n                py_filename = pyfile . source_from_cache ( filename ) \n            except ValueError : \n                continue \n        try : \n            pymtime = os . stat ( py_filename ) . st_mtime \n            if pymtime <= os . stat ( pyc_filename ) . st_mtime : \n                continue \n            if self . failed . get ( py_filename , None ) == pymtime : \n                continue \n        except OSError : \n            continue \n        try : \n            superreload ( m , reload , self . old_objects ) \n            if py_filename in self . failed : \n                del self . failed [ py_filename ] \n        except : \n            pass \n            self . failed [ py_filename ] = pymtime "}
{"14227": "\ndef _handle_execute_reply ( self , msg ) : \n    parent = msg [ 'parent_header' ] \n    msg_id = parent [ 'msg_id' ] \n    if msg_id not in self . outstanding : \n        if msg_id in self . history : \n            pass \n        else : \n            pass \n    else : \n        self . outstanding . remove ( msg_id ) \n    content = msg [ 'content' ] \n    header = msg [ 'header' ] \n    md = self . metadata [ msg_id ] \n    md . update ( self . _extract_metadata ( header , parent , content ) ) \n    self . metadata [ msg_id ] = md \n    e_outstanding = self . _outstanding_dict [ md [ 'engine_uuid' ] ] \n    if msg_id in e_outstanding : \n        e_outstanding . remove ( msg_id ) \n    if content [ 'status' ] == 'ok' : \n        self . results [ msg_id ] = ExecuteReply ( msg_id , content , md ) \n    elif content [ 'status' ] == 'aborted' : \n        self . results [ msg_id ] = error . TaskAborted ( msg_id ) \n    elif content [ 'status' ] == 'resubmitted' : \n        pass \n    else : \n        self . results [ msg_id ] = self . _unwrap_exception ( content ) "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            pass \n        if toktype == token . INDENT : \n            indent += 1 \n        elif toktype == token . DEDENT : \n            indent -= 1 \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + 1 ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14317": "\ndef _convert_to_metadata ( ) : \n    import glob \n    for fname in glob . glob ( '*.ipynb' ) : \n        pass \n        with open ( fname , 'r' ) as f : \n            nb = read ( f , u'json' ) \n        md = new_metadata ( ) \n        if u'name' in nb : \n            md . name = nb . name \n            del nb [ u'name' ] \n        nb . metadata = md \n        with open ( fname , 'w' ) as f : \n            write ( nb , f , u'json' ) "}
{"14328": "\ndef print_list_lines ( self , filename , first , last ) : \n    try : \n        Colors = self . color_scheme_table . active_colors \n        ColorsNormal = Colors . Normal \n        tpl_line = '%%s%s%%s %s%%s' % ( Colors . lineno , ColorsNormal ) \n        tpl_line_em = '%%s%s%%s %s%%s%s' % ( Colors . linenoEm , Colors . line , ColorsNormal ) \n        src = [ ] \n        for lineno in range ( first , last + 1 ) : \n            line = linecache . getline ( filename , lineno ) \n            if not line : \n                break \n            if lineno == self . curframe . f_lineno : \n                line = self . __format_line ( tpl_line_em , filename , lineno , line , arrow = True ) \n            else : \n                line = self . __format_line ( tpl_line , filename , lineno , line , arrow = False ) \n            src . append ( line ) \n            self . lineno = lineno \n        pass \n    except KeyboardInterrupt : \n        pass "}
{"14388": "\ndef _process_execute_ok ( self , msg ) : \n    payload = msg [ 'content' ] [ 'payload' ] \n    for item in payload : \n        if not self . _process_execute_payload ( item ) : \n            warning = 'Warning: received unknown payload of type %s' \n            pass "}
{"14524": "\ndef raw_input_multi ( header = '' , ps1 = '==> ' , ps2 = '..> ' , terminate_str = '.' ) : \n    try : \n        if header : \n            header += '\\n' \n        lines = [ raw_input ( header + ps1 ) ] \n    except EOFError : \n        return [ ] \n    terminate = [ terminate_str ] \n    try : \n        while lines [ - 1 : ] != terminate : \n            new_line = raw_input ( ps1 ) \n            while new_line . endswith ( '\\\\' ) : \n                new_line = new_line [ : - 1 ] + raw_input ( ps2 ) \n            lines . append ( new_line ) \n        return lines [ : - 1 ] \n    except EOFError : \n        pass \n        return lines "}
{"14570": "\ndef handle ( self , * args , ** options ) : \n    try : \n        while True : \n            Channel ( HEARTBEAT_CHANNEL ) . send ( { 'time' : time . time ( ) } ) \n            time . sleep ( HEARTBEAT_FREQUENCY ) \n    except KeyboardInterrupt : \n        pass "}
{"14588": "\ndef writeout_cache ( self , conn = None ) : \n    if conn is None : \n        conn = self . db \n    with self . db_input_cache_lock : \n        try : \n            self . _writeout_input_cache ( conn ) \n        except sqlite3 . IntegrityError : \n            self . new_session ( conn ) \n            pass \n            try : \n                self . _writeout_input_cache ( conn ) \n            except sqlite3 . IntegrityError : \n                pass \n        finally : \n            self . db_input_cache = [ ] \n    with self . db_output_cache_lock : \n        try : \n            self . _writeout_output_cache ( conn ) \n        except sqlite3 . IntegrityError : \n            pass \n        finally : \n            self . db_output_cache = [ ] "}
{"14605": "\ndef _system_body ( p ) : \n    enc = DEFAULT_ENCODING \n    for line in read_no_interrupt ( p . stdout ) . splitlines ( ) : \n        line = line . decode ( enc , 'replace' ) \n        pass \n    for line in read_no_interrupt ( p . stderr ) . splitlines ( ) : \n        line = line . decode ( enc , 'replace' ) \n        pass \n    return p . wait ( ) "}
{"14676": "\ndef _get_index ( self , index ) : \n    if index is None : \n        if self . finished : \n            pass \n            return None \n        index = self . block_index \n    else : \n        self . _validate_index ( index ) \n    return index "}
{"14679": "\ndef show ( self , index = None ) : \n    index = self . _get_index ( index ) \n    if index is None : \n        return \n    pass \n    pass \n    sys . stdout . flush ( ) "}
{"14680": "\ndef show_all ( self ) : \n    fname = self . title \n    title = self . title \n    nblocks = self . nblocks \n    silent = self . _silent \n    marquee = self . marquee \n    for index , block in enumerate ( self . src_blocks_colored ) : \n        if silent [ index ] : \n            pass \n        else : \n            pass \n        pass \n    sys . stdout . flush ( ) "}
{"14690": "\ndef cleanup ( controller , engines ) : \n    import signal , time \n    pass \n    pass \n    for e in engines : \n        e . send_signal ( signal . SIGINT ) \n    pass \n    controller . send_signal ( signal . SIGINT ) \n    time . sleep ( 0.1 ) \n    pass \n    controller . kill ( ) \n    pass "}
{"14702": "\ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = False \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules += module_list ( path ) \n        if time ( ) - t >= TIMEOUT_STORAGE and not store : \n            store = True \n            pass \n            pass \n            sys . stdout . flush ( ) \n        if time ( ) - t > TIMEOUT_GIVEUP : \n            pass \n            ip . db [ 'rootmodules' ] = [ ] \n            return [ ] \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2 : \n        pass \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14844": "\ndef getfigs ( * fig_nums ) : \n    from matplotlib . _pylab_helpers import Gcf \n    if not fig_nums : \n        fig_managers = Gcf . get_all_fig_managers ( ) \n        return [ fm . canvas . figure for fm in fig_managers ] \n    else : \n        figs = [ ] \n        for num in fig_nums : \n            f = Gcf . figs . get ( num ) \n            if f is None : \n                pass \n            else : \n                figs . append ( f . canvas . figure ) \n        return figs "}
{"14859": "\ndef pause ( self ) : \n    for tracer in self . tracers : \n        tracer . stop ( ) \n        stats = tracer . get_stats ( ) \n        if stats : \n            pass \n            for k in sorted ( stats . keys ( ) ) : \n                pass \n    threading . settrace ( None ) "}
{"14895": "\ndef system ( self , cmd ) : \n    enc = DEFAULT_ENCODING \n    patterns = [ pexpect . TIMEOUT , pexpect . EOF ] \n    EOF_index = patterns . index ( pexpect . EOF ) \n    out_size = 0 \n    try : \n        if hasattr ( pexpect , 'spawnb' ) : \n            child = pexpect . spawnb ( self . sh , args = [ '-c' , cmd ] ) \n        else : \n            child = pexpect . spawn ( self . sh , args = [ '-c' , cmd ] ) \n        flush = sys . stdout . flush \n        while True : \n            res_idx = child . expect_list ( patterns , self . read_timeout ) \n            pass \n            flush ( ) \n            if res_idx == EOF_index : \n                break \n            out_size = len ( child . before ) \n    except KeyboardInterrupt : \n        child . sendline ( chr ( 3 ) ) \n        try : \n            out_size = len ( child . before ) \n            child . expect_list ( patterns , self . terminate_timeout ) \n            pass \n            sys . stdout . flush ( ) \n        except KeyboardInterrupt : \n            pass \n        finally : \n            child . terminate ( force = True ) \n    child . isalive ( ) \n    return child . exitstatus "}
