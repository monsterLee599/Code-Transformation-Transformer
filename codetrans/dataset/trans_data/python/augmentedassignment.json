{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256 \n    for index , _ in enumerate ( data ) : \n        t = index \n        for i in range ( 8 ) : \n            t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) \n        e = o = r = - 1 \n        i , a = 0 , len ( path ) \n        while i < a : \n            e = ord ( path [ i ] ) \n            i = i + ( 1 ) \n            if e < 128 : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] \n            else : \n                if e < 2048 : \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n                else : \n                    if 55296 <= e < 57344 : \n                        e = ( 1023 & e ) + 64 \n                        i = i + ( 1 ) \n                        o = 1023 & t . url ( i ) \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] \n                    else : \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) \n    while 1 : \n        url = tmp ( ) \n        if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : \n            return url "}
{"11": "\ndef get_mgtv_real_url ( url ) : \n    content = loads ( get_content ( url ) ) \n    m3u_url = content [ 'info' ] \n    split = urlsplit ( m3u_url ) \n    base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) \n    content = get_content ( content [ 'info' ] ) \n    segment_list = [ ] \n    segments_size = 0 \n    for i in content . split ( ) : \n        if not i . startswith ( '#' ) : \n            segment_list . append ( base_url + i ) \n        elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : \n            segments_size = segments_size + ( int ( i [ i . rfind ( ':' ) + 1 : ] ) ) \n    return m3u_url , segments_size , segment_list "}
{"15": "\ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if sourceType == 'sina' : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'youku' : \n        youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n    elif sourceType == 'tudou' : \n        tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'qq' : \n        qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'letv' : \n        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'zhuzhan' : \n        url = 'http://www.acfun.cn/v/ac' + vid \n        yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n        seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n        for t in seq : \n            if yk_streams . get ( t ) : \n                preferred = yk_streams [ t ] \n                break \n        size = 0 \n        for url in preferred [ 0 ] : \n            _ , _ , seg_size = url_info ( url ) \n            size = size + ( seg_size ) \n        if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : \n            ext = 'flv' \n        else : \n            ext = 'mp4' \n        print_info ( site_info , title , ext , size ) \n        if not info_only : \n            download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) \n    else : \n        raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping danmaku.' ) \n            return \n        try : \n            title = get_filename ( title ) \n            print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"16": "\ndef matchall ( text , patterns ) : \n    ret = [ ] \n    for pattern in patterns : \n        match = re . findall ( pattern , text ) \n        ret = ret + ( match ) \n    return ret "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess = excess + ( 1 ) \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_ready_workers_running < num_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            elif num_workers_running > num_workers_expected : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                for _ in range ( excess ) : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess = excess - ( 1 ) \n                    wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n            elif num_workers_running == num_workers_expected : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10 ) \n                if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"58": "\ndef _normalize_mlengine_job_id ( job_id ) : \n    match = re . search ( r'\\d|\\{{2}' , job_id ) \n    if match and match . start ( ) == 0 : \n        job = 'z_{}' . format ( job_id ) \n    else : \n        job = job_id \n    tracker = 0 \n    cleansed_job_id = '' \n    for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : \n        cleansed_job_id = cleansed_job_id + ( re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) ) \n        cleansed_job_id = cleansed_job_id + ( job [ m . start ( ) : m . end ( ) ] ) \n        tracker = m . end ( ) \n    cleansed_job_id = cleansed_job_id + ( re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) ) \n    return cleansed_job_id "}
{"109": "\ndef list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : \n    if include_examples is None : \n        include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) \n    file_paths = [ ] \n    if directory is None : \n        return [ ] \n    elif os . path . isfile ( directory ) : \n        return [ directory ] \n    elif os . path . isdir ( directory ) : \n        patterns_by_dir = { } \n        for root , dirs , files in os . walk ( directory , followlinks = True ) : \n            patterns = patterns_by_dir . get ( root , [ ] ) \n            ignore_file = os . path . join ( root , '.airflowignore' ) \n            if os . path . isfile ( ignore_file ) : \n                with open ( ignore_file , 'r' ) as f : \n                    patterns = patterns + ( [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] ) \n            dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] \n            for d in dirs : \n                patterns_by_dir [ os . path . join ( root , d ) ] = patterns \n            for f in files : \n                try : \n                    file_path = os . path . join ( root , f ) \n                    if not os . path . isfile ( file_path ) : \n                        continue \n                    mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) \n                    if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : \n                        continue \n                    if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : \n                        continue \n                    might_contain_dag = True \n                    if safe_mode and not zipfile . is_zipfile ( file_path ) : \n                        with open ( file_path , 'rb' ) as fp : \n                            content = fp . read ( ) \n                            might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) \n                    if not might_contain_dag : \n                        continue \n                    file_paths . append ( file_path ) \n                except Exception : \n                    log = LoggingMixin ( ) . log \n                    log . exception ( \"Error while examining %s\" , f ) \n    if include_examples : \n        import airflow . example_dags \n        example_dag_folder = airflow . example_dags . __path__ [ 0 ] \n        file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) \n    return file_paths "}
{"123": "\ndef heartbeat ( self ) : \n    finished_processors = { } \n    running_processors = { } \n    for file_path , processor in self . _processors . items ( ) : \n        if processor . done : \n            self . log . debug ( \"Processor for %s finished\" , file_path ) \n            now = timezone . utcnow ( ) \n            finished_processors [ file_path ] = processor \n            self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) \n            self . _last_finish_time [ file_path ] = now \n            self . _run_count [ file_path ] = self . _run_count [ file_path ] + ( 1 ) \n        else : \n            running_processors [ file_path ] = processor \n    self . _processors = running_processors \n    self . log . debug ( \"%s/%s DAG parsing processes running\" , len ( self . _processors ) , self . _parallelism ) \n    self . log . debug ( \"%s file paths queued for processing\" , len ( self . _file_path_queue ) ) \n    simple_dags = [ ] \n    for file_path , processor in finished_processors . items ( ) : \n        if processor . result is None : \n            self . log . warning ( \"Processor for %s exited with return code %s.\" , processor . file_path , processor . exit_code ) \n        else : \n            for simple_dag in processor . result : \n                simple_dags . append ( simple_dag ) \n    if len ( self . _file_path_queue ) == 0 : \n        file_paths_in_progress = self . _processors . keys ( ) \n        now = timezone . utcnow ( ) \n        file_paths_recently_processed = [ ] \n        for file_path in self . _file_paths : \n            last_finish_time = self . get_last_finish_time ( file_path ) \n            if ( last_finish_time is not None and ( now - last_finish_time ) . total_seconds ( ) < self . _file_process_interval ) : \n                file_paths_recently_processed . append ( file_path ) \n        files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] \n        files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) \n        for file_path , processor in self . _processors . items ( ) : \n            self . log . debug ( \"File path %s is still being processed (started: %s)\" , processor . file_path , processor . start_time . isoformat ( ) ) \n        self . log . debug ( \"Queuing the following files for processing:\\n\\t%s\" , \"\\n\\t\" . join ( files_paths_to_queue ) ) \n        self . _file_path_queue . extend ( files_paths_to_queue ) \n    zombies = self . _find_zombies ( ) \n    while ( self . _parallelism - len ( self . _processors ) > 0 and len ( self . _file_path_queue ) > 0 ) : \n        file_path = self . _file_path_queue . pop ( 0 ) \n        processor = self . _processor_factory ( file_path , zombies ) \n        processor . start ( ) \n        self . log . debug ( \"Started a process (PID: %s) to generate tasks for %s\" , processor . pid , file_path ) \n        self . _processors [ file_path ] = processor \n    self . _run_count [ self . _heart_beat_key ] = self . _run_count [ self . _heart_beat_key ] + ( 1 ) \n    return simple_dags "}
{"134": "\ndef wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : \n    while timeout > 0 : \n        operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) \n        if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) \n        timeout = timeout - ( TIME_TO_SLEEP_IN_SECONDS ) \n    raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" ) "}
{"158": "\ndef template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : \n    text = utils . unescape ( text ) \n    try : \n        template_fields = get_template_field ( app . env , text ) \n    except RoleException as e : \n        msg = inliner . reporter . error ( \"invalid class name %s \\n%s\" % ( text , e , ) , line = lineno ) \n        prb = inliner . problematic ( rawtext , rawtext , msg ) \n        return [ prb ] , [ msg ] \n    node = nodes . inline ( rawtext = rawtext ) \n    for i , field in enumerate ( template_fields ) : \n        if i != 0 : \n            node = node + ( nodes . Text ( \", \" ) ) \n        node = node + ( nodes . literal ( field , \"\" , nodes . Text ( field ) ) ) \n    return [ node ] , [ ] "}
{"167": "\ndef pprinttable ( rows ) : \n    if not rows : \n        return \n    if hasattr ( rows [ 0 ] , '_fields' ) : \n        headers = rows [ 0 ] . _fields \n    else : \n        headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] \n    lens = [ len ( s ) for s in headers ] \n    for row in rows : \n        for i in range ( len ( rows [ 0 ] ) ) : \n            slenght = len ( \"{}\" . format ( row [ i ] ) ) \n            if slenght > lens [ i ] : \n                lens [ i ] = slenght \n    formats = [ ] \n    hformats = [ ] \n    for i in range ( len ( rows [ 0 ] ) ) : \n        if isinstance ( rows [ 0 ] [ i ] , int ) : \n            formats . append ( \"%%%dd\" % lens [ i ] ) \n        else : \n            formats . append ( \"%%-%ds\" % lens [ i ] ) \n        hformats . append ( \"%%-%ds\" % lens [ i ] ) \n    pattern = \" | \" . join ( formats ) \n    hpattern = \" | \" . join ( hformats ) \n    separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) \n    s = \"\" \n    s = s + ( separator + '\\n' ) \n    s = s + ( ( hpattern % tuple ( headers ) ) + '\\n' ) \n    s = s + ( separator + '\\n' ) \n    def f ( t ) : \n        return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t \n    for line in rows : \n        s = s + ( pattern % tuple ( f ( t ) for t in line ) + '\\n' ) \n    s = s + ( separator + '\\n' ) \n    return s "}
{"173": "\ndef run_cli ( self , pig , verbose = True ) : \n    with TemporaryDirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : \n        with NamedTemporaryFile ( dir = tmp_dir ) as f : \n            f . write ( pig . encode ( 'utf-8' ) ) \n            f . flush ( ) \n            fname = f . name \n            pig_bin = 'pig' \n            cmd_extra = [ ] \n            pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra \n            if self . pig_properties : \n                pig_properties_list = self . pig_properties . split ( ) \n                pig_cmd . extend ( pig_properties_list ) \n            if verbose : \n                self . log . info ( \"%s\" , \" \" . join ( pig_cmd ) ) \n            sp = subprocess . Popen ( pig_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = True ) \n            self . sp = sp \n            stdout = '' \n            for line in iter ( sp . stdout . readline , b'' ) : \n                stdout = stdout + ( line . decode ( 'utf-8' ) ) \n                if verbose : \n                    self . log . info ( line . strip ( ) ) \n            sp . wait ( ) \n            if sp . returncode : \n                raise AirflowException ( stdout ) \n            return stdout "}
{"292": "\ndef _do_api_call ( self , endpoint_info , json ) : \n    method , endpoint = endpoint_info \n    url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) \n    if 'token' in self . databricks_conn . extra_dejson : \n        self . log . info ( 'Using token auth.' ) \n        auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) \n    else : \n        self . log . info ( 'Using basic auth.' ) \n        auth = ( self . databricks_conn . login , self . databricks_conn . password ) \n    if method == 'GET' : \n        request_func = requests . get \n    elif method == 'POST' : \n        request_func = requests . post \n    else : \n        raise AirflowException ( 'Unexpected HTTP Method: ' + method ) \n    attempt_num = 1 \n    while True : \n        try : \n            response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) \n            response . raise_for_status ( ) \n            return response . json ( ) \n        except requests_exceptions . RequestException as e : \n            if not _retryable_error ( e ) : \n                raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) \n            self . _log_request_error ( attempt_num , e ) \n        if attempt_num == self . retry_limit : \n            raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) \n        attempt_num = attempt_num + ( 1 ) \n        sleep ( self . retry_delay ) "}
{"340": "\ndef list ( self , bucket_name , versions = None , max_results = None , prefix = None , delimiter = None ) : \n    client = self . get_conn ( ) \n    bucket = client . get_bucket ( bucket_name = bucket_name ) \n    ids = [ ] \n    pageToken = None \n    while True : \n        blobs = bucket . list_blobs ( max_results = max_results , page_token = pageToken , prefix = prefix , delimiter = delimiter , versions = versions ) \n        blob_names = [ ] \n        for blob in blobs : \n            blob_names . append ( blob . name ) \n        prefixes = blobs . prefixes \n        if prefixes : \n            ids = ids + ( list ( prefixes ) ) \n        else : \n            ids = ids + ( blob_names ) \n        pageToken = blobs . next_page_token \n        if pageToken is None : \n            break \n    return ids "}
{"367": "\ndef _prepare_cli_cmd ( self ) : \n    conn = self . conn \n    hive_bin = 'hive' \n    cmd_extra = [ ] \n    if self . use_beeline : \n        hive_bin = 'beeline' \n        jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) \n        if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : \n            template = conn . extra_dejson . get ( 'principal' , \"hive/_HOST@EXAMPLE.COM\" ) \n            if \"_HOST\" in template : \n                template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) \n            proxy_user = \"\" \n            if conn . extra_dejson . get ( 'proxy_user' ) == \"login\" and conn . login : \n                proxy_user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) \n            elif conn . extra_dejson . get ( 'proxy_user' ) == \"owner\" and self . run_as : \n                proxy_user = \"hive.server2.proxy.user={0}\" . format ( self . run_as ) \n            jdbc_url = jdbc_url + ( \";principal={template};{proxy_user}\" . format ( template = template , proxy_user = proxy_user ) ) \n        elif self . auth : \n            jdbc_url = jdbc_url + ( \";auth=\" + self . auth ) \n        jdbc_url = '\"{}\"' . format ( jdbc_url ) \n        cmd_extra = cmd_extra + ( [ '-u' , jdbc_url ] ) \n        if conn . login : \n            cmd_extra = cmd_extra + ( [ '-n' , conn . login ] ) \n        if conn . password : \n            cmd_extra = cmd_extra + ( [ '-p' , conn . password ] ) \n    hive_params_list = self . hive_cli_params . split ( ) \n    return [ hive_bin ] + cmd_extra + hive_params_list "}
{"370": "\ndef load_file ( self , filepath , table , delimiter = \",\" , field_dict = None , create = True , overwrite = True , partition = None , recreate = False , tblproperties = None ) : \n    hql = '' \n    if recreate : \n        hql = hql + ( \"DROP TABLE IF EXISTS {table};\\n\" . format ( table = table ) ) \n    if create or recreate : \n        if field_dict is None : \n            raise ValueError ( \"Must provide a field dict when creating a table\" ) \n        fields = \",\\n    \" . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) \n        hql = hql + ( \"CREATE TABLE IF NOT EXISTS {table} (\\n{fields})\\n\" . format ( table = table , fields = fields ) ) \n        if partition : \n            pfields = \",\\n    \" . join ( [ p + \" STRING\" for p in partition ] ) \n            hql = hql + ( \"PARTITIONED BY ({pfields})\\n\" . format ( pfields = pfields ) ) \n        hql = hql + ( \"ROW FORMAT DELIMITED\\n\" ) \n        hql = hql + ( \"FIELDS TERMINATED BY '{delimiter}'\\n\" . format ( delimiter = delimiter ) ) \n        hql = hql + ( \"STORED AS textfile\\n\" ) \n        if tblproperties is not None : \n            tprops = \", \" . join ( [ \"'{0}'='{1}'\" . format ( k , v ) for k , v in tblproperties . items ( ) ] ) \n            hql = hql + ( \"TBLPROPERTIES({tprops})\\n\" . format ( tprops = tprops ) ) \n    hql = hql + ( \";\" ) \n    self . log . info ( hql ) \n    self . run_cli ( hql ) \n    hql = \"LOAD DATA LOCAL INPATH '{filepath}' \" . format ( filepath = filepath ) \n    if overwrite : \n        hql = hql + ( \"OVERWRITE \" ) \n    hql = hql + ( \"INTO TABLE {table} \" . format ( table = table ) ) \n    if partition : \n        pvals = \", \" . join ( [ \"{0}='{1}'\" . format ( k , v ) for k , v in partition . items ( ) ] ) \n        hql = hql + ( \"PARTITION ({pvals})\" . format ( pvals = pvals ) ) \n    hql = hql + ( ';\\n' ) \n    self . log . info ( hql ) \n    self . run_cli ( hql ) "}
{"408": "\ndef buildhtmlheader ( self ) : \n    self . htmlheader = '' \n    global _js_initialized \n    if '_js_initialized' not in globals ( ) or not _js_initialized : \n        for css in self . header_css : \n            self . htmlheader = self . htmlheader + ( css ) \n        for js in self . header_js : \n            self . htmlheader = self . htmlheader + ( js ) "}
{"409": "\ndef buildcontainer ( self ) : \n    if self . container : \n        return \n    if self . width : \n        if self . width [ - 1 ] != '%' : \n            self . style = self . style + ( 'width:%spx;' % self . width ) \n        else : \n            self . style = self . style + ( 'width:%s;' % self . width ) \n    if self . height : \n        if self . height [ - 1 ] != '%' : \n            self . style = self . style + ( 'height:%spx;' % self . height ) \n        else : \n            self . style = self . style + ( 'height:%s;' % self . height ) \n    if self . style : \n        self . style = 'style=\"%s\"' % self . style \n    self . container = self . containerheader + '<div id=\"%s\"><svg %s></svg></div>\\n' % ( self . name , self . style ) "}
{"432": "\ndef poll_query_status ( self , query_execution_id , max_tries = None ) : \n    try_number = 1 \n    final_query_state = None \n    while True : \n        query_state = self . check_query_status ( query_execution_id ) \n        if query_state is None : \n            self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) \n        elif query_state in self . INTERMEDIATE_STATES : \n            self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) \n        else : \n            self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) \n            final_query_state = query_state \n            break \n        if max_tries and try_number >= max_tries : \n            final_query_state = query_state \n            break \n        try_number = try_number + ( 1 ) \n        sleep ( self . sleep_time ) \n    return final_query_state "}
{"435": "\ndef call ( self , path , query = None , get_all_pages = True , side_loading = False ) : \n    zendesk = self . get_conn ( ) \n    first_request_successful = False \n    while not first_request_successful : \n        try : \n            results = zendesk . call ( path , query ) \n            first_request_successful = True \n        except RateLimitError as rle : \n            self . __handle_rate_limit_exception ( rle ) \n    keys = [ path . split ( \"/\" ) [ - 1 ] . split ( \".json\" ) [ 0 ] ] \n    next_page = results [ 'next_page' ] \n    if side_loading : \n        keys = keys + ( query [ 'include' ] . split ( ',' ) ) \n    results = { key : results [ key ] for key in keys } \n    if get_all_pages : \n        while next_page is not None : \n            try : \n                next_url = next_page . split ( self . __url ) [ 1 ] \n                self . log . info ( \"Calling %s\" , next_url ) \n                more_res = zendesk . call ( next_url ) \n                for key in results : \n                    results [ key ] . extend ( more_res [ key ] ) \n                if next_page == more_res [ 'next_page' ] : \n                    break \n                else : \n                    next_page = more_res [ 'next_page' ] \n            except RateLimitError as rle : \n                self . __handle_rate_limit_exception ( rle ) \n            except ZendeskError as ze : \n                if b\"Use a start_time older than 5 minutes\" in ze . msg : \n                    break \n                else : \n                    raise ze \n    return results "}
{"446": "\ndef filter_for_filesize ( result , size = None ) : \n    if size : \n        log = LoggingMixin ( ) . log \n        log . debug ( 'Filtering for file size >= %s in files: %s' , size , map ( lambda x : x [ 'path' ] , result ) ) \n        size = size * ( settings . MEGABYTE ) \n        result = [ x for x in result if x [ 'length' ] >= size ] \n        log . debug ( 'HdfsSensor.poke: after size filter result is %s' , result ) \n    return result "}
{"462": "\ndef import_table ( self , table , target_dir = None , append = False , file_type = \"text\" , columns = None , split_by = None , where = None , direct = False , driver = None , extra_import_options = None ) : \n    cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) \n    cmd = cmd + ( [ \"--table\" , table ] ) \n    if columns : \n        cmd = cmd + ( [ \"--columns\" , columns ] ) \n    if where : \n        cmd = cmd + ( [ \"--where\" , where ] ) \n    self . Popen ( cmd ) "}
{"463": "\ndef import_query ( self , query , target_dir , append = False , file_type = \"text\" , split_by = None , direct = None , driver = None , extra_import_options = None ) : \n    cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) \n    cmd = cmd + ( [ \"--query\" , query ] ) \n    self . Popen ( cmd ) "}
{"480": "\ndef _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : \n    tis_changed = 0 \n    query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) \n    if self . using_sqlite : \n        tis_to_change = query . with_for_update ( ) . all ( ) \n        for ti in tis_to_change : \n            ti . set_state ( new_state , session = session ) \n            tis_changed = tis_changed + ( 1 ) \n    else : \n        subq = query . subquery ( ) \n        tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) \n        session . commit ( ) \n    if tis_changed > 0 : \n        self . log . warning ( \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state ) "}
{"481": "\ndef __get_concurrency_maps ( self , states , session = None ) : \n    TI = models . TaskInstance \n    ti_concurrency_query = ( session . query ( TI . task_id , TI . dag_id , func . count ( '*' ) ) . filter ( TI . state . in_ ( states ) ) . group_by ( TI . task_id , TI . dag_id ) ) . all ( ) \n    dag_map = defaultdict ( int ) \n    task_map = defaultdict ( int ) \n    for result in ti_concurrency_query : \n        task_id , dag_id , count = result \n        dag_map [ dag_id ] = dag_map [ dag_id ] + ( count ) \n        task_map [ ( dag_id , task_id ) ] = count \n    return dag_map , task_map "}
{"511": "\ndef _build_track_driver_status_command ( self ) : \n    connection_cmd = self . _get_spark_binary_path ( ) \n    connection_cmd = connection_cmd + ( [ \"--master\" , self . _connection [ 'master' ] ] ) \n    if self . _driver_id : \n        connection_cmd = connection_cmd + ( [ \"--status\" , self . _driver_id ] ) \n    else : \n        raise AirflowException ( \"Invalid status: attempted to poll driver \" + \"status but no driver id is known. Giving up.\" ) \n    self . log . debug ( \"Poll driver status cmd: %s\" , connection_cmd ) \n    return connection_cmd "}
{"516": "\ndef _wait_for_task_ended ( self ) : \n    try : \n        waiter = self . client . get_waiter ( 'job_execution_complete' ) \n        waiter . config . max_attempts = sys . maxsize \n        waiter . wait ( jobs = [ self . jobId ] ) \n    except ValueError : \n        retry = True \n        retries = 0 \n        while retries < self . max_retries and retry : \n            self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) \n            response = self . client . describe_jobs ( jobs = [ self . jobId ] ) \n            if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : \n                retry = False \n            sleep ( 1 + pow ( retries * 0.1 , 2 ) ) \n            retries = retries + ( 1 ) "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date = start_date + ( delta ) \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > 0 : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if num > 0 : \n                    start_date = start_date + ( delta ) \n                else : \n                    start_date = start_date - ( delta ) \n    return sorted ( dates ) "}
{"548": "\ndef get_uri ( self ) : \n    conn = self . get_connection ( getattr ( self , self . conn_name_attr ) ) \n    host = conn . host \n    if conn . port is not None : \n        host = host + ( ':{port}' . format ( port = conn . port ) ) \n    conn_type = 'http' if not conn . conn_type else conn . conn_type \n    endpoint = conn . extra_dejson . get ( 'endpoint' , 'pql' ) \n    return '{conn_type}://{host}/{endpoint}' . format ( conn_type = conn_type , host = host , endpoint = endpoint ) "}
{"555": "\ndef insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 , replace = False ) : \n    if target_fields : \n        target_fields = \", \" . join ( target_fields ) \n        target_fields = \"({})\" . format ( target_fields ) \n    else : \n        target_fields = '' \n    i = 0 \n    with closing ( self . get_conn ( ) ) as conn : \n        if self . supports_autocommit : \n            self . set_autocommit ( conn , False ) \n        conn . commit ( ) \n        with closing ( conn . cursor ( ) ) as cur : \n            for i , row in enumerate ( rows , 1 ) : \n                lst = [ ] \n                for cell in row : \n                    lst . append ( self . _serialize_cell ( cell , conn ) ) \n                values = tuple ( lst ) \n                placeholders = [ \"%s\" , ] * len ( values ) \n                if not replace : \n                    sql = \"INSERT INTO \" \n                else : \n                    sql = \"REPLACE INTO \" \n                sql = sql + ( \"{0} {1} VALUES ({2})\" . format ( table , target_fields , \",\" . join ( placeholders ) ) ) \n                cur . execute ( sql , values ) \n                if commit_every and i % commit_every == 0 : \n                    conn . commit ( ) \n                    self . log . info ( \"Loaded %s into %s rows so far\" , i , table ) \n        conn . commit ( ) \n    self . log . info ( \"Done loading. Loaded a total of %s rows\" , i ) "}
{"566": "\ndef _prepare_command ( self , cmd ) : \n    connection_cmd = [ \"spark-sql\" ] \n    if self . _conf : \n        for conf_el in self . _conf . split ( \",\" ) : \n            connection_cmd = connection_cmd + ( [ \"--conf\" , conf_el ] ) \n    if self . _total_executor_cores : \n        connection_cmd = connection_cmd + ( [ \"--total-executor-cores\" , str ( self . _total_executor_cores ) ] ) \n    if self . _executor_cores : \n        connection_cmd = connection_cmd + ( [ \"--executor-cores\" , str ( self . _executor_cores ) ] ) \n    if self . _executor_memory : \n        connection_cmd = connection_cmd + ( [ \"--executor-memory\" , self . _executor_memory ] ) \n    if self . _keytab : \n        connection_cmd = connection_cmd + ( [ \"--keytab\" , self . _keytab ] ) \n    if self . _principal : \n        connection_cmd = connection_cmd + ( [ \"--principal\" , self . _principal ] ) \n    if self . _num_executors : \n        connection_cmd = connection_cmd + ( [ \"--num-executors\" , str ( self . _num_executors ) ] ) \n    if self . _sql : \n        sql = self . _sql . strip ( ) \n        if sql . endswith ( \".sql\" ) or sql . endswith ( \".hql\" ) : \n            connection_cmd = connection_cmd + ( [ \"-f\" , sql ] ) \n        else : \n            connection_cmd = connection_cmd + ( [ \"-e\" , sql ] ) \n    if self . _master : \n        connection_cmd = connection_cmd + ( [ \"--master\" , self . _master ] ) \n    if self . _name : \n        connection_cmd = connection_cmd + ( [ \"--name\" , self . _name ] ) \n    if self . _verbose : \n        connection_cmd = connection_cmd + ( [ \"--verbose\" ] ) \n    if self . _yarn_queue : \n        connection_cmd = connection_cmd + ( [ \"--queue\" , self . _yarn_queue ] ) \n    connection_cmd = connection_cmd + ( cmd ) \n    self . log . debug ( \"Spark-Sql cmd: %s\" , connection_cmd ) \n    return connection_cmd "}
{"580": "\ndef adjust_hue ( img , hue_factor ) : \n    if not ( - 0.5 <= hue_factor <= 0.5 ) : \n        raise ValueError ( 'hue_factor is not in [-0.5, 0.5].' . format ( hue_factor ) ) \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    input_mode = img . mode \n    if input_mode in { 'L' , '1' , 'I' , 'F' } : \n        return img \n    h , s , v = img . convert ( 'HSV' ) . split ( ) \n    np_h = np . array ( h , dtype = np . uint8 ) \n    with np . errstate ( over = 'ignore' ) : \n        np_h = np_h + ( np . uint8 ( hue_factor * 255 ) ) \n    h = Image . fromarray ( np_h , 'L' ) \n    img = Image . merge ( 'HSV' , ( h , s , v ) ) . convert ( input_mode ) \n    return img "}
{"610": "\ndef custom_gradient ( fx , gx , x , fx_gx_manually_stopped = False , name = None ) : \n    def maybe_stop ( x ) : \n        if fx_gx_manually_stopped : \n            return x \n        return tf . stop_gradient ( x ) \n    with tf . compat . v1 . name_scope ( name , 'custom_gradient' , [ fx , gx , x ] ) : \n        fx = tf . convert_to_tensor ( value = fx , name = 'fx' ) \n        with tf . control_dependencies ( [ fx ] ) : \n            if is_list_like ( x ) : \n                x = [ identity ( x_ , name = 'x' ) for x_ in x ] \n            else : \n                x = [ identity ( x , name = 'x' ) ] \n            if is_list_like ( gx ) : \n                gx = [ identity ( gx_ , dtype = fx . dtype , name = 'gx' ) for gx_ in gx ] \n            else : \n                gx = [ identity ( gx , dtype = fx . dtype , name = 'gx' ) ] \n            override_grad = [ ] \n            for x_ , gx_ in zip ( x , gx ) : \n                equal_shape = tf . compat . v1 . assert_equal ( tf . shape ( input = x_ ) , tf . shape ( input = gx_ ) , message = 'Each `x` must have the same shape as each `gx`.' ) \n                with tf . control_dependencies ( [ equal_shape ] ) : \n                    zeros_like_x_ = x_ - tf . stop_gradient ( x_ ) \n                    override_grad . append ( tf . reduce_sum ( input_tensor = maybe_stop ( gx_ ) * zeros_like_x_ ) ) \n            override_grad = sum ( override_grad ) \n            override_grad = override_grad / ( tf . cast ( tf . size ( input = fx ) , dtype = fx . dtype . base_dtype ) ) \n            return maybe_stop ( fx ) + override_grad "}
{"638": "\ndef maybe_broadcast_structure ( from_structure : Any , to_structure : Any ) -> Any : \n    flat_from = tf . nest . flatten ( from_structure ) \n    flat_to = tf . nest . flatten ( to_structure ) \n    if len ( flat_from ) == 1 : \n        flat_from = flat_from * ( len ( flat_to ) ) \n    return tf . nest . pack_sequence_as ( to_structure , flat_from ) "}
{"647": "\ndef random_walk_normal_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_normal_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if len ( scales ) == 1 : \n                scales = scales * ( len ( state_parts ) ) \n            if len ( state_parts ) != len ( scales ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkNormalFn' ) \n            next_state_parts = [ tf . random . normal ( mean = state_part , stddev = scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"648": "\ndef random_walk_uniform_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_uniform_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if len ( scales ) == 1 : \n                scales = scales * ( len ( state_parts ) ) \n            if len ( state_parts ) != len ( scales ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkUniformFn' ) \n            next_state_parts = [ tf . random . uniform ( minval = state_part - scale_part , maxval = state_part + scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"655": "\ndef covertype ( ) : \n    import sklearn . datasets \n    data = sklearn . datasets . covtype . fetch_covtype ( ) \n    features = data . data \n    labels = data . target \n    features = features - ( features . mean ( 0 ) ) \n    features = features / ( features . std ( 0 ) ) \n    features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) \n    features = tf . cast ( features , dtype = tf . float32 ) \n    _ , counts = np . unique ( labels , return_counts = True ) \n    specific_category = np . argmax ( counts ) \n    labels = ( labels == specific_category ) \n    labels = tf . cast ( labels , dtype = tf . int32 ) \n    return features , labels "}
{"660": "\ndef _squeeze ( x , axis ) : \n    x = tf . convert_to_tensor ( value = x , name = 'x' ) \n    if axis is None : \n        return tf . squeeze ( x , axis = None ) \n    axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) \n    axis = axis + ( tf . zeros ( [ 1 ] , dtype = axis . dtype ) ) \n    keep_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) \n    return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep_axis ) ) "}
{"665": "\ndef sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = True , seed = None , name = None ) : \n    if dim < 1 or dim > _MAX_DIMENSION : \n        raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) \n    if ( num_results is None ) == ( sequence_indices is None ) : \n        raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) \n    if not dtype . is_floating : \n        raise ValueError ( 'dtype must be of `float`-type' ) \n    with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : \n        if num_results is not None : \n            num_results = tf . convert_to_tensor ( value = num_results ) \n        if sequence_indices is not None : \n            sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) \n        indices = _get_indices ( num_results , sequence_indices , dtype ) \n        radixes = tf . constant ( _PRIMES [ 0 : dim ] , dtype = dtype , shape = [ dim , 1 ] ) \n        max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) \n        max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) \n        exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , 1 ] ) \n        weight_mask = exponents_by_axes >= max_sizes_by_axes \n        capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) \n        weights = radixes ** capped_exponents \n        coeffs = tf . math . floordiv ( indices , weights ) \n        coeffs = coeffs * ( 1. - tf . cast ( weight_mask , dtype ) ) \n        coeffs = coeffs % ( radixes ) \n        if not randomized : \n            coeffs = coeffs / ( radixes ) \n            return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) \n        coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) \n        coeffs = coeffs * ( 1. - tf . cast ( weight_mask , dtype ) ) \n        coeffs = coeffs / ( radixes ) \n        base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        zero_correction = tf . random . uniform ( [ dim , 1 ] , seed = stream ( ) , dtype = dtype ) \n        zero_correction = zero_correction / ( radixes ** max_sizes_by_axes ) \n        return base_values + tf . reshape ( zero_correction , [ - 1 ] ) "}
{"676": "\ndef _prepare_args ( value_and_gradients_function , initial_step_size , val_initial , val_0 , approximate_wolfe_threshold ) : \n    eval_count = 0 \n    if val_initial is None : \n        if initial_step_size is not None : \n            initial_step_size = tf . convert_to_tensor ( value = initial_step_size ) \n        else : \n            initial_step_size = tf . convert_to_tensor ( value = 1.0 , dtype = tf . float32 ) \n        val_initial = value_and_gradients_function ( initial_step_size ) \n        eval_count = eval_count + ( 1 ) \n    if val_0 is None : \n        x_0 = tf . zeros_like ( val_initial . x ) \n        val_0 = value_and_gradients_function ( x_0 ) \n        eval_count = eval_count + ( 1 ) \n    f_lim = val_0 . f + ( approximate_wolfe_threshold * tf . abs ( val_0 . f ) ) \n    return val_0 , val_initial , f_lim , tf . convert_to_tensor ( value = eval_count ) "}
{"678": "\ndef quadrature_scheme_softmaxnormal_gauss_hermite ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_softmaxnormal_gauss_hermite\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        npdt = dtype_util . as_numpy_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = npdt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs = probs / ( np . linalg . norm ( probs , ord = 1 , keepdims = True ) ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = npdt ) \n        grid = softmax ( - distribution_util . pad ( ( normal_loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * normal_scale [ ... , tf . newaxis ] * grid ) , axis = - 2 , front = True ) , axis = - 2 ) \n        return grid , probs "}
{"679": "\ndef quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"softmax_normal_grid_and_probs\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        dt = dtype_util . base_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) \n        def _get_batch_ndims ( ) : \n            ndims = tensorshape_util . rank ( dist . batch_shape ) \n            if ndims is None : \n                ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] \n            return ndims \n        batch_ndims = _get_batch_ndims ( ) \n        def _get_final_shape ( qs ) : \n            bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , 1 ) \n            num_components = tf . compat . dimension_value ( bs [ - 1 ] ) \n            if num_components is not None : \n                num_components = num_components + ( 1 ) \n            tail = tf . TensorShape ( [ num_components , qs ] ) \n            return bs [ : - 1 ] . concatenate ( tail ) \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + 1 ) ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"713": "\ndef _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : \n    if dist_names is None : \n        dist_names = [ ] \n    else : \n        dist_names = dist_names . copy ( ) \n    n = len ( dist_fn_args ) \n    dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) \n    for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : \n        if not args : \n            continue \n        i = n - i_ - 1 \n        for j , arg_name in enumerate ( args ) : \n            dist_names [ i - j - 1 ] = arg_name \n    j = 0 \n    for i_ in range ( len ( dist_names ) ) : \n        i = n - i_ - 1 \n        if dist_names [ i ] is None : \n            dist_names [ i ] = leaf_name if j == 0 else leaf_name + str ( j ) \n            j = j + ( 1 ) \n    return tuple ( dist_names ) "}
{"737": "\ndef _compute_min_event_ndims ( bijector_list , compute_forward = True ) : \n    min_event_ndims = 0 \n    rank_changed_adjusted_max_min_event_ndims = 0 \n    if compute_forward : \n        bijector_list = reversed ( bijector_list ) \n    for b in bijector_list : \n        if compute_forward : \n            current_min_event_ndims = b . forward_min_event_ndims \n            current_inverse_min_event_ndims = b . inverse_min_event_ndims \n        else : \n            current_min_event_ndims = b . inverse_min_event_ndims \n            current_inverse_min_event_ndims = b . forward_min_event_ndims \n        if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims : \n            min_event_ndims = min_event_ndims + ( ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) ) \n        rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) \n        number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims = rank_changed_adjusted_max_min_event_ndims - ( number_of_changed_dimensions ) \n    return min_event_ndims "}
{"745": "\ndef _log_ndtr_asymptotic_series ( x , series_order ) : \n    npdt = dtype_util . as_numpy_dtype ( x . dtype ) \n    if series_order <= 0 : \n        return npdt ( 1 ) \n    x_2 = tf . square ( x ) \n    even_sum = tf . zeros_like ( x ) \n    odd_sum = tf . zeros_like ( x ) \n    x_2n = x_2 \n    for n in range ( 1 , series_order + 1 ) : \n        y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n \n        if n % 2 : \n            odd_sum = odd_sum + ( y ) \n        else : \n            even_sum = even_sum + ( y ) \n        x_2n = x_2n * ( x_2 ) \n    return 1. + even_sum - odd_sum "}
{"776": "\ndef _log_normalization ( self , name = 'log_normalization' ) : \n    with tf . name_scope ( name or 'log_normalization_lkj' ) : \n        logpi = np . log ( np . pi ) \n        ans = tf . zeros_like ( self . concentration ) \n        for k in range ( 1 , self . dimension ) : \n            ans = ans + ( logpi * ( k / 2. ) ) \n            ans = ans + ( tf . math . lgamma ( self . concentration + ( self . dimension - 1 - k ) / 2. ) ) \n            ans = ans - ( tf . math . lgamma ( self . concentration + ( self . dimension - 1 ) / 2. ) ) \n        return ans "}
{"779": "\ndef _broadcast_to ( tensor_to_broadcast , target_tensors ) : \n    output = tensor_to_broadcast \n    for tensor in target_tensors : \n        output = output + ( tf . zeros_like ( tensor ) ) \n    return output "}
{"782": "\ndef _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : \n    with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : \n        states = tf . convert_to_tensor ( value = states , name = 'states' ) \n        dt = states . dtype \n        auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) \n        if filter_threshold is not None : \n            filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) \n            mask = auto_corr < filter_threshold \n            mask = tf . cast ( mask , dtype = dt ) \n            mask = tf . cumsum ( mask , axis = 0 ) \n            mask = tf . maximum ( 1. - mask , 0. ) \n            auto_corr = auto_corr * ( mask ) \n        n = _axis_size ( states , axis = 0 ) \n        k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) \n        nk_factor = ( n - k ) / n \n        if auto_corr . shape . ndims is not None : \n            new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) \n        else : \n            new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) \n        nk_factor = tf . reshape ( nk_factor , new_shape ) \n        return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) ) "}
{"786": "\ndef quadrature_scheme_lognormal_gauss_hermite ( loc , scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"vector_diffeomixture_quadrature_gauss_hermite\" ) : \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        npdt = dtype_util . as_numpy_dtype ( loc . dtype ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs = probs / ( np . linalg . norm ( probs , ord = 1 , keepdims = True ) ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = loc . dtype ) \n        grid = ( loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * scale [ ... , tf . newaxis ] * grid ) \n        return grid , probs "}
{"794": "\ndef slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        batch_shape = tf . shape ( input = x_initial ) \n        dtype = step_size . dtype . base_dtype \n        left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) \n        left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) \n        left_endpoints = left_endpoints - ( left_increments ) \n        right_endpoints = left_endpoints + widths \n        left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) \n        right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) \n        left_ok = left_ep_values < log_slice_heights \n        right_ok = right_ep_values < log_slice_heights \n        both_ok = left_ok & right_ok \n        both_ok_f = tf . reshape ( both_ok , [ max_doublings + 1 , - 1 ] ) \n        best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) \n        point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( input = best_interval_idx ) ) ] , axis = 1 , name = 'point_index_gather' ) \n        left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + 1 , - 1 ] ) \n        right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + 1 , - 1 ] ) \n        lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) \n        upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) \n        both_ok = tf . reduce_any ( input_tensor = both_ok , axis = 0 ) \n        return upper_bounds , lower_bounds , both_ok "}
{"801": "\ndef _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : \n    if depth == 0 : \n        [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) \n        next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) \n        num_states = tf . cast ( next_log_joint > log_slice_sample , dtype = tf . int32 ) \n        continue_trajectory = ( next_log_joint > log_slice_sample - max_simulation_error ) \n        return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] \n    seed_stream = tfd . SeedStream ( seed , \"build_tree\" ) \n    [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n    if continue_trajectory : \n        if direction < 0 : \n            [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        else : \n            [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        num_states = num_states + ( far_num_states ) \n        accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) \n        if accept_far_state : \n            next_state = far_state \n            next_target_log_prob = far_target_log_prob \n            next_grads_target_log_prob = far_grads_target_log_prob \n        has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) \n        continue_trajectory = far_continue_trajectory and has_no_u_turn \n    return [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] "}
{"829": "\ndef _compute_log_acceptance_correction ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims ] ) : \n        proposed_log_density_parts = [ ] \n        dual_log_density_parts = [ ] \n        for [ current_state , proposed_state , current_volatility , proposed_volatility , current_drift , proposed_drift , step_size , ] in zip ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , ) : \n            axis = tf . range ( independent_chain_ndims , tf . rank ( current_state ) ) \n            state_diff = proposed_state - current_state \n            current_volatility = current_volatility * ( tf . sqrt ( step_size ) ) \n            proposed_energy = ( state_diff - current_drift ) / current_volatility \n            proposed_volatility = proposed_volatility * ( tf . sqrt ( step_size ) ) \n            proposed_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( current_volatility ) , 0.5 * ( proposed_energy ** 2 ) ] ) , axis = axis ) ) \n            proposed_log_density_parts . append ( - proposed_energy ) \n            dual_energy = ( state_diff + proposed_drift ) / proposed_volatility \n            dual_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( proposed_volatility ) , 0.5 * ( dual_energy ** 2 ) ] ) , axis = axis ) ) \n            dual_log_density_parts . append ( - dual_energy ) \n        proposed_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( proposed_log_density_parts , axis = - 1 ) , axis = - 1 ) \n        dual_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( dual_log_density_parts , axis = - 1 ) , axis = - 1 ) \n        return mcmc_util . safe_sum ( [ dual_log_density_reduce , - proposed_log_density_reduce ] ) "}
{"830": "\ndef _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : \n    state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] \n    needs_volatility_fn_gradients = grads_volatility_fn is None \n    if volatility_fn_results is None : \n        volatility_fn_results = volatility_fn ( * state_parts ) \n    volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) \n    if len ( volatility_fn_results ) == 1 : \n        volatility_fn_results = volatility_fn_results * ( len ( state_parts ) ) \n    if len ( state_parts ) != len ( volatility_fn_results ) : \n        raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) \n    volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) \n    if grads_volatility_fn is None : \n        [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) \n    if needs_volatility_fn_gradients : \n        grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] \n    return volatility_fn_results , grads_volatility_fn "}
{"837": "\ndef _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : \n    ds = [ ] \n    values_out = [ ] \n    seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) \n    gen = self . _model ( ) \n    index = 0 \n    d = next ( gen ) \n    try : \n        while True : \n            actual_distribution = d . distribution if isinstance ( d , self . Root ) else d \n            ds . append ( actual_distribution ) \n            if ( value is not None and len ( value ) > index and value [ index ] is not None ) : \n                seed ( ) \n                next_value = value [ index ] \n            else : \n                next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) \n            values_out . append ( next_value ) \n            index = index + ( 1 ) \n            d = gen . send ( next_value ) \n    except StopIteration : \n        pass \n    return ds , values_out "}
{"840": "\ndef get_topics_strings ( topics_words , alpha , vocabulary , topics_to_print = 10 , words_per_topic = 10 ) : \n    alpha = np . squeeze ( alpha , axis = 0 ) \n    highest_weight_topics = np . argsort ( - alpha , kind = \"mergesort\" ) \n    top_words = np . argsort ( - topics_words , axis = 1 ) \n    res = [ ] \n    for topic_idx in highest_weight_topics [ : topics_to_print ] : \n        l = [ \"index={} alpha={:.2f}\" . format ( topic_idx , alpha [ topic_idx ] ) ] \n        l = l + ( [ vocabulary [ word ] for word in top_words [ topic_idx , : words_per_topic ] ] ) \n        res . append ( \" \" . join ( l ) ) \n    return np . array ( res ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) \n        ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex \n        face_centroid = face_centroid / ( tf . cast ( dim , domain_dtype ) ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = 1 \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return ( True , current_simplex , current_objective_values , 0 ) \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = objective_at_reflected < best_objective_value \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) "}
{"867": "\ndef _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : \n    initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) \n    num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] \n    dim = num_vertices - 1 \n    num_evaluations = 0 \n    if objective_at_initial_simplex is None : \n        objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) \n        num_evaluations = num_evaluations + ( n_evals ) \n    objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) \n    return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations ) "}
{"868": "\ndef _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : \n    dim = tf . size ( input = initial_vertex ) \n    num_vertices = dim + 1 \n    unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial_vertex ) ] , axis = 0 ) ) \n    simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes \n    simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = 0 ) , simplex_face ] , axis = 0 ) \n    num_evaluations = 0 \n    if objective_at_initial_vertex is None : \n        objective_at_initial_vertex = objective_function ( initial_vertex ) \n        num_evaluations = num_evaluations + ( 1 ) \n    objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) \n    num_evaluations = num_evaluations + ( num_evals ) \n    objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = 0 ) , objective_at_simplex_face ] , axis = 0 ) \n    return ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations ) "}
{"882": "\ndef _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : \n    x = self . _maybe_rotate_dims ( x , rotate_right = True ) \n    log_prob = self . distribution . log_prob ( x , ** distribution_kwargs ) \n    if self . _is_maybe_event_override : \n        log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) \n    log_prob = log_prob + ( tf . cast ( ildj , log_prob . dtype ) ) \n    if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : \n        tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) \n    return log_prob "}
{"883": "\ndef _finish_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : \n    x = self . _maybe_rotate_dims ( x , rotate_right = True ) \n    prob = self . distribution . prob ( x , ** distribution_kwargs ) \n    if self . _is_maybe_event_override : \n        prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) \n    prob = prob * ( tf . exp ( tf . cast ( ildj , prob . dtype ) ) ) \n    if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : \n        tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) \n    return prob "}
{"885": "\ndef _undo_batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , name = None ) : \n    with tf . compat . v2 . name_scope ( name or \"undo_batchnorm\" ) : \n        rescale = tf . sqrt ( variance + variance_epsilon ) \n        if scale is not None : \n            rescale = rescale / ( scale ) \n        batch_unnormalized = x * rescale + ( mean - offset * rescale if offset is not None else mean ) \n        return batch_unnormalized "}
{"887": "\ndef _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : \n    param_shape = tf . shape ( input = param ) \n    insert_ones = tf . ones ( [ tf . size ( input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) \n    new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = 0 ) \n    full_batch_param = tf . reshape ( param , new_param_shape ) \n    param_slices = [ ] \n    param_dim_idx = 0 \n    batch_dim_idx = 0 \n    for slc in slices : \n        if slc is tf . newaxis : \n            param_slices . append ( slc ) \n            continue \n        if slc is Ellipsis : \n            if batch_dim_idx < 0 : \n                raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) \n            param_slices . append ( slc ) \n            num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + 1 : ] ] ) \n            batch_dim_idx = - num_remaining_non_newaxis_slices \n            param_dim_idx = batch_dim_idx - param_event_ndims \n            continue \n        param_dim_size = new_param_shape [ param_dim_idx ] \n        batch_dim_size = dist_batch_shape [ batch_dim_idx ] \n        is_broadcast = batch_dim_size > param_dim_size \n        if isinstance ( slc , slice ) : \n            start , stop , step = slc . start , slc . stop , slc . step \n            if start is not None : \n                start = tf . where ( is_broadcast , 0 , start ) \n            if stop is not None : \n                stop = tf . where ( is_broadcast , 1 , stop ) \n            if step is not None : \n                step = tf . where ( is_broadcast , 1 , step ) \n            param_slices . append ( slice ( start , stop , step ) ) \n        else : \n            param_slices . append ( tf . where ( is_broadcast , 0 , slc ) ) \n        param_dim_idx = param_dim_idx + ( 1 ) \n        batch_dim_idx = batch_dim_idx + ( 1 ) \n    param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) \n    return full_batch_param . __getitem__ ( param_slices ) "}
{"891": "\ndef batch_slice ( dist , params_event_ndims , params_overrides , slices ) : \n    if not isinstance ( slices , collections . Sequence ) : \n        slices = ( slices , ) \n    orig_dist , slice_overrides_seq = getattr ( dist , PROVENANCE_ATTR , ( dist , [ ] ) ) \n    slice_overrides_seq = slice_overrides_seq + ( [ ( slices , params_overrides ) ] ) \n    dist = _apply_slice_sequence ( orig_dist , params_event_ndims , slice_overrides_seq ) \n    setattr ( dist , PROVENANCE_ATTR , ( orig_dist , slice_overrides_seq ) ) \n    return dist "}
{"903": "\ndef make_tril_scale ( loc = None , scale_tril = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( tf . linalg . diag_part ( x ) , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( tf . linalg . diag_part ( x ) , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) , ] , x ) \n    with tf . name_scope ( name or \"make_tril_scale\" ) : \n        dtype = dtype_util . common_dtype ( [ loc , scale_tril , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_tril = _convert_to_tensor ( scale_tril , name = \"scale_tril\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n    if scale_tril is not None : \n        scale_tril = tf . linalg . band_part ( scale_tril , - 1 , 0 ) \n        tril_diag = tf . linalg . diag_part ( scale_tril ) \n        if scale_diag is not None : \n            tril_diag = tril_diag + ( scale_diag ) \n        if scale_identity_multiplier is not None : \n            tril_diag = tril_diag + ( scale_identity_multiplier [ ... , tf . newaxis ] ) \n        scale_tril = tf . linalg . set_diag ( scale_tril , tril_diag ) \n        return tf . linalg . LinearOperatorLowerTriangular ( tril = _maybe_attach_assertion ( scale_tril ) , is_non_singular = True , is_self_adjoint = False , is_positive_definite = assert_positive ) \n    return make_diag_scale ( loc = loc , scale_diag = scale_diag , scale_identity_multiplier = scale_identity_multiplier , shape_hint = shape_hint , validate_args = validate_args , assert_positive = assert_positive , name = name ) "}
{"904": "\ndef make_diag_scale ( loc = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None , dtype = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( x , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( x , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) ] , x ) \n    with tf . name_scope ( name or \"make_diag_scale\" ) : \n        if dtype is None : \n            dtype = dtype_util . common_dtype ( [ loc , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n        if scale_diag is not None : \n            if scale_identity_multiplier is not None : \n                scale_diag = scale_diag + ( scale_identity_multiplier [ ... , tf . newaxis ] ) \n            return tf . linalg . LinearOperatorDiag ( diag = _maybe_attach_assertion ( scale_diag ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive ) \n        if loc is None and shape_hint is None : \n            raise ValueError ( \"Cannot infer `event_shape` unless `loc` or \" \"`shape_hint` is specified.\" ) \n        num_rows = shape_hint \n        del shape_hint \n        if num_rows is None : \n            num_rows = tf . compat . dimension_value ( loc . shape [ - 1 ] ) \n            if num_rows is None : \n                num_rows = tf . shape ( input = loc ) [ - 1 ] \n        if scale_identity_multiplier is None : \n            return tf . linalg . LinearOperatorIdentity ( num_rows = num_rows , dtype = dtype , is_self_adjoint = True , is_positive_definite = True , assert_proper_shapes = validate_args ) \n        return tf . linalg . LinearOperatorScaledIdentity ( num_rows = num_rows , multiplier = _maybe_attach_assertion ( scale_identity_multiplier ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive , assert_proper_shapes = validate_args ) "}
{"911": "\ndef embed_check_nonnegative_integer_form ( x , name = \"embed_check_nonnegative_integer_form\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        assertions = [ assert_util . assert_non_negative ( x , message = \"'{}' must be non-negative.\" . format ( x ) ) , ] \n        if not dtype_util . is_integer ( x . dtype ) : \n            assertions = assertions + ( [ assert_integer_form ( x , message = \"'{}' cannot contain fractional components.\" . format ( x ) ) , ] ) \n        return with_dependencies ( assertions , x ) "}
{"925": "\ndef tridiag ( below = None , diag = None , above = None , name = None ) : \n    def _pad ( x ) : \n        shape = tf . concat ( [ tf . shape ( input = x ) [ : - 1 ] , [ 1 ] ] , axis = 0 ) \n        z = tf . zeros ( shape , dtype = x . dtype ) \n        return tf . concat ( [ z , x , z ] , axis = - 1 ) \n    def _add ( * x ) : \n        s = None \n        for y in x : \n            if y is None : \n                continue \n            elif s is None : \n                s = y \n            else : \n                s = s + ( y ) \n        if s is None : \n            raise ValueError ( \"Must specify at least one of `below`, `diag`, `above`.\" ) \n        return s \n    with tf . name_scope ( name or \"tridiag\" ) : \n        if below is not None : \n            below = tf . convert_to_tensor ( value = below , name = \"below\" ) \n            below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - 1 , 1 : ] \n        if diag is not None : \n            diag = tf . convert_to_tensor ( value = diag , name = \"diag\" ) \n            diag = tf . linalg . diag ( diag ) \n        if above is not None : \n            above = tf . convert_to_tensor ( value = above , name = \"above\" ) \n            above = tf . linalg . diag ( _pad ( above ) ) [ ... , 1 : , : - 1 ] \n        return _add ( below , diag , above ) "}
{"927": "\ndef process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : \n    with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : \n        if quadrature_grid_and_probs is None : \n            grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) \n            grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs / ( np . linalg . norm ( probs , ord = 1 , keepdims = True ) ) \n            grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n            probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) \n            return grid , probs \n        grid , probs = tuple ( quadrature_grid_and_probs ) \n        grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) \n        probs = probs / ( tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = \"probs\" ) ) \n        def _static_event_size ( x ) : \n            return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) \n        m , n = _static_event_size ( probs ) , _static_event_size ( grid ) \n        if m is not None and n is not None : \n            if m != n : \n                raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) \n        elif validate_args : \n            assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] \n            with tf . control_dependencies ( assertions ) : \n                grid = tf . identity ( grid ) \n                probs = tf . identity ( probs ) \n        return grid , probs "}
{"931": "\ndef _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : \n            raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) \n        if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : \n            if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != 0 : \n                raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) \n        elif validate_args : \n            assertions = assertions + ( [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] ) \n        rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) \n        msg = '`rightmost_transposed_ndims` must be non-negative.' \n        if rightmost_transposed_ndims_ is not None : \n            if rightmost_transposed_ndims_ < 0 : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) \n        elif validate_args : \n            assertions = assertions + ( [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] ) \n        return assertions "}
{"932": "\ndef _maybe_validate_perm ( perm , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_perm' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( perm . dtype ) : \n            raise TypeError ( '`perm` must be integer type' ) \n        msg = '`perm` must be a vector.' \n        if tensorshape_util . rank ( perm . shape ) is not None : \n            if tensorshape_util . rank ( perm . shape ) != 1 : \n                raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) \n        elif validate_args : \n            assertions = assertions + ( [ assert_util . assert_rank ( perm , 1 , message = msg ) ] ) \n        perm_ = tf . get_static_value ( perm ) \n        msg = '`perm` must be a valid permutation vector.' \n        if perm_ is not None : \n            if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) \n        elif validate_args : \n            assertions = assertions + ( [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] ) \n        return assertions "}
{"1029": "\ndef _flatten_summand_list ( kernels ) : \n    flattened = [ ] \n    for k in kernels : \n        if isinstance ( k , _SumKernel ) : \n            flattened = flattened + ( k . kernels ) \n        else : \n            flattened . append ( k ) \n    return flattened "}
{"1030": "\ndef _flatten_multiplicand_list ( kernels ) : \n    flattened = [ ] \n    for k in kernels : \n        if isinstance ( k , _ProductKernel ) : \n            flattened = flattened + ( k . kernels ) \n        else : \n            flattened . append ( k ) \n    return flattened "}
{"1044": "\ndef jensen_shannon ( logu , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"jensen_shannon\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        npdt = logu . dtype . as_numpy_dtype \n        y = tf . nn . softplus ( logu ) \n        if self_normalized : \n            y = y - ( np . log ( 2 ) . astype ( npdt ) ) \n        return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y "}
{"1048": "\ndef t_power ( logu , t , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"t_power\" , [ logu , t ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = \"t\" ) \n        fu = tf . math . expm1 ( t * logu ) \n        if self_normalized : \n            fu = fu - ( t * tf . math . expm1 ( logu ) ) \n        fu = fu * ( tf . where ( tf . logical_and ( 0. < t , t < 1. ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) ) \n        return fu "}
{"1051": "\ndef modified_gan ( logu , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"chi_square\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        y = tf . nn . softplus ( logu ) - logu \n        if self_normalized : \n            y = y + ( 0.5 * tf . math . expm1 ( logu ) ) \n        return y "}
{"1057": "\ndef _batch_gather_with_broadcast ( params , indices , axis ) : \n    leading_bcast_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) \n    params = params + ( tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) ) \n    indices = indices + ( tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) ) \n    return tf . compat . v1 . batch_gather ( params , indices ) "}
{"1058": "\ndef _broadcast_cat_event_and_params ( event , params , base_dtype ) : \n    if dtype_util . is_integer ( event . dtype ) : \n        pass \n    elif dtype_util . is_floating ( event . dtype ) : \n        event = tf . cast ( event , dtype = tf . int32 ) \n    else : \n        raise TypeError ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base_dtype ) ) \n    shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) \n    if not shape_known_statically or params . shape [ : - 1 ] != event . shape : \n        params = params * ( tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) ) \n        params_shape = tf . shape ( input = params ) [ : - 1 ] \n        event = event * ( tf . ones ( params_shape , dtype = event . dtype ) ) \n        if tensorshape_util . rank ( params . shape ) is not None : \n            tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) \n    return event , params "}
{"1060": "\ndef _broadcast_event_and_samples ( event , samples , event_ndims ) : \n    samples_shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event_ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event_ndims : ] ] , axis = 0 ) \n    event = event * ( tf . ones ( samples_shape , dtype = event . dtype ) ) \n    event = tf . expand_dims ( event , axis = - event_ndims - 1 ) \n    samples = samples * ( tf . ones_like ( event , dtype = samples . dtype ) ) \n    return event , samples "}
{"1064": "\ndef _bfgs_inv_hessian_update ( grad_delta , position_delta , normalization_factor , inv_hessian_estimate ) : \n    conditioned_grad_delta = _mul_right ( inv_hessian_estimate , grad_delta ) \n    conditioned_grad_delta_norm = tf . reduce_sum ( input_tensor = conditioned_grad_delta * grad_delta , axis = - 1 ) \n    cross_term = _tensor_product ( position_delta , conditioned_grad_delta ) \n    def _expand_scalar ( s ) : \n        return s [ ... , tf . newaxis , tf . newaxis ] \n    cross_term = cross_term + ( _tensor_product ( conditioned_grad_delta , position_delta ) ) \n    position_term = _tensor_product ( position_delta , position_delta ) \n    with tf . control_dependencies ( [ position_term ] ) : \n        position_term = position_term * ( _expand_scalar ( 1 + conditioned_grad_delta_norm / normalization_factor ) ) \n    return ( inv_hessian_estimate + ( position_term - cross_term ) / _expand_scalar ( normalization_factor ) ) "}
{"1083": "\ndef _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : \n    num_terms = tf . cast ( num_terms , dtype = dtype ) \n    def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : \n        denominator = 2. * n / concentration + rn \n        ddenominator_dk = - 2. * n / concentration ** 2 + drn_dconcentration \n        rn = 1. / denominator \n        drn_dconcentration = - ddenominator_dk / denominator ** 2 \n        multiplier = tf . sin ( n * x ) / n + vn \n        vn = rn * multiplier \n        dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) \n        n = n - ( 1. ) \n        return n , rn , drn_dconcentration , vn , dvn_dconcentration \n    ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : n > 0. , body = loop_body , loop_vars = ( num_terms , tf . zeros_like ( x , name = \"rn\" ) , tf . zeros_like ( x , name = \"drn_dconcentration\" ) , tf . zeros_like ( x , name = \"vn\" ) , tf . zeros_like ( x , name = \"dvn_dconcentration\" ) , ) , ) \n    cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi \n    dcdf_dconcentration = dvn_dconcentration / np . pi \n    cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) \n    dcdf_dconcentration = dcdf_dconcentration * ( tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) ) \n    return cdf_clipped , dcdf_dconcentration "}
{"1090": "\ndef _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed ) : \n    if initial_population is not None : \n        return [ tf . convert_to_tensor ( value = part ) for part in initial_population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'get_starting_population' ) \n    population = [ ] \n    for part in initial_position : \n        part = tf . convert_to_tensor ( value = part ) \n        part_event_shape = tf . shape ( input = part ) \n        population_part_shape = tf . concat ( [ [ population_size - 1 ] , part_event_shape ] , axis = 0 ) \n        population_part = tf . random . normal ( population_part_shape , stddev = population_stddev , dtype = part . dtype . base_dtype , seed = seed_stream ( ) ) \n        population_part = population_part + ( part ) \n        population_part = tf . concat ( [ [ part ] , population_part ] , axis = 0 ) \n        population . append ( population_part ) \n    return population "}
{"1101": "\ndef call ( self , inputs ) : \n    del inputs \n    latent_code = ed . MultivariateNormalDiag ( loc = tf . zeros ( self . latent_size ) , sample_shape = 1 , name = \"latent_code\" ) \n    state = self . lstm . zero_state ( 1 , dtype = tf . float32 ) \n    t = 0 \n    productions = [ ] \n    stack = [ self . grammar . start_symbol ] \n    while stack : \n        symbol = stack . pop ( ) \n        net , state = self . lstm ( latent_code , state ) \n        logits = ( self . output_layer ( net ) + self . grammar . mask ( symbol , on_value = 0. , off_value = - 1e9 ) ) \n        production = ed . OneHotCategorical ( logits = logits , name = \"production_\" + str ( t ) ) \n        _ , rhs = self . grammar . production_rules [ tf . argmax ( input = production , axis = - 1 ) ] \n        for symbol in rhs : \n            if symbol in self . grammar . nonterminal_symbols : \n                stack . append ( symbol ) \n        productions . append ( production ) \n        t = t + ( 1 ) \n    return tf . stack ( productions , axis = 1 ) "}
{"1115": "\ndef _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : \n    slices = [ ] \n    col = 0 \n    d_in = n_in // num_blocks \n    d_out = n_out // num_blocks \n    row = d_out if mask_type == MASK_EXCLUSIVE else 0 \n    for _ in range ( num_blocks ) : \n        row_slice = slice ( row , None ) \n        col_slice = slice ( col , col + d_in ) \n        slices . append ( [ row_slice , col_slice ] ) \n        col = col + ( d_in ) \n        row = row + ( d_out ) \n    return slices "}
{"1151": "\ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = 0 \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id = label_id + ( 1 ) \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if sys . version_info < ( 3 , ) : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    print ( 'Found %s texts.' % len ( texts ) ) \n    return texts "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length = reference_length + ( min ( len ( r ) for r in references ) ) \n        translation_length = translation_length + ( len ( translation ) ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] = matches_by_order [ len ( ngram ) - 1 ] + ( overlap [ ngram ] ) \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if possible_matches > 0 : \n                possible_matches_by_order [ order - 1 ] = possible_matches_by_order [ order - 1 ] + ( possible_matches ) \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if possible_matches_by_order [ i ] > 0 : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if min ( precisions ) > 0 : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1206": "\ndef summary_gradient_updates ( grads , opt , lr ) : \n    vars_grads = { } \n    for v in tf . trainable_variables ( ) : \n        vars_grads [ v . name ] = [ v , None , None ] \n    for g , v in grads : \n        vars_grads [ v . name ] [ 1 ] = g \n        vars_grads [ v . name ] [ 2 ] = opt . get_slot ( v , 'accumulator' ) \n    ret = [ ] \n    for vname , ( v , g , a ) in vars_grads . items ( ) : \n        if g is None : \n            continue \n        if isinstance ( g , tf . IndexedSlices ) : \n            updates = lr * g . values \n            if a is not None : \n                updates = updates / ( tf . sqrt ( tf . gather ( a , g . indices ) ) ) \n        else : \n            updates = lr * g \n            if a is not None : \n                updates = updates / ( tf . sqrt ( a ) ) \n        values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 \n        updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) \n        ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( \":\" , \"_\" ) , updates_norm / values_norm ) ) \n    return ret "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = 0 \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) \n        for _ in range ( self . char_conv_layers - 1 ) : \n            curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if self . conv_dropout > 0.0 : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n        curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ = self . char_output_dim_ + ( curr_filters_number ) \n    if len ( conv_outputs ) > 1 : \n        conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ 0 ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) \n    if self . intermediate_dropout > 0.0 : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    for i in range ( self . char_highway_layers - 1 ) : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if self . highway_dropout > 0.0 : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1287": "\ndef show_details ( item_data : Dict [ Any , Any ] ) -> str : \n    txt = \"\" \n    for key , value in item_data . items ( ) : \n        txt = txt + ( \"**\" + str ( key ) + \"**\" + ': ' + str ( value ) + \"  \\n\" ) \n    return txt "}
{"1294": "\ndef path_set_md5 ( url ) : \n    scheme , netloc , path , query_string , fragment = urlsplit ( url ) \n    path = path + ( '.md5' ) \n    return urlunsplit ( ( scheme , netloc , path , query_string , fragment ) ) "}
{"1301": "\ndef squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : \n    f1_total = 0.0 \n    for ground_truth , prediction in zip ( y_true , y_predicted ) : \n        prediction_tokens = normalize_answer ( prediction ) . split ( ) \n        f1s = [ ] \n        for gt in ground_truth : \n            gt_tokens = normalize_answer ( gt ) . split ( ) \n            if len ( gt_tokens ) == 0 or len ( prediction_tokens ) == 0 : \n                f1s . append ( float ( gt_tokens == prediction_tokens ) ) \n                continue \n            common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) \n            num_same = sum ( common . values ( ) ) \n            if num_same == 0 : \n                f1s . append ( 0.0 ) \n                continue \n            precision = 1.0 * num_same / len ( prediction_tokens ) \n            recall = 1.0 * num_same / len ( gt_tokens ) \n            f1 = ( 2 * precision * recall ) / ( precision + recall ) \n            f1s . append ( f1 ) \n        f1_total = f1_total + ( max ( f1s ) ) \n    return 100 * f1_total / len ( y_true ) if len ( y_true ) > 0 else 0 "}
{"1302": "\ndef recall_at_k ( y_true : List [ int ] , y_pred : List [ List [ np . ndarray ] ] , k : int ) : \n    num_examples = float ( len ( y_pred ) ) \n    predictions = np . array ( y_pred ) \n    predictions = np . flip ( np . argsort ( predictions , - 1 ) , - 1 ) [ : , : k ] \n    num_correct = 0 \n    for el in predictions : \n        if 0 in el : \n            num_correct = num_correct + ( 1 ) \n    return float ( num_correct ) / num_examples "}
{"1321": "\ndef show_status ( self , detailed = False ) : \n    if self . _retrieved_at + self . REFRESH_INTERVAL < time . time ( ) : \n        new_info = h2o . api ( \"GET /3/Cloud\" ) \n        self . _fill_from_h2ocluster ( new_info ) \n    ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) \n    allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) \n    free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) \n    unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) \n    status = \"locked\" if self . locked else \"accepting new members\" \n    if unhealthy_nodes == 0 : \n        status = status + ( \", healthy\" ) \n    else : \n        status = status + ( \", %d nodes are not healthy\" % unhealthy_nodes ) \n    api_extensions = self . list_api_extensions ( ) \n    H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) \n    if detailed : \n        keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] \n        header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] \n        table = [ [ k ] for k in keys ] \n        for node in self . nodes : \n            for i , k in enumerate ( keys ) : \n                table [ i ] . append ( node [ k ] ) \n        H2ODisplay ( table = table , header = header ) "}
{"1326": "\ndef stabilize ( self , test_func , error , timeoutSecs = 10 , retryDelaySecs = 0.5 ) : \n    start = time . time ( ) \n    numberOfRetries = 0 \n    while h2o_args . no_timeout or ( time . time ( ) - start < timeoutSecs ) : \n        if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : \n            break \n        time . sleep ( retryDelaySecs ) \n        numberOfRetries = numberOfRetries + ( 1 ) \n        if ( ( numberOfRetries % 50 ) == 0 ) : \n            check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) \n    else : \n        timeTakenSecs = time . time ( ) - start \n        if isinstance ( error , type ( '' ) ) : \n            raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) \n        else : \n            msg = error ( self , timeTakenSecs , numberOfRetries ) \n            raise Exception ( msg ) "}
{"1329": "\ndef model_builders ( self , algo = None , timeoutSecs = 10 , ** kwargs ) : \n    params_dict = { } \n    h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'model_builders' , False ) \n    request = '3/ModelBuilders.json' \n    if algo : \n        request = request + ( \"/\" + algo ) \n    result = self . do_json_request ( request , timeout = timeoutSecs , params = params_dict ) \n    h2o_sandbox . check_sandbox_for_errors ( ) \n    return result "}
{"1340": "\ndef wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : \n    log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) \n    for ip in ips : \n        if not skipAlive or not ssh_live ( ip , port ) : \n            log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) \n            count = 0 \n            while count < requiredsuccess : \n                if ssh_live ( ip , port ) : \n                    count = count + ( 1 ) \n                else : \n                    count = 0 \n                time . sleep ( 1 ) \n                h2o_cmd . dot ( ) "}
{"1344": "\ndef _wrap ( text , wrap_at = 120 , indent = 4 ) : \n    out = \"\" \n    curr_line_length = indent \n    space_needed = False \n    for word in text . split ( ) : \n        if curr_line_length + len ( word ) > wrap_at : \n            out = out + ( \"\\n\" + \" \" * indent ) \n            curr_line_length = indent \n            space_needed = False \n        if space_needed : \n            out = out + ( \" \" ) \n            curr_line_length = curr_line_length + ( 1 ) \n        out = out + ( word ) \n        curr_line_length = curr_line_length + ( len ( word ) ) \n        space_needed = True \n    return out "}
{"1352": "\ndef scrape_port_from_stdout ( self ) : \n    regex = re . compile ( r\"Open H2O Flow in your web browser: https?://([^:]+):(\\d+)\" ) \n    retries_left = 30 \n    while retries_left and not self . terminated : \n        with open ( self . output_file_name , \"r\" ) as f : \n            for line in f : \n                mm = re . search ( regex , line ) \n                if mm is not None : \n                    self . port = mm . group ( 2 ) \n                    print ( \"H2O cloud %d node %d listening on port %s\\n    with output file %s\" % ( self . cloud_num , self . node_num , self . port , self . output_file_name ) ) \n                    return \n        if self . terminated : \n            break \n        retries_left = retries_left - ( 1 ) \n        time . sleep ( 1 ) \n    if self . terminated : \n        return \n    print ( \"\\nERROR: Too many retries starting cloud %d.\\nCheck the output log %s.\\n\" % ( self . cloud_num , self . output_file_name ) ) \n    sys . exit ( 1 ) "}
{"1353": "\ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60 \n    while retries > 0 : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while len ( s ) > 0 : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( 1 ) \n                if size is not None : \n                    size = int ( size ) \n                    if size == nodes_per_cloud : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries = retries - ( 1 ) \n        if self . terminated : \n            return \n        time . sleep ( 1 ) \n    print ( \"\" ) \n    print ( \"ERROR: Too many retries starting cloud.\" ) \n    print ( \"\" ) \n    sys . exit ( 1 ) "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2 ] \n        assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and fr . f_code . co_filename == this_filename : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                for i in range ( fr . f_lineno - 1 ) : \n                    next ( f ) \n                g = tokenize . generate_tokens ( f . readline ) \n                step = 0 \n                args_tokens = [ ] \n                level = 0 \n                for ttt in g : \n                    if step == 0 : \n                        if ttt [ 0 ] != tokenize . NAME : \n                            continue \n                        if not ttt [ 1 ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = 1 \n                    elif step == 1 : \n                        assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \"(\" \n                        args_tokens . append ( [ ] ) \n                        step = 2 \n                    elif step == 2 : \n                        if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \",\" : \n                            args_tokens . append ( [ ] ) \n                        elif level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \")\" : \n                            break \n                        else : \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \"([{\" : \n                                level = level + ( 1 ) \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \")]}\" : \n                                level = level - ( 1 ) \n                            assert level >= 0 , \"Parse error: parentheses level became negative\" \n                            args_tokens [ - 1 ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1370": "\ndef _get_lambda_source_code ( lambda_fn , src ) : \n    def gen_lambdas ( ) : \n        def gen ( ) : \n            yield src + \"\\n\" \n        g = gen ( ) \n        step = 0 \n        tokens = [ ] \n        for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : \n            if step == 0 : \n                if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : \n                    step = 1 \n                    tokens = [ tok ] \n                    level = 0 \n            elif step == 1 : \n                if tok [ 0 ] == tokenize . NAME : \n                    tokens . append ( tok ) \n                    step = 2 \n                else : \n                    step = 0 \n            elif step == 2 : \n                if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : \n                    tokens . append ( tok ) \n                    step = 3 \n                else : \n                    step = 0 \n            elif step == 3 : \n                if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : \n                    yield tokenize . untokenize ( tokens ) . strip ( ) \n                    step = 0 \n                else : \n                    tokens . append ( tok ) \n                    if tok [ 0 ] == tokenize . OP : \n                        if tok [ 1 ] in \"[({\" : \n                            level = level + ( 1 ) \n                        if tok [ 1 ] in \"])}\" : \n                            level = level - ( 1 ) \n        assert not tokens \n    actual_code = lambda_fn . __code__ . co_code \n    for lambda_src in gen_lambdas ( ) : \n        try : \n            fn = eval ( lambda_src , globals ( ) , locals ( ) ) \n            if fn . __code__ . co_code == actual_code : \n                return lambda_src . split ( \":\" , 1 ) [ 1 ] . strip ( ) \n        except Exception : \n            pass \n    return \"<lambda>\" "}
{"1378": "\ndef _recalculate_model_parameters ( self , now ) : \n    time_until_end = self . _estimate_progress_completion_time ( now ) - now \n    assert time_until_end >= 0 , \"Estimated progress completion cannot be in the past.\" \n    x_real = self . _get_real_progress ( ) \n    if x_real == 1 : \n        t0 , x0 , v0 , ve = now , 1 , 0 , 0 \n    else : \n        x0 , v0 = self . _compute_progress_at_time ( now ) \n        t0 = now \n        if x0 >= 1 : \n            t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 \n            time_until_end = time_until_end + ( now - t0 ) \n        z = self . BETA * time_until_end \n        max_speed = ( 1 - x_real ** 2 ) / self . FINISH_DELAY \n        ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) \n        if ve < 0 : \n            v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) \n            ve = 0 \n        if ve > max_speed : \n            ve = max_speed \n    self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve "}
{"1379": "\ndef _estimate_progress_completion_time ( self , now ) : \n    assert self . _next_poll_time >= now \n    tlast , wlast = self . _progress_data [ - 1 ] \n    if wlast == self . _maxval : \n        current_completion_time = ( 1 - self . _x0 ) / self . _v0 + self . _t0 \n        return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) \n    tacc , wacc = 0 , 0 \n    factor = self . GAMMA \n    for t , x in self . _progress_data [ - 2 : : - 1 ] : \n        tacc = tacc + ( factor * ( tlast - t ) ) \n        wacc = wacc + ( factor * ( wlast - x ) ) \n        factor = factor * ( self . GAMMA ) \n        if factor < 1e-2 : \n            break \n    if wacc == 0 : \n        return now + 300 \n    t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc \n    if t_estimate <= self . _next_poll_time : \n        t_estimate = self . _next_poll_time + self . FINISH_DELAY \n    return t_estimate "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    for _ in range ( 20 ) : \n        if v == 0 : \n            return 1e20 \n        t = t + ( ( x_target - x ) / v ) \n        x , v = self . _compute_progress_at_time ( t ) \n        if abs ( x - x_target ) < 1e-3 : \n            return t \n    return time . time ( ) + 100 "}
{"1384": "\ndef _compute_widget_sizes ( self ) : \n    wl = [ 0 ] * len ( self . _widgets ) \n    flex_count = 0 \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            flex_count = flex_count + ( 1 ) \n        else : \n            wl [ i ] = widget . render ( 1 ) . length \n    remaining_width = self . _width - sum ( wl ) \n    remaining_width = remaining_width - ( len ( self . _widgets ) - 1 ) \n    if remaining_width < 10 * flex_count : \n        if self . _file_mode : \n            remaining_width = 10 * flex_count \n        else : \n            widget0 = self . _widgets [ 0 ] \n            if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( 0 ) . length >= 10 * flex_count : \n                remaining_width = remaining_width + ( widget0 . render ( 0 ) . length + 1 ) \n                self . _to_render = widget0 . render ( 0 ) . rendered + \"\\n\" \n                self . _widgets = self . _widgets [ 1 : ] \n            if remaining_width < 10 * flex_count : \n                self . _file_mode = True \n                remaining_width = 10 * flex_count \n    remaining_width = max ( remaining_width , 10 * flex_count ) \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            target_length = int ( remaining_width / flex_count ) \n            result = widget . render ( 1 , target_length ) \n            wl [ i ] = result . length \n            remaining_width = remaining_width - ( result . length ) \n            flex_count = flex_count - ( 1 ) \n    return wl "}
{"1408": "\ndef pop ( self , i ) : \n    if is_type ( i , str ) : \n        i = self . names . index ( i ) \n    col = H2OFrame . _expr ( expr = ExprNode ( \"cols\" , self , i ) ) \n    old_cache = self . _ex . _cache \n    self . _ex = ExprNode ( \"cols\" , self , - ( i + 1 ) ) \n    self . _ex . _cache . ncols = self . _ex . _cache . ncols - ( 1 ) \n    self . _ex . _cache . names = old_cache . names [ : i ] + old_cache . names [ i + 1 : ] \n    self . _ex . _cache . types = { name : old_cache . types [ name ] for name in self . _ex . _cache . names } \n    self . _ex . _cache . _data = None \n    col . _ex . _cache . ncols = 1 \n    col . _ex . _cache . names = [ old_cache . names [ i ] ] \n    return col "}
{"1411": "\ndef cbind ( self , data ) : \n    assert_is_type ( data , H2OFrame , numeric , [ H2OFrame , numeric ] ) \n    frames = [ data ] if not isinstance ( data , list ) else data \n    new_cols = list ( self . columns ) \n    new_types = dict ( self . types ) \n    for frame in frames : \n        if isinstance ( frame , H2OFrame ) : \n            if frame . nrow != self . nrow : \n                raise H2OValueError ( \"Cannot bind a dataframe with %d rows to a data frame with %d rows: \" \"the number of rows should match\" % ( frame . nrow , self . nrow ) ) \n            new_cols = new_cols + ( frame . columns ) \n            new_types . update ( frame . types ) \n        else : \n            new_cols = new_cols + ( [ None ] ) \n    unique_cols = set ( new_cols ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"cbind\" , self , * frames ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . ncols = len ( new_cols ) \n    if len ( new_cols ) == len ( unique_cols ) and None not in unique_cols : \n        fr . _ex . _cache . names = new_cols \n        fr . _ex . _cache . types = new_types \n    else : \n        fr . _ex . _cache . names = None \n        fr . _ex . _cache . types = None \n    return fr "}
{"1413": "\ndef split_frame ( self , ratios = None , destination_frames = None , seed = None ) : \n    assert_is_type ( ratios , [ numeric ] , None ) \n    assert_is_type ( destination_frames , [ str ] , None ) \n    assert_is_type ( seed , int , None ) \n    if ratios is None : \n        ratios = [ 0.75 ] \n    if not ratios : \n        raise ValueError ( \"Ratios array may not be empty\" ) \n    if destination_frames is not None : \n        if len ( ratios ) + 1 != len ( destination_frames ) : \n            raise ValueError ( \"The number of provided destination_frames must be one more \" \"than the number of provided ratios\" ) \n    num_slices = len ( ratios ) + 1 \n    boundaries = [ ] \n    last_boundary = 0 \n    i = 0 \n    while i < num_slices - 1 : \n        ratio = ratios [ i ] \n        if ratio < 0 : \n            raise ValueError ( \"Ratio must be greater than 0\" ) \n        boundary = last_boundary + ratio \n        if boundary >= 1.0 : \n            raise ValueError ( \"Ratios must add up to less than 1.0\" ) \n        boundaries . append ( boundary ) \n        last_boundary = boundary \n        i = i + ( 1 ) \n    splits = [ ] \n    tmp_runif = self . runif ( seed ) \n    tmp_runif . frame_id = \"%s_splitter\" % _py_tmp_key ( h2o . connection ( ) . session_id ) \n    i = 0 \n    while i < num_slices : \n        if i == 0 : \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( tmp_runif <= upper_boundary ) , : ] \n        elif i == num_slices - 1 : \n            lower_boundary = boundaries [ i - 1 ] \n            tmp_slice = self [ ( tmp_runif > lower_boundary ) , : ] \n        else : \n            lower_boundary = boundaries [ i - 1 ] \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( ( tmp_runif > lower_boundary ) & ( tmp_runif <= upper_boundary ) ) , : ] \n        if destination_frames is None : \n            splits . append ( tmp_slice ) \n        else : \n            destination_frame_id = destination_frames [ i ] \n            tmp_slice . frame_id = destination_frame_id \n            splits . append ( tmp_slice ) \n        i = i + ( 1 ) \n    del tmp_runif \n    return splits "}
{"1447": "\ndef move ( self , drow , dcol = 0 ) : \n    self . _start_row = self . _start_row + ( drow ) \n    self . _start_col = self . _start_col + ( dcol ) \n    self . _end_row = self . _end_row + ( drow ) \n    self . _end_col = self . _end_col + ( dcol ) "}
{"1481": "\ndef mojo_predict_csv ( input_csv_path , mojo_zip_path , output_csv_path = None , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : \n    default_java_options = '-Xmx4g -XX:ReservedCodeCacheSize=256m' \n    prediction_output_file = 'prediction.csv' \n    java = H2OLocalServer . _find_java ( ) \n    H2OLocalServer . _check_java ( java = java , verbose = verbose ) \n    if verbose : \n        print ( \"input_csv:\\t%s\" % input_csv_path ) \n    if not os . path . isfile ( input_csv_path ) : \n        raise RuntimeError ( \"Input csv cannot be found at %s\" % input_csv_path ) \n    mojo_zip_path = os . path . abspath ( mojo_zip_path ) \n    if verbose : \n        print ( \"mojo_zip:\\t%s\" % mojo_zip_path ) \n    if not os . path . isfile ( mojo_zip_path ) : \n        raise RuntimeError ( \"MOJO zip cannot be found at %s\" % mojo_zip_path ) \n    parent_dir = os . path . dirname ( mojo_zip_path ) \n    if output_csv_path is None : \n        output_csv_path = os . path . join ( parent_dir , prediction_output_file ) \n    if genmodel_jar_path is None : \n        genmodel_jar_path = os . path . join ( parent_dir , gen_model_file_name ) \n    if verbose : \n        print ( \"genmodel_jar:\\t%s\" % genmodel_jar_path ) \n    if not os . path . isfile ( genmodel_jar_path ) : \n        raise RuntimeError ( \"Genmodel jar cannot be found at %s\" % genmodel_jar_path ) \n    if verbose and output_csv_path is not None : \n        print ( \"output_csv:\\t%s\" % output_csv_path ) \n    if classpath is None : \n        classpath = genmodel_jar_path \n    if verbose : \n        print ( \"classpath:\\t%s\" % classpath ) \n    if java_options is None : \n        java_options = default_java_options \n    if verbose : \n        print ( \"java_options:\\t%s\" % java_options ) \n    cmd = [ java ] \n    for option in java_options . split ( ' ' ) : \n        cmd = cmd + ( [ option ] ) \n    cmd = cmd + ( [ \"-cp\" , classpath , h2o_predictor_class , \"--mojo\" , mojo_zip_path , \"--input\" , input_csv_path , '--output' , output_csv_path , '--decimal' ] ) \n    if verbose : \n        cmd_str = \" \" . join ( cmd ) \n        print ( \"java cmd:\\t%s\" % cmd_str ) \n    subprocess . check_call ( cmd , shell = False ) \n    with open ( output_csv_path ) as csv_file : \n        result = list ( csv . DictReader ( csv_file ) ) \n    return result "}
{"1494": "\ndef translate_name ( name ) : \n    parts = name . split ( \"_\" ) \n    i = 0 \n    while parts [ i ] == \"\" : \n        parts [ i ] = \"_\" \n        i = i + ( 1 ) \n    parts [ i ] = parts [ i ] . lower ( ) \n    for j in range ( i + 1 , len ( parts ) ) : \n        parts [ j ] = parts [ j ] . capitalize ( ) \n    i = len ( parts ) - 1 \n    while parts [ i ] == \"\" : \n        parts [ i ] = \"_\" \n        i = i - ( 1 ) \n    return \"\" . join ( parts ) "}
{"1503": "\ndef _log_start_transaction ( self , endpoint , data , json , files , params ) : \n    self . _requests_counter = self . _requests_counter + ( 1 ) \n    if not self . _is_logging : \n        return \n    msg = \"\\n---- %d --------------------------------------------------------\\n\" % self . _requests_counter \n    msg = msg + ( \"[%s] %s\\n\" % ( time . strftime ( \"%H:%M:%S\" ) , endpoint ) ) \n    if params is not None : \n        msg = msg + ( \"     params: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( params ) ) ) \n    if data is not None : \n        msg = msg + ( \"     body: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( data ) ) ) \n    if json is not None : \n        import json as j \n        msg = msg + ( \"     json: %s\\n\" % j . dumps ( json ) ) \n    if files is not None : \n        msg = msg + ( \"     file: %s\\n\" % \", \" . join ( f . name for f in viewvalues ( files ) ) ) \n    self . _log_message ( msg + \"\\n\" ) "}
{"1504": "\ndef _log_end_transaction ( self , start_time , response ) : \n    if not self . _is_logging : \n        return \n    elapsed_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n    msg = \"<<< HTTP %d %s   (%d ms)\\n\" % ( response . status_code , response . reason , elapsed_time ) \n    if \"Content-Type\" in response . headers : \n        msg = msg + ( \"    Content-Type: %s\\n\" % response . headers [ \"Content-Type\" ] ) \n    msg = msg + ( response . text ) \n    self . _log_message ( msg + \"\\n\\n\" ) "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = False \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if len ( found ) > 0 : \n                    if len ( g_current_testname ) > 0 : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( len ( temp_strings ) >= 6 ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : \n                if g_toContinue == True : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = False \n            else : \n                if g_toContinue : \n                    tempMessage = tempMessage + ( each_line ) \n            if ( ( len ( temp_strings ) > 5 ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) \n                if found and ( len ( endStr . strip ( ) ) > 0 ) : \n                    tempMessage = tempMessage + ( endStr ) \n                    messageType = temp_strings [ 5 ] \n                    g_toContinue = True \n        java_file . close ( ) "}
{"1551": "\ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        print ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            print ( self . _table_description ) \n    print ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if nr > 20 : \n        trunc_table = [ ] \n        trunc_table = trunc_table + ( [ v for v in table [ : 5 ] ] ) \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table = trunc_table + ( [ v for v in table [ ( nr - 5 ) : ] ] ) \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if nr > 20 and can_use_pandas ( ) : \n        print ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1557": "\ndef _uri2path ( self , uri ) : \n    if uri == self . package_name : \n        return os . path . join ( self . root_path , '__init__.py' ) \n    path = uri . replace ( '.' , os . path . sep ) \n    path = path . replace ( self . package_name + os . path . sep , '' ) \n    path = os . path . join ( self . root_path , path ) \n    if os . path . exists ( path + '.py' ) : \n        path = path + ( '.py' ) \n    elif os . path . exists ( os . path . join ( path , '__init__.py' ) ) : \n        path = os . path . join ( path , '__init__.py' ) \n    else : \n        return None \n    return path "}
{"1568": "\ndef extract_message_to_dict ( filename ) : \n    message_dict = { } \n    if os . path . isfile ( filename ) : \n        with open ( filename , 'r' ) as wfile : \n            key = \"\" \n            val = \"\" \n            startMess = False \n            while 1 : \n                each_line = wfile . readline ( ) \n                if not each_line : \n                    if startMess : \n                        add_to_dict ( val . strip ( ) , key , message_dict ) \n                    break \n                if \"keyname\" in each_line . lower ( ) : \n                    temp_strings = each_line . strip ( ) . split ( '=' ) \n                    if ( len ( temp_strings ) > 1 ) : \n                        if startMess : \n                            add_to_dict ( val . strip ( ) , key , message_dict ) \n                            val = \"\" \n                        key = temp_strings [ 1 ] . strip ( ) \n                        startMess = False \n                if ( len ( each_line ) > 1 ) and startMess : \n                    val = val + ( each_line ) \n                if \"ignoredmessage\" in each_line . lower ( ) : \n                    startMess = True \n                    temp_mess = each_line . split ( '=' ) \n                    if ( len ( temp_mess ) > 1 ) : \n                        val = temp_mess [ 1 ] \n    return message_dict "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if len ( argv ) < 2 : \n        usage ( ) \n    i = 1 \n    while ( i < len ( argv ) ) : \n        s = argv [ i ] \n        if ( s == \"--inputfileadd\" ) : \n            i = i + ( 1 ) \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        elif ( s == \"--inputfilerm\" ) : \n            i = i + ( 1 ) \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_old_messages_to_remove = argv [ i ] \n        elif ( s == \"--loadjavamessage\" ) : \n            i = i + ( 1 ) \n            if i > len ( argv ) : \n                usage ( ) \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == \"--savejavamessage\" ) : \n            i = i + ( 1 ) \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_save_java_message_filename = argv [ i ] \n        elif ( s == '--printjavamessage' ) : \n            i = i + ( 1 ) \n            g_print_java_messages = True \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == '--help' ) : \n            usage ( ) \n        else : \n            unknown_arg ( s ) \n        i = i + ( 1 ) "}
{"1623": "\ndef getresponse ( self ) : \n    status = self . _httprequest . status ( ) \n    status_text = self . _httprequest . status_text ( ) \n    resp_headers = self . _httprequest . get_all_response_headers ( ) \n    fixed_headers = [ ] \n    for resp_header in resp_headers . split ( '\\n' ) : \n        if ( resp_header . startswith ( '\\t' ) or resp_header . startswith ( ' ' ) ) and fixed_headers : \n            fixed_headers [ - 1 ] = fixed_headers [ - 1 ] + ( resp_header ) \n        else : \n            fixed_headers . append ( resp_header ) \n    headers = [ ] \n    for resp_header in fixed_headers : \n        if ':' in resp_header : \n            pos = resp_header . find ( ':' ) \n            headers . append ( ( resp_header [ : pos ] . lower ( ) , resp_header [ pos + 1 : ] . strip ( ) ) ) \n    body = self . _httprequest . response_body ( ) \n    length = len ( body ) \n    return _Response ( status , status_text , length , headers , body ) "}
{"1624": "\ndef _get_readable_id ( id_name , id_prefix_to_skip ) : \n    pos = id_name . find ( '//' ) \n    if pos != - 1 : \n        pos = pos + ( 2 ) \n        if id_prefix_to_skip : \n            pos = id_name . find ( id_prefix_to_skip , pos ) \n            if pos != - 1 : \n                pos = pos + ( len ( id_prefix_to_skip ) ) \n        pos = id_name . find ( '/' , pos ) \n        if pos != - 1 : \n            return id_name [ pos + 1 : ] \n    return id_name "}
{"1656": "\ndef delete_deployment ( self , service_name , deployment_name , delete_vhd = False ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    path = self . _get_deployment_path_using_name ( service_name , deployment_name ) \n    if delete_vhd : \n        path = path + ( '?comp=media' ) \n    return self . _perform_delete ( path , as_async = True ) "}
{"1700": "\ndef delete_vm_image ( self , vm_image_name , delete_vhd = False ) : \n    _validate_not_none ( 'vm_image_name' , vm_image_name ) \n    path = self . _get_vm_image_path ( vm_image_name ) \n    if delete_vhd : \n        path = path + ( '?comp=media' ) \n    return self . _perform_delete ( path , as_async = True ) "}
{"1701": "\ndef list_vm_images ( self , location = None , publisher = None , category = None ) : \n    path = self . _get_vm_image_path ( ) \n    query = '' \n    if location : \n        query = query + ( '&location=' + location ) \n    if publisher : \n        query = query + ( '&publisher=' + publisher ) \n    if category : \n        query = query + ( '&category=' + category ) \n    if query : \n        path = path + '?' + query . lstrip ( '&' ) \n    return self . _perform_get ( path , VMImages ) "}
{"1706": "\ndef delete_os_image ( self , image_name , delete_vhd = False ) : \n    _validate_not_none ( 'image_name' , image_name ) \n    path = self . _get_image_path ( image_name ) \n    if delete_vhd : \n        path = path + ( '?comp=media' ) \n    return self . _perform_delete ( path , as_async = True ) "}
{"1710": "\ndef delete_data_disk ( self , service_name , deployment_name , role_name , lun , delete_vhd = False ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    _validate_not_none ( 'lun' , lun ) \n    path = self . _get_data_disk_path ( service_name , deployment_name , role_name , lun ) \n    if delete_vhd : \n        path = path + ( '?comp=media' ) \n    return self . _perform_delete ( path , as_async = True ) "}
{"1713": "\ndef delete_disk ( self , disk_name , delete_vhd = False ) : \n    _validate_not_none ( 'disk_name' , disk_name ) \n    path = self . _get_disk_path ( disk_name ) \n    if delete_vhd : \n        path = path + ( '?comp=media' ) \n    return self . _perform_delete ( path ) "}
{"1731": "\ndef get_receiver ( self , session = None , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if self . entity and not self . requires_session and session : \n        raise ValueError ( \"A session cannot be used with a non-sessionful entitiy.\" ) \n    if self . entity and self . requires_session and not session : \n        raise ValueError ( \"This entity requires a session.\" ) \n    if int ( prefetch ) < 0 or int ( prefetch ) > 50000 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch = prefetch + ( 1 ) \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if session : \n        return SessionReceiver ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) \n    return Receiver ( handler_id , self . entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1732": "\ndef get_deadletter_receiver ( self , transfer_deadletter = False , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if int ( prefetch ) < 0 or int ( prefetch ) > 50000 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch = prefetch + ( 1 ) \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if transfer_deadletter : \n        entity_uri = self . mgmt_client . format_transfer_dead_letter_queue_name ( self . entity_uri ) \n    else : \n        entity_uri = self . mgmt_client . format_dead_letter_queue_name ( self . entity_uri ) \n    return Receiver ( handler_id , entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1804": "\ndef _update_request_uri_query ( self , request ) : \n    if '?' in request . path : \n        request . path , _ , query_string = request . path . partition ( '?' ) \n        if query_string : \n            query_params = query_string . split ( '&' ) \n            for query in query_params : \n                if '=' in query : \n                    name , _ , value = query . partition ( '=' ) \n                    request . query . append ( ( name , value ) ) \n    request . path = url_quote ( request . path , '/()$=\\',' ) \n    if request . query : \n        request . path = request . path + ( '?' ) \n        for name , value in request . query : \n            if value is not None : \n                request . path = request . path + ( name + '=' + url_quote ( value , '/()$=\\',' ) + '&' ) \n        request . path = request . path [ : - 1 ] \n    return request . path , request . query "}
{"1816": "\ndef _general_error_handler ( http_error ) : \n    message = str ( http_error ) \n    if http_error . respbody is not None : \n        message = message + ( '\\n' + http_error . respbody . decode ( 'utf-8-sig' ) ) \n    raise AzureHttpError ( message , http_error . status ) "}
{"1866": "\ndef delete_site ( self , webspace_name , website_name , delete_empty_server_farm = False , delete_metrics = False ) : \n    path = self . _get_sites_details_path ( webspace_name , website_name ) \n    query = '' \n    if delete_empty_server_farm : \n        query = query + ( '&deleteEmptyServerFarm=true' ) \n    if delete_metrics : \n        query = query + ( '&deleteMetrics=true' ) \n    if query : \n        path = path + '?' + query . lstrip ( '&' ) \n    return self . _perform_delete ( path ) "}
{"1875": "\ndef check_job_collection_name ( self , cloud_service_id , job_collection_id ) : \n    _validate_not_none ( 'cloud_service_id' , cloud_service_id ) \n    _validate_not_none ( 'job_collection_id' , job_collection_id ) \n    path = self . _get_cloud_services_path ( cloud_service_id , \"scheduler\" , \"jobCollections\" ) \n    path = path + ( \"?op=checknameavailability&resourceName=\" + job_collection_id ) \n    return self . _perform_post ( path , None , AvailabilityResponse ) "}
{"1885": "\ndef print_inplace ( msg ) : \n    term_width = get_terminal_size ( ) . columns \n    spacing = term_width - terminal_width ( msg ) \n    if is_win32 : \n        spacing = spacing - ( 1 ) \n    sys . stderr . write ( \"\\r{0}\" . format ( msg ) ) \n    sys . stderr . write ( \" \" * max ( 0 , spacing ) ) \n    sys . stderr . flush ( ) "}
{"1886": "\ndef format_filesize ( size ) : \n    for suffix in ( \"bytes\" , \"KB\" , \"MB\" , \"GB\" , \"TB\" ) : \n        if size < 1024.0 : \n            if suffix in ( \"GB\" , \"TB\" ) : \n                return \"{0:3.2f} {1}\" . format ( size , suffix ) \n            else : \n                return \"{0:3.1f} {1}\" . format ( size , suffix ) \n        size = size / ( 1024.0 ) "}
{"1887": "\ndef format_time ( elapsed ) : \n    hours = int ( elapsed / ( 60 * 60 ) ) \n    minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) \n    seconds = int ( elapsed % 60 ) \n    rval = \"\" \n    if hours : \n        rval = rval + ( \"{0}h\" . format ( hours ) ) \n    if elapsed > 60 : \n        rval = rval + ( \"{0}m\" . format ( minutes ) ) \n    rval = rval + ( \"{0}s\" . format ( seconds ) ) \n    return rval "}
{"1889": "\ndef progress ( iterator , prefix ) : \n    if terminal_width ( prefix ) > 25 : \n        prefix = ( \"..\" + get_cut_prefix ( prefix , 23 ) ) \n    speed_updated = start = time ( ) \n    speed_written = written = 0 \n    speed_history = deque ( maxlen = 5 ) \n    for data in iterator : \n        yield data \n        now = time ( ) \n        elapsed = now - start \n        written = written + ( len ( data ) ) \n        speed_elapsed = now - speed_updated \n        if speed_elapsed >= 0.5 : \n            speed_history . appendleft ( ( written - speed_written , speed_updated , ) ) \n            speed_updated = now \n            speed_written = written \n            speed_history_written = sum ( h [ 0 ] for h in speed_history ) \n            speed_history_elapsed = now - speed_history [ - 1 ] [ 1 ] \n            speed = speed_history_written / speed_history_elapsed \n            status = create_status_line ( prefix = prefix , written = format_filesize ( written ) , elapsed = format_time ( elapsed ) , speed = format_filesize ( speed ) ) \n            print_inplace ( status ) \n    sys . stderr . write ( \"\\n\" ) \n    sys . stderr . flush ( ) "}
{"1902": "\ndef spawn ( self , parameters = None , arguments = None , stderr = None , timeout = None , short_option_prefix = \"-\" , long_option_prefix = \"--\" ) : \n    stderr = stderr or self . stderr \n    cmd = self . bake ( self . _check_cmd ( ) , parameters , arguments , short_option_prefix , long_option_prefix ) \n    log . debug ( \"Spawning command: {0}\" , subprocess . list2cmdline ( cmd ) ) \n    try : \n        process = subprocess . Popen ( cmd , stderr = stderr , stdout = subprocess . PIPE ) \n    except ( OSError , IOError ) as err : \n        raise StreamError ( \"Failed to start process: {0} ({1})\" . format ( self . _check_cmd ( ) , str ( err ) ) ) \n    if timeout : \n        elapsed = 0 \n        while elapsed < timeout and not process . poll ( ) : \n            time . sleep ( 0.25 ) \n            elapsed = elapsed + ( 0.25 ) \n        if not process . poll ( ) : \n            try : \n                log . debug ( \"Process timeout expired ({0}s), killing process\" . format ( timeout ) ) \n                process . kill ( ) \n            except Exception : \n                pass \n        process . wait ( ) \n    return process "}
{"1929": "\ndef fetch_streams_with_retry ( plugin , interval , count ) : \n    try : \n        streams = fetch_streams ( plugin ) \n    except PluginError as err : \n        log . error ( u\"{0}\" , err ) \n        streams = None \n    if not streams : \n        log . info ( \"Waiting for streams, retrying every {0} \" \"second(s)\" , interval ) \n    attempts = 0 \n    while not streams : \n        sleep ( interval ) \n        try : \n            streams = fetch_streams ( plugin ) \n        except FatalPluginError as err : \n            raise \n        except PluginError as err : \n            log . error ( u\"{0}\" , err ) \n        if count > 0 : \n            attempts = attempts + ( 1 ) \n            if attempts >= count : \n                break \n    return streams "}
{"1944": "\ndef set_option ( self , key , value ) : \n    if key == \"rtmpdump\" : \n        key = \"rtmp-rtmpdump\" \n    elif key == \"rtmpdump-proxy\" : \n        key = \"rtmp-proxy\" \n    elif key == \"errorlog\" : \n        key = \"subprocess-errorlog\" \n    elif key == \"errorlog-path\" : \n        key = \"subprocess-errorlog-path\" \n    if key == \"http-proxy\" : \n        self . http . proxies [ \"http\" ] = update_scheme ( \"http://\" , value ) \n    elif key == \"https-proxy\" : \n        self . http . proxies [ \"https\" ] = update_scheme ( \"https://\" , value ) \n    elif key == \"http-cookies\" : \n        if isinstance ( value , dict ) : \n            self . http . cookies . update ( value ) \n        else : \n            self . http . parse_cookies ( value ) \n    elif key == \"http-headers\" : \n        if isinstance ( value , dict ) : \n            self . http . headers . update ( value ) \n        else : \n            self . http . parse_headers ( value ) \n    elif key == \"http-query-params\" : \n        if isinstance ( value , dict ) : \n            self . http . params . update ( value ) \n        else : \n            self . http . parse_query_params ( value ) \n    elif key == \"http-trust-env\" : \n        self . http . trust_env = value \n    elif key == \"http-ssl-verify\" : \n        self . http . verify = value \n    elif key == \"http-disable-dh\" : \n        if value : \n            requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS = requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS + ( ':!DH' ) \n            try : \n                requests . packages . urllib3 . contrib . pyopenssl . DEFAULT_SSL_CIPHER_LIST = requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS . encode ( \"ascii\" ) \n            except AttributeError : \n                pass \n    elif key == \"http-ssl-cert\" : \n        self . http . cert = value \n    elif key == \"http-timeout\" : \n        self . http . timeout = value \n    else : \n        self . options . set ( key , value ) "}
{"1950": "\ndef hours_minutes_seconds ( value ) : \n    try : \n        return int ( value ) \n    except ValueError : \n        pass \n    match = ( _hours_minutes_seconds_re . match ( value ) or _hours_minutes_seconds_2_re . match ( value ) ) \n    if not match : \n        raise ValueError \n    s = 0 \n    s = s + ( int ( match . group ( \"hours\" ) or \"0\" ) * 60 * 60 ) \n    s = s + ( int ( match . group ( \"minutes\" ) or \"0\" ) * 60 ) \n    s = s + ( int ( match . group ( \"seconds\" ) or \"0\" ) ) \n    return s "}
{"1975": "\ndef readBytes ( self , n ) : \n    if self . pos & 7 : \n        raise ValueError ( 'readBytes: need byte boundary' ) \n    result = self . data [ self . pos >> 3 : ( self . pos >> 3 ) + n ] \n    self . pos = self . pos + ( 8 * n ) \n    return result "}
{"1978": "\ndef setDecode ( self , decodeTable ) : \n    self . decodeTable = decodeTable \n    todo = set ( decodeTable ) \n    maskLength = 0 \n    lengthTable = { } \n    while todo : \n        mask = ( 1 << maskLength ) - 1 \n        splitSymbols = defaultdict ( list ) \n        for s in todo : \n            splitSymbols [ s & mask ] . append ( s ) \n        for s , subset in splitSymbols . items ( ) : \n            if len ( subset ) == 1 : \n                lengthTable [ self . decodeTable [ s ] ] = maskLength \n                todo . remove ( s ) \n        maskLength = maskLength + ( 1 ) \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    self . switchToPrefix ( ) "}
{"1979": "\ndef setLength ( self , lengthTable ) : \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    nextCodes = [ ] \n    code = 0 \n    for bits in range ( self . maxLength + 1 ) : \n        code <<= 1 \n        nextCodes . append ( code ) \n        code = code + ( sum ( x == bits for x in lengthTable . values ( ) ) ) \n    self . decodeTable = { } \n    for symbol in sorted ( lengthTable ) : \n        bits = lengthTable [ symbol ] \n        bitpattern = '{:0{}b}' . format ( nextCodes [ bits ] , bits ) \n        self . decodeTable [ int ( bitpattern [ : : - 1 ] , 2 ) ] = symbol \n        nextCodes [ bits ] = nextCodes [ bits ] + ( 1 ) \n    self . switchToPrefix ( ) "}
{"1981": "\ndef readTuple ( self , stream ) : \n    length , symbol = self . decodePeek ( stream . peek ( self . maxLength ) ) \n    stream . pos = stream . pos + ( length ) \n    return length , symbol "}
{"1987": "\ndef mnemonic ( self , index , verbose = False ) : \n    if index < 16 : \n        return [ 'last' , '2last' , '3last' , '4last' , 'last-1' , 'last+1' , 'last-2' , 'last+2' , 'last-3' , 'last+3' , '2last-1' , '2last+1' , '2last-2' , '2last+2' , '2last-3' , '2last+3' ] [ index ] \n    if index < 16 + self . NDIRECT : \n        return str ( index - 16 ) \n    index = index - ( self . NDIRECT + 16 ) \n    hcode = index >> self . NPOSTFIX \n    lcode = index & ( 1 << self . NPOSTFIX ) - 1 \n    if self . NPOSTFIX : \n        formatString = '1{0}{1}{2:0{3}b}{4:+d}' \n    else : \n        formatString = '1{0}{1}{4:+d}' \n    return formatString . format ( hcode & 1 , 'x' * ( 2 + hcode >> 1 ) if hcode < 13 or verbose else '[{}*x]' . format ( 2 + hcode >> 1 ) , lcode , self . NPOSTFIX , self . NDIRECT + 1 - ( 4 << self . NPOSTFIX ) ) "}
{"1992": "\ndef metablockLength ( self ) : \n    self . MLEN = self . verboseRead ( MetablockLengthAlphabet ( ) ) \n    if self . MLEN : \n        return False \n    self . verboseRead ( ReservedAlphabet ( ) ) \n    MSKIP = self . verboseRead ( SkipLengthAlphabet ( ) ) \n    self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n    self . stream . pos = self . stream . pos + ( 8 * MSKIP ) \n    print ( \"Skipping to {:x}\" . format ( self . stream . pos >> 3 ) ) \n    return True "}
{"1993": "\ndef uncompressed ( self ) : \n    ISUNCOMPRESSED = self . verboseRead ( BoolCode ( 'UNCMPR' , description = 'Is uncompressed?' ) ) \n    if ISUNCOMPRESSED : \n        self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n        print ( 'Uncompressed data:' ) \n        self . output = self . output + ( self . stream . readBytes ( self . MLEN ) ) \n        print ( outputFormatter ( self . output [ - self . MLEN : ] ) ) \n    return ISUNCOMPRESSED "}
{"2086": "\ndef selection_undo ( self , name = \"default\" , executor = None ) : \n    logger . debug ( \"undo\" ) \n    executor = executor or self . executor \n    assert self . selection_can_undo ( name = name ) \n    selection_history = self . selection_histories [ name ] \n    index = self . selection_history_indices [ name ] \n    self . selection_history_indices [ name ] = self . selection_history_indices [ name ] - ( 1 ) \n    self . signal_selection_changed . emit ( self ) \n    logger . debug ( \"undo: selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) "}
{"2087": "\ndef selection_redo ( self , name = \"default\" , executor = None ) : \n    logger . debug ( \"redo\" ) \n    executor = executor or self . executor \n    assert self . selection_can_redo ( name = name ) \n    selection_history = self . selection_histories [ name ] \n    index = self . selection_history_indices [ name ] \n    next = selection_history [ index + 1 ] \n    self . selection_history_indices [ name ] = self . selection_history_indices [ name ] + ( 1 ) \n    self . signal_selection_changed . emit ( self ) \n    logger . debug ( \"redo: selection history is %r, index is %r\" , selection_history , index ) "}
{"2099": "\ndef _selection ( self , create_selection , name , executor = None , execute_fully = False ) : \n    selection_history = self . selection_histories [ name ] \n    previous_index = self . selection_history_indices [ name ] \n    current = selection_history [ previous_index ] if selection_history else None \n    selection = create_selection ( current ) \n    executor = executor or self . executor \n    selection_history . append ( selection ) \n    self . selection_history_indices [ name ] = self . selection_history_indices [ name ] + ( 1 ) \n    del selection_history [ self . selection_history_indices [ name ] : - 1 ] \n    if 0 : \n        if self . is_local ( ) : \n            if selection : \n                result = vaex . promise . Promise . fulfilled ( None ) \n                self . signal_selection_changed . emit ( self ) \n            else : \n                result = vaex . promise . Promise . fulfilled ( None ) \n                self . signal_selection_changed . emit ( self ) \n        else : \n            self . signal_selection_changed . emit ( self ) \n            result = vaex . promise . Promise . fulfilled ( None ) \n    self . signal_selection_changed . emit ( self ) \n    result = vaex . promise . Promise . fulfilled ( None ) \n    logger . debug ( \"select selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) \n    return result "}
{"2170": "\ndef pseudo_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = hop_length , window = window ) \n    fft_basis = np . abs ( fft_basis ) \n    D = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , pad_mode = pad_mode ) ) \n    C = fft_basis . dot ( D ) \n    if scale : \n        C = C / ( np . sqrt ( n_fft ) ) \n    else : \n        lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n        C = C * ( np . sqrt ( lengths [ : , np . newaxis ] / n_fft ) ) \n    return C "}
{"2171": "\ndef icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if hop_length > min ( lengths ) : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    for octave in range ( n_octaves - 1 , - 1 , - 1 ) : \n        slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , - ( octave ) * bins_per_octave - 1 ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] \n        oct_hop = hop_length // 2 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , 1 , 2 , scale = True , res_type = res_type , fix = False ) \n            y [ : len ( y_oct ) ] = y [ : len ( y_oct ) ] + ( y_oct ) \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2172": "\ndef __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : \n    basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = True , window = window ) \n    n_fft = basis . shape [ 1 ] \n    if ( hop_length is not None and n_fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) : \n        n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) \n    basis = basis * ( lengths [ : , np . newaxis ] / float ( n_fft ) ) \n    fft = get_fftlib ( ) \n    fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2 ) + 1 ] \n    fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) \n    return fft_basis , n_fft , lengths "}
{"2176": "\ndef __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : \n    downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) \n    if downsample_count > 0 and res_type == 'kaiser_fast' : \n        downsample_factor = 2 ** ( downsample_count ) \n        hop_length = hop_length // ( downsample_factor ) \n        if len ( y ) < downsample_factor : \n            raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) \n        new_sr = sr / float ( downsample_factor ) \n        y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = True ) \n        if not scale : \n            y = y * ( np . sqrt ( downsample_factor ) ) \n        sr = new_sr \n    return y , sr , hop_length "}
{"2177": "\ndef __dtw_calc_accu_cost ( C , D , D_steps , step_sizes_sigma , weights_mul , weights_add , max_0 , max_1 ) : \n    for cur_n in range ( max_0 , D . shape [ 0 ] ) : \n        for cur_m in range ( max_1 , D . shape [ 1 ] ) : \n            for cur_step_idx , cur_w_add , cur_w_mul in zip ( range ( step_sizes_sigma . shape [ 0 ] ) , weights_add , weights_mul ) : \n                cur_D = D [ cur_n - step_sizes_sigma [ cur_step_idx , 0 ] , cur_m - step_sizes_sigma [ cur_step_idx , 1 ] ] \n                cur_C = cur_w_mul * C [ cur_n - max_0 , cur_m - max_1 ] \n                cur_C = cur_C + ( cur_w_add ) \n                cur_cost = cur_D + cur_C \n                if cur_cost < D [ cur_n , cur_m ] : \n                    D [ cur_n , cur_m ] = cur_cost \n                    D_steps [ cur_n , cur_m ] = cur_step_idx \n    return D , D_steps "}
{"2184": "\ndef transition_local ( n_states , width , window = 'triangle' , wrap = False ) : \n    if not isinstance ( n_states , int ) or n_states <= 1 : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    width = np . asarray ( width , dtype = int ) \n    if width . ndim == 0 : \n        width = np . tile ( width , n_states ) \n    if width . shape != ( n_states , ) : \n        raise ParameterError ( 'width={} must have length equal to n_states={}' . format ( width , n_states ) ) \n    if np . any ( width < 1 ) : \n        raise ParameterError ( 'width={} must be at least 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    for i , width_i in enumerate ( width ) : \n        trans_row = pad_center ( get_window ( window , width_i , fftbins = False ) , n_states ) \n        trans_row = np . roll ( trans_row , n_states // 2 + i + 1 ) \n        if not wrap : \n            trans_row [ min ( n_states , i + width_i // 2 + 1 ) : ] = 0 \n            trans_row [ : max ( 0 , i - width_i // 2 ) ] = 0 \n        transition [ i ] = trans_row \n    transition = transition / ( transition . sum ( axis = 1 , keepdims = True ) ) \n    return transition "}
{"2185": "\ndef onset_detect ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , backtrack = False , energy = None , units = 'frames' , ** kwargs ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) \n    onset_envelope = onset_envelope - ( onset_envelope . min ( ) ) \n    if not onset_envelope . any ( ) : \n        return np . array ( [ ] , dtype = np . int ) \n    onset_envelope = onset_envelope / ( onset_envelope . max ( ) ) \n    kwargs . setdefault ( 'pre_max' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'post_max' , 0.00 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'pre_avg' , 0.10 * sr // hop_length ) \n    kwargs . setdefault ( 'post_avg' , 0.10 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'wait' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'delta' , 0.07 ) \n    onsets = util . peak_pick ( onset_envelope , ** kwargs ) \n    if backtrack : \n        if energy is None : \n            energy = onset_envelope \n        onsets = onset_backtrack ( onsets , energy ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) \n    elif units == 'time' : \n        onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return onsets "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = False , center = True , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if lag < 1 or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if max_size < 1 or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048 ) \n    hop_length = kwargs . get ( 'hop_length' , 512 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if max_size == 1 : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) \n    elif ref . shape != S . shape : \n        raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = True \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = False \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = 0 ) \n    pad_width = lag \n    if center : \n        pad_width = pad_width + ( n_fft // ( 2 * hop_length ) ) \n    onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - 1 ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ 1 ] ] \n    return onset_env "}
{"2197": "\ndef __coord_fft_hz ( n , sr = 22050 , ** _kwargs ) : \n    n_fft = 2 * ( n - 1 ) \n    basis = core . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    fmax = basis [ - 1 ] \n    basis = basis - ( 0.5 * ( basis [ 1 ] - basis [ 0 ] ) ) \n    basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) \n    return basis "}
{"2198": "\ndef __coord_mel_hz ( n , fmin = 0 , fmax = 11025.0 , ** _kwargs ) : \n    if fmin is None : \n        fmin = 0 \n    if fmax is None : \n        fmax = 11025.0 \n    basis = core . mel_frequencies ( n , fmin = fmin , fmax = fmax ) \n    basis [ 1 : ] = basis [ 1 : ] - ( 0.5 * np . diff ( basis ) ) \n    basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) \n    return basis "}
{"2213": "\ndef phase_vocoder ( D , rate , hop_length = None ) : \n    n_fft = 2 * ( D . shape [ 0 ] - 1 ) \n    if hop_length is None : \n        hop_length = int ( n_fft // 4 ) \n    time_steps = np . arange ( 0 , D . shape [ 1 ] , rate , dtype = np . float ) \n    d_stretch = np . zeros ( ( D . shape [ 0 ] , len ( time_steps ) ) , D . dtype , order = 'F' ) \n    phi_advance = np . linspace ( 0 , np . pi * hop_length , D . shape [ 0 ] ) \n    phase_acc = np . angle ( D [ : , 0 ] ) \n    D = np . pad ( D , [ ( 0 , 0 ) , ( 0 , 2 ) ] , mode = 'constant' ) \n    for ( t , step ) in enumerate ( time_steps ) : \n        columns = D [ : , int ( step ) : int ( step + 2 ) ] \n        alpha = np . mod ( step , 1.0 ) \n        mag = ( ( 1.0 - alpha ) * np . abs ( columns [ : , 0 ] ) + alpha * np . abs ( columns [ : , 1 ] ) ) \n        d_stretch [ : , t ] = mag * np . exp ( 1.j * phase_acc ) \n        dphase = ( np . angle ( columns [ : , 1 ] ) - np . angle ( columns [ : , 0 ] ) - phi_advance ) \n        dphase = dphase - 2.0 * np . pi * np . round ( dphase / ( 2.0 * np . pi ) ) \n        phase_acc = phase_acc + ( phi_advance + dphase ) \n    return d_stretch "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = False , norm = 1 , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2 \n    if norm is not None and norm != 1 and norm != np . inf : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    for i in range ( n_mels ) : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2 ] / fdiff [ i + 1 ] \n        weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) \n    if norm == 1 : \n        enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) \n        weights = weights * ( enorm [ : , np . newaxis ] ) \n    if not np . all ( ( mel_f [ : - 2 ] == 0 ) | ( weights . max ( axis = 1 ) > 0 ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2221": "\ndef chroma ( sr , n_fft , n_chroma = 12 , A440 = 440.0 , ctroct = 5.0 , octwidth = 2 , norm = 2 , base_c = True , dtype = np . float32 ) : \n    wts = np . zeros ( ( n_chroma , n_fft ) ) \n    frequencies = np . linspace ( 0 , sr , n_fft , endpoint = False ) [ 1 : ] \n    frqbins = n_chroma * hz_to_octs ( frequencies , A440 ) \n    frqbins = np . concatenate ( ( [ frqbins [ 0 ] - 1.5 * n_chroma ] , frqbins ) ) \n    binwidthbins = np . concatenate ( ( np . maximum ( frqbins [ 1 : ] - frqbins [ : - 1 ] , 1.0 ) , [ 1 ] ) ) \n    D = np . subtract . outer ( frqbins , np . arange ( 0 , n_chroma , dtype = 'd' ) ) . T \n    n_chroma2 = np . round ( float ( n_chroma ) / 2 ) \n    D = np . remainder ( D + n_chroma2 + 10 * n_chroma , n_chroma ) - n_chroma2 \n    wts = np . exp ( - 0.5 * ( 2 * D / np . tile ( binwidthbins , ( n_chroma , 1 ) ) ) ** 2 ) \n    wts = util . normalize ( wts , norm = norm , axis = 0 ) \n    if octwidth is not None : \n        wts = wts * ( np . tile ( np . exp ( - 0.5 * ( ( ( frqbins / n_chroma - ctroct ) / octwidth ) ** 2 ) ) , ( n_chroma , 1 ) ) ) \n    if base_c : \n        wts = np . roll ( wts , - 3 , axis = 0 ) \n    return np . ascontiguousarray ( wts [ : , : int ( 1 + n_fft / 2 ) ] , dtype = dtype ) "}
{"2230": "\ndef __window_ss_fill ( x , win_sq , n_frames , hop_length ) : \n    n = len ( x ) \n    n_fft = len ( win_sq ) \n    for i in range ( n_frames ) : \n        sample = i * hop_length \n        x [ sample : min ( n , sample + n_fft ) ] = x [ sample : min ( n , sample + n_fft ) ] + ( win_sq [ : max ( 0 , min ( n_fft , n - sample ) ) ] ) "}
{"2232": "\ndef diagonal_filter ( window , n , slope = 1.0 , angle = None , zero_mean = False ) : \n    if angle is None : \n        angle = np . arctan ( slope ) \n    win = np . diag ( get_window ( window , n , fftbins = False ) ) \n    if not np . isclose ( angle , np . pi / 4 ) : \n        win = scipy . ndimage . rotate ( win , 45 - angle * 180 / np . pi , order = 5 , prefilter = False ) \n    np . clip ( win , 0 , None , out = win ) \n    win = win / ( win . sum ( ) ) \n    if zero_mean : \n        win = win - ( win . mean ( ) ) \n    return win "}
{"2253": "\ndef resample ( y , orig_sr , target_sr , res_type = 'kaiser_best' , fix = True , scale = False , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if orig_sr == target_sr : \n        return y \n    ratio = float ( target_sr ) / orig_sr \n    n_samples = int ( np . ceil ( y . shape [ - 1 ] * ratio ) ) \n    if res_type in ( 'scipy' , 'fft' ) : \n        y_hat = scipy . signal . resample ( y , n_samples , axis = - 1 ) \n    elif res_type == 'polyphase' : \n        if int ( orig_sr ) != orig_sr or int ( target_sr ) != target_sr : \n            raise ParameterError ( 'polyphase resampling is only supported for integer-valued sampling rates.' ) \n        orig_sr = int ( orig_sr ) \n        target_sr = int ( target_sr ) \n        gcd = np . gcd ( orig_sr , target_sr ) \n        y_hat = scipy . signal . resample_poly ( y , target_sr // gcd , orig_sr // gcd , axis = - 1 ) \n    else : \n        y_hat = resampy . resample ( y , orig_sr , target_sr , filter = res_type , axis = - 1 ) \n    if fix : \n        y_hat = util . fix_length ( y_hat , n_samples , ** kwargs ) \n    if scale : \n        y_hat = y_hat / ( np . sqrt ( ratio ) ) \n    return np . ascontiguousarray ( y_hat , dtype = y . dtype ) "}
{"2256": "\ndef clicks ( times = None , frames = None , sr = 22050 , hop_length = 512 , click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : \n    if times is None : \n        if frames is None : \n            raise ParameterError ( 'either \"times\" or \"frames\" must be provided' ) \n        positions = frames_to_samples ( frames , hop_length = hop_length ) \n    else : \n        positions = time_to_samples ( times , sr = sr ) \n    if click is not None : \n        util . valid_audio ( click , mono = True ) \n    else : \n        if click_duration <= 0 : \n            raise ParameterError ( 'click_duration must be strictly positive' ) \n        if click_freq <= 0 : \n            raise ParameterError ( 'click_freq must be strictly positive' ) \n        angular_freq = 2 * np . pi * click_freq / float ( sr ) \n        click = np . logspace ( 0 , - 10 , num = int ( np . round ( sr * click_duration ) ) , base = 2.0 ) \n        click = click * ( np . sin ( angular_freq * np . arange ( len ( click ) ) ) ) \n    if length is None : \n        length = positions . max ( ) + click . shape [ 0 ] \n    else : \n        if length < 1 : \n            raise ParameterError ( 'length must be a positive integer' ) \n        positions = positions [ positions < length ] \n    click_signal = np . zeros ( length , dtype = np . float32 ) \n    for start in positions : \n        end = start + click . shape [ 0 ] \n        if end >= length : \n            click_signal [ start : ] = click_signal [ start : ] + ( click [ : length - start ] ) \n        else : \n            click_signal [ start : end ] = click_signal [ start : end ] + ( click ) \n    return click_signal "}
{"2282": "\ndef peak_pick ( x , pre_max , post_max , pre_avg , post_avg , delta , wait ) : \n    if pre_max < 0 : \n        raise ParameterError ( 'pre_max must be non-negative' ) \n    if pre_avg < 0 : \n        raise ParameterError ( 'pre_avg must be non-negative' ) \n    if delta < 0 : \n        raise ParameterError ( 'delta must be non-negative' ) \n    if wait < 0 : \n        raise ParameterError ( 'wait must be non-negative' ) \n    if post_max <= 0 : \n        raise ParameterError ( 'post_max must be positive' ) \n    if post_avg <= 0 : \n        raise ParameterError ( 'post_avg must be positive' ) \n    if x . ndim != 1 : \n        raise ParameterError ( 'input array must be one-dimensional' ) \n    pre_max = valid_int ( pre_max , cast = np . ceil ) \n    post_max = valid_int ( post_max , cast = np . ceil ) \n    pre_avg = valid_int ( pre_avg , cast = np . ceil ) \n    post_avg = valid_int ( post_avg , cast = np . ceil ) \n    wait = valid_int ( wait , cast = np . ceil ) \n    max_length = pre_max + post_max \n    max_origin = np . ceil ( 0.5 * ( pre_max - post_max ) ) \n    mov_max = scipy . ndimage . filters . maximum_filter1d ( x , int ( max_length ) , mode = 'constant' , origin = int ( max_origin ) , cval = x . min ( ) ) \n    avg_length = pre_avg + post_avg \n    avg_origin = np . ceil ( 0.5 * ( pre_avg - post_avg ) ) \n    mov_avg = scipy . ndimage . filters . uniform_filter1d ( x , int ( avg_length ) , mode = 'nearest' , origin = int ( avg_origin ) ) \n    n = 0 \n    while n - pre_avg < 0 and n < x . shape [ 0 ] : \n        start = n - pre_avg \n        start = start if start > 0 else 0 \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n = n + ( 1 ) \n    n = x . shape [ 0 ] - post_avg \n    n = n if n > 0 else 0 \n    while n < x . shape [ 0 ] : \n        start = n - pre_avg \n        start = start if start > 0 else 0 \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n = n + ( 1 ) \n    detections = x * ( x == mov_max ) \n    detections = detections * ( detections >= ( mov_avg + delta ) ) \n    peaks = [ ] \n    last_onset = - np . inf \n    for i in np . nonzero ( detections ) [ 0 ] : \n        if i > last_onset + wait : \n            peaks . append ( i ) \n            last_onset = i \n    return np . array ( peaks ) "}
{"2288": "\ndef softmask ( X , X_ref , power = 1 , split_zeros = False ) : \n    if X . shape != X_ref . shape : \n        raise ParameterError ( 'Shape mismatch: {}!={}' . format ( X . shape , X_ref . shape ) ) \n    if np . any ( X < 0 ) or np . any ( X_ref < 0 ) : \n        raise ParameterError ( 'X and X_ref must be non-negative' ) \n    if power <= 0 : \n        raise ParameterError ( 'power must be strictly positive' ) \n    dtype = X . dtype \n    if not np . issubdtype ( dtype , np . floating ) : \n        dtype = np . float32 \n    Z = np . maximum ( X , X_ref ) . astype ( dtype ) \n    bad_idx = ( Z < np . finfo ( dtype ) . tiny ) \n    Z [ bad_idx ] = 1 \n    if np . isfinite ( power ) : \n        mask = ( X / Z ) ** power \n        ref_mask = ( X_ref / Z ) ** power \n        good_idx = ~ bad_idx \n        mask [ good_idx ] = mask [ good_idx ] / ( mask [ good_idx ] + ref_mask [ good_idx ] ) \n        if split_zeros : \n            mask [ bad_idx ] = 0.5 \n        else : \n            mask [ bad_idx ] = 0.0 \n    else : \n        mask = X > X_ref \n    return mask "}
{"2291": "\ndef read ( self ) : \n    if self . _cache : \n        img = self . _cache . get ( self . _position ) \n        if img is not None : \n            ret = True \n        else : \n            if self . _position != self . _get_real_position ( ) : \n                self . _set_real_position ( self . _position ) \n            ret , img = self . _vcap . read ( ) \n            if ret : \n                self . _cache . put ( self . _position , img ) \n    else : \n        ret , img = self . _vcap . read ( ) \n    if ret : \n        self . _position = self . _position + ( 1 ) \n    return img "}
{"2292": "\ndef get_frame ( self , frame_id ) : \n    if frame_id < 0 or frame_id >= self . _frame_cnt : \n        raise IndexError ( '\"frame_id\" must be between 0 and {}' . format ( self . _frame_cnt - 1 ) ) \n    if frame_id == self . _position : \n        return self . read ( ) \n    if self . _cache : \n        img = self . _cache . get ( frame_id ) \n        if img is not None : \n            self . _position = frame_id + 1 \n            return img \n    self . _set_real_position ( frame_id ) \n    ret , img = self . _vcap . read ( ) \n    if ret : \n        if self . _cache : \n            self . _cache . put ( self . _position , img ) \n        self . _position = self . _position + ( 1 ) \n    return img "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50 , chunksize = 1 , skip_first = False , keep_order = True ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num = task_num - ( nproc * chunksize * int ( skip_first ) ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if len ( results ) < nproc * chunksize : \n                continue \n            elif len ( results ) == nproc * chunksize : \n                prog_bar . start ( ) \n                continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2297": "\ndef imrotate ( img , angle , center = None , scale = 1.0 , border_value = 0 , auto_bound = False ) : \n    if center is not None and auto_bound : \n        raise ValueError ( '`auto_bound` conflicts with `center`' ) \n    h , w = img . shape [ : 2 ] \n    if center is None : \n        center = ( ( w - 1 ) * 0.5 , ( h - 1 ) * 0.5 ) \n    assert isinstance ( center , tuple ) \n    matrix = cv2 . getRotationMatrix2D ( center , - angle , scale ) \n    if auto_bound : \n        cos = np . abs ( matrix [ 0 , 0 ] ) \n        sin = np . abs ( matrix [ 0 , 1 ] ) \n        new_w = h * sin + w * cos \n        new_h = h * cos + w * sin \n        matrix [ 0 , 2 ] = matrix [ 0 , 2 ] + ( ( new_w - w ) * 0.5 ) \n        matrix [ 1 , 2 ] = matrix [ 1 , 2 ] + ( ( new_h - h ) * 0.5 ) \n        w = int ( np . round ( new_w ) ) \n        h = int ( np . round ( new_h ) ) \n    rotated = cv2 . warpAffine ( img , matrix , ( w , h ) , borderValue = border_value ) \n    return rotated "}
{"2314": "\ndef dequantize_flow ( dx , dy , max_val = 0.02 , denorm = True ) : \n    assert dx . shape == dy . shape \n    assert dx . ndim == 2 or ( dx . ndim == 3 and dx . shape [ - 1 ] == 1 ) \n    dx , dy = [ dequantize ( d , - max_val , max_val , 255 ) for d in [ dx , dy ] ] \n    if denorm : \n        dx = dx * ( dx . shape [ 1 ] ) \n        dy = dy * ( dx . shape [ 0 ] ) \n    flow = np . dstack ( ( dx , dy ) ) \n    return flow "}
{"2329": "\ndef list_from_file ( filename , prefix = '' , offset = 0 , max_num = 0 ) : \n    cnt = 0 \n    item_list = [ ] \n    with open ( filename , 'r' ) as f : \n        for _ in range ( offset ) : \n            f . readline ( ) \n        for line in f : \n            if max_num > 0 and cnt >= max_num : \n                break \n            item_list . append ( prefix + line . rstrip ( '\\n' ) ) \n            cnt = cnt + ( 1 ) \n    return item_list "}
{"2340": "\ndef slice_list ( in_list , lens ) : \n    if not isinstance ( lens , list ) : \n        raise TypeError ( '\"indices\" must be a list of integers' ) \n    elif sum ( lens ) != len ( in_list ) : \n        raise ValueError ( 'sum of lens and list length does not match: {} != {}' . format ( sum ( lens ) , len ( in_list ) ) ) \n    out_list = [ ] \n    idx = 0 \n    for i in range ( len ( lens ) ) : \n        out_list . append ( in_list [ idx : idx + lens [ i ] ] ) \n        idx = idx + ( lens [ i ] ) \n    return out_list "}
{"2350": "\ndef flow2rgb ( flow , color_wheel = None , unknown_thr = 1e6 ) : \n    assert flow . ndim == 3 and flow . shape [ - 1 ] == 2 \n    if color_wheel is None : \n        color_wheel = make_color_wheel ( ) \n    assert color_wheel . ndim == 2 and color_wheel . shape [ 1 ] == 3 \n    num_bins = color_wheel . shape [ 0 ] \n    dx = flow [ : , : , 0 ] . copy ( ) \n    dy = flow [ : , : , 1 ] . copy ( ) \n    ignore_inds = ( np . isnan ( dx ) | np . isnan ( dy ) | ( np . abs ( dx ) > unknown_thr ) | ( np . abs ( dy ) > unknown_thr ) ) \n    dx [ ignore_inds ] = 0 \n    dy [ ignore_inds ] = 0 \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    if np . any ( rad > np . finfo ( float ) . eps ) : \n        max_rad = np . max ( rad ) \n        dx = dx / ( max_rad ) \n        dy = dy / ( max_rad ) \n    [ h , w ] = dx . shape \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    angle = np . arctan2 ( - dy , - dx ) / np . pi \n    bin_real = ( angle + 1 ) / 2 * ( num_bins - 1 ) \n    bin_left = np . floor ( bin_real ) . astype ( int ) \n    bin_right = ( bin_left + 1 ) % num_bins \n    w = ( bin_real - bin_left . astype ( np . float32 ) ) [ ... , None ] \n    flow_img = ( 1 - w ) * color_wheel [ bin_left , : ] + w * color_wheel [ bin_right , : ] \n    small_ind = rad <= 1 \n    flow_img [ small_ind ] = 1 - rad [ small_ind , None ] * ( 1 - flow_img [ small_ind ] ) \n    flow_img [ np . logical_not ( small_ind ) ] = flow_img [ np . logical_not ( small_ind ) ] * ( 0.75 ) \n    flow_img [ ignore_inds , : ] = 0 \n    return flow_img "}
{"2351": "\ndef make_color_wheel ( bins = None ) : \n    if bins is None : \n        bins = [ 15 , 6 , 4 , 11 , 13 , 6 ] \n    assert len ( bins ) == 6 \n    RY , YG , GC , CB , BM , MR = tuple ( bins ) \n    ry = [ 1 , np . arange ( RY ) / RY , 0 ] \n    yg = [ 1 - np . arange ( YG ) / YG , 1 , 0 ] \n    gc = [ 0 , 1 , np . arange ( GC ) / GC ] \n    cb = [ 0 , 1 - np . arange ( CB ) / CB , 1 ] \n    bm = [ np . arange ( BM ) / BM , 0 , 1 ] \n    mr = [ 1 , 0 , 1 - np . arange ( MR ) / MR ] \n    num_bins = RY + YG + GC + CB + BM + MR \n    color_wheel = np . zeros ( ( 3 , num_bins ) , dtype = np . float32 ) \n    col = 0 \n    for i , color in enumerate ( [ ry , yg , gc , cb , bm , mr ] ) : \n        for j in range ( 3 ) : \n            color_wheel [ j , col : col + bins [ i ] ] = color [ j ] \n        col = col + ( bins [ i ] ) \n    return color_wheel . T "}
{"2367": "\ndef egg2dist ( self , egginfo_path , distinfo_path ) : \n    def adios ( p ) : \n        if os . path . exists ( p ) and not os . path . islink ( p ) and os . path . isdir ( p ) : \n            shutil . rmtree ( p ) \n        elif os . path . exists ( p ) : \n            os . unlink ( p ) \n    adios ( distinfo_path ) \n    if not os . path . exists ( egginfo_path ) : \n        import glob \n        pat = os . path . join ( os . path . dirname ( egginfo_path ) , '*.egg-info' ) \n        possible = glob . glob ( pat ) \n        err = \"Egg metadata expected at %s but not found\" % ( egginfo_path , ) \n        if possible : \n            alt = os . path . basename ( possible [ 0 ] ) \n            err = err + ( \" (%s found - possible misnamed archive file?)\" % ( alt , ) ) \n        raise ValueError ( err ) \n    if os . path . isfile ( egginfo_path ) : \n        pkginfo_path = egginfo_path \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , egginfo_path ) \n        os . mkdir ( distinfo_path ) \n    else : \n        pkginfo_path = os . path . join ( egginfo_path , 'PKG-INFO' ) \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , pkginfo_path ) \n        shutil . copytree ( egginfo_path , distinfo_path , ignore = lambda x , y : set ( ( 'PKG-INFO' , 'requires.txt' , 'SOURCES.txt' , 'not-zip-safe' , ) ) ) \n        dependency_links_path = os . path . join ( distinfo_path , 'dependency_links.txt' ) \n        with open ( dependency_links_path , 'r' ) as dependency_links_file : \n            dependency_links = dependency_links_file . read ( ) . strip ( ) \n        if not dependency_links : \n            adios ( dependency_links_path ) \n    write_pkg_info ( os . path . join ( distinfo_path , 'METADATA' ) , pkg_info ) \n    metadata_path = os . path . join ( distinfo_path , 'METADATA' ) \n    self . add_requirements ( metadata_path ) \n    metadata_json_path = os . path . join ( distinfo_path , 'metadata.json' ) \n    pymeta = pkginfo_to_dict ( metadata_path , distribution = self . distribution ) \n    if 'description' in pymeta : \n        description_filename = 'DESCRIPTION.rst' \n        description_text = pymeta . pop ( 'description' ) \n        description_path = os . path . join ( distinfo_path , description_filename ) \n        with open ( description_path , \"wb\" ) as description_file : \n            description_file . write ( description_text . encode ( 'utf-8' ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'description' ] = description_filename \n    license = self . license_file ( ) \n    if license : \n        license_filename = 'LICENSE.txt' \n        shutil . copy ( license , os . path . join ( self . distinfo_dir , license_filename ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'license' ] = license_filename \n    with open ( metadata_json_path , \"w\" ) as metadata_json : \n        json . dump ( pymeta , metadata_json , sort_keys = True ) \n    adios ( egginfo_path ) "}
{"2409": "\ndef _einsum_matmul_index_helper ( gate_indices , number_of_qubits ) : \n    if len ( gate_indices ) + number_of_qubits > 26 : \n        raise QiskitError ( \"Total number of free indexes limited to 26\" ) \n    tens_in = ascii_lowercase [ : number_of_qubits ] \n    tens_out = list ( tens_in ) \n    mat_left = \"\" \n    mat_right = \"\" \n    for pos , idx in enumerate ( reversed ( gate_indices ) ) : \n        mat_left = mat_left + ( ascii_lowercase [ - 1 - pos ] ) \n        mat_right = mat_right + ( tens_in [ - 1 - idx ] ) \n        tens_out [ - 1 - idx ] = ascii_lowercase [ - 1 - pos ] \n    tens_out = \"\" . join ( tens_out ) \n    return mat_left , mat_right , tens_in , tens_out "}
{"2423": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if num_dag_qubits > self . coupling_map . size ( ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    best_sub = self . _best_subset ( num_dag_qubits ) \n    layout = Layout ( ) \n    map_iter = 0 \n    for qreg in dag . qregs . values ( ) : \n        for i in range ( qreg . size ) : \n            layout [ ( qreg , i ) ] = int ( best_sub [ map_iter ] ) \n            map_iter = map_iter + ( 1 ) \n    self . property_set [ 'layout' ] = layout "}
{"2424": "\ndef _best_subset ( self , n_qubits ) : \n    if n_qubits == 1 : \n        return np . array ( [ 0 ] ) \n    device_qubits = self . coupling_map . size ( ) \n    cmap = np . asarray ( self . coupling_map . get_edges ( ) ) \n    data = np . ones_like ( cmap [ : , 0 ] ) \n    sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , 0 ] , cmap [ : , 1 ] ) ) , shape = ( device_qubits , device_qubits ) ) . tocsr ( ) \n    best = 0 \n    best_map = None \n    for k in range ( sp_cmap . shape [ 0 ] ) : \n        bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = False , return_predecessors = False ) \n        connection_count = 0 \n        sub_graph = [ ] \n        for i in range ( n_qubits ) : \n            node_idx = bfs [ i ] \n            for j in range ( sp_cmap . indptr [ node_idx ] , sp_cmap . indptr [ node_idx + 1 ] ) : \n                node = sp_cmap . indices [ j ] \n                for counter in range ( n_qubits ) : \n                    if node == bfs [ counter ] : \n                        connection_count = connection_count + ( 1 ) \n                        sub_graph . append ( [ node_idx , node ] ) \n                        break \n        if connection_count > best : \n            best = connection_count \n            best_map = bfs [ 0 : n_qubits ] \n            mapping = { } \n            for edge in range ( best_map . shape [ 0 ] ) : \n                mapping [ best_map [ edge ] ] = edge \n            new_cmap = [ [ mapping [ c [ 0 ] ] , mapping [ c [ 1 ] ] ] for c in sub_graph ] \n            rows = [ edge [ 0 ] for edge in new_cmap ] \n            cols = [ edge [ 1 ] for edge in new_cmap ] \n            data = [ 1 ] * len ( rows ) \n            sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , shape = ( n_qubits , n_qubits ) ) . tocsr ( ) \n            perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) \n            best_map = best_map [ perm ] \n    return best_map "}
{"2426": "\ndef average_data ( counts , observable ) : \n    if not isinstance ( observable , dict ) : \n        observable = make_dict_observable ( observable ) \n    temp = 0 \n    tot = sum ( counts . values ( ) ) \n    for key in counts : \n        if key in observable : \n            temp = temp + ( counts [ key ] * observable [ key ] / tot ) \n    return temp "}
{"2450": "\ndef choi_to_rauli ( choi , order = 1 ) : \n    if order == 0 : \n        order = 'weight' \n    elif order == 1 : \n        order = 'tensor' \n    num_qubits = int ( np . log2 ( np . sqrt ( len ( choi ) ) ) ) \n    pgp = pauli_group ( num_qubits , case = order ) \n    rauli = [ ] \n    for i in pgp : \n        for j in pgp : \n            pauliop = np . kron ( j . to_matrix ( ) . T , i . to_matrix ( ) ) \n            rauli = rauli + ( [ np . trace ( np . dot ( choi , pauliop ) ) ] ) \n    return np . array ( rauli ) . reshape ( 4 ** num_qubits , 4 ** num_qubits ) "}
{"2454": "\ndef shannon_entropy ( pvec , base = 2 ) : \n    if base == 2 : \n        def logfn ( x ) : \n            return - x * np . log2 ( x ) \n    elif base == np . e : \n        def logfn ( x ) : \n            return - x * np . log ( x ) \n    else : \n        def logfn ( x ) : \n            return - x * np . log ( x ) / np . log ( base ) \n    h = 0. \n    for x in pvec : \n        if 0 < x < 1 : \n            h = h + ( logfn ( x ) ) \n    return h "}
{"2456": "\ndef mutual_information ( state , d0 , d1 = None ) : \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    mi = entropy ( partial_trace ( state , [ 0 ] , dimensions = [ d0 , d1 ] ) ) \n    mi = mi + ( entropy ( partial_trace ( state , [ 1 ] , dimensions = [ d0 , d1 ] ) ) ) \n    mi = mi - ( entropy ( state ) ) \n    return mi "}
{"2469": "\ndef quaternion_from_axis_rotation ( angle , axis ) : \n    out = np . zeros ( 4 , dtype = float ) \n    if axis == 'x' : \n        out [ 1 ] = 1 \n    elif axis == 'y' : \n        out [ 2 ] = 1 \n    elif axis == 'z' : \n        out [ 3 ] = 1 \n    else : \n        raise ValueError ( 'Invalid axis input.' ) \n    out = out * ( math . sin ( angle / 2.0 ) ) \n    out [ 0 ] = math . cos ( angle / 2.0 ) \n    return Quaternion ( out ) "}
{"2471": "\ndef normalize ( self , inplace = False ) : \n    if inplace : \n        nrm = self . norm ( ) \n        self . data = self . data / ( nrm ) \n        return None \n    nrm = self . norm ( ) \n    data_copy = np . array ( self . data , copy = True ) \n    data_copy = data_copy / ( nrm ) \n    return Quaternion ( data_copy ) "}
{"2496": "\ndef _kraus_to_choi ( data , input_dim , output_dim ) : \n    choi = 0 \n    kraus_l , kraus_r = data \n    if kraus_r is None : \n        for i in kraus_l : \n            vec = i . ravel ( order = 'F' ) \n            choi = choi + ( np . outer ( vec , vec . conj ( ) ) ) \n    else : \n        for i , j in zip ( kraus_l , kraus_r ) : \n            choi = choi + ( np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) ) \n    return choi "}
{"2500": "\ndef _kraus_to_stinespring ( data , input_dim , output_dim ) : \n    stine_pair = [ None , None ] \n    for i , kraus in enumerate ( data ) : \n        if kraus is not None : \n            num_kraus = len ( kraus ) \n            stine = np . zeros ( ( output_dim * num_kraus , input_dim ) , dtype = complex ) \n            for j , mat in enumerate ( kraus ) : \n                vec = np . zeros ( num_kraus ) \n                vec [ j ] = 1 \n                stine = stine + ( np . kron ( mat , vec [ : , None ] ) ) \n            stine_pair [ i ] = stine \n    return tuple ( stine_pair ) "}
{"2501": "\ndef _kraus_to_superop ( data , input_dim , output_dim ) : \n    kraus_l , kraus_r = data \n    superop = 0 \n    if kraus_r is None : \n        for i in kraus_l : \n            superop = superop + ( np . kron ( np . conj ( i ) , i ) ) \n    else : \n        for i , j in zip ( kraus_l , kraus_r ) : \n            superop = superop + ( np . kron ( np . conj ( j ) , i ) ) \n    return superop "}
{"2521": "\ndef label_for_box ( instruction ) : \n    label = instruction . name . capitalize ( ) \n    params = TextDrawing . params_for_label ( instruction ) \n    if params : \n        label = label + ( \"(%s)\" % ',' . join ( params ) ) \n    return label "}
{"2527": "\ndef _html_checker ( job_var , interval , status , header , _interval_set = False ) : \n    job_status = job_var . status ( ) \n    job_status_name = job_status . name \n    job_status_msg = job_status . value \n    status . value = header % ( job_status_msg ) \n    while job_status_name not in [ 'DONE' , 'CANCELLED' ] : \n        time . sleep ( interval ) \n        job_status = job_var . status ( ) \n        job_status_name = job_status . name \n        job_status_msg = job_status . value \n        if job_status_name == 'ERROR' : \n            break \n        else : \n            if job_status_name == 'QUEUED' : \n                job_status_msg = job_status_msg + ( ' (%s)' % job_var . queue_position ( ) ) \n                if not _interval_set : \n                    interval = max ( job_var . queue_position ( ) , 2 ) \n            else : \n                if not _interval_set : \n                    interval = 2 \n            status . value = header % ( job_status_msg ) \n    status . value = header % ( job_status_msg ) "}
{"2532": "\ndef _fix_gaussian_width ( gaussian_samples , amp : float , center : float , sigma : float , zeroed_width : Union [ None , float ] = None , rescale_amp : bool = False , ret_scale_factor : bool = False ) -> np . ndarray : \n    if zeroed_width is None : \n        zeroed_width = 2 * ( center + 1 ) \n    zero_offset = gaussian ( np . array ( [ - zeroed_width / 2 ] ) , amp , center , sigma ) \n    gaussian_samples = gaussian_samples - ( zero_offset ) \n    amp_scale_factor = 1. \n    if rescale_amp : \n        amp_scale_factor = amp / ( amp - zero_offset ) \n        gaussian_samples = gaussian_samples * ( amp_scale_factor ) \n    if ret_scale_factor : \n        return gaussian_samples , amp_scale_factor \n    return gaussian_samples "}
{"2548": "\ndef qasm ( self ) : \n    string_temp = self . header + \"\\n\" \n    string_temp = string_temp + ( self . extension_lib + \"\\n\" ) \n    for register in self . qregs : \n        string_temp = string_temp + ( register . qasm ( ) + \"\\n\" ) \n    for register in self . cregs : \n        string_temp = string_temp + ( register . qasm ( ) + \"\\n\" ) \n    for instruction , qargs , cargs in self . data : \n        if instruction . name == 'measure' : \n            qubit = qargs [ 0 ] \n            clbit = cargs [ 0 ] \n            string_temp = string_temp + ( \"%s %s[%d] -> %s[%d];\\n\" % ( instruction . qasm ( ) , qubit [ 0 ] . name , qubit [ 1 ] , clbit [ 0 ] . name , clbit [ 1 ] ) ) \n        else : \n            string_temp = string_temp + ( \"%s %s;\\n\" % ( instruction . qasm ( ) , \",\" . join ( [ \"%s[%d]\" % ( j [ 0 ] . name , j [ 1 ] ) for j in qargs + cargs ] ) ) ) \n    return string_temp "}
{"2550": "\ndef size ( self ) : \n    gate_ops = 0 \n    for instr , _ , _ in self . data : \n        if instr . name not in [ 'barrier' , 'snapshot' ] : \n            gate_ops = gate_ops + ( 1 ) \n    return gate_ops "}
{"2552": "\ndef count_ops ( self ) : \n    count_ops = { } \n    for instr , _ , _ in self . data : \n        if instr . name in count_ops . keys ( ) : \n            count_ops [ instr . name ] = count_ops [ instr . name ] + ( 1 ) \n        else : \n            count_ops [ instr . name ] = 1 \n    return count_ops "}
{"2553": "\ndef num_connected_components ( self , unitary_only = False ) : \n    reg_offset = 0 \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset = reg_offset + ( reg . size ) \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( 1 if instr . control else 0 ) \n        if num_qargs >= 2 and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = 0 \n            if instr . control and not unitary_only : \n                creg = instr . control [ 0 ] \n                creg_int = reg_map [ creg . name ] \n                for coff in range ( creg . size ) : \n                    temp_int = creg_int + coff \n                    for k in range ( num_sub_graphs ) : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched = num_touched + ( 1 ) \n                            break \n            for item in args : \n                reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] \n                for k in range ( num_sub_graphs ) : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched = num_touched + ( 1 ) \n                            break \n            if num_touched > 1 : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                for idx in range ( num_sub_graphs ) : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs = num_sub_graphs - ( ( num_touched - 1 ) ) \n        if num_sub_graphs == 1 : \n            break \n    return num_sub_graphs "}
{"2639": "\ndef _initialize_backend_prop ( self ) : \n    backend_prop = self . backend_prop \n    for ginfo in backend_prop . gates : \n        if ginfo . gate == 'cx' : \n            for item in ginfo . parameters : \n                if item . name == 'gate_error' : \n                    g_reliab = 1.0 - item . value \n                    break \n                else : \n                    g_reliab = 1.0 \n            swap_reliab = - math . log ( pow ( g_reliab , 3 ) ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap_reliab ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap_reliab ) \n            self . cx_errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g_reliab \n            self . gate_list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) \n    idx = 0 \n    for q in backend_prop . qubits : \n        for nduv in q : \n            if nduv . name == 'readout_error' : \n                self . readout_errors [ idx ] = 1.0 - nduv . value \n                self . available_hw_qubits . append ( idx ) \n        idx = idx + ( 1 ) \n    for edge in self . cx_errors : \n        self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ 0 ] ] * self . readout_errors [ edge [ 1 ] ] \n    self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) \n    for i in swap_costs_temp : \n        self . swap_costs [ i ] = { } \n        for j in swap_costs_temp [ i ] : \n            if ( i , j ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] \n            elif ( j , i ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] \n            else : \n                best_reliab = 0.0 \n                for n in self . swap_graph . neighbors ( j ) : \n                    if ( n , j ) in self . cx_errors : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] \n                    else : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] \n                    if reliab > best_reliab : \n                        best_reliab = reliab \n                self . swap_costs [ i ] [ j ] = best_reliab "}
{"2640": "\ndef _create_program_graph ( self , dag ) : \n    idx = 0 \n    for q in dag . qubits ( ) : \n        self . qarg_to_id [ q [ 0 ] . name + str ( q [ 1 ] ) ] = idx \n        idx = idx + ( 1 ) \n    for gate in dag . twoQ_gates ( ) : \n        qid1 = self . _qarg_to_id ( gate . qargs [ 0 ] ) \n        qid2 = self . _qarg_to_id ( gate . qargs [ 1 ] ) \n        min_q = min ( qid1 , qid2 ) \n        max_q = max ( qid1 , qid2 ) \n        edge_weight = 1 \n        if self . prog_graph . has_edge ( min_q , max_q ) : \n            edge_weight = self . prog_graph [ min_q ] [ max_q ] [ 'weight' ] + 1 \n        self . prog_graph . add_edge ( min_q , max_q , weight = edge_weight ) \n    return idx "}
{"2643": "\ndef _select_best_remaining_qubit ( self , prog_qubit ) : \n    reliab_store = { } \n    for hw_qubit in self . available_hw_qubits : \n        reliab = 1 \n        for n in self . prog_graph . neighbors ( prog_qubit ) : \n            if n in self . prog2hw : \n                reliab = reliab * ( self . swap_costs [ self . prog2hw [ n ] ] [ hw_qubit ] ) \n        reliab = reliab * ( self . readout_errors [ hw_qubit ] ) \n        reliab_store [ hw_qubit ] = reliab \n    max_reliab = 0 \n    best_hw_qubit = None \n    for hw_qubit in reliab_store : \n        if reliab_store [ hw_qubit ] > max_reliab : \n            max_reliab = reliab_store [ hw_qubit ] \n            best_hw_qubit = hw_qubit \n    return best_hw_qubit "}
{"2656": "\ndef _separate_bitstring ( bitstring , creg_sizes ) : \n    substrings = [ ] \n    running_index = 0 \n    for _ , size in reversed ( creg_sizes ) : \n        substrings . append ( bitstring [ running_index : running_index + size ] ) \n        running_index = running_index + ( size ) \n    return ' ' . join ( substrings ) "}
{"2694": "\ndef _add_wire ( self , wire ) : \n    if wire not in self . wires : \n        self . wires . append ( wire ) \n        self . _max_node_id = self . _max_node_id + ( 1 ) \n        input_map_wire = self . input_map [ wire ] = self . _max_node_id \n        self . _max_node_id = self . _max_node_id + ( 1 ) \n        output_map_wire = self . _max_node_id \n        wire_name = \"%s[%s]\" % ( wire [ 0 ] . name , wire [ 1 ] ) \n        inp_node = DAGNode ( data_dict = { 'type' : 'in' , 'name' : wire_name , 'wire' : wire } , nid = input_map_wire ) \n        outp_node = DAGNode ( data_dict = { 'type' : 'out' , 'name' : wire_name , 'wire' : wire } , nid = output_map_wire ) \n        self . _id_to_node [ input_map_wire ] = inp_node \n        self . _id_to_node [ output_map_wire ] = outp_node \n        self . input_map [ wire ] = inp_node \n        self . output_map [ wire ] = outp_node \n        self . _multi_graph . add_node ( inp_node ) \n        self . _multi_graph . add_node ( outp_node ) \n        self . _multi_graph . add_edge ( inp_node , outp_node ) \n        self . _multi_graph . adj [ inp_node ] [ outp_node ] [ 0 ] [ \"name\" ] = \"%s[%s]\" % ( wire [ 0 ] . name , wire [ 1 ] ) \n        self . _multi_graph . adj [ inp_node ] [ outp_node ] [ 0 ] [ \"wire\" ] = wire \n    else : \n        raise DAGCircuitError ( \"duplicate wire %s\" % ( wire , ) ) "}
{"2697": "\ndef _add_op_node ( self , op , qargs , cargs , condition = None ) : \n    node_properties = { \"type\" : \"op\" , \"op\" : op , \"name\" : op . name , \"qargs\" : qargs , \"cargs\" : cargs , \"condition\" : condition } \n    self . _max_node_id = self . _max_node_id + ( 1 ) \n    new_node = DAGNode ( data_dict = node_properties , nid = self . _max_node_id ) \n    self . _multi_graph . add_node ( new_node ) \n    self . _id_to_node [ self . _max_node_id ] = new_node "}
{"2704": "\ndef _check_wires_list ( self , wires , node ) : \n    if len ( set ( wires ) ) != len ( wires ) : \n        raise DAGCircuitError ( \"duplicate wires\" ) \n    wire_tot = len ( node . qargs ) + len ( node . cargs ) \n    if node . condition is not None : \n        wire_tot = wire_tot + ( node . condition [ 0 ] . size ) \n    if len ( wires ) != wire_tot : \n        raise DAGCircuitError ( \"expected %d wires, got %d\" % ( wire_tot , len ( wires ) ) ) "}
{"2724": "\ndef multigraph_layers ( self ) : \n    predecessor_count = dict ( ) \n    cur_layer = [ node for node in self . input_map . values ( ) ] \n    yield cur_layer \n    next_layer = [ ] \n    while cur_layer : \n        for node in cur_layer : \n            for successor in self . _multi_graph . successors ( node ) : \n                multiplicity = self . _multi_graph . number_of_edges ( node , successor ) \n                if successor in predecessor_count : \n                    predecessor_count [ successor ] = predecessor_count [ successor ] - ( multiplicity ) \n                else : \n                    predecessor_count [ successor ] = self . _multi_graph . in_degree ( successor ) - multiplicity \n                if predecessor_count [ successor ] == 0 : \n                    next_layer . append ( successor ) \n                    del predecessor_count [ successor ] \n        yield next_layer \n        cur_layer = next_layer \n        next_layer = [ ] "}
{"2727": "\ndef count_ops ( self ) : \n    op_dict = { } \n    for node in self . topological_op_nodes ( ) : \n        name = node . name \n        if name not in op_dict : \n            op_dict [ name ] = 1 \n        else : \n            op_dict [ name ] = op_dict [ name ] + ( 1 ) \n    return op_dict "}
{"2731": "\ndef tomography_set ( meas_qubits , meas_basis = 'Pauli' , prep_qubits = None , prep_basis = None ) : \n    if not isinstance ( meas_qubits , list ) : \n        raise QiskitError ( 'Qubits argument must be a list' ) \n    num_of_qubits = len ( meas_qubits ) \n    if prep_qubits is None : \n        prep_qubits = meas_qubits \n    if not isinstance ( prep_qubits , list ) : \n        raise QiskitError ( 'prep_qubits argument must be a list' ) \n    if len ( prep_qubits ) != len ( meas_qubits ) : \n        raise QiskitError ( 'meas_qubits and prep_qubitsare different length' ) \n    if isinstance ( meas_basis , str ) : \n        if meas_basis . lower ( ) == 'pauli' : \n            meas_basis = PAULI_BASIS \n    if isinstance ( prep_basis , str ) : \n        if prep_basis . lower ( ) == 'pauli' : \n            prep_basis = PAULI_BASIS \n        elif prep_basis . lower ( ) == 'sic' : \n            prep_basis = SIC_BASIS \n    circuits = [ ] \n    circuit_labels = [ ] \n    if prep_basis is None : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'meas' : meas } ) \n            label = '_meas_' \n            for qubit , op in meas . items ( ) : \n                label = label + ( '%s(%d)' % ( op [ 0 ] , qubit ) ) \n            circuit_labels . append ( label ) \n        return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'meas_basis' : meas_basis } \n    num_of_s = len ( list ( prep_basis . values ( ) ) [ 0 ] ) \n    plst_single = [ ( b , s ) for b in prep_basis . keys ( ) for s in range ( num_of_s ) ] \n    for plst_product in product ( plst_single , repeat = num_of_qubits ) : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            prep = dict ( zip ( prep_qubits , plst_product ) ) \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'prep' : prep , 'meas' : meas } ) \n            label = '_prep_' \n            for qubit , op in prep . items ( ) : \n                label = label + ( '%s%d(%d)' % ( op [ 0 ] , op [ 1 ] , qubit ) ) \n            label = label + ( '_meas_' ) \n            for qubit , op in meas . items ( ) : \n                label = label + ( '%s(%d)' % ( op [ 0 ] , qubit ) ) \n            circuit_labels . append ( label ) \n    return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'prep_basis' : prep_basis , 'meas_basis' : meas_basis } "}
{"2735": "\ndef marginal_counts ( counts , meas_qubits ) : \n    num_of_qubits = len ( list ( counts . keys ( ) ) [ 0 ] ) \n    qs = sorted ( meas_qubits , reverse = True ) \n    meas_keys = count_keys ( len ( qs ) ) \n    rgx = [ reduce ( lambda x , y : ( key [ qs . index ( y ) ] if y in qs else '\\\\d' ) + x , range ( num_of_qubits ) , '' ) for key in meas_keys ] \n    meas_counts = [ ] \n    for m in rgx : \n        c = 0 \n        for key , val in counts . items ( ) : \n            if match ( m , key ) : \n                c = c + ( val ) \n        meas_counts . append ( c ) \n    return dict ( zip ( meas_keys , meas_counts ) ) "}
{"2740": "\ndef __wizard ( rho , epsilon = None ) : \n    if epsilon is None : \n        epsilon = 0. \n    dim = len ( rho ) \n    rho_wizard = np . zeros ( [ dim , dim ] ) \n    v , w = np . linalg . eigh ( rho ) \n    for j in range ( dim ) : \n        if v [ j ] < epsilon : \n            tmp = v [ j ] \n            v [ j ] = 0. \n            x = 0. \n            for k in range ( j + 1 , dim ) : \n                x = x + ( tmp / ( dim - ( j + 1 ) ) ) \n                v [ k ] = v [ k ] + tmp / ( dim - ( j + 1 ) ) \n    for j in range ( dim ) : \n        rho_wizard = rho_wizard + v [ j ] * outer ( w [ : , j ] ) \n    return rho_wizard "}
{"2741": "\ndef wigner_data ( q_result , meas_qubits , labels , shots = None ) : \n    num = len ( meas_qubits ) \n    dim = 2 ** num \n    p = [ 0.5 + 0.5 * np . sqrt ( 3 ) , 0.5 - 0.5 * np . sqrt ( 3 ) ] \n    parity = 1 \n    for i in range ( num ) : \n        parity = np . kron ( parity , p ) \n    w = [ 0 ] * len ( labels ) \n    wpt = 0 \n    counts = [ marginal_counts ( q_result . get_counts ( circ ) , meas_qubits ) for circ in labels ] \n    for entry in counts : \n        x = [ 0 ] * dim \n        for i in range ( dim ) : \n            if bin ( i ) [ 2 : ] . zfill ( num ) in entry : \n                x [ i ] = float ( entry [ bin ( i ) [ 2 : ] . zfill ( num ) ] ) \n        if shots is None : \n            shots = np . sum ( x ) \n        for i in range ( dim ) : \n            w [ wpt ] = w [ wpt ] + ( x [ i ] / shots ) * parity [ i ] \n        wpt = wpt + ( 1 ) \n    return w "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg = msg + ( ' (%s)' % job . queue_position ( ) ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if len ( msg ) < msg_len : \n            msg = msg + ( ' ' * ( msg_len - len ( msg ) ) ) \n        elif len ( msg ) > msg_len : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2748": "\ndef qubits_tab ( backend ) : \n    props = backend . properties ( ) . to_dict ( ) \n    header_html = \"<div><font style='font-weight:bold'>{key}</font>: {value}</div>\" \n    header_html = header_html . format ( key = 'last_update_date' , value = props [ 'last_update_date' ] ) \n    update_date_widget = widgets . HTML ( value = header_html ) \n    qubit_html = \"<table>\" \n    qubit_html = qubit_html + ( \"\"\"<style>table {    border-collapse: collapse;    width: auto;}th, td {    text-align: left;    padding: 8px;}tr:nth-child(even) {background-color: #f6f6f6;}</style>\"\"\" ) \n    qubit_html = qubit_html + ( \"<tr><th></th><th>Frequency</th><th>T1</th><th>T2</th>\" ) \n    qubit_html = qubit_html + ( \"<th>U1 gate error</th><th>U2 gate error</th><th>U3 gate error</th>\" ) \n    qubit_html = qubit_html + ( \"<th>Readout error</th></tr>\" ) \n    qubit_footer = \"</table>\" \n    for qub in range ( len ( props [ 'qubits' ] ) ) : \n        name = 'Q%s' % qub \n        qubit_data = props [ 'qubits' ] [ qub ] \n        gate_data = props [ 'gates' ] [ 3 * qub : 3 * qub + 3 ] \n        t1_info = qubit_data [ 0 ] \n        t2_info = qubit_data [ 1 ] \n        freq_info = qubit_data [ 2 ] \n        readout_info = qubit_data [ 3 ] \n        freq = str ( round ( freq_info [ 'value' ] , 5 ) ) + ' ' + freq_info [ 'unit' ] \n        T1 = str ( round ( t1_info [ 'value' ] , 5 ) ) + ' ' + t1_info [ 'unit' ] \n        T2 = str ( round ( t2_info [ 'value' ] , 5 ) ) + ' ' + t2_info [ 'unit' ] \n        U1 = str ( round ( gate_data [ 0 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) \n        U2 = str ( round ( gate_data [ 1 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) \n        U3 = str ( round ( gate_data [ 2 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) \n        readout_error = round ( readout_info [ 'value' ] , 5 ) \n        qubit_html = qubit_html + ( \"<tr><td><font style='font-weight:bold'>%s</font></td><td>%s</td>\" ) \n        qubit_html = qubit_html + ( \"<td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\" ) \n        qubit_html = qubit_html % ( name , freq , T1 , T2 , U1 , U2 , U3 , readout_error ) \n    qubit_html = qubit_html + ( qubit_footer ) \n    qubit_widget = widgets . HTML ( value = qubit_html ) \n    out = widgets . VBox ( [ update_date_widget , qubit_widget ] ) \n    return out "}
{"2750": "\ndef plot_job_history ( jobs , interval = 'year' ) : \n    def get_date ( job ) : \n        return datetime . datetime . strptime ( job . creation_date ( ) , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    current_time = datetime . datetime . now ( ) \n    if interval == 'year' : \n        bins = [ ( current_time - datetime . timedelta ( days = k * 365 / 12 ) ) for k in range ( 12 ) ] \n    elif interval == 'month' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30 ) ] \n    elif interval == 'week' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7 ) ] \n    binned_jobs = [ 0 ] * len ( bins ) \n    if interval == 'year' : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . month == dat . month : \n                    binned_jobs [ ind ] = binned_jobs [ ind ] + ( 1 ) \n                    break \n            else : \n                continue \n    else : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . day == dat . day and date . month == dat . month : \n                    binned_jobs [ ind ] = binned_jobs [ ind ] + ( 1 ) \n                    break \n            else : \n                continue \n    nz_bins = [ ] \n    nz_idx = [ ] \n    for ind , val in enumerate ( binned_jobs ) : \n        if val != 0 : \n            nz_idx . append ( ind ) \n            nz_bins . append ( val ) \n    total_jobs = sum ( binned_jobs ) \n    colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , '#7a5195' , '#ef5675' , '#bc5090' ] \n    if interval == 'year' : \n        labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2 : ] , bins [ b ] . month ) for b in nz_idx ] \n    else : \n        labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] \n    fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 ) ) \n    ax . pie ( nz_bins [ : : - 1 ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14 } , rotatelabels = True , counterclock = False ) \n    ax . add_artist ( Circle ( ( 0 , 0 ) , 0.7 , color = 'white' , zorder = 1 ) ) \n    ax . text ( 0 , 0 , total_jobs , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = 26 ) \n    fig . tight_layout ( ) \n    return fig "}
{"2799": "\ndef update_backend_info ( self , interval = 60 ) : \n    my_thread = threading . currentThread ( ) \n    current_interval = 0 \n    started = False \n    all_dead = False \n    stati = [ None ] * len ( self . _backends ) \n    while getattr ( my_thread , \"do_run\" , True ) and not all_dead : \n        if current_interval == interval or started is False : \n            for ind , back in enumerate ( self . _backends ) : \n                _value = self . children [ ind ] . children [ 2 ] . value \n                _head = _value . split ( '<b>' ) [ 0 ] \n                try : \n                    _status = back . status ( ) \n                    stati [ ind ] = _status \n                except Exception : \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5 style='color:#ff5c49'>\" ) \n                    self . children [ ind ] . _is_alive = False \n                else : \n                    self . children [ ind ] . _is_alive = True \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5>\" ) \n            idx = list ( range ( len ( self . _backends ) ) ) \n            pending = [ s . pending_jobs for s in stati ] \n            _ , least_idx = zip ( * sorted ( zip ( pending , idx ) ) ) \n            for ind in least_idx : \n                if stati [ ind ] . operational : \n                    least_pending_idx = ind \n                    break \n            for var in idx : \n                if var == least_pending_idx : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n                self . children [ var ] . children [ 3 ] . children [ 1 ] . value = pending [ var ] \n                self . children [ var ] . children [ 3 ] . children [ 1 ] . max = max ( self . children [ var ] . children [ 3 ] . children [ 1 ] . max , pending [ var ] + 10 ) \n                if stati [ var ] . operational : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n            started = True \n            current_interval = 0 \n        time . sleep ( 1 ) \n        all_dead = not any ( [ wid . _is_alive for wid in self . children ] ) \n        current_interval = current_interval + ( 1 ) "}
{"2807": "\ndef _get_image_depth ( self ) : \n    max_column_widths = [ ] \n    for layer in self . ops : \n        current_max = 0 \n        for op in layer : \n            arg_str_len = 0 \n            for arg in op . op . params : \n                arg_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , _truncate_float , str ( arg ) ) \n                arg_str_len = arg_str_len + ( len ( arg_str ) ) \n            current_max = max ( arg_str_len , current_max ) \n        max_column_widths . append ( current_max ) \n    columns = 2 \n    columns = columns + ( len ( self . ops ) ) \n    sum_column_widths = sum ( 1 + v / 3 for v in max_column_widths ) \n    return columns , math . ceil ( sum_column_widths ) + 4 "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n        self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if self . timeout > 120000 : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs = self . additional_libs + ( ' ssl io_benchmark_method_stream_transport_ssl' ) "}
{"2869": "\ndef compose_config ( self ) : \n    streams_config = '' \n    stat_benchmarks = '' \n    for stream in self . streams : \n        streams_config = streams_config + ( stream . compose_config ( ) ) \n        if not stream . is_main : \n            stat_benchmarks = stat_benchmarks + ( \" \" + \"benchmark_io%s\" % stream . sequence_no ) \n    kwargs = { } \n    kwargs [ 'threads' ] = self . threads \n    kwargs [ 'phantom_log' ] = self . phantom_log \n    kwargs [ 'stat_log' ] = self . stat_log \n    kwargs [ 'benchmarks_block' ] = streams_config \n    kwargs [ 'stat_benchmarks' ] = stat_benchmarks \n    kwargs [ 'additional_libs' ] = self . additional_libs \n    kwargs [ 'phantom_modules_path' ] = self . phantom_modules_path \n    filename = self . core . mkstemp ( \".conf\" , \"phantom_\" ) \n    self . core . add_artifact_file ( filename ) \n    logger . debug ( \"Generating phantom config: %s\" , filename ) \n    template_str = resource_string ( __name__ , \"config/phantom.conf.tpl\" ) \n    tpl = string . Template ( template_str ) \n    config = tpl . substitute ( kwargs ) \n    with open ( filename , 'w' ) as conffile : \n        conffile . write ( config ) \n    return filename "}
{"2870": "\ndef get_info ( self ) : \n    result = copy . copy ( self . streams [ 0 ] ) \n    result . stat_log = self . stat_log \n    result . steps = [ ] \n    result . ammo_file = '' \n    result . rps_schedule = None \n    result . ammo_count = 0 \n    result . duration = 0 \n    result . instances = 0 \n    result . loadscheme = [ ] \n    result . loop_count = 0 \n    for stream in self . streams : \n        sec_no = 0 \n        logger . debug ( \"Steps: %s\" , stream . stepper_wrapper . steps ) \n        for item in stream . stepper_wrapper . steps : \n            for x in range ( 0 , item [ 1 ] ) : \n                if len ( result . steps ) > sec_no : \n                    result . steps [ sec_no ] [ 0 ] = result . steps [ sec_no ] [ 0 ] + ( item [ 0 ] ) \n                else : \n                    result . steps . append ( [ item [ 0 ] , 1 ] ) \n                sec_no = sec_no + ( 1 ) \n        if result . rps_schedule : \n            result . rps_schedule = [ ] \n        else : \n            result . rps_schedule = stream . stepper_wrapper . loadscheme \n        if result . loadscheme : \n            result . loadscheme = '' \n        else : \n            result . loadscheme = '' \n        if result . loop_count : \n            result . loop_count = u'0' \n        else : \n            result . loop_count = stream . stepper_wrapper . loop_count \n        result . ammo_file = result . ammo_file + ( '{} ' . format ( stream . stepper_wrapper . ammo_file ) ) \n        result . ammo_count = result . ammo_count + ( stream . stepper_wrapper . ammo_count ) \n        result . duration = max ( result . duration , stream . stepper_wrapper . duration ) \n        result . instances = result . instances + ( stream . instances ) \n    if not result . ammo_count : \n        raise ValueError ( \"Total ammo count cannot be zero\" ) \n    return result "}
{"2871": "\ndef compose_config ( self ) : \n    self . stepper_wrapper . prepare_stepper ( ) \n    self . stpd = self . stepper_wrapper . stpd \n    if self . stepper_wrapper . instances : \n        self . instances = self . stepper_wrapper . instances \n    if not self . stpd : \n        raise RuntimeError ( \"Cannot proceed with no STPD file\" ) \n    kwargs = { } \n    kwargs [ 'sequence_no' ] = self . sequence_no \n    if self . ssl : \n        _auth_section = '' \n        _ciphers = '' \n        ssl_template = \"transport_t ssl_transport = transport_ssl_t {\\n\" \"                timeout = 1s\\n\" \"                %s\\n\" \"                %s}\\n\" \"                transport = ssl_transport\" \n        if self . client_certificate or self . client_key : \n            _auth_section = 'auth_t def_auth = auth_t { key = \"%s\" cert = \"%s\"} auth = def_auth' % ( self . client_key , self . client_certificate ) \n        if self . client_cipher_suites : \n            _ciphers = 'ciphers = \"%s\"' % self . client_cipher_suites \n        kwargs [ 'ssl_transport' ] = ssl_template % ( _auth_section , _ciphers ) \n    else : \n        kwargs [ 'ssl_transport' ] = \"\" \n    kwargs [ 'method_stream' ] = self . method_prefix + \"_ipv6_t\" if self . ipv6 else self . method_prefix + \"_ipv4_t\" \n    kwargs [ 'phout' ] = self . phout_file \n    kwargs [ 'answ_log' ] = self . answ_log \n    kwargs [ 'answ_log_level' ] = self . answ_log_level \n    kwargs [ 'comment_answ' ] = \"# \" if self . answ_log_level == 'none' else '' \n    kwargs [ 'stpd' ] = self . stpd \n    kwargs [ 'source_log_prefix' ] = self . source_log_prefix \n    kwargs [ 'method_options' ] = self . method_options \n    if self . tank_type : \n        kwargs [ 'proto' ] = \"proto=http_proto%s\" % self . sequence_no if self . tank_type == 'http' else \"proto=none_proto\" \n        kwargs [ 'comment_proto' ] = \"\" \n    else : \n        kwargs [ 'proto' ] = \"\" \n        kwargs [ 'comment_proto' ] = \"#\" \n    if self . gatling : \n        kwargs [ 'bind' ] = 'bind={ ' + self . gatling + ' }' \n    else : \n        kwargs [ 'bind' ] = '' \n    kwargs [ 'ip' ] = self . resolved_ip \n    kwargs [ 'port' ] = self . port \n    kwargs [ 'timeout' ] = self . timeout \n    kwargs [ 'instances' ] = self . instances \n    tune = '' \n    if self . phantom_http_entity : \n        tune = tune + ( \"entity = \" + self . phantom_http_entity + \"\\n\" ) \n    if self . phantom_http_field : \n        tune = tune + ( \"field = \" + self . phantom_http_field + \"\\n\" ) \n    if self . phantom_http_field_num : \n        tune = tune + ( \"field_num = {}\\n\" . format ( self . phantom_http_field_num ) ) \n    if self . phantom_http_line : \n        tune = tune + ( \"line = \" + self . phantom_http_line + \"\\n\" ) \n    if tune : \n        kwargs [ 'reply_limits' ] = 'reply_limits = {\\n' + tune + \"}\" \n    else : \n        kwargs [ 'reply_limits' ] = '' \n    if self . is_main : \n        fname = 'phantom_benchmark_main.tpl' \n    else : \n        fname = 'phantom_benchmark_additional.tpl' \n    template_str = resource_string ( __name__ , \"config/\" + fname ) \n    tpl = string . Template ( template_str ) \n    config = tpl . substitute ( kwargs ) \n    return config "}
{"2873": "\ndef expand_time ( str_time , default_unit = 's' , multiplier = 1 ) : \n    parser = re . compile ( r'(\\d+)([a-zA-Z]*)' ) \n    parts = parser . findall ( str_time ) \n    result = 0.0 \n    for value , unit in parts : \n        value = int ( value ) \n        unit = unit . lower ( ) \n        if unit == '' : \n            unit = default_unit \n        if unit == 'ms' : \n            result = result + ( value * 0.001 ) \n            continue \n        elif unit == 's' : \n            result = result + ( value ) \n            continue \n        elif unit == 'm' : \n            result = result + ( value * 60 ) \n            continue \n        elif unit == 'h' : \n            result = result + ( value * 60 * 60 ) \n            continue \n        elif unit == 'd' : \n            result = result + ( value * 60 * 60 * 24 ) \n            continue \n        elif unit == 'w' : \n            result = result + ( value * 60 * 60 * 24 * 7 ) \n            continue \n        else : \n            raise ValueError ( \"String contains unsupported unit %s: %s\" % ( unit , str_time ) ) \n    return int ( result * multiplier ) "}
{"2876": "\ndef __get_stpd_filename ( self ) : \n    if self . use_caching : \n        sep = \"|\" \n        hasher = hashlib . md5 ( ) \n        hashed_str = \"cache version 6\" + sep + ';' . join ( self . load_profile . schedule ) + sep + str ( self . loop_limit ) \n        hashed_str = hashed_str + ( sep + str ( self . ammo_limit ) + sep + ';' . join ( self . load_profile . schedule ) + sep + str ( self . autocases ) ) \n        hashed_str = hashed_str + ( sep + \";\" . join ( self . uris ) + sep + \";\" . join ( self . headers ) + sep + self . http_ver + sep + \";\" . join ( self . chosen_cases ) ) \n        hashed_str = hashed_str + ( sep + str ( self . enum_ammo ) + sep + str ( self . ammo_type ) ) \n        if self . load_profile . is_instances ( ) : \n            hashed_str = hashed_str + ( sep + str ( self . instances ) ) \n        if self . ammo_file : \n            opener = resource . get_opener ( self . ammo_file ) \n            hashed_str = hashed_str + ( sep + opener . hash ) \n        else : \n            if not self . uris : \n                raise RuntimeError ( \"Neither ammofile nor uris specified\" ) \n            hashed_str = hashed_str + ( sep + ';' . join ( self . uris ) + sep + ';' . join ( self . headers ) ) \n        self . log . debug ( \"stpd-hash source: %s\" , hashed_str ) \n        hasher . update ( hashed_str . encode ( 'utf8' ) ) \n        if not os . path . exists ( self . cache_dir ) : \n            os . makedirs ( self . cache_dir ) \n        stpd = self . cache_dir + '/' + os . path . basename ( self . ammo_file ) + \"_\" + hasher . hexdigest ( ) + \".stpd\" \n    else : \n        stpd = os . path . realpath ( \"ammo.stpd\" ) \n    self . log . debug ( \"Generated cache file name: %s\" , stpd ) \n    return stpd "}
{"2886": "\ndef count_matched_codes ( codes_regex , codes_dict ) : \n    total = 0 \n    for code , count in codes_dict . items ( ) : \n        if codes_regex . match ( str ( code ) ) : \n            total = total + ( count ) \n    return total "}
{"2888": "\ndef _feed ( self ) : \n    self . plan = StpdReader ( self . stpd_filename ) \n    if self . cached_stpd : \n        self . plan = list ( self . plan ) \n    for task in self . plan : \n        if self . quit . is_set ( ) : \n            logger . info ( \"Stop feeding: gonna quit\" ) \n            return \n        while True : \n            try : \n                self . task_queue . put ( task , timeout = 1 ) \n                break \n            except Full : \n                if self . quit . is_set ( ) or self . workers_finished : \n                    return \n                else : \n                    continue \n    workers_count = self . instances \n    logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers_count ) ) \n    retry_delay = 1 \n    for _ in range ( 5 ) : \n        try : \n            [ self . task_queue . put ( None , timeout = 1 ) for _ in xrange ( 0 , workers_count ) ] \n            break \n        except Full : \n            logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry_delay ) \n            time . sleep ( retry_delay ) \n            retry_delay = retry_delay * ( 2 ) \n    try : \n        logger . info ( \"Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        logger . info ( \"All workers exited.\" ) \n        self . workers_finished = True \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . task_queue . close ( ) \n        self . results . close ( ) \n        self . quit . set ( ) \n        logger . info ( \"Going to quit. Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        self . workers_finished = True "}
{"2899": "\ndef __discover_jmeter_udp_port ( self ) : \n    r = re . compile ( self . DISCOVER_PORT_PATTERN ) \n    with open ( self . process_stderr . name , 'r' ) as f : \n        cnt = 0 \n        while self . process . pid and cnt < 10 : \n            line = f . readline ( ) \n            m = r . match ( line ) \n            if m is None : \n                cnt = cnt + ( 1 ) \n                time . sleep ( 1 ) \n            else : \n                port = int ( m . group ( 'port' ) ) \n                return port \n        else : \n            logger . warning ( 'JMeter UDP port wasn\\'t discovered' ) \n            return None "}
{"2900": "\ndef __add_jmeter_components ( self , jmx , jtl , variables ) : \n    logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) \n    with open ( jmx , 'r' ) as src_jmx : \n        source_lines = src_jmx . readlines ( ) \n    try : \n        closing = source_lines . pop ( - 1 ) \n        if \"WorkBenchGui\" in source_lines [ - 5 ] : \n            logger . info ( \"WorkBench checkbox enabled...bypassing\" ) \n            last_string_count = 6 \n        else : \n            last_string_count = 2 \n        while last_string_count > 0 : \n            closing = source_lines . pop ( - 1 ) + closing \n            last_string_count = last_string_count - ( 1 ) \n        logger . debug ( \"Closing statement: %s\" , closing ) \n    except Exception as exc : \n        raise RuntimeError ( \"Failed to find the end of JMX XML: %s\" % exc ) \n    udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) \n    udv_set = [ ] \n    for var_name , var_value in variables . iteritems ( ) : \n        udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) \n    udv = \"\\n\" . join ( udv_set ) \n    if self . jmeter_ver >= 2.13 : \n        save_connect = '<connectTime>true</connectTime>' \n    else : \n        save_connect = '' \n    if self . ext_log in [ 'errors' , 'all' ] : \n        level_map = { 'errors' : 'true' , 'all' : 'false' } \n        tpl_resource = 'jmeter_writer_ext.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'ext_log' : self . ext_log_file , 'ext_level' : level_map [ self . ext_log ] , 'save_connect' : save_connect } \n    else : \n        tpl_resource = 'jmeter_writer.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'save_connect' : save_connect } \n    tpl = resource_string ( __name__ , 'config/' + tpl_resource ) \n    try : \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) \n    except OSError as exc : \n        logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) \n    logger . debug ( \"Modified JMX: %s\" , new_jmx ) \n    with open ( new_jmx , \"wb\" ) as fh : \n        fh . write ( '' . join ( source_lines ) ) \n        fh . write ( tpl % tpl_args ) \n        fh . write ( closing ) \n    return new_jmx "}
{"2906": "\ndef __check_disk ( self ) : \n    cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" \n    cmd = cmd + ( self . core . artifacts_base_dir ) \n    cmd = cmd + ( \" | tail -n 1 | awk '{print \\$4}' \\\"\" ) \n    res = execute ( cmd , True , 0.1 , True ) \n    logging . debug ( \"Result: %s\" , res ) \n    if not len ( res [ 1 ] ) : \n        self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) \n        return \n    disk_free = res [ 1 ] \n    self . log . debug ( \"Disk free space: %s/%s\" , disk_free . strip ( ) , self . disk_limit ) \n    if int ( disk_free . strip ( ) ) < self . disk_limit : \n        raise RuntimeError ( \"Not enough local resources: disk space less than %sMB in %s: %sMB\" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) ) "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ True if i == ' ' else False for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( True ) \n            elif is_space ( chunk ) : \n                result . append ( True ) \n            else : \n                result . append ( False ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result = result + ( chunk ) \n        else : \n            if left > 0 : \n                if len ( chunk ) <= left : \n                    result = result + ( chunk ) \n                    left = left - ( len ( chunk ) ) \n                else : \n                    leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] \n                    was_cut = not is_empty ( leftover , markups ) \n                    if was_cut : \n                        result = result + ( chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' ) \n                    else : \n                        result = result + ( chunk [ : left ] ) \n                    left = 0 \n    return result "}
{"2911": "\ndef __render_left_panel ( self ) : \n    self . log . debug ( \"Rendering left blocks\" ) \n    left_block = self . left_panel \n    left_block . render ( ) \n    blank_space = self . left_panel_width - left_block . width \n    lines = [ ] \n    pre_space = ' ' * int ( blank_space / 2 ) \n    if not left_block . lines : \n        lines = [ ( '' ) , ( self . markup . RED + 'BROKEN LEFT PANEL' + self . markup . RESET ) ] \n    else : \n        while self . left_panel . lines : \n            src_line = self . left_panel . lines . pop ( 0 ) \n            line = pre_space + self . __truncate ( src_line , self . left_panel_width ) \n            post_space = ' ' * ( self . left_panel_width - len ( self . markup . clean_markup ( line ) ) ) \n            line = line + ( post_space + self . markup . RESET ) \n            lines . append ( line ) \n    return lines "}
{"2912": "\ndef render_screen ( self ) : \n    self . term_width , self . term_height = get_terminal_size ( ) \n    self . log . debug ( \"Terminal size: %sx%s\" , self . term_width , self . term_height ) \n    self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100 ) ) - 1 \n    if self . right_panel_width > 0 : \n        self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 \n    else : \n        self . right_panel_width = 0 \n        self . left_panel_width = self . term_width - 1 \n    self . log . debug ( \"Left/right panels width: %s/%s\" , self . left_panel_width , self . right_panel_width ) \n    widget_output = [ ] \n    if self . right_panel_width : \n        widget_output = [ ] \n        self . log . debug ( \"There are %d info widgets\" % len ( self . info_widgets ) ) \n        for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : \n            self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) \n            widget_out = widget . render ( self ) . strip ( ) \n            if widget_out : \n                widget_output = widget_output + ( widget_out . split ( \"\\n\" ) ) \n                widget_output = widget_output + ( [ \"\" ] ) \n    left_lines = self . __render_left_panel ( ) \n    self . log . debug ( \"Composing final screen output\" ) \n    output = [ ] \n    for line_no in range ( 1 , self . term_height ) : \n        line = \" \" \n        if line_no > 1 and left_lines : \n            left_line = left_lines . pop ( 0 ) \n            left_line_plain = self . markup . clean_markup ( left_line ) \n            left_line = left_line + ( ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) ) \n            line = line + ( left_line ) \n        else : \n            line = line + ( ' ' * self . left_panel_width ) \n        if self . right_panel_width : \n            line = line + ( self . markup . RESET ) \n            line = line + ( self . markup . WHITE ) \n            line = line + ( self . RIGHT_PANEL_SEPARATOR ) \n            line = line + ( self . markup . RESET ) \n            right_line = self . __get_right_line ( widget_output ) \n            line = line + ( right_line ) \n        output . append ( line ) \n    return self . markup . new_line . join ( output ) + self . markup . new_line "}
{"2913": "\ndef add_info_widget ( self , widget ) : \n    index = widget . get_index ( ) \n    while index in self . info_widgets . keys ( ) : \n        index = index + ( 1 ) \n    self . info_widgets [ widget . get_index ( ) ] = widget "}
{"2915": "\ndef clean_len ( self , line ) : \n    if isinstance ( line , basestring ) : \n        return len ( self . screen . markup . clean_markup ( line ) ) \n    elif isinstance ( line , tuple ) or isinstance ( line , list ) : \n        markups = self . screen . markup . get_markup_vars ( ) \n        length = 0 \n        for i in line : \n            if i not in markups : \n                length = length + ( len ( i ) ) \n        return length "}
{"2928": "\ndef get_options ( self , section , prefix = '' ) : \n    res = [ ] \n    try : \n        for option in self . config . options ( section ) : \n            if not prefix or option . find ( prefix ) == 0 : \n                res = res + ( [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] ) \n    except ConfigParser . NoSectionError as ex : \n        logger . warning ( \"No section: %s\" , ex ) \n    logger . debug ( \"Section: [%s] prefix: '%s' options:\\n%s\" , section , prefix , res ) \n    return res "}
{"2930": "\ndef _decode_stat_data ( self , chunk ) : \n    for date_str , statistics in chunk . iteritems ( ) : \n        date_obj = datetime . datetime . strptime ( date_str . split ( \".\" ) [ 0 ] , '%Y-%m-%d %H:%M:%S' ) \n        chunk_date = int ( time . mktime ( date_obj . timetuple ( ) ) ) \n        instances = 0 \n        for benchmark_name , benchmark in statistics . iteritems ( ) : \n            if not benchmark_name . startswith ( \"benchmark_io\" ) : \n                continue \n            for method , meth_obj in benchmark . iteritems ( ) : \n                if \"mmtasks\" in meth_obj : \n                    instances = instances + ( meth_obj [ \"mmtasks\" ] [ 2 ] ) \n        offset = chunk_date - 1 - self . start_time \n        reqps = 0 \n        if 0 <= offset < len ( self . phantom_info . steps ) : \n            reqps = self . phantom_info . steps [ offset ] [ 0 ] \n        yield self . stats_item ( chunk_date - 1 , instances , reqps ) "}
{"2976": "\ndef _create_context_free_log ( run_config , pipeline_def ) : \n    check . inst_param ( run_config , 'run_config' , RunConfig ) \n    check . inst_param ( pipeline_def , 'pipeline_def' , PipelineDefinition ) \n    loggers = [ define_colored_console_logger ( 'dagster' ) ] \n    if run_config . event_callback : \n        loggers = loggers + ( [ construct_event_logger ( run_config . event_callback ) ] ) \n    elif run_config . loggers : \n        loggers = loggers + ( run_config . loggers ) \n    return DagsterLog ( run_config . run_id , get_logging_tags ( None , run_config , pipeline_def ) , loggers ) "}
{"2989": "\ndef coalesce_execution_steps ( execution_plan ) : \n    solid_order = _coalesce_solid_order ( execution_plan ) \n    steps = defaultdict ( list ) \n    for solid_name , solid_steps in itertools . groupby ( execution_plan . topological_steps ( ) , lambda x : x . solid_name ) : \n        steps [ solid_name ] = steps [ solid_name ] + ( list ( solid_steps ) ) \n    return OrderedDict ( [ ( solid_name , steps [ solid_name ] ) for solid_name in solid_order ] ) "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if num_part < 2 : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if num_part > len ( sizes ) : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if num_part == 2 : \n        total_nfps , u = min ( ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( 0 , len ( sizes ) - 1 ) ) \n        return [ ( sizes [ 0 ] , sizes [ u ] ) , ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2 ) ) \n    p2i = lambda p : p - 2 \n    for p in range ( 2 , num_part ) : \n        for u in range ( p - 1 , len ( sizes ) ) : \n            if p == 2 : \n                cost [ u , p2i ( p ) ] = min ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , u ] for u1 in range ( ( p - 1 ) - 1 , u ) ) \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , len ( sizes ) - 1 ) ) \n    partitions = [ ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] \n    p = p - ( 1 ) \n    while p > 1 : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + 1 , u ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , u ) ) \n        partitions . insert ( 0 , ( sizes [ u1_best + 1 ] , sizes [ u ] ) ) \n        u = u1_best \n        p = p - ( 1 ) \n    partitions . insert ( 0 , ( sizes [ 0 ] , sizes [ u ] ) ) \n    return [ partitions , total_nfps , cost ] "}
{"3011": "\ndef index ( self , entries ) : \n    if not self . is_empty ( ) : \n        raise ValueError ( \"Cannot call index again on a non-empty index\" ) \n    if not isinstance ( entries , list ) : \n        queue = deque ( [ ] ) \n        for key , minhash , size in entries : \n            if size <= 0 : \n                raise ValueError ( \"Set size must be positive\" ) \n            queue . append ( ( key , minhash , size ) ) \n        entries = list ( queue ) \n    if len ( entries ) == 0 : \n        raise ValueError ( \"entries is empty\" ) \n    sizes , counts = np . array ( sorted ( Counter ( e [ 2 ] for e in entries ) . most_common ( ) ) ) . T \n    partitions = optimal_partitions ( sizes , counts , len ( self . indexes ) ) \n    for i , ( lower , upper ) in enumerate ( partitions ) : \n        self . lowers [ i ] , self . uppers [ i ] = lower , upper \n    entries . sort ( key = lambda e : e [ 2 ] ) \n    curr_part = 0 \n    for key , minhash , size in entries : \n        if size > self . uppers [ curr_part ] : \n            curr_part = curr_part + ( 1 ) \n        for r in self . indexes [ curr_part ] : \n            self . indexes [ curr_part ] [ r ] . insert ( key , minhash ) "}
{"3019": "\ndef apk ( actual , predicted , k = 10 ) : \n    if len ( predicted ) > k : \n        predicted = predicted [ : k ] \n    score = 0.0 \n    num_hits = 0.0 \n    for i , p in enumerate ( predicted ) : \n        if p in actual and p not in predicted [ : i ] : \n            num_hits = num_hits + ( 1.0 ) \n            score = score + ( num_hits / ( i + 1.0 ) ) \n    if len ( actual ) == 0 : \n        return 0.0 \n    return score / min ( len ( actual ) , k ) "}
{"3022": "\ndef query ( self , minhash , k ) : \n    if k <= 0 : \n        raise ValueError ( \"k must be positive\" ) \n    if len ( minhash ) < self . k * self . l : \n        raise ValueError ( \"The num_perm of MinHash out of range\" ) \n    results = set ( ) \n    r = self . k \n    while r > 0 : \n        for key in self . _query ( minhash , r , self . l ) : \n            results . add ( key ) \n            if len ( results ) >= k : \n                return list ( results ) \n        r = r - ( 1 ) \n    return list ( results ) "}
{"3164": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) \n    fetching = True \n    page = 0 \n    nposts = 0 \n    since = int ( from_date . timestamp ( ) * 1000 ) \n    while fetching : \n        raw_posts = self . client . posts ( self . channel , page = page ) \n        posts_before = nposts \n        for post in self . _parse_posts ( raw_posts ) : \n            if post [ 'update_at' ] < since : \n                fetching = False \n                break \n            user_id = post [ 'user_id' ] \n            user = self . _get_or_fetch_user ( user_id ) \n            post [ 'user_data' ] = user \n            yield post \n            nposts = nposts + ( 1 ) \n        if fetching : \n            if posts_before == nposts : \n                fetching = False \n            else : \n                page = page + ( 1 ) \n    logger . info ( \"Fetch process completed: %s posts fetched\" , nposts ) "}
{"3168": "\ndef fetch_items ( self , category , ** kwargs ) : \n    logger . info ( \"Looking for rss entries at feed '%s'\" , self . url ) \n    nentries = 0 \n    raw_entries = self . client . get_entries ( ) \n    entries = self . parse_feed ( raw_entries ) [ 'entries' ] \n    for item in entries : \n        yield item \n        nentries = nentries + ( 1 ) \n    logger . info ( \"Total number of entries: %i\" , nentries ) "}
{"3187": "\ndef fetch_items ( self , path , payload ) : \n    page = 0 \n    last_page = None \n    url_next = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , path ) \n    logger . debug ( \"Get GitLab paginated items from \" + url_next ) \n    response = self . fetch ( url_next , payload = payload ) \n    items = response . text \n    page = page + ( 1 ) \n    if 'last' in response . links : \n        last_url = response . links [ 'last' ] [ 'url' ] \n        last_page = last_url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] \n        last_page = int ( last_page ) \n        logger . debug ( \"Page: %i/%i\" % ( page , last_page ) ) \n    while items : \n        yield items \n        items = None \n        if 'next' in response . links : \n            url_next = response . links [ 'next' ] [ 'url' ] \n            response = self . fetch ( url_next , payload = payload ) \n            page = page + ( 1 ) \n            items = response . text \n            logger . debug ( \"Page: %i/%i\" % ( page , last_page ) ) "}
{"3192": "\ndef conversation_members ( self , conversation ) : \n    members = 0 \n    resource = self . RCONVERSATION_INFO \n    params = { self . PCHANNEL : conversation , } \n    raw_response = self . _fetch ( resource , params ) \n    response = json . loads ( raw_response ) \n    members = members + ( len ( response [ \"members\" ] ) ) \n    while 'next_cursor' in response [ 'response_metadata' ] and response [ 'response_metadata' ] [ 'next_cursor' ] : \n        params [ 'cursor' ] = response [ 'response_metadata' ] [ 'next_cursor' ] \n        raw_response = self . _fetch ( resource , params ) \n        response = json . loads ( raw_response ) \n        members = members + ( len ( response [ \"members\" ] ) ) \n    return members "}
{"3206": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    logger . info ( \"Fetching events of '%s' group from %s to %s\" , self . group , str ( from_date ) , str ( to_date ) if to_date else '--' ) \n    to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None \n    nevents = 0 \n    stop_fetching = False \n    ev_pages = self . client . events ( self . group , from_date = from_date ) \n    for evp in ev_pages : \n        events = [ event for event in self . parse_json ( evp ) ] \n        for event in events : \n            event_id = event [ 'id' ] \n            event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) \n            event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) \n            event_ts = self . metadata_updated_on ( event ) \n            if to_date_ts and event_ts >= to_date_ts : \n                stop_fetching = True \n                continue \n            yield event \n            nevents = nevents + ( 1 ) \n        if stop_fetching : \n            break \n    logger . info ( \"Fetch process completed: %s events fetched\" , nevents ) "}
{"3207": "\ndef events ( self , group , from_date = DEFAULT_DATETIME ) : \n    date = datetime_to_utc ( from_date ) \n    date = date . strftime ( \"since:%Y-%m-%dT%H:%M:%S.000Z\" ) \n    resource = urijoin ( group , self . REVENTS ) \n    fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT_FIELDS ) \n    fixed_params = fixed_params + ( '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) ) \n    resource = resource + ( fixed_params ) \n    params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max_items } \n    try : \n        for page in self . _fetch ( resource , params ) : \n            yield page \n    except requests . exceptions . HTTPError as error : \n        if error . response . status_code == 410 : \n            msg = \"Group is no longer accessible: {}\" . format ( error ) \n            raise RepositoryError ( cause = msg ) \n        else : \n            raise error "}
{"3209": "\ndef rsvps ( self , group , event_id ) : \n    resource = urijoin ( group , self . REVENTS , event_id , self . RRSVPS ) \n    fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP_FIELDS ) \n    fixed_params = fixed_params + ( '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) ) \n    resource = resource + ( fixed_params ) \n    params = { self . PPAGE : self . max_items } \n    for page in self . _fetch ( resource , params ) : \n        yield page "}
{"3228": "\ndef __execute_from_remote ( self , cmd ) : \n    result = None \n    retries = 0 \n    while retries < self . MAX_RETRIES : \n        try : \n            result = subprocess . check_output ( cmd , shell = True ) \n            break \n        except subprocess . CalledProcessError as ex : \n            logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) \n            time . sleep ( self . RETRY_WAIT * retries ) \n            retries = retries + ( 1 ) \n    if result is None : \n        result = RuntimeError ( cmd + \" failed \" + str ( self . MAX_RETRIES ) + \" times. Giving up!\" ) \n    if self . archive : \n        cmd = self . sanitize_for_archive ( cmd ) \n        self . archive . store ( cmd , None , None , result ) \n    if isinstance ( result , RuntimeError ) : \n        raise result \n    return result "}
{"3239": "\ndef __fetch_items ( self , path , payload ) : \n    page = 0 \n    url_next = path \n    fetch_data = True \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            raw_content = self . __send_request ( url_next , payload ) \n            content = json . loads ( raw_content ) \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code in [ 410 ] : \n                logger . warning ( \"Data is not available - %s\" , url_next ) \n                raw_content = '{\"total_size\": 0, \"start\": 0, \"entries\": []}' \n                content = json . loads ( raw_content ) \n            else : \n                raise e \n        if 'next_collection_link' in content : \n            url_next = content [ 'next_collection_link' ] \n            payload = None \n        else : \n            fetch_data = False \n        yield raw_content \n        page = page + ( 1 ) "}
{"3259": "\ndef _fetch_and_parse_messages ( self , mailing_list , from_date ) : \n    from_date = datetime_to_utc ( from_date ) \n    nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) \n    for mbox in mailing_list . mboxes : \n        tmp_path = None \n        try : \n            tmp_path = self . _copy_mbox ( mbox ) \n            for message in self . parse_mbox ( tmp_path ) : \n                tmsgs = tmsgs + ( 1 ) \n                if not self . _validate_message ( message ) : \n                    imsgs = imsgs + ( 1 ) \n                    continue \n                dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) \n                if dt < from_date : \n                    logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from_date ) ) \n                    tmsgs = tmsgs - ( 1 ) \n                    continue \n                message = self . _casedict_to_dict ( message ) \n                nmsgs = nmsgs + ( 1 ) \n                logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) \n                yield message \n        except ( OSError , EOFError ) as e : \n            logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) \n        except Exception as e : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n            raise e \n        finally : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n    logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs ) "}
{"3265": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    branches = kwargs [ 'branches' ] \n    latest_items = kwargs [ 'latest_items' ] \n    no_update = kwargs [ 'no_update' ] \n    ncommits = 0 \n    try : \n        if os . path . isfile ( self . gitpath ) : \n            commits = self . __fetch_from_log ( ) \n        else : \n            commits = self . __fetch_from_repo ( from_date , to_date , branches , latest_items , no_update ) \n        for commit in commits : \n            yield commit \n            ncommits = ncommits + ( 1 ) \n    except EmptyRepositoryError : \n        pass \n    logger . info ( \"Fetch process completed: %s commits fetched\" , ncommits ) "}
{"3269": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        parsed = False \n        self . nline = self . nline + ( 1 ) \n        while not parsed : \n            parsed = self . handlers [ self . state ] ( line ) \n            if self . state == self . COMMIT and self . commit : \n                commit = self . _build_commit ( ) \n                logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n                yield commit \n    if self . commit : \n        commit = self . _build_commit ( ) \n        logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n        yield commit "}
{"3335": "\ndef remove_invalid_xml_chars ( raw_xml ) : \n    illegal_unichrs = [ ( 0x00 , 0x08 ) , ( 0x0B , 0x1F ) , ( 0x7F , 0x84 ) , ( 0x86 , 0x9F ) ] \n    illegal_ranges = [ '%s-%s' % ( chr ( low ) , chr ( high ) ) for ( low , high ) in illegal_unichrs if low < sys . maxunicode ] \n    illegal_xml_re = re . compile ( '[%s]' % '' . join ( illegal_ranges ) ) \n    purged_xml = '' \n    for c in raw_xml : \n        if illegal_xml_re . search ( c ) is not None : \n            c = ' ' \n        purged_xml = purged_xml + ( c ) \n    return purged_xml "}
{"3348": "\ndef get_items ( self , from_date , url , expand_fields = True ) : \n    start_at = 0 \n    req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n    issues = req . text \n    data = req . json ( ) \n    titems = data [ 'total' ] \n    nitems = data [ 'maxResults' ] \n    start_at = start_at + ( min ( nitems , titems ) ) \n    self . __log_status ( start_at , titems , url ) \n    while issues : \n        yield issues \n        issues = None \n        if data [ 'startAt' ] + nitems < titems : \n            req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n            data = req . json ( ) \n            start_at = start_at + ( nitems ) \n            issues = req . text \n            self . __log_status ( start_at , titems , url ) "}
{"3356": "\ndef get_questions ( self , from_date ) : \n    page = 1 \n    url = urijoin ( self . base_url , self . VERSION_API , \"questions\" ) \n    req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) \n    questions = req . text \n    data = req . json ( ) \n    tquestions = data [ 'total' ] \n    nquestions = data [ 'page_size' ] \n    self . __log_status ( data [ 'quota_remaining' ] , data [ 'quota_max' ] , nquestions , tquestions ) \n    while questions : \n        yield questions \n        questions = None \n        if data [ 'has_more' ] : \n            page = page + ( 1 ) \n            backoff = data . get ( 'backoff' , None ) \n            if backoff : \n                logger . debug ( \"Expensive query. Wait %s secs to send a new request\" , backoff ) \n                time . sleep ( float ( backoff ) ) \n            req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) \n            data = req . json ( ) \n            questions = req . text \n            nquestions = nquestions + ( data [ 'page_size' ] ) \n            self . __log_status ( data [ 'quota_remaining' ] , data [ 'quota_max' ] , nquestions , tquestions ) "}
{"3360": "\ndef __fetch_1_27 ( self , from_date = None ) : \n    logger . info ( \"Looking for pages at url '%s'\" , self . url ) \n    npages = 0 \n    tpages = 0 \n    pages_done = [ ] \n    namespaces_contents = self . __get_namespaces_contents ( ) \n    arvcontinue = '' \n    while arvcontinue is not None : \n        raw_pages = self . client . get_pages_from_allrevisions ( namespaces_contents , from_date , arvcontinue ) \n        data_json = json . loads ( raw_pages ) \n        arvcontinue = data_json [ 'continue' ] [ 'arvcontinue' ] if 'continue' in data_json else None \n        pages_json = data_json [ 'query' ] [ 'allrevisions' ] \n        for page in pages_json : \n            if page [ 'pageid' ] in pages_done : \n                logger . debug ( \"Page %s already processed; skipped\" , page [ 'pageid' ] ) \n                continue \n            tpages = tpages + ( 1 ) \n            pages_done . append ( page [ 'pageid' ] ) \n            page_reviews = self . __get_page_reviews ( page ) \n            if not page_reviews : \n                logger . warning ( \"Revisions not found in %s [page id: %s], page skipped\" , page [ 'title' ] , page [ 'pageid' ] ) \n                continue \n            yield page_reviews \n            npages = npages + ( 1 ) \n    logger . info ( \"Total number of pages: %i, skipped %i\" , tpages , tpages - npages ) "}
{"3367": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Fetching articles of '%s' group on '%s' offset %s\" , self . group , self . host , str ( offset ) ) \n    narts , iarts , tarts = ( 0 , 0 , 0 ) \n    _ , _ , first , last , _ = self . client . group ( self . group ) \n    if offset <= last : \n        first = max ( first , offset ) \n        _ , overview = self . client . over ( ( first , last ) ) \n    else : \n        overview = [ ] \n    tarts = len ( overview ) \n    logger . debug ( \"Total number of articles to fetch: %s\" , tarts ) \n    for article_id , _ in overview : \n        try : \n            article_raw = self . client . article ( article_id ) \n            article = self . __parse_article ( article_raw ) \n        except ParseError : \n            logger . warning ( \"Error parsing %s article; skipping\" , article_id ) \n            iarts = iarts + ( 1 ) \n            continue \n        except nntplib . NNTPTemporaryError as e : \n            logger . warning ( \"Error '%s' fetching article %s; skipping\" , e . response , article_id ) \n            iarts = iarts + ( 1 ) \n            continue \n        yield article \n        narts = narts + ( 1 ) "}
{"3375": "\ndef setup_rate_limit_handler ( self , sleep_for_rate = False , min_rate_to_sleep = MIN_RATE_LIMIT , rate_limit_header = RATE_LIMIT_HEADER , rate_limit_reset_header = RATE_LIMIT_RESET_HEADER ) : \n    self . rate_limit = None \n    self . rate_limit_reset_ts = None \n    self . sleep_for_rate = sleep_for_rate \n    self . rate_limit_header = rate_limit_header \n    self . rate_limit_reset_header = rate_limit_reset_header \n    if min_rate_to_sleep > self . MAX_RATE_LIMIT : \n        msg = \"Minimum rate to sleep value exceeded (%d).\" \n        msg = msg + ( \"High values might cause the client to sleep forever.\" ) \n        msg = msg + ( \"Reset to %d.\" ) \n        self . min_rate_to_sleep = self . MAX_RATE_LIMIT \n        logger . warning ( msg , min_rate_to_sleep , self . MAX_RATE_LIMIT ) \n    else : \n        self . min_rate_to_sleep = min_rate_to_sleep "}
{"3381": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        self . nline = self . nline + ( 1 ) \n        if self . SUPYBOT_EMPTY_REGEX . match ( line ) : \n            continue \n        ts , msg = self . _parse_supybot_timestamp ( line ) \n        if self . SUPYBOT_EMPTY_COMMENT_REGEX . match ( msg ) : \n            continue \n        elif self . SUPYBOT_EMPTY_COMMENT_ACTION_REGEX . match ( msg ) : \n            continue \n        elif self . SUPYBOT_EMPTY_BOT_REGEX . match ( msg ) : \n            continue \n        itype , nick , body = self . _parse_supybot_msg ( msg ) \n        item = self . _build_item ( ts , itype , nick , body ) \n        yield item "}
{"3384": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Looking for topics at '%s', updated from '%s'\" , self . url , str ( from_date ) ) \n    ntopics = 0 \n    topics_ids = self . __fetch_and_parse_topics_ids ( from_date ) \n    for topic_id in topics_ids : \n        topic = self . __fetch_and_parse_topic ( topic_id ) \n        ntopics = ntopics + ( 1 ) \n        yield topic \n    logger . info ( \"Fetch process completed: %s topics fetched\" , ntopics ) "}
{"3388": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching tasks of '%s' from %s\" , self . url , str ( from_date ) ) \n    ntasks = 0 \n    for task in self . __fetch_tasks ( from_date ) : \n        yield task \n        ntasks = ntasks + ( 1 ) \n    logger . info ( \"Fetch process completed: %s tasks fetched\" , ntasks ) "}
{"3490": "\ndef fail ( message , exc_info = None , status = 1 , stacktrace = False ) : \n    text = message \n    if exc_info : \n        text = text + ( str ( exc_info ) ) \n    error ( text ) \n    if stacktrace : \n        error ( traceback . format_exc ( ) ) \n    clean_tempfiles ( ) \n    if __name__ == '__main__' : \n        sys . exit ( status ) \n    else : \n        raise RuntimeError ( status ) "}
{"3501": "\ndef processed ( self ) : \n    self . processed_tasks = self . processed_tasks + ( 1 ) \n    qsize = self . tasks . qsize ( ) \n    if qsize > 0 : \n        progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed_tasks , qsize , len ( self . workers ) ) \n    else : \n        progress ( '[%d task(s) completed, %d thread(s)]' , self . processed_tasks , len ( self . workers ) ) "}
{"3510": "\ndef source_expand ( self , source ) : \n    result = [ ] \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    for src in source : \n        tmp = self . opt . recursive \n        self . opt . recursive = False \n        result = result + ( [ f [ 'name' ] for f in self . s3walk ( src , True ) ] ) \n        self . opt . recursive = tmp \n    if ( len ( result ) == 0 ) and ( not self . opt . ignore_empty_source ) : \n        fail ( \"[Runtime Failure] Source doesn't exist.\" ) \n    return result "}
{"3527": "\ndef partial_match ( self , path , filter_path ) : \n    if not path or not filter_path : \n        return True \n    if path [ - 1 ] == PATH_SEP : \n        path = path [ 0 : - 1 ] \n    if filter_path [ - 1 ] == PATH_SEP : \n        filter_path = filter_path + ( '*' ) \n    pi = path . split ( PATH_SEP ) \n    fi = filter_path . split ( PATH_SEP ) \n    min_len = min ( len ( pi ) , len ( fi ) ) \n    matched = fnmatch . fnmatch ( PATH_SEP . join ( pi [ 0 : min_len ] ) , PATH_SEP . join ( fi [ 0 : min_len ] ) ) \n    return matched and ( self . opt . recursive or len ( pi ) <= len ( fi ) ) "}
{"3543": "\ndef get_handler ( self , args ) : \n    if len ( args ) == 2 : \n        args = args + ( [ '.' ] ) \n    self . validate ( 'cmd|s3|local' , args ) \n    source = args [ 1 ] \n    target = args [ 2 ] \n    self . s3handler ( ) . get_files ( source , target ) "}
{"3550": "\ndef _totalsize_handler ( self , args ) : \n    total_size = 0 \n    for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : \n        total_size = total_size + ( size ) \n    message ( str ( total_size ) ) "}
{"3627": "\ndef check_consistency ( self ) -> None : \n    checker_id = None \n    existing_ids = [ ] \n    for message in self . messages : \n        if checker_id is not None and checker_id != message . msgid [ 1 : 3 ] : \n            error_msg = \"Inconsistent checker part in message id \" \n            error_msg = error_msg + ( \"'{}' (expected 'x{checker_id}xx' \" . format ( message . msgid , checker_id = checker_id ) ) \n            error_msg = error_msg + ( \"because we already had {existing_ids}).\" . format ( existing_ids = existing_ids ) ) \n            raise InvalidMessageError ( error_msg ) \n        checker_id = message . msgid [ 1 : 3 ] \n        existing_ids . append ( message . msgid ) "}
{"3673": "\ndef _check_new_format ( self , node , func ) : \n    if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : \n        return \n    if node . starargs or node . kwargs : \n        return \n    try : \n        strnode = next ( func . bound . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : \n        return \n    try : \n        call_site = CallSite . from_call ( node ) \n    except astroid . InferenceError : \n        return \n    try : \n        fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) \n    except utils . IncompleteFormatString : \n        self . add_message ( \"bad-format-string\" , node = node ) \n        return \n    positional_arguments = call_site . positional_arguments \n    named_arguments = call_site . keyword_arguments \n    named_fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } \n    if num_args and manual_pos : \n        self . add_message ( \"format-combined-specification\" , node = node ) \n        return \n    check_args = False \n    num_args = num_args + ( sum ( 1 for field in named_fields if field == \"\" ) ) \n    if named_fields : \n        for field in named_fields : \n            if field and field not in named_arguments : \n                self . add_message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) \n        for field in named_arguments : \n            if field not in named_fields : \n                self . add_message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) \n        num_args = num_args or manual_pos \n        if positional_arguments or num_args : \n            empty = any ( True for field in named_fields if field == \"\" ) \n            if named_arguments or empty : \n                check_args = True \n    else : \n        check_args = True \n    if check_args : \n        num_args = num_args or manual_pos \n        if len ( positional_arguments ) > num_args : \n            self . add_message ( \"too-many-format-args\" , node = node ) \n        elif len ( positional_arguments ) < num_args : \n            self . add_message ( \"too-few-format-args\" , node = node ) \n    self . _detect_vacuous_formatting ( node , positional_arguments ) \n    self . _check_new_format_specifiers ( node , fields , named_arguments ) "}
{"3674": "\ndef process_non_raw_string_token ( self , prefix , string_body , start_row ) : \n    i = 0 \n    while True : \n        i = string_body . find ( \"\\\\\" , i ) \n        if i == - 1 : \n            break \n        next_char = string_body [ i + 1 ] \n        match = string_body [ i : i + 2 ] \n        if next_char in self . UNICODE_ESCAPE_CHARACTERS : \n            if \"u\" in prefix : \n                pass \n            elif ( _PY3K or self . _unicode_literals ) and \"b\" not in prefix : \n                pass \n            else : \n                self . add_message ( \"anomalous-unicode-escape-in-string\" , line = start_row , args = ( match , ) , ) \n        elif next_char not in self . ESCAPE_CHARACTERS : \n            self . add_message ( \"anomalous-backslash-in-string\" , line = start_row , args = ( match , ) ) \n        i = i + ( 2 ) "}
{"3675": "\ndef visit_section ( self , layout ) : \n    self . section = self . section + ( 1 ) \n    self . writeln ( ) \n    self . format_children ( layout ) \n    self . section = self . section - ( 1 ) \n    self . writeln ( ) "}
{"3676": "\ndef visit_evaluationsection ( self , layout ) : \n    self . section = self . section + ( 1 ) \n    self . format_children ( layout ) \n    self . section = self . section - ( 1 ) \n    self . writeln ( ) "}
{"3683": "\ndef _raise_duplicate_symbol ( msgid , symbol , other_symbol ) : \n    symbols = [ symbol , other_symbol ] \n    symbols . sort ( ) \n    error_message = \"Message id '{msgid}' cannot have both \" . format ( msgid = msgid ) \n    error_message = error_message + ( \"'{other_symbol}' and '{symbol}' as symbolic name.\" . format ( other_symbol = symbols [ 0 ] , symbol = symbols [ 1 ] ) ) \n    raise InvalidMessageError ( error_message ) "}
{"3684": "\ndef _raise_duplicate_msg_id ( symbol , msgid , other_msgid ) : \n    msgids = [ msgid , other_msgid ] \n    msgids . sort ( ) \n    error_message = \"Message symbol '{symbol}' cannot be used for \" . format ( symbol = symbol ) \n    error_message = error_message + ( \"'{other_msgid}' and '{msgid}' at the same time.\" . format ( other_msgid = msgids [ 0 ] , msgid = msgids [ 1 ] ) ) \n    raise InvalidMessageError ( error_message ) "}
{"3691": "\ndef report_messages_stats ( sect , stats , _ ) : \n    if not stats [ \"by_msg\" ] : \n        raise exceptions . EmptyReportError ( ) \n    in_order = sorted ( [ ( value , msg_id ) for msg_id , value in stats [ \"by_msg\" ] . items ( ) if not msg_id . startswith ( \"I\" ) ] ) \n    in_order . reverse ( ) \n    lines = ( \"message id\" , \"occurrences\" ) \n    for value , msg_id in in_order : \n        lines = lines + ( ( msg_id , str ( value ) ) ) \n    sect . append ( report_nodes . Table ( children = lines , cols = 2 , rheaders = 1 ) ) "}
{"3705": "\ndef _report_evaluation ( self ) : \n    previous_stats = config . load_results ( self . file_state . base_name ) \n    if self . stats [ \"statement\" ] == 0 : \n        return \n    evaluation = self . config . evaluation \n    try : \n        note = eval ( evaluation , { } , self . stats ) \n    except Exception as ex : \n        msg = \"An exception occurred while rating: %s\" % ex \n    else : \n        self . stats [ \"global_note\" ] = note \n        msg = \"Your code has been rated at %.2f/10\" % note \n        pnote = previous_stats . get ( \"global_note\" ) \n        if pnote is not None : \n            msg = msg + ( \" (previous run: %.2f/10, %+.2f)\" % ( pnote , note - pnote ) ) \n    if self . config . score : \n        sect = report_nodes . EvaluationSection ( msg ) \n        self . reporter . display_reports ( sect ) "}
{"3723": "\ndef get_table_content ( self , table ) : \n    result = [ [ ] ] \n    cols = table . cols \n    for cell in self . compute_content ( table ) : \n        if cols == 0 : \n            result . append ( [ ] ) \n            cols = table . cols \n        cols = cols - ( 1 ) \n        result [ - 1 ] . append ( cell ) \n    while len ( result [ - 1 ] ) < cols : \n        result [ - 1 ] . append ( \"\" ) \n    return result "}
{"3749": "\ndef get_packages ( directory , prefix ) : \n    result = [ ] \n    for package in os . listdir ( directory ) : \n        absfile = join ( directory , package ) \n        if isdir ( absfile ) : \n            if exists ( join ( absfile , \"__init__.py\" ) ) : \n                if prefix : \n                    result . append ( \"%s.%s\" % ( prefix , package ) ) \n                else : \n                    result . append ( package ) \n                result = result + ( get_packages ( absfile , result [ - 1 ] ) ) \n    return result "}
{"3752": "\ndef report_similarities ( sect , stats , old_stats ) : \n    lines = [ \"\" , \"now\" , \"previous\" , \"difference\" ] \n    lines = lines + ( table_lines_from_stats ( stats , old_stats , ( \"nb_duplicated_lines\" , \"percent_duplicated_lines\" ) ) ) \n    sect . append ( Table ( children = lines , cols = 4 , rheaders = 1 , cheaders = 1 ) ) "}
{"3756": "\ndef _display_sims ( self , sims ) : \n    nb_lignes_dupliquees = 0 \n    for num , couples in sims : \n        print ( ) \n        print ( num , \"similar lines in\" , len ( couples ) , \"files\" ) \n        couples = sorted ( couples ) \n        for lineset , idx in couples : \n            print ( \"==%s:%s\" % ( lineset . name , idx ) ) \n        for line in lineset . _real_lines [ idx : idx + num ] : \n            print ( \"  \" , line . rstrip ( ) ) \n        nb_lignes_dupliquees = nb_lignes_dupliquees + ( num * ( len ( couples ) - 1 ) ) \n    nb_total_lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) \n    print ( \"TOTAL lines=%s duplicates=%s percent=%.2f\" % ( nb_total_lignes , nb_lignes_dupliquees , nb_lignes_dupliquees * 100.0 / nb_total_lignes , ) ) "}
{"3757": "\ndef _find_common ( self , lineset1 , lineset2 ) : \n    lines1 = lineset1 . enumerate_stripped \n    lines2 = lineset2 . enumerate_stripped \n    find = lineset2 . find \n    index1 = 0 \n    min_lines = self . min_lines \n    while index1 < len ( lineset1 ) : \n        skip = 1 \n        num = 0 \n        for index2 in find ( lineset1 [ index1 ] ) : \n            non_blank = 0 \n            for num , ( ( _ , line1 ) , ( _ , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : \n                if line1 != line2 : \n                    if non_blank > min_lines : \n                        yield num , lineset1 , index1 , lineset2 , index2 \n                    skip = max ( skip , num ) \n                    break \n                if line1 : \n                    non_blank = non_blank + ( 1 ) \n            else : \n                num = num + ( 1 ) \n                if non_blank > min_lines : \n                    yield num , lineset1 , index1 , lineset2 , index2 \n                skip = max ( skip , num ) \n        index1 = index1 + ( skip ) "}
{"3759": "\ndef enumerate_stripped ( self , start_at = 0 ) : \n    idx = start_at \n    if start_at : \n        lines = self . _stripped_lines [ start_at : ] \n    else : \n        lines = self . _stripped_lines \n    for line in lines : \n        yield idx , line \n        idx = idx + ( 1 ) "}
{"3782": "\ndef display_reports ( self , layout ) : \n    self . section = 0 \n    if hasattr ( layout , \"report_id\" ) : \n        layout . children [ 0 ] . children [ 0 ] . data = layout . children [ 0 ] . children [ 0 ] . data + ( \" (%s)\" % layout . report_id ) \n    self . _display ( layout ) "}
{"3789": "\ndef visit_if ( self , node ) : \n    self . _check_boolean_expressions ( node ) \n    branches = 1 \n    if node . orelse and ( len ( node . orelse ) > 1 or not isinstance ( node . orelse [ 0 ] , If ) ) : \n        branches = branches + ( 1 ) \n    self . _inc_branch ( node , branches ) \n    self . _inc_all_stmts ( branches ) "}
{"3814": "\ndef format_help ( self , checkerref = False ) : \n    desc = self . descr \n    if checkerref : \n        desc = desc + ( \" This message belongs to the %s checker.\" % self . checker . name ) \n    title = self . msg \n    if self . symbol : \n        msgid = \"%s (%s)\" % ( self . symbol , self . msgid ) \n    else : \n        msgid = self . msgid \n    if self . minversion or self . maxversion : \n        restr = [ ] \n        if self . minversion : \n            restr . append ( \"< %s\" % \".\" . join ( [ str ( n ) for n in self . minversion ] ) ) \n        if self . maxversion : \n            restr . append ( \">= %s\" % \".\" . join ( [ str ( n ) for n in self . maxversion ] ) ) \n        restr = \" or \" . join ( restr ) \n        if checkerref : \n            desc = desc + ( \" It can't be emitted when using Python %s.\" % restr ) \n        else : \n            desc = desc + ( \" This message can't be emitted when using Python %s.\" % restr ) \n    desc = normalize_text ( \" \" . join ( desc . split ( ) ) , indent = \"  \" ) \n    if title != \"%s\" : \n        title = title . splitlines ( ) [ 0 ] \n        return \":%s: *%s*\\n%s\" % ( msgid , title . rstrip ( \" \" ) , desc ) \n    return \":%s:\\n%s\" % ( msgid , desc ) "}
{"3828": "\ndef print_full_documentation ( self , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    print ( \"Pylint global options and switches\" , file = stream ) \n    print ( \"----------------------------------\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"Pylint provides global options and switches.\" , file = stream ) \n    print ( \"\" , file = stream ) \n    by_checker = { } \n    for checker in self . get_checkers ( ) : \n        if checker . name == \"master\" : \n            if checker . options : \n                for section , options in checker . options_by_section ( ) : \n                    if section is None : \n                        title = \"General options\" \n                    else : \n                        title = \"%s options\" % section . capitalize ( ) \n                    print ( title , file = stream ) \n                    print ( \"~\" * len ( title ) , file = stream ) \n                    _rest_format_section ( stream , None , options ) \n                    print ( \"\" , file = stream ) \n        else : \n            name = checker . name \n            try : \n                by_checker [ name ] [ \"options\" ] = by_checker [ name ] [ \"options\" ] + ( checker . options_and_values ( ) ) \n                by_checker [ name ] [ \"msgs\" ] . update ( checker . msgs ) \n                by_checker [ name ] [ \"reports\" ] = by_checker [ name ] [ \"reports\" ] + ( checker . reports ) \n            except KeyError : \n                by_checker [ name ] = { \"options\" : list ( checker . options_and_values ( ) ) , \"msgs\" : dict ( checker . msgs ) , \"reports\" : list ( checker . reports ) , } \n    print ( \"Pylint checkers' options and switches\" , file = stream ) \n    print ( \"-------------------------------------\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"Pylint checkers can provide three set of features:\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"* options that control their execution,\" , file = stream ) \n    print ( \"* messages that they can raise,\" , file = stream ) \n    print ( \"* reports that they can generate.\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"Below is a list of all checkers and their features.\" , file = stream ) \n    print ( \"\" , file = stream ) \n    for checker , info in sorted ( by_checker . items ( ) ) : \n        self . _print_checker_doc ( checker , info , stream = stream ) "}
{"3830": "\ndef _get_indent_length ( line ) : \n    result = 0 \n    for char in line : \n        if char == \" \" : \n            result = result + ( 1 ) \n        elif char == \"\\t\" : \n            result = result + ( _TAB_LENGTH ) \n        else : \n            break \n    return result "}
{"3833": "\ndef handle_line_start ( self , pos ) : \n    if self . _line_start > - 1 : \n        return \n    check_token_position = pos \n    if self . _tokens . token ( pos ) == _ASYNC_TOKEN : \n        check_token_position = check_token_position + ( 1 ) \n    self . _is_block_opener = ( self . _tokens . token ( check_token_position ) in _CONTINUATION_BLOCK_OPENERS ) \n    self . _line_start = pos "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and tokens [ start ] [ 1 ] == \"for\" : \n        self . _pop_token ( ) \n    if tokens [ start + 1 ] [ 1 ] != \"(\" : \n        return \n    found_and_or = False \n    depth = 0 \n    keyword_token = str ( tokens [ start ] [ 1 ] ) \n    line_num = tokens [ start ] [ 2 ] [ 0 ] \n    for i in range ( start , len ( tokens ) - 1 ) : \n        token = tokens [ i ] \n        if token [ 0 ] == tokenize . NL : \n            return \n        if token [ 1 ] == \"(\" : \n            depth = depth + ( 1 ) \n        elif token [ 1 ] == \")\" : \n            depth = depth - ( 1 ) \n            if depth : \n                continue \n            if tokens [ i + 1 ] [ 1 ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + 1 ] [ 0 ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                if i == start + 2 : \n                    return \n                if keyword_token == \"not\" : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token in ( \"return\" , \"yield\" ) : \n                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token not in self . _keywords_with_parens : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n            return \n        elif depth == 1 : \n            if token [ 1 ] == \",\" : \n                return \n            if token [ 1 ] in ( \"and\" , \"or\" ) : \n                found_and_or = True \n            elif token [ 1 ] == \"yield\" : \n                return \n            elif token [ 1 ] == \"for\" : \n                return "}
{"3840": "\ndef _has_valid_type_annotation ( self , tokens , i ) : \n    if not self . _inside_brackets ( \"(\" ) : \n        return False \n    bracket_level = 0 \n    for token in tokens [ i - 1 : : - 1 ] : \n        if token [ 1 ] == \":\" : \n            return True \n        if token [ 1 ] == \"(\" : \n            return False \n        if token [ 1 ] == \"]\" : \n            bracket_level = bracket_level + ( 1 ) \n        elif token [ 1 ] == \"[\" : \n            bracket_level = bracket_level - ( 1 ) \n        elif token [ 1 ] == \",\" : \n            if not bracket_level : \n                return False \n        elif token [ 1 ] in ( \".\" , \"...\" ) : \n            continue \n        elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : \n            return False \n    return False "}
{"3846": "\ndef check_indent_level ( self , string , expected , line_num ) : \n    indent = self . config . indent_string \n    if indent == \"\\\\t\" : \n        indent = \"\\t\" \n    level = 0 \n    unit_size = len ( indent ) \n    while string [ : unit_size ] == indent : \n        string = string [ unit_size : ] \n        level = level + ( 1 ) \n    suppl = \"\" \n    while string and string [ 0 ] in \" \\t\" : \n        if string [ 0 ] != indent [ 0 ] : \n            if string [ 0 ] == \"\\t\" : \n                args = ( \"tab\" , \"space\" ) \n            else : \n                args = ( \"space\" , \"tab\" ) \n            self . add_message ( \"mixed-indentation\" , args = args , line = line_num ) \n            return level \n        suppl = suppl + ( string [ 0 ] ) \n        string = string [ 1 : ] \n    if level != expected or suppl : \n        i_type = \"spaces\" \n        if indent [ 0 ] == \"\\t\" : \n            i_type = \"tabs\" \n        self . add_message ( \"bad-indentation\" , line = line_num , args = ( level * unit_size + len ( suppl ) , i_type , expected * unit_size ) , ) \n    return None "}
{"3861": "\ndef generate_config ( self , stream = None , skipsections = ( ) , encoding = None ) : \n    options_by_section = { } \n    sections = [ ] \n    for provider in self . options_providers : \n        for section , options in provider . options_by_section ( ) : \n            if section is None : \n                section = provider . name \n            if section in skipsections : \n                continue \n            options = [ ( n , d , v ) for ( n , d , v ) in options if d . get ( \"type\" ) is not None and not d . get ( \"deprecated\" ) ] \n            if not options : \n                continue \n            if section not in sections : \n                sections . append ( section ) \n            alloptions = options_by_section . setdefault ( section , [ ] ) \n            alloptions = alloptions + ( options ) \n    stream = stream or sys . stdout \n    printed = False \n    for section in sections : \n        if printed : \n            print ( \"\\n\" , file = stream ) \n        utils . format_section ( stream , section . upper ( ) , sorted ( options_by_section [ section ] ) ) \n        printed = True "}
{"3882": "\ndef report_by_type_stats ( sect , stats , _ ) : \n    nice_stats = { } \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        try : \n            total = stats [ node_type ] \n        except KeyError : \n            raise exceptions . EmptyReportError ( ) \n        nice_stats [ node_type ] = { } \n        if total != 0 : \n            try : \n                documented = total - stats [ \"undocumented_\" + node_type ] \n                percent = ( documented * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"NC\" \n            try : \n                percent = ( stats [ \"badname_\" + node_type ] * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"NC\" \n    lines = ( \"type\" , \"number\" , \"old number\" , \"difference\" , \"%documented\" , \"%badname\" ) \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        new = stats [ node_type ] \n        lines = lines + ( ( node_type , str ( new ) , \"NC\" , \"NC\" , nice_stats [ node_type ] . get ( \"percent_documented\" , \"0\" ) , nice_stats [ node_type ] . get ( \"percent_badname\" , \"0\" ) , ) ) \n    sect . append ( reporter_nodes . Table ( children = lines , cols = 6 , rheaders = 1 ) ) "}
{"3899": "\ndef _check_name ( self , node_type , name , node , confidence = interfaces . HIGH ) : \n    def _should_exempt_from_invalid_name ( node ) : \n        if node_type == \"variable\" : \n            inferred = utils . safe_infer ( node ) \n            if isinstance ( inferred , astroid . ClassDef ) : \n                return True \n        return False \n    if utils . is_inside_except ( node ) : \n        clobbering , _ = utils . clobber_in_except ( node ) \n        if clobbering : \n            return \n    if name in self . config . good_names : \n        return \n    if name in self . config . bad_names : \n        self . stats [ \"badname_\" + node_type ] = self . stats [ \"badname_\" + node_type ] + ( 1 ) \n        self . add_message ( \"blacklisted-name\" , node = node , args = name ) \n        return \n    regexp = self . _name_regexps [ node_type ] \n    match = regexp . match ( name ) \n    if _is_multi_naming_match ( match , node_type , confidence ) : \n        name_group = self . _find_name_group ( node_type ) \n        bad_name_group = self . _bad_names . setdefault ( name_group , { } ) \n        warnings = bad_name_group . setdefault ( match . lastgroup , [ ] ) \n        warnings . append ( ( node , node_type , name , confidence ) ) \n    if match is None and not _should_exempt_from_invalid_name ( node ) : \n        self . _raise_name_warning ( node , node_type , name , confidence ) "}
{"3900": "\ndef _check_docstring ( self , node_type , node , report_missing = True , confidence = interfaces . HIGH ) : \n    docstring = node . doc \n    if docstring is None : \n        if not report_missing : \n            return \n        lines = utils . get_node_last_lineno ( node ) - node . lineno \n        if node_type == \"module\" and not lines : \n            return \n        max_lines = self . config . docstring_min_length \n        if node_type != \"module\" and max_lines > - 1 and lines < max_lines : \n            return \n        self . stats [ \"undocumented_\" + node_type ] = self . stats [ \"undocumented_\" + node_type ] + ( 1 ) \n        if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : \n            func = utils . safe_infer ( node . body [ 0 ] . value . func ) \n            if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : \n                if PY3K and func . bound . name == \"str\" : \n                    return \n                if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : \n                    return \n        self . add_message ( \"missing-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) \n    elif not docstring . strip ( ) : \n        self . stats [ \"undocumented_\" + node_type ] = self . stats [ \"undocumented_\" + node_type ] + ( 1 ) \n        self . add_message ( \"empty-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) "}
{"3903": "\ndef _subgraph_parse ( self , node , pathnode , extra_blocks ) : \n    loose_ends = [ ] \n    self . tail = node \n    self . dispatch_list ( node . body ) \n    loose_ends . append ( self . tail ) \n    for extra in extra_blocks : \n        self . tail = node \n        self . dispatch_list ( extra . body ) \n        loose_ends . append ( self . tail ) \n    if node . orelse : \n        self . tail = node \n        self . dispatch_list ( node . orelse ) \n        loose_ends . append ( self . tail ) \n    else : \n        loose_ends . append ( node ) \n    if node : \n        bottom = \"%s\" % self . _bottom_counter \n        self . _bottom_counter = self . _bottom_counter + ( 1 ) \n        for le in loose_ends : \n            self . graph . connect ( le , bottom ) \n        self . tail = bottom "}
{"3906": "\ndef walk ( self , astroid ) : \n    cid = astroid . __class__ . __name__ . lower ( ) \n    visit_events = self . visit_events . get ( cid , ( ) ) \n    leave_events = self . leave_events . get ( cid , ( ) ) \n    if astroid . is_statement : \n        self . nbstatements = self . nbstatements + ( 1 ) \n    for cb in visit_events or ( ) : \n        cb ( astroid ) \n    for child in astroid . get_children ( ) : \n        self . walk ( child ) \n    for cb in leave_events or ( ) : \n        cb ( astroid ) "}
{"3930": "\ndef authorize ( self , callback = None , state = None , ** kwargs ) : \n    params = dict ( self . request_token_params ) or { } \n    params . update ( ** kwargs ) \n    if self . request_token_url : \n        token = self . generate_request_token ( callback ) [ 0 ] \n        url = '%s?oauth_token=%s' % ( self . expand_url ( self . authorize_url ) , url_quote ( token ) ) \n        if params : \n            url = url + ( '&' + url_encode ( params ) ) \n    else : \n        assert callback is not None , 'Callback is required for OAuth2' \n        client = self . make_client ( ) \n        if 'scope' in params : \n            scope = params . pop ( 'scope' ) \n        else : \n            scope = None \n        if isinstance ( scope , str ) : \n            scope = _encode ( scope , self . encoding ) \n        if 'state' in params : \n            if not state : \n                state = params . pop ( 'state' ) \n            else : \n                params . pop ( 'state' ) \n        if callable ( state ) : \n            state = state ( ) \n        session [ '%s_oauthredir' % self . name ] = callback \n        url = client . prepare_request_uri ( self . expand_url ( self . authorize_url ) , redirect_uri = callback , scope = scope , state = state , ** params ) \n    return redirect ( url ) "}
{"3932": "\ndef handle_oauth2_response ( self , args ) : \n    client = self . make_client ( ) \n    remote_args = { 'code' : args . get ( 'code' ) , 'client_secret' : self . consumer_secret , 'redirect_uri' : session . get ( '%s_oauthredir' % self . name ) } \n    log . debug ( 'Prepare oauth2 remote args %r' , remote_args ) \n    remote_args . update ( self . access_token_params ) \n    headers = copy ( self . _access_token_headers ) \n    if self . access_token_method == 'POST' : \n        headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) \n        body = client . prepare_request_body ( ** remote_args ) \n        resp , content = self . http_request ( self . expand_url ( self . access_token_url ) , headers = headers , data = to_bytes ( body , self . encoding ) , method = self . access_token_method , ) \n    elif self . access_token_method == 'GET' : \n        qs = client . prepare_request_body ( ** remote_args ) \n        url = self . expand_url ( self . access_token_url ) \n        url = url + ( ( '?' in url and '&' or '?' ) + qs ) \n        resp , content = self . http_request ( url , headers = headers , method = self . access_token_method , ) \n    else : \n        raise OAuthException ( 'Unsupported access_token_method: %s' % self . access_token_method ) \n    data = parse_response ( resp , content , content_type = self . content_type ) \n    if resp . code not in ( 200 , 201 ) : \n        raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) \n    return data "}
{"4130": "\ndef _getter ( self , url , subkey = None ) : \n    kwargs = { } \n    if 'basic' in self . auth : \n        kwargs [ 'auth' ] = self . auth [ 'basic' ] \n    results = [ ] \n    link = dict ( next = url ) \n    while 'next' in link : \n        response = self . session . get ( link [ 'next' ] , ** kwargs ) \n        if response . status_code == 404 and 'token' in self . auth : \n            log . warn ( \"A '404' from github may indicate an auth \" \"failure. Make sure both that your token is correct \" \"and that it has 'public_repo' and not 'public \" \"access' rights.\" ) \n        json_res = self . json_response ( response ) \n        if subkey is not None : \n            json_res = json_res [ subkey ] \n        results = results + ( json_res ) \n        link = self . _link_field_to_dict ( response . headers . get ( 'link' , None ) ) \n    return results "}
{"4134": "\ndef aggregate_issues ( conf , main_section , debug ) : \n    log . info ( \"Starting to aggregate remote issues.\" ) \n    targets = aslist ( conf . get ( main_section , 'targets' ) ) \n    queue = multiprocessing . Queue ( ) \n    log . info ( \"Spawning %i workers.\" % len ( targets ) ) \n    processes = [ ] \n    if debug : \n        for target in targets : \n            _aggregate_issues ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) \n    else : \n        for target in targets : \n            proc = multiprocessing . Process ( target = _aggregate_issues , args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) ) \n            proc . start ( ) \n            processes . append ( proc ) \n            time . sleep ( 1 ) \n    currently_running = len ( targets ) \n    while currently_running > 0 : \n        issue = queue . get ( True ) \n        if isinstance ( issue , tuple ) : \n            completion_type , args = issue \n            if completion_type == SERVICE_FINISHED_ERROR : \n                target , e = args \n                log . info ( \"Terminating workers\" ) \n                for process in processes : \n                    process . terminate ( ) \n                raise RuntimeError ( \"critical error in target '{}'\" . format ( target ) ) \n            currently_running = currently_running - ( 1 ) \n            continue \n        yield issue \n    log . info ( \"Done aggregating remote issues.\" ) "}
{"4146": "\ndef merge_left ( field , local_task , remote_issue , hamming = False ) : \n    local_field = local_task . get ( field , [ ] ) \n    remote_field = remote_issue . get ( field , [ ] ) \n    if field not in local_task : \n        local_task [ field ] = [ ] \n    new_count = 0 \n    for remote in remote_field : \n        for local in local_field : \n            if ( ( hamming and get_annotation_hamming_distance ( remote , local ) == 0 ) or ( remote == local ) ) : \n                break \n        else : \n            log . debug ( \"%s not found in %r\" % ( remote , local_field ) ) \n            local_task [ field ] . append ( remote ) \n            new_count = new_count + ( 1 ) \n    if new_count > 0 : \n        log . debug ( 'Added %s new values to %s (total: %s)' % ( new_count , field , len ( local_task [ field ] ) , ) ) "}
{"4150": "\ndef multi_rouge_n ( sequences , scores_ids , n = 2 ) : \n    ngrams = [ _get_word_ngrams ( n , sequence ) for sequence in sequences ] \n    counts = [ len ( ngram ) for ngram in ngrams ] \n    scores = [ ] \n    for hyp_id , ref_id in scores_ids : \n        evaluated_ngrams = ngrams [ hyp_id ] \n        evaluated_count = counts [ hyp_id ] \n        reference_ngrams = ngrams [ ref_id ] \n        reference_count = counts [ ref_id ] \n        overlapping_ngrams = evaluated_ngrams . intersection ( reference_ngrams ) \n        overlapping_count = len ( overlapping_ngrams ) \n        scores = scores + ( [ f_r_p_rouge_n ( evaluated_count , reference_count , overlapping_count ) ] ) \n    return scores "}
{"4232": "\ndef begin ( script = 'TEMP3D_default.mlx' , file_in = None , mlp_in = None ) : \n    script_file = open ( script , 'w' ) \n    script_file . write ( '' . join ( [ '<!DOCTYPE FilterScript>\\n' , '<FilterScript>\\n' ] ) ) \n    script_file . close ( ) \n    current_layer = - 1 \n    last_layer = - 1 \n    stl = False \n    if mlp_in is not None : \n        if not isinstance ( mlp_in , list ) : \n            mlp_in = [ mlp_in ] \n        for val in mlp_in : \n            tree = ET . parse ( val ) \n            for elem in tree . iter ( tag = 'MLMesh' ) : \n                filename = ( elem . attrib [ 'filename' ] ) \n                current_layer = current_layer + ( 1 ) \n                last_layer = last_layer + ( 1 ) \n                if os . path . splitext ( filename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : \n                    layers . change ( script , current_layer ) \n                    clean . merge_vert ( script ) \n                    stl = True \n    if file_in is not None : \n        if not isinstance ( file_in , list ) : \n            file_in = [ file_in ] \n        for val in file_in : \n            current_layer = current_layer + ( 1 ) \n            last_layer = last_layer + ( 1 ) \n            if os . path . splitext ( val ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : \n                layers . change ( script , current_layer ) \n                clean . merge_vert ( script ) \n                stl = True \n    if stl : \n        layers . change ( script , last_layer ) \n    elif last_layer == - 1 : \n        file_in = [ 'TEMP3D.xyz' ] \n        file_in_descriptor = open ( file_in [ 0 ] , 'w' ) \n        file_in_descriptor . write ( '0 0 0' ) \n        file_in_descriptor . close ( ) \n        layers . delete ( script ) \n    return current_layer , last_layer "}
{"4278": "\ndef obj_overhead ( self ) : \n    overhead = [ self , self . _resulting_events , self . _events_list , self . _process ] \n    overhead_count = _get_object_count_by_type ( overhead ) \n    overhead_count [ dict ] = overhead_count [ dict ] + ( 2 ) \n    return overhead_count "}
{"4283": "\ndef run ( self ) : \n    existing_objects = _get_in_memory_objects ( ) \n    prof , result = self . profile ( ) \n    new_objects = _get_in_memory_objects ( ) \n    new_obj_count = _get_obj_count_difference ( new_objects , existing_objects ) \n    result_obj_count = new_obj_count - prof . obj_overhead \n    result_obj_count [ list ] = result_obj_count [ list ] - ( 1 ) \n    pretty_obj_count = _format_obj_count ( result_obj_count ) \n    return { 'objectName' : self . _object_name , 'codeEvents' : prof . code_events , 'totalEvents' : len ( prof . code_events ) , 'objectsCount' : pretty_obj_count , 'result' : result , 'timestamp' : int ( time . time ( ) ) } "}
{"4290": "\ndef _replace_sysargs ( self ) : \n    sys . argv [ : ] = [ self . _run_object ] \n    if self . _run_args : \n        sys . argv = sys . argv + ( self . _run_args . split ( ) ) "}
{"4291": "\ndef sample ( self , signum , frame ) : \n    stack = [ ] \n    while frame and frame != self . base_frame : \n        stack . append ( ( frame . f_code . co_name , frame . f_code . co_filename , frame . f_code . co_firstlineno ) ) \n        frame = frame . f_back \n    self . _stats [ tuple ( stack ) ] = self . _stats [ tuple ( stack ) ] + ( 1 ) \n    signal . setitimer ( signal . ITIMER_PROF , _SAMPLE_INTERVAL ) "}
{"4293": "\ndef _fill_sample_count ( self , node ) : \n    node [ 'sampleCount' ] = node [ 'sampleCount' ] + ( sum ( self . _fill_sample_count ( child ) for child in node [ 'children' ] ) ) \n    return node [ 'sampleCount' ] "}
{"4315": "\ndef lines_without_stdlib ( self ) : \n    prev_line = None \n    current_module_path = inspect . getabsfile ( inspect . currentframe ( ) ) \n    for module_path , lineno , runtime in self . lines : \n        module_abspath = os . path . abspath ( module_path ) \n        if not prev_line : \n            prev_line = [ module_abspath , lineno , runtime ] \n        else : \n            if ( not check_standard_dir ( module_path ) and module_abspath != current_module_path ) : \n                yield prev_line \n                prev_line = [ module_abspath , lineno , runtime ] \n            else : \n                prev_line [ 2 ] = prev_line [ 2 ] + ( runtime ) \n    yield prev_line "}
{"4316": "\ndef fill_heatmap ( self ) : \n    for module_path , lineno , runtime in self . lines_without_stdlib : \n        self . _execution_count [ module_path ] [ lineno ] = self . _execution_count [ module_path ] [ lineno ] + ( 1 ) \n        self . _heatmap [ module_path ] [ lineno ] = self . _heatmap [ module_path ] [ lineno ] + ( runtime ) "}
{"4317": "\ndef _skip_lines ( src_code , skip_map ) : \n    if not skip_map : \n        return [ [ 'line' , j + 1 , l ] for j , l in enumerate ( src_code ) ] \n    code_with_skips , i = [ ] , 0 \n    for line , length in skip_map : \n        code_with_skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src_code [ i : line ] ) ) \n        if ( code_with_skips and code_with_skips [ - 1 ] [ 0 ] == 'skip' ) : \n            code_with_skips [ - 1 ] [ 1 ] = code_with_skips [ - 1 ] [ 1 ] + ( length ) \n        else : \n            code_with_skips . append ( [ 'skip' , length ] ) \n        i = line + length \n    code_with_skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src_code [ i : ] ) ) \n    return code_with_skips "}
{"4321": "\ndef profile_function ( self ) : \n    with _CodeHeatmapCalculator ( ) as prof : \n        result = self . _run_object ( * self . _run_args , ** self . _run_kwargs ) \n    code_lines , start_line = inspect . getsourcelines ( self . _run_object ) \n    source_lines = [ ] \n    for line in code_lines : \n        source_lines . append ( ( 'line' , start_line , line ) ) \n        start_line = start_line + ( 1 ) \n    filename = os . path . abspath ( inspect . getsourcefile ( self . _run_object ) ) \n    heatmap = prof . heatmap [ filename ] \n    run_time = sum ( time for time in heatmap . values ( ) ) \n    return { 'objectName' : self . _object_name , 'runTime' : run_time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . _object_name , 'heatmap' : heatmap , 'executionCount' : prof . execution_count [ filename ] , 'srcCode' : source_lines , 'runTime' : run_time } ] } "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if self . verbose > 0 : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( 0 , n_fits , n_folds ) : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score = this_score * ( this_n_test_samples ) \n                n_test_samples = n_test_samples + ( this_n_test_samples ) \n            score = score + ( this_score ) \n        if self . iid : \n            score = score / ( float ( n_test_samples ) ) \n        else : \n            score = score / ( float ( n_folds ) ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4346": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    def mapper ( X ) : \n        X = check_array ( X , ( 'csr' , 'csc' ) , dtype = np . float64 ) \n        if hasattr ( X , \"toarray\" ) : \n            mean , var = mean_variance_axis ( X , axis = 0 ) \n        else : \n            mean , var = np . mean ( X , axis = 0 ) , np . var ( X , axis = 0 ) \n        return X . shape [ 0 ] , mean , var \n    def reducer ( a , b ) : \n        n_a , mean_a , var_a = a \n        n_b , mean_b , var_b = b \n        n_ab = n_a + n_b \n        mean_ab = ( ( mean_a * n_a ) + ( mean_b * n_b ) ) / n_ab \n        var_ab = ( ( ( n_a * var_a ) + ( n_b * var_b ) ) / n_ab ) + ( ( n_a * n_b ) * ( ( mean_b - mean_a ) / n_ab ) ** 2 ) \n        return ( n_ab , mean_ab , var_ab ) \n    _ , _ , self . variances_ = X . map ( mapper ) . treeReduce ( reducer ) \n    if np . all ( self . variances_ <= self . threshold ) : \n        msg = \"No feature in X meets the variance threshold {0:.5f}\" \n        if X . shape [ 0 ] == 1 : \n            msg = msg + ( \" (X contains only one sample)\" ) \n        raise ValueError ( msg . format ( self . threshold ) ) \n    return self "}
{"4349": "\ndef _block_collection ( iterator , dtype , bsize = - 1 ) : \n    i = 0 \n    accumulated = [ ] \n    for a in iterator : \n        if ( bsize > 0 ) and ( i >= bsize ) : \n            yield _pack_accumulated ( accumulated , dtype ) \n            accumulated = [ ] \n            i = 0 \n        accumulated . append ( a ) \n        i = i + ( 1 ) \n    if i > 0 : \n        yield _pack_accumulated ( accumulated , dtype ) "}
{"4350": "\ndef _block_tuple ( iterator , dtypes , bsize = - 1 ) : \n    i = 0 \n    blocked_tuple = None \n    for tuple_i in iterator : \n        if blocked_tuple is None : \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n        if ( bsize > 0 ) and ( i >= bsize ) : \n            yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n            i = 0 \n        for x_j , x in zip ( tuple_i , blocked_tuple ) : \n            x . append ( x_j ) \n        i = i + ( 1 ) \n    if i > 0 : \n        yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) "}
{"4366": "\ndef get_url ( self ) : \n    url = super ( ExecuteHomeAssistant , self ) . get_url ( ) \n    if not self . data . get ( 'event' ) : \n        raise InvalidConfig ( extra_body = 'Event option is required for HomeAsistant on {} device.' . format ( self . name ) ) \n    url = url + ( '/api/events/{}' . format ( self . data [ 'event' ] ) ) \n    return url "}
{"4398": "\ndef _maybeBind ( self ) : \n    if self . _ready or self . _selfIsReadonlyNode or time . time ( ) < self . _lastBindAttemptTime + self . _syncObj . conf . bindRetryTime : \n        return \n    self . _lastBindAttemptTime = time . time ( ) \n    try : \n        self . _server . bind ( ) \n    except Exception as e : \n        self . _bindAttempts = self . _bindAttempts + ( 1 ) \n        if self . _syncObj . conf . maxBindRetries and self . _bindAttempts >= self . _syncObj . conf . maxBindRetries : \n            self . _bindOverEvent . set ( ) \n            raise TransportNotReadyError \n    else : \n        self . _ready = True \n        self . _bindOverEvent . set ( ) "}
{"4400": "\ndef _onIncomingMessageReceived ( self , conn , message ) : \n    if self . _syncObj . encryptor and not conn . sendRandKey : \n        conn . sendRandKey = message \n        conn . recvRandKey = os . urandom ( 32 ) \n        conn . send ( conn . recvRandKey ) \n        return \n    if isinstance ( message , list ) : \n        done = False \n        try : \n            if message [ 0 ] == 'status' : \n                conn . send ( self . _syncObj . getStatus ( ) ) \n                done = True \n            elif message [ 0 ] == 'add' : \n                self . _syncObj . addNodeToCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'ADD' , arg = message [ 1 ] ) ) \n                done = True \n            elif message [ 0 ] == 'remove' : \n                if message [ 1 ] == self . _selfNode . address : \n                    conn . send ( 'FAIL REMOVE ' + message [ 1 ] ) \n                else : \n                    self . _syncObj . removeNodeFromCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'REMOVE' , arg = message [ 1 ] ) ) \n                done = True \n            elif message [ 0 ] == 'set_version' : \n                self . _syncObj . setCodeVersion ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'SET_VERSION' , arg = str ( message [ 1 ] ) ) ) \n                done = True \n        except Exception as e : \n            conn . send ( str ( e ) ) \n            done = True \n        if done : \n            return \n    node = self . _nodeAddrToNode [ message ] if message in self . _nodeAddrToNode else None \n    if node is None and message != 'readonly' : \n        conn . disconnect ( ) \n        self . _unknownConnections . discard ( conn ) \n        return \n    readonly = node is None \n    if readonly : \n        nodeId = str ( self . _readonlyNodesCounter ) \n        node = Node ( nodeId ) \n        self . _readonlyNodes . add ( node ) \n        self . _readonlyNodesCounter = self . _readonlyNodesCounter + ( 1 ) \n    self . _unknownConnections . discard ( conn ) \n    self . _connections [ node ] = conn \n    conn . setOnMessageReceivedCallback ( functools . partial ( self . _onMessageReceived , node ) ) \n    if not readonly : \n        self . _onNodeConnected ( node ) \n    else : \n        self . _onReadonlyNodeConnected ( node ) "}
{"4429": "\ndef save_output ( results , output_directory = \"output\" ) : \n    aggregate_reports = results [ \"aggregate_reports\" ] \n    forensic_reports = results [ \"forensic_reports\" ] \n    if os . path . exists ( output_directory ) : \n        if not os . path . isdir ( output_directory ) : \n            raise ValueError ( \"{0} is not a directory\" . format ( output_directory ) ) \n    else : \n        os . makedirs ( output_directory ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_json : \n        agg_json . write ( json . dumps ( aggregate_reports , ensure_ascii = False , indent = 2 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_csv : \n        csv = parsed_aggregate_reports_to_csv ( aggregate_reports ) \n        agg_csv . write ( csv ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_json : \n        for_json . write ( json . dumps ( forensic_reports , ensure_ascii = False , indent = 2 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_csv : \n        csv = parsed_forensic_reports_to_csv ( forensic_reports ) \n        for_csv . write ( csv ) \n    samples_directory = os . path . join ( output_directory , \"samples\" ) \n    if not os . path . exists ( samples_directory ) : \n        os . makedirs ( samples_directory ) \n    sample_filenames = [ ] \n    for forensic_report in forensic_reports : \n        sample = forensic_report [ \"sample\" ] \n        message_count = 0 \n        parsed_sample = forensic_report [ \"parsed_sample\" ] \n        subject = parsed_sample [ \"filename_safe_subject\" ] \n        filename = subject \n        while filename in sample_filenames : \n            message_count = message_count + ( 1 ) \n            filename = \"{0} ({1})\" . format ( subject , message_count ) \n        sample_filenames . append ( filename ) \n        filename = \"{0}.eml\" . format ( filename ) \n        path = os . path . join ( samples_directory , filename ) \n        with open ( path , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as sample_file : \n            sample_file . write ( sample ) "}
{"4431": "\ndef email_results ( results , host , mail_from , mail_to , port = 0 , ssl = False , user = None , password = None , subject = None , attachment_filename = None , message = None , ssl_context = None ) : \n    logging . debug ( \"Emailing report to: {0}\" . format ( \",\" . join ( mail_to ) ) ) \n    date_string = datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) \n    if attachment_filename : \n        if not attachment_filename . lower ( ) . endswith ( \".zip\" ) : \n            attachment_filename = attachment_filename + ( \".zip\" ) \n        filename = attachment_filename \n    else : \n        filename = \"DMARC-{0}.zip\" . format ( date_string ) \n    assert isinstance ( mail_to , list ) \n    msg = MIMEMultipart ( ) \n    msg [ 'From' ] = mail_from \n    msg [ 'To' ] = \", \" . join ( mail_to ) \n    msg [ 'Date' ] = email . utils . formatdate ( localtime = True ) \n    msg [ 'Subject' ] = subject or \"DMARC results for {0}\" . format ( date_string ) \n    text = message or \"Please see the attached zip file\\n\" \n    msg . attach ( MIMEText ( text ) ) \n    zip_bytes = get_report_zip ( results ) \n    part = MIMEApplication ( zip_bytes , Name = filename ) \n    part [ 'Content-Disposition' ] = 'attachment; filename=\"{0}\"' . format ( filename ) \n    msg . attach ( part ) \n    try : \n        if ssl_context is None : \n            ssl_context = create_default_context ( ) \n        if ssl : \n            server = smtplib . SMTP_SSL ( host , port = port , context = ssl_context ) \n            server . connect ( host , port ) \n            server . ehlo_or_helo_if_needed ( ) \n        else : \n            server = smtplib . SMTP ( host , port = port ) \n            server . connect ( host , port ) \n            server . ehlo_or_helo_if_needed ( ) \n            if server . has_extn ( \"starttls\" ) : \n                server . starttls ( context = ssl_context ) \n                server . ehlo ( ) \n            else : \n                logger . warning ( \"SMTP server does not support STARTTLS. \" \"Proceeding in plain text!\" ) \n        if user and password : \n            server . login ( user , password ) \n        server . sendmail ( mail_from , mail_to , msg . as_string ( ) ) \n    except smtplib . SMTPException as error : \n        error = error . __str__ ( ) . lstrip ( \"b'\" ) . rstrip ( \"'\" ) . rstrip ( \".\" ) \n        raise SMTPError ( error ) \n    except socket . gaierror : \n        raise SMTPError ( \"DNS resolution failed\" ) \n    except ConnectionRefusedError : \n        raise SMTPError ( \"Connection refused\" ) \n    except ConnectionResetError : \n        raise SMTPError ( \"Connection reset\" ) \n    except ConnectionAbortedError : \n        raise SMTPError ( \"Connection aborted\" ) \n    except TimeoutError : \n        raise SMTPError ( \"Connection timed out\" ) \n    except SSLError as error : \n        raise SMTPError ( \"SSL error: {0}\" . format ( error . __str__ ( ) ) ) \n    except CertificateError as error : \n        raise SMTPError ( \"Certificate error: {0}\" . format ( error . __str__ ( ) ) ) "}
{"4432": "\ndef save_aggregate_reports_to_splunk ( self , aggregate_reports ) : \n    logger . debug ( \"Saving aggregate reports to Splunk\" ) \n    if type ( aggregate_reports ) == dict : \n        aggregate_reports = [ aggregate_reports ] \n    if len ( aggregate_reports ) < 1 : \n        return \n    data = self . _common_data . copy ( ) \n    json_str = \"\" \n    for report in aggregate_reports : \n        for record in report [ \"records\" ] : \n            new_report = dict ( ) \n            for metadata in report [ \"report_metadata\" ] : \n                new_report [ metadata ] = report [ \"report_metadata\" ] [ metadata ] \n            new_report [ \"published_policy\" ] = report [ \"policy_published\" ] \n            new_report [ \"source_ip_address\" ] = record [ \"source\" ] [ \"ip_address\" ] \n            new_report [ \"source_country\" ] = record [ \"source\" ] [ \"country\" ] \n            new_report [ \"source_reverse_dns\" ] = record [ \"source\" ] [ \"reverse_dns\" ] \n            new_report [ \"source_base_domain\" ] = record [ \"source\" ] [ \"base_domain\" ] \n            new_report [ \"message_count\" ] = record [ \"count\" ] \n            new_report [ \"disposition\" ] = record [ \"policy_evaluated\" ] [ \"disposition\" ] \n            new_report [ \"spf_aligned\" ] = record [ \"alignment\" ] [ \"spf\" ] \n            new_report [ \"dkim_aligned\" ] = record [ \"alignment\" ] [ \"dkim\" ] \n            new_report [ \"passed_dmarc\" ] = record [ \"alignment\" ] [ \"dmarc\" ] \n            new_report [ \"header_from\" ] = record [ \"identifiers\" ] [ \"header_from\" ] \n            new_report [ \"envelope_from\" ] = record [ \"identifiers\" ] [ \"envelope_from\" ] \n            if \"dkim\" in record [ \"auth_results\" ] : \n                new_report [ \"dkim_results\" ] = record [ \"auth_results\" ] [ \"dkim\" ] \n            if \"spf\" in record [ \"auth_results\" ] : \n                new_report [ \"spf_results\" ] = record [ \"auth_results\" ] [ \"spf\" ] \n            data [ \"sourcetype\" ] = \"dmarc:aggregate\" \n            timestamp = human_timestamp_to_timestamp ( new_report [ \"begin_date\" ] ) \n            data [ \"time\" ] = timestamp \n            data [ \"event\" ] = new_report . copy ( ) \n            json_str = json_str + ( \"{0}\\n\" . format ( json . dumps ( data ) ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if response [ \"code\" ] != 0 : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4433": "\ndef save_forensic_reports_to_splunk ( self , forensic_reports ) : \n    logger . debug ( \"Saving forensic reports to Splunk\" ) \n    if type ( forensic_reports ) == dict : \n        forensic_reports = [ forensic_reports ] \n    if len ( forensic_reports ) < 1 : \n        return \n    json_str = \"\" \n    for report in forensic_reports : \n        data = self . _common_data . copy ( ) \n        data [ \"sourcetype\" ] = \"dmarc:forensic\" \n        timestamp = human_timestamp_to_timestamp ( report [ \"arrival_date_utc\" ] ) \n        data [ \"time\" ] = timestamp \n        data [ \"event\" ] = report . copy ( ) \n        json_str = json_str + ( \"{0}\\n\" . format ( json . dumps ( data ) ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if response [ \"code\" ] != 0 : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4434": "\ndef decode_base64 ( data ) : \n    data = bytes ( data , encoding = \"ascii\" ) \n    missing_padding = len ( data ) % 4 \n    if missing_padding != 0 : \n        data = data + ( b'=' * ( 4 - missing_padding ) ) \n    return base64 . b64decode ( data ) "}
{"4442": "\ndef cli_parse ( file_path , sa , nameservers , dns_timeout , parallel = False ) : \n    try : \n        file_results = parse_report_file ( file_path , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) \n    except ParserError as error : \n        return error , file_path \n    finally : \n        global counter \n        with counter . get_lock ( ) : \n            counter . value = counter . value + ( 1 ) \n    return file_results , file_path "}
{"4446": "\ndef _publish ( self , subject , reply , payload , payload_size ) : \n    if subject == \"\" : \n        raise ErrBadSubject \n    payload_size_bytes = ( \"%d\" % payload_size ) . encode ( ) \n    pub_cmd = b'' . join ( [ PUB_OP , _SPC_ , subject . encode ( ) , _SPC_ , reply , _SPC_ , payload_size_bytes , _CRLF_ , payload , _CRLF_ ] ) \n    self . stats [ 'out_msgs' ] = self . stats [ 'out_msgs' ] + ( 1 ) \n    self . stats [ 'out_bytes' ] = self . stats [ 'out_bytes' ] + ( payload_size ) \n    yield from self . _send_command ( pub_cmd ) \n    if self . _flush_queue . empty ( ) : \n        yield from self . _flush_pending ( ) "}
{"4450": "\ndef _select_next_server ( self ) : \n    while True : \n        if len ( self . _server_pool ) == 0 : \n            self . _current_server = None \n            raise ErrNoServers \n        now = time . monotonic ( ) \n        s = self . _server_pool . pop ( 0 ) \n        if self . options [ \"max_reconnect_attempts\" ] > 0 : \n            if s . reconnects > self . options [ \"max_reconnect_attempts\" ] : \n                continue \n        self . _server_pool . append ( s ) \n        if s . last_attempt is not None and now < s . last_attempt + self . options [ \"reconnect_time_wait\" ] : \n            yield from asyncio . sleep ( self . options [ \"reconnect_time_wait\" ] , loop = self . _loop ) \n        try : \n            s . last_attempt = time . monotonic ( ) \n            r , w = yield from asyncio . open_connection ( s . uri . hostname , s . uri . port , loop = self . _loop , limit = DEFAULT_BUFFER_SIZE ) \n            self . _current_server = s \n            self . _bare_io_reader = self . _io_reader = r \n            self . _bare_io_writer = self . _io_writer = w \n            break \n        except Exception as e : \n            s . last_attempt = time . monotonic ( ) \n            s . reconnects = s . reconnects + ( 1 ) \n            self . _err = e \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            continue "}
{"4454": "\ndef _process_pong ( self ) : \n    if len ( self . _pongs ) > 0 : \n        future = self . _pongs . pop ( 0 ) \n        future . set_result ( True ) \n        self . _pongs_received = self . _pongs_received + ( 1 ) \n        self . _pings_outstanding = self . _pings_outstanding - ( 1 ) "}
{"4455": "\ndef _process_msg ( self , sid , subject , reply , data ) : \n    payload_size = len ( data ) \n    self . stats [ 'in_msgs' ] = self . stats [ 'in_msgs' ] + ( 1 ) \n    self . stats [ 'in_bytes' ] = self . stats [ 'in_bytes' ] + ( payload_size ) \n    sub = self . _subs . get ( sid ) \n    if sub is None : \n        return \n    sub . received = sub . received + ( 1 ) \n    if sub . max_msgs > 0 and sub . received >= sub . max_msgs : \n        self . _subs . pop ( sid , None ) \n    msg = self . _build_message ( subject , reply , data ) \n    if sub . future is not None : \n        if sub . future . cancelled ( ) : \n            return \n        sub . future . set_result ( msg ) \n        return \n    try : \n        sub . pending_size = sub . pending_size + ( payload_size ) \n        if sub . pending_size >= sub . pending_bytes_limit : \n            sub . pending_size = sub . pending_size - ( payload_size ) \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) \n            return \n        sub . pending_queue . put_nowait ( msg ) \n    except asyncio . QueueFull : \n        if self . _error_cb is not None : \n            yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) "}
{"4477": "\ndef _get_top_words ( model , feature_names , n_top_words = 40 ) : \n    topic_words = [ ] \n    for topic in model . components_ : \n        top_words = [ feature_names [ i ] for i in topic . argsort ( ) [ : - n_top_words - 1 : - 1 ] ] \n        topic_words = topic_words + ( [ top_words ] ) \n    return topic_words "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active = prop_mask_active / ( num_vox ) \n        indices = np . where ( prop_mask_active > activation_threshold ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( dists > 5.5 ) & ( dists < 6.5 ) ) [ 0 ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ dists <= r ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    elif return_type == 'data' : \n        return self . get_image_data ( ids ) "}
{"4529": "\nasync def get_all_albums ( self , * , market = 'US' ) -> List [ Album ] : \n    from . album import Album \n    albums = [ ] \n    offset = 0 \n    total = await self . total_albums ( market = market ) \n    while len ( albums ) < total : \n        data = await self . __client . http . artist_albums ( self . id , limit = 50 , offset = offset , market = market ) \n        offset = offset + ( 50 ) \n        albums = albums + ( list ( Album ( self . __client , item ) for item in data [ 'items' ] ) ) \n    return albums "}
{"4541": "\nasync def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : \n    tracks = [ ] \n    offset = 0 \n    total = self . total_tracks or None \n    while True : \n        data = await self . __client . http . album_tracks ( self . id , limit = 50 , offset = offset , market = market ) \n        if total is None : \n            total = data [ 'total' ] \n        offset = offset + ( 50 ) \n        tracks = tracks + ( list ( Track ( self . __client , item ) for item in data [ 'items' ] ) ) \n        if len ( tracks ) >= total : \n            break \n    return tracks "}
{"4557": "\nasync def get_all_tracks ( self ) -> List [ PlaylistTrack ] : \n    if isinstance ( self . _tracks , PartialTracks ) : \n        return await self . _tracks . build ( ) \n    _tracks = [ ] \n    offset = 0 \n    while len ( self . tracks ) < self . total_tracks : \n        data = await self . __client . http . get_playlist_tracks ( self . owner . id , self . id , limit = 50 , offset = offset ) \n        _tracks = _tracks + ( [ PlaylistTrack ( self . __client , item ) for item in data [ 'items' ] ] ) \n        offset = offset + ( 50 ) \n    self . total_tracks = len ( self . _tracks ) \n    return list ( self . _tracks ) "}
{"4589": "\ndef _format_domain ( cls , extracted_domain ) : \n    if not extracted_domain . startswith ( \"#\" ) : \n        if \"#\" in extracted_domain : \n            extracted_domain = extracted_domain [ : extracted_domain . find ( \"#\" ) ] . strip ( ) \n        if \" \" in extracted_domain or \"\\t\" in extracted_domain : \n            splited_line = extracted_domain . split ( ) \n            index = 1 \n            while index < len ( splited_line ) : \n                if splited_line [ index ] : \n                    break \n                index = index + ( 1 ) \n            return splited_line [ index ] \n        return extracted_domain \n    return \"\" "}
{"4597": "\ndef _create_directory ( cls , directory , loop = False ) : \n    if not loop and PyFunceble . directory_separator in directory : \n        splited_directory = directory . split ( PyFunceble . directory_separator ) \n        full_path_to_create = \"\" \n        for single_directory in splited_directory : \n            full_path_to_create = full_path_to_create + ( single_directory + PyFunceble . directory_separator ) \n            cls . _create_directory ( full_path_to_create , True ) \n    if not PyFunceble . path . isdir ( directory ) : \n        AutoSave . travis_permissions ( ) \n        PyFunceble . mkdir ( directory ) \n        AutoSave . travis_permissions ( ) "}
{"4598": "\ndef delete_uneeded ( self ) : \n    structure = self . _get_structure ( ) \n    list_of_key = list ( structure . keys ( ) ) \n    structure = structure [ list_of_key [ 0 ] ] \n    parent_path = list_of_key [ 0 ] \n    if not parent_path . endswith ( PyFunceble . directory_separator ) : \n        parent_path = parent_path + ( PyFunceble . directory_separator ) \n    for root , _ , _ in PyFunceble . walk ( parent_path ) : \n        root = Directory ( root ) . fix_path ( ) \n        if root . replace ( parent_path , \"\" ) not in structure : \n            PyFunceble . rmtree ( root ) "}
{"4599": "\ndef _set_path_to_configs ( cls , path_to_config ) : \n    if not path_to_config . endswith ( PyFunceble . directory_separator ) : \n        default = parsed = path_to_config + PyFunceble . directory_separator \n    else : \n        default = parsed = path_to_config \n    parsed = parsed + ( PyFunceble . CONFIGURATION_FILENAME ) \n    default = default + ( PyFunceble . DEFAULT_CONFIGURATION_FILENAME ) \n    return ( parsed , default ) "}
{"4611": "\ndef _analytic_host_file_directory ( self ) : \n    output_dir = ( self . output_parent_dir + PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"parent\" ] ) \n    if self . domain_status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"potentially_up\" ] : \n        output_dir = output_dir + ( PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"potentially_up\" ] ) \n    elif ( self . domain_status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"potentially_down\" ] ) : \n        output_dir = output_dir + ( PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"potentially_down\" ] ) \n    elif self . domain_status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"suspicious\" ] : \n        output_dir = output_dir + ( PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"suspicious\" ] ) \n    else : \n        output_dir = output_dir + ( PyFunceble . OUTPUTS [ \"analytic\" ] [ \"directories\" ] [ \"up\" ] ) \n    return output_dir "}
{"4618": "\ndef hierarchical ( cls , element ) : \n    to_sort = \"\" \n    full_extension = \"\" \n    element = element . lower ( ) \n    url_base = Check ( ) . is_url_valid ( element , return_base = True ) \n    if not isinstance ( url_base , str ) : \n        if \".\" in element : \n            extension_index = element . rindex ( \".\" ) + 1 \n            extension = element [ extension_index : ] \n            if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n                for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                    formatted_suffix = \".\" + suffix \n                    if element . endswith ( formatted_suffix ) : \n                        suffix_index = element . rindex ( formatted_suffix ) \n                        to_sort = element [ : suffix_index ] \n                        full_extension = suffix \n                        break \n            if not full_extension : \n                full_extension = element [ extension_index : ] \n                to_sort = element [ : extension_index - 1 ] \n            full_extension = full_extension + ( \".\" ) \n            tros_ot = to_sort [ : : - 1 ] \n            if \".\" in tros_ot : \n                full_extension = ( tros_ot [ : tros_ot . index ( \".\" ) ] [ : : - 1 ] + \".\" + full_extension ) \n                tros_ot = tros_ot [ tros_ot . index ( \".\" ) + 1 : ] \n                reversion = full_extension + \".\" . join ( [ x [ : : - 1 ] for x in tros_ot . split ( \".\" ) ] ) \n                return ( Regex ( reversion , cls . regex_replace , replace_with = \"@funilrys\" ) . replace ( ) . replace ( \"@funilrys\" , \"\" ) ) \n            return ( Regex ( to_sort + full_extension , cls . regex_replace , replace_with = \"@funilrys\" , ) . replace ( ) . replace ( \"@funilrys\" , \"\" ) ) \n        return element \n    protocol_position = element . rindex ( url_base ) \n    protocol = element [ : protocol_position ] \n    return protocol + cls . hierarchical ( url_base ) "}
{"4632": "\ndef whois ( self , record ) : \n    if PyFunceble . CONFIGURATION [ \"debug\" ] and PyFunceble . CONFIGURATION [ \"logs\" ] : \n        if PyFunceble . INTERN [ \"referer\" ] : \n            referer = PyFunceble . INTERN [ \"referer\" ] \n        else : \n            referer = None \n        to_write = { self . current_time : { \"domain\" : PyFunceble . INTERN [ \"to_test\" ] , \"record\" : record , \"referer\" : referer , } } \n        if self . output : \n            output = self . output \n        else : \n            output = PyFunceble . OUTPUT_DIRECTORY \n            output = output + ( PyFunceble . OUTPUTS [ \"parent_directory\" ] ) \n            output = output + ( PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] ) \n            output = output + ( PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"whois\" ] ) \n        current_content = self . _get_content ( output ) \n        current_content . update ( to_write ) \n        self . _write_content ( current_content , output ) "}
{"4633": "\ndef expiration_date ( self , extracted ) : \n    if PyFunceble . CONFIGURATION [ \"logs\" ] : \n        if PyFunceble . INTERN [ \"referer\" ] : \n            referer = PyFunceble . INTERN [ \"referer\" ] \n        else : \n            referer = None \n        to_write = { self . current_time : { \"domain\" : PyFunceble . INTERN [ \"to_test\" ] , \"expiration_date\" : extracted , \"whois_server\" : referer , } } \n        if self . output : \n            output = self . output \n        else : \n            output = PyFunceble . OUTPUT_DIRECTORY \n            output = output + ( PyFunceble . OUTPUTS [ \"parent_directory\" ] ) \n            output = output + ( PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] ) \n            output = output + ( PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"date_format\" ] ) \n        current_content = self . _get_content ( output ) \n        current_content . update ( to_write ) \n        self . _write_content ( current_content , output ) \n        if PyFunceble . CONFIGURATION [ \"share_logs\" ] : \n            PyFunceble . requests . post ( PyFunceble . LINKS [ \"api_date_format\" ] , data = to_write [ self . current_time ] , ) "}
{"4634": "\ndef referer_not_found ( self , extension ) : \n    if PyFunceble . CONFIGURATION [ \"logs\" ] : \n        to_write = { self . current_time : { \"domain\" : PyFunceble . INTERN [ \"to_test\" ] , \"extension\" : extension , } } \n        if self . output : \n            output = self . output \n        else : \n            output = PyFunceble . OUTPUT_DIRECTORY \n            output = output + ( PyFunceble . OUTPUTS [ \"parent_directory\" ] ) \n            output = output + ( PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] ) \n            output = output + ( PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"no_referer\" ] ) \n        current_content = self . _get_content ( output ) \n        current_content . update ( to_write ) \n        self . _write_content ( current_content , output ) \n        if PyFunceble . CONFIGURATION [ \"share_logs\" ] : \n            PyFunceble . requests . post ( PyFunceble . LINKS [ \"api_no_referer\" ] , data = to_write [ self . current_time ] ) "}
{"4636": "\ndef _header_constructor ( cls , data_to_print , header_separator = \"-\" , column_separator = \" \" ) : \n    header_data = [ ] \n    header_size = \"\" \n    before_size = \"%-\" \n    after_size = \"s\" \n    if header_separator : \n        header_separator_data = [ ] \n    length_data_to_print = len ( data_to_print ) - 1 \n    i = 0 \n    for data in data_to_print : \n        size = data_to_print [ data ] \n        header_data . append ( data ) \n        header_size = header_size + ( before_size + str ( size ) + after_size ) \n        if i < length_data_to_print : \n            header_size = header_size + ( column_separator ) \n        if header_separator : \n            header_separator_data . append ( header_separator * size ) \n        i = i + ( 1 ) \n    if header_separator : \n        return [ header_size % tuple ( header_data ) , header_size % tuple ( header_separator_data ) , ] \n    return [ header_size % tuple ( header_data ) ] "}
{"4646": "\ndef file_to_delete ( cls ) : \n    directory = PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] \n    if not directory . endswith ( PyFunceble . directory_separator ) : \n        directory = directory + ( PyFunceble . directory_separator ) \n    result = [ ] \n    for root , _ , files in PyFunceble . walk ( directory ) : \n        for file in files : \n            if file not in [ \".gitignore\" , \".keep\" ] : \n                if root . endswith ( PyFunceble . directory_separator ) : \n                    result . append ( root + file ) \n                else : \n                    result . append ( root + PyFunceble . directory_separator + file ) \n    return result "}
{"4666": "\ndef count ( self ) : \n    if self . status : \n        PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] = PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] + ( 1 ) \n        if ( self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] = PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] + ( 1 ) \n        elif self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] = PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] + ( 1 ) \n        else : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] = PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] + ( 1 ) "}
{"4688": "\ndef whois ( cls , whois_server , domain = None , timeout = None ) : \n    if domain is None : \n        domain = PyFunceble . INTERN [ \"to_test\" ] \n    if timeout is None : \n        timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] \n    if whois_server : \n        req = PyFunceble . socket . socket ( PyFunceble . socket . AF_INET , PyFunceble . socket . SOCK_STREAM ) \n        if timeout % 3 == 0 : \n            req . settimeout ( timeout ) \n        else : \n            req . settimeout ( 3 ) \n        try : \n            req . connect ( ( whois_server , 43 ) ) \n        except PyFunceble . socket . error : \n            return None \n        req . send ( ( domain + \"\\r\\n\" ) . encode ( ) ) \n        response = b\"\" \n        while True : \n            try : \n                data = req . recv ( 4096 ) \n            except ( PyFunceble . socket . timeout , ConnectionResetError ) : \n                req . close ( ) \n                return None \n            response = response + ( data ) \n            if not data : \n                break \n        req . close ( ) \n        try : \n            return response . decode ( ) \n        except UnicodeDecodeError : \n            return response . decode ( \"utf-8\" , \"replace\" ) \n    return None "}
{"4705": "\ndef filter_from_import ( line , unused_module ) : \n    ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , string = line , maxsplit = 1 ) \n    base_module = re . search ( pattern = r'\\bfrom\\s+([^ ]+)' , string = indentation ) . group ( 1 ) \n    imports = re . split ( pattern = r',' , string = imports . strip ( ) ) \n    imports = [ base_module + '.' + x . strip ( ) for x in imports ] \n    filtered_imports = [ x . replace ( base_module + '.' , '' ) for x in imports if x not in unused_module ] \n    if not filtered_imports : \n        return get_indentation ( line ) + 'pass' + get_line_ending ( line ) \n    indentation = indentation + ( 'import ' ) \n    return ( indentation + ', ' . join ( sorted ( filtered_imports ) ) + get_line_ending ( line ) ) "}
{"4706": "\ndef break_up_import ( line ) : \n    assert '\\\\' not in line \n    assert '(' not in line \n    assert ')' not in line \n    assert ';' not in line \n    assert '#' not in line \n    assert not line . lstrip ( ) . startswith ( 'from' ) \n    newline = get_line_ending ( line ) \n    if not newline : \n        return line \n    ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , string = line , maxsplit = 1 ) \n    indentation = indentation + ( 'import ' ) \n    assert newline \n    return '' . join ( [ indentation + i . strip ( ) + newline for i in sorted ( imports . split ( ',' ) ) ] ) "}
{"4721": "\ndef find_files ( filenames , recursive , exclude ) : \n    while filenames : \n        name = filenames . pop ( 0 ) \n        if recursive and os . path . isdir ( name ) : \n            for root , directories , children in os . walk ( name ) : \n                filenames = filenames + ( [ os . path . join ( root , f ) for f in children if match_file ( os . path . join ( root , f ) , exclude ) ] ) \n                directories [ : ] = [ d for d in directories if match_file ( os . path . join ( root , d ) , exclude ) ] \n        else : \n            if not is_exclude_file ( name , exclude ) : \n                yield name "}
{"4760": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( BigInteger , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length % 8 : \n        raise exceptions . InvalidPrimitiveLength ( \"invalid big integer length read; \" \"expected: multiple of 8, observed: {0}\" . format ( self . length ) ) \n    sign = 1 \n    binary = '' \n    for _ in range ( self . length ) : \n        byte = struct . unpack ( '!B' , istream . read ( 1 ) ) [ 0 ] \n        bits = \"{0:b}\" . format ( byte ) \n        pad = len ( bits ) % 8 \n        if pad : \n            bits = ( '0' * ( 8 - pad ) ) + bits \n        binary = binary + ( bits ) \n    if binary [ 0 ] == '1' : \n        sign = - 1 \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    self . value = int ( binary , 2 ) * sign "}
{"4761": "\ndef write ( self , ostream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    binary = \"{0:b}\" . format ( abs ( self . value ) ) \n    binary = ( \"0\" * ( 64 - ( len ( binary ) % 64 ) ) ) + binary \n    if self . value < 0 : \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    hexadecimal = b'' \n    for i in range ( 0 , len ( binary ) , 8 ) : \n        byte = binary [ i : i + 8 ] \n        byte = int ( byte , 2 ) \n        hexadecimal = hexadecimal + ( struct . pack ( '!B' , byte ) ) \n    self . length = len ( hexadecimal ) \n    super ( BigInteger , self ) . write ( ostream , kmip_version = kmip_version ) \n    ostream . write ( hexadecimal ) "}
{"4959": "\ndef find_globals ( code ) : \n    cur_byte = 0 \n    byte_code = code . co_code \n    names = set ( ) \n    while cur_byte < len ( byte_code ) : \n        op = ord ( byte_code [ cur_byte ] ) \n        if op >= dis . HAVE_ARGUMENT : \n            if op == _LOAD_GLOBAL : \n                oparg = ord ( byte_code [ cur_byte + 1 ] ) + ( ord ( byte_code [ cur_byte + 2 ] ) << 8 ) \n                name = code . co_names [ oparg ] \n                names . add ( name ) \n            cur_byte = cur_byte + ( 2 ) \n        cur_byte = cur_byte + ( 1 ) \n    return names "}
{"5002": "\ndef redirects_handler ( * args , ** kwargs ) : \n    path = args [ 0 ] . path \n    shift = '../' \n    if 'delete' in path : \n        shift = shift + ( '../' ) \n    elif 'history' in path : \n        if 'item_id' not in kwargs : \n            shift = shift + ( '../' ) \n    return HttpResponseRedirect ( path + shift ) "}
{"5010": "\ndef get_urls ( self ) : \n    urls = super ( TreeAdmin , self ) . get_urls ( ) \n    prefix_change = 'change/' if DJANGO_POST_19 else '' \n    sitetree_urls = [ url ( r'^change/$' , redirects_handler , name = get_tree_item_url_name ( 'changelist' ) ) , url ( r'^((?P<tree_id>\\d+)/)?%sitem_add/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_add ) , name = get_tree_item_url_name ( 'add' ) ) , url ( r'^(?P<tree_id>\\d+)/%sitem_(?P<item_id>\\d+)/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_edit ) , name = get_tree_item_url_name ( 'change' ) ) , url ( r'^%sitem_(?P<item_id>\\d+)/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_edit ) , name = get_tree_item_url_name ( 'change' ) ) , url ( r'^((?P<tree_id>\\d+)/)?%sitem_(?P<item_id>\\d+)/delete/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_delete ) , name = get_tree_item_url_name ( 'delete' ) ) , url ( r'^((?P<tree_id>\\d+)/)?%sitem_(?P<item_id>\\d+)/history/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_history ) , name = get_tree_item_url_name ( 'history' ) ) , url ( r'^(?P<tree_id>\\d+)/%sitem_(?P<item_id>\\d+)/move_(?P<direction>(up|down))/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_move ) , name = get_tree_item_url_name ( 'move' ) ) , ] \n    if not DJANGO_POST_19 : \n        sitetree_urls = patterns_func ( '' , * sitetree_urls ) \n    if SMUGGLER_INSTALLED : \n        sitetree_urls = sitetree_urls + ( ( url ( r'^dump_all/$' , self . admin_site . admin_view ( self . dump_view ) , name = 'sitetree_dump' ) , ) ) \n    return sitetree_urls + urls "}
{"5064": "\ndef parse ( self , data ) : \n    data = '\\n' . join ( self . strip ( data . split ( '\\n' ) ) ) \n    tag_re = re . compile ( r'^:\\n?(?P<full_tag>(?P<tag>[0-9]{2}|NS)(?P<sub_tag>[A-Z])?):' , re . MULTILINE ) \n    matches = list ( tag_re . finditer ( data ) ) \n    valid_matches = list ( self . sanatize_tag_id_matches ( matches ) ) \n    for i , match in enumerate ( valid_matches ) : \n        tag_id = self . normalize_tag_id ( match . group ( 'tag' ) ) \n        tag = self . tags . get ( match . group ( 'full_tag' ) ) or self . tags [ tag_id ] \n        if valid_matches [ i + 1 : ] : \n            tag_data = data [ match . end ( ) : valid_matches [ i + 1 ] . start ( ) ] . strip ( ) \n        else : \n            tag_data = data [ match . end ( ) : ] . strip ( ) \n        tag_dict = tag . parse ( self , tag_data ) \n        for processor in self . processors . get ( 'pre_%s' % tag . slug , [ ] ) : \n            tag_dict = processor ( self , tag , tag_dict ) \n        result = tag ( self , tag_dict ) \n        for processor in self . processors . get ( 'post_%s' % tag . slug , [ ] ) : \n            result = processor ( self , tag , tag_dict , result ) \n        if isinstance ( tag , mt940 . tags . Statement ) : \n            if not self . transactions : \n                transaction = Transaction ( self ) \n                self . transactions . append ( transaction ) \n            if transaction . data . get ( 'id' ) : \n                transaction = Transaction ( self , result ) \n                self . transactions . append ( transaction ) \n            else : \n                transaction . data . update ( result ) \n        elif issubclass ( tag . scope , Transaction ) and self . transactions : \n            for k , v in _compat . iteritems ( result ) : \n                if k in transaction . data and hasattr ( v , 'strip' ) : \n                    transaction . data [ k ] = transaction . data [ k ] + ( '\\n%s' % v . strip ( ) ) \n                else : \n                    transaction . data [ k ] = v \n        elif issubclass ( tag . scope , Transactions ) : \n            self . data . update ( result ) \n    return self . transactions "}
{"5072": "\ndef read ( self ) : \n    packet = self . packet \n    with self . __read_lock : \n        buffer = self . __buffer \n        while len ( buffer ) < packet : \n            buffer = buffer + ( self . _read_data ( ) ) \n        length = self . __unpack ( buffer [ : packet ] ) [ 0 ] + packet \n        while len ( buffer ) < length : \n            buffer = buffer + ( self . _read_data ( ) ) \n        term , self . __buffer = decode ( buffer [ packet : ] ) \n    return term "}
{"5102": "\ndef absorb ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % HASH_LENGTH ) or HASH_LENGTH ) \n    trits = trits + ( [ 0 ] * ( HASH_LENGTH - pad ) ) \n    if length is None : \n        length = len ( trits ) \n    if length < 1 : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while offset < length : \n        start = offset \n        stop = min ( start + HASH_LENGTH , length ) \n        self . _state [ 0 : stop - start ] = trits [ start : stop ] \n        self . _transform ( ) \n        offset = offset + ( HASH_LENGTH ) "}
{"5103": "\ndef squeeze ( self , trits , offset = 0 , length = HASH_LENGTH ) : \n    if length % HASH_LENGTH != 0 : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze`.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } ) \n    trits . extend ( [ 0 ] * max ( 0 , length - len ( trits ) ) ) \n    if len ( trits ) - offset < HASH_LENGTH : \n        raise with_context ( exc = ValueError ( 'Invalid offset passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length } , ) \n    while length >= HASH_LENGTH : \n        trits [ offset : offset + HASH_LENGTH ] = self . _state [ 0 : HASH_LENGTH ] \n        self . _transform ( ) \n        offset = offset + ( HASH_LENGTH ) \n        length = length - ( HASH_LENGTH ) "}
{"5104": "\ndef _transform ( self ) : \n    state_length = STATE_LENGTH \n    truth_table = TRUTH_TABLE \n    prev_state = self . _state [ : ] \n    new_state = prev_state [ : ] \n    index = 0 \n    for _ in range ( NUMBER_OF_ROUNDS ) : \n        prev_trit = prev_state [ index ] \n        for pos in range ( state_length ) : \n            index = index + ( ( 364 if index < 365 else - 365 ) ) \n            new_trit = prev_state [ index ] \n            new_state [ pos ] = truth_table [ prev_trit + ( 3 * new_trit ) + 4 ] \n            prev_trit = new_trit \n        prev_state = new_state \n        new_state = new_state [ : ] \n    self . _state = new_state "}
{"5108": "\ndef add_trits ( left , right ) : \n    target_len = max ( len ( left ) , len ( right ) ) \n    res = [ 0 ] * target_len \n    left = left + ( [ 0 ] * ( target_len - len ( left ) ) ) \n    right = right + ( [ 0 ] * ( target_len - len ( right ) ) ) \n    carry = 0 \n    for i in range ( len ( res ) ) : \n        res [ i ] , carry = _full_add_trits ( left [ i ] , right [ i ] , carry ) \n    return res "}
{"5109": "\ndef trits_from_int ( n , pad = 1 ) : \n    if n == 0 : \n        trits = [ ] \n    else : \n        quotient , remainder = divmod ( n , 3 ) \n        if remainder == 2 : \n            quotient = quotient + ( 1 ) \n            remainder = - 1 \n        trits = [ remainder ] + trits_from_int ( quotient , pad = 0 ) \n    if pad : \n        trits = trits + ( [ 0 ] * max ( 0 , pad - len ( trits ) ) ) \n    return trits "}
{"5138": "\ndef decode ( self , input , errors = 'strict' ) : \n    if isinstance ( input , memoryview ) : \n        input = input . tobytes ( ) \n    if not isinstance ( input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) \n    if not isinstance ( input , bytearray ) : \n        input = bytearray ( input ) \n    bytes_ = bytearray ( ) \n    for i in range ( 0 , len ( input ) , 2 ) : \n        try : \n            first , second = input [ i : i + 2 ] \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) \n            elif errors == 'replace' : \n                bytes_ = bytes_ + ( b'?' ) \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) \n            elif errors == 'replace' : \n                bytes_ = bytes_ + ( b'?' ) \n    return binary_type ( bytes_ ) , len ( input ) "}
{"5145": "\ndef get_messages ( self , errors = 'drop' ) : \n    decode_errors = 'strict' if errors == 'drop' else errors \n    messages = [ ] \n    for group in self . group_transactions ( ) : \n        if group [ 0 ] . value < 0 : \n            continue \n        message_trytes = TryteString ( b'' ) \n        for txn in group : \n            message_trytes = message_trytes + ( txn . signature_message_fragment ) \n        if message_trytes : \n            try : \n                messages . append ( message_trytes . decode ( decode_errors ) ) \n            except ( TrytesDecodeError , UnicodeDecodeError ) : \n                if errors != 'drop' : \n                    raise \n    return messages "}
{"5154": "\ndef _create_validator ( self ) : \n    grouped_transactions = self . bundle . group_transactions ( ) \n    bundle_hash = self . bundle . hash \n    last_index = len ( self . bundle ) - 1 \n    balance = 0 \n    counter = 0 \n    for group in grouped_transactions : \n        for txn in group : \n            balance = balance + ( txn . value ) \n            if txn . bundle_hash != bundle_hash : \n                yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) \n            if txn . current_index != counter : \n                yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current_index , i = counter , ) ) \n            if txn . last_index != last_index : \n                yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last_index , expected = last_index , i = counter , ) ) \n            counter = counter + ( 1 ) \n    if balance != 0 : \n        yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) \n    if not self . _errors : \n        signature_validation_queue = [ ] \n        for group in grouped_transactions : \n            if group [ 0 ] . value >= 0 : \n                continue \n            validate_group_signature = True \n            for j , txn in enumerate ( group ) : \n                if ( j > 0 ) and ( txn . value != 0 ) : \n                    yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current_index , ) ) \n                    validate_group_signature = False \n                    continue \n            if validate_group_signature : \n                signature_validation_queue . append ( group ) \n        if signature_validation_queue : \n            for error in self . _get_bundle_signature_errors ( signature_validation_queue ) : \n                yield error "}
{"5163": "\ndef absorb ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits = trits + ( [ 0 ] * ( TRIT_HASH_LENGTH - pad ) ) \n    if length is None : \n        length = len ( trits ) \n    if length < 1 : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while offset < length : \n        stop = min ( offset + TRIT_HASH_LENGTH , length ) \n        if stop - offset == TRIT_HASH_LENGTH : \n            trits [ stop - 1 ] = 0 \n        signed_nums = conv . convertToBytes ( trits [ offset : stop ] ) \n        unsigned_bytes = bytearray ( conv . convert_sign ( b ) for b in signed_nums ) \n        self . k . update ( unsigned_bytes ) \n        offset = offset + ( TRIT_HASH_LENGTH ) "}
{"5164": "\ndef squeeze ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits = trits + ( [ 0 ] * ( TRIT_HASH_LENGTH - pad ) ) \n    if length is None : \n        length = len ( trits ) or TRIT_HASH_LENGTH \n    if length < 1 : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while offset < length : \n        unsigned_hash = self . k . digest ( ) \n        if PY2 : \n            unsigned_hash = map ( ord , unsigned_hash ) \n        signed_hash = [ conv . convert_sign ( b ) for b in unsigned_hash ] \n        trits_from_hash = conv . convertToTrits ( signed_hash ) \n        trits_from_hash [ TRIT_HASH_LENGTH - 1 ] = 0 \n        stop = min ( TRIT_HASH_LENGTH , length - offset ) \n        trits [ offset : offset + stop ] = trits_from_hash [ 0 : stop ] \n        flipped_bytes = bytearray ( conv . convert_sign ( ~ b ) for b in unsigned_hash ) \n        self . reset ( ) \n        self . k . update ( flipped_bytes ) \n        offset = offset + ( TRIT_HASH_LENGTH ) "}
{"5171": "\ndef sign_inputs ( self , key_generator ) : \n    if not self . hash : \n        raise RuntimeError ( 'Cannot sign inputs until bundle is finalized.' ) \n    i = 0 \n    while i < len ( self ) : \n        txn = self [ i ] \n        if txn . value < 0 : \n            if txn . address . key_index is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``key_index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            if txn . address . security_level is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``security_level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            self . sign_input_at ( i , key_generator . get_key_for ( txn . address ) ) \n            i = i + ( txn . address . security_level ) \n        else : \n            i = i + ( 1 ) "}
{"5190": "\ndef to_csv ( self ) : \n    header = [ ] \n    component_header = [ ] \n    for row in self : \n        for j in row . __dict__ . keys ( ) : \n            if j == '_colour' : \n                j = 'colour' \n            header . append ( j ) \n        for k in row . component . __dict__ . keys ( ) : \n            component_header . append ( k ) \n    header = set ( header ) \n    component_header = set ( component_header ) \n    header . remove ( 'component' ) \n    header_row = '' \n    if 'colour' in header : \n        header_row = header_row + ( 'colour,' ) \n        header . remove ( 'colour' ) \n        has_colour = True \n    for item in header : \n        header_row = header_row + ( item + ',' ) \n    for item in component_header : \n        header_row = header_row + ( 'component ' + item + ',' ) \n    result = header_row . strip ( ',' ) + '\\n' \n    for row in self : \n        if has_colour : \n            result = result + ( row . __dict__ . get ( '_colour' , '' ) + ',' ) \n        for item in header : \n            result = result + ( str ( row . __dict__ . get ( item , '' ) ) + ',' ) \n        for item in component_header : \n            result = result + ( str ( row . component . __dict__ . get ( item , '' ) ) + ',' ) \n        result = result + ( '\\n' ) \n    return result "}
{"5194": "\ndef get_component ( self , colour , tolerance = 0 , default = None ) : \n    if not ( 0 <= tolerance <= np . sqrt ( 195075 ) ) : \n        raise LegendError ( 'Tolerance must be between 0 and 441.67' ) \n    for decor in self . __list : \n        if colour . lower ( ) == decor . colour : \n            return decor . component \n    r1 , g1 , b1 = utils . hex_to_rgb ( colour ) \n    best_match = '#000000' \n    best_match_dist = np . sqrt ( r1 ** 2. + g1 ** 2. + b1 ** 2. ) \n    for decor in self . __list : \n        r2 , g2 , b2 = decor . rgb \n        distance = np . sqrt ( ( r2 - r1 ) ** 2. + ( g2 - g1 ) ** 2. + ( b2 - b1 ) ** 2. ) \n        if distance < best_match_dist : \n            best_match = decor . component \n            best_match_dist = distance \n            best_match_colour = decor . colour \n    if best_match_dist <= tolerance : \n        return best_match \n    else : \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( \"always\" ) \n            w = \"No match found for {0} \" . format ( colour . lower ( ) ) \n            w = w + ( \"with tolerance of {0}. Best match is \" . format ( tolerance ) ) \n            w = w + ( \"{0}, {1}\" . format ( best_match . summary ( ) , best_match_colour ) ) \n            w = w + ( \", d={0}\" . format ( best_match_dist ) ) \n            warnings . warn ( w ) \n        return default "}
{"5198": "\ndef Rock ( * args , ** kwargs ) : \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"always\" ) \n        w = \"The 'Rock' class was renamed 'Component'. \" \n        w = w + ( \"Please update your code.\" ) \n        warnings . warn ( w , DeprecationWarning , stacklevel = 2 ) \n    return Component ( * args , ** kwargs ) "}
{"5202": "\ndef unique ( self ) : \n    all_rx = set ( [ iv . primary for iv in self ] ) \n    table = { r : 0 for r in all_rx } \n    for iv in self : \n        table [ iv . primary ] = table [ iv . primary ] + ( iv . thickness ) \n    return sorted ( table . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) "}
{"5209": "\ndef from_log ( cls , log , cutoff = None , components = None , legend = None , legend_field = None , field = None , right = False , basis = None , source = 'Log' ) : \n    if ( components is None ) and ( legend is None ) and ( field is None ) : \n        m = 'You must provide a list of components, and legend, or a field.' \n        raise StriplogError ( m ) \n    if ( legend is not None ) and ( legend_field is None ) : \n        try : \n            components = [ deepcopy ( decor . component ) for decor in legend ] \n        except AttributeError : \n            pass \n    if legend_field is not None : \n        field_values = [ getattr ( d , legend_field , 0 ) for d in legend ] \n        components = [ Component ( ) for i in range ( int ( max ( field_values ) + 1 ) ) ] \n        for i , decor in enumerate ( legend ) : \n            components [ i ] = deepcopy ( decor . component ) \n    if cutoff is not None : \n        try : \n            n = len ( cutoff ) \n        except TypeError : \n            n = 1 \n        if len ( components ) < n + 1 : \n            m = 'For n cutoffs, you need to provide at least' \n            m = m + ( 'n+1 components.' ) \n            raise StriplogError ( m ) \n        try : \n            a = np . digitize ( log , cutoff , right ) \n        except ValueError : \n            a = np . digitize ( log , [ cutoff ] , right ) \n    else : \n        a = np . copy ( log ) \n    tops , values = utils . tops_from_loglike ( a ) \n    if basis is None : \n        m = 'You must provide a depth or elevation basis.' \n        raise StriplogError ( m ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components , field = field ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5226": "\ndef merge_overlaps ( self ) : \n    overlaps = np . array ( self . find_overlaps ( index = True ) ) \n    if not overlaps . any ( ) : \n        return \n    for overlap in overlaps : \n        before = self [ overlap ] . copy ( ) \n        after = self [ overlap + 1 ] . copy ( ) \n        del self [ overlap ] \n        del self [ overlap ] \n        new_segment = before . merge ( after ) \n        self . __insert ( overlap , new_segment ) \n        overlaps = overlaps + ( 1 ) \n    return "}
{"5227": "\ndef hist ( self , lumping = None , summary = False , sort = True , plot = True , legend = None , ax = None ) : \n    comps = [ ] \n    labels = [ ] \n    entries = defaultdict ( int ) \n    for i in self : \n        if lumping : \n            k = i . primary [ lumping ] \n        else : \n            if summary : \n                k = i . primary . summary ( ) \n            else : \n                k = i . primary \n        comps . append ( i . primary ) \n        labels . append ( i . primary . summary ( ) ) \n        entries [ k ] = entries [ k ] + ( i . thickness ) \n    if sort : \n        allitems = sorted ( entries . items ( ) , key = lambda i : i [ 1 ] , reverse = True ) \n        ents , counts = zip ( * allitems ) \n    else : \n        ents , counts = tuple ( entries . keys ( ) ) , tuple ( entries . values ( ) ) \n    if plot : \n        if ax is None : \n            fig , ax = plt . subplots ( ) \n            return_ax = False \n        else : \n            return_ax = True \n        ind = np . arange ( len ( ents ) ) \n        bars = ax . bar ( ind , counts , align = 'center' ) \n        ax . set_xticks ( ind ) \n        ax . set_xticklabels ( labels ) \n        if legend : \n            colours = [ legend . get_colour ( c ) for c in comps ] \n            for b , c in zip ( bars , colours ) : \n                b . set_color ( c ) \n        ax . set_ylabel ( 'Thickness [m]' ) \n    else : \n        bars = [ ] \n    if plot and return_ax : \n        return counts , ents , ax \n    return counts , ents , bars "}
{"5229": "\ndef crop ( self , extent , copy = False ) : \n    try : \n        if extent [ 0 ] is None : \n            extent = ( self . start . z , extent [ 1 ] ) \n        if extent [ 1 ] is None : \n            extent = ( extent [ 0 ] , self . stop . z ) \n    except : \n        m = \"You must provide a 2-tuple for the new extents. Use None for\" \n        m = m + ( \" the existing start or stop.\" ) \n        raise StriplogError ( m ) \n    first_ix = self . read_at ( extent [ 0 ] , index = True ) \n    last_ix = self . read_at ( extent [ 1 ] , index = True ) \n    first = self [ first_ix ] . split_at ( extent [ 0 ] ) [ 1 ] \n    last = self [ last_ix ] . split_at ( extent [ 1 ] ) [ 0 ] \n    new_list = self . __list [ first_ix : last_ix + 1 ] . copy ( ) \n    new_list [ 0 ] = first \n    new_list [ - 1 ] = last \n    if copy : \n        return Striplog ( new_list ) \n    else : \n        self . __list = new_list \n        return "}
{"5266": "\ndef safe_unicode ( self , buf ) : \n    tmp = \"\" \n    buf = \"\" . join ( b for b in buf ) \n    for character in buf : \n        tmp = tmp + ( character ) \n    return tmp "}
{"5284": "\ndef build ( self , pre = None , shortest = False ) : \n    global REF_LEVEL \n    REF_LEVEL = REF_LEVEL + ( 1 ) \n    try : \n        if pre is None : \n            pre = [ ] \n        definition = self . fuzzer . get_ref ( self . cat , self . refname ) \n        res = utils . val ( definition , pre , shortest = ( shortest or REF_LEVEL >= self . max_recursion ) ) \n        return res \n    finally : \n        REF_LEVEL = REF_LEVEL - ( 1 ) "}
{"5289": "\ndef randfloat ( a , b = None ) : \n    if b is None : \n        max_ = a \n        min_ = 0.0 \n    else : \n        min_ = a \n        max_ = b \n    diff = max_ - min_ \n    res = _random ( ) \n    res = res * ( diff ) \n    res = res + ( min_ ) \n    return res "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == False : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = 0 \n    while total_gend < num : \n        if len ( self . _last_pref_keys ) > 0 and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            print ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend = total_gend + ( 1 ) \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5344": "\ndef sublists ( self , i : int = None , pattern : str = None ) -> List [ 'WikiList' ] : \n    patterns = ( r'\\#' , r'\\*' , '[:;]' ) if pattern is None else ( pattern , ) \n    self_pattern = self . pattern \n    lists = self . lists \n    sublists = [ ] \n    sublists_append = sublists . append \n    if i is None : \n        for pattern in patterns : \n            for lst in lists ( self_pattern + pattern ) : \n                sublists_append ( lst ) \n        return sublists \n    match = self . _match \n    fullitem_spans = match . spans ( 'fullitem' ) \n    ss = self . _span [ 0 ] \n    ms = match . start ( ) \n    s , e = fullitem_spans [ i ] \n    e = e - ( ms - ss ) \n    s = s - ( ms - ss ) \n    for pattern in patterns : \n        for lst in lists ( self_pattern + pattern ) : \n            ls , le = lst . _span \n            if s < ls and le <= e : \n                sublists_append ( lst ) \n    return sublists "}
{"5349": "\ndef _pattern ( trie : dict ) -> str : \n    if '' in trie : \n        if len ( trie ) == 1 : \n            return '' \n        optional = True \n        del trie [ '' ] \n    else : \n        optional = False \n    subpattern_to_chars = _defaultdict ( list ) \n    for char , sub_trie in trie . items ( ) : \n        subpattern = _pattern ( sub_trie ) \n        subpattern_to_chars [ subpattern ] . append ( char ) \n    alts = [ ] \n    for subpattern , chars in subpattern_to_chars . items ( ) : \n        if len ( chars ) == 1 : \n            alts . append ( chars [ 0 ] + subpattern ) \n        else : \n            chars . sort ( reverse = True ) \n            alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) \n    if len ( alts ) == 1 : \n        result = alts [ 0 ] \n        if optional : \n            if len ( result ) == 1 : \n                result = result + ( '?+' ) \n            else : \n                result = '(?:' + result + ')?+' \n    else : \n        alts . sort ( reverse = True ) \n        result = '(?>' + '|' . join ( alts ) + ')' \n        if optional : \n            result = result + ( '?+' ) \n    return result "}
{"5350": "\ndef _check_index ( self , key : Union [ slice , int ] ) -> ( int , int ) : \n    ss , se = self . _span \n    if isinstance ( key , int ) : \n        if key < 0 : \n            key = key + ( se - ss ) \n            if key < 0 : \n                raise IndexError ( 'index out of range' ) \n        elif key >= se - ss : \n            raise IndexError ( 'index out of range' ) \n        start = ss + key \n        return start , start + 1 \n    if key . step is not None : \n        raise NotImplementedError ( 'step is not implemented for string setter.' ) \n    start , stop = key . start or 0 , key . stop \n    if start < 0 : \n        start = start + ( se - ss ) \n        if start < 0 : \n            raise IndexError ( 'start index out of range' ) \n    if stop is None : \n        stop = se - ss \n    elif stop < 0 : \n        stop = stop + ( se - ss ) \n    if start > stop : \n        raise IndexError ( 'stop index out of range or start is after the stop' ) \n    return start + ss , stop + ss "}
{"5351": "\ndef insert ( self , index : int , string : str ) -> None : \n    ss , se = self . _span \n    lststr = self . _lststr \n    lststr0 = lststr [ 0 ] \n    if index < 0 : \n        index = index + ( se - ss ) \n        if index < 0 : \n            index = 0 \n    elif index > se - ss : \n        index = se - ss \n    index = index + ( ss ) \n    lststr [ 0 ] = lststr0 [ : index ] + string + lststr0 [ index : ] \n    string_len = len ( string ) \n    self . _insert_update ( index = index , length = string_len ) \n    type_to_spans = self . _type_to_spans \n    for type_ , spans in parse_to_spans ( bytearray ( string , 'ascii' , 'replace' ) ) . items ( ) : \n        for s , e in spans : \n            insort ( type_to_spans [ type_ ] , [ index + s , index + e ] ) "}
{"5354": "\ndef _shrink_update ( self , rmstart : int , rmstop : int ) -> None : \n    for spans in self . _type_to_spans . values ( ) : \n        i = len ( spans ) - 1 \n        while i >= 0 : \n            s , e = span = spans [ i ] \n            if rmstop <= s : \n                rmlength = rmstop - rmstart \n                span [ : ] = s - rmlength , e - rmlength \n                i = i - ( 1 ) \n                continue \n            break \n        else : \n            continue \n        while True : \n            if rmstart <= s : \n                if rmstop < e : \n                    span [ : ] = rmstart , e + rmstart - rmstop \n                    i = i - ( 1 ) \n                    if i < 0 : \n                        break \n                    s , e = span = spans [ i ] \n                    continue \n                spans . pop ( i ) [ : ] = - 1 , - 1 \n                i = i - ( 1 ) \n                if i < 0 : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            break \n        while i >= 0 : \n            if e <= rmstart : \n                i = i - ( 1 ) \n                if i < 0 : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            span [ 1 ] = span [ 1 ] - ( rmstop - rmstart ) \n            i = i - ( 1 ) \n            if i < 0 : \n                break \n            s , e = span = spans [ i ] \n            continue "}
{"5355": "\ndef _insert_update ( self , index : int , length : int ) -> None : \n    ss , se = self . _span \n    for spans in self . _type_to_spans . values ( ) : \n        for span in spans : \n            if index < span [ 1 ] or span [ 1 ] == index == se : \n                span [ 1 ] = span [ 1 ] + ( length ) \n                if index < span [ 0 ] or span [ 0 ] == index != ss : \n                    span [ 0 ] = span [ 0 ] + ( length ) "}
{"5356": "\ndef nesting_level ( self ) -> int : \n    ss , se = self . _span \n    level = 0 \n    type_to_spans = self . _type_to_spans \n    for type_ in ( 'Template' , 'ParserFunction' ) : \n        spans = type_to_spans [ type_ ] \n        for s , e in spans [ : bisect ( spans , [ ss + 1 ] ) ] : \n            if se <= e : \n                level = level + ( 1 ) \n    return level "}
{"5378": "\ndef rm_dup_args_safe ( self , tag : str = None ) -> None : \n    name_to_lastarg_vals = { } \n    for arg in reversed ( self . arguments ) : \n        name = arg . name . strip ( WS ) \n        if arg . positional : \n            val = arg . value \n        else : \n            val = arg . value . strip ( WS ) \n        if name in name_to_lastarg_vals : \n            if not val : \n                del arg [ 0 : len ( arg . string ) ] \n            else : \n                lastarg , dup_vals = name_to_lastarg_vals [ name ] \n                if val in dup_vals : \n                    del arg [ 0 : len ( arg . string ) ] \n                elif '' in dup_vals : \n                    del lastarg [ 0 : len ( lastarg . string ) ] \n                    dup_vals . pop ( 0 ) \n                else : \n                    dup_vals . append ( val ) \n                    if tag : \n                        arg . value = arg . value + ( tag ) \n        else : \n            name_to_lastarg_vals [ name ] = ( arg , [ val ] ) "}
{"5434": "\ndef merge_las ( * las_files ) : \n    if len ( las_files ) == 1 : \n        las_files = las_files [ 0 ] \n    if not las_files : \n        raise ValueError ( \"No files to merge\" ) \n    if not utils . files_have_same_dtype ( las_files ) : \n        raise ValueError ( \"All files must have the same point format\" ) \n    header = las_files [ 0 ] . header \n    num_pts_merged = sum ( len ( las . points ) for las in las_files ) \n    merged = create_from_header ( header ) \n    for dim_name , dim_type in las_files [ 0 ] . points_data . point_format . extra_dims : \n        merged . add_extra_dim ( dim_name , dim_type ) \n    merged . points = np . zeros ( num_pts_merged , merged . points . dtype ) \n    merged_x = np . zeros ( num_pts_merged , np . float64 ) \n    merged_y = np . zeros ( num_pts_merged , np . float64 ) \n    merged_z = np . zeros ( num_pts_merged , np . float64 ) \n    offset = 0 \n    for i , las in enumerate ( las_files , start = 1 ) : \n        slc = slice ( offset , offset + len ( las . points ) ) \n        merged . points [ slc ] = las . points \n        merged_x [ slc ] = las . x \n        merged_y [ slc ] = las . y \n        merged_z [ slc ] = las . z \n        merged [ 'point_source_id' ] [ slc ] = i \n        offset = offset + ( len ( las . points ) ) \n    merged . x = merged_x \n    merged . y = merged_y \n    merged . z = merged_z \n    return merged "}
{"5445": "\ndef convert_header ( cls , old_header , new_version ) : \n    new_header_class = cls . header_class_for_version ( new_version ) \n    b = bytearray ( old_header ) \n    b = b + ( b\"\\x00\" * ( ctypes . sizeof ( new_header_class ) - len ( b ) ) ) \n    new_header = new_header_class . from_buffer ( b ) \n    new_header . version = str ( new_version ) \n    return new_header "}
{"5454": "\ndef print_hex ( data ) : \n    hex_msg = \"\" \n    for c in data : \n        hex_msg = hex_msg + ( \"\\\\x\" + format ( c , \"02x\" ) ) \n    _LOGGER . debug ( hex_msg ) "}
{"5456": "\ndef list_set_bits ( r , expected_length ) : \n    set_bit_numbers = [ ] \n    bit_index = 0x1 \n    assert ( len ( r ) == expected_length + 1 ) \n    for b in r [ 1 : ] : \n        for i in range ( 8 ) : \n            if ( ( b >> i ) & 1 ) == 1 : \n                set_bit_numbers . append ( bit_index ) \n            bit_index = bit_index + ( 1 ) \n    return set_bit_numbers "}
{"5461": "\nasync def disarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending disarm command.\" ) \n    while len ( code ) < 16 : \n        code = code + ( 'F' ) \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x84' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5462": "\nasync def clear_alarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending clear the alarm command.\" ) \n    while len ( code ) < 16 : \n        code = code + ( 'F' ) \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x85' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5463": "\nasync def set_output ( self , code , output_id , state ) : \n    _LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output_id , code ) \n    while len ( code ) < 16 : \n        code = code + ( 'F' ) \n    code_bytes = bytearray . fromhex ( code ) \n    mode_command = 0x88 if state else 0x89 \n    data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) + code_bytes + output_bytes ( output_id ) ) \n    await self . _send_data ( data ) "}
{"5583": "\ndef read ( self , amount ) : \n    if not self . _bandwidth_limiting_enabled : \n        return self . _fileobj . read ( amount ) \n    self . _bytes_seen = self . _bytes_seen + ( amount ) \n    if self . _bytes_seen < self . _bytes_threshold : \n        return self . _fileobj . read ( amount ) \n    self . _consume_through_leaky_bucket ( ) \n    return self . _fileobj . read ( amount ) "}
{"5585": "\ndef schedule_consumption ( self , amt , token , time_to_consume ) : \n    self . _total_wait = self . _total_wait + ( time_to_consume ) \n    self . _tokens_to_scheduled_consumption [ token ] = { 'wait_duration' : self . _total_wait , 'time_to_consume' : time_to_consume , } \n    return self . _total_wait "}
{"5593": "\ndef decrement ( self ) : \n    with self . _lock : \n        if self . _count == 0 : \n            raise RuntimeError ( 'Counter is at zero. It cannot dip below zero' ) \n        self . _count = self . _count - ( 1 ) \n        if self . _is_finalized and self . _count == 0 : \n            self . _callback ( ) "}
{"5602": "\ndef _main ( self , client , bucket , key , fileobj , extra_args , callbacks , max_attempts , download_output_manager , io_chunksize , start_index = 0 , bandwidth_limiter = None ) : \n    last_exception = None \n    for i in range ( max_attempts ) : \n        try : \n            response = client . get_object ( Bucket = bucket , Key = key , ** extra_args ) \n            streaming_body = StreamReaderProgress ( response [ 'Body' ] , callbacks ) \n            if bandwidth_limiter : \n                streaming_body = bandwidth_limiter . get_bandwith_limited_stream ( streaming_body , self . _transfer_coordinator ) \n            current_index = start_index \n            chunks = DownloadChunkIterator ( streaming_body , io_chunksize ) \n            for chunk in chunks : \n                if not self . _transfer_coordinator . done ( ) : \n                    self . _handle_io ( download_output_manager , fileobj , chunk , current_index ) \n                    current_index = current_index + ( len ( chunk ) ) \n                else : \n                    return \n            return \n        except S3_RETRYABLE_DOWNLOAD_ERRORS as e : \n            logger . debug ( \"Retrying exception caught (%s), \" \"retrying request, (attempt %s / %s)\" , e , i , max_attempts , exc_info = True ) \n            last_exception = e \n            invoke_progress_callbacks ( callbacks , start_index - current_index ) \n            continue \n    raise RetriesExceededError ( last_exception ) "}
{"5604": "\ndef request_writes ( self , offset , data ) : \n    if offset < self . _next_offset : \n        return [ ] \n    writes = [ ] \n    if offset in self . _pending_offsets : \n        return [ ] \n    heapq . heappush ( self . _writes , ( offset , data ) ) \n    self . _pending_offsets . add ( offset ) \n    while self . _writes and self . _writes [ 0 ] [ 0 ] == self . _next_offset : \n        next_write = heapq . heappop ( self . _writes ) \n        writes . append ( { 'offset' : next_write [ 0 ] , 'data' : next_write [ 1 ] } ) \n        self . _pending_offsets . remove ( next_write [ 0 ] ) \n        self . _next_offset = self . _next_offset + ( len ( next_write [ 1 ] ) ) \n    return writes "}
{"5719": "\ndef _join_translated ( translated_parts , os_sep_class ) : \n    res = '' \n    for part in translated_parts [ : - 1 ] : \n        if part == '.*' : \n            res = res + ( part ) \n        else : \n            res = res + ( part + os_sep_class ) \n    if translated_parts [ - 1 ] == '.*' : \n        res = res + ( '.+' ) \n        res = res + ( '({os_sep_class}?.*)?' . format ( os_sep_class = os_sep_class ) ) \n    else : \n        res = res + ( translated_parts [ - 1 ] ) \n    return res "}
{"5771": "\ndef config ( self , decimals_as_strings = True , ts_as_dates = False , sequencing = False , ts = False , ** kwargs ) : \n    flags = 0 \n    if decimals_as_strings : \n        flags = flags + ( 8 ) \n    if ts_as_dates : \n        flags = flags + ( 32 ) \n    if ts : \n        flags = flags + ( 32768 ) \n    if sequencing : \n        flags = flags + ( 65536 ) \n    q = { 'event' : 'conf' , 'flags' : flags } \n    q . update ( kwargs ) \n    self . conn . bitfinex_config = q \n    self . conn . send ( ** q ) "}
{"5858": "\ndef format_function ( func_body , func_type = None , indent = 2 , format_locals = True , ) : \n    if func_type is None : \n        yield 'func' \n    else : \n        param_section = ' (param {})' . format ( ' ' . join ( map ( format_lang_type , func_type . param_types ) ) ) if func_type . param_types else '' \n        result_section = ' (result {})' . format ( format_lang_type ( func_type . return_type ) ) if func_type . return_type else '' \n        yield 'func' + param_section + result_section \n    if format_locals and func_body . locals : \n        yield '(locals {})' . format ( ' ' . join ( itertools . chain . from_iterable ( itertools . repeat ( format_lang_type ( x . type ) , x . count ) for x in func_body . locals ) ) ) \n    level = 1 \n    for cur_insn in decode_bytecode ( func_body . code ) : \n        if cur_insn . op . flags & INSN_LEAVE_BLOCK : \n            level = level - ( 1 ) \n        yield ' ' * ( level * indent ) + format_instruction ( cur_insn ) \n        if cur_insn . op . flags & INSN_ENTER_BLOCK : \n            level = level + ( 1 ) "}
{"5865": "\ndef handler ( self , reader , writer ) : \n    buffer = b'' \n    while b'\\n\\n' not in buffer : \n        buffer = buffer + ( yield from reader . read ( self . buf_size ) ) \n    lines = buffer [ : - 2 ] . decode ( self . default_encoding ) . split ( '\\n' ) \n    headers = OrderedDict ( [ line . split ( ': ' , 1 ) for line in lines if ': ' in line ] ) \n    agi_network_script = headers . get ( 'agi_network_script' ) \n    log . info ( 'Received FastAGI request from %r for \"%s\" route' , writer . get_extra_info ( 'peername' ) , agi_network_script ) \n    log . debug ( \"Asterisk Headers: %r\" , headers ) \n    if agi_network_script is not None : \n        route = self . _route . get ( agi_network_script ) \n        if route is not None : \n            request = Request ( app = self , headers = headers , reader = reader , writer = writer , encoding = self . default_encoding ) \n            try : \n                yield from route ( request ) \n            except BaseException : \n                log . exception ( 'An exception has been raised for the request \"%s\"' , agi_network_script ) \n        else : \n            log . error ( 'No route for the request \"%s\"' , agi_network_script ) \n    else : \n        log . error ( 'No agi_network_script header for the request' ) \n    log . debug ( \"Closing client socket\" ) \n    writer . close ( ) "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = 0 \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            elif type . lower ( ) == \"pref\" : \n                pref = pref + ( 1 ) \n            elif re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                pref = pref + ( int ( type . split ( \"=\" ) [ 1 ] ) ) \n            else : \n                if type . lower ( ) . startswith ( \"x-\" ) : \n                    custom_types . append ( type [ 2 : ] ) \n                    standard_types . append ( type ) \n                else : \n                    custom_types . append ( type ) \n                    standard_types . append ( \"X-{}\" . format ( type ) ) \n    return ( standard_types , custom_types , pref ) "}
{"5927": "\ndef _compare_uids ( uid1 , uid2 ) : \n    sum = 0 \n    for char1 , char2 in zip ( uid1 , uid2 ) : \n        if char1 == char2 : \n            sum = sum + ( 1 ) \n        else : \n            break \n    return sum "}
{"5935": "\ndef load ( self , query = None , search_in_source_files = False ) : \n    if self . _loaded : \n        return \n    logging . debug ( 'Loading Vdir %s with query %s' , self . name , query ) \n    errors = 0 \n    for filename in self . _find_vcard_files ( search = query , search_in_source_files = search_in_source_files ) : \n        try : \n            card = CarddavObject . from_file ( self , filename , self . _private_objects , self . _localize_dates ) \n        except ( IOError , vobject . base . ParseError ) as err : \n            verb = \"open\" if isinstance ( err , IOError ) else \"parse\" \n            logging . debug ( \"Error: Could not %s file %s\\n%s\" , verb , filename , err ) \n            if self . _skip : \n                errors = errors + ( 1 ) \n            else : \n                logging . error ( \"The vcard file %s of address book %s could not be \" \"parsed\\nUse --debug for more information or \" \"--skip-unparsable to proceed\" , filename , self . name ) \n                sys . exit ( 2 ) \n        else : \n            uid = card . get_uid ( ) \n            if not uid : \n                logging . warning ( \"Card %s from address book %s has no UID \" \"and will not be availbale.\" , card , self . name ) \n            elif uid in self . contacts : \n                logging . warning ( \"Card %s and %s from address book %s have the same \" \"UID. The former will not be availbale.\" , card , self . contacts [ uid ] , self . name ) \n            else : \n                self . contacts [ uid ] = card \n    self . _loaded = True \n    if errors : \n        logging . warning ( \"%d of %d vCard files of address book %s could not be parsed.\" , errors , len ( self . contacts ) + errors , self ) \n    logging . debug ( 'Loded %s contacts from address book %s.' , len ( self . contacts ) , self . name ) "}
{"5947": "\ndef confirm ( action , default = None , skip = False ) : \n    MAX_ITERATIONS = 3 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , True : ( 'Y' , 'n' ) , False : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = 1 \n                while not choice and cnt < MAX_ITERATIONS : \n                    choice = safe_input ( prompt ) \n                    cnt = cnt + ( 1 ) \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return True \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return False \n    if default is not None : \n        return default \n    return None "}
{"5952": "\ndef delete ( self , blocksize = 100 ) : \n    from . columns import MODELS_REFERENCED \n    if not self . _model . _no_fk or self . _model . _namespace in MODELS_REFERENCED : \n        raise QueryError ( \"Can't delete entities of models with foreign key relationships\" ) \n    de = [ ] \n    i = 0 \n    for result in self . iter_result ( pagesize = blocksize ) : \n        de . append ( result ) \n        i = i + ( 1 ) \n        if i >= blocksize : \n            session . delete ( de ) \n            del de [ : ] \n            i = 0 \n    if de : \n        session . delete ( de ) "}
{"5961": "\ndef clean_old_index ( model , block_size = 100 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2 ] ) ) \n    has_hscan = version >= [ 2 , 8 ] \n    pipe = conn . pipeline ( True ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10 ) \n    force_hscan = kwargs . get ( 'force_hscan' , False ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = 0 \n        while cursor != b'0' : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , 0 , 0 ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned = scanned + ( block_size ) \n            if scanned > max_id : \n                max_id = scanned + 1 \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while cursor != b'0' : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , 1 , 0 ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned = scanned + ( block_size ) \n                if scanned > max_id : \n                    max_id = scanned + 1 \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        for i in range ( 1 , max_id + 1 , block_size ) : \n            ids = list ( range ( i , min ( i + block_size , max_id + 1 ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - 1 ) , max_id \n    yield max_id , max_id "}
{"5973": "\ndef _add_call_item_to_queue ( pending_work_items , running_work_items , work_ids , call_queue ) : \n    while True : \n        if call_queue . full ( ) : \n            return \n        try : \n            work_id = work_ids . get ( block = False ) \n        except queue . Empty : \n            return \n        else : \n            work_item = pending_work_items [ work_id ] \n            if work_item . future . set_running_or_notify_cancel ( ) : \n                running_work_items = running_work_items + ( [ work_id ] ) \n                call_queue . put ( _CallItem ( work_id , work_item . fn , work_item . args , work_item . kwargs ) , block = True ) \n            else : \n                del pending_work_items [ work_id ] \n                continue "}
{"5985": "\ndef get_exitcodes_terminated_worker ( processes ) : \n    patience = 5 \n    exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n    while len ( exitcodes ) == 0 and patience > 0 : \n        patience = patience - ( 1 ) \n        exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n        time . sleep ( .05 ) \n    return _format_exitcodes ( exitcodes ) "}
{"5988": "\ndef ensure_running ( self ) : \n    with self . _lock : \n        if self . _fd is not None : \n            if self . _check_alive ( ) : \n                return \n            os . close ( self . _fd ) \n            try : \n                os . waitpid ( self . _pid , 0 ) \n            except OSError : \n                pass \n            self . _fd = None \n            self . _pid = None \n            warnings . warn ( 'semaphore_tracker: process died unexpectedly, ' 'relaunching.  Some semaphores might leak.' ) \n        fds_to_pass = [ ] \n        try : \n            fds_to_pass . append ( sys . stderr . fileno ( ) ) \n        except Exception : \n            pass \n        r , w = os . pipe ( ) \n        cmd = 'from {} import main; main({}, {})' . format ( main . __module__ , r , VERBOSE ) \n        try : \n            fds_to_pass . append ( r ) \n            exe = spawn . get_executable ( ) \n            args = [ exe ] + util . _args_from_interpreter_flags ( ) \n            if sys . version_info [ : 2 ] <= ( 3 , 3 ) : \n                import re \n                for i in range ( 1 , len ( args ) ) : \n                    args [ i ] = re . sub ( \"-R+\" , \"-R\" , args [ i ] ) \n            args = args + ( [ '-c' , cmd ] ) \n            util . debug ( \"launching Semaphore tracker: {}\" . format ( args ) ) \n            try : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_BLOCK , _IGNORED_SIGNALS ) \n                pid = spawnv_passfds ( exe , args , fds_to_pass ) \n            finally : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n        except BaseException : \n            os . close ( w ) \n            raise \n        else : \n            self . _fd = w \n            self . _pid = pid \n        finally : \n            os . close ( r ) "}
{"6002": "\ndef read_command ( self , prompt = '' ) : \n    self . input_lineno = self . input_lineno + ( 1 ) \n    line = self . readline ( ) \n    if self . verbose : \n        location = \"%s line %s\" % ( self . script_name , self . input_lineno ) \n        self . msg ( '+ %s: %s' % ( location , line ) ) \n        pass \n    return line "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - 1 , cur_line = 0 , start_line = - 1 , end_line = None , relative_pos = False , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    statement_count = 10000 \n    if end_line is None : \n        end_line = 10000 \n    elif relative_pos : \n        end_line = end_line + ( start_line - 1 ) \n        pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if start_line > cur_line : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and offset > end_offset : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( start_line > cur_line ) or start_offset and start_offset > offset ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count = statement_count - ( 1 ) \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( cur_line > end_line ) or ( end_offset and offset > end_offset ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and start_offset <= offset : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if offset == lasti : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6006": "\ndef count_frames ( frame , count_start = 0 ) : \n    count = - count_start \n    while frame : \n        count = count + ( 1 ) \n        frame = frame . f_back \n    return count "}
{"6007": "\ndef get_call_function_name ( frame ) : \n    f_back = frame . f_back \n    if not f_back : \n        return None \n    if 'CALL_FUNCTION' != Mbytecode . op_at_frame ( f_back ) : \n        return None \n    co = f_back . f_code \n    code = co . co_code \n    linestarts = dict ( dis . findlinestarts ( co ) ) \n    offset = f_back . f_lasti \n    while offset >= 0 : \n        if offset in linestarts : \n            op = code [ offset ] \n            offset = offset + ( 1 ) \n            arg = code [ offset ] \n            extended_arg = 0 \n            while True : \n                if PYTHON_VERSION >= 3.6 : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg = extended_arg + ( ( arg << 8 ) ) \n                        continue \n                    arg = code [ offset ] + extended_arg \n                else : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg = extended_arg + ( ( arg << 256 ) ) \n                        continue \n                    arg = code [ offset ] + code [ offset + 1 ] * 256 + extended_arg \n                break \n            return co . co_names [ arg ] \n        offset = offset - ( 1 ) \n        pass \n    return None "}
{"6010": "\ndef short_help ( self , subcmd_cb , subcmd_name , label = False ) : \n    entry = self . lookup ( subcmd_name ) \n    if entry : \n        if label : \n            prefix = entry . name \n        else : \n            prefix = '' \n            pass \n        if hasattr ( entry , 'short_help' ) : \n            if prefix : \n                prefix = prefix + ( ' -- ' ) \n            self . cmd_obj . msg ( prefix + entry . short_help ) \n            pass \n        pass \n    else : \n        self . undefined_subcmd ( \"help\" , subcmd_name ) \n        pass \n    return "}
{"6015": "\ndef run ( self , args ) : \n    if not self . proc . curframe : \n        self . errmsg ( \"No line number information available.\" ) \n        return \n    if len ( args ) == 3 : \n        answer = self . lineinfo ( args [ 2 ] ) \n        if answer [ 0 ] : \n            item , filename , lineno = answer \n            if not os . path . isfile ( filename ) : \n                filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n            self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) \n        return \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    if not os . path . isfile ( filename ) : \n        filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n        pass \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) \n    msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) \n    if self . proc . event : \n        msg2 = msg2 + ( ', %s event' % self . proc . event ) \n        pass \n    self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) \n    return False "}
{"6033": "\ndef format ( self , show_enabled = True ) : \n    what = '' \n    if show_enabled : \n        if self . enabled : \n            what = what + ( ' y ' ) \n        else : \n            what = what + ( ' n ' ) \n            pass \n        pass \n    if self . fmt : \n        what = what + ( self . fmt + ' ' ) \n        pass \n    what = what + ( self . arg ) \n    return '%3d: %s' % ( self . number , what ) "}
{"6067": "\ndef run ( self , args ) : \n    if len ( args ) == 0 : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return False \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ 0 ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m = m + ( \" cached in debugger\" ) \n        if filename_cache [ filename ] != filename : \n            m = m + ( ' as:' ) \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m = m + ( '.' ) \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( len ( matches ) > 1 ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        elif len ( matches ) == 1 : \n            canonic_name = pyficache . unmap_file ( matches [ 0 ] ) \n            m = m + ( \" matched debugger cache file:\\n  \" + canonic_name ) \n            self . msg ( m ) \n        else : \n            self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if name == v ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ 1 : ] : \n        processed_arg = False \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = False , arrange_vertical = False , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = True \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6077": "\ndef t_whitespace ( self , s ) : \n    self . add_token ( 'SPACE' , s ) \n    self . pos = self . pos + ( len ( s ) ) \n    pass "}
{"6098": "\ndef wait_for_build ( self , interval = 5 , path = None ) : \n    path = path or '' \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) [ 'last_build_info' ] \n        if not response : \n            raise ValueError ( 'This project is not building!' ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log = next_log + ( 120 ) \n        time . sleep ( interval ) "}
{"6106": "\ndef wait_for ( self , job_id , base_path = None , interval = 5 ) : \n    if base_path is None : \n        base_path = 'jobs/id' \n    path = '%s%d' % ( ensure_trailing_slash ( base_path ) , job_id ) \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log = next_log + ( 120 ) \n        time . sleep ( interval ) "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        print ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == 0 : \n            print ( 'No such project exists!' ) \n            return \n        if len ( projects ) > 1 : \n            print ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ 0 ] [ 'project_id' ] \n        print ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000 ) : \n        counter = counter + ( 1 ) \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        print ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        print ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6185": "\ndef add_missing_row ( df : pd . DataFrame , id_cols : List [ str ] , reference_col : str , complete_index : Union [ Dict [ str , str ] , List [ str ] ] = None , method : str = None , cols_to_keep : List [ str ] = None ) -> pd . DataFrame : \n    if cols_to_keep is None : \n        cols_for_index = [ reference_col ] \n    else : \n        cols_for_index = [ reference_col ] + cols_to_keep \n    check_params_columns_duplicate ( id_cols + cols_for_index ) \n    if method == 'between' or method == 'between_and_after' : \n        df [ 'start' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( min ) \n        id_cols = id_cols + ( [ 'start' ] ) \n    if method == 'between' or method == 'between_and_before' : \n        df [ 'end' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( max ) \n        id_cols = id_cols + ( [ 'end' ] ) \n    names = id_cols + cols_for_index \n    new_df = df . set_index ( names ) \n    index_values = df . groupby ( id_cols ) . sum ( ) . index . values \n    if complete_index is None : \n        complete_index = df . groupby ( cols_for_index ) . sum ( ) . index . values \n    elif isinstance ( complete_index , dict ) : \n        if complete_index [ 'type' ] == 'date' : \n            freq = complete_index [ 'freq' ] \n            date_format = complete_index [ 'format' ] \n            start = complete_index [ 'start' ] \n            end = complete_index [ 'end' ] \n            if isinstance ( freq , dict ) : \n                freq = pd . DateOffset ( ** { k : int ( v ) for k , v in freq . items ( ) } ) \n            complete_index = pd . date_range ( start = start , end = end , freq = freq ) \n            complete_index = complete_index . strftime ( date_format ) \n        else : \n            raise ParamsValueError ( f'Unknown complete index type: ' f'{complete_index[\"type\"]}' ) \n    if not isinstance ( index_values [ 0 ] , tuple ) : \n        index_values = [ ( x , ) for x in index_values ] \n    if not isinstance ( complete_index [ 0 ] , tuple ) : \n        complete_index = [ ( x , ) for x in complete_index ] \n    new_tuples_index = [ x + y for x in index_values for y in complete_index ] \n    new_index = pd . MultiIndex . from_tuples ( new_tuples_index , names = names ) \n    new_df = new_df . reindex ( new_index ) . reset_index ( ) \n    if method == 'between' or method == 'between_and_after' : \n        new_df = new_df [ new_df [ reference_col ] >= new_df [ 'start' ] ] \n        del new_df [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        new_df = new_df [ new_df [ reference_col ] <= new_df [ 'end' ] ] \n        del new_df [ 'end' ] \n    return new_df "}
{"6200": "\ndef add_months ( dateobj , nb_months : int ) : \n    nb_years , nb_months = divmod ( nb_months , 12 ) \n    month = dateobj . month + nb_months \n    if month > 12 : \n        nb_years = nb_years + ( 1 ) \n        month = month - ( 12 ) \n    year = dateobj . year + nb_years \n    lastday = monthrange ( year , month ) [ 1 ] \n    return dateobj . replace ( year = year , month = month , day = min ( lastday , dateobj . day ) ) "}
{"6215": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    iteration = 0 \n    while True : \n        if not iteration % self . config . test_frequency and test_set : \n            try : \n                self . test ( iteration , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not iteration % self . validation_frequency and valid_set : \n            try : \n                if not self . evaluate ( iteration , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        train_message = \"\" \n        try : \n            train_message = self . train_func ( train_set ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if not iteration % self . config . monitor_frequency : \n            logging . info ( 'monitor (iter=%i) %s' , iteration + 1 , train_message ) \n        iteration = iteration + ( 1 ) \n        if hasattr ( self . network , \"iteration_callback\" ) : \n            self . network . iteration_callback ( ) \n        yield train_message \n    if valid_set : \n        self . set_params ( self . best_params ) \n    if test_set : \n        self . test ( 0 , test_set ) "}
{"6217": "\ndef compute_alignments ( self , prev_state , precomputed_values , mask = None ) : \n    WaSp = T . dot ( prev_state , self . Wa ) \n    UaH = precomputed_values \n    if UaH . ndim == 2 : \n        preact = WaSp [ : , None , : ] + UaH [ None , : , : ] \n    else : \n        preact = WaSp [ : , None , : ] + UaH \n    act = T . activate ( preact , 'tanh' ) \n    align_scores = T . dot ( act , self . Va ) \n    if mask : \n        mask = ( 1 - mask ) * - 99.00 \n        if align_scores . ndim == 3 : \n            align_scores = align_scores + ( mask [ None , : ] ) \n        else : \n            align_scores = align_scores + ( mask ) \n    align_weights = T . nnet . softmax ( align_scores ) \n    return align_weights "}
{"6224": "\ndef invoke ( self ) : \n    self . _counter = self . _counter + ( 1 ) \n    if self . _counter % self . _freq == 0 : \n        cnt = 0. \n        sum_map = defaultdict ( float ) \n        for x in self . _trainer . get_data ( self . _data_split ) : \n            val_map = self . run ( x ) \n            if not isinstance ( val_map , dict ) : \n                raise Exception ( \"Monitor.run must return a dict.\" ) \n            for k , val in val_map . items ( ) : \n                sum_map [ k ] = sum_map [ k ] + ( val ) \n            cnt = cnt + ( 1 ) \n        for k in sum_map : \n            sum_map [ k ] = sum_map [ k ] / ( cnt ) \n        new_best = self . compare ( sum_map ) \n        self . _trainer . report ( sum_map , self . _data_split , new_best = new_best ) \n        if new_best : \n            self . _trainer . save_checkpoint ( self . _save_path ) "}
{"6231": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    self . _epoch = 0 \n    while True : \n        if self . _skip_epochs > 0 : \n            logging . info ( \"skipping one epoch ...\" ) \n            self . _skip_epochs = self . _skip_epochs - ( 1 ) \n            self . _epoch = self . _epoch + ( 1 ) \n            yield None \n            continue \n        if not self . _epoch % self . config . test_frequency and test_set : \n            try : \n                self . _run_test ( self . _epoch , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not self . _epoch % self . validation_frequency and valid_set : \n            try : \n                if not self . _run_valid ( self . _epoch , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        try : \n            costs = self . _run_train ( self . _epoch , train_set , train_size ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if np . isnan ( costs [ 0 ] [ 1 ] ) : \n            logging . info ( \"NaN detected in costs, rollback to last parameters\" ) \n            self . set_params ( * self . checkpoint ) \n        else : \n            self . _epoch = self . _epoch + ( 1 ) \n            self . network . epoch_callback ( ) \n        yield dict ( costs ) \n    if valid_set and self . config . get ( \"save_best_parameters\" , True ) : \n        self . set_params ( * self . best_params ) \n    if test_set : \n        self . _run_test ( - 1 , test_set ) "}
{"6234": "\ndef report ( self , score_map , type = \"valid\" , epoch = - 1 , new_best = False ) : \n    type_str = type \n    if len ( type_str ) < 5 : \n        type_str = type_str + ( \" \" * ( 5 - len ( type_str ) ) ) \n    info = \" \" . join ( \"%s=%.2f\" % el for el in score_map . items ( ) ) \n    current_epoch = epoch if epoch > 0 else self . current_epoch ( ) \n    epoch_str = \"epoch={}\" . format ( current_epoch + 1 ) \n    if epoch < 0 : \n        epoch_str = \"dryrun\" \n        sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) \n    marker = \" *\" if new_best else \"\" \n    message = \"{} ({}) {}{}\" . format ( type_str , epoch_str , info , marker ) \n    self . network . train_logger . record ( message ) \n    logging . info ( message ) "}
{"6246": "\ndef register_layer ( self , layer ) : \n    if type ( layer ) == Block : \n        layer . fix ( ) \n    self . parameter_count = self . parameter_count + ( layer . parameter_count ) \n    self . parameters . extend ( layer . parameters ) \n    self . free_parameters . extend ( layer . free_parameters ) \n    self . training_monitors . extend ( layer . training_monitors ) \n    self . testing_monitors . extend ( layer . testing_monitors ) \n    self . updates . extend ( layer . updates ) \n    self . training_updates . extend ( layer . training_updates ) \n    self . input_variables . extend ( layer . external_inputs ) \n    self . target_variables . extend ( layer . external_targets ) \n    self . training_callbacks . extend ( layer . training_callbacks ) \n    self . testing_callbacks . extend ( layer . testing_callbacks ) \n    self . epoch_callbacks . extend ( layer . epoch_callbacks ) "}
{"6254": "\ndef register_parameters ( self , * parameters ) : \n    for param in parameters : \n        self . parameter_count = self . parameter_count + ( np . prod ( param . get_value ( ) . shape ) ) \n    self . parameters . extend ( parameters ) "}
{"6257": "\ndef register_monitors ( self , * monitors ) : \n    for key , node in monitors : \n        if key not in self . _registered_monitors : \n            node = node * ( 1.0 ) \n            self . training_monitors . append ( ( key , node ) ) \n            self . testing_monitors . append ( ( key , node ) ) \n            self . _registered_monitors . add ( key ) "}
{"6305": "\ndef generate_frames ( self , frame_duration_ms , zero_pad = True ) : \n    Frame = collections . namedtuple ( \"Frame\" , \"bytes timestamp duration\" ) \n    bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000 ) * self . sample_width ) \n    offset = 0 \n    timestamp = 0.0 \n    frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width \n    while offset + bytes_per_frame < len ( self . raw_data ) : \n        yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) \n        timestamp = timestamp + ( frame_duration_s ) \n        offset = offset + ( bytes_per_frame ) \n    if zero_pad : \n        rest = self . raw_data [ offset : ] \n        zeros = bytes ( bytes_per_frame - len ( rest ) ) \n        yield Frame ( rest + zeros , timestamp , frame_duration_s ) "}
{"6306": "\ndef normalize_spl_by_average ( self , db ) : \n    arr = self . to_numpy_array ( ) . copy ( ) \n    if len ( arr ) == 0 : \n        raise ValueError ( \"Cannot normalize the SPL of an empty AudioSegment\" ) \n    def rms ( x ) : \n        return np . sqrt ( np . mean ( np . square ( x ) ) ) \n    desired_rms = P_REF_PCM * ( ( 10 ** ( db / 20.0 ) ) - 1E-9 ) \n    max_ntries = 50 \n    res_rms = 0.0 \n    ntries = 0 \n    factor = 0.1 \n    left = 0.0 \n    right = desired_rms \n    while ( ntries < max_ntries ) and not util . isclose ( res_rms , desired_rms , abs_tol = 0.1 ) : \n        res_rms = rms ( arr * factor ) \n        if res_rms < desired_rms : \n            left = factor \n        else : \n            right = factor \n        factor = 0.5 * ( left + right ) \n        ntries = ntries + ( 1 ) \n    dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ self . sample_width ] \n    new_seg = from_numpy_array ( np . array ( arr * factor , dtype = dtype ) , self . frame_rate ) \n    return new_seg "}
{"6322": "\ndef _break_poorly_matched_fronts ( fronts , threshold = 0.1 , threshold_overlap_samples = 3 ) : \n    assert threshold_overlap_samples > 0 , \"Number of samples of overlap must be greater than zero\" \n    breaks_after = { } \n    for front_id in _get_front_ids_one_at_a_time ( fronts ) : \n        front = _get_front_idxs_from_id ( fronts , front_id ) \n        for i , ( f , s ) in enumerate ( front ) : \n            if i < len ( front ) - 1 : \n                next_f , next_s = front [ i + 1 ] \n                low_s = min ( s , next_s ) \n                high_s = max ( s , next_s ) \n                sig_this_f = fronts [ f , low_s : high_s ] \n                sig_next_f = fronts [ next_f , low_s : high_s ] \n                assert len ( sig_next_f ) == len ( sig_this_f ) \n                if len ( sig_next_f ) > threshold_overlap_samples : \n                    correlation = signal . correlate ( sig_this_f , sig_next_f , mode = 'same' ) \n                    assert len ( correlation ) > 0 \n                    correlation = correlation / max ( correlation + 1E-9 ) \n                    similarity = np . sum ( correlation ) / len ( correlation ) \n                    if similarity < threshold : \n                        if front_id in breaks_after : \n                            breaks_after [ front_id ] . append ( ( f , s ) ) \n                        else : \n                            breaks_after [ front_id ] = [ ] \n    taken_ids = sorted ( np . unique ( fronts ) ) \n    next_id = taken_ids [ - 1 ] + 1 \n    for id in breaks_after . keys ( ) : \n        for f , s in breaks_after [ id ] : \n            fidxs , sidxs = np . where ( fronts == id ) \n            idxs_greater_than_f = [ fidx for fidx in fidxs if fidx > f ] \n            start = len ( sidxs ) - len ( idxs_greater_than_f ) \n            indexes = ( idxs_greater_than_f , sidxs [ start : ] ) \n            fronts [ indexes ] = next_id \n            next_id = next_id + ( 1 ) \n    _remove_fronts_that_are_too_small ( fronts , 3 ) "}
{"6376": "\ndef getBits_from_array ( array , wordWidth , start , end , reinterpretElmToType = None ) : \n    inPartOffset = 0 \n    value = Bits ( end - start , None ) . fromPy ( None ) \n    while start != end : \n        assert start < end , ( start , end ) \n        dataWordIndex = start // wordWidth \n        v = array [ dataWordIndex ] \n        if reinterpretElmToType is not None : \n            v = v . _reinterpret_cast ( reinterpretElmToType ) \n        endOfWord = ( dataWordIndex + 1 ) * wordWidth \n        width = min ( end , endOfWord ) - start \n        offset = start % wordWidth \n        val = selectBitRange ( v . val , offset , width ) \n        vldMask = selectBitRange ( v . vldMask , offset , width ) \n        updateTime = v . updateTime \n        m = mask ( width ) \n        value . val |= ( val & m ) << inPartOffset \n        value . vldMask |= ( vldMask & m ) << inPartOffset \n        value . updateMask = max ( value . updateTime , updateTime ) \n        inPartOffset = inPartOffset + ( width ) \n        start = start + ( width ) \n    return value "}
{"6385": "\ndef Case ( self , caseVal , * statements ) : \n    assert self . parentStm is None \n    caseVal = toHVal ( caseVal , self . switchOn . _dtype ) \n    assert isinstance ( caseVal , Value ) , caseVal \n    assert caseVal . _isFullVld ( ) , \"Cmp with invalid value\" \n    assert caseVal not in self . _case_value_index , ( \"Switch statement already has case for value \" , caseVal ) \n    self . rank = self . rank + ( 1 ) \n    case = [ ] \n    self . _case_value_index [ caseVal ] = len ( self . cases ) \n    self . cases . append ( ( caseVal , case ) ) \n    cond = self . switchOn . _eq ( caseVal ) \n    self . _inputs . append ( cond ) \n    cond . endpoints . append ( self ) \n    self . _register_stements ( statements , case ) \n    return self "}
{"6386": "\ndef Default ( self , * statements ) : \n    assert self . parentStm is None \n    self . rank = self . rank + ( 1 ) \n    self . default = [ ] \n    self . _register_stements ( statements , self . default ) \n    return self "}
{"6395": "\ndef flatten ( iterables , level = inf ) : \n    if level >= 0 and isinstance ( iterables , ( list , tuple , GeneratorType , map , zip ) ) : \n        level = level - ( 1 ) \n        for i in iterables : \n            yield from flatten ( i , level = level ) \n    else : \n        yield iterables "}
{"6414": "\ndef _bit_length ( self ) : \n    try : \n        interfaces = self . _interfaces \n    except AttributeError : \n        interfaces = None \n    if interfaces is None : \n        _intf = self . _clone ( ) \n        _intf . _loadDeclarations ( ) \n        interfaces = _intf . _interfaces \n    if interfaces : \n        w = 0 \n        for i in interfaces : \n            w = w + ( i . _bit_length ( ) ) \n        return w \n    else : \n        return self . _dtype . bit_length ( ) "}
{"6418": "\ndef reinterpret_bits_to_hstruct ( sigOrVal , hStructT ) : \n    container = hStructT . fromPy ( None ) \n    offset = 0 \n    for f in hStructT . fields : \n        t = f . dtype \n        width = t . bit_length ( ) \n        if f . name is not None : \n            s = sigOrVal [ ( width + offset ) : offset ] \n            s = s . _reinterpret_cast ( t ) \n            setattr ( container , f . name , s ) \n        offset = offset + ( width ) \n    return container "}
{"6422": "\ndef framesFromTransTmpl ( transaction : 'TransTmpl' , wordWidth : int , maxFrameLen : Union [ int , float ] = inf , maxPaddingWords : Union [ int , float ] = inf , trimPaddingWordsOnStart : bool = False , trimPaddingWordsOnEnd : bool = False ) -> Generator [ 'FrameTmpl' , None , None ] : \n    isFirstInFrame = True \n    partsPending = False \n    startOfThisFrame = 0 \n    assert maxFrameLen > 0 \n    assert maxPaddingWords >= 0 \n    if maxPaddingWords < inf : \n        assert trimPaddingWordsOnStart or trimPaddingWordsOnEnd , \"Padding has to be cut off somewhere\" \n    it = TransTmplWordIterator ( wordWidth ) \n    lastWordI = 0 \n    endOfThisFrame = maxFrameLen \n    parts = [ ] \n    for wordI , word in it . groupByWordIndex ( transaction , 0 ) : \n        if wordI * wordWidth >= endOfThisFrame : \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : \n                _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            else : \n                _endOfThisFrame = wordI * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI \n        if ( not isFirstInFrame and trimPaddingWordsOnEnd and wordI - lastWordI > 1 ) : \n            _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI - 1 \n        if isFirstInFrame : \n            partsPending = True \n            isFirstInFrame = False \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnStart and paddingWords > maxPaddingWords : \n                startOfThisFrame = startOfThisFrame + ( paddingWords * wordWidth ) \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n        parts . extend ( word ) \n        lastWordI = wordI \n    endOfThisFrame = transaction . bitAddrEnd \n    withPadding = not ( trimPaddingWordsOnEnd or trimPaddingWordsOnStart ) \n    if partsPending or ( withPadding and endOfThisFrame != startOfThisFrame ) : \n        endOfLastWord = ( lastWordI + 1 ) * wordWidth \n        if endOfThisFrame < endOfLastWord : \n            endOfThisFrame = endOfLastWord \n        else : \n            paddingWords = it . fullWordCnt ( endOfLastWord , endOfThisFrame ) \n            if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : \n                endOfThisFrame = endOfThisFrame - ( paddingWords * wordWidth ) \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , endOfThisFrame ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , parts ) \n        parts = [ ] \n        startOfThisFrame = endOfThisFrame \n    while withPadding and startOfThisFrame < transaction . bitAddrEnd : \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , transaction . bitAddrEnd ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , [ ] ) \n        startOfThisFrame = endOfThisFrame "}
{"6423": "\ndef walkWords ( self , showPadding : bool = False ) : \n    wIndex = 0 \n    lastEnd = self . startBitAddr \n    parts = [ ] \n    for p in self . parts : \n        end = p . startOfPart \n        if showPadding and end != lastEnd : \n            while end != lastEnd : \n                assert end >= lastEnd , ( end , lastEnd ) \n                endOfWord = ceil ( ( lastEnd + 1 ) / self . wordWidth ) * self . wordWidth \n                endOfPadding = min ( endOfWord , end ) \n                _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n                parts . append ( _p ) \n                if endOfPadding >= endOfWord : \n                    yield ( wIndex , parts ) \n                    wIndex = wIndex + ( 1 ) \n                    parts = [ ] \n                lastEnd = endOfPadding \n        if self . _wordIndx ( lastEnd ) != self . _wordIndx ( p . startOfPart ) : \n            yield ( wIndex , parts ) \n            wIndex = wIndex + ( 1 ) \n            parts = [ ] \n            lastEnd = p . endOfPart \n        parts . append ( p ) \n        lastEnd = p . endOfPart \n        if lastEnd % self . wordWidth == 0 : \n            yield ( wIndex , parts ) \n            wIndex = wIndex + ( 1 ) \n            parts = [ ] \n    if showPadding and ( parts or lastEnd != self . endBitAddr or lastEnd % self . wordWidth != 0 ) : \n        end = ceil ( self . endBitAddr / self . wordWidth ) * self . wordWidth \n        while end != lastEnd : \n            assert end >= lastEnd , ( end , lastEnd ) \n            endOfWord = ( ( lastEnd // self . wordWidth ) + 1 ) * self . wordWidth \n            endOfPadding = min ( endOfWord , end ) \n            _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n            _p . parent = self \n            parts . append ( _p ) \n            if endOfPadding >= endOfWord : \n                yield ( wIndex , parts ) \n                wIndex = wIndex + ( 1 ) \n                parts = [ ] \n            lastEnd = endOfPadding \n        if parts : \n            yield ( wIndex , parts ) "}
{"6432": "\ndef _merge_statements ( statements : List [ \"HdlStatement\" ] ) -> Tuple [ List [ \"HdlStatement\" ] , int ] : \n    order = { } \n    for i , stm in enumerate ( statements ) : \n        order [ stm ] = i \n    new_statements = [ ] \n    rank_decrease = 0 \n    for rank , stms in groupedby ( statements , lambda s : s . rank ) : \n        if rank == 0 : \n            new_statements . extend ( stms ) \n        else : \n            if len ( stms ) == 1 : \n                new_statements . extend ( stms ) \n                continue \n            for iA , stmA in enumerate ( stms ) : \n                if stmA is None : \n                    continue \n                for iB , stmB in enumerate ( islice ( stms , iA + 1 , None ) ) : \n                    if stmB is None : \n                        continue \n                    if stmA . _is_mergable ( stmB ) : \n                        rank_decrease = rank_decrease + ( stmB . rank ) \n                        stmA . _merge_with_other_stm ( stmB ) \n                        stms [ iA + 1 + iB ] = None \n                        new_statements . append ( stmA ) \n                    else : \n                        new_statements . append ( stmA ) \n                        new_statements . append ( stmB ) \n    new_statements . sort ( key = lambda stm : order [ stm ] ) \n    return new_statements , rank_decrease "}
{"6436": "\ndef _set_parent_stm ( self , parentStm : \"HdlStatement\" ) : \n    was_top = self . parentStm is None \n    self . parentStm = parentStm \n    if not self . _now_is_event_dependent and parentStm . _now_is_event_dependent : \n        self . _on_parent_event_dependent ( ) \n    topStatement = parentStm \n    while topStatement . parentStm is not None : \n        topStatement = topStatement . parentStm \n    parent_out_add = topStatement . _outputs . append \n    parent_in_add = topStatement . _inputs . append \n    if was_top : \n        for inp in self . _inputs : \n            inp . endpoints . discard ( self ) \n            inp . endpoints . append ( topStatement ) \n            parent_in_add ( inp ) \n        for outp in self . _outputs : \n            outp . drivers . discard ( self ) \n            outp . drivers . append ( topStatement ) \n            parent_out_add ( outp ) \n        ctx = self . _get_rtl_context ( ) \n        ctx . statements . discard ( self ) \n    parentStm . rank = parentStm . rank + ( self . rank ) "}
{"6443": "\ndef HStruct_unpack ( structT , data , getDataFn = None , dataWidth = None ) : \n    if getDataFn is None : \n        assert dataWidth is not None \n        def _getDataFn ( x ) : \n            return toHVal ( x ) . _auto_cast ( Bits ( dataWidth ) ) \n        getDataFn = _getDataFn \n    val = structT . fromPy ( None ) \n    fData = iter ( data ) \n    actualOffset = 0 \n    actual = None \n    for v in walkFlattenFields ( val , skipPadding = False ) : \n        required = v . _dtype . bit_length ( ) \n        if actual is None : \n            actualOffset = 0 \n            try : \n                actual = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            if dataWidth is None : \n                dataWidth = actual . _dtype . bit_length ( ) \n            actuallyHave = dataWidth \n        else : \n            actuallyHave = actual . _dtype . bit_length ( ) - actualOffset \n        while actuallyHave < required : \n            try : \n                d = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            actual = d . _concat ( actual ) \n            actuallyHave = actuallyHave + ( dataWidth ) \n        if actuallyHave >= required : \n            _v = actual [ ( required + actualOffset ) : actualOffset ] \n            _v = _v . _auto_cast ( v . _dtype ) \n            v . val = _v . val \n            v . vldMask = _v . vldMask \n            v . updateTime = _v . updateTime \n            actuallyHave = actuallyHave - ( required ) \n            actualOffset = actualOffset + ( required ) \n        if actuallyHave == 0 : \n            actual = None \n    if actual is not None : \n        assert actual . _dtype . bit_length ( ) - actualOffset < dataWidth , \"It should be just a padding at the end of frame\" \n    return val "}
{"6455": "\ndef finalize ( self ) : \n    ff_to_remove = 0 \n    res = self . resources \n    for m , addrDict in self . memories . items ( ) : \n        rwSyncPorts , rSyncPorts , wSyncPorts = 0 , 0 , 0 \n        rwAsyncPorts , rAsyncPorts , wAsyncPorts = 0 , 0 , 0 \n        rSync_wAsyncPorts , rAsync_wSyncPorts = 0 , 0 \n        for _ , ( rSync , wSync , rAsync , wAsync ) in addrDict . items ( ) : \n            if rSync : \n                ff_to_remove = ff_to_remove + ( rSync * m . _dtype . elmType . bit_length ( ) ) \n            rwSync = min ( rSync , wSync ) \n            rSync = rSync - ( rwSync ) \n            wSync = wSync - ( rwSync ) \n            rwAsync = min ( rAsync , wAsync ) \n            rAsync = rAsync - ( rwAsync ) \n            wAsync = wAsync - ( rwAsync ) \n            rSync_wAsync = min ( rSync , wAsync ) \n            rSync = rSync - ( rSync_wAsync ) \n            wAsync = wAsync - ( rSync_wAsync ) \n            rAsync_wSync = min ( rAsync , wSync ) \n            rAsync = rAsync - ( rAsync_wSync ) \n            wSync = wSync - ( rAsync_wSync ) \n            rwSyncPorts = rwSyncPorts + ( rwSync ) \n            rSyncPorts = rSyncPorts + ( rSync ) \n            wSyncPorts = wSyncPorts + ( wSync ) \n            rwAsyncPorts = rwAsyncPorts + ( rwAsync ) \n            rAsyncPorts = rAsyncPorts + ( rAsync ) \n            wAsyncPorts = wAsyncPorts + ( wAsync ) \n            rSync_wAsyncPorts = rSync_wAsyncPorts + ( rSync_wAsync ) \n            rAsync_wSyncPorts = rAsync_wSyncPorts + ( rAsync_wSync ) \n        k = ResourceRAM ( m . _dtype . elmType . bit_length ( ) , int ( m . _dtype . size ) , rwSyncPorts , rSyncPorts , wSyncPorts , rSync_wAsyncPorts , rwAsyncPorts , rAsyncPorts , wAsyncPorts , rAsync_wSyncPorts ) \n        res [ k ] = res . get ( k , 0 ) + 1 \n    self . memories . clear ( ) \n    if ff_to_remove : \n        ff_cnt = res [ ResourceFF ] \n        ff_cnt = ff_cnt - ( ff_to_remove ) \n        if ff_cnt : \n            res [ ResourceFF ] = ff_cnt \n        else : \n            del res [ ResourceFF ] "}
{"6461": "\ndef connectPacked ( srcPacked , dstInterface , exclude = None ) : \n    offset = 0 \n    connections = [ ] \n    for i in reversed ( list ( walkPhysInterfaces ( dstInterface ) ) ) : \n        if exclude is not None and i in exclude : \n            continue \n        sig = i . _sig \n        t = sig . _dtype \n        if t == BIT : \n            s = srcPacked [ offset ] \n            offset = offset + ( 1 ) \n        else : \n            w = t . bit_length ( ) \n            s = srcPacked [ ( w + offset ) : offset ] \n            offset = offset + ( w ) \n        connections . append ( sig ( s ) ) \n    return connections "}
{"6477": "\ndef _loadFromHStruct ( self , dtype : HdlType , bitAddr : int ) : \n    for f in dtype . fields : \n        t = f . dtype \n        origin = f \n        isPadding = f . name is None \n        if isPadding : \n            width = t . bit_length ( ) \n            bitAddr = bitAddr + ( width ) \n        else : \n            fi = TransTmpl ( t , bitAddr , parent = self , origin = origin ) \n            self . children . append ( fi ) \n            bitAddr = fi . bitAddrEnd \n    return bitAddr "}
{"6481": "\ndef signFix ( val , width ) : \n    if val > 0 : \n        msb = 1 << ( width - 1 ) \n        if val & msb : \n            val = val - ( mask ( width ) + 1 ) \n    return val "}
{"6493": "\ndef withIndent ( self , indent = 1 ) : \n    ctx = copy ( self ) \n    ctx . indent = ctx . indent + ( indent ) \n    return ctx "}
{"6537": "\ndef volume_up ( self ) : \n    self . _volume_level = self . _volume_level + ( self . _volume_step / self . _max_volume ) \n    self . _device . vol_up ( num = self . _volume_step ) "}
{"6538": "\ndef volume_down ( self ) : \n    self . _volume_level = self . _volume_level - ( self . _volume_step / self . _max_volume ) \n    self . _device . vol_down ( num = self . _volume_step ) "}
{"6549": "\ndef pop ( self ) : \n    move = self . move_stack . pop ( ) \n    self . transpositions . subtract ( ( self . zobrist_hash ( ) , ) ) \n    self . move_number = self . move_number - ( 1 ) \n    captured_piece_type = self . captured_piece_stack . pop ( ) \n    captured_piece_color = self . turn \n    if not move : \n        self . turn ^= 1 \n        return move \n    piece_type = self . piece_type_at ( move . to_square ) \n    if move . promotion : \n        piece_type = PIECE_PROMOTED . index ( piece_type ) \n    if move . from_square is None : \n        self . add_piece_into_hand ( piece_type , self . turn ^ 1 ) \n    else : \n        self . set_piece_at ( move . from_square , Piece ( piece_type , self . turn ^ 1 ) ) \n    if captured_piece_type : \n        self . remove_piece_from_hand ( captured_piece_type , captured_piece_color ^ 1 ) \n        self . set_piece_at ( move . to_square , Piece ( captured_piece_type , captured_piece_color ) ) \n    else : \n        self . remove_piece_at ( move . to_square ) \n    self . turn ^= 1 \n    return move "}
{"6550": "\ndef sfen ( self ) : \n    sfen = [ ] \n    empty = 0 \n    for square in SQUARES : \n        piece = self . piece_at ( square ) \n        if not piece : \n            empty = empty + ( 1 ) \n        else : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            sfen . append ( piece . symbol ( ) ) \n        if BB_SQUARES [ square ] & BB_FILE_1 : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            if square != I1 : \n                sfen . append ( '/' ) \n    sfen . append ( ' ' ) \n    if self . turn == WHITE : \n        sfen . append ( 'w' ) \n    else : \n        sfen . append ( 'b' ) \n    sfen . append ( ' ' ) \n    pih_len = 0 \n    for color in COLORS : \n        p = self . pieces_in_hand [ color ] \n        pih_len = pih_len + ( len ( p ) ) \n        for piece_type in sorted ( p . keys ( ) , reverse = True ) : \n            if p [ piece_type ] >= 1 : \n                if p [ piece_type ] > 1 : \n                    sfen . append ( str ( p [ piece_type ] ) ) \n                piece = Piece ( piece_type , color ) \n                sfen . append ( piece . symbol ( ) ) \n    if pih_len == 0 : \n        sfen . append ( '-' ) \n    sfen . append ( ' ' ) \n    sfen . append ( str ( self . move_number ) ) \n    return '' . join ( sfen ) "}
{"6561": "\ndef argparser_add_argument ( parser : argparse . ArgumentParser , config : GoodConf ) : \n    help = \"Config file.\" \n    if config . file_env_var : \n        help = help + ( ( \" Can also be configured via the \" \"environment variable: {}\" . format ( config . file_env_var ) ) ) \n    if config . default_files : \n        help = help + ( ( \" Defaults to the first file that exists from \" \"[{}].\" . format ( ', ' . join ( config . default_files ) ) ) ) \n    parser . add_argument ( '-C' , '--config' , metavar = 'FILE' , help = help ) "}
{"6570": "\ndef _compute_missing_rates ( self , currency ) : \n    rates = self . _rates [ currency ] \n    tmp = defaultdict ( lambda : [ None , None ] ) \n    for date in sorted ( rates ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist = dist + ( 1 ) \n            tmp [ date ] [ 0 ] = closest_rate , dist \n    for date in sorted ( rates , reverse = True ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist = dist + ( 1 ) \n            tmp [ date ] [ 1 ] = closest_rate , dist \n    for date in sorted ( tmp ) : \n        ( r0 , d0 ) , ( r1 , d1 ) = tmp [ date ] \n        rates [ date ] = ( r0 * d1 + r1 * d0 ) / ( d0 + d1 ) \n        if self . verbose : \n            print ( ( '{0}: filling {1} missing rate using {2} ({3}d old) and ' '{4} ({5}d later)' ) . format ( currency , date , r0 , d0 , r1 , d1 ) ) "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = True ) : \n    scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( index < 0 ) . any ( ) or ( index > n ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( index == n ) \n    index [ omegas ] = index [ omegas ] - ( 1 ) \n    offset [ omegas ] = offset [ omegas ] + ( interval_length ) \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ 0 ] = 1.0 \n    T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    for i in range ( 2 , coefficient_count ) : \n        T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2 ] \n    components = ( T . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        components = components [ : , 0 ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ 0 ] = 0.0 \n    dT [ 1 ] = 1.0 \n    if coefficient_count > 2 : \n        dT [ 2 ] = twot1 + twot1 \n        for i in range ( 3 , coefficient_count ) : \n            dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2 ] + T [ i - 1 ] + T [ i - 1 ] \n    dT = dT * ( 2.0 ) \n    dT = dT / ( interval_length ) \n    rates = ( dT . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        rates = rates [ : , 0 ] \n    return components , rates "}
{"6621": "\ndef map_entity ( self , entity : dal . Price ) -> PriceModel : \n    if not entity : \n        return None \n    result = PriceModel ( ) \n    result . currency = entity . currency \n    dt_string = entity . date \n    format_string = \"%Y-%m-%d\" \n    if entity . time : \n        dt_string = dt_string + ( f\"T{entity.time}\" ) \n        format_string = format_string + ( \"T%H:%M:%S\" ) \n    price_datetime = datetime . strptime ( dt_string , format_string ) \n    result . datum = Datum ( ) \n    result . datum . from_datetime ( price_datetime ) \n    assert isinstance ( result . datum , Datum ) \n    result . symbol = SecuritySymbol ( entity . namespace , entity . symbol ) \n    value = Decimal ( entity . value ) / Decimal ( entity . denom ) \n    result . value = Decimal ( value ) \n    return result "}
{"6638": "\ndef prune_all ( self ) -> int : \n    from . repositories import PriceRepository \n    repo = PriceRepository ( ) \n    items = repo . query . distinct ( dal . Price . namespace , dal . Price . symbol ) . all ( ) \n    count = 0 \n    for item in items : \n        symbol = SecuritySymbol ( item . namespace , item . symbol ) \n        deleted = self . prune ( symbol ) \n        if deleted : \n            count = count + ( 1 ) \n    return count "}
{"6694": "\ndef stream ( url , headers , stream_to = None , retry = True ) : \n    bot . debug ( \"GET %s\" % url ) \n    if DISABLE_SSL_CHECK is True : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    response = requests . get ( url , headers = headers , verify = not DISABLE_SSL_CHECK , stream = True ) \n    if response . status_code in [ 401 , 403 ] : \n        headers = update_token ( headers ) \n        return stream ( url , headers , stream_to , retry = False ) \n    elif response . status_code == 200 : \n        content_size = None \n        if 'Content-Length' in response . headers : \n            progress = 0 \n            content_size = int ( response . headers [ 'Content-Length' ] ) \n            bot . show_progress ( progress , content_size , length = 35 ) \n        chunk_size = 1 << 20 \n        with open ( stream_to , 'wb' ) as filey : \n            for chunk in response . iter_content ( chunk_size = chunk_size ) : \n                filey . write ( chunk ) \n                if content_size is not None : \n                    progress = progress + ( chunk_size ) \n                    bot . show_progress ( iteration = progress , total = content_size , length = 35 , carriage_return = False ) \n        sys . stdout . write ( '\\n' ) \n        return stream_to \n    bot . error ( \"Problem with stream, response %s\" % ( response . status_code ) ) \n    sys . exit ( 1 ) "}
{"6816": "\ndef canonicalize ( self , mol ) : \n    tautomers = self . _enumerate_tautomers ( mol ) \n    if len ( tautomers ) == 1 : \n        return tautomers [ 0 ] \n    highest = None \n    for t in tautomers : \n        smiles = Chem . MolToSmiles ( t , isomericSmiles = True ) \n        log . debug ( 'Tautomer: %s' , smiles ) \n        score = 0 \n        ssr = Chem . GetSymmSSSR ( t ) \n        for ring in ssr : \n            btypes = { t . GetBondBetweenAtoms ( * pair ) . GetBondType ( ) for pair in pairwise ( ring ) } \n            elements = { t . GetAtomWithIdx ( idx ) . GetAtomicNum ( ) for idx in ring } \n            if btypes == { BondType . AROMATIC } : \n                log . debug ( 'Score +100 (aromatic ring)' ) \n                score = score + ( 100 ) \n                if elements == { 6 } : \n                    log . debug ( 'Score +150 (carbocyclic aromatic ring)' ) \n                    score = score + ( 150 ) \n        for tscore in self . scores : \n            for match in t . GetSubstructMatches ( tscore . smarts ) : \n                log . debug ( 'Score %+d (%s)' , tscore . score , tscore . name ) \n                score = score + ( tscore . score ) \n        for atom in t . GetAtoms ( ) : \n            if atom . GetAtomicNum ( ) in { 15 , 16 , 34 , 52 } : \n                hs = atom . GetTotalNumHs ( ) \n                if hs : \n                    log . debug ( 'Score %+d (%s-H bonds)' , - hs , atom . GetSymbol ( ) ) \n                    score = score - ( hs ) \n        if not highest or highest [ 'score' ] < score or ( highest [ 'score' ] == score and smiles < highest [ 'smiles' ] ) : \n            log . debug ( 'New highest tautomer: %s (%s)' , smiles , score ) \n            highest = { 'smiles' : smiles , 'tautomer' : t , 'score' : score } \n    return highest [ 'tautomer' ] "}
{"6831": "\ndef choose ( self , mol ) : \n    log . debug ( 'Running LargestFragmentChooser' ) \n    fragments = Chem . GetMolFrags ( mol , asMols = True ) \n    largest = None \n    for f in fragments : \n        smiles = Chem . MolToSmiles ( f , isomericSmiles = True ) \n        log . debug ( 'Fragment: %s' , smiles ) \n        organic = is_organic ( f ) \n        if self . prefer_organic : \n            if largest and largest [ 'organic' ] and not organic : \n                continue \n            if largest and organic and not largest [ 'organic' ] : \n                largest = None \n        atoms = 0 \n        for a in f . GetAtoms ( ) : \n            atoms = atoms + ( 1 + a . GetTotalNumHs ( ) ) \n        if largest and atoms < largest [ 'atoms' ] : \n            continue \n        weight = rdMolDescriptors . CalcExactMolWt ( f ) \n        if largest and atoms == largest [ 'atoms' ] and weight < largest [ 'weight' ] : \n            continue \n        if largest and atoms == largest [ 'atoms' ] and weight == largest [ 'weight' ] and smiles > largest [ 'smiles' ] : \n            continue \n        log . debug ( 'New largest fragment: %s (%s)' , smiles , atoms ) \n        largest = { 'smiles' : smiles , 'fragment' : f , 'atoms' : atoms , 'weight' : weight , 'organic' : organic } \n    return largest [ 'fragment' ] "}
{"6837": "\ndef get_total_contributors ( self , repo ) : \n    repo_contributors = 0 \n    for contributor in repo . iter_contributors ( ) : \n        repo_contributors = repo_contributors + ( 1 ) \n        self . unique_contributors [ contributor . id ] . append ( repo . name ) \n        self . contributors_json [ repo . name ] . append ( contributor . to_json ( ) ) \n    return repo_contributors "}
{"6838": "\ndef get_pull_reqs ( self , repo ) : \n    pull_reqs_open = 0 \n    pull_reqs_closed = 0 \n    for pull_request in repo . iter_pulls ( state = 'all' ) : \n        self . pull_requests_json [ repo . name ] . append ( pull_request . to_json ( ) ) \n        if pull_request . closed_at is not None : \n            pull_reqs_closed = pull_reqs_closed + ( 1 ) \n        else : \n            pull_reqs_open = pull_reqs_open + ( 1 ) \n    return pull_reqs_open , pull_reqs_closed "}
{"6839": "\ndef get_issues ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_issues = repo . iter_issues ( state = 'all' ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2 : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_issues = repo . iter_issues ( state = 'all' ) \n                is_only_today = True \n        if not is_only_today : \n            all_issues = repo . iter_issues ( since = date , state = 'all' ) \n    for issue in all_issues : \n        self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) \n    closed_issues = 0 \n    for issue in repo . iter_issues ( state = 'closed' ) : \n        if issue is not None : \n            closed_issues = closed_issues + ( 1 ) \n    return closed_issues "}
{"6842": "\ndef get_commits ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_commits = repo . iter_commits ( ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2 : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_commits = repo . iter_commits ( ) \n                is_only_today = True \n        if not is_only_today : \n            all_commits = repo . iter_commits ( since = date ) \n    for commit in all_commits : \n        self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) \n    count = 0 \n    for commit in repo . iter_commits ( ) : \n        count = count + ( 1 ) \n    return count "}
{"6864": "\ndef check_data_redundancy ( self , file_path = '' , dict_to_check = { } ) : \n    count = 0 \n    exists = os . path . isfile ( file_path ) \n    previous_dates = { } \n    if exists : \n        with open ( file_path , 'r' ) as input : \n            input . readline ( ) \n            for row in csv . reader ( input ) : \n                timestamp = calendar . timegm ( time . strptime ( row [ 0 ] , '%Y-%m-%d' ) ) \n                if timestamp in dict_to_check : \n                    del dict_to_check [ timestamp ] \n                count = count + ( 1 ) \n        input . close ( ) \n    return count "}
{"6865": "\ndef write_data_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , name = '' , row_count = 0 ) : \n    exists = os . path . isfile ( file_path ) \n    with open ( file_path , 'a' ) as out : \n        if not exists : \n            out . write ( 'date,organization,' + name + ',unique_' + name + ',id\\n' ) \n        sorted_dict = sorted ( dict_to_write ) \n        for day in sorted_dict : \n            day_formatted = datetime . datetime . utcfromtimestamp ( day ) . strftime ( '%Y-%m-%d' ) \n            out . write ( day_formatted + ',' + organization + ',' + str ( dict_to_write [ day ] [ 0 ] ) + ',' + str ( dict_to_write [ day ] [ 1 ] ) + ',' + str ( row_count ) + '\\n' ) \n            row_count = row_count + ( 1 ) "}
{"6894": "\ndef calc_total_commits ( self , starting_commits = 0 ) : \n    for week_of_commits in self . commits_dict_list : \n        try : \n            self . commits [ week_of_commits [ 'week' ] ] = self . commits [ week_of_commits [ 'week' ] ] - ( week_of_commits [ 'total' ] ) \n        except KeyError : \n            total = self . commits [ week_of_commits [ 'week' ] ] = - week_of_commits [ 'total' ] \n    self . sorted_weeks = sorted ( self . commits ) \n    for week in reversed ( self . sorted_weeks ) : \n        self . commits [ week ] = self . commits [ week ] + starting_commits \n        starting_commits = self . commits [ week ] "}
{"6912": "\ndef mark ( self , value = 1 ) : \n    self . counter = self . counter + ( value ) \n    self . m1_rate . update ( value ) \n    self . m5_rate . update ( value ) \n    self . m15_rate . update ( value ) "}
{"6918": "\ndef _buffered_send_metric ( self , metric_str ) : \n    self . batch_count = self . batch_count + ( 1 ) \n    self . batch_buffer = self . batch_buffer + ( metric_str ) \n    if self . batch_count >= self . batch_size : \n        self . _send ( ) "}
{"6942": "\ndef success ( self ) : \n    if self . interval == 0.0 : \n        return \n    self . short_interval = self . short_interval - ( self . short_unit ) \n    self . long_interval = self . long_interval - ( self . long_unit ) \n    self . short_interval = max ( self . short_interval , Decimal ( 0 ) ) \n    self . long_interval = max ( self . long_interval , Decimal ( 0 ) ) \n    self . update_interval ( ) "}
{"6943": "\ndef failure ( self ) : \n    self . short_interval = self . short_interval + ( self . short_unit ) \n    self . long_interval = self . long_interval + ( self . long_unit ) \n    self . short_interval = min ( self . short_interval , self . max_short_timer ) \n    self . long_interval = min ( self . long_interval , self . max_long_timer ) \n    self . update_interval ( ) "}
{"6957": "\ndef parse_log ( log_file ) : \n    template = OrderedDict ( [ ( \"clean_len\" , 0 ) , ( \"total_trim\" , 0 ) , ( \"total_trim_perc\" , 0 ) , ( \"5trim\" , 0 ) , ( \"3trim\" , 0 ) , ( \"bad_reads\" , 0 ) ] ) \n    with open ( log_file ) as fh : \n        for line in fh : \n            fields = [ int ( x ) for x in line . strip ( ) . split ( ) [ - 4 : ] ] \n            if not fields [ 0 ] : \n                template [ \"bad_reads\" ] = template [ \"bad_reads\" ] + ( 1 ) \n            template [ \"5trim\" ] = template [ \"5trim\" ] + ( fields [ 1 ] ) \n            template [ \"3trim\" ] = template [ \"3trim\" ] + ( fields [ 3 ] ) \n            template [ \"total_trim\" ] = template [ \"total_trim\" ] + ( fields [ 1 ] + fields [ 3 ] ) \n            template [ \"clean_len\" ] = template [ \"clean_len\" ] + ( fields [ 0 ] ) \n        total_len = template [ \"clean_len\" ] + template [ \"total_trim\" ] \n        if total_len : \n            template [ \"total_trim_perc\" ] = round ( ( template [ \"total_trim\" ] / total_len ) * 100 , 2 ) \n        else : \n            template [ \"total_trim_perc\" ] = 0 \n    return template "}
{"6960": "\ndef main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , clear ) : \n    logger . info ( \"Starting trimmomatic\" ) \n    cli = [ \"java\" , \"-Xmx{}\" . format ( \"$task.memory\" [ : - 1 ] . lower ( ) . replace ( \" \" , \"\" ) ) , \"-jar\" , TRIM_PATH . strip ( ) , \"PE\" , \"-threads\" , \"$task.cpus\" ] \n    try : \n        phred = int ( phred ) \n        phred_flag = \"-phred{}\" . format ( str ( phred ) ) \n        cli = cli + ( [ phred_flag ] ) \n    except ValueError : \n        pass \n    cli = cli + ( fastq_pair ) \n    output_names = [ ] \n    for i in range ( len ( fastq_pair ) ) : \n        output_names . append ( \"{}_{}_trim.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n        output_names . append ( \"{}_{}_U.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n    cli = cli + ( output_names ) \n    if trim_range != [ \"None\" ] : \n        cli = cli + ( [ \"CROP:{}\" . format ( trim_range [ 1 ] ) , \"HEADCROP:{}\" . format ( trim_range [ 0 ] ) , ] ) \n    if os . path . exists ( adapters_file ) : \n        logger . debug ( \"Using the provided adapters file '{}'\" . format ( adapters_file ) ) \n    else : \n        logger . debug ( \"Adapters file '{}' not provided or does not exist. Using\" \" default adapters\" . format ( adapters_file ) ) \n        adapters_file = merge_default_adapters ( ) \n    cli = cli + ( [ \"ILLUMINACLIP:{}:3:30:10:6:true\" . format ( adapters_file ) ] ) \n    logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , \"{}_trimlog.txt\" . format ( sample_id ) ) \n    cli = cli + ( [ \"SLIDINGWINDOW:{}\" . format ( trim_opts [ 0 ] ) , \"LEADING:{}\" . format ( trim_opts [ 1 ] ) , \"TRAILING:{}\" . format ( trim_opts [ 2 ] ) , \"MINLEN:{}\" . format ( trim_opts [ 3 ] ) , \"TOPHRED33\" , \"-trimlog\" , logfile ] ) \n    logger . debug ( \"Running trimmomatic subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished trimmomatic subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Finished trimmomatic subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished trimmomatic with return code: {}\" . format ( p . returncode ) ) \n    trimmomatic_log ( logfile , sample_id ) \n    if p . returncode == 0 and os . path . exists ( \"{}_1_trim.fastq.gz\" . format ( SAMPLE_ID ) ) : \n        clean_up ( fastq_pair , clear ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        if p . returncode != 0 : \n            status_fh . write ( \"fail\" ) \n            return \n        else : \n            status_fh . write ( \"pass\" ) "}
{"6972": "\ndef set_compiler_channels ( self , channel_list , operator = \"mix\" ) : \n    if not channel_list : \n        raise eh . ProcessError ( \"At least one status channel must be \" \"provided to include this process in the \" \"pipeline\" ) \n    if len ( channel_list ) == 1 : \n        logger . debug ( \"Setting only one status channel: {}\" . format ( channel_list [ 0 ] ) ) \n        self . _context = { \"compile_channels\" : channel_list [ 0 ] } \n    else : \n        first_status = channel_list [ 0 ] \n        if operator == \"mix\" : \n            lst = \",\" . join ( channel_list [ 1 : ] ) \n            s = \"{}.mix({})\" . format ( first_status , lst ) \n        elif operator == \"join\" : \n            s = first_status \n            for ch in channel_list [ 1 : ] : \n                s = s + ( \".join({})\" . format ( ch ) ) \n            s = s + ( \".map{ ot -> [ ot[0], ot[1..-1] ] }\" ) \n        logger . debug ( \"Status channel string: {}\" . format ( s ) ) \n        self . _context = { \"compile_channels\" : s } "}
{"6977": "\ndef _parse_assembly ( self , assembly_file ) : \n    seq_temp = [ ] \n    contig_id = 0 \n    cov , header = None , None \n    with open ( assembly_file ) as fh : \n        logger . debug ( \"Starting iteration of assembly file: {}\" . format ( assembly_file ) ) \n        for line in fh : \n            if not line . strip ( ) : \n                continue \n            else : \n                line = line . strip ( ) \n            if line . startswith ( \">\" ) : \n                if seq_temp : \n                    seq = \"\" . join ( seq_temp ) \n                    logger . debug ( \"Populating contig with contig_id '{}', \" \"header '{}' and cov '{}'\" . format ( contig_id , header , cov ) ) \n                    self . _populate_contigs ( contig_id , header , cov , seq ) \n                    seq_temp = [ ] \n                    contig_id = contig_id + ( 1 ) \n                header = line [ 1 : ] \n                cov = self . _parse_coverage ( line ) \n            else : \n                seq_temp . append ( line ) \n        logger . debug ( \"Populating contig with contig_id '{}', \" \"header '{}' and cov '{}'\" . format ( contig_id , header , cov ) ) \n        seq = \"\" . join ( seq_temp ) \n        self . _populate_contigs ( contig_id , header , cov , seq ) "}
{"6986": "\ndef parse_pipeline ( pipeline_str ) : \n    if os . path . exists ( pipeline_str ) : \n        logger . debug ( \"Found pipeline file: {}\" . format ( pipeline_str ) ) \n        with open ( pipeline_str ) as fh : \n            pipeline_str = \"\" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) \n    logger . info ( colored_print ( \"Resulting pipeline string:\\n\" ) ) \n    logger . info ( colored_print ( pipeline_str + \"\\n\" ) ) \n    insanity_checks ( pipeline_str ) \n    logger . debug ( \"Parsing pipeline string: {}\" . format ( pipeline_str ) ) \n    pipeline_links = [ ] \n    lane = 1 \n    pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( pipeline_str ) \n    nforks = pipeline_str_modified . count ( FORK_TOKEN ) \n    logger . debug ( \"Found {} fork(s)\" . format ( nforks ) ) \n    if not nforks : \n        logger . debug ( \"Detected linear pipeline string : {}\" . format ( pipeline_str ) ) \n        linear_pipeline = [ \"__init__\" ] + pipeline_str_modified . split ( ) \n        pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) \n        pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n        return pipeline_links \n    for i in range ( nforks ) : \n        logger . debug ( \"Processing fork {} in lane {}\" . format ( i , lane ) ) \n        fields = pipeline_str_modified . split ( FORK_TOKEN , i + 1 ) \n        previous_process = fields [ - 2 ] . split ( LANE_TOKEN ) [ - 1 ] . split ( ) \n        logger . debug ( \"Previous processes string: {}\" . format ( fields [ - 2 ] ) ) \n        logger . debug ( \"Previous processes list: {}\" . format ( previous_process ) ) \n        next_lanes = get_lanes ( fields [ - 1 ] ) \n        logger . debug ( \"Next lanes object: {}\" . format ( next_lanes ) ) \n        fork_sink = [ x [ 0 ] for x in next_lanes ] \n        logger . debug ( \"The fork sinks into the processes: {}\" . format ( fork_sink ) ) \n        if i == 0 : \n            if not previous_process : \n                previous_process = [ \"__init__\" ] \n                lane = 0 \n            else : \n                previous_process = [ \"__init__\" ] + previous_process \n            pipeline_links . extend ( linear_connection ( previous_process , lane ) ) \n        fork_source = previous_process [ - 1 ] \n        logger . debug ( \"Fork source is set to: {}\" . format ( fork_source ) ) \n        fork_lane = get_source_lane ( previous_process , pipeline_links ) \n        logger . debug ( \"Fork lane is set to: {}\" . format ( fork_lane ) ) \n        pipeline_links . extend ( fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) \n        pipeline_links . extend ( linear_lane_connection ( next_lanes , lane ) ) \n        lane = lane + ( len ( fork_sink ) ) \n    pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n    return pipeline_links "}
{"6988": "\ndef get_lanes ( lanes_str ) : \n    logger . debug ( \"Parsing lanes from raw string: {}\" . format ( lanes_str ) ) \n    parsed_lanes = \"\" \n    infork = 0 \n    for i in lanes_str : \n        if i == FORK_TOKEN : \n            infork = infork + ( 1 ) \n        if i == CLOSE_TOKEN : \n            infork = infork - ( 1 ) \n        if infork < 0 : \n            break \n        if infork == 0 : \n            if i not in [ FORK_TOKEN , CLOSE_TOKEN ] : \n                parsed_lanes = parsed_lanes + ( i ) \n    return [ x . split ( ) for x in parsed_lanes . split ( LANE_TOKEN ) ] "}
{"6990": "\ndef fork_connection ( source , sink , source_lane , lane ) : \n    logger . debug ( \"Establishing forking of source '{}' into processes\" \" '{}'. Source lane set to '{}' and lane set to '{}'\" . format ( source , sink , source_lane , lane ) ) \n    res = [ ] \n    lane_counter = lane + 1 \n    for p in sink : \n        res . append ( { \"input\" : { \"process\" : source , \"lane\" : source_lane } , \"output\" : { \"process\" : p , \"lane\" : lane_counter } } ) \n        lane_counter = lane_counter + ( 1 ) \n    return res "}
{"7004": "\ndef update_inspection ( self ) : \n    try : \n        self . log_parser ( ) \n    except ( FileNotFoundError , StopIteration ) as e : \n        logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ 0 ] ) ) \n        self . log_retry = self . log_retry + ( 1 ) \n        if self . log_retry == self . MAX_RETRIES : \n            raise e \n    try : \n        self . trace_parser ( ) \n    except ( FileNotFoundError , StopIteration ) as e : \n        logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ 0 ] ) ) \n        self . trace_retry = self . trace_retry + ( 1 ) \n        if self . trace_retry == self . MAX_RETRIES : \n            raise e "}
{"7006": "\ndef _updown ( self , direction ) : \n    if direction == \"up\" and self . top_line != 0 : \n        self . top_line = self . top_line - ( 1 ) \n    elif direction == \"down\" and self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3 : \n        self . top_line = self . top_line + ( 1 ) "}
{"7007": "\ndef _rightleft ( self , direction ) : \n    if direction == \"left\" and self . padding != 0 : \n        self . padding = self . padding - ( 1 ) \n    if direction == \"right\" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max_width : \n        self . padding = self . padding + ( 1 ) "}
{"7013": "\ndef main ( sample_id , assembly , min_size ) : \n    logger . info ( \"Starting script\" ) \n    f_open = open ( assembly , \"rU\" ) \n    entry = ( x [ 1 ] for x in groupby ( f_open , lambda line : line [ 0 ] == \">\" ) ) \n    success = 0 \n    for header in entry : \n        headerStr = header . __next__ ( ) [ 1 : ] . strip ( ) \n        seq = \"\" . join ( s . strip ( ) for s in entry . __next__ ( ) ) \n        if len ( seq ) >= min_size : \n            with open ( sample_id + '_' + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + '.fasta' , \"w\" ) as output_file : \n                output_file . write ( \">\" + sample_id + \"_\" + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + \"\\\\n\" + seq + \"\\\\n\" ) \n                success = success + ( 1 ) \n    f_open . close ( ) \n    logger . info ( \"{} sequences sucessfully splitted.\" . format ( success ) ) "}
{"7019": "\ndef build_upstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 1 ] is not None : \n            if len ( process_descriptions [ task ] [ 1 ] . split ( \"|\" ) ) > 1 : \n                local_forks = process_descriptions [ task ] [ 1 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks = count_forks + ( 1 ) \n                        task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 1 ] in total_tasks : \n                    task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_upstream ( process_descriptions , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                else : \n                    logger . error ( colored_print ( \"{} not in provided protocols as \" \"input for {}\" . format ( process_descriptions [ task ] [ 1 ] , task ) , \"red_bold\" ) ) \n                    sys . exit ( ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7020": "\ndef build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 2 ] is not None : \n            if len ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) ) > 1 : \n                local_forks = process_descriptions [ task ] [ 2 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks = count_forks + ( 1 ) \n                        task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 2 ] in total_tasks : \n                    task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7029": "\ndef _parser ( self , fl ) : \n    with open ( fl ) as fh : \n        for line in fh : \n            if line . startswith ( \"#\" ) or line . strip ( ) == \"\" : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            try : \n                coverage = float ( fields [ 8 ] ) \n            except ValueError : \n                coverage = None \n            try : \n                identity = float ( fields [ 9 ] ) \n            except ValueError : \n                identity = None \n            try : \n                accession = fields [ 11 ] \n            except IndexError : \n                accession = None \n            self . storage [ self . _key ] = { \"log_file\" : os . path . basename ( fl ) , \"infile\" : fields [ 0 ] , \"reference\" : fields [ 1 ] , \"seq_range\" : ( int ( fields [ 2 ] ) , int ( fields [ 3 ] ) ) , \"gene\" : fields [ 4 ] , \"accession\" : accession , \"database\" : fields [ 10 ] , \"coverage\" : coverage , \"identity\" : identity } \n            self . _key = self . _key + ( 1 ) "}
{"7036": "\ndef get_summary_stats ( self , output_csv = None ) : \n    contig_size_list = [ ] \n    self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) \n    for contig_id , sequence in self . contigs . items ( ) : \n        logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) \n        contig_len = len ( sequence ) \n        contig_size_list . append ( contig_len ) \n        self . summary_info [ \"total_len\" ] = self . summary_info [ \"total_len\" ] + ( contig_len ) \n        self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) \n        self . summary_info [ \"missing_data\" ] = self . summary_info [ \"missing_data\" ] + ( sequence . count ( \"N\" ) ) \n    logger . debug ( \"Getting average contig size\" ) \n    self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) \n    logger . debug ( \"Getting average GC content\" ) \n    self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) \n    logger . debug ( \"Getting N50\" ) \n    cum_size = 0 \n    for l in sorted ( contig_size_list , reverse = True ) : \n        cum_size = cum_size + ( l ) \n        if cum_size >= self . summary_info [ \"total_len\" ] / 2 : \n            self . summary_info [ \"n50\" ] = l \n            break \n    if output_csv : \n        logger . debug ( \"Writing report to csv\" ) \n        with open ( output_csv , \"w\" ) as fh : \n            summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) \n            fh . write ( summary_line ) "}
{"7037": "\ndef _get_window_labels ( self , window ) : \n    if not self . summary_info : \n        self . get_summary_stats ( ) \n    c = 0 \n    xbars = [ ] \n    for contig , seq in self . contigs . items ( ) : \n        contig_id = self . _get_contig_id ( contig ) \n        self . contig_boundaries [ contig_id ] = [ c , c + len ( seq ) ] \n        c = c + ( len ( seq ) ) \n        xbars . append ( ( contig_id , c , contig ) ) \n    return xbars "}
{"7040": "\ndef main ( sample_id , fastq_pair , clear ) : \n    logger . info ( \"Starting skesa\" ) \n    if \"_trim.\" in fastq_pair [ 0 ] : \n        sample_id = sample_id + ( \"_trim\" ) \n    version = __get_version_skesa ( ) [ \"version\" ] \n    output_file = \"{}_skesa{}.fasta\" . format ( sample_id , version . replace ( \".\" , \"\" ) ) \n    cli = [ \"skesa\" , \"--fastq\" , \"{},{}\" . format ( fastq_pair [ 0 ] , fastq_pair [ 1 ] ) , \"--gz\" , \"--use_paired_ends\" , \"--cores\" , \"${task.cpus}\" ] \n    logger . debug ( \"Running Skesa subprocess with command: {}\" . format ( cli ) ) \n    with open ( output_file , \"w\" ) as fh : \n        p = subprocess . Popen ( cli , stdout = fh , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished Skesa subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished Skesa subprocess with STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished Skesa with return code: {}\" . format ( p . returncode ) ) \n    if clear == \"true\" and os . path . exists ( output_file ) : \n        clean_up ( fastq_pair ) \n    with open ( \".status\" , \"w\" ) as fh : \n        if p . returncode != 0 : \n            fh . write ( \"error\" ) \n            raise SystemExit ( p . returncode ) \n        else : \n            fh . write ( \"pass\" ) "}
{"7051": "\ndef _build_header ( self ) : \n    logger . debug ( \"===============\" ) \n    logger . debug ( \"Building header\" ) \n    logger . debug ( \"===============\" ) \n    self . template = self . template + ( hs . header ) "}
{"7052": "\ndef _build_footer ( self ) : \n    logger . debug ( \"===============\" ) \n    logger . debug ( \"Building header\" ) \n    logger . debug ( \"===============\" ) \n    self . template = self . template + ( fs . footer ) "}
{"7057": "\ndef _get_resources_string ( res_dict , pid ) : \n    config_str = \"\" \n    ignore_directives = [ \"container\" , \"version\" ] \n    for p , directives in res_dict . items ( ) : \n        for d , val in directives . items ( ) : \n            if d in ignore_directives : \n                continue \n            config_str = config_str + ( '\\n\\t${}_{}.{} = {}' . format ( p , pid , d , val ) ) \n    return config_str "}
{"7058": "\ndef _get_container_string ( cont_dict , pid ) : \n    config_str = \"\" \n    for p , directives in cont_dict . items ( ) : \n        container = \"\" \n        if \"container\" in directives : \n            container = container + ( directives [ \"container\" ] ) \n            if \"version\" in directives : \n                container = container + ( \":{}\" . format ( directives [ \"version\" ] ) ) \n            else : \n                container = container + ( \":latest\" ) \n        if container : \n            config_str = config_str + ( '\\n\\t${}_{}.container = \"{}\"' . format ( p , pid , container ) ) \n    return config_str "}
{"7059": "\ndef _get_params_string ( self ) : \n    params_str = \"\" \n    for p in self . processes : \n        logger . debug ( \"[{}] Adding parameters: {}\\n\" . format ( p . template , p . params ) ) \n        if p . params and p . template != \"init\" : \n            p . set_param_id ( \"_{}\" . format ( p . pid ) ) \n            params_str = params_str + ( \"\\n\\t/*\" ) \n            params_str = params_str + ( \"\\n\\tComponent '{}_{}'\\n\" . format ( p . template , p . pid ) ) \n            params_str = params_str + ( \"\\t{}\\n\" . format ( \"-\" * ( len ( p . template ) + len ( p . pid ) + 12 ) ) ) \n            params_str = params_str + ( \"\\t*/\\n\" ) \n        for param , val in p . params . items ( ) : \n            if p . template == \"init\" : \n                param_id = param \n            else : \n                param_id = \"{}_{}\" . format ( param , p . pid ) \n            params_str = params_str + ( \"\\t{} = {}\\n\" . format ( param_id , val [ \"default\" ] ) ) \n    return params_str "}
{"7061": "\ndef _get_manifest_string ( self ) : \n    config_str = \"\" \n    config_str = config_str + ( '\\n\\tname = \"{}\"' . format ( self . pipeline_name ) ) \n    config_str = config_str + ( '\\n\\tmainScript = \"{}\"' . format ( self . nf_file ) ) \n    return config_str "}
{"7062": "\ndef _set_configurations ( self ) : \n    logger . debug ( \"======================\" ) \n    logger . debug ( \"Setting configurations\" ) \n    logger . debug ( \"======================\" ) \n    resources = \"\" \n    containers = \"\" \n    params = \"\" \n    manifest = \"\" \n    if self . merge_params : \n        params = params + ( self . _get_merged_params_string ( ) ) \n        help_list = self . _get_merged_params_help ( ) \n    else : \n        params = params + ( self . _get_params_string ( ) ) \n        help_list = self . _get_params_help ( ) \n    for p in self . processes : \n        if not p . directives : \n            continue \n        logger . debug ( \"[{}] Adding directives: {}\" . format ( p . template , p . directives ) ) \n        resources = resources + ( self . _get_resources_string ( p . directives , p . pid ) ) \n        containers = containers + ( self . _get_container_string ( p . directives , p . pid ) ) \n    manifest = self . _get_manifest_string ( ) \n    self . resources = self . _render_config ( \"resources.config\" , { \"process_info\" : resources } ) \n    self . containers = self . _render_config ( \"containers.config\" , { \"container_info\" : containers } ) \n    self . params = self . _render_config ( \"params.config\" , { \"params_info\" : params } ) \n    self . manifest = self . _render_config ( \"manifest.config\" , { \"manifest_info\" : manifest } ) \n    self . help = self . _render_config ( \"Helper.groovy\" , { \"nf_file\" : basename ( self . nf_file ) , \"help_list\" : help_list , \"version\" : __version__ , \"pipeline_name\" : \" \" . join ( [ x . upper ( ) for x in self . pipeline_name ] ) } ) \n    self . user_config = self . _render_config ( \"user.config\" , { } ) "}
{"7064": "\ndef render_pipeline ( self ) : \n    dict_viz = { \"name\" : \"root\" , \"children\" : [ ] } \n    last_of_us = { } \n    f_tree = self . _fork_tree if self . _fork_tree else { 1 : [ 1 ] } \n    for x , ( k , v ) in enumerate ( f_tree . items ( ) ) : \n        for p in self . processes [ 1 : ] : \n            if x == 0 and p . lane not in [ k ] + v : \n                continue \n            if x > 0 and p . lane not in v : \n                continue \n            if not p . parent_lane : \n                lst = dict_viz [ \"children\" ] \n            else : \n                lst = last_of_us [ p . parent_lane ] \n            tooltip = { \"name\" : \"{}_{}\" . format ( p . template , p . pid ) , \"process\" : { \"pid\" : p . pid , \"input\" : p . input_type , \"output\" : p . output_type if p . output_type else \"None\" , \"lane\" : p . lane , } , \"children\" : [ ] } \n            dir_var = \"\" \n            for k2 , v2 in p . directives . items ( ) : \n                dir_var = dir_var + ( k2 ) \n                for d in v2 : \n                    try : \n                        directive = v2 [ d ] . replace ( \"'\" , \"\" ) . replace ( '\"' , '' ) if isinstance ( v2 [ d ] , str ) else v2 [ d ] \n                        dir_var = dir_var + ( \"{}: {}\" . format ( d , directive ) ) \n                    except KeyError : \n                        pass \n            if dir_var : \n                tooltip [ \"process\" ] [ \"directives\" ] = dir_var \n            else : \n                tooltip [ \"process\" ] [ \"directives\" ] = \"N/A\" \n            lst . append ( tooltip ) \n            last_of_us [ p . lane ] = lst [ - 1 ] [ \"children\" ] \n    self . dag_to_file ( dict_viz ) \n    with open ( os . path . join ( dirname ( self . nf_file ) , \".forkTree.json\" ) , \"w\" ) as fh : \n        fh . write ( json . dumps ( self . _fork_tree ) ) \n    return self . _render_config ( \"pipeline_graph.html\" , { \"data\" : dict_viz } ) "}
{"7069": "\ndef build ( self ) : \n    logger . info ( colored_print ( \"\\tSuccessfully connected {} process(es) with {} \" \"fork(s) across {} lane(s) \\u2713\" . format ( len ( self . processes [ 1 : ] ) , len ( self . _fork_tree ) , self . lanes ) ) ) \n    self . _build_header ( ) \n    self . _set_channels ( ) \n    self . _set_init_process ( ) \n    self . _set_secondary_channels ( ) \n    logger . info ( colored_print ( \"\\tSuccessfully set {} secondary channel(s) \\u2713\" . format ( len ( self . secondary_channels ) ) ) ) \n    self . _set_compiler_channels ( ) \n    self . _set_configurations ( ) \n    logger . info ( colored_print ( \"\\tFinished configurations \\u2713\" ) ) \n    for p in self . processes : \n        self . template = self . template + ( \"\\n{}\" . format ( p . template_str ) ) \n    self . _build_footer ( ) \n    project_root = dirname ( self . nf_file ) \n    self . write_configs ( project_root ) \n    with open ( self . nf_file , \"w\" ) as fh : \n        fh . write ( self . template ) \n    logger . info ( colored_print ( \"\\tPipeline written into {} \\u2713\" . format ( self . nf_file ) ) ) "}
{"7071": "\ndef main ( sample_id , fastq_pair , max_len , kmer , clear ) : \n    logger . info ( \"Starting spades\" ) \n    logger . info ( \"Setting SPAdes kmers\" ) \n    kmers = set_kmers ( kmer , max_len ) \n    logger . info ( \"SPAdes kmers set to: {}\" . format ( kmers ) ) \n    cli = [ \"metaspades.py\" , \"--only-assembler\" , \"--threads\" , \"$task.cpus\" , \"-o\" , \".\" ] \n    if kmers : \n        cli = cli + ( [ \"-k {}\" . format ( \",\" . join ( [ str ( x ) for x in kmers ] ) ) ] ) \n    cli = cli + ( [ \"-1\" , fastq_pair [ 0 ] , \"-2\" , fastq_pair [ 1 ] ] ) \n    logger . debug ( \"Running metaSPAdes subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished metaSPAdes subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished metaSPAdes subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished metaSPAdes with return code: {}\" . format ( p . returncode ) ) \n    with open ( \".status\" , \"w\" ) as fh : \n        if p . returncode != 0 : \n            fh . write ( \"error\" ) \n            return \n        else : \n            fh . write ( \"pass\" ) \n    if \"_trim.\" in fastq_pair [ 0 ] : \n        sample_id = sample_id + ( \"_trim\" ) \n    assembly_file = \"{}_metaspades.fasta\" . format ( sample_id ) \n    os . rename ( \"contigs.fasta\" , assembly_file ) \n    logger . info ( \"Setting main assembly file to: {}\" . format ( assembly_file ) ) \n    if clear == \"true\" and os . path . exists ( assembly_file ) : \n        clean_up ( fastq_pair ) "}
{"7079": "\ndef main ( fastq_pair , adapter_file , cpus ) : \n    logger . info ( \"Starting fastqc\" ) \n    if os . path . exists ( adapter_file ) : \n        logger . info ( \"Adapters file provided: {}\" . format ( adapter_file ) ) \n        adapters = convert_adatpers ( adapter_file ) \n    else : \n        logger . info ( \"Adapters file '{}' not provided or does not \" \"exist\" . format ( adapter_file ) ) \n        adapters = None \n    cli = [ \"fastqc\" , \"--extract\" , \"--nogroup\" , \"--format\" , \"fastq\" , \"--threads\" , str ( cpus ) ] \n    if adapters : \n        cli = cli + ( [ \"--adapters\" , \"{}\" . format ( adapters ) ] ) \n    cli = cli + ( fastq_pair ) \n    logger . debug ( \"Running fastqc subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE , shell = False ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished fastqc subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished fastqc subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished fastqc with return code: {}\" . format ( p . returncode ) ) \n    logger . info ( \"Checking if FastQC output was correctly generated\" ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        for fastq in fastq_pair : \n            fpath = join ( fastq . rsplit ( \".\" , 2 ) [ 0 ] + \"_fastqc\" , \"fastqc_data.txt\" ) \n            logger . debug ( \"Checking path: {}\" . format ( fpath ) ) \n            if not exists ( fpath ) : \n                logger . warning ( \"Path does not exist: {}\" . format ( fpath ) ) \n                status_fh . write ( \"fail\" ) \n                return \n            logger . debug ( \"Found path: {}\" . format ( fpath ) ) \n            status_fh . write ( \"pass\" ) \n    logger . info ( \"Retrieving relevant FastQC output files\" ) \n    for i , fastq in enumerate ( fastq_pair ) : \n        fastqc_dir = fastq . rsplit ( \".\" , 2 ) [ 0 ] + \"_fastqc\" \n        summary_file = join ( fastqc_dir , \"summary.txt\" ) \n        logger . debug ( \"Retrieving summary file: {}\" . format ( summary_file ) ) \n        fastqc_data_file = join ( fastqc_dir , \"fastqc_data.txt\" ) \n        logger . debug ( \"Retrieving data file: {}\" . format ( fastqc_data_file ) ) \n        os . rename ( fastqc_data_file , \"pair_{}_data\" . format ( i + 1 ) ) \n        os . rename ( summary_file , \"pair_{}_summary\" . format ( i + 1 ) ) "}
{"7081": "\ndef main ( mash_output , hash_cutoff , sample_id , assembly_file ) : \n    input_f = open ( mash_output , \"r\" ) \n    master_dict = { } \n    for line in input_f : \n        tab_split = line . split ( \"\\t\" ) \n        current_seq = tab_split [ 1 ] . strip ( ) \n        ref_accession = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3 ] ) \n        mash_dist = tab_split [ 2 ] . strip ( ) \n        hashes_list = tab_split [ - 1 ] . strip ( ) . split ( \"/\" ) \n        perc_hashes = float ( hashes_list [ 0 ] ) / float ( hashes_list [ 1 ] ) \n        if ref_accession in master_dict . keys ( ) : \n            current_seq = current_seq + ( \", {}\" . format ( master_dict [ ref_accession ] [ - 1 ] ) ) \n        if perc_hashes > float ( hash_cutoff ) : \n            master_dict [ ref_accession ] = [ round ( 1 - float ( mash_dist ) , 2 ) , round ( perc_hashes , 2 ) , current_seq ] \n    send_to_output ( master_dict , mash_output , sample_id , assembly_file ) "}
{"7086": "\ndef proc_collector ( process_map , args , pipeline_string ) : \n    arguments_list = [ ] \n    if args . detailed_list : \n        arguments_list = arguments_list + ( [ \"input_type\" , \"output_type\" , \"description\" , \"dependencies\" , \"conflicts\" , \"directives\" ] ) \n    if args . short_list : \n        arguments_list = arguments_list + ( [ \"description\" ] ) \n    if arguments_list : \n        procs_dict = { } \n        for name , cls in process_map . items ( ) : \n            cls_inst = cls ( template = name ) \n            if pipeline_string : \n                if name not in pipeline_string : \n                    continue \n            d = { arg_key : vars ( cls_inst ) [ arg_key ] for arg_key in vars ( cls_inst ) if arg_key in arguments_list } \n            procs_dict [ name ] = d \n        procs_dict_parser ( procs_dict ) \n        sys . exit ( 0 ) "}
{"7090": "\ndef parse_coverage_table ( coverage_file ) : \n    coverage_dict = OrderedDict ( ) \n    total_cov = 0 \n    with open ( coverage_file ) as fh : \n        for line in fh : \n            contig , cov = line . strip ( ) . split ( ) \n            coverage_dict [ contig ] = { \"cov\" : int ( cov ) } \n            total_cov = total_cov + ( int ( cov ) ) \n            logger . debug ( \"Processing contig '{}' with coverage '{}'\" \"\" . format ( contig , cov ) ) \n    return coverage_dict , total_cov "}
{"7092": "\ndef filter_bam ( coverage_info , bam_file , min_coverage , output_bam ) : \n    contig_list = [ x for x , vals in coverage_info . items ( ) if vals [ \"cov\" ] >= min_coverage ] \n    cli = [ \"samtools\" , \"view\" , \"-bh\" , \"-F\" , \"4\" , \"-o\" , output_bam , \"-@\" , \"1\" , bam_file , ] \n    cli = cli + ( contig_list ) \n    logger . debug ( \"Runnig samtools view subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished samtools view subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished samtools view subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished samtools view with return code: {}\" . format ( p . returncode ) ) \n    if not p . returncode : \n        cli = [ \"samtools\" , \"index\" , output_bam ] \n        logger . debug ( \"Runnig samtools index subprocess with command: \" \"{}\" . format ( cli ) ) \n        p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n        stdout , stderr = p . communicate ( ) \n        try : \n            stderr = stderr . decode ( \"utf8\" ) \n            stdout = stdout . decode ( \"utf8\" ) \n        except ( UnicodeDecodeError , AttributeError ) : \n            stderr = str ( stderr ) \n            stdout = str ( stdout ) \n        logger . info ( \"Finished samtools index subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n        logger . info ( \"Fished samtools index subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n        logger . info ( \"Finished samtools index with return code: {}\" . format ( p . returncode ) ) "}
{"7094": "\ndef get_assembly_size ( assembly_file ) : \n    assembly_size = 0 \n    contig_size = { } \n    header = \"\" \n    with open ( assembly_file ) as fh : \n        for line in fh : \n            if line . strip ( ) == \"\" : \n                continue \n            if line . startswith ( \">\" ) : \n                header = line . strip ( ) [ 1 : ] \n                contig_size [ header ] = 0 \n            else : \n                line_len = len ( line . strip ( ) ) \n                assembly_size = assembly_size + ( line_len ) \n                contig_size [ header ] = contig_size [ header ] + ( line_len ) \n    return assembly_size , contig_size "}
{"7103": "\ndef compute_ssm ( X , metric = \"seuclidean\" ) : \n    D = distance . pdist ( X , metric = metric ) \n    D = distance . squareform ( D ) \n    D = D / ( D . max ( ) ) \n    return 1 - D "}
{"7104": "\ndef compute_nc ( X , G ) : \n    N = X . shape [ 0 ] \n    M = G . shape [ 0 ] \n    nc = np . zeros ( N ) \n    for i in range ( M // 2 , N - M // 2 + 1 ) : \n        nc [ i ] = np . sum ( X [ i - M // 2 : i + M // 2 , i - M // 2 : i + M // 2 ] * G ) \n    nc = nc + ( nc . min ( ) ) \n    nc = nc / ( nc . max ( ) ) \n    return nc "}
{"7106": "\ndef compute_nc ( X ) : \n    N = X . shape [ 0 ] \n    nc = np . zeros ( N ) \n    for i in range ( N - 1 ) : \n        nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + 1 , : ] ) \n    nc = nc + ( np . abs ( nc . min ( ) ) ) \n    nc = nc / ( float ( nc . max ( ) ) ) \n    return nc "}
{"7112": "\ndef plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id , title = None ) : \n    import matplotlib . pyplot as plt \n    bid_lid = boundaries_id \n    if labels_id is not None : \n        bid_lid = bid_lid + ( \" + \" + labels_id ) \n    try : \n        jam = jams . load ( file_struct . ref_file ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ 0 ] \n        ref_inters , ref_labels = ann . to_interval_values ( ) \n        ref_times = utils . intervals_to_times ( ref_inters ) \n        all_boundaries = [ ref_times , est_times ] \n        all_labels = [ ref_labels , est_labels ] \n        algo_ids = [ \"GT\" , bid_lid ] \n    except : \n        logging . warning ( \"No references found in %s. Not plotting groundtruth\" % file_struct . ref_file ) \n        all_boundaries = [ est_times ] \n        all_labels = [ est_labels ] \n        algo_ids = [ bid_lid ] \n    N = len ( all_boundaries ) \n    for i , labels in enumerate ( all_labels ) : \n        all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ 0 ] \n    cm = plt . get_cmap ( 'gist_rainbow' ) \n    max_label = max ( max ( labels ) for labels in all_labels ) \n    figsize = ( 8 , 4 ) \n    plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if i == 0 : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) \n        if labels_id is not None : \n            labels = all_labels [ i ] \n            inters = utils . times_to_intervals ( boundaries ) \n            for label , inter in zip ( labels , inters ) : \n                plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) \n    _plot_formatting ( title , os . path . basename ( file_struct . audio_file ) , algo_ids , all_boundaries [ 0 ] [ - 1 ] , N , None ) "}
{"7124": "\ndef get_dataset_files ( in_path ) : \n    audio_files = [ ] \n    for ext in ds_config . audio_exts : \n        audio_files = audio_files + ( glob . glob ( os . path . join ( in_path , ds_config . audio_dir , \"*\" + ext ) ) ) \n    utils . ensure_dir ( os . path . join ( in_path , ds_config . features_dir ) ) \n    utils . ensure_dir ( os . path . join ( in_path , ds_config . estimations_dir ) ) \n    utils . ensure_dir ( os . path . join ( in_path , ds_config . references_dir ) ) \n    file_structs = [ ] \n    for audio_file in audio_files : \n        file_structs . append ( FileStruct ( audio_file ) ) \n    file_structs = sorted ( file_structs , key = lambda file_struct : file_struct . audio_file ) \n    return file_structs "}
{"7127": "\ndef write_mirex ( times , labels , out_file ) : \n    inters = msaf . utils . times_to_intervals ( times ) \n    assert len ( inters ) == len ( labels ) \n    out_str = \"\" \n    for inter , label in zip ( inters , labels ) : \n        out_str = out_str + ( \"%.3f\\t%.3f\\t%s\\n\" % ( inter [ 0 ] , inter [ 1 ] , label ) ) \n    with open ( out_file , \"w\" ) as f : \n        f . write ( out_str [ : - 1 ] ) "}
{"7147": "\ndef get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) : \n    utils . ensure_dir ( msaf . config . results_dir ) \n    file_name = os . path . join ( msaf . config . results_dir , \"results\" ) \n    file_name = file_name + ( \"_boundsE%s_labelsE%s\" % ( boundaries_id , labels_id ) ) \n    file_name = file_name + ( \"_annotatorE%d\" % ( annotator_id ) ) \n    sorted_keys = sorted ( config . keys ( ) , key = str . lower ) \n    for key in sorted_keys : \n        file_name = file_name + ( \"_%sE%s\" % ( key , str ( config [ key ] ) . replace ( \"/\" , \"_\" ) ) ) \n    if len ( file_name ) > 255 - len ( msaf . config . results_ext ) : \n        file_name = file_name [ : 255 - len ( msaf . config . results_ext ) ] \n    return file_name + msaf . config . results_ext "}
{"7154": "\ndef min_max_normalize ( F , floor = 0.001 ) : \n    F = F + ( - F . min ( ) + floor ) \n    F = F / F . max ( axis = 0 ) \n    return F "}
{"7158": "\ndef sonify_clicks ( audio , clicks , out_file , fs , offset = 0 ) : \n    times = clicks + offset \n    click = np . sin ( 2 * np . pi * np . arange ( fs * .1 ) * 1000 / ( 1. * fs ) ) \n    click = click * ( np . exp ( - np . arange ( fs * .1 ) / ( fs * .01 ) ) ) \n    length = int ( times . max ( ) * fs + click . shape [ 0 ] + 1 ) \n    audio_clicks = mir_eval . sonify . clicks ( times , fs , length = length ) \n    out_audio = np . zeros ( max ( len ( audio ) , len ( audio_clicks ) ) ) \n    out_audio [ : len ( audio ) ] = audio \n    out_audio [ : len ( audio_clicks ) ] = out_audio [ : len ( audio_clicks ) ] + ( audio_clicks ) \n    scipy . io . wavfile . write ( out_file , fs , out_audio ) "}
{"7163": "\ndef estimate_K_knee ( self , th = .015 , maxK = 12 ) : \n    if self . X . shape [ 0 ] < maxK : \n        maxK = self . X . shape [ 0 ] \n    if maxK < 2 : \n        maxK = 2 \n    K = np . arange ( 1 , maxK ) \n    bics = [ ] \n    for k in K : \n        means , labels = self . run_kmeans ( self . X , k ) \n        bic = self . compute_bic ( self . X , means , labels , K = k , R = self . X . shape [ 0 ] ) \n        bics . append ( bic ) \n    diff_bics = np . diff ( bics ) \n    finalK = K [ - 1 ] \n    if len ( bics ) == 1 : \n        finalK = 2 \n    else : \n        bics = np . asarray ( bics ) \n        bics = bics - ( bics . min ( ) ) \n        diff_bics = diff_bics - ( diff_bics . min ( ) ) \n        for i in range ( len ( K [ : - 1 ] ) ) : \n            if diff_bics [ i ] < th and K [ i ] != 1 : \n                finalK = K [ i ] \n                break \n    if self . plot : \n        plt . subplot ( 2 , 1 , 1 ) \n        plt . plot ( K , bics , label = \"BIC\" ) \n        plt . plot ( K [ : - 1 ] , diff_bics , label = \"BIC diff\" ) \n        plt . legend ( loc = 2 ) \n        plt . subplot ( 2 , 1 , 2 ) \n        plt . scatter ( self . X [ : , 0 ] , self . X [ : , 1 ] ) \n        plt . show ( ) \n    return finalK "}
{"7166": "\ndef compute_bic ( self , D , means , labels , K , R ) : \n    D = vq . whiten ( D ) \n    Rn = D . shape [ 0 ] \n    M = D . shape [ 1 ] \n    if R == K : \n        return 1 \n    mle_var = 0 \n    for k in range ( len ( means ) ) : \n        X = D [ np . argwhere ( labels == k ) ] \n        X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) \n        for x in X : \n            mle_var = mle_var + ( distance . euclidean ( x , means [ k ] ) ) \n    mle_var = mle_var / ( float ( R - K ) ) \n    l_D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) \n    p = ( K - 1 ) + M * K + mle_var \n    return l_D - p / 2. * np . log ( R ) "}
{"7233": "\ndef match ( self , request ) : \n    if self . _times <= 0 : \n        raise PookExpiredMock ( 'Mock expired' ) \n    for test in self . filters : \n        if not test ( request , self ) : \n            return False , [ ] \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    matches , errors = self . matchers . match ( request ) \n    if not matches : \n        return False , errors \n    self . _calls . append ( request ) \n    self . _matches = self . _matches + ( 1 ) \n    if not self . _persist : \n        self . _times = self . _times - ( 1 ) \n    if self . _error : \n        raise self . _error \n    for callback in self . callbacks : \n        callback ( request , self ) \n    return True , [ ] "}
{"7242": "\ndef match ( self , request ) : \n    for test in self . filters : \n        if not test ( request , self ) : \n            return False \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    match_errors = [ ] \n    for mock in self . mocks [ : ] : \n        try : \n            matches , errors = mock . match ( request . copy ( ) ) \n            if len ( errors ) : \n                match_errors = match_errors + ( errors ) \n            if matches : \n                return mock \n        except PookExpiredMock : \n            self . mocks . remove ( mock ) \n    if not self . should_use_network ( request ) : \n        msg = 'pook error!\\n\\n' \n        msg = msg + ( ( '=> Cannot match any mock for the ' 'following request:\\n{}' . format ( request ) ) ) \n        if self . debug : \n            err = '\\n\\n' . join ( [ str ( err ) for err in match_errors ] ) \n            if err : \n                msg = msg + ( '\\n\\n=> Detailed matching errors:\\n{}\\n' . format ( err ) ) \n        raise PookNoMatches ( msg ) \n    self . unmatched_reqs . append ( request ) "}
{"7257": "\ndef recurse ( self , full_matrix = False ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        n_leaves = len ( n . _ii ) \n        if full_matrix : \n            M = np . zeros ( ( n_leaves , n_leaves ) , dtype = float ) \n        r = np . zeros ( n_leaves , dtype = float ) \n        c_count = 0 \n        for c in n : \n            ssq = self . branch_variance ( c ) \n            nc = len ( c . _ii ) \n            if c . is_terminal ( ) : \n                if full_matrix : \n                    M [ c_count , c_count ] = 1.0 / ssq \n                r [ c_count ] = 1.0 / ssq \n            else : \n                if full_matrix : \n                    M [ c_count : c_count + nc , c_count : c_count + nc ] = c . cinv - ssq * np . outer ( c . r , c . r ) / ( 1 + ssq * c . s ) \n                r [ c_count : c_count + nc ] = c . r / ( 1 + ssq * c . s ) \n            c_count = c_count + ( nc ) \n        if full_matrix : \n            n . cinv = M \n        n . r = r \n        n . s = n . r . sum ( ) "}
{"7258": "\ndef _calculate_averages ( self ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        Q = np . zeros ( 6 , dtype = float ) \n        for c in n : \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            Q = Q + ( self . propagate_averages ( c , tv , bv , var ) ) \n        n . Q = Q \n    for n in self . tree . find_clades ( order = 'preorder' ) : \n        O = np . zeros ( 6 , dtype = float ) \n        if n == self . tree . root : \n            n . Qtot = n . Q \n            continue \n        for c in n . up : \n            if c == n : \n                continue \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O = O + ( self . propagate_averages ( c , tv , bv , var ) ) \n        if n . up != self . tree . root : \n            c = n . up \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O = O + ( self . propagate_averages ( c , tv , bv , var , outgroup = True ) ) \n        n . O = O \n        if not n . is_terminal ( ) : \n            tv = self . tip_value ( n ) \n            bv = self . branch_value ( n ) \n            var = self . branch_variance ( n ) \n            n . Qtot = n . Q + self . propagate_averages ( n , tv , bv , var , outgroup = True ) "}
{"7264": "\ndef calc_branch_count ( self ) : \n    self . tree_events = np . array ( sorted ( [ ( n . time_before_present , len ( n . clades ) - 1 ) for n in self . tree . find_clades ( ) if not n . bad_branch ] , key = lambda x : - x [ 0 ] ) ) \n    from collections import defaultdict \n    dn_branch = defaultdict ( int ) \n    for ( t , dn ) in self . tree_events : \n        dn_branch [ t ] = dn_branch [ t ] + ( dn ) \n    unique_mergers = np . array ( sorted ( dn_branch . items ( ) , key = lambda x : - x [ 0 ] ) ) \n    nbranches = [ [ ttconf . BIG_NUMBER , 1 ] , [ unique_mergers [ 0 , 0 ] + ttconf . TINY_NUMBER , 1 ] ] \n    for ti , ( t , dn ) in enumerate ( unique_mergers [ : - 1 ] ) : \n        new_n = nbranches [ - 1 ] [ 1 ] + dn \n        next_t = unique_mergers [ ti + 1 , 0 ] + ttconf . TINY_NUMBER \n        nbranches . append ( [ t , new_n ] ) \n        nbranches . append ( [ next_t , new_n ] ) \n    new_n = new_n + ( unique_mergers [ - 1 , 1 ] ) \n    nbranches . append ( [ next_t , new_n ] ) \n    nbranches . append ( [ - ttconf . BIG_NUMBER , new_n ] ) \n    nbranches = np . array ( nbranches ) \n    self . nbranches = interp1d ( nbranches [ : , 0 ] , nbranches [ : , 1 ] , kind = 'linear' ) "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = 0 \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        elif l . is_terminal ( ) : \n            self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , 0 , warn = True ) \n            failed_leaves = failed_leaves + ( 1 ) \n            l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n            if failed_leaves > self . tree . count_terminals ( ) / 3 : \n                self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , 0 , warn = True ) \n                self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2 , warn = True ) \n                break \n        else : \n            pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , 0 , warn = True ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7275": "\ndef _prepare_nodes ( self ) : \n    self . tree . root . up = None \n    self . tree . root . bad_branch = self . tree . root . bad_branch if hasattr ( self . tree . root , 'bad_branch' ) else False \n    internal_node_count = 0 \n    for clade in self . tree . get_nonterminals ( order = 'preorder' ) : \n        internal_node_count = internal_node_count + ( 1 ) \n        if clade . name is None : \n            clade . name = \"NODE_\" + format ( self . _internal_node_count , '07d' ) \n            self . _internal_node_count = self . _internal_node_count + ( 1 ) \n        for c in clade . clades : \n            if c . is_terminal ( ) : \n                c . bad_branch = c . bad_branch if hasattr ( c , 'bad_branch' ) else False \n            c . up = clade \n    for clade in self . tree . get_nonterminals ( order = 'postorder' ) : \n        clade . bad_branch = all ( [ c . bad_branch for c in clade ] ) \n    self . _calc_dist2root ( ) \n    self . _internal_node_count = max ( internal_node_count , self . _internal_node_count ) "}
{"7280": "\ndef _fitch_anc ( self , ** kwargs ) : \n    for l in self . tree . get_terminals ( ) : \n        l . state = [ [ k ] for k in l . cseq ] \n    L = len ( self . tree . get_terminals ( ) [ 0 ] . cseq ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\" , 2 ) \n    for node in self . tree . get_nonterminals ( order = 'postorder' ) : \n        node . state = [ self . _fitch_state ( node , k ) for k in range ( L ) ] \n    ambs = [ i for i in range ( L ) if len ( self . tree . root . state [ i ] ) > 1 ] \n    if len ( ambs ) > 0 : \n        for amb in ambs : \n            self . logger ( \"Ambiguous state of the root sequence \" \"in the position %d: %s, \" \"choosing %s\" % ( amb , str ( self . tree . root . state [ amb ] ) , self . tree . root . state [ amb ] [ 0 ] ) , 4 ) \n    self . tree . root . cseq = np . array ( [ k [ np . random . randint ( len ( k ) ) if len ( k ) > 1 else 0 ] for k in self . tree . root . state ] ) \n    if self . is_vcf : \n        self . tree . root . sequence = self . dict_sequence ( self . tree . root ) \n    else : \n        self . tree . root . sequence = self . expanded_sequence ( self . tree . root ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \" \"Fitch profiles.\" , 2 ) \n    N_diff = 0 \n    for node in self . tree . get_nonterminals ( order = 'preorder' ) : \n        if node . up != None : \n            sequence = np . array ( [ node . up . cseq [ i ] if node . up . cseq [ i ] in node . state [ i ] else node . state [ i ] [ 0 ] for i in range ( L ) ] ) \n            if hasattr ( node , 'sequence' ) : \n                N_diff = N_diff + ( ( sequence != node . cseq ) . sum ( ) ) \n            else : \n                N_diff = N_diff + ( L ) \n            node . cseq = sequence \n            if self . is_vcf : \n                node . sequence = self . dict_sequence ( node ) \n            else : \n                node . sequence = self . expanded_sequence ( node ) \n            node . mutations = self . get_mutations ( node ) \n        node . profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n        del node . state \n    self . logger ( \"Done ancestral state reconstruction\" , 3 ) \n    for node in self . tree . get_terminals ( ) : \n        node . profile = seq2prof ( node . original_cseq , self . gtr . profile_map ) \n    return N_diff "}
{"7284": "\ndef ancestral_likelihood ( self ) : \n    log_lh = np . zeros ( self . multiplicity . shape [ 0 ] ) \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n            profile = profile * ( self . gtr . Pi ) \n            profile = profile . sum ( axis = 1 ) \n            log_lh = log_lh + ( np . log ( profile ) ) \n            continue \n        t = node . branch_length \n        indices = np . array ( [ ( np . argmax ( self . gtr . alphabet == a ) , np . argmax ( self . gtr . alphabet == b ) ) for a , b in zip ( node . up . cseq , node . cseq ) ] ) \n        logQt = np . log ( self . gtr . expQt ( t ) ) \n        lh = logQt [ indices [ : , 1 ] , indices [ : , 0 ] ] \n        log_lh = log_lh + ( lh ) \n    return log_lh "}
{"7289": "\ndef optimize_seq_and_branch_len ( self , reuse_branch_len = True , prune_short = True , marginal_sequences = False , branch_length_mode = 'joint' , max_iter = 5 , infer_gtr = False , ** kwargs ) : \n    if branch_length_mode == 'marginal' : \n        marginal_sequences = True \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: sequences...\" , 1 ) \n    if reuse_branch_len : \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = infer_gtr , marginal = marginal_sequences , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    else : \n        N_diff = self . reconstruct_anc ( method = 'fitch' , infer_gtr = infer_gtr , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , marginal = False ) \n    n = 0 \n    while n < max_iter : \n        n = n + ( 1 ) \n        if prune_short : \n            self . prune_short_branches ( ) \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = False , marginal = marginal_sequences , ** kwargs ) \n        self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\" \" #Nuc changed since prev reconstructions: %d\" % ( n , N_diff ) , 2 ) \n        if N_diff < 1 : \n            break \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    self . tree . unconstrained_sequence_LH = ( self . tree . sequence_LH * self . multiplicity ) . sum ( ) \n    self . _prepare_nodes ( ) \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self . tree . unconstrained_sequence_LH , 2 ) \n    return ttconf . SUCCESS "}
{"7294": "\ndef _check_fix_Q ( self , fixed_mu = False ) : \n    self . Pi = self . Pi / ( self . Pi . sum ( ) ) \n    self . W = self . W + ( self . break_degen + self . break_degen . T ) \n    np . fill_diagonal ( self . W , 0 ) \n    Wdiag = - ( self . Q ) . sum ( axis = 0 ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W = self . W / ( scale_factor ) \n    if not fixed_mu : \n        self . mu = self . mu * ( scale_factor ) \n    if ( self . Q . sum ( axis = 0 ) < 1e-10 ) . sum ( ) < self . alphabet . shape [ 0 ] : \n        print ( \"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\" , self . Q . sum ( axis = 0 ) ) \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7304": "\ndef resolve_polytomies ( self , merge_compressed = False ) : \n    self . logger ( \"TreeTime.resolve_polytomies: resolving multiple mergers...\" , 1 ) \n    poly_found = 0 \n    for n in self . tree . find_clades ( ) : \n        if len ( n . clades ) > 2 : \n            prior_n_clades = len ( n . clades ) \n            self . _poly ( n , merge_compressed ) \n            poly_found = poly_found + ( prior_n_clades - len ( n . clades ) ) \n    obsolete_nodes = [ n for n in self . tree . find_clades ( ) if len ( n . clades ) == 1 and n . up is not None ] \n    for node in obsolete_nodes : \n        self . logger ( 'TreeTime.resolve_polytomies: remove obsolete node ' + node . name , 4 ) \n        if node . up is not None : \n            self . tree . collapse ( node ) \n    if poly_found : \n        self . logger ( 'TreeTime.resolve_polytomies: introduces %d new nodes' % poly_found , 3 ) \n    else : \n        self . logger ( 'TreeTime.resolve_polytomies: No more polytomies to resolve' , 3 ) \n    return poly_found "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob = log_prob - ( log_prob . min ( ) ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( log_prob < 0.693147 ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if L < 2 : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , 0 ) \n        return ttconf . ERROR \n    bad_branch_counter = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                node . raw_date_constraint = None \n                node . bad_branch = True \n            else : \n                try : \n                    tmp = np . mean ( tmp_date ) \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = False \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = True \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = True \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter = bad_branch_counter + ( 1 ) \n    if bad_branch_counter > self . tree . count_terminals ( ) - 3 : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , 1 , warn = True ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7318": "\ndef timetree_likelihood ( self ) : \n    LH = 0 \n    for node in self . tree . find_clades ( order = 'preorder' ) : \n        if node . up is None : \n            continue \n        LH = LH - ( node . branch_length_interpolator ( node . branch_length ) ) \n    if self . aln : \n        LH = LH + ( self . gtr . sequence_logLH ( self . tree . root . cseq , pattern_multiplicity = self . multiplicity ) ) \n    return LH "}
{"7345": "\ndef filter ( self , * filters , ** kwargs ) : \n    f = list ( filters ) \n    if kwargs : \n        f = f + ( [ Filter ( ** kwargs ) ] ) \n    return self . _clone ( filters = f ) "}
{"7350": "\ndef next ( self ) : \n    if not hasattr ( self , '_cursor' ) : \n        self . __iter__ ( ) \n    if self . _cursor == len ( self ) : \n        raise StopIteration ( ) \n    if self . _buffer_idx == len ( self . _buffer ) : \n        self . execute ( self . _page_offset + self . _buffer_idx ) \n        self . _buffer_idx = 0 \n    self . _cursor = self . _cursor + ( 1 ) \n    self . _buffer_idx = self . _buffer_idx + ( 1 ) \n    return self . _buffer [ self . _buffer_idx - 1 ] "}
{"7413": "\ndef _first_weekday ( weekday , d ) : \n    while weekday != d . weekday ( ) : \n        d = d + ( timedelta ( days = 1 ) ) \n    return d "}
{"7414": "\ndef repeat ( self , day = None ) : \n    if day is None : \n        day = self . day \n    try : \n        d = date ( self . year , self . month , day ) \n    except ValueError : \n        return self . count \n    if self . count_first and d <= self . end_repeat : \n        self . count_it ( d . day ) \n    d = d + ( timedelta ( days = self . num ) ) \n    if self . end_on is not None : \n        while d . month == self . month and d <= self . end_repeat and d . day <= self . end_on : \n            self . count_it ( d . day ) \n            d = d + ( timedelta ( days = self . num ) ) \n    else : \n        while d . month == self . month and d <= self . end_repeat : \n            self . count_it ( d . day ) \n            d = d + ( timedelta ( days = self . num ) ) "}
{"7415": "\ndef repeat_reverse ( self , start , end ) : \n    day = start \n    diff = start - end \n    try : \n        if date ( self . year , self . month , day ) <= self . end_repeat : \n            self . count_it ( day ) \n    except ValueError : \n        pass \n    for i in xrange ( diff ) : \n        day = day - ( 1 ) \n        try : \n            if date ( self . year , self . month , day ) <= self . end_repeat : \n                self . count_it ( day ) \n        except ValueError : \n            pass "}
{"7487": "\ndef verified_excel_file ( store , institute_list , temp_excel_dir ) : \n    document_lines = [ ] \n    written_files = 0 \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    LOG . info ( 'Creating verified variant document..' ) \n    for cust in institute_list : \n        verif_vars = store . verified ( institute_id = cust ) \n        LOG . info ( 'Found {} verified variants for customer {}' . format ( len ( verif_vars ) , cust ) ) \n        if not verif_vars : \n            continue \n        unique_callers = set ( ) \n        for var_type , var_callers in CALLERS . items ( ) : \n            for caller in var_callers : \n                unique_callers . add ( caller . get ( 'id' ) ) \n        cust_verified = export_verified_variants ( verif_vars , unique_callers ) \n        document_name = '.' . join ( [ cust , '_verified_variants' , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( temp_excel_dir , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        row = 0 \n        for col , field in enumerate ( VERIFIED_VARIANTS_HEADER + list ( unique_callers ) ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( cust_verified , 1 ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( temp_excel_dir , document_name ) ) : \n            written_files = written_files + ( 1 ) \n    return written_files "}
{"7493": "\ndef variants ( context , case_id , institute , force , cancer , cancer_research , sv , sv_research , snv , snv_research , str_clinical , chrom , start , end , hgnc_id , hgnc_symbol , rank_treshold ) : \n    LOG . info ( \"Running scout load variants\" ) \n    adapter = context . obj [ 'adapter' ] \n    if institute : \n        case_id = \"{0}-{1}\" . format ( institute , case_id ) \n    else : \n        institute = case_id . split ( '-' ) [ 0 ] \n    case_obj = adapter . case ( case_id = case_id ) \n    if case_obj is None : \n        LOG . info ( \"No matching case found\" ) \n        context . abort ( ) \n    files = [ { 'category' : 'cancer' , 'variant_type' : 'clinical' , 'upload' : cancer } , { 'category' : 'cancer' , 'variant_type' : 'research' , 'upload' : cancer_research } , { 'category' : 'sv' , 'variant_type' : 'clinical' , 'upload' : sv } , { 'category' : 'sv' , 'variant_type' : 'research' , 'upload' : sv_research } , { 'category' : 'snv' , 'variant_type' : 'clinical' , 'upload' : snv } , { 'category' : 'snv' , 'variant_type' : 'research' , 'upload' : snv_research } , { 'category' : 'str' , 'variant_type' : 'clinical' , 'upload' : str_clinical } , ] \n    gene_obj = None \n    if ( hgnc_id or hgnc_symbol ) : \n        if hgnc_id : \n            gene_obj = adapter . hgnc_gene ( hgnc_id ) \n        if hgnc_symbol : \n            for res in adapter . gene_by_alias ( hgnc_symbol ) : \n                gene_obj = res \n        if not gene_obj : \n            LOG . warning ( \"The gene could not be found\" ) \n            context . abort ( ) \n    i = 0 \n    for file_type in files : \n        variant_type = file_type [ 'variant_type' ] \n        category = file_type [ 'category' ] \n        if file_type [ 'upload' ] : \n            i = i + ( 1 ) \n            if variant_type == 'research' : \n                if not ( force or case_obj [ 'research_requested' ] ) : \n                    LOG . warn ( \"research not requested, use '--force'\" ) \n                    context . abort ( ) \n            LOG . info ( \"Delete {0} {1} variants for case {2}\" . format ( variant_type , category , case_id ) ) \n            adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = variant_type , category = category ) \n            LOG . info ( \"Load {0} {1} variants for case {2}\" . format ( variant_type , category , case_id ) ) \n            try : \n                adapter . load_variants ( case_obj = case_obj , variant_type = variant_type , category = category , rank_threshold = rank_treshold , chrom = chrom , start = start , end = end , gene_obj = gene_obj ) \n            except Exception as e : \n                LOG . warning ( e ) \n                context . abort ( ) \n    if i == 0 : \n        LOG . info ( \"No files where specified to upload variants from\" ) "}
{"7502": "\ndef check_weekday ( year , month , day , reverse = False ) : \n    d = date ( year , month , day ) \n    while d . weekday ( ) in ( 5 , 6 ) : \n        if reverse : \n            d = d - ( timedelta ( days = 1 ) ) \n        else : \n            d = d + ( timedelta ( days = 1 ) ) \n    return d . year , d . month , d . day "}
{"7510": "\ndef mt_report ( context , case_id , test , outpath = None ) : \n    LOG . info ( 'exporting mitochondrial variants for case \"{}\"' . format ( case_id ) ) \n    adapter = context . obj [ 'adapter' ] \n    query = { 'chrom' : 'MT' } \n    case_obj = adapter . case ( case_id = case_id ) \n    if not case_obj : \n        LOG . warning ( 'Could not find a scout case with id \"{}\". No report was created.' . format ( case_id ) ) \n        context . abort ( ) \n    samples = case_obj . get ( 'individuals' ) \n    mt_variants = list ( adapter . variants ( case_id = case_id , query = query , nr_of_variants = - 1 , sort_key = 'position' ) ) \n    if not mt_variants : \n        LOG . warning ( 'There are no MT variants associated to case {} in database!' . format ( case_id ) ) \n        context . abort ( ) \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    if not outpath : \n        outpath = str ( os . getcwd ( ) ) \n    written_files = 0 \n    for sample in samples : \n        sample_id = sample [ 'individual_id' ] \n        sample_lines = export_mt_variants ( variants = mt_variants , sample_id = sample_id ) \n        document_name = '.' . join ( [ case_obj [ 'display_name' ] , sample_id , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( outpath , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        if test and sample_lines and workbook : \n            written_files = written_files + ( 1 ) \n            continue \n        row = 0 \n        for col , field in enumerate ( MT_EXPORT_HEADER ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( sample_lines , 1 ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( outpath , document_name ) ) : \n            written_files = written_files + ( 1 ) \n    if test : \n        LOG . info ( \"Number of excel files that can be written to folder {0}: {1}\" . format ( outpath , written_files ) ) \n    else : \n        LOG . info ( \"Number of excel files written to folder {0}: {1}\" . format ( outpath , written_files ) ) \n    return written_files "}
{"7533": "\ndef load_transcripts ( adapter , transcripts_lines = None , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    if transcripts_lines is None : \n        transcripts_lines = fetch_ensembl_transcripts ( build = build ) \n    transcripts_dict = parse_transcripts ( transcripts_lines ) \n    for ens_tx_id in list ( transcripts_dict ) : \n        parsed_tx = transcripts_dict [ ens_tx_id ] \n        ens_gene_id = parsed_tx [ 'ensembl_gene_id' ] \n        gene_obj = ensembl_genes . get ( ens_gene_id ) \n        if not gene_obj : \n            transcripts_dict . pop ( ens_tx_id ) \n            LOG . debug ( \"Gene %s does not exist in build %s\" , ens_gene_id , build ) \n            continue \n        parsed_tx [ 'hgnc_id' ] = gene_obj [ 'hgnc_id' ] \n        parsed_tx [ 'primary_transcripts' ] = set ( gene_obj . get ( 'primary_transcripts' , [ ] ) ) \n    ref_seq_transcripts = 0 \n    nr_primary_transcripts = 0 \n    nr_transcripts = len ( transcripts_dict ) \n    transcript_objs = [ ] \n    with progressbar ( transcripts_dict . values ( ) , label = \"Building transcripts\" , length = nr_transcripts ) as bar : \n        for tx_data in bar : \n            tx_data [ 'is_primary' ] = False \n            primary_transcripts = tx_data [ 'primary_transcripts' ] \n            refseq_identifier = None \n            refseq_identifiers = [ ] \n            for category in TRANSCRIPT_CATEGORIES : \n                identifiers = tx_data [ category ] \n                if not identifiers : \n                    continue \n                for refseq_id in identifiers : \n                    refseq_identifiers . append ( refseq_id ) \n                    ref_seq_transcripts = ref_seq_transcripts + ( 1 ) \n                    if refseq_id in primary_transcripts : \n                        refseq_identifier = refseq_id \n                        tx_data [ 'is_primary' ] = True \n                        nr_primary_transcripts = nr_primary_transcripts + ( 1 ) \n                    if not refseq_identifier : \n                        refseq_identifier = refseq_id \n            if refseq_identifier : \n                tx_data [ 'refseq_id' ] = refseq_identifier \n            if refseq_identifiers : \n                tx_data [ 'refseq_identifiers' ] = refseq_identifiers \n            tx_obj = build_transcript ( tx_data , build ) \n            transcript_objs . append ( tx_obj ) \n    LOG . info ( \"Loading transcripts...\" ) \n    if len ( transcript_objs ) > 0 : \n        adapter . load_transcript_bulk ( transcript_objs ) \n    LOG . info ( 'Number of transcripts in build %s: %s' , build , nr_transcripts ) \n    LOG . info ( 'Number of transcripts with refseq identifier: %s' , ref_seq_transcripts ) \n    LOG . info ( 'Number of primary transcripts: %s' , nr_primary_transcripts ) \n    return transcript_objs "}
{"7544": "\ndef cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n    else : \n        models = adapter . cases ( collaborator = institute , name_query = display_name ) \n        models = [ case_obj for case_obj in models ] \n    if not models : \n        LOG . info ( \"No cases could be found\" ) \n        return \n    header = [ 'case_id' , 'display_name' , 'institute' ] \n    if variants_treshold : \n        LOG . info ( \"Only show cases with more than %s variants\" , variants_treshold ) \n        nr_variants = True \n    if nr_variants : \n        LOG . info ( \"Displaying number of variants for each case\" ) \n        header . append ( 'clinical' ) \n        header . append ( 'research' ) \n    click . echo ( \"#\" + '\\t' . join ( header ) ) \n    for model in models : \n        output_str = \"{:<12}\\t{:<12}\\t{:<12}\" \n        output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] \n        if nr_variants : \n            output_str = output_str + ( \"\\t{:<12}\\t{:<12}\" ) \n            nr_clinical = 0 \n            nr_research = 0 \n            variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) \n            i = 0 \n            for i , var in enumerate ( variants , 1 ) : \n                if var [ 'variant_type' ] == 'clinical' : \n                    nr_clinical = nr_clinical + ( 1 ) \n                else : \n                    nr_research = nr_research + ( 1 ) \n            output_values . extend ( [ nr_clinical , nr_research ] ) \n            if variants_treshold and i < variants_treshold : \n                LOG . debug ( \"Case %s had to few variants, skipping\" , model [ '_id' ] ) \n                continue \n        click . echo ( output_str . format ( * output_values ) ) "}
{"7563": "\ndef parse_reqs ( req_path = './requirements.txt' ) : \n    install_requires = [ ] \n    with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : \n        lines = ( line . strip ( ) for line in handle if line . strip ( ) and not line . startswith ( '#' ) ) \n        for line in lines : \n            if line . startswith ( '-r' ) : \n                install_requires = install_requires + ( parse_reqs ( req_path = line [ 3 : ] ) ) \n            else : \n                install_requires . append ( line ) \n    return install_requires "}
{"7572": "\ndef load_hgnc_genes ( adapter , genes = None , ensembl_lines = None , hgnc_lines = None , exac_lines = None , mim2gene_lines = None , genemap_lines = None , hpo_lines = None , build = '37' , omim_api_key = '' ) : \n    gene_objects = list ( ) \n    if not genes : \n        if ensembl_lines is None : \n            ensembl_lines = fetch_ensembl_genes ( build = build ) \n        hgnc_lines = hgnc_lines or fetch_hgnc ( ) \n        exac_lines = exac_lines or fetch_exac_constraint ( ) \n        if not ( mim2gene_lines and genemap_lines ) : \n            if not omim_api_key : \n                raise SyntaxError ( \"Need to provide omim api key\" ) \n            mim_files = fetch_mim_files ( omim_api_key , mim2genes = True , genemap2 = True ) \n            mim2gene_lines = mim_files [ 'mim2genes' ] \n            genemap_lines = mim_files [ 'genemap2' ] \n        if not hpo_lines : \n            hpo_files = fetch_hpo_files ( hpogenes = True ) \n            hpo_lines = hpo_files [ 'hpogenes' ] \n        genes = link_genes ( ensembl_lines = ensembl_lines , hgnc_lines = hgnc_lines , exac_lines = exac_lines , mim2gene_lines = mim2gene_lines , genemap_lines = genemap_lines , hpo_lines = hpo_lines ) \n    non_existing = 0 \n    nr_genes = len ( genes ) \n    with progressbar ( genes . values ( ) , label = \"Building genes\" , length = nr_genes ) as bar : \n        for gene_data in bar : \n            if not gene_data . get ( 'chromosome' ) : \n                LOG . debug ( \"skipping gene: %s. No coordinates found\" , gene_data . get ( 'hgnc_symbol' , '?' ) ) \n                non_existing = non_existing + ( 1 ) \n                continue \n            gene_obj = build_hgnc_gene ( gene_data , build = build ) \n            gene_objects . append ( gene_obj ) \n    LOG . info ( \"Loading genes build %s\" , build ) \n    adapter . load_hgnc_bulk ( gene_objects ) \n    LOG . info ( \"Loading done. %s genes loaded\" , len ( gene_objects ) ) \n    LOG . info ( \"Nr of genes without coordinates in build %s: %s\" , build , non_existing ) \n    return gene_objects "}
{"7580": "\ndef verified ( context , collaborator , test , outpath = None ) : \n    written_files = 0 \n    collaborator = collaborator or 'cust000' \n    LOG . info ( 'Exporting verified variants for cust {}' . format ( collaborator ) ) \n    adapter = context . obj [ 'adapter' ] \n    verified_vars = adapter . verified ( institute_id = collaborator ) \n    LOG . info ( 'FOUND {} verified variants for institute {}' . format ( len ( verified_vars ) , collaborator ) ) \n    if not verified_vars : \n        LOG . warning ( 'There are no verified variants for institute {} in database!' . format ( collaborator ) ) \n        return None \n    document_lines = export_verified_variants ( verified_vars ) \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    document_name = '.' . join ( [ 'verified_variants' , collaborator , today ] ) + '.xlsx' \n    if test and document_lines : \n        written_files = written_files + ( 1 ) \n        LOG . info ( 'Success. Verified variants file contains {} lines' . format ( len ( document_lines ) ) ) \n        return written_files \n    if not outpath : \n        outpath = str ( os . getcwd ( ) ) \n    workbook = Workbook ( os . path . join ( outpath , document_name ) ) \n    Report_Sheet = workbook . add_worksheet ( ) \n    row = 0 \n    for col , field in enumerate ( VERIFIED_VARIANTS_HEADER ) : \n        Report_Sheet . write ( row , col , field ) \n    for row , line in enumerate ( document_lines , 1 ) : \n        for col , field in enumerate ( line ) : \n            Report_Sheet . write ( row , col , field ) \n    workbook . close ( ) \n    if os . path . exists ( os . path . join ( outpath , document_name ) ) : \n        LOG . info ( 'Success. Verified variants file of {} lines was written to disk' . format ( len ( document_lines ) ) ) \n        written_files = written_files + ( 1 ) \n    return written_files "}
{"7582": "\ndef get_vcf_entry ( variant_obj , case_id = None ) : \n    if variant_obj [ 'category' ] == 'snv' : \n        var_type = 'TYPE' \n    else : \n        var_type = 'SVTYPE' \n    info_field = ';' . join ( [ 'END=' + str ( variant_obj [ 'end' ] ) , var_type + '=' + variant_obj [ 'sub_category' ] . upper ( ) ] ) \n    variant_string = \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\" . format ( variant_obj [ 'chromosome' ] , variant_obj [ 'position' ] , variant_obj [ 'dbsnp_id' ] , variant_obj [ 'reference' ] , variant_obj [ 'alternative' ] , variant_obj [ 'quality' ] , ';' . join ( variant_obj [ 'filters' ] ) , info_field ) \n    if case_id : \n        variant_string = variant_string + ( \"\\tGT\" ) \n        for sample in variant_obj [ 'samples' ] : \n            variant_string = variant_string + ( \"\\t\" + sample [ 'genotype_call' ] ) \n    return variant_string "}
{"7607": "\ndef load_exons ( adapter , exon_lines , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    hgnc_id_transcripts = adapter . id_transcripts_by_gene ( build = build ) \n    if isinstance ( exon_lines , DataFrame ) : \n        exons = parse_ensembl_exon_request ( exon_lines ) \n        nr_exons = exon_lines . shape [ 0 ] \n    else : \n        exons = parse_ensembl_exons ( exon_lines ) \n        nr_exons = 1000000 \n    start_insertion = datetime . now ( ) \n    loaded_exons = 0 \n    LOG . info ( \"Loading exons...\" ) \n    with progressbar ( exons , label = \"Loading exons\" , length = nr_exons ) as bar : \n        for exon in bar : \n            ensg_id = exon [ 'gene' ] \n            enst_id = exon [ 'transcript' ] \n            gene_obj = ensembl_genes . get ( ensg_id ) \n            if not gene_obj : \n                continue \n            hgnc_id = gene_obj [ 'hgnc_id' ] \n            if not enst_id in hgnc_id_transcripts [ hgnc_id ] : \n                continue \n            exon [ 'hgnc_id' ] = hgnc_id \n            exon_obj = build_exon ( exon , build ) \n            adapter . load_exon ( exon_obj ) \n            loaded_exons = loaded_exons + ( 1 ) \n    LOG . info ( 'Number of exons in build {0}: {1}' . format ( build , nr_exons ) ) \n    LOG . info ( 'Number loaded: {0}' . format ( loaded_exons ) ) \n    LOG . info ( 'Time to load exons: {0}' . format ( datetime . now ( ) - start_insertion ) ) "}
{"7657": "\ndef update_indexes ( self ) : \n    LOG . info ( \"Updating indexes...\" ) \n    nr_updated = 0 \n    for collection_name in INDEXES : \n        existing_indexes = self . indexes ( collection_name ) \n        indexes = INDEXES [ collection_name ] \n        for index in indexes : \n            index_name = index . document . get ( 'name' ) \n            if index_name not in existing_indexes : \n                nr_updated = nr_updated + ( 1 ) \n                LOG . info ( \"Adding index : %s\" % index_name ) \n                self . db [ collection_name ] . create_indexes ( indexes ) \n    if nr_updated == 0 : \n        LOG . info ( \"All indexes in place\" ) "}
{"7690": "\ndef matchmaker_match ( institute_id , case_name , target ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_accepts = current_app . config . get ( 'MME_ACCEPTS' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    nodes = current_app . mme_nodes \n    if not mme_base_url or not mme_token or not mme_accepts : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    match_results = controllers . mme_match ( case_obj , target , mme_base_url , mme_token , nodes , mme_accepts ) \n    ok_responses = 0 \n    for match_results in match_results : \n        match_results [ 'status_code' ] == 200 \n        ok_responses = ok_responses + ( 1 ) \n    if ok_responses : \n        flash ( \"Match request sent. Look for eventual matches in 'Matches' page.\" , 'info' ) \n    else : \n        flash ( 'An error occurred while sending match request.' , 'danger' ) \n    return redirect ( request . referrer ) "}
{"7691": "\ndef matchmaker_delete ( institute_id , case_name ) : \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    if not mme_base_url or not mme_token : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    delete_result = controllers . mme_delete ( case_obj , mme_base_url , mme_token ) \n    n_deleted = 0 \n    category = 'warning' \n    for resp in delete_result : \n        if resp [ 'status_code' ] == 200 : \n            n_deleted = n_deleted + ( 1 ) \n        else : \n            flash ( resp [ 'message' ] , category ) \n    if n_deleted : \n        category = 'success' \n        user_obj = store . user ( current_user . email ) \n        store . case_mme_delete ( case_obj = case_obj , user_obj = user_obj ) \n    flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n_deleted , len ( delete_result ) ) , category ) \n    return redirect ( request . referrer ) "}
{"7713": "\ndef mt_excel_files ( store , case_obj , temp_excel_dir ) : \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    samples = case_obj . get ( 'individuals' ) \n    query = { 'chrom' : 'MT' } \n    mt_variants = list ( store . variants ( case_id = case_obj [ '_id' ] , query = query , nr_of_variants = - 1 , sort_key = 'position' ) ) \n    written_files = 0 \n    for sample in samples : \n        sample_id = sample [ 'individual_id' ] \n        sample_lines = export_mt_variants ( variants = mt_variants , sample_id = sample_id ) \n        document_name = '.' . join ( [ case_obj [ 'display_name' ] , sample_id , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( temp_excel_dir , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        row = 0 \n        for col , field in enumerate ( MT_EXPORT_HEADER ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( sample_lines , 1 ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( temp_excel_dir , document_name ) ) : \n            written_files = written_files + ( 1 ) \n    return written_files "}
{"7778": "\ndef popover_helper ( self ) : \n    display_month = month_name [ self . mo ] \n    if isinstance ( display_month , six . binary_type ) and self . encoding : \n        display_month = display_month . decode ( 'utf-8' ) \n    self . when = ( '<p><b>When:</b> ' + display_month + ' ' + str ( self . day ) + ', ' + self . event . l_start_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l_end_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + '</p>' ) \n    if self . event . location . exists ( ) : \n        self . where = '<p><b>Where:</b> ' \n        for l in self . event . location . all ( ) : \n            self . where = self . where + ( l . name ) \n        self . where = self . where + ( '</p>' ) \n    else : \n        self . where = '' \n    self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] \n    self . desc = self . desc + ( ( '...</p>' if len ( self . event . description ) > 100 else '</p>' ) ) \n    self . event_url = self . event . get_absolute_url ( ) \n    t = LEGACY_CALENDAR_TIME_FORMAT if self . event . l_start_date . minute else LEGACY_CALENDAR_HOUR_FORMAT \n    self . title2 = ( self . event . l_start_date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title ) "}
{"7788": "\ndef get_general_case_info ( adapter , institute_id = None , slice_query = None ) : \n    general = { } \n    name_query = slice_query \n    cases = adapter . cases ( owner = institute_id , name_query = name_query ) \n    phenotype_cases = 0 \n    causative_cases = 0 \n    pinned_cases = 0 \n    cohort_cases = 0 \n    pedigree = { 1 : { 'title' : 'Single' , 'count' : 0 } , 2 : { 'title' : 'Duo' , 'count' : 0 } , 3 : { 'title' : 'Trio' , 'count' : 0 } , 'many' : { 'title' : 'Many' , 'count' : 0 } , } \n    case_ids = set ( ) \n    total_cases = 0 \n    for total_cases , case in enumerate ( cases , 1 ) : \n        if institute_id : \n            case_ids . add ( case [ '_id' ] ) \n        if case . get ( 'phenotype_terms' ) : \n            phenotype_cases = phenotype_cases + ( 1 ) \n        if case . get ( 'causatives' ) : \n            causative_cases = causative_cases + ( 1 ) \n        if case . get ( 'suspects' ) : \n            pinned_cases = pinned_cases + ( 1 ) \n        if case . get ( 'cohorts' ) : \n            cohort_cases = cohort_cases + ( 1 ) \n        nr_individuals = len ( case . get ( 'individuals' , [ ] ) ) \n        if nr_individuals == 0 : \n            continue \n        if nr_individuals > 3 : \n            pedigree [ 'many' ] [ 'count' ] = pedigree [ 'many' ] [ 'count' ] + ( 1 ) \n        else : \n            pedigree [ nr_individuals ] [ 'count' ] = pedigree [ nr_individuals ] [ 'count' ] + ( 1 ) \n    general [ 'total_cases' ] = total_cases \n    general [ 'phenotype_cases' ] = phenotype_cases \n    general [ 'causative_cases' ] = causative_cases \n    general [ 'pinned_cases' ] = pinned_cases \n    general [ 'cohort_cases' ] = cohort_cases \n    general [ 'pedigree' ] = pedigree \n    general [ 'case_ids' ] = case_ids \n    return general "}
{"7792": "\ndef check_for_cancelled_events ( self , d ) : \n    for event in self . events : \n        for cn in event . cancellations . all ( ) : \n            if cn . date == d : \n                event . title = event . title + ( ' (CANCELLED)' ) "}
{"7794": "\ndef hpo_terms ( self , query = None , hpo_term = None , text = None , limit = None ) : \n    query_dict = { } \n    search_term = None \n    if query : \n        query_dict = { '$or' : [ { 'hpo_id' : { '$regex' : query , '$options' : 'i' } } , { 'description' : { '$regex' : query , '$options' : 'i' } } , ] } \n        search_term = query \n    elif text : \n        new_string = '' \n        for i , word in enumerate ( text . split ( ' ' ) ) : \n            if i == 0 : \n                new_string = new_string + ( word ) \n            else : \n                new_string = new_string + ( ' \\\"{0}\\\"' . format ( word ) ) \n        LOG . info ( \"Search HPO terms with %s\" , new_string ) \n        query_dict [ '$text' ] = { '$search' : new_string } \n        search_term = text \n    elif hpo_term : \n        query_dict [ 'hpo_id' ] = hpo_term \n        search_term = hpo_term \n    limit = limit or int ( 10e10 ) \n    res = self . hpo_term_collection . find ( query_dict ) . limit ( limit ) . sort ( 'hpo_number' , ASCENDING ) \n    LOG . info ( \"Found {0} terms with search word {1}\" . format ( res . count ( ) , search_term ) ) \n    return res "}
{"7798": "\ndef generate_hpo_gene_list ( self , * hpo_terms ) : \n    genes = { } \n    for term in hpo_terms : \n        hpo_obj = self . hpo_term ( term ) \n        if hpo_obj : \n            for hgnc_id in hpo_obj [ 'genes' ] : \n                if hgnc_id in genes : \n                    genes [ hgnc_id ] = genes [ hgnc_id ] + ( 1 ) \n                else : \n                    genes [ hgnc_id ] = 1 \n        else : \n            LOG . warning ( \"Term %s could not be found\" , term ) \n    sorted_genes = sorted ( genes . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n    return sorted_genes "}
{"7805": "\ndef info ( self ) : \n    for key , val in self . header . items ( ) : \n        if key == b'src_raj' : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if key == b'src_dej' : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        if key == b'tsamp' : \n            val = val * ( u . second ) \n        if key in ( 'foff' , 'fch1' ) : \n            val = val * ( u . MHz ) \n        if key == b'tstart' : \n            print ( \"%16s : %32s\" % ( \"tstart (ISOT)\" , Time ( val , format = 'mjd' ) . isot ) ) \n            key = \"tstart (MJD)\" \n        print ( \"%16s : %32s\" % ( key , val ) ) \n    print ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n_ints_in_file ) ) \n    print ( \"%16s : %32s\" % ( \"Data shape\" , self . data . shape ) ) \n    print ( \"%16s : %32s\" % ( \"Start freq (MHz)\" , self . freqs [ 0 ] ) ) \n    print ( \"%16s : %32s\" % ( \"Stop freq (MHz)\" , self . freqs [ - 1 ] ) ) "}
{"7848": "\ndef find_n_data_blocks ( self ) : \n    self . file_obj . seek ( 0 ) \n    header0 , data_idx0 = self . read_header ( ) \n    self . file_obj . seek ( data_idx0 ) \n    block_size = int ( header0 [ 'BLOCSIZE' ] ) \n    n_bits = int ( header0 [ 'NBITS' ] ) \n    self . file_obj . seek ( int ( header0 [ 'BLOCSIZE' ] ) , 1 ) \n    n_blocks = 1 \n    end_found = False \n    while not end_found : \n        try : \n            header , data_idx = self . read_header ( ) \n            self . file_obj . seek ( data_idx ) \n            self . file_obj . seek ( header [ 'BLOCSIZE' ] , 1 ) \n            n_blocks = n_blocks + ( 1 ) \n        except EndOfFileError : \n            end_found = True \n            break \n    self . file_obj . seek ( 0 ) \n    return n_blocks "}
{"7854": "\ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    if not HAS_BITSHUFFLE : \n        print ( \"Error: the bitshuffle library is required to run this script.\" ) \n        exit ( ) \n    parser = ArgumentParser ( description = \"Command line utility for creating HDF5 Raw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) \n    args = parser . parse_args ( ) \n    fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] \n    filelist = glob . glob ( fileroot + '*.raw' ) \n    filelist = sorted ( filelist ) \n    r = GuppiRaw ( filelist [ 0 ] ) \n    header , data = r . read_next_data_block ( ) \n    dshape = data . shape \n    print ( dshape ) \n    n_blocks_total = 0 \n    for filename in filelist : \n        print ( filename ) \n        r = GuppiRaw ( filename ) \n        n_blocks_total = n_blocks_total + ( r . n_blocks ) \n    print ( n_blocks_total ) \n    full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) \n    h5 = h5py . File ( fileroot + '.h5' , 'w' ) \n    h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' \n    block_size = 0 \n    dset = h5 . create_dataset ( 'data' , shape = full_dshape , dtype = data . dtype ) \n    h5_idx = 0 \n    for filename in filelist : \n        print ( \"\\nReading %s header...\" % filename ) \n        r = GuppiRaw ( filename ) \n        h5 = h5py . File ( filename + '.h5' , 'w' ) \n        header , data = r . read_next_data_block ( ) \n        for ii in range ( 0 , r . n_blocks ) : \n            t0 = time . time ( ) \n            print ( \"Reading block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            header , data = r . read_next_data_block ( ) \n            t1 = time . time ( ) \n            t2 = time . time ( ) \n            print ( \"Writing block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            dset [ h5_idx , : ] = data \n            t3 = time . time ( ) \n            print ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) \n            h5_idx = h5_idx + ( 1 ) \n            for key , value in header . items ( ) : \n                dset . attrs [ key ] = value \n        h5 . close ( ) \n        t1 = time . time ( ) \n        print ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) ) "}
{"7863": "\ndef len_header ( filename ) : \n    with open ( filename , 'rb' ) as f : \n        header_sub_count = 0 \n        eoh_found = False \n        while not eoh_found : \n            header_sub = f . read ( 512 ) \n            header_sub_count = header_sub_count + ( 1 ) \n            if b'HEADER_END' in header_sub : \n                idx_end = header_sub . index ( b'HEADER_END' ) + len ( b'HEADER_END' ) \n                eoh_found = True \n                break \n        idx_end = ( header_sub_count - 1 ) * 512 + idx_end \n    return idx_end "}
{"7866": "\ndef generate_sigproc_header ( f ) : \n    header_string = b'' \n    header_string = header_string + ( to_sigproc_keyword ( b'HEADER_START' ) ) \n    for keyword in f . header . keys ( ) : \n        if keyword == b'src_raj' : \n            header_string = header_string + ( to_sigproc_keyword ( b'src_raj' ) + to_sigproc_angle ( f . header [ b'src_raj' ] ) ) \n        elif keyword == b'src_dej' : \n            header_string = header_string + ( to_sigproc_keyword ( b'src_dej' ) + to_sigproc_angle ( f . header [ b'src_dej' ] ) ) \n        elif keyword == b'az_start' or keyword == b'za_start' : \n            header_string = header_string + ( to_sigproc_keyword ( keyword ) + np . float64 ( f . header [ keyword ] ) . tostring ( ) ) \n        elif keyword not in header_keyword_types . keys ( ) : \n            pass \n        else : \n            header_string = header_string + ( to_sigproc_keyword ( keyword , f . header [ keyword ] ) ) \n    header_string = header_string + ( to_sigproc_keyword ( b'HEADER_END' ) ) \n    return header_string "}
{"7915": "\ndef makePlot ( args ) : \n    gRvs = np . linspace ( 5.7 , 16.1 , 101 ) \n    spts = [ 'B0V' , 'B5V' , 'A0V' , 'A5V' , 'F0V' , 'G0V' , 'G5V' , 'K0V' , 'K1IIIMP' , 'K4V' , 'K1III' ] \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    deltaHue = 240.0 / ( len ( spts ) - 1 ) \n    hsv = np . zeros ( ( 1 , 1 , 3 ) ) \n    hsv [ 0 , 0 , 1 ] = 1.0 \n    hsv [ 0 , 0 , 2 ] = 0.9 \n    count = 0 \n    for spt in spts : \n        hsv [ 0 , 0 , 0 ] = ( 240 - count * deltaHue ) / 360.0 \n        vmag = vminGrvsFromVmini ( vminiFromSpt ( spt ) ) + gRvs \n        vradErrors = vradErrorSkyAvg ( vmag , spt ) \n        plt . plot ( vmag , vradErrors , '-' , label = spt , color = hsv_to_rgb ( hsv ) [ 0 , 0 , : ] ) \n        count = count + ( 1 ) \n    plt . grid ( which = 'both' ) \n    plt . xlim ( 9 , 17.5 ) \n    plt . ylim ( 0 , 20 ) \n    plt . xticks ( np . arange ( 9 , 18 , 1 ) ) \n    plt . yticks ( np . arange ( 0 , 20.5 , 5 ) ) \n    plt . xlabel ( '$V$ [mag]' ) \n    plt . ylabel ( 'End-of-mission radial velocity error [km s$^{-1}$]' ) \n    leg = plt . legend ( loc = 0 , handlelength = 2.0 , labelspacing = 0.10 ) \n    for t in leg . get_texts ( ) : \n        t . set_fontsize ( 12 ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.png' ) \n    else : \n        plt . show ( ) "}
{"7929": "\ndef eventstr ( event_tuple = None , event = None , register = None , parameters = None ) : \n    if len ( event_tuple ) == 3 : \n        event , register , parameters = event_tuple \n    elif len ( event_tuple ) == 2 : \n        event , register = event_tuple \n    event_dscr = [ event , register ] \n    if parameters : \n        for k , v in sorted ( event_tuple [ 2 ] . items ( ) ) : \n            if type ( v ) is int : \n                k = k + ( \"={}\" . format ( hex ( v ) ) ) \n            event_dscr . append ( k ) \n    return \":\" . join ( event_dscr ) "}
{"7930": "\ndef build_minimal_runs ( events ) : \n    events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] \n    scheduled_runs = { } \n    scheduled_events = [ ] \n    cur_run = 0 \n    while len ( scheduled_events ) != len ( events ) : \n        for event_tpl in events : \n            event , registers , parameters = event_tpl \n            if event_tpl in scheduled_events : \n                continue \n            for possible_reg in register_options ( registers ) : \n                s = scheduled_runs . setdefault ( cur_run , { } ) \n                if possible_reg not in s : \n                    s [ possible_reg ] = ( event , possible_reg , parameters ) \n                    scheduled_events . append ( event_tpl ) \n                    break \n        cur_run = cur_run + ( 1 ) \n    runs = [ list ( v . values ( ) ) for v in scheduled_runs . values ( ) ] \n    return runs "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while idx < len ( code ) - 1 : \n            if comment_start is None and code [ idx : idx + 2 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx = idx - ( end_idx - idx ) \n            elif comment_start is None and code [ idx : idx + 2 ] == '/*' : \n                comment_start = idx \n            elif comment_start is not None and code [ idx : idx + 2 ] == '*/' : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                idx = idx - ( idx - comment_start ) \n                comment_start = None \n            idx = idx + ( 1 ) \n    return code "}
{"7938": "\ndef calculate_cycles ( self ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    elements_per_cacheline = float ( self . machine [ 'cacheline size' ] ) // element_size \n    iterations_per_cacheline = ( sympy . Integer ( self . machine [ 'cacheline size' ] ) / sympy . Integer ( self . kernel . bytes_per_iteration ) ) \n    self . results [ 'iterations per cacheline' ] = iterations_per_cacheline \n    cacheline_size = float ( self . machine [ 'cacheline size' ] ) \n    loads , stores = ( self . predictor . get_loads ( ) , self . predictor . get_stores ( ) ) \n    for cache_level , cache_info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) [ 1 : ] : \n        throughput , duplexness = cache_info [ 'non-overlap upstream throughput' ] \n        if type ( throughput ) is str and throughput == 'full socket memory bandwidth' : \n            read_streams = loads [ cache_level ] \n            write_streams = stores [ cache_level ] \n            threads_per_core = 1 \n            bw , measurement_kernel = self . machine . get_bandwidth ( cache_level , read_streams , write_streams , threads_per_core ) \n            if duplexness == 'half-duplex' : \n                cycles = float ( loads [ cache_level ] + stores [ cache_level ] ) * float ( elements_per_cacheline ) * float ( element_size ) * float ( self . machine [ 'clock' ] ) / float ( bw ) \n            else : \n                raise NotImplementedError ( \"full-duplex mode is not (yet) supported for memory transfers.\" ) \n            if 'penalty cycles per read stream' in cache_info : \n                cycles = cycles + ( stores [ cache_level ] * cache_info [ 'penalty cycles per read stream' ] ) \n            self . results . update ( { 'memory bandwidth kernel' : measurement_kernel , 'memory bandwidth' : bw } ) \n        else : \n            throughput = float ( throughput ) / cacheline_size \n            if duplexness == 'half-duplex' : \n                cycles = ( loads [ cache_level ] + stores [ cache_level ] ) / float ( throughput ) \n            elif duplexness == 'full-duplex' : \n                cycles = max ( loads [ cache_level ] / float ( throughput ) , stores [ cache_level ] / float ( throughput ) ) \n            else : \n                raise ValueError ( \"Duplexness of cache throughput may only be 'half-duplex'\" \"or 'full-duplex', found {} in {}.\" . format ( duplexness , cache_info [ 'name' ] ) ) \n        self . results [ 'cycles' ] . append ( ( cache_info [ 'level' ] , cycles ) ) \n        self . results [ cache_info [ 'level' ] ] = cycles \n    return self . results "}
{"7945": "\ndef userselect_block ( blocks , default = None , debug = False ) : \n    print ( \"Blocks found in assembly file:\" ) \n    print ( \"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) \n    for idx , b in blocks : \n        print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) \n        if debug : \n            ln = b [ 'first_line' ] \n            print ( ' ' * 4 + 'Code:' ) \n            for l in b [ 'lines' ] : \n                print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) \n                ln = ln + ( 1 ) \n            print ( ' ' * 4 + 'Metadata:' ) \n            print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) \n    block_idx = - 1 \n    while not ( 0 <= block_idx < len ( blocks ) ) : \n        block_idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - 1 \n    return block_idx "}
{"7950": "\ndef space ( start , stop , num , endpoint = True , log = False , base = 10 ) : \n    assert type ( start ) is int and type ( stop ) is int and type ( num ) is int , \"start, stop and num need to be intergers\" \n    assert num >= 2 , \"num has to be atleast 2\" \n    if log : \n        start = math . log ( start , base ) \n        stop = math . log ( stop , base ) \n    if endpoint : \n        step_length = float ( ( stop - start ) ) / float ( num - 1 ) \n    else : \n        step_length = float ( ( stop - start ) ) / float ( num ) \n    i = 0 \n    while i < num : \n        if log : \n            yield int ( round ( base ** ( start + i * step_length ) ) ) \n        else : \n            yield int ( round ( start + i * step_length ) ) \n        i = i + ( 1 ) "}
{"7963": "\ndef array_sizes ( self , in_bytes = False , subs_consts = False ) : \n    var_sizes = { } \n    for var_name , var_info in self . variables . items ( ) : \n        var_type , var_size = var_info \n        if var_size is None : \n            continue \n        var_sizes [ var_name ] = reduce ( operator . mul , var_size , 1 ) \n        if in_bytes : \n            element_size = self . datatypes_size [ var_type ] \n            var_sizes [ var_name ] = var_sizes [ var_name ] * ( element_size ) \n    if subs_consts : \n        return { k : self . subs_consts ( v ) for k , v in var_sizes . items ( ) } \n    else : \n        return var_sizes "}
{"7964": "\ndef _calculate_relative_offset ( self , name , access_dimensions ) : \n    offset = 0 \n    base_dims = self . variables [ name ] [ 1 ] \n    for dim , offset_info in enumerate ( access_dimensions ) : \n        offset_type , idx_name , dim_offset = offset_info \n        assert offset_type == 'rel' , 'Only relative access to arrays is supported at the moment' \n        if offset_type == 'rel' : \n            offset = offset + ( self . subs_consts ( dim_offset * reduce ( operator . mul , base_dims [ dim + 1 : ] , sympy . Integer ( 1 ) ) ) ) \n        else : \n            pass \n    return offset "}
{"7972": "\ndef global_iterator ( self ) : \n    global_iterator = sympy . Integer ( 0 ) \n    total_length = sympy . Integer ( 1 ) \n    for var_name , start , end , incr in reversed ( self . _loop_stack ) : \n        loop_var = symbol_pos_int ( var_name ) \n        length = end - start \n        global_iterator = global_iterator + ( ( loop_var - start ) * total_length ) \n        total_length = total_length * ( length ) \n    return global_iterator "}
{"7975": "\ndef print_kernel_info ( self , output_file = sys . stdout ) : \n    table = ( '     idx |        min        max       step\\n' + '---------+---------------------------------\\n' ) \n    for l in self . _loop_stack : \n        table = table + ( '{:>8} | {!r:>10} {!r:>10} {!r:>10}\\n' . format ( * l ) ) \n    print ( prefix_indent ( 'loop stack:        ' , table ) , file = output_file ) \n    table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) \n    for name , offsets in list ( self . sources . items ( ) ) : \n        prefix = '{:>8} | ' . format ( name ) \n        right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) \n        table = table + ( prefix_indent ( prefix , right_side , later_prefix = '         | ' ) ) \n    print ( prefix_indent ( 'data sources:      ' , table ) , file = output_file ) \n    table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) \n    for name , offsets in list ( self . destinations . items ( ) ) : \n        prefix = '{:>8} | ' . format ( name ) \n        right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) \n        table = table + ( prefix_indent ( prefix , right_side , later_prefix = '         | ' ) ) \n    print ( prefix_indent ( 'data destinations: ' , table ) , file = output_file ) \n    table = ( ' op | count \\n' + '----+-------\\n' ) \n    for op , count in list ( self . _flops . items ( ) ) : \n        table = table + ( '{:>3} | {:>4}\\n' . format ( op , count ) ) \n    table = table + ( '     =======\\n' ) \n    table = table + ( '      {:>4}' . format ( sum ( self . _flops . values ( ) ) ) ) \n    print ( prefix_indent ( 'FLOPs:     ' , table ) , file = output_file ) "}
{"7976": "\ndef print_variables_info ( self , output_file = sys . stdout ) : \n    table = ( '    name |   type size             \\n' + '---------+-------------------------\\n' ) \n    for name , var_info in list ( self . variables . items ( ) ) : \n        table = table + ( '{:>8} | {:>6} {!s:<10}\\n' . format ( name , var_info [ 0 ] , var_info [ 1 ] ) ) \n    print ( prefix_indent ( 'variables: ' , table ) , file = output_file ) "}
{"7977": "\ndef print_constants_info ( self , output_file = sys . stdout ) : \n    table = ( '    name | value     \\n' + '---------+-----------\\n' ) \n    for name , value in list ( self . constants . items ( ) ) : \n        table = table + ( '{!s:>8} | {:<10}\\n' . format ( name , value ) ) \n    print ( prefix_indent ( 'constants: ' , table ) , file = output_file ) "}
{"7980": "\ndef _get_offsets ( self , aref , dim = 0 ) : \n    if isinstance ( aref , c_ast . ID ) : \n        return None \n    assert type ( aref . name ) in [ c_ast . ArrayRef , c_ast . ID ] , \"array references must only be used with variables or other array references\" \n    assert type ( aref . subscript ) in [ c_ast . ID , c_ast . Constant , c_ast . BinaryOp ] , 'array subscript must only contain variables or binary operations' \n    idxs = [ self . conv_ast_to_sym ( aref . subscript ) ] \n    if type ( aref . name ) is c_ast . ArrayRef : \n        idxs = idxs + ( self . _get_offsets ( aref . name , dim = dim + 1 ) ) \n    if dim == 0 : \n        idxs . reverse ( ) \n    return tuple ( idxs ) "}
{"7983": "\ndef _build_const_declartions ( self , with_init = True ) : \n    decls = [ ] \n    index_type = self . get_index_type ( ) \n    i = 2 \n    for k in self . constants : \n        type_decl = c_ast . TypeDecl ( k . name , [ 'const' ] , c_ast . IdentifierType ( index_type ) ) \n        init = None \n        if with_init : \n            init = c_ast . FuncCall ( c_ast . ID ( 'atoi' ) , c_ast . ExprList ( [ c_ast . ArrayRef ( c_ast . ID ( 'argv' ) , c_ast . Constant ( 'int' , str ( i ) ) ) ] ) ) \n        i = i + ( 1 ) \n        decls . append ( c_ast . Decl ( k . name , [ 'const' ] , [ ] , [ ] , type_decl , init , None ) ) \n    return decls "}
{"7992": "\ndef get_kernel_code ( self , openmp = False , as_filename = False , name = 'kernel' ) : \n    assert self . kernel_ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" \n    file_name = 'kernel' \n    if openmp : \n        file_name = file_name + ( '-omp' ) \n    file_name = file_name + ( '.c' ) \n    fp , already_available = self . _get_intermediate_file ( file_name , machine_and_compiler_dependent = False ) \n    if already_available : \n        code = fp . read ( ) \n    else : \n        array_declarations , array_dimensions = self . _build_array_declarations ( ) \n        if openmp : \n            kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) \n            for aref in find_node_type ( kernel , c_ast . ArrayRef ) : \n                transform_multidim_to_1d_ref ( aref , array_dimensions ) \n            omp_pragmas = [ p for p in find_node_type ( kernel , c_ast . Pragma ) if 'omp' in p . string ] \n            if not omp_pragmas : \n                kernel . insert ( 0 , c_ast . Pragma ( \"omp for\" ) ) \n        else : \n            kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) \n            for aref in find_node_type ( kernel , c_ast . ArrayRef ) : \n                transform_multidim_to_1d_ref ( aref , array_dimensions ) \n        function_ast = c_ast . FuncDef ( decl = c_ast . Decl ( name = name , type = self . _build_kernel_function_declaration ( name = name ) , quals = [ ] , storage = [ ] , funcspec = [ ] , init = None , bitsize = None ) , body = c_ast . Compound ( block_items = kernel ) , param_decls = None ) \n        code = CGenerator ( ) . visit ( function_ast ) \n        code = '#include \"kerncraft.h\"\\n\\n' + code \n        fp . write ( code ) \n    fp . close ( ) \n    if as_filename : \n        return fp . name \n    else : \n        return code "}
{"7996": "\ndef build_executable ( self , lflags = None , verbose = False , openmp = False ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = True ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            print ( 'Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) \n            sys . exit ( 1 ) \n        compiler_args = compiler_args + ( [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] ) \n        if os . environ . get ( 'LIKWID_LIB' ) == '' : \n            compiler_args = compiler_args [ : - 1 ] \n        if lflags is None : \n            lflags = [ ] \n        lflags = lflags + ( os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] ) \n        compiler_args = compiler_args + ( os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] ) \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            print ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            print ( \"Build failed:\" , e , file = sys . stderr ) \n            sys . exit ( 1 ) \n    else : \n        if verbose : \n            print ( 'Executing (build_executable): ' , 'using cached' , out_filename ) \n    return out_filename "}
{"8000": "\ndef get_cachesim ( self , cores = 1 ) : \n    cache_dict = { } \n    for c in self [ 'memory hierarchy' ] : \n        if 'cache per group' not in c : \n            continue \n        cache_dict [ c [ 'level' ] ] = deepcopy ( c [ 'cache per group' ] ) \n        if c [ 'cores per group' ] > 1 : \n            cache_dict [ c [ 'level' ] ] [ 'sets' ] = cache_dict [ c [ 'level' ] ] [ 'sets' ] // ( cores ) \n    cs , caches , mem = cachesim . CacheSimulator . from_dict ( cache_dict ) \n    return cs "}
{"8004": "\ndef _enforce_no_overlap ( self , start_at = 0 ) : \n    i = start_at \n    while i + 1 < len ( self . data ) : \n        if self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 0 ] : \n            if self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 1 ] : \n                self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] \n            del self . data [ i + 1 ] \n        i = i + ( 1 ) "}
{"8016": "\ndef parse_description ( ) : \n    from os . path import dirname , join , exists \n    readme_fpath = join ( dirname ( __file__ ) , 'README.md' ) \n    if exists ( readme_fpath ) : \n        textlines = [ ] \n        with open ( readme_fpath , 'r' ) as f : \n            capture = False \n            for line in f . readlines ( ) : \n                if '# Purpose' in line : \n                    capture = True \n                elif line . startswith ( '##' ) : \n                    break \n                elif capture : \n                    textlines = textlines + ( [ line ] ) \n        text = '' . join ( textlines ) . strip ( ) \n        text = text . replace ( '\\n\\n' , '_NLHACK_' ) \n        text = text . replace ( '\\n' , ' ' ) \n        text = text . replace ( '_NLHACK_' , '\\n\\n' ) \n        return text \n    return '' "}
{"8060": "\ndef recording_state ( recording_id , status ) : \n    if config ( ) [ 'agent' ] [ 'backup_mode' ] : \n        return \n    params = [ ( 'state' , status ) ] \n    url = config ( ) [ 'service-capture.admin' ] [ 0 ] \n    url = url + ( '/recordings/%s' % recording_id ) \n    try : \n        result = http_request ( url , params ) \n        logger . info ( result ) \n    except pycurl . error as e : \n        logger . warning ( 'Could not set recording state to %s: %s' % ( status , e ) ) "}
{"8077": "\ndef events ( ) : \n    db = get_session ( ) \n    upcoming_events = db . query ( UpcomingEvent ) . order_by ( UpcomingEvent . start ) \n    recorded_events = db . query ( RecordedEvent ) . order_by ( RecordedEvent . start . desc ( ) ) \n    result = [ event . serialize ( ) for event in upcoming_events ] \n    result = result + ( [ event . serialize ( ) for event in recorded_events ] ) \n    return make_data_response ( result ) "}
{"8082": "\ndef ingest ( event ) : \n    set_service_status ( Service . INGEST , ServiceStatus . BUSY ) \n    notify . notify ( 'STATUS=Uploading' ) \n    recording_state ( event . uid , 'uploading' ) \n    update_event_status ( event , Status . UPLOADING ) \n    service = config ( 'service-ingest' ) \n    service = service [ randrange ( 0 , len ( service ) ) ] \n    logger . info ( 'Selecting ingest service to use: ' + service ) \n    logger . info ( 'Creating new mediapackage' ) \n    mediapackage = http_request ( service + '/createMediaPackage' ) \n    prop = 'org.opencastproject.capture.agent.properties' \n    dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' \n    for attachment in event . get_data ( ) . get ( 'attach' ) : \n        data = attachment . get ( 'data' ) \n        if attachment . get ( 'x-apple-filename' ) == prop : \n            workflow_def , workflow_config = get_config_params ( data ) \n        elif attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : \n            name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] \n            logger . info ( 'Adding %s DC catalog' % name ) \n            fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublinCore' , data . encode ( 'utf-8' ) ) ] \n            mediapackage = http_request ( service + '/addDCCatalog' , fields ) \n    for ( flavor , track ) in event . get_tracks ( ) : \n        logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) \n        track = track . encode ( 'ascii' , 'ignore' ) \n        fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM_FILE , track ) ) ] \n        mediapackage = http_request ( service + '/addTrack' , fields ) \n    logger . info ( 'Ingest recording' ) \n    fields = [ ( 'mediaPackage' , mediapackage ) ] \n    if workflow_def : \n        fields . append ( ( 'workflowDefinitionId' , workflow_def ) ) \n    if event . uid : \n        fields . append ( ( 'workflowInstanceId' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) \n    fields = fields + ( workflow_config ) \n    mediapackage = http_request ( service + '/ingest' , fields ) \n    recording_state ( event . uid , 'upload_finished' ) \n    update_event_status ( event , Status . FINISHED_UPLOADING ) \n    notify . notify ( 'STATUS=Running' ) \n    set_service_status_immediate ( Service . INGEST , ServiceStatus . IDLE ) \n    logger . info ( 'Finished ingest' ) "}
{"8088": "\ndef add_content ( self , content ) : \n    assert isinstance ( content , six . text_type ) \n    self . content = self . content + ( content ) "}
{"8154": "\ndef get_client ( self ) : \n    client = None \n    try : \n        client = self . clients . get ( block = False ) \n    except queue . Empty : \n        pass \n    if not client : \n        self . client_id = self . client_id + ( 1 ) \n        kwargs = dict ( self . kwargs ) \n        kwargs [ 'verbose_id' ] = kwargs . get ( 'verbose_id' , '' ) + str ( self . client_id ) \n        client = self . client_class ( * self . args , ** kwargs ) \n    return client "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = Crypto . Random . new ( ) . read ( 16 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = True \n    left = None \n    if content_length is not None and content_length >= 0 : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and size > left : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and left > 0 : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16 ) \n            break \n        if left is not None : \n            left = left - ( len ( chunk ) ) \n            if left <= 0 : \n                reading = False \n        block = chunk \n        trailing = len ( block ) % 16 \n        while trailing : \n            size = 16 - trailing \n            if left is not None and size > left : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and left > 0 : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = False \n                chunk = chr ( trailing ) * ( 16 - trailing ) \n            elif left is not None : \n                left = left - ( len ( chunk ) ) \n                if left <= 0 : \n                    reading = False \n            block = block + ( chunk ) \n            trailing = len ( block ) % 16 \n        yield encryptor . encrypt ( block ) "}
{"8156": "\ndef aes_decrypt ( key , stdin , chunk_size = 65536 ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = stdin . read ( 16 ) \n    while len ( iv ) < 16 : \n        chunk = stdin . read ( 16 - len ( iv ) ) \n        if not chunk : \n            raise IOError ( 'EOF reading IV' ) \n    decryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    data = '' \n    while True : \n        chunk = stdin . read ( chunk_size ) \n        if not chunk : \n            if len ( data ) != 16 : \n                raise IOError ( 'EOF reading encrypted stream' ) \n            data = decryptor . decrypt ( data ) \n            trailing = ord ( data [ - 1 ] ) \n            if trailing > 15 : \n                raise IOError ( 'EOF reading encrypted stream or trailing value corrupted ' '%s' % trailing ) \n            yield data [ : trailing ] \n            break \n        data = data + ( chunk ) \n        if len ( data ) > 16 : \n            trailing = ( len ( data ) % 16 ) or 16 \n            yield decryptor . decrypt ( data [ : - trailing ] ) \n            data = data [ - trailing : ] "}
{"8157": "\ndef cli_put_directory_structure ( context , path ) : \n    if not context . input_ : \n        raise ReturnCode ( 'called cli_put_directory_structure without context.input_ set' ) \n    if not os . path . isdir ( context . input_ ) : \n        raise ReturnCode ( '%r is not a directory' % context . input_ ) \n    if not path : \n        raise ReturnCode ( 'uploading a directory structure requires at least a container ' 'name' ) \n    new_context = context . copy ( ) \n    new_context . input_ = None \n    container = path . split ( '/' , 1 ) [ 0 ] \n    cli_put_container ( new_context , container ) \n    ilen = len ( context . input_ ) \n    if not context . input_ . endswith ( os . sep ) : \n        ilen = ilen + ( 1 ) \n    conc = Concurrency ( context . concurrency ) \n    for ( dirpath , dirnames , filenames ) in os . walk ( context . input_ ) : \n        if not dirnames and not filenames : \n            new_context = context . copy ( ) \n            new_context . headers = dict ( context . headers ) \n            new_context . headers [ 'content-type' ] = 'text/directory' \n            new_context . headers [ 'x-object-meta-mtime' ] = '%f' % os . path . getmtime ( context . input_ ) \n            new_context . input_ = None \n            new_context . empty = True \n            new_path = path \n            if path [ - 1 ] != '/' : \n                new_path = new_path + ( '/' ) \n            new_path = new_path + ( dirpath [ ilen : ] ) \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n        else : \n            for fname in filenames : \n                new_context = context . copy ( ) \n                new_context . input_ = os . path . join ( dirpath , fname ) \n                new_path = path \n                if path [ - 1 ] != '/' : \n                    new_path = new_path + ( '/' ) \n                if dirpath [ ilen : ] : \n                    new_path = new_path + ( dirpath [ ilen : ] + '/' ) \n                new_path = new_path + ( fname ) \n                for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                    if exc_value : \n                        conc . join ( ) \n                        raise exc_value \n                conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            raise exc_value "}
{"8163": "\ndef cli_trans ( context , x_trans_id ) : \n    with context . io_manager . with_stdout ( ) as fp : \n        trans_time = get_trans_id_time ( x_trans_id ) \n        trans_info = x_trans_id [ 34 : ] \n        msg = 'X-Trans-Id:      ' + x_trans_id + '\\n' \n        if not trans_time : \n            msg = msg + ( 'Time Stamp:      None, old style id with no time ' 'embedded\\nUTC Time:        None, old style id with no time ' 'embedded\\n' ) \n        else : \n            msg = msg + ( 'Time Stamp:      %s\\nUTC Time:        %s\\n' % ( trans_time , time . strftime ( '%a %Y-%m-%d %H:%M:%S UTC' , time . gmtime ( trans_time ) ) ) ) \n        msg = msg + ( 'Additional Info: ' + trans_info + '\\n' ) \n        fp . write ( msg ) \n        fp . flush ( ) "}
{"8173": "\ndef reader_acquire ( self ) : \n    self . _order_mutex . acquire ( ) \n    self . _readers_mutex . acquire ( ) \n    if self . _readers == 0 : \n        self . _access_mutex . acquire ( ) \n    self . _readers = self . _readers + ( 1 ) \n    self . _order_mutex . release ( ) \n    self . _readers_mutex . release ( ) "}
{"8174": "\ndef reader_release ( self ) : \n    self . _readers_mutex . acquire ( ) \n    self . _readers = self . _readers - ( 1 ) \n    if self . _readers == 0 : \n        self . _access_mutex . release ( ) \n    self . _readers_mutex . release ( ) "}
{"8182": "\ndef execute_perceval_job ( backend , backend_args , qitems , task_id , category , archive_args = None , max_retries = MAX_JOB_RETRIES ) : \n    rq_job = rq . get_current_job ( ) \n    job = PercevalJob ( rq_job . id , task_id , backend , category , rq_job . connection , qitems ) \n    logger . debug ( \"Running job #%s (task: %s) (%s) (cat:%s)\" , job . job_id , task_id , backend , category ) \n    if not job . has_archiving ( ) and archive_args : \n        raise AttributeError ( \"archive attributes set but archive is not supported\" ) \n    run_job = True \n    resume = False \n    failures = 0 \n    while run_job : \n        try : \n            job . run ( backend_args , archive_args = archive_args , resume = resume ) \n        except AttributeError as e : \n            raise e \n        except Exception as e : \n            logger . debug ( \"Error running job %s (%s) - %s\" , job . job_id , backend , str ( e ) ) \n            failures = failures + ( 1 ) \n            if not job . has_resuming ( ) or failures >= max_retries : \n                logger . error ( \"Cancelling job #%s (task: %s) (%s)\" , job . job_id , task_id , backend ) \n                raise e \n            logger . warning ( \"Resuming job #%s (task: %s) (%s) due to a failure (n %s, max %s)\" , job . job_id , task_id , backend , failures , max_retries ) \n            resume = True \n        else : \n            run_job = False \n    result = job . result \n    logger . debug ( \"Job #%s (task: %s) completed (%s) - %s items (%s) fetched\" , result . job_id , task_id , result . backend , str ( result . nitems ) , result . category ) \n    return result "}
{"8184": "\ndef run ( self , backend_args , archive_args = None , resume = False ) : \n    args = backend_args . copy ( ) \n    if archive_args : \n        self . initialize_archive_manager ( archive_args [ 'archive_path' ] ) \n    if not resume : \n        max_date = backend_args . get ( 'from_date' , None ) \n        offset = backend_args . get ( 'offset' , None ) \n        if max_date : \n            max_date = datetime_to_utc ( max_date ) . timestamp ( ) \n        self . _result = JobResult ( self . job_id , self . task_id , self . backend , self . category , None , max_date , 0 , offset = offset , nresumed = 0 ) \n    else : \n        if self . result . max_date : \n            args [ 'from_date' ] = unixtime_to_datetime ( self . result . max_date ) \n        if self . result . offset : \n            args [ 'offset' ] = self . result . offset \n        self . _result . nresumed = self . _result . nresumed + ( 1 ) \n    for item in self . _execute ( args , archive_args ) : \n        self . conn . rpush ( self . qitems , pickle . dumps ( item ) ) \n        self . _result . nitems = self . _result . nitems + ( 1 ) \n        self . _result . last_uuid = item [ 'uuid' ] \n        if not self . result . max_date or self . result . max_date < item [ 'updated_on' ] : \n            self . _result . max_date = item [ 'updated_on' ] \n        if 'offset' in item : \n            self . _result . offset = item [ 'offset' ] "}
{"8210": "\ndef get_view_name ( self , respect_name = True ) : \n    if isinstance ( self , type ) : \n        view = self \n    else : \n        view = self . __class__ \n    if respect_name : \n        name = getattr ( view , \"name\" , None ) \n        if name is not None : \n            return name \n    name = view . __name__ \n    for suffix in ( \"ViewSet\" , \"View\" , \"API\" , \"Admin\" ) : \n        name = formatting . remove_trailing_string ( name , suffix ) \n    name = formatting . camelcase_to_spaces ( name ) \n    suffix = getattr ( view , \"suffix\" , None ) \n    if suffix : \n        name = name + ( \" \" + suffix ) \n    return name "}
{"8256": "\ndef retry ( n , errors , wait = 0.0 , logger_name = None ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( * args , ** kwargs ) : \n            retries = 0 \n            while True : \n                try : \n                    result = func ( * args , ** kwargs ) \n                    if retries and logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'Retry of `%s` successful' % func . __name__ ) \n                    return result \n                except errors : \n                    if retries >= n : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                        raise \n                    elif logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                    retries = retries + ( 1 ) \n                    if wait : \n                        time . sleep ( wait ) \n        return new_func \n    return wrapper "}
{"8269": "\ndef _build_model_eqs ( traj ) : \n    model_eqs = traj . model . eqs \n    post_eqs = { } \n    for name_post in [ 'i' , 'e' ] : \n        variables_dict = { } \n        new_model_eqs = model_eqs . replace ( 'POST' , name_post ) \n        for name_pre in [ 'i' , 'e' ] : \n            conn_eqs = traj . model . synaptic . eqs \n            new_conn_eqs = conn_eqs . replace ( 'PRE' , name_pre ) \n            new_model_eqs = new_model_eqs + ( new_conn_eqs ) \n            tau1 = traj . model . synaptic [ 'tau1' ] \n            tau2 = traj . model . synaptic [ 'tau2_' + name_pre ] \n            normalization = ( tau1 - tau2 ) / tau2 \n            invtau1 = 1.0 / tau1 \n            invtau2 = 1.0 / tau2 \n            variables_dict [ 'invtau1_' + name_pre ] = invtau1 \n            variables_dict [ 'invtau2_' + name_pre ] = invtau2 \n            variables_dict [ 'normalization_' + name_pre ] = normalization \n            variables_dict [ 'tau1_' + name_pre ] = tau1 \n            variables_dict [ 'tau2_' + name_pre ] = tau2 \n        variables_dict [ 'tau_' + name_post ] = traj . model [ 'tau_' + name_post ] \n        post_eqs [ name_post ] = Equations ( new_model_eqs , ** variables_dict ) \n    return post_eqs "}
{"8297": "\ndef _add_prefix ( self , split_names , start_node , group_type_name ) : \n    root = self . _root_instance \n    prepend = [ ] \n    if start_node . v_depth < 3 and not group_type_name == GROUP : \n        if start_node . v_depth == 0 : \n            if group_type_name == DERIVED_PARAMETER_GROUP : \n                if split_names [ 0 ] == 'derived_parameters' : \n                    return split_names \n                else : \n                    prepend = prepend + ( [ 'derived_parameters' ] ) \n            elif group_type_name == RESULT_GROUP : \n                if split_names [ 0 ] == 'results' : \n                    return split_names \n                else : \n                    prepend = prepend + ( [ 'results' ] ) \n            elif group_type_name == CONFIG_GROUP : \n                if split_names [ 0 ] == 'config' : \n                    return split_names \n                else : \n                    prepend = prepend + ( [ 'config' ] ) \n            elif group_type_name == PARAMETER_GROUP : \n                if split_names [ 0 ] == 'parameters' : \n                    return split_names [ 0 ] \n                else : \n                    prepend = prepend + ( [ 'parameters' ] ) \n            else : \n                raise RuntimeError ( 'Why are you here?' ) \n        if root . _is_run and root . _auto_run_prepend : \n            dummy = root . f_wildcard ( '$' , - 1 ) \n            crun = root . f_wildcard ( '$' ) \n            if any ( name in root . _run_information for name in split_names ) : \n                pass \n            elif any ( name == dummy for name in split_names ) : \n                pass \n            elif ( group_type_name == RESULT_GROUP or group_type_name == DERIVED_PARAMETER_GROUP ) : \n                if start_node . v_depth == 0 : \n                    prepend = prepend + ( [ 'runs' , crun ] ) \n                elif start_node . v_depth == 1 : \n                    if len ( split_names ) == 1 and split_names [ 0 ] == 'runs' : \n                        return split_names \n                    else : \n                        prepend = prepend + ( [ 'runs' , crun ] ) \n                elif start_node . v_depth == 2 and start_node . v_name == 'runs' : \n                    prepend = prepend + ( [ crun ] ) \n    if prepend : \n        split_names = prepend + split_names \n    return split_names "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - 1 ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if key != candidate . v_name or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if parent_full_name != '' : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if len ( candidate_split_name ) > max_depth : \n                break \n            if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            elif shortcuts : \n                candidate_set = set ( candidate_split_name ) \n                climbing = True \n                for name in split_name : \n                    if name not in candidate_set : \n                        climbing = False \n                        break \n                if climbing : \n                    count = 0 \n                    candidate_length = len ( candidate_split_name ) \n                    for idx in range ( candidate_length ) : \n                        if idx + split_length - count > candidate_length : \n                            break \n                        if split_name [ count ] == candidate_split_name [ idx ] : \n                            count = count + ( 1 ) \n                            if count == len ( split_name ) : \n                                result_list . append ( candidate ) \n                                full_name_set . add ( candidate . v_full_name ) \n                                break \n    return result_list "}
{"8343": "\ndef add_commit_variables ( traj , commit ) : \n    git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) \n    git_short_name = str ( commit . hexsha [ 0 : 7 ] ) \n    git_commit_name = 'commit_%s_' % git_short_name \n    git_commit_name = 'git.' + git_commit_name + git_time_value \n    if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = False ) : \n        git_commit_name = git_commit_name + ( '.' ) \n        traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) \n        traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) \n        traj . f_add_config ( git_commit_name + 'committed_date' , commit . committed_date , comment = 'Date of commit as unix epoch seconds' ) \n        traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , comment = 'The commit message' ) "}
{"8344": "\ndef make_git_commit ( environment , git_repository , user_message , git_fail ) : \n    repo = git . Repo ( git_repository ) \n    index = repo . index \n    traj = environment . v_trajectory \n    if traj . v_comment : \n        commentstr = ', Comment: `%s`' % traj . v_comment \n    else : \n        commentstr = '' \n    if user_message : \n        user_message = user_message + ( ' -- ' ) \n    message = '%sTrajectory: `%s`, Time: `%s`, %s' % ( user_message , traj . v_name , traj . v_time , commentstr ) \n    diff = index . diff ( None ) \n    if diff : \n        if git_fail : \n            raise pex . GitDiffError ( 'Found not committed changes!' ) \n        repo . git . add ( '-u' ) \n        commit = index . commit ( message ) \n        new_commit = True \n    else : \n        commit = repo . commit ( None ) \n        new_commit = False \n    add_commit_variables ( traj , commit ) \n    return new_commit , commit . hexsha "}
{"8357": "\ndef f_ann_to_str ( self ) : \n    resstr = '' \n    for key in sorted ( self . _dict . keys ( ) ) : \n        resstr = resstr + ( '%s=%s; ' % ( key , str ( self . _dict [ key ] ) ) ) \n    return resstr [ : - 2 ] "}
{"8370": "\ndef _req_rep_retry ( self , request ) : \n    retries_left = self . RETRIES \n    while retries_left : \n        self . _logger . log ( 1 , 'Sending REQ `%s`' , request ) \n        self . _send_request ( request ) \n        socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) \n        if socks . get ( self . _socket ) == zmq . POLLIN : \n            response = self . _receive_response ( ) \n            self . _logger . log ( 1 , 'Received REP `%s`' , response ) \n            return response , self . RETRIES - retries_left \n        else : \n            self . _logger . debug ( 'No response from server (%d retries left)' % retries_left ) \n            self . _close_socket ( confused = True ) \n            retries_left = retries_left - ( 1 ) \n            if retries_left == 0 : \n                raise RuntimeError ( 'Server seems to be offline!' ) \n            time . sleep ( self . SLEEP ) \n            self . _start_socket ( ) "}
{"8372": "\ndef listen ( self ) : \n    count = 0 \n    self . _start ( ) \n    while True : \n        result = self . _socket . recv_pyobj ( ) \n        if isinstance ( result , tuple ) : \n            request , data = result \n        else : \n            request = result \n            data = None \n        if request == self . SPACE : \n            if self . queue . qsize ( ) + count < self . queue_maxsize : \n                self . _socket . send_string ( self . SPACE_AVAILABLE ) \n                count = count + ( 1 ) \n            else : \n                self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) \n        elif request == self . PING : \n            self . _socket . send_string ( self . PONG ) \n        elif request == self . DATA : \n            self . _socket . send_string ( self . STORING ) \n            self . queue . put ( data ) \n            count = count - ( 1 ) \n        elif request == self . DONE : \n            self . _socket . send_string ( ZMQServer . CLOSED ) \n            self . queue . put ( ( 'DONE' , [ ] , { } ) ) \n            self . _close ( ) \n            break \n        else : \n            raise RuntimeError ( 'I did not understand your request %s' % request ) "}
{"8392": "\ndef signal_update ( self ) : \n    if not self . active : \n        return \n    self . _updates = self . _updates + ( 1 ) \n    current_time = time . time ( ) \n    dt = current_time - self . _last_time \n    if dt > self . _display_time : \n        dfullt = current_time - self . _start_time \n        seconds = int ( dfullt ) % 60 \n        minutes = int ( dfullt ) / 60 \n        if minutes == 0 : \n            formatted_time = '%ds' % seconds \n        else : \n            formatted_time = '%dm%02ds' % ( minutes , seconds ) \n        nodespersecond = self . _updates / dfullt \n        message = 'Processed %d nodes in %s (%.2f nodes/s).' % ( self . _updates , formatted_time , nodespersecond ) \n        self . _logger . info ( message ) \n        self . _last_time = current_time "}
{"8397": "\ndef _srvc_check_hdf_properties ( self , traj ) : \n    for attr_name in HDF5StorageService . ATTR_LIST : \n        try : \n            config = traj . f_get ( 'config.hdf5.' + attr_name ) . f_get ( ) \n            setattr ( self , attr_name , config ) \n        except AttributeError : \n            self . _logger . debug ( 'Could not find `%s` in traj config, ' 'using (default) value `%s`.' % ( attr_name , str ( getattr ( self , attr_name ) ) ) ) \n    for attr_name , table_name in HDF5StorageService . NAME_TABLE_MAPPING . items ( ) : \n        try : \n            if table_name in ( 'parameters' , 'config' ) : \n                table_name = table_name + ( '_overview' ) \n            config = traj . f_get ( 'config.hdf5.overview.' + table_name ) . f_get ( ) \n            setattr ( self , attr_name , config ) \n        except AttributeError : \n            self . _logger . debug ( 'Could not find `%s` in traj config, ' 'using (default) value `%s`.' % ( table_name , str ( getattr ( self , attr_name ) ) ) ) \n    for attr_name , name in HDF5StorageService . PR_ATTR_NAME_MAPPING . items ( ) : \n        try : \n            config = traj . f_get ( 'config.hdf5.' + name ) . f_get ( ) \n            setattr ( self , attr_name , config ) \n        except AttributeError : \n            self . _logger . debug ( 'Could not find `%s` in traj config, ' 'using (default) value `%s`.' % ( name , str ( getattr ( self , attr_name ) ) ) ) \n    if ( ( not self . _overview_results_summary or not self . _overview_derived_parameters_summary ) and self . _purge_duplicate_comments ) : \n        raise RuntimeError ( 'You chose to purge duplicate comments but disabled a summary ' 'table. You can only use the purging if you enable ' 'the summary tables.' ) \n    self . _filters = None "}
{"8405": "\ndef _tree_load_sub_branch ( self , traj_node , branch_name , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _trajectory = None , _as_new = False , _hdf5_group = None ) : \n    if load_data == pypetconstants . LOAD_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if _trajectory is None : \n        _trajectory = traj_node . v_root \n    if _hdf5_group is None : \n        hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) \n        if hdf5_group_name == '' : \n            _hdf5_group = self . _trajectory_group \n        else : \n            try : \n                _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_group_name ) \n            except pt . NoSuchNodeError : \n                self . _logger . error ( 'Cannot find `%s` the hdf5 node `%s` does not exist!' % ( traj_node . v_full_name , hdf5_group_name ) ) \n                raise \n    split_names = branch_name . split ( '.' ) \n    final_group_name = split_names . pop ( ) \n    current_depth = 1 \n    for name in split_names : \n        if current_depth > max_depth : \n            return \n        _hdf5_group = getattr ( _hdf5_group , name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) \n        current_depth = current_depth + ( 1 ) \n        traj_node = traj_node . _children [ name ] \n    if current_depth <= max_depth : \n        _hdf5_group = getattr ( _hdf5_group , final_group_name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) "}
{"8410": "\ndef _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : \n    for table_name in tables_to_make : \n        paramdescriptiondict = { } \n        expectedrows = 0 \n        paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = 0 ) \n        paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = 1 ) \n        paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) \n        paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) \n        if table_name == 'config_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _config ) \n        if table_name == 'parameters_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _parameters ) \n        if table_name == 'explored_parameters_overview' : \n            paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) \n            paramdescriptiondict [ 'length' ] = pt . IntCol ( ) \n            if traj is not None : \n                expectedrows = len ( traj . _explored_parameters ) \n        if table_name . endswith ( 'summary' ) : \n            paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) \n        if table_name == 'derived_parameters_overview' : \n            expectedrows = self . _derived_parameters_per_run \n            if traj is not None : \n                expectedrows = expectedrows * ( len ( traj ) ) \n                expectedrows = expectedrows + ( len ( traj . _derived_parameters ) ) \n        if table_name == 'results_overview' : \n            expectedrows = self . _results_per_run \n            if traj is not None : \n                expectedrows = expectedrows * ( len ( traj ) ) \n                expectedrows = expectedrows + ( len ( traj . _results ) ) \n        if expectedrows > 0 : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) \n        else : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) \n        paramtable . flush ( ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = 0 \n        maximum_display_other = 10 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if counter < maximum_display_other : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                elif counter == maximum_display_other : \n                    self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter = counter + ( 1 ) \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = True "}
{"8412": "\ndef _tree_store_sub_branch ( self , traj_node , branch_name , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , hdf5_group = None ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if hdf5_group is None : \n        location = traj_node . v_full_name \n        hdf5_location = location . replace ( '.' , '/' ) \n        try : \n            if location == '' : \n                hdf5_group = self . _trajectory_group \n            else : \n                hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_location ) \n        except pt . NoSuchNodeError : \n            self . _logger . debug ( 'Cannot store `%s` the parental hdf5 node with path `%s` does ' 'not exist on disk.' % ( traj_node . v_name , hdf5_location ) ) \n            if traj_node . v_is_leaf : \n                self . _logger . error ( 'Cannot store `%s` the parental hdf5 ' 'node with path `%s` does ' 'not exist on disk! The child ' 'you want to store is a leaf node,' 'that cannot be stored without ' 'the parental node existing on ' 'disk.' % ( traj_node . v_name , hdf5_location ) ) \n                raise \n            else : \n                self . _logger . debug ( 'I will try to store the path from trajectory root to ' 'the child now.' ) \n                self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , traj_node . v_full_name + '.' + branch_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth + traj_node . v_depth , hdf5_group = self . _trajectory_group ) \n                return \n    current_depth = 1 \n    split_names = branch_name . split ( '.' ) \n    leaf_name = split_names . pop ( ) \n    for name in split_names : \n        if current_depth > max_depth : \n            return \n        self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) \n        current_depth = current_depth + ( 1 ) \n        traj_node = traj_node . _children [ name ] \n        hdf5_group = getattr ( hdf5_group , name ) \n    if current_depth <= max_depth : \n        self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) "}
{"8445": "\ndef _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , ** kwargs ) : \n    datasize = data . shape [ 0 ] \n    try : \n        description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) \n        description_dicts = [ { } ] \n        if len ( description_dict ) > ptpa . MAX_COLUMNS : \n            new_table_group = self . _hdf5file . create_group ( where = hdf5_group , name = tablename , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            count = 0 \n            for innerkey in description_dict : \n                val = description_dict [ innerkey ] \n                if count == ptpa . MAX_COLUMNS : \n                    description_dicts . append ( { } ) \n                    count = 0 \n                description_dicts [ - 1 ] [ innerkey ] = val \n                count = count + ( 1 ) \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , 1 ) \n            hdf5_group = new_table_group \n        else : \n            description_dicts = [ description_dict ] \n        for idx , descr_dict in enumerate ( description_dicts ) : \n            if idx == 0 : \n                tblname = tablename \n            else : \n                tblname = tablename + '_%d' % idx \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = datasize , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            row = table . row \n            for n in range ( datasize ) : \n                for key in descr_dict : \n                    row [ key ] = data [ key ] [ n ] \n                row . append ( ) \n            if idx == 0 and len ( description_dict ) <= ptpa . MAX_COLUMNS : \n                for field_name in data_type_dict : \n                    type_description = data_type_dict [ field_name ] \n                    self . _all_set_attr ( table , field_name , type_description ) \n                setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n        if len ( description_dict ) > ptpa . MAX_COLUMNS : \n            tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE \n            field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) \n            data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } \n            descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = len ( field_names ) , filters = self . _all_get_filters ( kwargs ) ) \n            row = table . row \n            for n in range ( len ( field_names ) ) : \n                for key in data_type_table_dict : \n                    row [ key ] = data_type_table_dict [ key ] [ n ] \n                row . append ( ) \n            setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , 1 ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing table `%s` of `%s`.' % ( tablename , fullname ) ) \n        raise "}
{"8539": "\ndef _store ( self ) : \n    store_dict = { } \n    if self . _data is not None : \n        dump = pickle . dumps ( self . _data , protocol = self . v_protocol ) \n        store_dict [ 'data' ] = dump \n        store_dict [ PickleParameter . PROTOCOL ] = self . v_protocol \n    if self . f_has_range ( ) : \n        store_dict [ 'explored_data' ] = ObjectTable ( columns = [ 'idx' ] , index = list ( range ( len ( self ) ) ) ) \n        smart_dict = { } \n        count = 0 \n        for idx , val in enumerate ( self . _explored_range ) : \n            obj_id = id ( val ) \n            if obj_id in smart_dict : \n                name_id = smart_dict [ obj_id ] \n                add = False \n            else : \n                name_id = count \n                add = True \n            name = self . _build_name ( name_id ) \n            store_dict [ 'explored_data' ] [ 'idx' ] [ idx ] = name_id \n            if add : \n                store_dict [ name ] = pickle . dumps ( val , protocol = self . v_protocol ) \n                smart_dict [ obj_id ] = name_id \n                count = count + ( 1 ) \n    self . _locked = True \n    return store_dict "}
{"8542": "\ndef f_val_to_str ( self ) : \n    resstrlist = [ ] \n    strlen = 0 \n    for key in self . _data : \n        val = self . _data [ key ] \n        resstr = '%s=%s, ' % ( key , repr ( val ) ) \n        resstrlist . append ( resstr ) \n        strlen = strlen + ( len ( resstr ) ) \n        if strlen > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : \n            break \n    return_string = \"\" . join ( resstrlist ) \n    if len ( return_string ) > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : \n        return_string = return_string [ 0 : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3 ] + '...' \n    else : \n        return_string = return_string [ 0 : - 2 ] \n    return return_string "}
{"8567": "\ndef _execute_network_run ( self , traj , network , network_dict , component_list , analyser_list , pre_run = False ) : \n    subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) \n    subrun_number = 0 \n    while len ( subrun_list ) > 0 : \n        current_subrun = subrun_list . pop ( 0 ) \n        for component in component_list : \n            component . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . _logger . info ( 'STARTING subrun `%s` (#%d) lasting %s.' % ( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) \n        network . run ( duration = current_subrun . f_get ( ) , report = self . _report , report_period = self . _report_period ) \n        for analyser in analyser_list : \n            analyser . analyse ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for component in component_list : \n            component . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        subrun_number = subrun_number + ( 1 ) "}
{"8572": "\ndef make_filename ( traj ) : \n    explored_parameters = traj . f_get_explored_parameters ( ) \n    filename = '' \n    for param in explored_parameters . values ( ) : \n        short_name = param . v_name \n        val = param . f_get ( ) \n        filename = filename + ( '%s_%s__' % ( short_name , str ( val ) ) ) \n    return filename [ : - 2 ] + '.png' "}
{"8593": "\ndef gen_renewing_time ( lease_time , elapsed = 0 ) : \n    renewing_time = int ( lease_time ) * RENEW_PERC - elapsed \n    range_fuzz = int ( lease_time ) * REBIND_PERC - renewing_time \n    logger . debug ( 'rebinding fuzz range %s' , range_fuzz ) \n    fuzz = random . uniform ( - ( range_fuzz ) , + ( range_fuzz ) ) \n    renewing_time = renewing_time + ( fuzz ) \n    logger . debug ( 'Renewing time %s.' , renewing_time ) \n    return renewing_time "}
{"8598": "\ndef send_discover ( self ) : \n    assert self . client \n    assert self . current_state == STATE_INIT or self . current_state == STATE_SELECTING \n    pkt = self . client . gen_discover ( ) \n    sendp ( pkt ) \n    if self . discover_attempts < MAX_ATTEMPTS_DISCOVER : \n        self . discover_attempts = self . discover_attempts + ( 1 ) \n    timeout = gen_timeout_resend ( self . discover_attempts ) \n    self . set_timeout ( self . current_state , self . timeout_selecting , timeout ) "}
{"8600": "\ndef send_request ( self ) : \n    assert self . client \n    if self . current_state == STATE_BOUND : \n        pkt = self . client . gen_request_unicast ( ) \n    else : \n        pkt = self . client . gen_request ( ) \n    sendp ( pkt ) \n    logger . debug ( 'Modifying FSM obj, setting time_sent_request.' ) \n    self . time_sent_request = nowutc ( ) \n    logger . info ( 'DHCPREQUEST of %s on %s to %s port %s' , self . client . iface , self . client . client_ip , self . client . server_ip , self . client . server_port ) \n    if self . request_attempts < MAX_ATTEMPTS_REQUEST : \n        self . request_attempts = self . request_attempts * ( 2 ) \n        logger . debug ( 'Increased request attempts to %s' , self . request_attempts ) \n    if self . current_state == STATE_RENEWING : \n        timeout_renewing = gen_timeout_request_renew ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_renewing , timeout_renewing ) \n    elif self . current_state == STATE_REBINDING : \n        timeout_rebinding = gen_timeout_request_rebind ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_rebinding , timeout_rebinding ) \n    else : \n        timeout_requesting = gen_timeout_resend ( self . request_attempts ) \n        self . set_timeout ( self . current_state , self . timeout_requesting , timeout_requesting ) "}
{"8681": "\ndef process ( self ) : \n    self . rh = RelationHandler ( ) \n    self . rh . apply_file ( self . filename ) \n    logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) \n    node_ids , stop_node_ids , way_ids , reverse_map = self . __collect_ids ( ) \n    self . nh = NodeHandler ( node_ids ) \n    self . nh . apply_file ( self . filename , locations = True ) \n    count = 0 \n    for idx , missing_node_id in enumerate ( self . nh . missing_node_ids ) : \n        count = count + ( 1 ) \n        logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse_map [ missing_node_id ] , missing_node_id ) \n    if count : \n        logging . warning ( '%d nodes that appear in relations are missing.' , count ) \n    else : \n        logging . debug ( 'Lucky you! All relation member nodes were found.' ) \n    self . wh = WayHandler ( way_ids ) \n    self . wh . apply_file ( self . filename , locations = True ) "}
{"8690": "\ndef build_shape ( relation , nodes , ways ) : \n    sequence_index = 0 \n    for member_type , member_id , member_role in relation . member_info : \n        if member_id in nodes : \n            yield Shape ( relation . id , nodes [ member_id ] . lat , nodes [ member_id ] . lon , sequence_index ) \n            sequence_index = sequence_index + ( 1 ) \n        elif member_id in ways : \n            continue \n        else : \n            pass "}
{"8724": "\ndef run_command ( self , args : List [ str ] , max_num_processes : int = None , max_stack_size : int = None , max_virtual_memory : int = None , as_root : bool = False , stdin : FileIO = None , timeout : int = None , check : bool = False , truncate_stdout : int = None , truncate_stderr : int = None ) -> 'CompletedCommand' : \n    cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] \n    if stdin is None : \n        cmd . append ( '--stdin_devnull' ) \n    if max_num_processes is not None : \n        cmd = cmd + ( [ '--max_num_processes' , str ( max_num_processes ) ] ) \n    if max_stack_size is not None : \n        cmd = cmd + ( [ '--max_stack_size' , str ( max_stack_size ) ] ) \n    if max_virtual_memory is not None : \n        cmd = cmd + ( [ '--max_virtual_memory' , str ( max_virtual_memory ) ] ) \n    if timeout is not None : \n        cmd = cmd + ( [ '--timeout' , str ( timeout ) ] ) \n    if truncate_stdout is not None : \n        cmd = cmd + ( [ '--truncate_stdout' , str ( truncate_stdout ) ] ) \n    if truncate_stderr is not None : \n        cmd = cmd + ( [ '--truncate_stderr' , str ( truncate_stderr ) ] ) \n    if not as_root : \n        cmd = cmd + ( [ '--linux_user_id' , str ( self . _linux_uid ) ] ) \n    cmd = cmd + ( args ) \n    if self . debug : \n        print ( 'running: {}' . format ( cmd ) , flush = True ) \n    with tempfile . TemporaryFile ( ) as f : \n        try : \n            subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = True ) \n            f . seek ( 0 ) \n            json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            results_json = json . loads ( f . read ( json_len ) . decode ( ) ) \n            stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stdout = tempfile . NamedTemporaryFile ( ) \n            stdout . write ( f . read ( stdout_len ) ) \n            stdout . seek ( 0 ) \n            stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stderr = tempfile . NamedTemporaryFile ( ) \n            stderr . write ( f . read ( stderr_len ) ) \n            stderr . seek ( 0 ) \n            result = CompletedCommand ( return_code = results_json [ 'return_code' ] , timed_out = results_json [ 'timed_out' ] , stdout = stdout , stderr = stderr , stdout_truncated = results_json [ 'stdout_truncated' ] , stderr_truncated = results_json [ 'stderr_truncated' ] ) \n            if ( result . return_code != 0 or results_json [ 'timed_out' ] ) and check : \n                raise subprocess . CalledProcessError ( result . return_code , cmd , output = result . stdout , stderr = result . stderr ) \n            return result \n        except subprocess . CalledProcessError as e : \n            f . seek ( 0 ) \n            print ( f . read ( ) ) \n            print ( e . stderr ) \n            raise "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if len ( unique_labels ) != 2 : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] = self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] + ( 1 ) \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( class_labels == unique_labels [ 0 ] ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ 0 ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if fraction > overall_class_fraction : \n            self . feature_map [ feature_instance ] = unique_labels [ 0 ] \n        elif fraction == overall_class_fraction : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = unique_labels [ 1 ] \n    return self "}
{"8893": "\ndef partition ( coll , n : int ) : \n    assert n > 0 \n    start = 0 \n    stop = n \n    while stop <= len ( coll ) : \n        yield tuple ( e for e in coll [ start : stop ] ) \n        start = start + ( n ) \n        stop = stop + ( n ) \n    if start < len ( coll ) < stop : \n        stop = len ( coll ) \n        yield tuple ( e for e in coll [ start : stop ] ) "}
{"8921": "\ndef pushback ( self ) -> None : \n    if abs ( self . _idx - 1 ) > self . _pushback_depth : \n        raise IndexError ( \"Exceeded pushback depth\" ) \n    self . _idx = self . _idx - ( 1 ) "}
{"8922": "\ndef next_token ( self ) -> str : \n    if self . _idx < StreamReader . DEFAULT_INDEX : \n        self . _idx = self . _idx + ( 1 ) \n    else : \n        c = self . _stream . read ( 1 ) \n        self . _update_loc ( c ) \n        self . _buffer . append ( c ) \n    return self . peek ( ) "}
{"8984": "\ndef nthrest ( coll , i : int ) : \n    while True : \n        if coll is None : \n            return None \n        if i == 0 : \n            return coll \n        i = i - ( 1 ) \n        coll = rest ( coll ) "}
{"8985": "\ndef nthnext ( coll , i : int ) -> Optional [ ISeq ] : \n    while True : \n        if coll is None : \n            return None \n        if i == 0 : \n            return to_seq ( coll ) \n        i = i - ( 1 ) \n        coll = next_ ( coll ) "}
{"9045": "\ndef parse_str_to_expression ( fiql_str ) : \n    nesting_lvl = 0 \n    last_element = None \n    expression = Expression ( ) \n    for ( preamble , selector , comparison , argument ) in iter_parse ( fiql_str ) : \n        if preamble : \n            for char in preamble : \n                if char == '(' : \n                    if isinstance ( last_element , BaseExpression ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Expression ) ) \n                    expression = expression . create_nested_expression ( ) \n                    nesting_lvl = nesting_lvl + ( 1 ) \n                elif char == ')' : \n                    expression = expression . get_parent ( ) \n                    last_element = expression \n                    nesting_lvl = nesting_lvl - ( 1 ) \n                else : \n                    if not expression . has_constraint ( ) : \n                        raise FiqlFormatException ( \"%s proceeding initial %s\" % ( Operator , Constraint ) ) \n                    if isinstance ( last_element , Operator ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( Operator , Operator ) ) \n                    last_element = Operator ( char ) \n                    expression = expression . add_operator ( last_element ) \n        if selector : \n            if isinstance ( last_element , BaseExpression ) : \n                raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Constraint ) ) \n            last_element = Constraint ( selector , comparison , argument ) \n            expression . add_element ( last_element ) \n    if nesting_lvl != 0 : \n        raise FiqlFormatException ( \"At least one nested expression was not correctly closed\" ) \n    if not expression . has_constraint ( ) : \n        raise FiqlFormatException ( \"Parsed string '%s' contained no constraint\" % fiql_str ) \n    return expression "}
{"9098": "\ndef from_str ( cls , human_readable_str , decimal = False , bits = False ) : \n    divisor = 1000 if decimal else 1024 \n    num = [ ] \n    c = \"\" \n    for c in human_readable_str : \n        if c not in cls . digits : \n            break \n        num . append ( c ) \n    num = \"\" . join ( num ) \n    try : \n        num = int ( num ) \n    except ValueError : \n        num = float ( num ) \n    if bits : \n        num = num / ( 8 ) \n    return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) ) "}
{"9146": "\ndef _build_expr ( tokens , higher_oplevel = - 1 , ldelim = \"(\" , rdelim = \")\" ) : \n    if isinstance ( tokens , str ) : \n        return tokens \n    if len ( tokens ) == 2 : \n        return \"\" . join ( tokens ) \n    oplevel = _get_op_level ( tokens [ 1 ] ) \n    stoken = \"\" \n    for num , item in enumerate ( tokens ) : \n        if num % 2 == 0 : \n            stoken = stoken + ( _build_expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) ) \n        else : \n            stoken = stoken + ( item ) \n    if ( oplevel < higher_oplevel ) or ( ( oplevel == higher_oplevel ) and ( oplevel in _OP_PREC_PAR ) ) : \n        stoken = ldelim + stoken + rdelim \n    return stoken "}
{"9155": "\ndef peng ( number , frac_length , rjust = True ) : \n    if number == 0 : \n        number = \"0.{zrs}\" . format ( zrs = \"0\" * frac_length ) if frac_length else \"0\" \n        return \"{0} \" . format ( number . rjust ( 5 + frac_length ) ) if rjust else number \n    sign = + 1 if number >= 0 else - 1 \n    ssign = \"-\" if sign == - 1 else \"\" \n    anumber = abs ( number ) \n    if anumber < 1e-24 : \n        anumber = 1e-24 \n        number = sign * 1e-24 \n    exp = 3.0 * math . floor ( math . floor ( math . log10 ( anumber ) ) / 3.0 ) \n    mant = number / 10 ** exp \n    smant = str ( mant ) \n    ppos = smant . find ( \".\" ) \n    if len ( smant ) - ppos - 1 > frac_length : \n        mant = mant + ( sign * 5 * 10 ** ( - frac_length - 1 ) ) \n        if abs ( mant ) >= 1000 : \n            exp = exp + ( 3 ) \n            mant = mant / 1e3 \n        smant = str ( mant ) \n        ppos = smant . find ( \".\" ) \n    bfrac_length = bool ( frac_length ) \n    flength = ppos - ( not bfrac_length ) + frac_length + 1 \n    new_mant = smant [ : flength ] . ljust ( flength , \"0\" ) \n    if exp > 24 : \n        new_mant , exp = ( \"{sign}999.{frac}\" . format ( sign = ssign , frac = \"9\" * frac_length ) , 24 , ) \n    new_mant = new_mant . rjust ( rjust * ( 4 + bfrac_length + frac_length ) ) \n    num = \"{mant}{suffix}\" . format ( mant = new_mant , suffix = _POWER_TO_SUFFIX_DICT [ exp ] if exp else \" \" * bool ( rjust ) ) \n    return num "}
{"9161": "\ndef remove_extra_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    op_group = \"\" \n    for item1 in _OP_PREC : \n        if isinstance ( item1 , list ) : \n            for item2 in item1 : \n                op_group = op_group + ( item2 ) \n        else : \n            op_group = op_group + ( item1 ) \n    iobj = zip ( [ expr , ldelim , rdelim ] , [ \"expr\" , \"ldelim\" , \"rdelim\" ] ) \n    for item , desc in iobj : \n        if not isinstance ( item , str ) : \n            raise RuntimeError ( \"Argument `{0}` is not valid\" . format ( desc ) ) \n    if ( len ( ldelim ) != 1 ) or ( ( len ( ldelim ) == 1 ) and ( ldelim in op_group ) ) : \n        raise RuntimeError ( \"Argument `ldelim` is not valid\" ) \n    if ( len ( rdelim ) != 1 ) or ( ( len ( rdelim ) == 1 ) and ( rdelim in op_group ) ) : \n        raise RuntimeError ( \"Argument `rdelim` is not valid\" ) \n    if expr . count ( ldelim ) != expr . count ( rdelim ) : \n        raise RuntimeError ( \"Mismatched delimiters\" ) \n    if not expr : \n        return expr \n    vchars = ( \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \".0123456789\" r\"_()[]\\{\\}\" + rdelim + ldelim + op_group ) \n    if any ( [ item not in vchars for item in expr ] ) or ( \"__\" in expr ) : \n        raise RuntimeError ( \"Argument `expr` is not valid\" ) \n    expr = _remove_consecutive_delims ( expr , ldelim = ldelim , rdelim = rdelim ) \n    expr = expr . replace ( ldelim + rdelim , \"\" ) \n    return _remove_extra_delims ( expr , ldelim = ldelim , rdelim = rdelim ) "}
{"9163": "\ndef to_scientific_tuple ( number ) : \n    convert = not isinstance ( number , str ) \n    if ( convert and ( number == 0 ) ) or ( ( not convert ) and ( not number . strip ( \"0\" ) . strip ( \".\" ) ) ) : \n        return ( \"0\" , 0 ) \n    sign , digits , exp = Decimal ( str ( number ) if convert else number ) . as_tuple ( ) \n    mant = ( \"{sign}{itg}{frac}\" . format ( sign = \"-\" if sign else \"\" , itg = digits [ 0 ] , frac = ( \".{frac}\" . format ( frac = \"\" . join ( [ str ( num ) for num in digits [ 1 : ] ] ) ) if len ( digits ) > 1 else \"\" ) , ) . rstrip ( \"0\" ) . rstrip ( \".\" ) ) \n    exp = exp + ( len ( digits ) - 1 ) \n    return NumComp ( mant , exp ) "}
{"9164": "\ndef find_sourcemap_comment ( filepath , block_size = 100 ) : \n    MAX_TRACKBACK = 2 \n    block_number = - 1 \n    blocks = [ ] \n    sourcemap = None \n    try : \n        of = io . open ( filepath , 'br+' ) \n        of . seek ( 0 , os . SEEK_END ) \n        block_end_byte = of . tell ( ) \n        while block_end_byte > 0 and MAX_TRACKBACK > 0 : \n            if ( block_end_byte - block_size > 0 ) : \n                of . seek ( block_number * block_size , os . SEEK_END ) \n                blocks . append ( of . read ( block_size ) ) \n            else : \n                of . seek ( 0 , os . SEEK_SET ) \n                blocks = [ of . read ( block_end_byte ) ] \n            content = b'' . join ( reversed ( blocks ) ) \n            lines_found = content . count ( b'\\n' ) \n            MAX_TRACKBACK = MAX_TRACKBACK - ( lines_found ) \n            block_end_byte = block_end_byte - ( block_size ) \n            block_number = block_number - ( 1 ) \n            if SOURCEMAPPING_URL_COMMENT in content : \n                offset = 0 \n                lines = content . split ( b'\\n' ) \n                for i , line in enumerate ( lines ) : \n                    if line . startswith ( SOURCEMAPPING_URL_COMMENT ) : \n                        offset = len ( line ) \n                        sourcemap = line \n                        break \n                while i + 1 < len ( lines ) : \n                    offset = offset + ( 1 ) \n                    offset = offset + ( len ( lines [ i + 1 ] ) ) \n                    i = i + ( 1 ) \n                if sourcemap : \n                    offset = offset + ( 1 ) \n                    of . seek ( - offset , os . SEEK_END ) \n                    of . truncate ( ) \n                return force_text ( sourcemap ) \n    finally : \n        of . close ( ) \n    return sourcemap "}
{"9166": "\ndef bundle ( self ) : \n    outfile , rel_path = self . get_paths ( ) \n    options = self . opts \n    if self . system . _has_jspm_log ( ) : \n        self . command = self . command + ( ' --log {log}' ) \n        options . setdefault ( 'log' , 'err' ) \n    if options . get ( 'minify' ) : \n        self . command = self . command + ( ' --minify' ) \n    if options . get ( 'skip_source_maps' ) : \n        self . command = self . command + ( ' --skip-source-maps' ) \n    try : \n        cmd = self . command . format ( app = self . app , outfile = outfile , ** options ) \n        proc = subprocess . Popen ( cmd , shell = True , cwd = self . system . cwd , stdout = self . stdout , stdin = self . stdin , stderr = self . stderr ) \n        result , err = proc . communicate ( ) \n        if err and self . system . _has_jspm_log ( ) : \n            fmt = 'Could not bundle \\'%s\\': \\n%s' \n            logger . warn ( fmt , self . app , err ) \n            raise BundleError ( fmt % ( self . app , err ) ) \n        if result . strip ( ) : \n            logger . info ( result ) \n    except ( IOError , OSError ) as e : \n        if isinstance ( e , BundleError ) : \n            raise \n        raise BundleError ( 'Unable to apply %s (%r): %s' % ( self . __class__ . __name__ , cmd , e ) ) \n    else : \n        if not options . get ( 'sfx' ) : \n            sourcemap = find_sourcemap_comment ( outfile ) \n            with open ( outfile , 'a' ) as of : \n                of . write ( \"\\nSystem.import('{app}{ext}');\\n{sourcemap}\" . format ( app = self . app , ext = '.js' if self . needs_ext ( ) else '' , sourcemap = sourcemap if sourcemap else '' , ) ) \n    return rel_path "}
{"9169": "\ndef format_hexdump ( arg ) : \n    line = '' \n    for i in range ( 0 , len ( arg ) , 16 ) : \n        if i > 0 : \n            line = line + ( '\\n' ) \n        chunk = arg [ i : i + 16 ] \n        hex_chunk = hexlify ( chunk ) . decode ( 'utf-8' ) \n        hex_line = ' ' . join ( hex_chunk [ j : j + 2 ] for j in range ( 0 , len ( hex_chunk ) , 2 ) ) \n        if len ( hex_line ) < ( 3 * 16 ) - 1 : \n            hex_line = hex_line + ( ' ' * ( ( ( 3 * 16 ) - 1 ) - len ( hex_line ) ) ) \n        ascii_line = '' . join ( _convert_to_ascii ( x ) for x in chunk ) \n        offset_line = '%08x' % i \n        line = line + ( \"%s  %s  %s\" % ( offset_line , hex_line , ascii_line ) ) \n    return line "}
{"9175": "\ndef _builtin_help ( self , args ) : \n    if len ( args ) == 0 : \n        return self . list_dir ( self . contexts [ - 1 ] ) \n    if len ( args ) == 1 : \n        func = self . find_function ( self . contexts [ - 1 ] , args [ 0 ] ) \n        return annotate . get_help ( func ) \n    help_text = \"Too many arguments: \" + str ( args ) + \"\\n\" \n    help_text = help_text + ( \"Usage: help [function]\" ) \n    return help_text "}
{"9177": "\ndef list_dir ( self , context ) : \n    doc = inspect . getdoc ( context ) \n    listing = \"\" \n    listing = listing + ( \"\\n\" ) \n    listing = listing + ( annotate . context_name ( context ) + \"\\n\" ) \n    if doc is not None : \n        doc = inspect . cleandoc ( doc ) \n        listing = listing + ( doc + \"\\n\" ) \n    listing = listing + ( \"\\nDefined Functions:\\n\" ) \n    is_dict = False \n    if isinstance ( context , dict ) : \n        funs = context . keys ( ) \n        is_dict = True \n    else : \n        funs = utils . find_all ( context ) \n    for fun in sorted ( funs ) : \n        override_name = None \n        if is_dict : \n            override_name = fun \n        fun = self . find_function ( context , fun ) \n        if isinstance ( fun , dict ) : \n            if is_dict : \n                listing = listing + ( \" - \" + override_name + '\\n' ) \n            else : \n                listing = listing + ( \" - \" + fun . metadata . name + '\\n' ) \n        else : \n            listing = listing + ( \" - \" + fun . metadata . signature ( name = override_name ) + '\\n' ) \n        if annotate . short_description ( fun ) != \"\" : \n            listing = listing + ( \"   \" + annotate . short_description ( fun ) + '\\n' ) \n    listing = listing + ( \"\\nBuiltin Functions\\n\" ) \n    for bif in sorted ( self . builtins . keys ( ) ) : \n        listing = listing + ( ' - ' + bif + '\\n' ) \n    listing = listing + ( '\\n' ) \n    return listing "}
{"9210": "\ndef convert_positional_argument ( self , index , arg_value ) : \n    if self . _has_self : \n        if index == 0 : \n            return arg_value \n        index = index - ( 1 ) \n    arg_name = self . arg_names [ index ] \n    return self . convert_argument ( arg_name , arg_value ) "}
{"9213": "\ndef format ( self , exclude_class = False ) : \n    if exclude_class : \n        msg = self . msg \n    else : \n        msg = \"%s: %s\" % ( self . __class__ . __name__ , self . msg ) \n    if len ( self . params ) != 0 : \n        paramstring = \"\\n\" . join ( [ str ( key ) + \": \" + str ( val ) for key , val in self . params . items ( ) ] ) \n        msg = msg + ( \"\\nAdditional Information:\\n\" + paramstring ) \n    return msg "}
{"9219": "\ndef get_help ( func ) : \n    help_text = \"\" \n    if isinstance ( func , dict ) : \n        name = context_name ( func ) \n        help_text = \"\\n\" + name + \"\\n\\n\" \n        doc = inspect . getdoc ( func ) \n        if doc is not None : \n            doc = inspect . cleandoc ( doc ) \n            help_text = help_text + ( doc + '\\n' ) \n        return help_text \n    sig = func . metadata . signature ( ) \n    doc = inspect . getdoc ( func ) \n    if doc is not None : \n        doc = inspect . cleandoc ( doc ) \n    help_text = help_text + ( \"\\n\" + sig + \"\\n\\n\" ) \n    if doc is not None : \n        help_text = help_text + ( doc + '\\n' ) \n    if inspect . isclass ( func ) : \n        func = func . __init__ \n    if func . metadata . load_from_doc : \n        return help_text \n    help_text = help_text + ( \"\\nArguments:\\n\" ) \n    for key , info in func . metadata . annotated_params . items ( ) : \n        type_name = info . type_name \n        desc = \"\" \n        if info . desc is not None : \n            desc = info . desc \n        help_text = help_text + ( \"  - %s (%s): %s\\n\" % ( key , type_name , desc ) ) \n    return help_text "}
{"9265": "\ndef _upload_chunk ( self , spider ) : \n    if not self . items : \n        return \n    f = self . _make_fileobj ( ) \n    object_key = self . object_key_template . format ( ** self . _get_uri_params ( spider ) ) \n    try : \n        self . s3 . upload_fileobj ( f , self . bucket_name , object_key ) \n    except ClientError : \n        self . stats . inc_value ( 'pipeline/s3/fail' ) \n        raise \n    else : \n        self . stats . inc_value ( 'pipeline/s3/success' ) \n    finally : \n        self . chunk_number = self . chunk_number + ( len ( self . items ) ) \n        self . items = [ ] "}
{"9281": "\ndef _call ( self , method , params = None , request_id = None ) : \n    params = params or [ ] \n    rid = request_id or self . _id_counter \n    if request_id is None : \n        self . _id_counter = self . _id_counter + ( 1 ) \n    payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } \n    headers = { 'Content-Type' : 'application/json' } \n    scheme = 'https' if self . tls else 'http' \n    url = '{}://{}:{}' . format ( scheme , self . host , self . port ) \n    try : \n        response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) \n        response . raise_for_status ( ) \n    except HTTPError : \n        raise TransportError ( 'Got unsuccessful response from server (status code: {})' . format ( response . status_code ) , response = response ) \n    try : \n        response_data = response . json ( ) \n    except ValueError as e : \n        raise ProtocolError ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) \n    if response_data . get ( 'error' ) : \n        code = response_data [ 'error' ] . get ( 'code' , '' ) \n        message = response_data [ 'error' ] . get ( 'message' , '' ) \n        raise ProtocolError ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response_data ) \n    elif 'result' not in response_data : \n        raise ProtocolError ( 'Response is empty (result field is missing)' , response = response , data = response_data ) \n    return response_data [ 'result' ] "}
{"9292": "\ndef render ( self , code ) : \n    if self . _callbacks [ 'initialize' ] is not None : \n        self . _callbacks [ 'initialize' ] ( code ) \n    ypos = 1.0 \n    for line in code : \n        xpos = self . quiet_zone \n        for mod in line : \n            if mod == '0' : \n                color = self . background \n            else : \n                color = self . foreground \n            self . _callbacks [ 'paint_module' ] ( xpos , ypos , self . module_width , color ) \n            xpos = xpos + ( self . module_width ) \n        self . _callbacks [ 'paint_module' ] ( xpos , ypos , self . quiet_zone , self . background ) \n        ypos = ypos + ( self . module_height ) \n    if self . text and self . _callbacks [ 'paint_text' ] is not None : \n        ypos = ypos + ( self . text_distance ) \n        if self . center_text : \n            xpos = xpos / 2.0 \n        else : \n            xpos = self . quiet_zone + 4.0 \n        self . _callbacks [ 'paint_text' ] ( xpos , ypos ) \n    return self . _callbacks [ 'finish' ] ( ) "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max_cols = 80 ) : \n    try : \n        name = getattr ( obj , \"name\" , \"\" ) \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + 1 + indent * 4 + ( 5 if context else 0 ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if k == \"index\" : \n                continue \n            if k == \"parent\" and parent != \"\" : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if len ( line + param ) + 3 > max_cols - padding : \n                param_lines . append ( line ) \n                line = \"\" \n            line = line + ( param + \", \" ) \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9357": "\ndef _updateColAttrs ( self , grid ) : \n    col = 0 \n    for column in self . columns : \n        attr = gridlib . GridCellAttr ( ) \n        if False : \n            attr . SetReadOnly ( ) \n        if False : \n            attr . SetRenderer ( renderer ) \n        grid . SetColSize ( col , column . width ) \n        grid . SetColAttr ( col , attr ) \n        col = col + ( 1 ) "}
{"9375": "\ndef mangle_signature ( sig , max_chars = 30 ) : \n    s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) \n    s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) \n    s = re . sub ( r\"\\\\'\" , \"\" , s ) \n    s = re . sub ( r\"'[^']*'\" , \"\" , s ) \n    args = [ ] \n    opts = [ ] \n    opt_re = re . compile ( r\"^(.*, |)([a-zA-Z0-9_*]+)=\" ) \n    while s : \n        m = opt_re . search ( s ) \n        if not m : \n            args = s . split ( ', ' ) \n            break \n        opts . insert ( 0 , m . group ( 2 ) ) \n        s = m . group ( 1 ) [ : - 2 ] \n    sig = limited_join ( \", \" , args , max_chars = max_chars - 2 ) \n    if opts : \n        if not sig : \n            sig = \"[%s]\" % limited_join ( \", \" , opts , max_chars = max_chars - 4 ) \n        elif len ( sig ) < max_chars - 4 - 2 - 3 : \n            sig = sig + ( \"[, %s]\" % limited_join ( \", \" , opts , max_chars = max_chars - len ( sig ) - 4 - 2 ) ) \n    return u\"(%s)\" % sig "}
{"9443": "\ndef failure_message ( description , options ) : \n    message = \"expected to find {}\" . format ( description ) \n    if options [ \"count\" ] is not None : \n        message = message + ( \" {count} {times}\" . format ( count = options [ \"count\" ] , times = declension ( \"time\" , \"times\" , options [ \"count\" ] ) ) ) \n    elif options [ \"between\" ] is not None : \n        between = options [ \"between\" ] \n        if between : \n            first , last = between [ 0 ] , between [ - 1 ] \n        else : \n            first , last = None , None \n        message = message + ( \" between {first} and {last} times\" . format ( first = first , last = last ) ) \n    elif options [ \"maximum\" ] is not None : \n        message = message + ( \" at most {maximum} {times}\" . format ( maximum = options [ \"maximum\" ] , times = declension ( \"time\" , \"times\" , options [ \"maximum\" ] ) ) ) \n    elif options [ \"minimum\" ] is not None : \n        message = message + ( \" at least {minimum} {times}\" . format ( minimum = options [ \"minimum\" ] , times = declension ( \"time\" , \"times\" , options [ \"minimum\" ] ) ) ) \n    return message "}
{"9448": "\ndef resolves_for ( self , session ) : \n    if self . url : \n        self . actual_path = session . current_url \n    else : \n        result = urlparse ( session . current_url ) \n        if self . only_path : \n            self . actual_path = result . path \n        else : \n            request_uri = result . path \n            if result . query : \n                request_uri = request_uri + ( \"?{0}\" . format ( result . query ) ) \n            self . actual_path = request_uri \n    if isregex ( self . expected_path ) : \n        return self . expected_path . search ( self . actual_path ) \n    else : \n        return normalize_url ( self . actual_path ) == normalize_url ( self . expected_path ) "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if self . _display . verbosity > 1 : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if len ( reason ) < 50 : \n            line = line + ( \" -- {}\" . format ( reason ) ) \n            print ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            print ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            print ( self . _indent_text ( reason , 8 ) ) \n            print ( reason ) "}
{"9523": "\ndef bar ( iter_content , parts , title = '' ) : \n    parts = max ( float ( parts ) , 1.0 ) \n    cells = 10 \n    progress = 0 \n    step = cells / parts \n    draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) \n    for chunk in iter_content : \n        yield chunk \n        progress = progress + ( step ) \n        draw ( progress ) \n        sys . stdout . flush ( ) \n    draw ( cells ) \n    print ( '' ) "}
{"9538": "\ndef get_td_at_index ( tr , index ) : \n    current = 0 \n    for td in tr . xpath ( './/w:tc' , namespaces = tr . nsmap ) : \n        if index == current : \n            return td \n        current = current + ( get_grid_span ( td ) ) "}
{"9556": "\ndef plot_images ( imgs , loc , title = None , channels = 1 ) : \n    n = int ( np . sqrt ( len ( imgs ) ) ) \n    assert n * n == len ( imgs ) , 'images array must contain a square number of rows!' \n    s = int ( np . sqrt ( len ( imgs [ 0 ] ) / channels ) ) \n    assert s * s == len ( imgs [ 0 ] ) / channels , 'images must be square!' \n    img = np . zeros ( ( ( s + 1 ) * n - 1 , ( s + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( s + 1 ) : ( r + 1 ) * ( s + 1 ) - 1 , c * ( s + 1 ) : ( c + 1 ) * ( s + 1 ) - 1 ] = pix . reshape ( ( s , s , channels ) ) \n    img = img - ( img . min ( ) ) \n    img = img / ( img . max ( ) ) \n    ax = plt . gcf ( ) . add_subplot ( loc ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) \n    if title : \n        ax . set_title ( title ) "}
{"9558": "\ndef plot_filters ( filters ) : \n    imgs = filters . get_value ( ) \n    N , channels , x , y = imgs . shape \n    n = int ( np . sqrt ( N ) ) \n    assert n * n == N , 'filters must contain a square number of rows!' \n    assert channels == 1 or channels == 3 , 'can only plot grayscale or rgb filters!' \n    img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) \n    img = img - ( img . min ( ) ) \n    img = img / ( img . max ( ) ) \n    ax = plt . gcf ( ) . add_subplot ( 111 ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) "}
{"9575": "\ndef random_matrix ( rows , cols , mean = 0 , std = 1 , sparsity = 0 , radius = 0 , diagonal = 0 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    arr = mean + std * rng . randn ( rows , cols ) \n    if 1 > sparsity > 0 : \n        k = min ( rows , cols ) \n        mask = rng . binomial ( n = 1 , p = 1 - sparsity , size = ( rows , cols ) ) . astype ( bool ) \n        mask [ : k , : k ] |= np . eye ( k ) . astype ( bool ) \n        arr = arr * ( mask ) \n    if radius > 0 : \n        u , s , vT = np . linalg . svd ( arr , full_matrices = False ) \n        arr = np . dot ( np . dot ( u , np . diag ( radius * s / abs ( s [ 0 ] ) ) ) , vT ) \n    if diagonal != 0 : \n        arr = diagonal * np . eye ( max ( rows , cols ) ) [ : rows , : cols ] \n    return arr . astype ( FLOAT ) "}
{"9606": "\ndef log_params ( self ) : \n    total = 0 \n    for p in self . params : \n        shape = p . get_value ( ) . shape \n        util . log ( 'parameter \"{}\" {}' , p . name , shape ) \n        total = total + ( np . prod ( shape ) ) \n    return total "}
{"9612": "\ndef loggabor ( self , x_pos , y_pos , sf_0 , B_sf , theta , B_theta , preprocess = True ) : \n    env = np . multiply ( self . band ( sf_0 , B_sf ) , self . orientation ( theta , B_theta ) ) \n    if not ( x_pos == 0. ) and not ( y_pos == 0. ) : \n        env = env . astype ( np . complex128 ) * self . trans ( x_pos * 1. , y_pos * 1. ) \n    if preprocess : \n        env = env * ( self . f_mask ) \n    env = env / ( np . sqrt ( ( np . abs ( env ) ** 2 ) . mean ( ) ) ) \n    env = env * ( np . sqrt ( 2. ) ) \n    return env "}
{"9638": "\ndef generate_annotation_id ( self ) : \n    if not self . maxaid : \n        valid_anns = [ int ( '' . join ( filter ( str . isdigit , a ) ) ) for a in self . timeslots ] \n        self . maxaid = max ( valid_anns + [ 1 ] ) + 1 \n    else : \n        self . maxaid = self . maxaid + ( 1 ) \n    return 'a{:d}' . format ( self . maxaid ) "}
{"9639": "\ndef generate_ts_id ( self , time = None ) : \n    if time and time < 0 : \n        raise ValueError ( 'Time is negative...' ) \n    if not self . maxts : \n        valid_ts = [ int ( '' . join ( filter ( str . isdigit , a ) ) ) for a in self . timeslots ] \n        self . maxts = max ( valid_ts + [ 1 ] ) + 1 \n    else : \n        self . maxts = self . maxts + ( 1 ) \n    ts = 'ts{:d}' . format ( self . maxts ) \n    self . timeslots [ ts ] = time \n    return ts "}
{"9651": "\ndef remove_ref_annotation ( self , id_tier , time ) : \n    removed = 0 \n    bucket = [ ] \n    for aid , ( ref , value , _ , _ ) in self . tiers [ id_tier ] [ 1 ] . items ( ) : \n        begin , end , rvalue , _ = self . tiers [ self . annotations [ ref ] ] [ 0 ] [ ref ] \n        begin = self . timeslots [ begin ] \n        end = self . timeslots [ end ] \n        if begin <= time and end >= time : \n            removed = removed + ( 1 ) \n            bucket . append ( aid ) \n    for aid in bucket : \n        del ( self . tiers [ id_tier ] [ 1 ] [ aid ] ) \n    return removed "}
{"9656": "\ndef shift_annotations ( self , time ) : \n    total_re = [ ] \n    total_sq = [ ] \n    for name , tier in self . tiers . items ( ) : \n        squashed = [ ] \n        for aid , ( begin , end , value , _ ) in tier [ 0 ] . items ( ) : \n            if self . timeslots [ end ] + time <= 0 : \n                squashed . append ( ( name , aid ) ) \n            elif self . timeslots [ begin ] + time < 0 : \n                total_sq . append ( ( name , self . timeslots [ begin ] , self . timeslots [ end ] , value ) ) \n                self . timeslots [ begin ] = 0 \n            else : \n                self . timeslots [ begin ] = self . timeslots [ begin ] + ( time ) \n                self . timeslots [ end ] = self . timeslots [ end ] + ( time ) \n        for name , aid in squashed : \n            start , end , value , _ = self . tiers [ name ] [ 0 ] [ aid ] \n            del ( self . tiers [ name ] [ 0 ] [ aid ] ) \n            del ( self . annotations [ aid ] ) \n            total_re . append ( ( name , self . timeslots [ start ] , self . timeslots [ end ] , value ) ) \n    return total_sq , total_re "}
{"9658": "\ndef debug_storage ( storage , base_info = False , chars = True , runs = False ) : \n    import codecs \n    import locale \n    import sys \n    if six . PY2 : \n        stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) \n    else : \n        stderr = sys . stderr \n    caller = inspect . stack ( ) [ 1 ] [ 3 ] \n    stderr . write ( 'in %s\\n' % caller ) \n    if base_info : \n        stderr . write ( u'  base level  : %d\\n' % storage [ 'base_level' ] ) \n        stderr . write ( u'  base dir    : %s\\n' % storage [ 'base_dir' ] ) \n    if runs : \n        stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) \n    if chars : \n        output = u'  Chars       : ' \n        for _ch in storage [ 'chars' ] : \n            if _ch != '\\n' : \n                output = output + ( _ch [ 'ch' ] ) \n            else : \n                output = output + ( 'C' ) \n        stderr . write ( output + u'\\n' ) \n        output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) \n        stderr . write ( output ) \n        _types = [ _ch [ 'type' ] . ljust ( 3 ) for _ch in storage [ 'chars' ] ] \n        for i in range ( 3 ) : \n            if i : \n                output = u'                %s\\n' \n            else : \n                output = u'  Res. types  : %s\\n' \n            stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) ) "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != 0 : \n                overflow_counter = overflow_counter + ( 1 ) \n                continue \n            new_level = level_func ( embedding_level ) \n            if new_level < EXPLICIT_LEVEL_LIMIT : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif embedding_level == EXPLICIT_LEVEL_LIMIT - 2 : \n                almost_overflow_counter = almost_overflow_counter + ( 1 ) \n            else : \n                overflow_counter = overflow_counter + ( 1 ) \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if directional_override != 'N' : \n                    _ch [ 'type' ] = directional_override \n            elif bidi_type == 'PDF' : \n                if overflow_counter : \n                    overflow_counter = overflow_counter - ( 1 ) \n                elif almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - 1 : \n                    almost_overflow_counter = almost_overflow_counter - ( 1 ) \n                elif levels : \n                    embedding_level , directional_override = levels . pop ( ) \n            elif bidi_type == 'B' : \n                levels . clear ( ) \n                overflow_counter = almost_overflow_counter = 0 \n                embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9662": "\ndef calc_level_runs ( storage ) : \n    storage [ 'runs' ] . clear ( ) \n    chars = storage [ 'chars' ] \n    if not chars : \n        return \n    def calc_level_run ( b_l , b_r ) : \n        return [ 'L' , 'R' ] [ max ( b_l , b_r ) % 2 ] \n    first_char = chars [ 0 ] \n    sor = calc_level_run ( storage [ 'base_level' ] , first_char [ 'level' ] ) \n    eor = None \n    run_start = run_length = 0 \n    prev_level , prev_type = first_char [ 'level' ] , first_char [ 'type' ] \n    for _ch in chars : \n        curr_level , curr_type = _ch [ 'level' ] , _ch [ 'type' ] \n        if curr_level == prev_level : \n            run_length = run_length + ( 1 ) \n        else : \n            eor = calc_level_run ( prev_level , curr_level ) \n            storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : prev_type , 'length' : run_length } ) \n            sor = eor \n            run_start = run_start + ( run_length ) \n            run_length = 1 \n        prev_level , prev_type = curr_level , curr_type \n    eor = calc_level_run ( curr_level , storage [ 'base_level' ] ) \n    storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : curr_type , 'length' : run_length } ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if char_level > highest_level : \n            highest_level = char_level \n        if char_level % 2 and char_level < lowest_odd_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end = line_end - ( 1 ) \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9675": "\ndef add_to_filemenu ( ) : \n    if hasattr ( cmds , 'about' ) and not cmds . about ( batch = True ) : \n        mel . eval ( \"evalDeferred buildFileMenu\" ) \n        script = inspect . getsource ( _add_to_filemenu ) \n        script = script + ( \"\\n_add_to_filemenu()\" ) \n        cmds . evalDeferred ( script ) "}
{"9678": "\ndef _show_no_gui ( ) : \n    messagebox = QtWidgets . QMessageBox ( ) \n    messagebox . setIcon ( messagebox . Warning ) \n    messagebox . setWindowIcon ( QtGui . QIcon ( os . path . join ( os . path . dirname ( pyblish . __file__ ) , \"icons\" , \"logo-32x32.svg\" ) ) ) \n    spacer = QtWidgets . QWidget ( ) \n    spacer . setMinimumSize ( 400 , 0 ) \n    spacer . setSizePolicy ( QtWidgets . QSizePolicy . Minimum , QtWidgets . QSizePolicy . Expanding ) \n    layout = messagebox . layout ( ) \n    layout . addWidget ( spacer , layout . rowCount ( ) , 0 , 1 , layout . columnCount ( ) ) \n    messagebox . setWindowTitle ( \"Uh oh\" ) \n    text = \"No registered GUI found.\\n\\n\" \n    if not pyblish . api . registered_guis ( ) : \n        text = text + ( ( \"In order to show you a GUI, one must first be registered. \" \"\\n\" \"Pyblish supports one or more graphical user interfaces \" \"to be registered at once, the next acting as a fallback to \" \"the previous.\" \"\\n\" \"\\n\" \"For example, to use Pyblish Lite, first install it:\" \"\\n\" \"\\n\" \"$ pip install pyblish-lite\" \"\\n\" \"\\n\" \"Then register it, like so:\" \"\\n\" \"\\n\" \">>> import pyblish.api\\n\" \">>> pyblish.api.register_gui(\\\"pyblish_lite\\\")\" \"\\n\" \"\\n\" \"The next time you try running this, Lite will appear.\" \"\\n\" \"See http://api.pyblish.com/register_gui.html for \" \"more information.\" ) ) \n    else : \n        text = text + ( ( \"None of the registered graphical user interfaces \" \"could be found.\" \"\\n\" \"These interfaces are currently registered:\" \"\\n\" \"%s\" % \"\\n\" . join ( pyblish . api . registered_guis ( ) ) ) ) \n    messagebox . setText ( text ) \n    messagebox . setStandardButtons ( messagebox . Ok ) \n    messagebox . exec_ ( ) "}
{"9685": "\ndef float_range ( start = 0 , stop = None , step = 1 ) : \n    start = float ( start ) \n    while start < stop : \n        yield start \n        start = start + ( step ) "}
{"9690": "\ndef calculate_left_margin ( self ) : \n    bl = 7 \n    if self . rotate_y_labels : \n        max_y_label_height_px = self . y_label_font_size \n    else : \n        label_lengths = map ( len , self . get_y_labels ( ) ) \n        max_y_label_len = max ( label_lengths ) \n        max_y_label_height_px = 0.6 * max_y_label_len * self . y_label_font_size \n    if self . show_y_labels : \n        bl = bl + ( max_y_label_height_px ) \n    if self . stagger_y_labels : \n        bl = bl + ( max_y_label_height_px + 10 ) \n    if self . show_y_title : \n        bl = bl + ( self . y_title_font_size + 5 ) \n    self . border_left = bl "}
{"9691": "\ndef calculate_right_margin ( self ) : \n    br = 7 \n    if self . key and self . key_position == 'right' : \n        max_key_len = max ( map ( len , self . keys ( ) ) ) \n        br = br + ( max_key_len * self . key_font_size * 0.6 ) \n        br = br + ( self . KEY_BOX_SIZE ) \n        br = br + ( 10 ) \n    self . border_right = br "}
{"9692": "\ndef calculate_top_margin ( self ) : \n    self . border_top = 5 \n    if self . show_graph_title : \n        self . border_top = self . border_top + ( self . title_font_size ) \n    self . border_top = self . border_top + ( 5 ) \n    if self . show_graph_subtitle : \n        self . border_top = self . border_top + ( self . subtitle_font_size ) "}
{"9694": "\ndef calculate_bottom_margin ( self ) : \n    bb = 7 \n    if self . key and self . key_position == 'bottom' : \n        bb = bb + ( len ( self . data ) * ( self . font_size + 5 ) ) \n        bb = bb + ( 10 ) \n    if self . show_x_labels : \n        max_x_label_height_px = self . x_label_font_size \n        if self . rotate_x_labels : \n            label_lengths = map ( len , self . get_x_labels ( ) ) \n            max_x_label_len = functools . reduce ( max , label_lengths ) \n            max_x_label_height_px = max_x_label_height_px * ( 0.6 * max_x_label_len ) \n        bb = bb + ( max_x_label_height_px ) \n        if self . stagger_x_labels : \n            bb = bb + ( max_x_label_height_px + 10 ) \n    if self . show_x_title : \n        bb = bb + ( self . x_title_font_size + 5 ) \n    self . border_bottom = bb "}
{"9702": "\ndef render_inline_styles ( self ) : \n    if not self . css_inline : \n        return \n    styles = self . parse_css ( ) \n    for node in self . root . xpath ( '//*[@class]' ) : \n        cl = '.' + node . attrib [ 'class' ] \n        if cl not in styles : \n            continue \n        style = styles [ cl ] \n        if 'style' in node . attrib : \n            style = style + ( node . attrib [ 'style' ] ) \n        node . attrib [ 'style' ] = style "}
{"9762": "\ndef bytes_to_readable ( num ) : \n    if num < 512 : \n        return \"0 Kb\" \n    elif num < 1024 : \n        return \"1 Kb\" \n    for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : \n        if abs ( num ) < 1024.0 : \n            return \"%3.1f%s\" % ( num , unit ) \n        num = num / ( 1024.0 ) \n    return \"%.1f%s\" % ( num , 'Yb' ) "}
{"9770": "\ndef volume_disk_temp_avg ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            total_temp = 0 \n            total_disks = 0 \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None : \n                    total_disks = total_disks + ( 1 ) \n                    total_temp = total_temp + ( disk_temp ) \n            if total_temp > 0 and total_disks > 0 : \n                return round ( total_temp / total_disks , 0 ) "}
{"9807": "\ndef jsonify_good_event ( event , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = True ) : \n    if len ( event ) != len ( known_fields ) : \n        raise SnowplowEventTransformationException ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known_fields ) , len ( event ) ) ] ) \n    else : \n        output = { } \n        errors = [ ] \n        if add_geolocation_data and event [ LATITUDE_INDEX ] != '' and event [ LONGITUDE_INDEX ] != '' : \n            output [ 'geo_location' ] = event [ LATITUDE_INDEX ] + ',' + event [ LONGITUDE_INDEX ] \n        for i in range ( len ( event ) ) : \n            key = known_fields [ i ] [ 0 ] \n            if event [ i ] != '' : \n                try : \n                    kvpairs = known_fields [ i ] [ 1 ] ( key , event [ i ] ) \n                    for kvpair in kvpairs : \n                        output [ kvpair [ 0 ] ] = kvpair [ 1 ] \n                except SnowplowEventTransformationException as sete : \n                    errors = errors + ( sete . error_messages ) \n                except Exception as e : \n                    errors = errors + ( [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known_fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] ) \n        if errors : \n            raise SnowplowEventTransformationException ( errors ) \n        else : \n            return output "}
{"9810": "\ndef print_variables ( self , context ) : \n    text = [ ] \n    for name , expr in self . variables : \n        data = '' \n        try : \n            if isinstance ( expr . var , Variable ) : \n                data = expr . var . resolve ( context ) \n            else : \n                data = expr . resolve ( context ) \n        except VariableDoesNotExist as e : \n            keys = [ ] \n            for scope in context : \n                keys = keys + ( scope . keys ( ) ) \n            keys = sorted ( set ( keys ) ) \n            return ERROR_TYPE_BLOCK . format ( style = PRE_ALERT_STYLE , error = escape ( u\"Variable '{0}' not found!  Available context variables are:\\n\\n{1}\" . format ( expr , u', ' . join ( keys ) ) ) ) \n        else : \n            textdata = linebreaksbr ( pformat_django_context_html ( data ) ) \n        if isinstance ( data , SHORT_NAME_TYPES ) : \n            text . append ( BASIC_TYPE_BLOCK . format ( style = PRE_STYLE , name = name , value = textdata ) ) \n        else : \n            text . append ( OBJECT_TYPE_BLOCK . format ( style = PRE_STYLE , name = name , type = data . __class__ . __name__ , value = textdata ) ) \n    return u'' . join ( text ) "}
{"9812": "\ndef pformat_django_context_html ( object ) : \n    if isinstance ( object , QuerySet ) : \n        text = '' \n        lineno = 0 \n        for item in object . all ( ) [ : 21 ] : \n            lineno = lineno + ( 1 ) \n            if lineno >= 21 : \n                text = text + ( u'   (remaining items truncated...)' ) \n                break \n            text = text + ( u'   {0}\\n' . format ( escape ( repr ( item ) ) ) ) \n        return text \n    elif isinstance ( object , Manager ) : \n        return mark_safe ( u'    (use <kbd>.all</kbd> to read it)' ) \n    elif isinstance ( object , six . string_types ) : \n        return escape ( repr ( object ) ) \n    elif isinstance ( object , Promise ) : \n        return escape ( _format_lazy ( object ) ) \n    elif isinstance ( object , dict ) : \n        return _format_dict ( object ) \n    elif isinstance ( object , list ) : \n        return _format_list ( object ) \n    elif hasattr ( object , '__dict__' ) : \n        return _format_object ( object ) \n    else : \n        text = DebugPrettyPrinter ( width = 200 ) . pformat ( object ) \n        return _style_text ( text ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and ord ( ch ) < 127 ) : \n            result = result + ( ch ) \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result = result + ( ( '{' + lch + '}' if brackets and lch [ 0 : 1 ] == '\\\\' else lch ) ) \n            elif ( ( ord ( ch ) >= 32 and ord ( ch ) <= 127 ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result = result + ( ch ) \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result = result + ( r'{\\bfseries ?}' ) \n                else : \n                    result = result + ( ch ) \n    return result "}
{"9824": "\ndef _unascii ( s ) : \n    m = _U_ESCAPE . search ( s ) \n    if not m : \n        return s if PY2 else s . encode ( 'utf-8' ) \n    chunks = [ ] \n    pos = 0 \n    while m : \n        start = m . start ( ) \n        end = m . end ( ) \n        g = m . group ( 1 ) \n        if g is None : \n            chunks . append ( s [ pos : end ] ) \n        else : \n            c = int ( g , 16 ) \n            if c < 0x20 : \n                chunks . append ( s [ pos : end ] ) \n            else : \n                if PY3 : \n                    if c & 0xfc00 == 0xd800 and s [ end : end + 2 ] == '\\\\u' : \n                        esc2 = s [ end + 2 : end + 6 ] \n                        c2 = int ( esc2 , 16 ) \n                        if c2 & 0xfc00 == 0xdc00 : \n                            c = 0x10000 + ( ( ( c - 0xd800 ) << 10 ) | ( c2 - 0xdc00 ) ) \n                            end = end + ( 6 ) \n                chunks . append ( s [ pos : start ] ) \n                chunks . append ( unichr ( c ) ) \n        pos = end \n        m = _U_ESCAPE . search ( s , pos ) \n    chunks . append ( s [ pos : ] ) \n    return ( '' . join ( chunks ) ) . encode ( \"utf-8\" ) "}
{"9873": "\ndef build_uri ( self , path , query_params ) : \n    url = 'https://api.trello.com/1' + self . clean_path ( path ) \n    url = url + ( '?' + urlencode ( query_params ) ) \n    return url "}
{"9908": "\ndef encodeLength ( value ) : \n    encoded = bytearray ( ) \n    while True : \n        digit = value % 128 \n        value = value // ( 128 ) \n        if value > 0 : \n            digit |= 128 \n        encoded . append ( digit ) \n        if value <= 0 : \n            break \n    return encoded "}
{"9909": "\ndef decodeLength ( encoded ) : \n    value = 0 \n    multiplier = 1 \n    for i in encoded : \n        value = value + ( ( i & 0x7F ) * multiplier ) \n        multiplier = multiplier * ( 0x80 ) \n        if ( i & 0x80 ) != 0x80 : \n            break \n    return value "}
{"9912": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen = lenLen + ( 1 ) \n    packet_remaining = packet [ lenLen + 1 : ] \n    version_str , packet_remaining = decodeString ( packet_remaining ) \n    version_id = int ( packet_remaining [ 0 ] ) \n    if version_id == v31 [ 'level' ] : \n        self . version = v31 \n    else : \n        self . version = v311 \n    flags = packet_remaining [ 1 ] \n    self . cleanStart = ( flags & 0x02 ) != 0 \n    willFlag = ( flags & 0x04 ) != 0 \n    willQoS = ( flags >> 3 ) & 0x03 \n    willRetain = ( flags & 0x20 ) != 0 \n    userFlag = ( flags & 0x80 ) != 0 \n    passFlag = ( flags & 0x40 ) != 0 \n    packet_remaining = packet_remaining [ 2 : ] \n    self . keepalive = decode16Int ( packet_remaining ) \n    packet_remaining = packet_remaining [ 2 : ] \n    self . clientId , packet_remaining = decodeString ( packet_remaining ) \n    if willFlag : \n        self . willRetain = willRetain \n        self . willQoS = willQoS \n        self . willTopic , packet_remaining = decodeString ( packet_remaining ) \n        self . willMessage , packet_remaining = decodeString ( packet_remaining ) \n    if userFlag : \n        self . username , packet_remaining = decodeString ( packet_remaining ) \n    if passFlag : \n        l = decode16Int ( packet_remaining ) \n        self . password = packet_remaining [ 2 : 2 + l ] "}
{"9914": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen = lenLen + ( 1 ) \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . session = ( packet_remaining [ 0 ] & 0x01 ) == 0x01 \n    self . resultCode = int ( packet_remaining [ 1 ] ) "}
{"9915": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen = lenLen + ( 1 ) \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . msgId = decode16Int ( packet_remaining [ 0 : 2 ] ) \n    self . topics = [ ] \n    packet_remaining = packet_remaining [ 2 : ] \n    while len ( packet_remaining ) : \n        topic , packet_remaining = decodeString ( packet_remaining ) \n        qos = int ( packet_remaining [ 0 ] ) & 0x03 \n        self . topics . append ( ( topic , qos ) ) \n        packet_remaining = packet_remaining [ 1 : ] "}
{"9918": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen = lenLen + ( 1 ) \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . msgId = decode16Int ( packet_remaining [ 0 : 2 ] ) \n    self . topics = [ ] \n    packet_remaining = packet_remaining [ 2 : ] \n    while len ( packet_remaining ) : \n        l = decode16Int ( packet_remaining [ 0 : 2 ] ) \n        topic = packet_remaining [ 2 : 2 + l ] . decode ( encoding = 'utf-8' ) \n        self . topics . append ( topic ) \n        packet_remaining = packet_remaining [ 2 + l : ] "}
{"9921": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen = lenLen + ( 1 ) \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 \n    self . qos = ( packet [ 0 ] & 0x06 ) >> 1 \n    self . retain = ( packet [ 0 ] & 0x01 ) == 0x01 \n    self . topic , _ = decodeString ( packet_remaining ) \n    topicLen = decode16Int ( packet_remaining ) \n    if self . qos : \n        self . msgId = decode16Int ( packet_remaining [ topicLen + 2 : topicLen + 4 ] ) \n        self . payload = packet_remaining [ topicLen + 4 : ] \n    else : \n        self . msgId = None \n        self . payload = packet_remaining [ topicLen + 2 : ] "}
{"9922": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen = lenLen + ( 1 ) \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . msgId = decode16Int ( packet_remaining ) \n    self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 "}
{"9935": "\ndef get_total_n_points ( d ) : \n    n = 0 \n    for di in d . values ( ) : \n        n = n + ( len ( di ) ) \n    return n "}
{"9937": "\ndef unitpicker ( a , llim = 0.1 , denominator = None , focus_stage = None ) : \n    if not isinstance ( a , ( int , float ) ) : \n        a = nominal_values ( a ) \n        a = np . percentile ( a [ ~ np . isnan ( a ) ] , 25 ) \n    if denominator is not None : \n        pd = pretty_element ( denominator ) \n    else : \n        pd = '' \n    if focus_stage == 'calibrated' : \n        udict = { 0 : 'mol/mol ' + pd , 1 : 'mmol/mol ' + pd , 2 : '$\\mu$mol/mol ' + pd , 3 : 'nmol/mol ' + pd , 4 : 'pmol/mol ' + pd , 5 : 'fmol/mol ' + pd } \n    elif focus_stage == 'ratios' : \n        udict = { 0 : 'counts/count ' + pd , 1 : '$10^{-3}$ counts/count ' + pd , 2 : '$10^{-6}$ counts/count ' + pd , 3 : '$10^{-9}$ counts/count ' + pd , 4 : '$10^{-12}$ counts/count ' + pd , 5 : '$10^{-15}$ counts/count ' + pd } \n    elif focus_stage in ( 'rawdata' , 'despiked' , 'bkgsub' ) : \n        udict = udict = { 0 : 'counts' , 1 : '$10^{-3}$ counts' , 2 : '$10^{-6}$ counts' , 3 : '$10^{-9}$ counts' , 4 : '$10^{-12}$ counts' , 5 : '$10^{-15}$ counts' } \n    else : \n        udict = { 0 : '' , 1 : '' , 2 : '' , 3 : '' , 4 : '' , 5 : '' } \n    a = abs ( a ) \n    n = 0 \n    if a < llim : \n        while a < llim : \n            a = a * ( 1000 ) \n            n = n + ( 1 ) \n    return float ( 1000 ** n ) , udict [ n ] "}
{"9944": "\ndef fastsmooth ( a , win = 11 ) : \n    if win % 2 == 0 : \n        win = win + ( 1 ) \n    kernel = np . ones ( win ) / win \n    npad = int ( ( win - 1 ) / 2 ) \n    spad = np . full ( npad + 1 , np . mean ( a [ : ( npad + 1 ) ] ) ) \n    epad = np . full ( npad - 1 , np . mean ( a [ - ( npad - 1 ) : ] ) ) \n    return np . concatenate ( [ spad , np . convolve ( a , kernel , 'valid' ) , epad ] ) "}
{"9945": "\ndef fastgrad ( a , win = 11 ) : \n    if win % 2 == 0 : \n        win = win + ( 1 ) \n    wins = rolling_window ( a , win , 'ends' ) \n    a = map ( lambda x : np . polyfit ( np . arange ( win ) , x , 1 ) [ 0 ] , wins ) \n    return np . array ( list ( a ) ) "}
{"9949": "\ndef cluster_DBSCAN ( data , eps = None , min_samples = None , n_clusters = None , maxiter = 200 , ** kwargs ) : \n    if n_clusters is None : \n        if eps is None : \n            eps = 0.3 \n        db = cl . DBSCAN ( eps = eps , min_samples = min_samples , ** kwargs ) . fit ( data ) \n    else : \n        clusters = 0 \n        eps_temp = 1 / .95 \n        niter = 0 \n        while clusters < n_clusters : \n            clusters_last = clusters \n            eps_temp = eps_temp * ( 0.95 ) \n            db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n            clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n            if clusters < clusters_last : \n                eps_temp = eps_temp * ( 1 / 0.95 ) \n                db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n                clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n                warnings . warn ( ( '\\n\\n***Unable to find {:.0f} clusters in ' 'data. Found {:.0f} with an eps of {:.2e}' '' ) . format ( n_clusters , clusters , eps_temp ) ) \n                break \n            niter = niter + ( 1 ) \n            if niter == maxiter : \n                warnings . warn ( ( '\\n\\n***Maximum iterations ({:.0f}) reached' ', {:.0f} clusters not found.\\nDeacrease ' 'min_samples or n_clusters (or increase ' 'maxiter).' ) . format ( maxiter , n_clusters ) ) \n                break \n    labels = db . labels_ \n    core_samples_mask = np . zeros_like ( labels ) \n    core_samples_mask [ db . core_sample_indices_ ] = True \n    return labels , core_samples_mask "}
{"9953": "\ndef print_all ( ) : \n    _ , conf = read_latoolscfg ( ) \n    default = conf [ 'DEFAULT' ] [ 'config' ] \n    pstr = '\\nCurrently defined LAtools configurations:\\n\\n' \n    for s in conf . sections ( ) : \n        if s == default : \n            pstr = pstr + ( s + ' [DEFAULT]\\n' ) \n        elif s == 'REPRODUCE' : \n            pstr = pstr + ( s + ' [DO NOT ALTER]\\n' ) \n        else : \n            pstr = pstr + ( s + '\\n' ) \n        for k , v in conf [ s ] . items ( ) : \n            if k != 'config' : \n                if v [ : 9 ] == 'resources' : \n                    v = pkgrs . resource_filename ( 'latools' , v ) \n                pstr = pstr + ( '   ' + k + ': ' + v + '\\n' ) \n        pstr = pstr + ( '\\n' ) \n    print ( pstr ) \n    return "}
{"9954": "\ndef copy_SRM_file ( destination = None , config = 'DEFAULT' ) : \n    conf = read_configuration ( ) \n    src = pkgrs . resource_filename ( 'latools' , conf [ 'srmfile' ] ) \n    if destination is None : \n        destination = './LAtools_' + conf [ 'config' ] + '_SRMTable.csv' \n    if os . path . isdir ( destination ) : \n        destination = destination + ( 'LAtools_' + conf [ 'config' ] + '_SRMTable.csv' ) \n    copyfile ( src , destination ) \n    print ( src + ' \\n    copied to:\\n      ' + destination ) \n    return "}
{"9961": "\ndef mkrngs ( self ) : \n    bbool = bool_2_indices ( self . bkg ) \n    if bbool is not None : \n        self . bkgrng = self . Time [ bbool ] \n    else : \n        self . bkgrng = [ [ np . nan , np . nan ] ] \n    sbool = bool_2_indices ( self . sig ) \n    if sbool is not None : \n        self . sigrng = self . Time [ sbool ] \n    else : \n        self . sigrng = [ [ np . nan , np . nan ] ] \n    tbool = bool_2_indices ( self . trn ) \n    if tbool is not None : \n        self . trnrng = self . Time [ tbool ] \n    else : \n        self . trnrng = [ [ np . nan , np . nan ] ] \n    self . ns = np . zeros ( self . Time . size ) \n    n = 1 \n    for i in range ( len ( self . sig ) - 1 ) : \n        if self . sig [ i ] : \n            self . ns [ i ] = n \n        if self . sig [ i ] and ~ self . sig [ i + 1 ] : \n            n = n + ( 1 ) \n    self . n = int ( max ( self . ns ) ) \n    return "}
{"9968": "\ndef calc_correlation ( self , x_analyte , y_analyte , window = 15 , filt = True , recalc = True ) : \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    if label in self . correlations and not recalc : \n        return \n    if window % 2 != 1 : \n        window = window + ( 1 ) \n    ind = self . filt . grab_filt ( filt , [ x_analyte , y_analyte ] ) \n    x = nominal_values ( self . focus [ x_analyte ] ) \n    x [ ~ ind ] = np . nan \n    xr = rolling_window ( x , window , pad = np . nan ) \n    y = nominal_values ( self . focus [ y_analyte ] ) \n    y [ ~ ind ] = np . nan \n    yr = rolling_window ( y , window , pad = np . nan ) \n    r , p = zip ( * map ( nan_pearsonr , xr , yr ) ) \n    r = np . array ( r ) \n    p = np . array ( p ) \n    self . correlations [ label ] = r , p \n    return "}
{"9969": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = 15 , r_threshold = 0.9 , p_threshold = 0.05 , filt = True , recalc = False ) : \n    if window % 2 != 1 : \n        window = window + ( 1 ) \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    setn = self . filt . maxset + 1 \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    self . calc_correlation ( x_analyte , y_analyte , window , filt , recalc ) \n    r , p = self . correlations [ label ] \n    cfilt = ( abs ( r ) > r_threshold ) & ( p < p_threshold ) \n    cfilt = ~ cfilt \n    name = x_analyte + '_' + y_analyte + '_corr' \n    self . filt . add ( name = name , filt = cfilt , info = ( x_analyte + ' vs. ' + y_analyte + ' correlation filter.' ) , params = params , setn = setn ) \n    self . filt . off ( filt = name ) \n    self . filt . on ( analyte = y_analyte , filt = name ) \n    return "}
{"9972": "\ndef histograms ( dat , keys = None , bins = 25 , logy = False , cmap = None , ncol = 4 ) : \n    if keys is None : \n        keys = dat . keys ( ) \n    ncol = int ( ncol ) \n    nrow = calc_nrow ( len ( keys ) , ncol ) \n    fig , axs = plt . subplots ( nrow , 4 , figsize = [ ncol * 2 , nrow * 2 ] ) \n    pn = 0 \n    for k , ax in zip ( keys , axs . flat ) : \n        tmp = nominal_values ( dat [ k ] ) \n        x = tmp [ ~ np . isnan ( tmp ) ] \n        if cmap is not None : \n            c = cmap [ k ] \n        else : \n            c = ( 0 , 0 , 0 , 0.5 ) \n        ax . hist ( x , bins = bins , color = c ) \n        if logy : \n            ax . set_yscale ( 'log' ) \n            ylab = '$log_{10}(n)$' \n        else : \n            ylab = 'n' \n        ax . set_ylim ( 1 , ax . get_ylim ( ) [ 1 ] ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( ylab ) \n        ax . set_yticklabels ( [ ] ) \n        ax . text ( .95 , .95 , k , ha = 'right' , va = 'top' , transform = ax . transAxes ) \n        pn = pn + ( 1 ) \n    for ax in axs . flat [ pn : ] : \n        ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"9977": "\ndef calc_M ( molecule ) : \n    els = elements ( ) \n    parens = re . compile ( '\\(([A-z0-9]+)\\)([0-9]+)?' ) \n    stoich = re . compile ( '([A-Z][a-z]?)([0-9]+)?' ) \n    ps = parens . findall ( molecule ) \n    rem = parens . sub ( '' , molecule ) \n    m = 0 \n    if len ( ps ) > 0 : \n        for sub , ns in ps : \n            ms = 0 \n            for e , n in stoich . findall ( sub ) : \n                me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n                if n == '' : \n                    n = 1 \n                else : \n                    n = int ( n ) \n                ms = ms + ( me * n ) \n            if ns == '' : \n                ns = 1 \n            else : \n                ns = int ( ns ) \n            m = m + ( ms * ns ) \n    for e , n in stoich . findall ( rem ) : \n        me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n        if n == '' : \n            n = 1 \n        else : \n            n = int ( n ) \n        m = m + ( me * n ) \n    return m "}
{"9978": "\ndef gen_keywords ( * args : Union [ ANSIColors , ANSIStyles ] , ** kwargs : Union [ ANSIColors , ANSIStyles ] ) -> tuple : \n    fields : tuple = tuple ( ) \n    values : tuple = tuple ( ) \n    for tpl in args : \n        fields = fields + ( tpl . _fields ) \n        values = values + ( tpl ) \n    for prefix , tpl in kwargs . items ( ) : \n        fields = fields + ( tuple ( map ( lambda x : '_' . join ( [ prefix , x ] ) , tpl . _fields ) ) ) \n        values = values + ( tpl ) \n    return namedtuple ( 'ANSISequences' , fields ) ( * values ) "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s = s + ( 'Subset: All Samples\\n\\n' ) \n            s = s + ( self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s = s + ( 'Subset: ' + str ( n ) + '\\n' ) \n                s = s + ( 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' ) \n                s = s + ( self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) ) \n            if len ( self . subsets [ 'not_in_set' ] ) > 0 : \n                s = s + ( '\\nNot in Subset:\\n' ) \n                s = s + ( 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' ) \n                s = s + ( self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) ) \n        print ( s ) \n        return \n    elif sample is not None : \n        s = s + ( 'Sample: ' + sample + '\\n' ) \n        s = s + ( self . data [ sample ] . filt . __repr__ ( ) ) \n        print ( s ) \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s = s + ( 'Subset: ' + str ( n ) + '\\n' ) \n            s = s + ( 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' ) \n            s = s + ( self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) ) \n        print ( s ) \n        return "}
{"10000": "\ndef gradient_histogram ( self , analytes = None , win = 15 , filt = False , bins = None , samples = None , subset = None , recalc = True , ncol = 4 ) : \n    if analytes is None : \n        analytes = [ a for a in self . analytes if self . internal_standard not in a ] \n    if not hasattr ( self , 'gradients' ) : \n        self . gradients = Bunch ( ) \n    ncol = int ( ncol ) \n    n = len ( analytes ) \n    nrow = plot . calc_nrow ( n , ncol ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axs = plt . subplots ( nrow , ncol , figsize = [ 3. * ncol , 2.5 * nrow ] ) \n    if not isinstance ( axs , np . ndarray ) : \n        axs = [ axs ] \n    i = 0 \n    for a , ax in zip ( analytes , axs . flatten ( ) ) : \n        d = nominal_values ( self . gradients [ a ] ) \n        d = d [ ~ np . isnan ( d ) ] \n        m , u = unitpicker ( d , focus_stage = self . focus_stage , denominator = self . internal_standard ) \n        if bins is None : \n            ibins = np . linspace ( * np . percentile ( d * m , [ 1 , 99 ] ) , 50 ) \n        else : \n            ibins = bins \n        ax . hist ( d * m , bins = ibins , color = self . cmaps [ a ] ) \n        ax . axvline ( 0 , ls = 'dashed' , lw = 1 , c = ( 0 , 0 , 0 , 0.7 ) ) \n        ax . set_title ( a , loc = 'left' ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( 'N' ) \n        ax . set_xlabel ( u + '/s' ) \n        i = i + ( 1 ) \n    if i < ncol * nrow : \n        for ax in axs . flatten ( ) [ i : ] : \n            ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"10006": "\ndef sample_stats ( self , analytes = None , filt = True , stats = [ 'mean' , 'std' ] , eachtrace = True , csf_dict = { } ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats_calced = [ ] \n    stat_fns = Bunch ( ) \n    stat_dict = { 'mean' : np . nanmean , 'std' : np . nanstd , 'nanmean' : np . nanmean , 'nanstd' : np . nanstd , 'se' : stderr , 'H15_mean' : H15_mean , 'H15_std' : H15_std , 'H15_se' : H15_se } \n    for s in stats : \n        if isinstance ( s , str ) : \n            if s in stat_dict . keys ( ) : \n                self . stats_calced . append ( s ) \n                stat_fns [ s ] = stat_dict [ s ] \n            if s in csf_dict . keys ( ) : \n                self . stats_calced . append ( s ) \n                exec ( csf_dict [ s ] ) \n                stat_fns [ s ] = eval ( s ) \n        elif callable ( s ) : \n            self . stats_calced . append ( s . __name__ ) \n            stat_fns [ s . __name__ ] = s \n            if not hasattr ( self , 'custom_stat_functions' ) : \n                self . custom_stat_functions = '' \n            self . custom_stat_functions = self . custom_stat_functions + ( inspect . getsource ( s ) + '\\n\\n\\n\\n' ) \n    with self . pbar . set ( total = len ( self . samples ) , desc = 'Calculating Stats' ) as prog : \n        for s in self . samples : \n            if self . srm_identifier not in s : \n                self . data [ s ] . sample_stats ( analytes , filt = filt , stat_fns = stat_fns , eachtrace = eachtrace ) \n                self . stats [ s ] = self . data [ s ] . stats \n            prog . update ( ) "}
{"10017": "\ndef noise_despike ( sig , win = 3 , nlim = 24. , maxiter = 4 ) : \n    if win % 2 != 1 : \n        win = win + ( 1 ) \n    kernel = np . ones ( win ) / win \n    over = np . ones ( len ( sig ) , dtype = bool ) \n    npad = int ( ( win - 1 ) / 2 ) \n    over [ : npad ] = False \n    over [ - npad : ] = False \n    nloops = 0 \n    while any ( over ) and ( nloops < maxiter ) : \n        rmean = np . convolve ( sig , kernel , 'valid' ) \n        rstd = rmean ** 0.5 \n        over [ npad : - npad ] = ( sig [ npad : - npad ] > rmean + nlim * rstd ) \n        if any ( over ) : \n            sig [ npad : - npad ] [ over [ npad : - npad ] ] = rmean [ over [ npad : - npad ] ] \n            nloops = nloops + ( 1 ) \n    return sig "}
{"10018": "\ndef expdecay_despike ( sig , expdecay_coef , tstep , maxiter = 3 ) : \n    noise = np . std ( sig [ : 5 ] ) \n    for i in [ 10 , 20 , 30 , 50 ] : \n        inoise = np . std ( sig [ : i ] ) \n        if inoise < 1.5 * noise : \n            noise = inoise \n    rms_noise3 = 3 * noise \n    i = 0 \n    f = True \n    while ( i < maxiter ) and f : \n        siglo = np . roll ( sig * np . exp ( tstep * expdecay_coef ) , 1 ) \n        sighi = np . roll ( sig * np . exp ( - tstep * expdecay_coef ) , - 1 ) \n        loind = ( sig < siglo - rms_noise3 ) & ( sig < np . roll ( sig , - 1 ) - rms_noise3 ) \n        hiind = ( sig > sighi + rms_noise3 ) & ( sig > np . roll ( sig , 1 ) + rms_noise3 ) \n        sig [ loind ] = sig [ np . roll ( loind , - 1 ) ] \n        sig [ hiind ] = sig [ np . roll ( hiind , - 1 ) ] \n        f = any ( np . concatenate ( [ loind , hiind ] ) ) \n        i = i + ( 1 ) \n    return sig "}
{"10019": "\ndef add ( self , name , filt , info = '' , params = ( ) , setn = None ) : \n    iname = '{:.0f}_' . format ( self . n ) + name \n    self . index [ self . n ] = iname \n    if setn is None : \n        setn = self . maxset + 1 \n    self . maxset = setn \n    if setn not in self . sets . keys ( ) : \n        self . sets [ setn ] = [ iname ] \n    else : \n        self . sets [ setn ] . append ( iname ) \n    self . components [ iname ] = filt \n    self . info [ iname ] = info \n    self . params [ iname ] = params \n    for a in self . analytes : \n        self . switches [ a ] [ iname ] = False \n    self . n = self . n + ( 1 ) \n    return "}
{"10026": "\ndef get_info ( self ) : \n    out = '' \n    for k in sorted ( self . components . keys ( ) ) : \n        out = out + ( '{:s}: {:s}' . format ( k , self . info [ k ] ) + '\\n' ) \n    return ( out ) "}
{"10048": "\ndef get_url ( self , nbfile ) : \n    urls = self . urls \n    if isinstance ( urls , dict ) : \n        return urls . get ( nbfile ) \n    elif isstring ( urls ) : \n        if not urls . endswith ( '/' ) : \n            urls = urls + ( '/' ) \n        return urls + nbfile "}
{"10060": "\ndef received ( self , src , body ) : \n    self . _msgid = self . _msgid + ( 1 ) \n    message = IncomingMessage ( src , body , self . _msgid ) \n    self . _traffic . append ( message ) \n    self . _receive_message ( message ) \n    return message "}
{"10076": "\ndef send ( self , message ) : \n    assert message . send_to , \"No recipients have been added\" \n    if message . has_bad_headers ( self . mail . default_sender ) : \n        raise BadHeaderError \n    if message . date is None : \n        message . date = time . time ( ) \n    sender = message . sender or self . mail . default_sender \n    if self . host : \n        self . host . sendmail ( sanitize_address ( sender ) if sender is not None else None , message . send_to , message . as_string ( self . mail . default_sender ) , message . mail_options , message . rcpt_options ) \n    email_dispatched . send ( message , mail = self . mail ) \n    self . num_emails = self . num_emails + ( 1 ) \n    if self . num_emails == self . mail . max_emails : \n        self . num_emails = 0 \n        if self . host : \n            self . host . quit ( ) \n            self . host = self . configure_host ( ) "}
{"10087": "\ndef _cauchy_equation ( wavelength , coefficients ) : \n    n = 0. \n    for i , c in enumerate ( coefficients ) : \n        exponent = 2 * i \n        n = n + ( c / wavelength ** exponent ) \n    return n "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if self . processes == 1 : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if end > number_pages : \n                end = number_pages \n            begin = begin + ( 1 ) \n            end = end + ( 1 ) \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10101": "\ndef _init_population_stats ( self , vcf_reader , dependent_tag_id ) : \n    n = 0 \n    mean = 0 \n    M2 = 0 \n    try : \n        vcf_reader . open ( ) \n        for vcf_record in vcf_reader . vcf_records ( ) : \n            for tag_values in vcf_record . sample_tag_values . values ( ) : \n                value = self . _get_dependent_value ( tag_values , dependent_tag_id ) \n                if value is not None : \n                    n = n + ( 1 ) \n                    delta = value - mean \n                    mean = mean + ( delta / n ) \n                    M2 = M2 + ( delta * ( value - mean ) ) \n    finally : \n        vcf_reader . close ( ) \n    mean = round ( mean , self . _MAX_PRECISION ) \n    stdev = 0 \n    if n == 0 : \n        mean = None \n        stdev = None \n    elif n >= 2 : \n        variance = M2 / n \n        stdev = round ( math . sqrt ( variance ) , self . _MAX_PRECISION ) \n    return mean , stdev "}
{"10106": "\ndef seek_next_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        data_len , data = self . read ( self . read_size ) \n        data_where = 0 \n        if not data_len : \n            break \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ - 1 ] == b'\\r' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            terminator_len , terminator_data = self . read ( 1 ) \n            if terminator_len and terminator_data [ 0 ] == b'\\n' [ 0 ] : \n                data_len = data_len + ( 1 ) \n                data = data + ( b'\\n' ) \n            else : \n                self . file . seek ( terminator_where ) \n        while data_where < data_len : \n            terminator = self . prefix_line_terminator ( data [ data_where : ] ) \n            if terminator : \n                self . file . seek ( where + offset + data_where + len ( terminator ) ) \n                return self . file . tell ( ) \n            else : \n                data_where = data_where + ( 1 ) \n        offset = offset + ( data_len ) \n        self . file . seek ( where + offset ) \n    return - 1 "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        if offset == where : \n            break \n        read_size = self . read_size if self . read_size <= where else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ 0 ] == b'\\n' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            if terminator_where > data_len + 1 : \n                self . file . seek ( where - offset - data_len - 1 , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( 1 ) \n                if terminator_data [ 0 ] == b'\\r' [ 0 ] : \n                    data_len = data_len + ( 1 ) \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while data_where > 0 : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and offset == 0 and data_where == data_len : \n                data_where = data_where - ( len ( terminator ) ) \n            elif terminator : \n                self . file . seek ( where - offset - ( data_len - data_where ) ) \n                return self . file . tell ( ) \n            else : \n                data_where = data_where - ( 1 ) \n        offset = offset + ( data_len ) \n    if where == 0 : \n        return - 1 \n    else : \n        self . file . seek ( 0 ) \n        return 0 "}
{"10129": "\ndef iter_osm_stream ( start_sqn = None , base_url = 'https://planet.openstreetmap.org/replication/minute' , expected_interval = 60 , parse_timestamps = True , state_dir = None ) : \n    if state_dir : \n        if not os . path . exists ( state_dir ) : \n            raise Exception ( 'Specified state_dir \"%s\" doesn\\'t exist.' % state_dir ) \n        if os . path . exists ( '%s/state.txt' % state_dir ) : \n            with open ( '%s/state.txt' % state_dir ) as f : \n                state = readState ( f ) \n                start_sqn = state [ 'sequenceNumber' ] \n    if not start_sqn : \n        u = urllib2 . urlopen ( '%s/state.txt' % base_url ) \n        state = readState ( u ) \n    else : \n        sqnStr = str ( start_sqn ) . zfill ( 9 ) \n        u = urllib2 . urlopen ( '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) ) \n        state = readState ( u ) \n    interval_fudge = 0.0 \n    while True : \n        sqnStr = state [ 'sequenceNumber' ] . zfill ( 9 ) \n        url = '%s/%s/%s/%s.osc.gz' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        content = urllib2 . urlopen ( url ) \n        content = StringIO . StringIO ( content . read ( ) ) \n        gzipper = gzip . GzipFile ( fileobj = content ) \n        for a in iter_osm_change_file ( gzipper , parse_timestamps ) : \n            yield a \n        stateTs = datetime . datetime . strptime ( state [ 'timestamp' ] , \"%Y-%m-%dT%H:%M:%SZ\" ) \n        yield ( None , model . Finished ( state [ 'sequenceNumber' ] , stateTs ) ) \n        nextTs = stateTs + datetime . timedelta ( seconds = expected_interval + interval_fudge ) \n        if datetime . datetime . utcnow ( ) < nextTs : \n            timeToSleep = ( nextTs - datetime . datetime . utcnow ( ) ) . total_seconds ( ) \n        else : \n            timeToSleep = 0.0 \n        time . sleep ( timeToSleep ) \n        sqnStr = str ( int ( state [ 'sequenceNumber' ] ) + 1 ) . zfill ( 9 ) \n        url = '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        delay = 1.0 \n        while True : \n            try : \n                u = urllib2 . urlopen ( url ) \n                interval_fudge = interval_fudge - ( ( interval_fudge / 2.0 ) ) \n                break \n            except urllib2 . HTTPError as e : \n                if e . code == 404 : \n                    time . sleep ( delay ) \n                    delay = min ( delay * 2 , 13 ) \n                    interval_fudge = interval_fudge + ( delay ) \n        if state_dir : \n            with open ( '%s/state.txt' % state_dir , 'w' ) as f : \n                f . write ( u . read ( ) ) \n            with open ( '%s/state.txt' % state_dir , 'r' ) as f : \n                state = readState ( f ) \n        else : \n            state = readState ( u ) "}
{"10147": "\ndef _add_discount ( self , product , quantity , discounts ) : \n    def matches ( discount ) : \n        if isinstance ( discount . clause , conditions . DiscountForCategory ) : \n            return discount . clause . category == product . category \n        else : \n            return discount . clause . product == product \n    def value ( discount ) : \n        if discount . clause . percentage is not None : \n            return discount . clause . percentage * product . price \n        else : \n            return discount . clause . price \n    discounts = [ i for i in discounts if matches ( i ) ] \n    discounts . sort ( key = value ) \n    for candidate in reversed ( discounts ) : \n        if quantity == 0 : \n            break \n        elif candidate . quantity == 0 : \n            continue \n        discount_item = commerce . DiscountItem . objects . create ( product = product , cart = self . cart , discount = candidate . discount , quantity = quantity , ) \n        ours = discount_item . quantity \n        allowed = candidate . quantity \n        if ours > allowed : \n            discount_item . quantity = allowed \n            discount_item . save ( ) \n            quantity = ours - allowed \n        else : \n            quantity = 0 \n        candidate . quantity = candidate . quantity - ( discount_item . quantity ) "}
{"10153": "\ndef items_sold ( ) : \n    data = None \n    headings = None \n    line_items = commerce . LineItem . objects . filter ( invoice__status = commerce . Invoice . STATUS_PAID , ) . select_related ( \"invoice\" ) \n    line_items = line_items . order_by ( \"-price\" , \"description\" , ) . values ( \"price\" , \"description\" , ) . annotate ( total_quantity = Sum ( \"quantity\" ) , ) \n    headings = [ \"Description\" , \"Quantity\" , \"Price\" , \"Total\" ] \n    data = [ ] \n    total_income = 0 \n    for line in line_items : \n        cost = line [ \"total_quantity\" ] * line [ \"price\" ] \n        data . append ( [ line [ \"description\" ] , line [ \"total_quantity\" ] , line [ \"price\" ] , cost , ] ) \n        total_income = total_income + ( cost ) \n    data . append ( [ \"(TOTAL)\" , \"--\" , \"--\" , total_income , ] ) \n    return ListReport ( \"Items sold\" , headings , data ) "}
{"10160": "\ndef paid_invoices_by_date ( request , form ) : \n    products = form . cleaned_data [ \"product\" ] \n    categories = form . cleaned_data [ \"category\" ] \n    invoices = commerce . Invoice . objects . filter ( ( Q ( lineitem__product__in = products ) | Q ( lineitem__product__category__in = categories ) ) , status = commerce . Invoice . STATUS_PAID , ) \n    payments = commerce . PaymentBase . objects . all ( ) \n    payments = payments . filter ( invoice__in = invoices , ) \n    payments = payments . order_by ( \"invoice\" ) \n    invoice_max_time = payments . values ( \"invoice\" ) . annotate ( max_time = Max ( \"time\" ) ) \n    zero_value_invoices = invoices . filter ( value = 0 ) \n    times = itertools . chain ( ( line [ \"max_time\" ] for line in invoice_max_time ) , ( invoice . issue_time for invoice in zero_value_invoices ) , ) \n    by_date = collections . defaultdict ( int ) \n    for time in times : \n        date = datetime . datetime ( year = time . year , month = time . month , day = time . day ) \n        by_date [ date ] = by_date [ date ] + ( 1 ) \n    data = [ ( date_ , count ) for date_ , count in sorted ( by_date . items ( ) ) ] \n    data = [ ( date_ . strftime ( \"%Y-%m-%d\" ) , count ) for date_ , count in data ] \n    return ListReport ( \"Paid Invoices By Date\" , [ \"date\" , \"count\" ] , data , ) "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if i > 0 and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter = counter + ( 1 ) \n    print ( ) \n    return session_id "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if prev_q [ 'chr' ] < current_q [ 'chr' ] : \n            return True \n        if prev_q [ 'chr' ] > current_q [ 'chr' ] : \n            return False \n        if prev_q [ 'start' ] < current_q [ 'start' ] : \n            return True \n        if prev_q [ 'start' ] > current_q [ 'start' ] : \n            return False \n        if prev_q [ 'stop' ] < current_q [ 'stop' ] : \n            return True \n        if prev_q [ 'stop' ] > current_q [ 'stop' ] : \n            return False \n        return True \n    ct_pointer = 0 \n    query_pointer = 0 \n    last_query_pointer = - 1 \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while query_pointer < len ( sorted_queries ) and ct_pointer < len ( ct ) : \n        if last_query_pointer != query_pointer : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if q_chr < c_chr : \n            query_pointer = query_pointer + ( 1 ) \n            continue \n        if q_chr > c_chr : \n            ct_pointer = ct_pointer + ( 1 ) \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if q_start > c_stop : \n            ct_pointer = ct_pointer + ( 1 ) \n            continue \n        if q_stop < c_start : \n            query_pointer = query_pointer + ( 1 ) \n            continue \n        if search_mode == 'any' : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        elif search_mode == 'exact' and q_start == c_start and q_stop == c_stop : \n            q_alt = q . alt \n            c_alt = c . alt \n            if not ( q_alt and c_alt and q_alt != c_alt ) : \n                matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n        elif search_mode == 'include_smaller' : \n            raise NotImplementedError \n        elif search_mode == 'include_larger' : \n            raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer = ct_pointer + ( 1 ) \n    return dict ( matches ) "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if period < 1 or period > 3 : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + \"2903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10249": "\ndef setMeterPassword ( self , new_pwd , pwd = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMeterPassword\" ) \n    try : \n        if len ( new_pwd ) != 8 or len ( pwd ) != 8 : \n            self . writeCmdMsg ( \"Passwords must be exactly eight characters.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Pre command read failed: check serial line.\" ) \n        else : \n            if not self . serialCmdPwdAuth ( pwd ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_pwd = binascii . hexlify ( new_pwd . zfill ( 8 ) ) \n                req_str = \"015731023030323028\" + req_pwd + \"2903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMeterPassword): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = 0 \n    if kwh_scale == ScaleKWH . EmptyScale : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count = count + ( 1 ) \n            continue \n        if len ( contents ) == 0 : \n            count = count + ( 1 ) \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if fld_type == FieldType . Float : \n                float_data = float ( str ( raw_data ) ) \n                divisor = 1 \n                if fld_scale == ScaleType . KWH : \n                    divisor = 1 \n                    if kwh_scale == ScaleKWH . Scale10 : \n                        divisor = 10 \n                    elif kwh_scale == ScaleKWH . Scale100 : \n                        divisor = 100 \n                    elif ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : \n                        ekm_log ( \"Unrecognized kwh scale.\" ) \n                elif fld_scale == ScaleType . Div10 : \n                    divisor = 10 \n                elif fld_scale == ScaleType . Div100 : \n                    divisor = 100 \n                elif fld_scale != ScaleType . No : \n                    ekm_log ( \"Unrecognized float scale.\" ) \n                float_data = float_data / ( divisor ) \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            elif fld_type == FieldType . Hex : \n                hex_data = raw_data . encode ( 'hex' ) \n                def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n            elif fld_type == FieldType . Int : \n                integer_data = int ( raw_data ) \n                integer_data_str = str ( integer_data ) \n                if len ( integer_data_str ) == 0 : \n                    integer_data_str = str ( 0 ) \n                def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n            elif fld_type == FieldType . String : \n                string_data = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n            elif fld_type == FieldType . PowerFactor : \n                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n            else : \n                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count = count + ( 1 ) \n    return True "}
{"10253": "\ndef crcMeterRead ( self , raw_read , def_buf ) : \n    try : \n        if len ( raw_read ) == 0 : \n            ekm_log ( \"(\" + self . m_context + \") Empty return read.\" ) \n            return False \n        sent_crc = self . calc_crc16 ( raw_read [ 1 : - 2 ] ) \n        logstr = \"(\" + self . m_context + \")CRC sent = \" + str ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] ) \n        logstr = logstr + ( \" CRC calc = \" + sent_crc ) \n        ekm_log ( logstr ) \n        if int ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] , 16 ) == int ( sent_crc , 16 ) : \n            return True \n    except struct . error : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except TypeError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except ValueError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    return False "}
{"10256": "\ndef setCTRatio ( self , new_ct , password = \"00000000\" ) : \n    ret = False \n    self . setContext ( \"setCTRatio\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if ( ( new_ct != CTRatio . Amps_100 ) and ( new_ct != CTRatio . Amps_200 ) and ( new_ct != CTRatio . Amps_400 ) and ( new_ct != CTRatio . Amps_600 ) and ( new_ct != CTRatio . Amps_800 ) and ( new_ct != CTRatio . Amps_1000 ) and ( new_ct != CTRatio . Amps_1200 ) and ( new_ct != CTRatio . Amps_1500 ) and ( new_ct != CTRatio . Amps_2000 ) and ( new_ct != CTRatio . Amps_3000 ) and ( new_ct != CTRatio . Amps_4000 ) and ( new_ct != CTRatio . Amps_5000 ) ) : \n            self . writeCmdMsg ( \"Legal CT Ratios: 100, 200, 400, 600, \" + \"800, 1000, 1200, 1500, 2000, 3000, 4000 and 5000\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030443028\" + binascii . hexlify ( str ( new_ct ) . zfill ( 4 ) ) + \"2903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setCTRatio): 06 returned.\" ) \n                    ret = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return ret "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( hour < 0 ) or ( hour > 23 ) or ( minute < 0 ) or ( minute > 59 ) or ( tariff < 0 ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + 1 ) ) \n        return False \n    period = period + ( 1 ) \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return False \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return False \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return False \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return True "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season = season + ( 1 ) \n    schedule = schedule + ( 1 ) \n    if ( ( season < 1 ) or ( season > Extents . Seasons ) or ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( month > 12 ) or ( month < 0 ) or ( day < 0 ) or ( day > 31 ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return False \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return False \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return False \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return False \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return True "}
{"10259": "\ndef setSeasonSchedules ( self , cmd_dict = None , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setSeasonSchedules\" ) \n    if not cmd_dict : \n        cmd_dict = self . m_seasons_sched_params \n    try : \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Month\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Day\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_1_Schedule\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Month\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Day\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_2_Schedule\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Month\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Day\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_3_Schedule\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Month\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Day\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( cmd_dict [ \"Season_4_Schedule\" ] ) . zfill ( 2 ) ) ) \n                req_table = req_table + ( binascii . hexlify ( str ( 0 ) . zfill ( 24 ) ) ) \n                req_str = \"015731023030383028\" + req_table + \"2903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setSeasonSchedules): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday = holiday + ( 1 ) \n    if ( month > 12 ) or ( month < 0 ) or ( day > 31 ) or ( day < 0 ) or ( holiday < 1 ) or ( holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return False \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return False \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return False \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return True "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str = req_str + ( req_crc ) \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if tableset == ReadSchedules . Schedules_1_To_4 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return True \n        elif tableset == ReadSchedules . Schedules_5_To_6 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n            self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                self . setContext ( \"\" ) \n                return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10262": "\ndef extractSchedule ( self , schedule , period ) : \n    ret = namedtuple ( \"ret\" , [ \"Hour\" , \"Min\" , \"Tariff\" , \"Period\" , \"Schedule\" ] ) \n    work_table = self . m_schd_1_to_4 \n    if Schedules . Schedule_5 <= schedule <= Schedules . Schedule_6 : \n        work_table = self . m_schd_5_to_6 \n    period = period + ( 1 ) \n    schedule = schedule + ( 1 ) \n    ret . Period = str ( period ) \n    ret . Schedule = str ( schedule ) \n    if ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( period < 0 ) or ( period > Extents . Periods ) : \n        ekm_log ( \"Out of bounds: tariff \" + str ( period ) + \" for schedule \" + str ( schedule ) ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    idxhr = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Hour\" \n    idxmin = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Min\" \n    idxrate = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Tariff\" \n    if idxhr not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxhr ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxmin not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxmin ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxrate not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxrate ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    ret . Hour = work_table [ idxhr ] [ MeterData . StringValue ] \n    ret . Min = work_table [ idxmin ] [ MeterData . StringValue ] . zfill ( 2 ) \n    ret . Tariff = work_table [ idxrate ] [ MeterData . StringValue ] \n    return ret "}
{"10263": "\ndef readMonthTariffs ( self , months_type ) : \n    self . setContext ( \"readMonthTariffs\" ) \n    try : \n        req_type = binascii . hexlify ( str ( months_type ) . zfill ( 1 ) ) \n        req_str = \"01523102303031\" + req_type + \"282903\" \n        work_table = self . m_mons \n        if months_type == ReadMonths . kWhReverse : \n            work_table = self . m_rev_mons \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str = req_str + ( req_crc ) \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , work_table ) \n        self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if str ( return_crc ) == str ( work_table [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Months CRC success, type = \" + str ( req_type ) ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10264": "\ndef extractMonthTariff ( self , month ) : \n    ret = namedtuple ( \"ret\" , [ \"Month\" , Field . kWh_Tariff_1 , Field . kWh_Tariff_2 , Field . kWh_Tariff_3 , Field . kWh_Tariff_4 , Field . kWh_Tot , Field . Rev_kWh_Tariff_1 , Field . Rev_kWh_Tariff_2 , Field . Rev_kWh_Tariff_3 , Field . Rev_kWh_Tariff_4 , Field . Rev_kWh_Tot ] ) \n    month = month + ( 1 ) \n    ret . Month = str ( month ) \n    if ( month < 1 ) or ( month > Extents . Months ) : \n        ret . kWh_Tariff_1 = ret . kWh_Tariff_2 = ret . kWh_Tariff_3 = ret . kWh_Tariff_4 = str ( 0 ) \n        ret . Rev_kWh_Tariff_1 = ret . Rev_kWh_Tariff_2 = ret . Rev_kWh_Tariff_3 = ret . Rev_kWh_Tariff_4 = str ( 0 ) \n        ret . kWh_Tot = ret . Rev_kWh_Tot = str ( 0 ) \n        ekm_log ( \"Out of range(Extents.Months) month = \" + str ( month ) ) \n        return ret \n    base_str = \"Month_\" + str ( month ) + \"_\" \n    ret . kWh_Tariff_1 = self . m_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_2 = self . m_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_3 = self . m_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_4 = self . m_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . kWh_Tot = self . m_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_1 = self . m_rev_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_2 = self . m_rev_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_3 = self . m_rev_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_4 = self . m_rev_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tot = self . m_rev_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    return ret "}
{"10265": "\ndef readHolidayDates ( self ) : \n    self . setContext ( \"readHolidayDates\" ) \n    try : \n        req_str = \"0152310230304230282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str = req_str + ( req_crc ) \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) \n        self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if str ( return_crc ) == str ( self . m_hldy [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Holidays and Schedules CRC success\" ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10266": "\ndef extractHolidayDate ( self , setting_holiday ) : \n    ret = namedtuple ( \"result\" , [ \"Holiday\" , \"Month\" , \"Day\" ] ) \n    setting_holiday = setting_holiday + ( 1 ) \n    ret . Holiday = str ( setting_holiday ) \n    if ( setting_holiday < 1 ) or ( setting_holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds:  holiday \" + str ( setting_holiday ) ) \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    idxday = \"Holiday_\" + str ( setting_holiday ) + \"_Day\" \n    idxmon = \"Holiday_\" + str ( setting_holiday ) + \"_Mon\" \n    if idxmon not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    if idxday not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    ret . Day = self . m_hldy [ idxday ] [ MeterData . StringValue ] \n    ret . Month = self . m_hldy [ idxmon ] [ MeterData . StringValue ] \n    return ret "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if seconds < 0 or seconds > 9999 : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"\" \n                req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + \"2903\" ) \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10280": "\ndef setPulseInputRatio ( self , line_in , new_cnst , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setPulseInputRatio\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4 ) ) \n                line_const = binascii . hexlify ( str ( line_in - 1 ) ) \n                req_str = \"01573102303041\" + line_const + \"28\" + req_const + \"2903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10281": "\ndef setZeroResettableKWH ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setZeroResettableKWH\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"0157310230304433282903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                fill_len = 40 - len ( self . m_lcd_items ) \n                for lcdid in self . m_lcd_items : \n                    append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) \n                    req_table = req_table + ( append_val ) \n                for i in range ( 0 , fill_len ) : \n                    append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2 ) ) \n                    req_table = req_table + ( append_val ) \n                req_str = \"015731023030443228\" + req_table + \"2903\" \n                req_str = req_str + ( self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10286": "\ndef text ( length = None , at_least = 10 , at_most = 15 , lowercase = True , uppercase = True , digits = True , spaces = True , punctuation = False ) : \n    base_string = '' \n    if lowercase : \n        base_string = base_string + ( string . ascii_lowercase ) \n    if uppercase : \n        base_string = base_string + ( string . ascii_uppercase ) \n    if digits : \n        base_string = base_string + ( string . digits ) \n    if spaces : \n        base_string = base_string + ( ' ' ) \n    if punctuation : \n        base_string = base_string + ( string . punctuation ) \n    if len ( base_string ) == 0 : \n        return '' \n    if not length : \n        length = random . randint ( at_least , at_most ) \n    result = '' \n    for i in xrange ( 0 , length ) : \n        result = result + ( random . choice ( base_string ) ) \n    return result "}
{"10359": "\ndef make_rows ( num_columns , seq ) : \n    num_rows , partial = divmod ( len ( seq ) , num_columns ) \n    if partial : \n        num_rows = num_rows + ( 1 ) \n    try : \n        result = more_itertools . grouper ( seq , num_rows ) \n    except TypeError : \n        result = more_itertools . grouper ( num_rows , seq ) \n    return zip ( * result ) "}
{"10365": "\ndef partition_items ( count , bin_size ) : \n    num_bins = int ( math . ceil ( count / float ( bin_size ) ) ) \n    bins = [ 0 ] * num_bins \n    for i in range ( count ) : \n        bins [ i % num_bins ] = bins [ i % num_bins ] + ( 1 ) \n    return bins "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = 1 if pat . group ( 5 ) == 'odd' else 0 \n    elif pat . group ( 6 ) : \n        a = 0 \n        b = int ( pat . group ( 6 ) ) \n    else : \n        sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n        coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 \n    reverse = False \n    if lexeme == 'nth-last-child' : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return False \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx = idx + ( 1 ) \n        if a == 0 : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and ( idx * a + b ) >= 0 \n        return m \n    return validate "}
{"10397": "\ndef get_fn ( self , fn , max_lines = None ) : \n    stat = os . stat ( self . logfile ) \n    if ( stat . st_ino == self . lastInode ) and ( stat . st_size == self . lastSize ) : \n        return [ ] \n    if ( stat . st_ino != self . lastInode ) or ( stat . st_size < self . lastSize ) : \n        self . lastSize = 0 \n    fi = open ( self . logfile , 'rt' ) \n    fi . seek ( self . lastSize ) \n    self . lastInode = stat . st_ino \n    lines = 0 \n    for i in fi : \n        lines = lines + ( 1 ) \n        if max_lines and ( lines > max_lines ) : \n            self . storeLast ( ) \n            fi . close ( ) \n            return \n        if '\\n' in i : \n            self . lastSize = self . lastSize + ( len ( i ) ) \n            if self . parser : \n                line = self . parser ( i . strip ( '\\n' ) ) \n            else : \n                line = i . strip ( '\\n' ) \n            fn ( line ) \n    self . storeLast ( ) \n    fi . close ( ) "}
{"10408": "\ndef sendEvent ( self , source , events ) : \n    if isinstance ( events , list ) : \n        self . eventCounter = self . eventCounter + ( len ( events ) ) \n    else : \n        self . eventCounter = self . eventCounter + ( 1 ) \n        events = [ events ] \n    queue = self . _aggregateQueue ( events ) \n    if queue : \n        if ( source in self . critical ) or ( source in self . warn ) : \n            self . setStates ( source , queue ) \n        self . routeEvent ( source , queue ) \n    queue = [ ] \n    self . lastEvents [ source ] = time . time ( ) "}
{"10457": "\ndef sendEvents ( self , events ) : \n    self . pressure = self . pressure + ( 1 ) \n    self . sendString ( self . encodeMessage ( events ) ) "}
{"10465": "\ndef _saslprep_do_mapping ( chars ) : \n    i = 0 \n    while i < len ( chars ) : \n        c = chars [ i ] \n        if stringprep . in_table_c12 ( c ) : \n            chars [ i ] = \"\\u0020\" \n        elif stringprep . in_table_b1 ( c ) : \n            del chars [ i ] \n            continue \n        i = i + ( 1 ) "}
{"10505": "\ndef list ( self , path ) : \n    self . __validate_storage_path ( path ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] not in self . __BROWSABLE_TYPES : \n        raise StorageArgumentException ( 'The entity type \"{0}\" cannot be' 'listed' . format ( entity [ 'entity_type' ] ) ) \n    entity_uuid = entity [ 'uuid' ] \n    file_names = [ ] \n    more_pages = True \n    page_number = 1 \n    while more_pages : \n        response = self . api_client . list_folder_content ( entity_uuid , page = page_number , ordering = 'name' ) \n        more_pages = response [ 'next' ] is not None \n        page_number = page_number + ( 1 ) \n        for child in response [ 'results' ] : \n            pattern = '/{name}' if child [ 'entity_type' ] == 'folder' else '{name}' \n            file_names . append ( pattern . format ( name = child [ 'name' ] ) ) \n    return file_names "}
{"10543": "\ndef tshift ( self , t ) : \n    raw = self . raw ( ) \n    for i in range ( len ( raw ) ) : \n        raw [ i ] [ \"t\" ] = raw [ i ] [ \"t\" ] + ( t ) \n    return self "}
{"10544": "\ndef sum ( self ) : \n    raw = self . raw ( ) \n    s = 0 \n    for i in range ( len ( raw ) ) : \n        s = s + ( raw [ i ] [ \"d\" ] ) \n    return s "}
{"10618": "\ndef __reconnect ( self ) : \n    self . status = \"reconnecting\" \n    if self . disconnected_time - self . connected_time > 15 * 60 : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    else : \n        self . reconnect_time = self . reconnect_time * ( self . reconnect_time_backoff_multiplier ) \n    if self . reconnect_time > self . reconnect_time_max_seconds : \n        self . reconnect_time = self . reconnect_time_max_seconds \n    self . reconnect_time = self . reconnect_time * ( 1 + random . uniform ( - 0.2 , 0.2 ) ) \n    if self . reconnect_time < self . reconnect_time_starting_seconds : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    logging . warn ( \"ConnectorDB:WS: Attempting to reconnect in %fs\" , self . reconnect_time ) \n    self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) \n    self . reconnector . daemon = True \n    self . reconnector . start ( ) "}
{"10620": "\ndef __on_open ( self , ws ) : \n    logging . debug ( \"ConnectorDB: Websocket opened\" ) \n    self . reconnect_time = self . reconnect_time / ( self . reconnect_time_backoff_multiplier ) \n    self . status = \"connected\" \n    self . lastpingtime = time . time ( ) \n    self . __ensure_ping ( ) \n    self . connected_time = time . time ( ) \n    self . ws_openlock . release ( ) "}
{"10623": "\ndef __on_message ( self , ws , msg ) : \n    msg = json . loads ( msg ) \n    logging . debug ( \"ConnectorDB:WS: Msg '%s'\" , msg [ \"stream\" ] ) \n    stream_key = msg [ \"stream\" ] + \":\" \n    if \"transform\" in msg : \n        stream_key = stream_key + ( msg [ \"transform\" ] ) \n    self . subscription_lock . acquire ( ) \n    if stream_key in self . subscriptions : \n        subscription_function = self . subscriptions [ stream_key ] \n        self . subscription_lock . release ( ) \n        fresult = subscription_function ( msg [ \"stream\" ] , msg [ \"data\" ] ) \n        if fresult is True : \n            fresult = msg [ \"data\" ] \n        if fresult is not False and fresult is not None and msg [ \"stream\" ] . endswith ( \"/downlink\" ) and msg [ \"stream\" ] . count ( \"/\" ) == 3 : \n            self . insert ( msg [ \"stream\" ] [ : - 9 ] , fresult ) \n    else : \n        self . subscription_lock . release ( ) \n        logging . warn ( \"ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s\" , msg [ \"stream\" ] , list ( self . subscriptions . keys ( ) ) ) "}
{"10645": "\ndef start ( self , job ) : \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-worker:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"\\\"SPARK_MASTER_IP=\" + self . masterIP + \":\" + _SPARK_MASTER_PORT + \"\\\"\" , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . masterIP + \":\" + _SPARK_MASTER_PORT ] ) [ : - 1 ] \n    self . __start_datanode ( job ) \n    hdfs_down = True \n    retries = 0 \n    while hdfs_down and ( retries < 5 ) : \n        _log . info ( \"Sleeping 30 seconds before checking HDFS startup.\" ) \n        time . sleep ( 30 ) \n        clusterID = \"\" \n        try : \n            clusterID = subprocess . check_output ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"grep\" , \"clusterID\" , \"-R\" , \"/opt/apache-hadoop/logs\" ] ) \n        except : \n            pass \n        if \"Incompatible\" in clusterID : \n            _log . warning ( \"Hadoop Datanode failed to start with: %s\" , clusterID ) \n            _log . warning ( \"Retrying container startup, retry #%d.\" , retries ) \n            retries = retries + ( 1 ) \n            _log . warning ( \"Removing ephemeral hdfs directory.\" ) \n            subprocess . check_call ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"rm\" , \"-rf\" , \"/ephemeral/hdfs\" ] ) \n            _log . warning ( \"Killing container %s.\" , self . hdfsContainerID ) \n            subprocess . check_call ( [ \"docker\" , \"kill\" , self . hdfsContainerID ] ) \n            _log . info ( \"Restarting datanode.\" ) \n            self . __start_datanode ( job ) \n        else : \n            _log . info ( \"HDFS datanode started up OK!\" ) \n            hdfs_down = False \n    if retries >= 5 : \n        raise RuntimeError ( \"Failed %d times trying to start HDFS datanode.\" % retries ) \n    return "}
{"10649": "\ndef base_tokenizer ( fp ) : \n    if isinstance ( fp , StringIO ) : \n        template_file = fp \n        size = template_file . len \n    else : \n        if os . fstat ( fp . fileno ( ) ) . st_size == 0 : \n            yield TOKEN_EOF , 'EOF' , 0 , 0 \n            return \n        template_file = mmap . mmap ( fp . fileno ( ) , 0 , access = mmap . ACCESS_READ ) \n        size = template_file . size ( ) \n    lineno = 0 \n    while 1 : \n        lineno = lineno + ( 1 ) \n        pos = 1 \n        if template_file . tell ( ) == size : \n            yield TOKEN_EOF , 'EOF' , lineno , 0 \n            break \n        line = template_file . readline ( ) . decode ( 'utf-8' ) \n        line = line . replace ( '\\r\\n' , '' ) \n        line = line . replace ( '\\n' , '' ) \n        if re_comment . match ( line ) : \n            continue \n        last_text = deque ( ) \n        while line : \n            line_len = len ( line ) \n            for token in tokens : \n                m = token . regex . match ( line ) \n                if m : \n                    if last_text : \n                        yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos \n                        pos = pos + ( len ( last_text ) ) \n                        last_text . clear ( ) \n                    offset , value = m . end ( ) , m . group ( ) \n                    line = line [ offset : ] \n                    yield token , value , lineno , pos \n                    pos = pos + ( offset ) \n                    break \n            if line_len == len ( line ) : \n                last_text . append ( line [ 0 ] ) \n                line = line [ 1 : ] \n        if last_text : \n            yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos \n            pos = pos + ( len ( last_text ) ) \n            last_text . clear ( ) \n        yield TOKEN_NEWLINE , '\\n' , lineno , pos \n    template_file . close ( ) "}
{"10651": "\ndef fetch_config ( zone , conn ) : \n    more_to_fetch = True \n    cfg_chunks = [ ] \n    next_name = None \n    next_type = None \n    next_identifier = None \n    while more_to_fetch == True : \n        more_to_fetch = False \n        getstr = '/%s/hostedzone/%s/rrset' % ( R53_API_VERSION , zone ) \n        if next_name is not None : \n            getstr = getstr + ( '?name=%s&type=%s' % ( next_name , next_type ) ) \n            if next_identifier is not None : \n                getstr = getstr + ( '&identifier=%s' % next_identifier ) \n        log . debug ( 'requesting %s' % getstr ) \n        resp = conn . make_request ( 'GET' , getstr ) \n        etree = lxml . etree . parse ( resp ) \n        cfg_chunks . append ( etree ) \n        root = etree . getroot ( ) \n        truncated = root . find ( '{%s}IsTruncated' % R53_XMLNS ) \n        if truncated is not None and truncated . text == 'true' : \n            more_to_fetch = True \n            next_name = root . find ( '{%s}NextRecordName' % R53_XMLNS ) . text \n            next_type = root . find ( '{%s}NextRecordType' % R53_XMLNS ) . text \n            try : \n                next_identifier = root . find ( '{%s}NextRecordIdentifier' % R53_XMLNS ) . text \n            except AttributeError : \n                next_identifier = None \n    return cfg_chunks "}
{"10653": "\ndef validate_changeset ( changeset ) : \n    errors = [ ] \n    changes = changeset . findall ( './/{%s}Change' % R53_XMLNS ) \n    num_changes = len ( changes ) \n    if num_changes == 0 : \n        errors . append ( 'changeset must have at least one <Change> element' ) \n    if num_changes > 100 : \n        errors . append ( 'changeset has %d <Change> elements: max is 100' % num_changes ) \n    rrs = changeset . findall ( './/{%s}ResourceRecord' % R53_XMLNS ) \n    num_rrs = len ( rrs ) \n    if num_rrs > 1000 : \n        errors . append ( 'changeset has %d ResourceRecord elements: max is 1000' % num_rrs ) \n    values = changeset . findall ( './/{%s}Value' % R53_XMLNS ) \n    num_chars = 0 \n    for value in values : \n        num_chars = num_chars + ( len ( value . text ) ) \n    if num_chars > 10000 : \n        errors . append ( 'changeset has %d chars in <Value> text: max is 10000' % num_chars ) \n    return errors "}
{"10694": "\ndef get_annotated_lines ( self ) : \n    lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] \n    if hasattr ( self . code , 'co_firstlineno' ) : \n        lineno = self . code . co_firstlineno - 1 \n        while lineno > 0 : \n            if _funcdef_re . match ( lines [ lineno ] . code ) : \n                break \n            lineno = lineno - ( 1 ) \n        try : \n            offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) \n        except TokenError : \n            offset = 0 \n        for line in lines [ lineno : lineno + offset ] : \n            line . in_frame = True \n    try : \n        lines [ self . lineno - 1 ] . current = True \n    except IndexError : \n        pass \n    return lines "}
{"10714": "\ndef push ( self ) : \n    self . _refcnt = self . _refcnt + ( 1 ) \n    _app_ctx_stack . push ( self ) \n    appcontext_pushed . send ( self . app ) "}
{"10715": "\ndef pop ( self , exc = None ) : \n    self . _refcnt = self . _refcnt - ( 1 ) \n    if self . _refcnt <= 0 : \n        if exc is None : \n            exc = sys . exc_info ( ) [ 1 ] \n        self . app . do_teardown_appcontext ( exc ) \n    rv = _app_ctx_stack . pop ( ) \n    assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) \n    appcontext_popped . send ( self . app ) "}
{"10726": "\ndef url_for ( endpoint , ** values ) : \n    appctx = _app_ctx_stack . top \n    reqctx = _request_ctx_stack . top \n    if appctx is None : \n        raise RuntimeError ( 'Attempted to generate a URL without the ' 'application context being pushed. This has to be ' 'executed when application context is available.' ) \n    if reqctx is not None : \n        url_adapter = reqctx . url_adapter \n        blueprint_name = request . blueprint \n        if not reqctx . request . _is_old_module : \n            if endpoint [ : 1 ] == '.' : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + endpoint \n                else : \n                    endpoint = endpoint [ 1 : ] \n        else : \n            if '.' not in endpoint : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + '.' + endpoint \n            elif endpoint . startswith ( '.' ) : \n                endpoint = endpoint [ 1 : ] \n        external = values . pop ( '_external' , False ) \n    else : \n        url_adapter = appctx . url_adapter \n        if url_adapter is None : \n            raise RuntimeError ( 'Application was not able to create a URL ' 'adapter for request independent URL generation. ' 'You might be able to fix this by setting ' 'the SERVER_NAME config variable.' ) \n        external = values . pop ( '_external' , True ) \n    anchor = values . pop ( '_anchor' , None ) \n    method = values . pop ( '_method' , None ) \n    scheme = values . pop ( '_scheme' , None ) \n    appctx . app . inject_url_defaults ( endpoint , values ) \n    if scheme is not None : \n        if not external : \n            raise ValueError ( 'When specifying _scheme, _external must be True' ) \n        url_adapter . url_scheme = scheme \n    try : \n        rv = url_adapter . build ( endpoint , values , method = method , force_external = external ) \n    except BuildError as error : \n        values [ '_external' ] = external \n        values [ '_anchor' ] = anchor \n        values [ '_method' ] = method \n        return appctx . app . handle_url_build_error ( error , endpoint , values ) \n    if anchor is not None : \n        rv = rv + ( '#' + url_quote ( anchor ) ) \n    return rv "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and cc [ 'max-age' ] == 0 : \n        no_cache = True \n    if no_cache : \n        return False \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return False \n    if resp . status == 301 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return False \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age = current_age + ( min_fresh ) \n    fresh = ( freshness_lifetime > current_age ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return False "}
{"10761": "\ndef compile_templates ( self , target , extensions = None , filter_func = None , zip = 'deflated' , log_function = None , ignore_errors = True , py_compile = False ) : \n    from jinja2 . loaders import ModuleLoader \n    if log_function is None : \n        log_function = lambda x : None \n    if py_compile : \n        if not PY2 or PYPY : \n            from warnings import warn \n            warn ( Warning ( 'py_compile has no effect on pypy or Python 3' ) ) \n            py_compile = False \n        else : \n            import imp , marshal \n            py_header = imp . get_magic ( ) + u'\\xff\\xff\\xff\\xff' . encode ( 'iso-8859-15' ) \n            if sys . version_info >= ( 3 , 3 ) : \n                py_header = py_header + ( u'\\x00\\x00\\x00\\x00' . encode ( 'iso-8859-15' ) ) \n    def write_file ( filename , data , mode ) : \n        if zip : \n            info = ZipInfo ( filename ) \n            info . external_attr = 0o755 << 16 \n            zip_file . writestr ( info , data ) \n        else : \n            f = open ( os . path . join ( target , filename ) , mode ) \n            try : \n                f . write ( data ) \n            finally : \n                f . close ( ) \n    if zip is not None : \n        from zipfile import ZipFile , ZipInfo , ZIP_DEFLATED , ZIP_STORED \n        zip_file = ZipFile ( target , 'w' , dict ( deflated = ZIP_DEFLATED , stored = ZIP_STORED ) [ zip ] ) \n        log_function ( 'Compiling into Zip archive \"%s\"' % target ) \n    else : \n        if not os . path . isdir ( target ) : \n            os . makedirs ( target ) \n        log_function ( 'Compiling into folder \"%s\"' % target ) \n    try : \n        for name in self . list_templates ( extensions , filter_func ) : \n            source , filename , _ = self . loader . get_source ( self , name ) \n            try : \n                code = self . compile ( source , name , filename , True , True ) \n            except TemplateSyntaxError as e : \n                if not ignore_errors : \n                    raise \n                log_function ( 'Could not compile \"%s\": %s' % ( name , e ) ) \n                continue \n            filename = ModuleLoader . get_module_filename ( name ) \n            if py_compile : \n                c = self . _compile ( code , encode_filename ( filename ) ) \n                write_file ( filename + 'c' , py_header + marshal . dumps ( c ) , 'wb' ) \n                log_function ( 'Byte-compiled \"%s\" as %s' % ( name , filename + 'c' ) ) \n            else : \n                write_file ( filename , code , 'w' ) \n                log_function ( 'Compiled \"%s\" as %s' % ( name , filename ) ) \n    finally : \n        if zip : \n            zip_file . close ( ) \n    log_function ( 'Finished compiling templates' ) "}
{"10776": "\ndef parse_pattern ( pattern ) : \n    if isinstance ( pattern , NumberPattern ) : \n        return pattern \n    def _match_number ( pattern ) : \n        rv = number_re . search ( pattern ) \n        if rv is None : \n            raise ValueError ( 'Invalid number pattern %r' % pattern ) \n        return rv . groups ( ) \n    pos_pattern = pattern \n    if ';' in pattern : \n        pos_pattern , neg_pattern = pattern . split ( ';' , 1 ) \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix , _ , neg_suffix = _match_number ( neg_pattern ) \n    else : \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix = '-' + pos_prefix \n        neg_suffix = pos_suffix \n    if 'E' in number : \n        number , exp = number . split ( 'E' , 1 ) \n    else : \n        exp = None \n    if '@' in number : \n        if '.' in number and '0' in number : \n            raise ValueError ( 'Significant digit patterns can not contain ' '\"@\" or \"0\"' ) \n    if '.' in number : \n        integer , fraction = number . rsplit ( '.' , 1 ) \n    else : \n        integer = number \n        fraction = '' \n    def parse_precision ( p ) : \n        min = max = 0 \n        for c in p : \n            if c in '@0' : \n                min = min + ( 1 ) \n                max = max + ( 1 ) \n            elif c == '#' : \n                max = max + ( 1 ) \n            elif c == ',' : \n                continue \n            else : \n                break \n        return min , max \n    int_prec = parse_precision ( integer ) \n    frac_prec = parse_precision ( fraction ) \n    if exp : \n        exp_plus = exp . startswith ( '+' ) \n        exp = exp . lstrip ( '+' ) \n        exp_prec = parse_precision ( exp ) \n    else : \n        exp_plus = None \n        exp_prec = None \n    grouping = babel . numbers . parse_grouping ( integer ) \n    return NumberPattern ( pattern , ( pos_prefix , neg_prefix ) , ( pos_suffix , neg_suffix ) , grouping , int_prec , frac_prec , exp_prec , exp_plus ) "}
{"10805": "\ndef markup_serialize_tokens ( tokens , markup_func ) : \n    for token in tokens : \n        for pre in token . pre_tags : \n            yield pre \n        html = token . html ( ) \n        html = markup_func ( html , token . annotation ) \n        if token . trailing_whitespace : \n            html = html + ( token . trailing_whitespace ) \n        yield html \n        for post in token . post_tags : \n            yield post "}
{"10840": "\ndef pkginfo_to_metadata ( egg_info_path , pkginfo_path ) : \n    pkg_info = read_pkg_info ( pkginfo_path ) \n    pkg_info . replace_header ( 'Metadata-Version' , '2.0' ) \n    requires_path = os . path . join ( egg_info_path , 'requires.txt' ) \n    if os . path . exists ( requires_path ) : \n        requires = open ( requires_path ) . read ( ) \n        for extra , reqs in pkg_resources . split_sections ( requires ) : \n            condition = '' \n            if extra and ':' in extra : \n                extra , condition = extra . split ( ':' , 1 ) \n            if extra : \n                pkg_info [ 'Provides-Extra' ] = extra \n                if condition : \n                    condition = condition + ( \" and \" ) \n                condition = condition + ( 'extra == %s' % repr ( extra ) ) \n            if condition : \n                condition = '; ' + condition \n            for new_req in convert_requirements ( reqs ) : \n                pkg_info [ 'Requires-Dist' ] = new_req + condition \n    description = pkg_info [ 'Description' ] \n    if description : \n        pkg_info . set_payload ( dedent_description ( pkg_info ) ) \n        del pkg_info [ 'Description' ] \n    return pkg_info "}
{"10874": "\ndef convertArgsToTokens ( self , data ) : \n    tdict = [ ] \n    tokens = [ ] \n    d = open ( data , 'r' ) \n    for line in d . readlines ( ) : \n        tdict . append ( line . rstrip ( ) ) \n        tokens = tokens + ( line . split ( ) ) \n    d . close ( ) \n    tokens = list ( set ( tokens ) ) \n    return tdict , tokens "}
{"10883": "\ndef local_open ( url ) : \n    scheme , server , path , param , query , frag = urlparse ( url ) \n    filename = url2pathname ( path ) \n    if os . path . isfile ( filename ) : \n        return urllib2 . urlopen ( url ) \n    elif path . endswith ( '/' ) and os . path . isdir ( filename ) : \n        files = [ ] \n        for f in os . listdir ( filename ) : \n            if f == 'index.html' : \n                with open ( os . path . join ( filename , f ) , 'r' ) as fp : \n                    body = fp . read ( ) \n                break \n            elif os . path . isdir ( os . path . join ( filename , f ) ) : \n                f = f + ( '/' ) \n            files . append ( \"<a href=%r>%s</a>\" % ( f , f ) ) \n        else : \n            body = ( \"<html><head><title>%s</title>\" % url ) + \"</head><body>%s</body></html>\" % '\\n' . join ( files ) \n        status , message = 200 , \"OK\" \n    else : \n        status , message , body = 404 , \"Path not found\" , \"Not found\" \n    headers = { 'content-type' : 'text/html' } \n    return HTTPError ( url , status , message , headers , StringIO ( body ) ) "}
{"10904": "\ndef _download_http_url ( link , session , temp_dir ) : \n    target_url = link . url . split ( '#' , 1 ) [ 0 ] \n    try : \n        resp = session . get ( target_url , headers = { \"Accept-Encoding\" : \"identity\" } , stream = True , ) \n        resp . raise_for_status ( ) \n    except requests . HTTPError as exc : \n        logger . critical ( \"HTTP error %s while getting %s\" , exc . response . status_code , link , ) \n        raise \n    content_type = resp . headers . get ( 'content-type' , '' ) \n    filename = link . filename \n    content_disposition = resp . headers . get ( 'content-disposition' ) \n    if content_disposition : \n        type , params = cgi . parse_header ( content_disposition ) \n        filename = params . get ( 'filename' ) or filename \n    ext = splitext ( filename ) [ 1 ] \n    if not ext : \n        ext = mimetypes . guess_extension ( content_type ) \n        if ext : \n            filename = filename + ( ext ) \n    if not ext and link . url != resp . url : \n        ext = os . path . splitext ( resp . url ) [ 1 ] \n        if ext : \n            filename = filename + ( ext ) \n    file_path = os . path . join ( temp_dir , filename ) \n    with open ( file_path , 'wb' ) as content_file : \n        _download_url ( resp , link , content_file ) \n    return file_path , content_type "}
{"10908": "\ndef _decode ( self , data , decode_content , flush_decoder ) : \n    try : \n        if decode_content and self . _decoder : \n            data = self . _decoder . decompress ( data ) \n    except ( IOError , zlib . error ) as e : \n        content_encoding = self . headers . get ( 'content-encoding' , '' ) . lower ( ) \n        raise DecodeError ( \"Received response with content-encoding: %s, but \" \"failed to decode it.\" % content_encoding , e ) \n    if flush_decoder and decode_content and self . _decoder : \n        buf = self . _decoder . decompress ( binary_type ( ) ) \n        data = data + ( buf + self . _decoder . flush ( ) ) \n    return data "}
{"10926": "\ndef iterate_pages ( self ) : \n    try : \n        while True : \n            yield self . _query ( ItemPage = self . current_page , ** self . kwargs ) \n            self . current_page = self . current_page + ( 1 ) \n    except NoMorePages : \n        pass "}
{"11015": "\ndef build_chain ( self , source , chain ) : \n    for group in WalkByGroup ( source , chain . order + 1 ) : \n        pre = group [ : - 1 ] \n        res = group [ - 1 ] \n        if pre not in chain . content : \n            chain . content [ pre ] = { res : 1 } \n        else : \n            if res not in chain . content [ pre ] : \n                chain . content [ pre ] [ res ] = 1 \n            else : \n                chain . content [ pre ] [ res ] = chain . content [ pre ] [ res ] + ( 1 ) \n    chain . decache ( ) "}
{"11016": "\ndef generate_sentence ( self , chain ) : \n    def weighted_choice ( choices ) : \n        total_weight = sum ( weight for val , weight in choices ) \n        rand = random . uniform ( 0 , total_weight ) \n        upto = 0 \n        for val , weight in choices : \n            if upto + weight >= rand : \n                return val \n            upto = upto + ( weight ) \n    sentence = list ( random . choice ( chain . startwords ) ) \n    while not sentence [ - 1 ] [ - 1 ] in [ '.' , '?' , '!' ] : \n        sentence . append ( weighted_choice ( chain . content [ tuple ( sentence [ - 2 : ] ) ] . items ( ) ) ) \n    return ' ' . join ( sentence ) "}
{"11031": "\ndef format_duration ( seconds ) : \n    units , divider = get_time_units_and_multiplier ( seconds ) \n    seconds = seconds * ( divider ) \n    return \"%.3f %s\" % ( seconds , units ) "}
{"11073": "\ndef render_prev_next_links ( self , scheme = None ) : \n    output = '' \n    if self . has_prev : \n        output = output + ( '<link rel=\"prev\" href=\"{}\" />\\n' . format ( self . get_full_page_url ( self . prev , scheme = scheme ) ) ) \n    if self . has_next : \n        output = output + ( '<link rel=\"next\" href=\"{}\" />\\n' . format ( self . get_full_page_url ( self . next , scheme = scheme ) ) ) \n    return Markup ( output ) "}
{"11074": "\ndef render_seo_links ( self , scheme = None ) : \n    out = self . render_prev_next_links ( scheme = scheme ) \n    if self . total_pages == 1 : \n        out = out + ( self . render_canonical_link ( scheme = scheme ) ) \n    return out "}
{"11076": "\ndef select_content_type ( requested , available ) : \n    class Match ( object ) : \n        WILDCARD , PARTIAL , FULL_TYPE , = 2 , 1 , 0 \n        def __init__ ( self , candidate , pattern ) : \n            self . candidate = candidate \n            self . pattern = pattern \n            if pattern . content_type == pattern . content_subtype == '*' : \n                self . match_type = self . WILDCARD \n            elif pattern . content_subtype == '*' : \n                self . match_type = self . PARTIAL \n            else : \n                self . match_type = self . FULL_TYPE \n            self . parameter_distance = len ( self . candidate . parameters ) \n            for key , value in candidate . parameters . items ( ) : \n                if key in pattern . parameters : \n                    if pattern . parameters [ key ] == value : \n                        self . parameter_distance = self . parameter_distance - ( 1 ) \n                    else : \n                        self . parameter_distance = self . parameter_distance + ( 1 ) \n    def extract_quality ( obj ) : \n        return getattr ( obj , 'quality' , 1.0 ) \n    matches = [ ] \n    for pattern in sorted ( requested , key = extract_quality , reverse = True ) : \n        for candidate in sorted ( available ) : \n            if _content_type_matches ( candidate , pattern ) : \n                if candidate == pattern : \n                    if extract_quality ( pattern ) == 0.0 : \n                        raise errors . NoMatch \n                    return candidate , pattern \n                matches . append ( Match ( candidate , pattern ) ) \n    if not matches : \n        raise errors . NoMatch \n    matches = sorted ( matches , key = attrgetter ( 'match_type' , 'parameter_distance' ) ) \n    return matches [ 0 ] . candidate , matches [ 0 ] . pattern "}
{"11088": "\ndef luhn_check ( card_number ) : \n    sum = 0 \n    num_digits = len ( card_number ) \n    oddeven = num_digits & 1 \n    for count in range ( 0 , num_digits ) : \n        digit = int ( card_number [ count ] ) \n        if not ( ( count & 1 ) ^ oddeven ) : \n            digit = digit * ( 2 ) \n        if digit > 9 : \n            digit = digit - ( 9 ) \n        sum = sum + ( digit ) \n    return ( sum % 10 ) == 0 "}
{"11093": "\ndef split_line ( line , min_line_length = 30 , max_line_length = 100 ) : \n    if len ( line ) <= max_line_length : \n        return [ line ] \n    indent = 0 \n    while line [ indent ] == ' ' and indent < len ( line ) : \n        indent = indent + ( 1 ) \n    i = max_line_length \n    split_point = None \n    while i > min_line_length : \n        if line [ i ] == ' ' : \n            split_point = i \n            break \n        i = i - ( 1 ) \n    if split_point is None : \n        i = max_line_length + 1 \n        while i < len ( line ) : \n            if line [ i ] == ' ' : \n                split_point = i \n                break \n            i = i + ( 1 ) \n    if split_point is None : \n        return [ line ] \n    else : \n        line1 = line [ : split_point ] \n        line2 = ' ' * indent + line [ split_point + 1 : ] \n        return [ line1 ] + split_line ( line2 , min_line_length , max_line_length ) "}
{"11095": "\ndef consistency ( self , desired_version = None , include_package = False , strictness = None ) : \n    keys_to_check = list ( self . versions . keys ( ) ) \n    if not include_package and 'package' in keys_to_check : \n        keys_to_check . remove ( 'package' ) \n    if desired_version is None : \n        try : \n            desired_version = self . versions [ 'setup.py' ] \n        except KeyError : \n            desired_version = self . versions [ keys_to_check [ 0 ] ] \n    if strictness is None : \n        strictness = self . strictness \n    desired = self . _version ( desired_version , strictness ) \n    error_keys = [ ] \n    for key in keys_to_check : \n        test = self . _version ( self . versions [ key ] , strictness ) \n        if test != desired : \n            error_keys = error_keys + ( [ key ] ) \n    msg = \"\" \n    for key in error_keys : \n        msg = msg + ( \"Error: desired {d} != {v} ({k})\\n\" . format ( d = str ( desired ) , v = str ( self . versions [ key ] ) , k = str ( key ) ) ) \n    return msg "}
{"11103": "\ndef log_attempt ( self , key ) : \n    with self . lock : \n        if key not in self . attempts : \n            self . attempts [ key ] = 1 \n        else : \n            self . attempts [ key ] = self . attempts [ key ] + ( 1 ) \n            if self . attempts [ key ] >= self . max_attempts : \n                log . info ( 'Account %s locked due to too many login attempts' % key ) \n                self . locks [ key ] = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = self . lock_duration ) "}
{"11112": "\ndef add_months ( months , timestamp = datetime . datetime . utcnow ( ) ) : \n    month = timestamp . month \n    new_month = month + months \n    years = 0 \n    while new_month < 1 : \n        new_month = new_month + ( 12 ) \n        years = years - ( 1 ) \n    while new_month > 12 : \n        new_month = new_month - ( 12 ) \n        years = years + ( 1 ) \n    year = timestamp . year + years \n    try : \n        return datetime . datetime ( year , new_month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) \n    except ValueError : \n        if months > 0 : \n            new_month = new_month + ( 1 ) \n            if new_month > 12 : \n                new_month = new_month - ( 12 ) \n                year = year + ( 1 ) \n            return datetime . datetime ( year , new_month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day , timestamp . hour , timestamp . minute , timestamp . second ) "}
{"11113": "\ndef add_months_to_date ( months , date ) : \n    month = date . month \n    new_month = month + months \n    years = 0 \n    while new_month < 1 : \n        new_month = new_month + ( 12 ) \n        years = years - ( 1 ) \n    while new_month > 12 : \n        new_month = new_month - ( 12 ) \n        years = years + ( 1 ) \n    year = date . year + years \n    try : \n        return datetime . date ( year , new_month , date . day ) \n    except ValueError : \n        if months > 0 : \n            new_month = new_month + ( 1 ) \n            if new_month > 12 : \n                new_month = new_month - ( 12 ) \n                year = year + ( 1 ) \n            return datetime . datetime ( year , new_month , 1 ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day ) "}
{"11181": "\ndef vlq2int ( data ) : \n    byte = ord ( data . read ( 1 ) ) \n    value = byte & 0x7F \n    shift = 1 \n    while byte & 0x80 != 0 : \n        byte = ord ( data . read ( 1 ) ) \n        value = ( ( byte & 0x7F ) << shift * 7 ) | value \n        shift = shift + ( 1 ) \n    return value "}
{"11183": "\ndef _parse_header ( self ) : \n    header = OrderedDict ( ) \n    user_data_header = self . archive . header [ 'user_data_header' ] [ 'content' ] \n    if re . search ( r'StarCraft II replay' , user_data_header ) : \n        user_data_header = StringIO . StringIO ( user_data_header ) \n        user_data_header . seek ( 30 ) \n        header . update ( read_table ( user_data_header , [ 'release_flag' , 'major_version' , 'minor_version' , 'maintenance_version' , 'build_number' , 'unknown' , 'unknown' , 'duration' ] ) ) \n        header [ 'version' ] = '%s.%s.%s.%s' % ( header [ 'major_version' ] , header [ 'minor_version' ] , header [ 'maintenance_version' ] , header [ 'build_number' ] ) \n        if not header [ 'release_flag' ] : \n            header [ 'version' ] = header [ 'version' ] + ( ' (dev)' ) \n        header [ 'duration' ] = header [ 'duration' ] / ( 16 ) \n    else : \n        raise ValueError ( \"The given file is not a StarCraft II replay.\" ) \n    return header "}
{"11184": "\ndef get_duration ( self , seconds ) : \n    duration = \"\" \n    minutes , seconds = divmod ( seconds , 60 ) \n    if minutes >= 60 : \n        hours , minutes = divmod ( minutes , 60 ) \n        duration = \"%sh \" % hours \n    duration = duration + ( \"%sm %ss\" % ( minutes , seconds ) ) \n    return duration "}
{"11189": "\ndef data_processing ( self ) : \n    the_file_name = str ( self . result_file ) \n    the_file = open ( the_file_name , 'r' ) \n    lines = the_file . readlines ( ) \n    lines_array = [ ] \n    for line in lines : \n        line = line . split ( ',' ) \n        lines_array . append ( line ) \n    labels_line = lines_array [ 0 ] \n    cell_labels_line = 0 \n    flag = True \n    try : \n        while flag : \n            if \"wave length (nm)\" in labels_line [ cell_labels_line ] : \n                index = labels_line . index ( labels_line [ cell_labels_line ] ) \n                flag = False \n            else : \n                cell_labels_line = cell_labels_line + ( 1 ) \n    except IndexError : \n        raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) \n    self . information = [ ] \n    data_wavelength = [ ] \n    self . num_line = 0 \n    for line in lines_array : \n        cell_line = 0 \n        self . information . append ( [ ] ) \n        data_wavelength . append ( [ ] ) \n        while cell_line < len ( line ) : \n            if cell_line < index : \n                self . information [ self . num_line ] . append ( line [ cell_line ] ) \n            elif cell_line > index : \n                data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) \n            cell_line = cell_line + ( 1 ) \n        self . num_line = self . num_line + ( 1 ) \n    line_wavelength = 0 \n    for row_data_wavelength in data_wavelength : \n        row_data_wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row_data_wavelength ] \n        data_wavelength [ line_wavelength ] = row_data_wavelength \n        line_wavelength = line_wavelength + ( 1 ) \n    self . wavelength = data_wavelength [ 0 ] \n    self . data_wanted = data_wavelength [ 1 : ] \n    the_file . close ( ) "}
{"11191": "\ndef print_graphic_information ( self , num_curve , information ) : \n    label_information = information [ 0 ] \n    data_information = information [ 1 : ] \n    count_nb_label = 0 \n    nb_label = len ( label_information ) \n    while count_nb_label <= nb_label : \n        self . ui . column1_label . setText ( label_information [ 0 ] . strip ( '\\\"' ) ) \n        self . ui . column2_label . setText ( label_information [ 1 ] . strip ( '\\\"' ) ) \n        self . ui . column3_label . setText ( label_information [ 2 ] . strip ( '\\\"' ) ) \n        self . ui . column4_label . setText ( label_information [ 3 ] . strip ( '\\\"' ) ) \n        self . ui . column5_label . setText ( label_information [ 4 ] . strip ( '\\\"' ) ) \n        self . ui . column6_label . setText ( label_information [ 5 ] . strip ( '\\\"' ) ) \n        self . ui . column7_label . setText ( label_information [ 6 ] . strip ( '\\\"' ) ) \n        self . ui . column8_label . setText ( label_information [ 7 ] . strip ( '\\\"' ) ) \n        count_nb_label = count_nb_label + ( 1 ) \n    line_of_data = 0 \n    while line_of_data < len ( data_information ) : \n        if line_of_data == num_curve : \n            self . ui . column1_result . setText ( data_information [ line_of_data ] [ 0 ] ) \n            self . ui . column2_result . setText ( data_information [ line_of_data ] [ 1 ] ) \n            self . ui . column3_result . setText ( data_information [ line_of_data ] [ 2 ] ) \n            self . ui . column4_result . setText ( data_information [ line_of_data ] [ 3 ] ) \n            self . ui . column5_result . setText ( data_information [ line_of_data ] [ 4 ] ) \n            self . ui . column6_result . setText ( data_information [ line_of_data ] [ 5 ] ) \n            self . ui . column7_result . setText ( data_information [ line_of_data ] [ 6 ] ) \n            self . ui . column8_result . setText ( data_information [ line_of_data ] [ 7 ] ) \n        line_of_data = line_of_data + ( 1 ) "}
{"11208": "\ndef issue_funds ( ctx , amount = 'uint256' , rtgs_hash = 'bytes32' , returns = STATUS ) : \n    ctx . accounts [ ctx . msg_sender ] = ctx . accounts [ ctx . msg_sender ] + ( amount ) \n    ctx . issued_amounts [ ctx . msg_sender ] = ctx . issued_amounts [ ctx . msg_sender ] + ( amount ) \n    ctx . Issuance ( ctx . msg_sender , rtgs_hash , amount ) \n    return OK "}
{"11214": "\ndef mk_privkeys ( num ) : \n    privkeys = [ ] \n    assert num <= num_colors \n    for i in range ( num ) : \n        j = 0 \n        while True : \n            k = sha3 ( str ( j ) ) \n            a = privtoaddr ( k ) \n            an = big_endian_to_int ( a ) \n            if an % num_colors == i : \n                break \n            j = j + ( 1 ) \n        privkeys . append ( k ) \n    return privkeys "}
{"11215": "\ndef delay ( self , sender , receiver , packet , add_delay = 0 ) : \n    bw = min ( sender . ul_bandwidth , receiver . dl_bandwidth ) \n    delay = sender . base_latency + receiver . base_latency \n    delay = delay + ( len ( packet ) / bw ) \n    delay = delay + ( add_delay ) \n    return delay "}
{"11222": "\ndef img_from_vgg ( x ) : \n    x = x . transpose ( ( 1 , 2 , 0 ) ) \n    x [ : , : , 0 ] = x [ : , : , 0 ] + ( 103.939 ) \n    x [ : , : , 1 ] = x [ : , : , 1 ] + ( 116.779 ) \n    x [ : , : , 2 ] = x [ : , : , 2 ] + ( 123.68 ) \n    x = x [ : , : , : : - 1 ] \n    return x "}
{"11223": "\ndef img_to_vgg ( x ) : \n    x = x [ : , : , : : - 1 ] \n    x [ : , : , 0 ] = x [ : , : , 0 ] - ( 103.939 ) \n    x [ : , : , 1 ] = x [ : , : , 1 ] - ( 116.779 ) \n    x [ : , : , 2 ] = x [ : , : , 2 ] - ( 123.68 ) \n    x = x . transpose ( ( 2 , 0 , 1 ) ) \n    return x "}
{"11230": "\ndef put ( self , items , panic = True ) : \n    if not self . initiated : \n        self . _initiate ( ) \n    try : \n        row_status = self . mload . put_row ( self . preprocessor ( items ) ) \n        self . applied_count = self . applied_count + ( 1 ) \n    except ( TeradataPTError , EncoderError ) as error : \n        self . error_count = self . error_count + ( 1 ) \n        if panic : \n            raise error \n        log . info ( \"BulkLoad\" , error ) "}
{"11244": "\ndef float_with_multiplier ( string ) : \n    match = re_float_with_multiplier . search ( string ) \n    if not match or not match . group ( 'num' ) : \n        raise ValueError ( 'String \"{}\" is not numeric!' . format ( string ) ) \n    num = float ( match . group ( 'num' ) ) \n    multi = match . group ( 'multi' ) \n    if multi : \n        try : \n            num = num * ( multipliers [ multi ] ) \n        except KeyError : \n            raise ValueError ( 'Unknown multiplier: {}' . format ( multi ) ) \n    return num "}
{"11252": "\ndef update ( self , psd_state , samples_array ) : \n    freq_array , pwr_array = simplespectral . welch ( samples_array , self . _sample_rate , nperseg = self . _bins , window = self . _fft_window , noverlap = self . _fft_overlap_bins , detrend = self . _detrend ) \n    if self . _remove_dc : \n        pwr_array [ 0 ] = ( pwr_array [ 1 ] + pwr_array [ - 1 ] ) / 2 \n    with psd_state [ 'update_lock' ] : \n        psd_state [ 'repeats' ] = psd_state [ 'repeats' ] + ( 1 ) \n        if psd_state [ 'pwr_array' ] is None : \n            psd_state [ 'pwr_array' ] = pwr_array \n        else : \n            psd_state [ 'pwr_array' ] = psd_state [ 'pwr_array' ] + ( pwr_array ) "}
{"11262": "\ndef sweep ( self , min_freq , max_freq , bins , repeats , runs = 0 , time_limit = 0 , overlap = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop = False , log_scale = True , remove_dc = False , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = False , base_buffer_size = 0 , max_buffer_size = 0 , max_threads = 0 , max_queue_size = 0 ) : \n    self . setup ( bins , repeats , base_buffer_size , max_buffer_size , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = overlap if crop else 0 , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , tune_delay = tune_delay , reset_stream = reset_stream , max_threads = max_threads , max_queue_size = max_queue_size ) \n    try : \n        freq_list = self . freq_plan ( min_freq - lnb_lo , max_freq - lnb_lo , bins , overlap ) \n        t_start = time . time ( ) \n        run = 0 \n        while not _shutdown and ( runs == 0 or run < runs ) : \n            run = run + ( 1 ) \n            t_run_start = time . time ( ) \n            logger . debug ( 'Run: {}' . format ( run ) ) \n            for freq in freq_list : \n                psd_future , acq_time_start , acq_time_stop = self . psd ( freq ) \n                self . _writer . write_async ( psd_future , acq_time_start , acq_time_stop , len ( self . _buffer ) * self . _buffer_repeats ) \n                if _shutdown : \n                    break \n            write_next_future = self . _writer . write_next_async ( ) \n            t_run = time . time ( ) \n            logger . debug ( '  Total run time: {:.3f} s' . format ( t_run - t_run_start ) ) \n            if time_limit and ( time . time ( ) - t_start ) >= time_limit : \n                logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time_limit , run ) ) \n                break \n        write_next_future . result ( ) \n        logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer_overflow_count ) ) \n        logging . debug ( 'PSD worker threads: {}' . format ( self . _psd . _executor . _max_workers ) ) \n        logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . _psd . _executor . max_queue_size_reached , self . _psd . _executor . max_queue_size ) ) \n        logging . debug ( 'Writer worker threads: {}' . format ( self . _writer . _executor . _max_workers ) ) \n        logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . _writer . _executor . max_queue_size_reached , self . _writer . _executor . max_queue_size ) ) \n    finally : \n        self . stop ( ) \n        t_stop = time . time ( ) \n        logger . info ( 'Total time: {:.3f} s' . format ( t_stop - t_start ) ) "}
{"11275": "\ndef get_bit_num ( bit_pattern ) : \n    if bit_pattern == 0 : \n        return None \n    bit_num = 0 \n    while ( bit_pattern & 1 ) == 0 : \n        bit_pattern = bit_pattern >> 1 \n        bit_num = bit_num + ( 1 ) \n        if bit_num > 7 : \n            bit_num = 0 \n            break \n    return bit_num "}
{"11285": "\ndef render ( self , form , form_style , context , template_pack = TEMPLATE_PACK ) : \n    links , content = '' , '' \n    if not self . css_id : \n        self . css_id = \"-\" . join ( [ \"tabsholder\" , text_type ( randint ( 1000 , 9999 ) ) ] ) \n    for tab in self . fields : \n        tab . active = False \n    self . open_target_group_for_form ( form ) \n    for tab in self . fields : \n        content = content + ( render_field ( tab , form , form_style , context , template_pack = template_pack ) ) \n        links = links + ( tab . render_link ( form , template_pack ) ) \n    context . update ( { 'tabs' : self , 'links' : links , 'content' : content } ) \n    template = self . get_template_name ( template_pack ) \n    return render_to_string ( template , context . flatten ( ) ) "}
{"11308": "\ndef _dmpaft_cmd ( self , time_fields ) : \n    records = [ ] \n    tbuf = struct . pack ( '2H' , * time_fields ) \n    self . _cmd ( 'DMPAFT' ) \n    crc = VProCRC . get ( tbuf ) \n    crc = struct . pack ( '>H' , crc ) \n    log_raw ( 'send' , tbuf + crc ) \n    self . port . write ( tbuf + crc ) \n    ack = self . port . read ( len ( self . ACK ) ) \n    log_raw ( 'read' , ack ) \n    if ack != self . ACK : \n        return \n    raw = self . port . read ( DmpStruct . size ) \n    log_raw ( 'read' , raw ) \n    if not VProCRC . verify ( raw ) : \n        log_raw ( 'send ESC' , self . ESC ) \n        self . port . write ( self . ESC ) \n        return \n    log_raw ( 'send ACK' , self . ACK ) \n    self . port . write ( self . ACK ) \n    dmp = DmpStruct . unpack ( raw ) \n    log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) \n    for i in xrange ( dmp [ 'Pages' ] ) : \n        raw = self . port . read ( DmpPageStruct . size ) \n        log_raw ( 'read' , raw ) \n        if not VProCRC . verify ( raw ) : \n            log_raw ( 'send ESC' , self . ESC ) \n            self . port . write ( self . ESC ) \n            return \n        log_raw ( 'send ACK' , self . ACK ) \n        self . port . write ( self . ACK ) \n        page = DmpPageStruct . unpack ( raw ) \n        offset = 0 \n        if i == 0 : \n            offset = dmp [ 'Offset' ] * ArchiveAStruct . size \n        while offset < ArchiveAStruct . size * 5 : \n            log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) \n            if self . _use_rev_b_archive ( page [ 'Records' ] , offset ) : \n                a = ArchiveBStruct . unpack_from ( page [ 'Records' ] , offset ) \n            else : \n                a = ArchiveAStruct . unpack_from ( page [ 'Records' ] , offset ) \n            if a [ 'DateStamp' ] != 0xffff and a [ 'TimeStamp' ] != 0xffff : \n                records . append ( a ) \n            offset = offset + ( ArchiveAStruct . size ) \n    log . info ( 'read all pages' ) \n    return records "}
{"11314": "\ndef get ( self , station , interval ) : \n    rec = station . fields [ 'Archive' ] \n    if rec : \n        threshold = station . fields [ 'WindSpeed10Min' ] + GUST_MPH_MIN \n        if rec [ 'WindHi' ] >= threshold : \n            self . value = ( rec [ 'WindHi' ] , rec [ 'WindHiDir' ] ) \n            self . count = GUST_TTL * 60 / interval \n        else : \n            self . value = self . NO_VALUE \n    if self . count : \n        self . count = self . count - ( 1 ) \n    else : \n        self . value = self . NO_VALUE \n    log . debug ( 'wind gust of {0} mph from {1}' . format ( * self . value ) ) \n    return self . value "}
{"11327": "\ndef unduplicate_field_names ( field_names ) : \n    res = [ ] \n    for k in field_names : \n        if k in res : \n            i = 1 \n            while k + '_' + str ( i ) in res : \n                i = i + ( 1 ) \n            k = k + ( '_' + str ( i ) ) \n        res . append ( k ) \n    return res "}
{"11392": "\ndef slicify ( slc , dim ) : \n    if isinstance ( slc , slice ) : \n        start = 0 if slc . start is None else slc . start \n        stop = dim if slc . stop is None else slc . stop \n        step = 1 if slc . step is None else slc . step \n        if start < 0 : \n            start = start + ( dim ) \n        if stop < 0 : \n            stop = stop + ( dim ) \n        if step > 0 : \n            if start < 0 : \n                start = 0 \n            if stop > dim : \n                stop = dim \n        else : \n            if stop < 0 : \n                stop = - 1 \n            if start > dim : \n                start = dim - 1 \n        return slice ( start , stop , step ) \n    elif isinstance ( slc , int ) : \n        if slc < 0 : \n            slc = slc + ( dim ) \n        return slice ( slc , slc + 1 , 1 ) \n    else : \n        raise ValueError ( \"Type for slice %s not recongized\" % type ( slc ) ) "}
{"11398": "\ndef wrapped ( f ) : \n    import inspect \n    def extract ( func ) : \n        append = \"\" \n        args = inspect . getargspec ( func ) \n        for i , a in enumerate ( args . args ) : \n            if i < ( len ( args ) - len ( args . defaults ) ) : \n                append = append + ( str ( a ) + \", \" ) \n            else : \n                default = args . defaults [ i - len ( args . defaults ) ] \n                if hasattr ( default , \"__name__\" ) : \n                    default = default . __name__ \n                else : \n                    default = str ( default ) \n                append = append + ( str ( a ) + \"=\" + default + \", \" ) \n        append = append [ : - 2 ] + \")\" \n        return append \n    doc = f . __doc__ + \"\\n\" \n    doc = doc + ( \"    local -> array(\" + extract ( getattr ( ConstructLocal , f . __name__ ) ) + \"\\n\" ) \n    doc = doc + ( \"    spark -> array(\" + extract ( getattr ( ConstructSpark , f . __name__ ) ) + \"\\n\" ) \n    f . __doc__ = doc \n    return f "}
{"11485": "\ndef seed ( self , values ) : \n    if not values : \n        seed_ids = [ int , str , random , self , values , self . __class__ ] \n        random . shuffle ( seed_ids ) \n        values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] \n    mash = Mash ( ) \n    self . c = 1 \n    self . s0 = mash ( ' ' ) \n    self . s1 = mash ( ' ' ) \n    self . s2 = mash ( ' ' ) \n    for val in values : \n        self . s0 = self . s0 - ( mash ( val ) ) \n        if self . s0 < 0 : \n            self . s0 = self . s0 + ( 1 ) \n        self . s1 = self . s1 - ( mash ( val ) ) \n        if self . s1 < 0 : \n            self . s1 = self . s1 + ( 1 ) \n        self . s2 = self . s2 - ( mash ( val ) ) \n        if self . s2 < 0 : \n            self . s2 = self . s2 + ( 1 ) "}
{"11540": "\ndef trending ( self , rating = None , limit = DEFAULT_SEARCH_LIMIT ) : \n    results_yielded = 0 \n    page , per_page = 0 , 25 \n    params = { 'rating' : rating } if rating else { } \n    fetch = partial ( self . _fetch , 'trending' , ** params ) \n    while True : \n        data = fetch ( offset = page , limit = per_page ) \n        page = page + ( per_page ) \n        if not data [ 'data' ] : \n            raise StopIteration \n        for item in data [ 'data' ] : \n            results_yielded = results_yielded + ( 1 ) \n            yield GiphyImage ( item ) \n            if limit is not None and results_yielded >= limit : \n                raise StopIteration \n        if ( page >= data [ 'pagination' ] [ 'total_count' ] or ( limit is not None and results_yielded >= limit ) ) : \n            raise StopIteration "}
{"11568": "\ndef newick ( self ) : \n    label = self . name or '' \n    if self . _length : \n        label = label + ( ':' + self . _length ) \n    descendants = ',' . join ( [ n . newick for n in self . descendants ] ) \n    if descendants : \n        descendants = '(' + descendants + ')' \n    return descendants + label "}
{"11589": "\ndef url_concat ( url , args ) : \n    if not args : \n        return url \n    if url [ - 1 ] not in ( '?' , '&' ) : \n        url = url + ( '&' if ( '?' in url ) else '?' ) \n    return url + urllib . urlencode ( args ) "}
{"11593": "\ndef parse_line ( self , line ) : \n    if line [ 0 ] . isspace ( ) : \n        new_part = ' ' + line . lstrip ( ) \n        self . _as_list [ self . _last_key ] [ - 1 ] = self . _as_list [ self . _last_key ] [ - 1 ] + ( new_part ) \n        dict . __setitem__ ( self , self . _last_key , self [ self . _last_key ] + new_part ) \n    else : \n        name , value = line . split ( \":\" , 1 ) \n        self . add ( name , value . strip ( ) ) "}
{"11607": "\ndef resize_pbc_for_lipids ( pbc , relL , relU , absL , absU , uparea , area , hole , proteins ) : \n    if any ( relL ) and any ( relU ) : \n        if 0 in ( pbc . x , pbc . y , pbc . z ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n    elif any ( absL ) or any ( absU ) : \n        if pbc . z == 0 : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n        if 0 in ( pbc . x , pbc . y ) : \n            pbc . x = pbc . y = 1 \n        upsize = sum ( absU ) * uparea \n        losize = sum ( absL ) * area \n        holesize = np . pi * hole ** 2 \n        xysize = pbc . x * pbc . y \n        psize_up = sum ( [ p . areaxy ( 0 , 2.4 ) for p in proteins ] ) \n        psize_lo = sum ( [ p . areaxy ( - 2.4 , 0 ) for p in proteins ] ) \n        unavail_up = holesize + psize_up \n        unavail_lo = holesize + psize_lo \n        upscale = ( upsize + unavail_up ) / xysize \n        loscale = ( losize + unavail_lo ) / xysize \n        area_scale = max ( upscale , loscale ) \n        aspect_ratio = pbc . x / pbc . y \n        scale_x = np . sqrt ( area_scale / aspect_ratio ) \n        scale_y = np . sqrt ( area_scale / aspect_ratio ) \n        pbc . box [ : 2 , : ] = pbc . box [ : 2 , : ] * ( math . sqrt ( area_scale ) ) "}
{"11649": "\ndef _add_attachments ( self ) : \n    num_attached = 0 \n    if self . attachments : \n        if isinstance ( self . attachments , str ) : \n            self . attachments = [ self . attachments ] \n        for item in self . attachments : \n            doc = MIMEApplication ( open ( item , \"rb\" ) . read ( ) ) \n            doc . add_header ( \"Content-Disposition\" , \"attachment\" , filename = item ) \n            self . message . attach ( doc ) \n            num_attached = num_attached + ( 1 ) \n    return num_attached "}
{"11653": "\ndef send ( self ) : \n    self . _generate_email ( ) \n    if self . verbose : \n        print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    recipients = [ ] \n    for i in ( self . to , self . cc , self . bcc ) : \n        if i : \n            if isinstance ( i , MutableSequence ) : \n                recipients = recipients + ( i ) \n            else : \n                recipients . append ( i ) \n    session = self . _get_session ( ) \n    if self . verbose : \n        print ( timestamp ( ) , \"Login successful.\" ) \n    session . sendmail ( self . from_ , recipients , self . message . as_string ( ) ) \n    session . quit ( ) \n    if self . verbose : \n        print ( timestamp ( ) , \"Logged out.\" ) \n    if self . verbose : \n        print ( timestamp ( ) , type ( self ) . __name__ + \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , ) \n    print ( \"Message sent.\" ) "}
{"11664": "\ndef save ( self , filename = None , v2_version = 4 , v23_sep = '/' ) : \n    framedata = self . _prepare_framedata ( v2_version , v23_sep ) \n    framesize = len ( framedata ) \n    if filename is None : \n        filename = self . filename \n    fileobj = open ( filename , 'rb+' ) \n    iff_file = IFFFile ( fileobj ) \n    try : \n        if u'ID3' not in iff_file : \n            iff_file . insert_chunk ( u'ID3' ) \n        chunk = iff_file [ u'ID3' ] \n        fileobj . seek ( chunk . data_offset ) \n        header = fileobj . read ( 10 ) \n        header = self . _prepare_id3_header ( header , framesize , v2_version ) \n        header , new_size , _ = header \n        data = header + framedata + ( b'\\x00' * ( new_size - framesize ) ) \n        new_size = new_size + ( 10 ) \n        if new_size > chunk . size : \n            insert_at = chunk . offset + chunk . size \n            insert_size = new_size - chunk . size + new_size % 2 \n            insert_bytes ( fileobj , insert_size , insert_at ) \n            chunk . resize ( new_size ) \n        fileobj . seek ( chunk . data_offset ) \n        fileobj . write ( data ) \n    finally : \n        fileobj . close ( ) "}
{"11697": "\ndef size ( self ) : \n    header_size = 27 \n    for datum in self . packets : \n        quot , rem = divmod ( len ( datum ) , 255 ) \n        header_size = header_size + ( quot + 1 ) \n    if not self . complete and rem == 0 : \n        header_size = header_size - ( 1 ) \n    header_size = header_size + ( sum ( map ( len , self . packets ) ) ) \n    return header_size "}
{"11706": "\ndef insert_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    assert 0 < size \n    assert 0 <= offset \n    locked = False \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset \n    fobj . write ( b'\\x00' * size ) \n    fobj . flush ( ) \n    try : \n        try : \n            import mmap \n            file_map = mmap . mmap ( fobj . fileno ( ) , filesize + size ) \n            try : \n                file_map . move ( offset + size , offset , movesize ) \n            finally : \n                file_map . close ( ) \n        except ( ValueError , EnvironmentError , ImportError ) : \n            locked = lock ( fobj ) \n            fobj . truncate ( filesize ) \n            fobj . seek ( 0 , 2 ) \n            padsize = size \n            while padsize : \n                addsize = min ( BUFFER_SIZE , padsize ) \n                fobj . write ( b\"\\x00\" * addsize ) \n                padsize = padsize - ( addsize ) \n            fobj . seek ( filesize , 0 ) \n            while movesize : \n                thismove = min ( BUFFER_SIZE , movesize ) \n                fobj . seek ( - thismove , 1 ) \n                nextpos = fobj . tell ( ) \n                data = fobj . read ( thismove ) \n                fobj . seek ( - thismove + size , 1 ) \n                fobj . write ( data ) \n                fobj . seek ( nextpos ) \n                movesize = movesize - ( thismove ) \n            fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11707": "\ndef delete_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    locked = False \n    assert 0 < size \n    assert 0 <= offset \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset - size \n    assert 0 <= movesize \n    try : \n        if movesize > 0 : \n            fobj . flush ( ) \n            try : \n                import mmap \n                file_map = mmap . mmap ( fobj . fileno ( ) , filesize ) \n                try : \n                    file_map . move ( offset , offset + size , movesize ) \n                finally : \n                    file_map . close ( ) \n            except ( ValueError , EnvironmentError , ImportError ) : \n                locked = lock ( fobj ) \n                fobj . seek ( offset + size ) \n                buf = fobj . read ( BUFFER_SIZE ) \n                while buf : \n                    fobj . seek ( offset ) \n                    fobj . write ( buf ) \n                    offset = offset + ( len ( buf ) ) \n                    fobj . seek ( offset + size ) \n                    buf = fobj . read ( BUFFER_SIZE ) \n        fobj . truncate ( filesize - size ) \n        fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11717": "\ndef __fullread ( self , size ) : \n    try : \n        if size < 0 : \n            raise ValueError ( 'Requested bytes (%s) less than zero' % size ) \n        if size > self . __filesize : \n            raise EOFError ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . __filesize ) , self . filename ) ) \n    except AttributeError : \n        pass \n    data = self . _fileobj . read ( size ) \n    if len ( data ) != size : \n        raise EOFError \n    self . __readbytes = self . __readbytes + ( size ) \n    return data "}
{"11721": "\ndef update_to_v24 ( self ) : \n    self . __update_common ( ) \n    if self . __unknown_version == self . _V23 : \n        converted = [ ] \n        for frame in self . unknown_frames : \n            try : \n                name , size , flags = unpack ( '>4sLH' , frame [ : 10 ] ) \n                frame = BinaryFrame . fromData ( self , flags , frame [ 10 : ] ) \n            except ( struct . error , error ) : \n                continue \n            name = name . decode ( 'ascii' ) \n            converted . append ( self . __save_frame ( frame , name = name ) ) \n        self . unknown_frames [ : ] = converted \n        self . __unknown_version = self . _V24 \n    try : \n        date = text_type ( self . get ( \"TYER\" , \"\" ) ) \n        if date . strip ( u\"\\x00\" ) : \n            self . pop ( \"TYER\" ) \n            dat = text_type ( self . get ( \"TDAT\" , \"\" ) ) \n            if dat . strip ( \"\\x00\" ) : \n                self . pop ( \"TDAT\" ) \n                date = \"%s-%s-%s\" % ( date , dat [ 2 : ] , dat [ : 2 ] ) \n                time = text_type ( self . get ( \"TIME\" , \"\" ) ) \n                if time . strip ( \"\\x00\" ) : \n                    self . pop ( \"TIME\" ) \n                    date = date + ( \"T%s:%s:00\" % ( time [ : 2 ] , time [ 2 : ] ) ) \n            if \"TDRC\" not in self : \n                self . add ( TDRC ( encoding = 0 , text = date ) ) \n    except UnicodeDecodeError : \n        pass \n    if \"TORY\" in self : \n        f = self . pop ( \"TORY\" ) \n        if \"TDOR\" not in self : \n            try : \n                self . add ( TDOR ( encoding = 0 , text = str ( f ) ) ) \n            except UnicodeDecodeError : \n                pass \n    if \"IPLS\" in self : \n        f = self . pop ( \"IPLS\" ) \n        if \"TIPL\" not in self : \n            self . add ( TIPL ( encoding = f . encoding , people = f . people ) ) \n    for key in [ \"RVAD\" , \"EQUA\" , \"TRDA\" , \"TSIZ\" , \"TDAT\" , \"TIME\" , \"CRM\" ] : \n        if key in self : \n            del ( self [ key ] ) "}
{"11727": "\ndef dump_encoding ( file , encoding_name , encoding_list ) : \n    write = file . write \n    write ( \"  /* the following are indices into the SID name table */\\n\" ) \n    write ( \"  static const unsigned short  \" + encoding_name + \"[\" + repr ( len ( encoding_list ) ) + \"] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"    \" \n    comma = \"\" \n    col = 0 \n    for value in encoding_list : \n        line = line + ( comma ) \n        line = line + ( \"%3d\" % value ) \n        comma = \",\" \n        col = col + ( 1 ) \n        if col == 16 : \n            col = 0 \n            comma = \",\\n    \" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11728": "\ndef dump_array ( the_array , write , array_name ) : \n    write ( \"  static const unsigned char  \" + array_name + \"[\" + repr ( len ( the_array ) ) + \"L] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"\" \n    comma = \"    \" \n    col = 0 \n    for value in the_array : \n        line = line + ( comma ) \n        line = line + ( \"%3d\" % ord ( value ) ) \n        comma = \",\" \n        col = col + ( 1 ) \n        if col == 16 : \n            col = 0 \n            comma = \",\\n    \" \n        if len ( line ) > 1024 : \n            write ( line ) \n            line = \"\" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = False ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and header > 4 : \n            available = available + ( header - 4 ) \n            header = 4 \n        if len ( data ) > available : \n            padding = self . metadata_blocks [ - 1 ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if newlength > 0 : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        elif len ( data ) < available : \n            self . metadata_blocks [ - 1 ] . length = self . metadata_blocks [ - 1 ] . length + ( ( available - len ( data ) ) ) \n            data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n            assert len ( data ) == available \n        if len ( data ) != available : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128 , 2 ) \n            except IOError : \n                pass \n            else : \n                if f . read ( 3 ) == b\"TAG\" : \n                    f . seek ( - 128 , 2 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11774": "\ndef get_next_colour ( ) : \n    colour = settings . GECKOBOARD_COLOURS [ get_next_colour . cur_colour ] \n    get_next_colour . cur_colour = get_next_colour . cur_colour + ( 1 ) \n    if get_next_colour . cur_colour >= len ( settings . GECKOBOARD_COLOURS ) : \n        get_next_colour . cur_colour = 0 \n    return colour "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    lnum = 1 \n    try : \n        while True : \n            if maxlines : \n                if lnum > maxlines : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum = lnum + ( 1 ) \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    inum = 1 \n    try : \n        while True : \n            if maxitems : \n                if inum > maxitems : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum = inum + ( 1 ) \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11840": "\ndef _Streamer__read_process ( self , path , read_size , cbuf , stop , barrier , cyclic , offset , read_skip , sync ) : \n    import tables as tb \n    h5_file = tb . open_file ( self . filename , 'r' , ** self . h5_kw_args ) \n    ary = h5_file . get_node ( path ) \n    i = offset \n    while not stop . is_set ( ) : \n        vals = ary [ i : i + read_size ] \n        if i + read_size > len ( ary ) : \n            vals = np . concatenate ( [ vals , ary [ 0 : read_size - len ( vals ) ] ] ) \n        if sync is None : \n            with cbuf . put_direct ( ) as put_ary : \n                put_ary [ : ] = vals \n        else : \n            with sync . do ( cbuf . put_direct ( ) , i , ( i + read_size ) % len ( ary ) ) as put_ary : \n                put_ary [ : ] = vals \n        i = i + ( read_skip ) \n        if cyclic : \n            if i >= len ( ary ) : \n                i = i % ( len ( ary ) ) \n                barrier . wait ( ) \n        else : \n            if i + read_size > len ( ary ) : \n                break "}
{"11850": "\ndef _read_varint ( self ) : \n    buff = self . _fd . read ( 1 ) \n    if buff == b'' : \n        return 0 \n    while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 == 1 : \n        new_byte = self . _fd . read ( 1 ) \n        if new_byte == b'' : \n            raise EOFError ( 'unexpected EOF.' ) \n        buff = buff + ( new_byte ) \n    varint , _ = decodeVarint ( buff , 0 ) \n    return varint "}
{"11857": "\ndef make_fake_movie ( nframes , mask_shape = ( 64 , 64 ) , mask_center = None , bg_intensity = 0.1 , mask_sigma = 10 , dt = 0.02 , rate = 1.0 , tau = 1. , sigma = 0.001 , seed = None ) : \n    gen = np . random . RandomState ( seed ) \n    n = gen . poisson ( rate * dt , size = nframes ) \n    gamma = np . exp ( - dt / tau ) \n    c = signal . lfilter ( np . r_ [ 1 ] , np . r_ [ 1 , - gamma ] , n , axis = 0 ) \n    nr , nc = mask_shape \n    npix = nr * nc \n    if mask_center is None : \n        mask_center = ( nc // 2. , nr // 2. ) \n    a , b = mask_center \n    y , x = np . ogrid [ : nr , : nc ] \n    xs = ( x - a ) ** 2. \n    ys = ( y - b ) ** 2. \n    twoss = 2. * mask_sigma ** 2. \n    alpha = np . exp ( - 1 * ( ( xs / twoss ) + ( ys / twoss ) ) ) . ravel ( ) \n    alpha = alpha / ( alpha . sum ( ) ) \n    beta = gen . randn ( npix ) * bg_intensity \n    lamb = rate \n    epsilon = gen . randn ( npix , nframes ) * sigma \n    F = c [ None , : ] * alpha [ : , None ] + beta [ : , None ] + epsilon \n    theta = ( sigma , alpha , beta , lamb , gamma ) \n    return F , c , n , theta "}
{"11859": "\ndef until_condition ( self , condition , condition_description ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    while True : \n        try : \n            if not hasattr ( condition , '__call__' ) : \n                raise TypeError ( \"condition is not callable\" ) \n            value = condition ( ) \n            if type ( value ) is bool and value is not False : \n                return value \n            elif type ( value ) is not bool and value is not None : \n                return value \n            else : \n                logger . debug ( \"#\" + str ( count ) + \" - wait until \" + condition_description ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0} : {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n        time . sleep ( self . _poll ) \n        count = count + ( 1 ) \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"condition <\" + condition_description + \"> was not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11860": "\ndef until_traits_are_present ( self , element_with_traits ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    missing_traits_descriptions = None \n    while True : \n        missing_traits_descriptions = [ ] \n        try : \n            missing_traits_descriptions = element_with_traits . evaluate_traits ( ) \n            if len ( missing_traits_descriptions ) == 0 : \n                return True \n            else : \n                logger . debug ( \"#{0} - wait until all traits are present: <{1}>\" . format ( str ( count ) , '> <' . join ( missing_traits_descriptions ) ) ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0}: {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n            pass \n        time . sleep ( self . _poll ) \n        count = count + ( 1 ) \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"conditions \" + '<' + '> <' . join ( missing_traits_descriptions ) + '>' + \" not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( 0 , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply = reply + ( codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries = tries + ( 1 ) \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"11919": "\ndef __fetch_items ( self , path , page = 1 ) : \n    fetch_data = True \n    parsed_crates = 0 \n    total_crates = 0 \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            payload = { 'sort' : 'alphabetical' , 'page' : page } \n            raw_content = self . fetch ( path , payload = payload ) \n            content = json . loads ( raw_content ) \n            parsed_crates = parsed_crates + ( len ( content [ 'crates' ] ) ) \n            if not total_crates : \n                total_crates = content [ 'meta' ] [ 'total' ] \n        except requests . exceptions . HTTPError as e : \n            logger . error ( \"HTTP exception raised - %s\" , e . response . text ) \n            raise e \n        yield raw_content \n        page = page + ( 1 ) \n        if parsed_crates >= total_crates : \n            fetch_data = False "}
{"11921": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Looking for questions at url '%s' using offset %s\" , self . url , str ( offset ) ) \n    nquestions = 0 \n    tquestions = 0 \n    equestions = 0 \n    page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    page_offset = page * KitsuneClient . ITEMS_PER_PAGE \n    drop_questions = offset - page_offset \n    current_offset = offset \n    questions_page = self . client . get_questions ( offset ) \n    while True : \n        try : \n            raw_questions = next ( questions_page ) \n        except StopIteration : \n            break \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code == 500 : \n                logger . exception ( e ) \n                logger . error ( \"Problem getting Kitsune questions. \" \"Loosing %i questions. Going to the next page.\" , KitsuneClient . ITEMS_PER_PAGE ) \n                equestions = equestions + ( KitsuneClient . ITEMS_PER_PAGE ) \n                current_offset = current_offset + ( KitsuneClient . ITEMS_PER_PAGE ) \n                questions_page = self . client . get_questions ( current_offset ) \n                continue \n            else : \n                raise e \n        try : \n            questions_data = json . loads ( raw_questions ) \n            tquestions = questions_data [ 'count' ] \n            questions = questions_data [ 'results' ] \n        except ( ValueError , KeyError ) as ex : \n            logger . error ( ex ) \n            cause = ( \"Bad JSON format for mozilla_questions: %s\" % ( raw_questions ) ) \n            raise ParseError ( cause = cause ) \n        for question in questions : \n            if drop_questions > 0 : \n                drop_questions = drop_questions - ( 1 ) \n                continue \n            question [ 'offset' ] = current_offset \n            current_offset = current_offset + ( 1 ) \n            question [ 'answers_data' ] = [ ] \n            for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : \n                answers = json . loads ( raw_answers ) [ 'results' ] \n                question [ 'answers_data' ] = question [ 'answers_data' ] + ( answers ) \n            yield question \n            nquestions = nquestions + ( 1 ) \n        logger . debug ( \"Questions: %i/%i\" , nquestions + offset , tquestions ) \n    logger . info ( \"Total number of questions: %i (%i total)\" , nquestions , tquestions ) \n    logger . info ( \"Questions with errors dropped: %i\" , equestions ) "}
{"11922": "\ndef get_questions ( self , offset = None ) : \n    page = KitsuneClient . FIRST_PAGE \n    if offset : \n        page = page + ( int ( offset / KitsuneClient . ITEMS_PER_PAGE ) ) \n    while True : \n        api_questions_url = urijoin ( self . base_url , '/question' ) + '/' \n        params = { \"page\" : page , \"ordering\" : \"updated\" } \n        questions = self . fetch ( api_questions_url , params ) \n        yield questions \n        questions_json = json . loads ( questions ) \n        next_uri = questions_json [ 'next' ] \n        if not next_uri : \n            break \n        page = page + ( 1 ) "}
{"11926": "\ndef get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : \n    more = True \n    next_uri = None \n    page = ReMoClient . FIRST_PAGE \n    page = page + ( int ( offset / ReMoClient . ITEMS_PER_PAGE ) ) \n    if category == CATEGORY_EVENT : \n        api = self . api_events_url \n    elif category == CATEGORY_ACTIVITY : \n        api = self . api_activities_url \n    elif category == CATEGORY_USER : \n        api = self . api_users_url \n    else : \n        raise ValueError ( category + ' not supported in ReMo' ) \n    while more : \n        params = { \"page\" : page , \"orderby\" : \"ASC\" } \n        logger . debug ( \"ReMo client calls APIv2: %s params: %s\" , api , str ( params ) ) \n        raw_items = self . fetch ( api , payload = params ) \n        yield raw_items \n        items_data = json . loads ( raw_items ) \n        next_uri = items_data [ 'next' ] \n        if not next_uri : \n            more = False \n        else : \n            parsed_uri = urllib . parse . urlparse ( next_uri ) \n            parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) \n            page = parsed_params [ 'page' ] [ 0 ] "}
{"11936": "\ndef parse ( self ) : \n    nevents_wrong = 0 \n    feed_json = json . loads ( self . feed ) \n    if 'entry' not in feed_json [ 'feed' ] : \n        return \n    self . cells = feed_json [ 'feed' ] [ 'entry' ] \n    self . ncell = 0 \n    event_fields = self . __get_event_fields ( ) \n    while self . ncell < len ( self . cells ) : \n        event = self . __get_next_event ( event_fields ) \n        if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : \n            logger . warning ( \"Wrong event data: %s\" , event ) \n            nevents_wrong = nevents_wrong + ( 1 ) \n            continue \n        yield event \n    logger . info ( \"Total number of wrong events: %i\" , nevents_wrong ) "}
{"11994": "\ndef decode ( data ) : \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    pos = 0 \n    while pos < len ( data ) : \n        header_byte = data [ pos ] \n        if header_byte > 127 : \n            header_byte = header_byte - ( 256 ) \n        pos = pos + ( 1 ) \n        if 0 <= header_byte <= 127 : \n            result . extend ( data [ pos : pos + header_byte + 1 ] ) \n            pos = pos + ( header_byte + 1 ) \n        elif header_byte == - 128 : \n            pass \n        else : \n            result . extend ( [ data [ pos ] ] * ( 1 - header_byte ) ) \n            pos = pos + ( 1 ) \n    return bytes ( result ) "}
{"11995": "\ndef encode ( data ) : \n    if len ( data ) == 0 : \n        return data \n    if len ( data ) == 1 : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = 0 \n    repeat_count = 0 \n    MAX_LENGTH = 127 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if len ( buf ) == 0 : \n            return \n        result . append ( len ( buf ) - 1 ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256 - ( repeat_count - 1 ) ) \n        result . append ( data [ pos ] ) \n    while pos < len ( data ) - 1 : \n        current_byte = data [ pos ] \n        if data [ pos ] == data [ pos + 1 ] : \n            if state == 'RAW' : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = 1 \n            elif state == 'RLE' : \n                if repeat_count == MAX_LENGTH : \n                    finish_rle ( ) \n                    repeat_count = 0 \n                repeat_count = repeat_count + ( 1 ) \n        else : \n            if state == 'RLE' : \n                repeat_count = repeat_count + ( 1 ) \n                finish_rle ( ) \n                state = 'RAW' \n                repeat_count = 0 \n            elif state == 'RAW' : \n                if len ( buf ) == MAX_LENGTH : \n                    finish_raw ( ) \n                buf . append ( current_byte ) \n        pos = pos + ( 1 ) \n    if state == 'RAW' : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count = repeat_count + ( 1 ) \n        finish_rle ( ) \n    return bytes ( result ) "}
{"11997": "\ndef format ( self , number , ** kwargs ) : \n    if check_type ( number , 'list' ) : \n        return map ( lambda val : self . format ( val , ** kwargs ) ) \n    number = self . parse ( number ) \n    if check_type ( kwargs , 'dict' ) : \n        options = ( self . settings [ 'number' ] . update ( kwargs ) ) \n    precision = self . _change_precision ( options [ 'precision' ] ) \n    negative = ( lambda num : \"-\" if num < 0 else \"\" ) ( number ) \n    base = str ( int ( self . to_fixed ( abs ( number ) or 0 , precision ) ) , 10 ) \n    mod = ( lambda num : len ( num ) % 3 if len ( num ) > 3 else 0 ) ( base ) \n    num = negative + ( lambda num : base [ 0 : num ] if num else '' ) ( mod ) \n    num = num + ( re . sub ( '/(\\d{3})(?=\\d)/g' , '$1' + options [ 'thousand' ] , base [ mod : ] ) ) \n    num = num + ( ( lambda val : options [ 'decimal' ] + self . to_fixed ( abs ( number ) , precision ) . split ( '.' ) [ 1 ] if val else '' ) ( precision ) ) \n    return num "}
{"12017": "\ndef save_collection ( png_filename_base , numpy_data , start_layers_at = 1 ) : \n    file_ext = png_filename_base . split ( '.' ) [ - 1 ] \n    if file_ext in [ 'png' ] : \n        file_base = '.' . join ( png_filename_base . split ( '.' ) [ : - 1 ] ) \n    else : \n        file_base = png_filename_base \n        file_ext = \".png\" \n    file_base_array = file_base . split ( '*' ) \n    output_files = [ ] \n    i = start_layers_at \n    for layer in numpy_data : \n        layer_filename = ( str ( i ) . zfill ( 6 ) ) . join ( file_base_array ) + file_ext \n        output_files . append ( save ( layer_filename , layer ) ) \n        i = i + ( 1 ) \n    return output_files "}
{"12149": "\ndef _out ( self , stream , page = None ) : \n    if page is not None : \n        page . buffer = page . buffer + ( str ( stream ) + \"\\n\" ) \n    else : \n        self . buffer = self . buffer + ( str ( stream ) + \"\\n\" ) "}
{"12171": "\ndef _put_header ( self ) : \n    self . session . _out ( '%%PDF-%s' % self . pdf_version ) \n    if self . session . compression : \n        self . session . buffer = self . session . buffer + ( '%' + chr ( 235 ) + chr ( 236 ) + chr ( 237 ) + chr ( 238 ) + \"\\n\" ) "}
{"12172": "\ndef _put_pages ( self ) : \n    self . document . _get_orientation_changes ( ) \n    self . document . _output_pages ( ) \n    self . session . _add_object ( 1 ) \n    self . session . _out ( '<</Type /Pages' ) \n    kids = '/Kids [' \n    for i in xrange ( 0 , len ( self . document . pages ) ) : \n        kids = kids + ( str ( 3 + 2 * i ) + ' 0 R ' ) \n    self . session . _out ( kids + ']' ) \n    self . session . _out ( '/Count %s' % len ( self . document . pages ) ) \n    self . session . _out ( '/MediaBox [0 0 %.2f %.2f]' % ( self . document . page . width , self . document . page . height ) ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12177": "\ndef floyd ( seqs , f = None , start = None , key = lambda x : x ) : \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while hare_value != tortise_value : \n        yield hare_value \n        yield hare . next ( ) \n        hare_value = hare . next ( ) \n        tortise_value = tortise . next ( ) \n    if f is None : \n        raise CycleDetected ( ) \n    hare_value = f ( hare_value ) \n    first = 0 \n    tortise_value = start \n    while key ( tortise_value ) != key ( hare_value ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first = first + ( 1 ) \n    period = 1 \n    hare_value = f ( tortise_value ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        hare_value = f ( hare_value ) \n        period = period + ( 1 ) \n    raise CycleDetected ( period = period , first = first ) "}
{"12180": "\ndef brent ( seqs , f = None , start = None , key = lambda x : x ) : \n    power = period = 1 \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        yield hare_value \n        if power == period : \n            power = power * ( 2 ) \n            period = 0 \n            if f : \n                tortise = f_generator ( f , hare_value ) \n                tortise_value = tortise . next ( ) \n            else : \n                while tortise_value != hare_value : \n                    tortise_value = tortise . next ( ) \n        hare_value = hare . next ( ) \n        period = period + ( 1 ) \n    if f is None : \n        raise CycleDetected ( ) \n    first = 0 \n    tortise_value = hare_value = start \n    for _ in xrange ( period ) : \n        hare_value = f ( hare_value ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first = first + ( 1 ) \n    raise CycleDetected ( period = period , first = first ) "}
{"12186": "\ndef x_plus ( self , dx = None ) : \n    if dx is None : \n        self . x = self . x + ( self . dx ) \n    else : \n        self . x = self . x + dx "}
{"12187": "\ndef y_plus ( self , dy = None ) : \n    if dy is None : \n        self . y = self . y + ( self . dy ) \n    else : \n        self . y = self . y + dy "}
{"12209": "\ndef duration ( self ) : \n    ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2 + self . esw ** 2 ) \n    esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) \n    aRs = ( ( G * self . rhos * ( 1. + self . MpMs ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) \n    inc = np . arccos ( self . bcirc / aRs ) \n    becc = self . bcirc * ( 1 - ecc ** 2 ) / ( 1 - esw ) \n    tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . RpRs ) ** 2 - becc ** 2 ) ** 0.5 / ( np . sin ( inc ) * aRs ) ) \n    tdur = tdur * ( np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) ) \n    return tdur "}
{"12210": "\ndef update ( self , ** kwargs ) : \n    if kwargs . get ( 'verify_kwargs' , True ) : \n        valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . _fields_ ] \n        valid = valid + ( [ 'b' , 'times' ] ) \n        for k in kwargs . keys ( ) : \n            if k not in valid : \n                raise Exception ( \"Invalid kwarg '%s'.\" % k ) \n    if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : KIPPING } ) \n    elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : NONLINEAR } ) \n    self . limbdark . update ( ** kwargs ) \n    self . transit . update ( ** kwargs ) \n    self . settings . update ( ** kwargs ) "}
{"12220": "\ndef command ( self , verb , args = None ) : \n    if self . __generating : \n        raise NNTPSyncError ( \"Command issued while a generator is active\" ) \n    cmd = verb \n    if args : \n        cmd = cmd + ( \" \" + args ) \n    cmd = cmd + ( \"\\r\\n\" ) \n    self . socket . sendall ( cmd ) \n    try : \n        code , message = self . status ( ) \n    except NNTPTemporaryError as e : \n        if e . code ( ) != 480 : \n            raise e \n        code , message = self . command ( \"AUTHINFO USER\" , self . username ) \n        if code == 381 : \n            code , message = self . command ( \"AUTHINFO PASS\" , self . password ) \n        if code != 281 : \n            raise NNTPReplyError ( code , message ) \n        code , message = self . command ( verb , args ) \n    return code , message "}
{"12227": "\ndef newnews_gen ( self , pattern , timestamp ) : \n    if timestamp . tzinfo : \n        ts = timestamp . asttimezone ( date . TZ_GMT ) \n    else : \n        ts = timestamp . replace ( tzinfo = date . TZ_GMT ) \n    args = pattern \n    args = args + ( \" \" + ts . strftime ( \"%Y%m%d %H%M%S %Z\" ) ) \n    code , message = self . command ( \"NEWNEWS\" , args ) \n    if code != 230 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12242": "\ndef xhdr ( self , header , msgid_range = None ) : \n    args = header \n    if range is not None : \n        args = args + ( \" \" + utils . unparse_msgid_range ( msgid_range ) ) \n    code , message = self . command ( \"XHDR\" , args ) \n    if code != 221 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12243": "\ndef xzhdr ( self , header , msgid_range = None ) : \n    args = header \n    if msgid_range is not None : \n        args = args + ( \" \" + utils . unparse_msgid_range ( msgid_range ) ) \n    code , message = self . command ( \"XZHDR\" , args ) \n    if code != 221 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message , compressed = True ) "}
{"12263": "\ndef unparse_range ( obj ) : \n    if isinstance ( obj , ( int , long ) ) : \n        return str ( obj ) \n    if isinstance ( obj , tuple ) : \n        arg = str ( obj [ 0 ] ) + \"-\" \n        if len ( obj ) > 1 : \n            arg = arg + ( str ( obj [ 1 ] ) ) \n        return arg \n    raise ValueError ( \"Must be an integer or tuple\" ) "}
{"12269": "\ndef defaults_docstring ( defaults , header = None , indent = None , footer = None ) : \n    if indent is None : \n        indent = '' \n    if header is None : \n        header = '' \n    if footer is None : \n        footer = '' \n    width = 60 \n    hbar = '\\n' \n    s = hbar + ( header ) + hbar \n    for key , value , desc in defaults : \n        if isinstance ( value , basestring ) : \n            value = \"'\" + value + \"'\" \n        if hasattr ( value , '__call__' ) : \n            value = \"<\" + value . __name__ + \">\" \n        s = s + ( indent + '%-12s\\n' % ( \"%s :\" % key ) ) \n        s = s + ( indent + indent + ( indent + 23 * ' ' ) . join ( desc . split ( '\\n' ) ) ) \n        s = s + ( ' [%s]\\n\\n' % str ( value ) ) \n    s = s + ( hbar ) \n    s = s + ( footer ) \n    return s "}
{"12270": "\ndef defaults_decorator ( defaults ) : \n    def decorator ( func ) : \n        kwargs = dict ( header = 'Keyword arguments\\n-----------------\\n' , indent = '  ' , footer = '\\n' ) \n        doc = defaults_docstring ( defaults , ** kwargs ) \n        if func . __doc__ is None : \n            func . __doc__ = '' \n        func . __doc__ = func . __doc__ + ( doc ) \n        return func \n    return decorator "}
{"12305": "\ndef prefixed_by ( prefix ) : \n    def prefixed_by_ ( name , value = None ) : \n        return name . startswith ( prefix ) \n    prefixed_by_ . __name__ = prefixed_by_ . __name__ + ( prefix ) \n    return prefixed_by_ "}
{"12313": "\ndef fromlist ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    self . files = fortunes = [ ] \n    count = 0 \n    for file in files : \n        fortune = load_fortune ( file , offensive = offensive , lang = lang ) \n        if fortune is None : \n            logger . warn ( \"Can't load: %s\" , file ) \n            continue \n        count = count + ( 1 if equal else fortune . size ) \n        fortunes . append ( ( fortune , count ) ) \n    if not fortunes : \n        raise ValueError ( 'All fortune files specified are invalid' ) \n    self . count = count \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12314": "\ndef set_chance ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    total = 0. \n    file = [ ] \n    leftover = [ ] \n    for name , chance in files : \n        if total >= 1 : \n            break \n        fortune = load_fortune ( name , offensive = offensive , lang = lang ) \n        if fortune is None or not fortune . size : \n            continue \n        if chance : \n            file . append ( ( fortune , chance ) ) \n            total = total + ( chance ) \n        else : \n            leftover . append ( fortune ) \n    if leftover and total < 1 : \n        left = 1 - total \n        if equal : \n            perfile = left / len ( leftover ) \n            for fortune in leftover : \n                file . append ( ( fortune , perfile ) ) \n        else : \n            entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) \n            logger . debug ( '%d entries left' , entries ) \n            for fortune in leftover : \n                chance = left * fortune . size / entries \n                file . append ( ( fortune , chance ) ) \n    self . count = count = 65536 \n    bound = 0 \n    self . files = fortunes = [ ] \n    for file , chance in file : \n        bound = bound + ( int ( chance * count ) ) \n        fortunes . append ( ( file , bound ) ) \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12339": "\ndef _ast_repetition_group_to_code ( self , repetition_group , ignore_whitespace = False , ** kwargs ) : \n    lines = [ \"zero_or_more(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( repetition_group . expression ) ) ) \n    lines [ - 1 ] = lines [ - 1 ] + ( \",\" ) \n    lines . append ( self . _indent ( \"ignore_whitespace={}\" . format ( bool ( ignore_whitespace ) ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12341": "\ndef _ast_op_alternate_to_code ( self , opr , ** kwargs ) : \n    hoist_target = OP_ALTERNATE \n    operands = self . _hoist_operands ( opr . operands , lambda t : isinstance ( t , OptreeNode ) and t . opnode . operator is hoist_target ) \n    lines = [ \"alternation([\" ] \n    for op in operands : \n        lines . extend ( self . _indent ( self . _ast_to_code ( op ) ) ) \n        lines [ - 1 ] = lines [ - 1 ] + ( \",\" ) \n    lines . append ( \"])\" ) \n    return lines "}
{"12342": "\ndef _ast_op_concat_to_code ( self , opr , * , ignore_whitespace , ** kwargs ) : \n    hoist_target = OP_CONCAT if ignore_whitespace else OP_WS_CONCAT \n    operands = self . _hoist_operands ( opr . operands , lambda t : isinstance ( t , OptreeNode ) and t . opnode . operator is hoist_target ) \n    lines = [ \"concatenation([\" ] \n    for op in operands : \n        lines . extend ( self . _indent ( self . _ast_to_code ( op , ignore_whitespace = ignore_whitespace ) ) ) \n        lines [ - 1 ] = lines [ - 1 ] + ( \",\" ) \n    lines . append ( \"], ignore_whitespace={})\" . format ( bool ( ignore_whitespace ) ) ) \n    return lines "}
{"12343": "\ndef _ast_op_exclude_to_code ( self , opr , ** kwargs ) : \n    opl , opr = opr . operands \n    lines = [ \"exclusion(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( opl ) ) ) \n    lines [ - 1 ] = lines [ - 1 ] + ( \",\" ) \n    lines . extend ( self . _indent ( self . _ast_to_code ( opr ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12344": "\ndef _ast_op_multiply_to_code ( self , opr , ignore_whitespace = False , ** kwargs ) : \n    opl , opr = opr . operands \n    if isinstance ( opl , Number ) : \n        times = opl . value \n        subject = self . _ast_to_code ( opr ) \n    else : \n        times = opr . value \n        subject = self . _ast_to_code ( opl ) \n    lines = [ \"repeated(\" ] \n    lines . extend ( self . _indent ( subject ) ) \n    lines [ - 1 ] = lines [ - 1 ] + ( \",\" ) \n    lines . append ( \"{0}times={1},\" . format ( self . indent , times ) ) \n    lines . append ( \"{0}ignore_whitespace={1}\" . format ( self . indent , bool ( ignore_whitespace ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12345": "\ndef _ast_op_repeat_to_code ( self , opr , ignore_whitespace = False , ** kwargs ) : \n    lines = [ \"one_or_more(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( opr . operands [ 0 ] ) ) ) \n    lines [ - 1 ] = lines [ - 1 ] + ( \",\" ) \n    lines . append ( self . _indent ( \"ignore_whitespace={}\" . format ( bool ( ignore_whitespace ) ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12357": "\ndef _reduce ( nodes ) : \n    i = 0 \n    while i < len ( nodes ) : \n        if isinstance ( nodes [ i ] , OperatorNode ) : \n            break \n        else : \n            i = i + ( 1 ) \n    if i == len ( nodes ) : \n        raise OperatorError ( \"No operator found\" ) \n    operator_node = nodes [ i ] \n    operator = operator_node . operator \n    operands_lbound = i - operator . cardinality \n    if operands_lbound < 0 : \n        raise OperatorError ( \"Insufficient operands for operator {0}\" . format ( operator . symbol ) ) \n    return nodes [ : operands_lbound ] + [ OptreeNode ( operator_node , tuple ( nodes [ operands_lbound : i ] ) ) ] + nodes [ i + 1 : ] "}
{"12385": "\ndef add_ignored ( self , ignored ) : \n    if ignored : \n        if self . ignored : \n            self . ignored = ignored + self . ignored \n        else : \n            self . ignored = ignored \n    self . consumed = self . consumed + ( len ( ignored ) ) "}
{"12391": "\ndef compressed ( self , new_type = None , * , include_ignored = False ) : \n    values = [ ] \n    consumed = 0 \n    ignored = None \n    for i , child in enumerate ( self . children ) : \n        consumed = consumed + ( child . consumed ) \n        if i == 0 and not include_ignored : \n            ignored = child . ignored \n        if child . is_value : \n            if include_ignored : \n                values . append ( \"{0}{1}\" . format ( child . ignored or \"\" , child . value ) ) \n            else : \n                values . append ( child . value ) \n        else : \n            values . append ( child . compressed ( include_ignored = include_ignored ) . value ) \n    return ParseNode ( new_type or self . node_type , children = [ \"\" . join ( values ) ] , consumed = consumed , ignored = ignored , position = self . position ) "}
{"12394": "\ndef step_next_char ( self ) : \n    self . _index = self . _index + ( 1 ) \n    self . _col_offset = self . _col_offset + ( 1 ) \n    if self . _index > self . _maxindex : \n        self . _maxindex = self . _index \n        self . _maxcol = self . _col_offset \n        self . _maxline = self . _lineno "}
{"12395": "\ndef step_next_line ( self ) : \n    self . _eol . append ( self . position ) \n    self . _lineno = self . _lineno + ( 1 ) \n    self . _col_offset = 0 "}
{"12397": "\ndef last_readed_line ( self ) -> str : \n    mpos = self . _cursor . max_readed_position \n    mindex = mpos . index \n    prevline = mindex - 1 if mindex == self . eos_index else mindex \n    while prevline >= 0 and self . _content [ prevline ] != '\\n' : \n        prevline = prevline - ( 1 ) \n    nextline = mindex \n    while nextline < self . eos_index and self . _content [ nextline ] != '\\n' : \n        nextline = nextline + ( 1 ) \n    last_line = self . _content [ prevline + 1 : nextline ] \n    return last_line "}
{"12398": "\ndef incpos ( self , length : int = 1 ) -> int : \n    if length < 0 : \n        raise ValueError ( \"length must be positive\" ) \n    i = 0 \n    while ( i < length ) : \n        if self . _cursor . index < self . _len : \n            if self . peek_char == '\\n' : \n                self . _cursor . step_next_line ( ) \n            self . _cursor . step_next_char ( ) \n        i = i + ( 1 ) \n    return self . _cursor . index "}
{"12403": "\ndef count_vars ( self ) -> int : \n    n = 0 \n    for s in self . _hsig . values ( ) : \n        if hasattr ( s , 'is_var' ) and s . is_var : \n            n = n + ( 1 ) \n    return n "}
{"12404": "\ndef count_funs ( self ) -> int : \n    n = 0 \n    for s in self . _hsig . values ( ) : \n        if hasattr ( s , 'is_fun' ) and s . is_fun : \n            n = n + ( 1 ) \n    return n "}
{"12411": "\ndef difference ( self , sig : Scope ) -> Scope : \n    new = Scope ( sig = self . _hsig . values ( ) , state = self . state ) \n    new = new - ( sig ) \n    return new "}
{"12488": "\ndef to_dot ( self ) -> str : \n    txt = \"\" \n    txt = txt + ( \"digraph S%d {\\n\" % id ( self ) ) \n    if self . label is not None : \n        txt = txt + ( '\\tlabel=\"%s\";\\n' % ( self . label + '\\l' ) . replace ( '\\n' , '\\l' ) ) \n    txt = txt + ( \"\\trankdir=LR;\\n\" ) \n    txt = txt + ( '\\tgraph [labeljust=l, labelloc=t, nojustify=true];\\n' ) \n    txt = txt + ( \"\\tesep=1;\\n\" ) \n    txt = txt + ( '\\tranksep=\"equally\";\\n' ) \n    txt = txt + ( \"\\tnode [shape = circle];\\n\" ) \n    txt = txt + ( \"\\tsplines = ortho;\\n\" ) \n    for s in self . states . values ( ) : \n        txt = txt + ( s [ 1 ] . to_dot ( ) ) \n    txt = txt + ( \"}\\n\" ) \n    return txt "}
{"12498": "\ndef dump_nodes ( self ) : \n    print ( \"DUMP NODE LOCAL INFOS\" ) \n    try : \n        print ( \"map Id->node name\" ) \n        for k , v in self . id_cache . items ( ) : \n            print ( \"[%d]=%s\" % ( k , v ) ) \n        print ( \"map tag->capture infos\" ) \n        for k , v in self . tag_cache . items ( ) : \n            print ( \"[%s]=%s\" % ( k , v ) ) \n        print ( \"map nodes->tag resolution\" ) \n        for k , v in self . rule_nodes . items ( ) : \n            txt = \"['%s']=%d\" % ( k , id ( v ) ) \n            if k in self . tag_cache : \n                tag = self . tag_cache [ k ] \n                txt = txt + ( \" tag <%s>\" % tag ) \n                k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n                if k in self . _stream . value_cache : \n                    txt = txt + ( \" cache <%s>\" % self . _stream . value_cache [ k ] ) \n            print ( txt ) \n    except Exception as err : \n        print ( \"RECV Exception %s\" % err ) \n    import sys \n    sys . stdout . flush ( ) \n    return True "}
{"12508": "\ndef visit_Alt ( self , node : parsing . Alt ) -> [ ast . stmt ] : \n    clauses = [ self . visit ( clause ) for clause in node . ptlist ] \n    for clause in clauses : \n        if not isinstance ( clause , ast . expr ) : \n            break \n    else : \n        return ast . BoolOp ( ast . Or ( ) , clauses ) \n    res = ast . Try ( [ ] , [ ast . ExceptHandler ( ast . Name ( 'AltTrue' , ast . Load ( ) ) , None , [ ast . Pass ( ) ] ) ] , [ ] , [ ] ) \n    alt_true = [ ast . Raise ( ast . Call ( ast . Name ( 'AltTrue' , ast . Load ( ) ) , [ ] , [ ] , None , None ) , None ) ] \n    alt_false = [ ast . ExceptHandler ( ast . Name ( 'AltFalse' , ast . Load ( ) ) , None , [ ast . Pass ( ) ] ) ] \n    self . in_try = self . in_try + ( 1 ) \n    for clause in node . ptlist : \n        res . body . append ( ast . Try ( self . _clause ( self . visit ( clause ) ) + alt_true , alt_false , [ ] , [ ] ) ) \n    self . in_try = self . in_try - ( 1 ) \n    res . body . append ( self . __exit_scope ( ) ) \n    return [ res ] "}
{"12510": "\ndef visit_RepOptional ( self , node : parsing . RepOptional ) -> ( [ ast . stmt ] or ast . expr ) : \n    cl_ast = self . visit ( node . pt ) \n    if isinstance ( cl_ast , ast . expr ) : \n        return ast . BoolOp ( ast . Or ( ) , [ cl_ast , ast . Name ( 'True' , ast . Load ( ) ) ] ) \n    self . in_optional = self . in_optional + ( 1 ) \n    cl_ast = self . visit ( node . pt ) \n    self . in_optional = self . in_optional - ( 1 ) \n    return cl_ast "}
{"12511": "\ndef visit_Rep0N ( self , node : parsing . Rep0N ) -> [ ast . stmt ] : \n    cl_ast = self . visit ( node . pt ) \n    if isinstance ( cl_ast , ast . expr ) : \n        return [ ast . While ( cl_ast , [ ast . Pass ( ) ] , [ ] ) ] \n    self . in_loop = self . in_loop + ( 1 ) \n    clause = self . _clause ( self . visit ( node . pt ) ) \n    self . in_loop = self . in_loop - ( 1 ) \n    return [ ast . While ( ast . Name ( 'True' , ast . Load ( ) ) , clause , [ ] ) ] "}
{"12512": "\ndef visit_Rep1N ( self , node : parsing . Rep0N ) -> [ ast . stmt ] : \n    clause = self . visit ( node . pt ) \n    if isinstance ( clause , ast . expr ) : \n        return ( self . _clause ( clause ) + self . visit_Rep0N ( node ) ) \n    self . in_loop = self . in_loop + ( 1 ) \n    clause = self . _clause ( self . visit ( node . pt ) ) \n    self . in_loop = self . in_loop - ( 1 ) \n    return self . _clause ( self . visit ( node . pt ) ) + [ ast . While ( ast . Name ( 'True' , ast . Load ( ) ) , clause , [ ] ) ] "}
{"12513": "\ndef catend ( dst : str , src : str , indent ) -> str : \n    res = dst \n    txtsrc = src \n    if not isinstance ( src , str ) : \n        txtsrc = str ( src ) \n    for c in list ( txtsrc ) : \n        if len ( res ) > 0 and res [ - 1 ] == '\\n' : \n            res = res + ( ( indentable . char_indent * indentable . num_indent ) * ( indent - 1 ) + c ) \n        else : \n            res = res + ( c ) \n    return res "}
{"12516": "\ndef echo_nodes ( self , * rest ) : \n    txt = \"\" \n    for thing in rest : \n        if isinstance ( thing , Node ) : \n            txt = txt + ( self . value ( thing ) ) \n        else : \n            txt = txt + ( str ( thing ) ) \n    print ( txt ) \n    return True "}
{"12517": "\ndef populate_from_sequence ( seq : list , r : ref ( Edge ) , sr : state . StateRegister ) : \n    base_state = r \n    idxlast = len ( seq ) - 1 \n    idx = 0 \n    for m in seq : \n        if isinstance ( m , list ) : \n            for item in m : \n                populate_from_sequence ( item , r , sr ) \n        elif isinstance ( m , MatchExpr ) : \n            eX = r ( ) . get_next_edge ( m ) \n            if eX is None : \n                sX = None \n                if idx != idxlast : \n                    sX = state . State ( sr ) \n                    sX . matchDefault ( base_state ( ) . s ) \n                else : \n                    sX = base_state ( ) . s \n                eX = Edge ( sX ) \n                r ( ) . next_edge [ id ( sX ) ] = eX \n                m . attach ( r ( ) . s , sX , sr ) \n            r = ref ( eX ) \n        idx = idx + ( 1 ) "}
{"12569": "\ndef inspectrecords ( sources , recid , entity = None ) : \n    for idx , source in enumerate ( sources , 1 ) : \n        click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        if not recid : \n            click . secho ( 'Record identifiers' , fg = 'green' ) \n            total = 0 \n            for r in ( d [ 'recid' ] for d in data ) : \n                click . echo ( r ) \n                total = total + ( 1 ) \n            click . echo ( '{0} records found in dump.' . format ( total ) ) \n            return \n        data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) \n        if not data : \n            click . secho ( \"Record not found.\" , fg = 'yellow' ) \n            return \n        for record in data : \n            if entity is None : \n                click . echo ( json . dumps ( record , indent = 2 ) ) \n            if entity == 'files' : \n                click . secho ( 'Files' , fg = 'green' ) \n                click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) \n            if entity == 'json' : \n                click . secho ( 'Records (JSON)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'modification_datetime' ] ) , fg = 'yellow' ) \n                    click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) \n            if entity == 'marcxml' : \n                click . secho ( 'Records (MARCXML)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) \n                    click . echo ( revision ) "}
{"12583": "\ndef stitch ( images ) : \n    if type ( images ) != ImageCollection : \n        images = ImageCollection ( images ) \n    calc_translations_parallel ( images ) \n    _translation_warn ( images ) \n    yoffset , xoffset = images . median_translation ( ) \n    if xoffset != yoffset : \n        warn ( 'yoffset != xoffset: %s != %s' % ( yoffset , xoffset ) ) \n    y , x = imread ( images [ 0 ] . path ) . shape \n    height = y * len ( images . rows ) + yoffset * ( len ( images . rows ) - 1 ) \n    width = x * len ( images . cols ) + xoffset * ( len ( images . cols ) - 1 ) \n    merged = np . zeros ( ( height , width , 2 ) , dtype = np . int ) \n    for image in images : \n        r , c = image . row , image . col \n        mask = _merge_slice ( r , c , y , x , yoffset , xoffset ) \n        img = _add_ones_dim ( imread ( image . path ) ) \n        merged [ mask ] = merged [ mask ] + ( img ) \n    merged [ ... , 0 ] = merged [ ... , 0 ] / ( merged [ ... , 1 ] ) \n    return merged [ ... , 0 ] . astype ( np . uint8 ) , ( yoffset , xoffset ) "}
{"12600": "\ndef dump ( thing , query , from_date , file_prefix , chunk_size , limit , thing_flags ) : \n    init_app_context ( ) \n    file_prefix = file_prefix if file_prefix else '{0}_dump' . format ( thing ) \n    kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , '_' ) , True ) for f in thing_flags ) \n    try : \n        thing_func = collect_things_entry_points ( ) [ thing ] \n    except KeyError : \n        click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) \n    click . echo ( \"Querying {0}...\" . format ( thing ) ) \n    count , items = thing_func . get ( query , from_date , limit = limit , ** kwargs ) \n    progress_i = 0 \n    click . echo ( \"Dumping {0}...\" . format ( thing ) ) \n    with click . progressbar ( length = count ) as bar : \n        for i , chunk_ids in enumerate ( grouper ( items , chunk_size ) ) : \n            with open ( '{0}_{1}.json' . format ( file_prefix , i ) , 'w' ) as fp : \n                fp . write ( \"[\\n\" ) \n                for _id in chunk_ids : \n                    try : \n                        json . dump ( thing_func . dump ( _id , from_date , ** kwargs ) , fp , default = set_serializer ) \n                        fp . write ( \",\" ) \n                    except Exception as e : \n                        click . secho ( \"Failed dump {0} {1} ({2})\" . format ( thing , _id , e . message ) , fg = 'red' ) \n                    progress_i = progress_i + ( 1 ) \n                    bar . update ( progress_i ) \n                fp . seek ( fp . tell ( ) - 1 ) \n                fp . write ( \"\\n]\" ) "}
{"12601": "\ndef check ( thing ) : \n    init_app_context ( ) \n    try : \n        thing_func = collect_things_entry_points ( ) [ thing ] \n    except KeyError : \n        click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) \n    click . echo ( \"Querying {0}...\" . format ( thing ) ) \n    count , items = thing_func . get_check ( ) \n    i = 0 \n    click . echo ( \"Checking {0}...\" . format ( thing ) ) \n    with click . progressbar ( length = count ) as bar : \n        for _id in items : \n            thing_func . check ( _id ) \n            i = i + ( 1 ) \n            bar . update ( i ) "}
{"12652": "\ndef _get_region ( self , buffer , start , count ) : \n    byte_start = self . stride * start \n    byte_size = self . stride * count \n    array_count = self . count * count \n    if self . stride == self . size or not array_count : \n        ptr_type = ctypes . POINTER ( self . c_type * array_count ) \n        return buffer . get_region ( byte_start , byte_size , ptr_type ) \n    else : \n        byte_start = byte_start + ( self . offset ) \n        byte_size = byte_size - ( self . offset ) \n        elem_stride = self . stride // ctypes . sizeof ( self . c_type ) \n        elem_offset = self . offset // ctypes . sizeof ( self . c_type ) \n        ptr_type = ctypes . POINTER ( self . c_type * int ( ( count * elem_stride - elem_offset ) ) ) \n        region = buffer . get_region ( byte_start , byte_size , ptr_type ) \n        return vertexbuffer . IndirectArrayRegion ( region , array_count , self . count , elem_stride ) "}
{"12657": "\ndef addLayer ( self , layer , z_index = None ) : \n    if z_index is None : \n        z_index = layer . z_index \n    i = 0 \n    for l , z in self . layers : \n        if z > z_index : \n            break \n        i = i + ( 1 ) \n    self . _layers [ layer . name ] = layer \n    self . layers . insert ( i , [ layer , z_index ] ) "}
{"12664": "\ndef _make_conn ( shape ) : \n    shape = np . array ( shape ) \n    Ne = shape . prod ( ) \n    if len ( shape ) == 2 : \n        nx , ny = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) \n        for j in range ( shape [ 1 ] ) : \n            for i in range ( shape [ 0 ] ) : \n                conn [ counter ] = pattern + 1 + i + j * nx \n                counter = counter + ( 1 ) \n    if len ( shape ) == 3 : \n        nx , ny , nz = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) \n        for k in range ( shape [ 2 ] ) : \n            for j in range ( shape [ 1 ] ) : \n                for i in range ( shape [ 0 ] ) : \n                    conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny \n                    counter = counter + ( 1 ) \n    return conn "}
{"12670": "\ndef angles ( self , zfill = 3 ) : \n    elements = self . elements . sort_index ( axis = 1 ) \n    etypes = elements [ ( \"type\" , \"argiope\" ) ] . unique ( ) \n    out = [ ] \n    for etype in etypes : \n        etype_info = ELEMENTS [ etype ] \n        angles_info = etype_info . angles \n        loc = elements [ ( \"type\" , \"argiope\" , \"\" ) ] == etype \n        index = elements . loc [ loc ] . index \n        angles_data = self . split ( into = \"angles\" , loc = loc , at = \"coords\" ) \n        data = angles_data . values . reshape ( index . size , angles_info . shape [ 0 ] , angles_info . shape [ 1 ] , 3 ) \n        edges = data [ : , : , [ 0 , 2 ] , : ] - data [ : , : , 1 : 2 , : ] \n        edges = edges / ( np . linalg . norm ( edges , axis = 3 ) . reshape ( index . size , angles_info . shape [ 0 ] , 2 , 1 ) ) \n        angles = np . degrees ( np . arccos ( ( edges [ : , : , 0 ] * edges [ : , : , 1 ] ) . sum ( axis = 2 ) ) ) \n        deviation = angles - etype_info . optimal_angles \n        angles_df = pd . DataFrame ( index = index , data = angles , columns = pd . MultiIndex . from_product ( [ [ \"angles\" ] , [ \"a\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ 0 ] ) ] ] ) ) \n        deviation_df = pd . DataFrame ( index = index , data = deviation , columns = pd . MultiIndex . from_product ( [ [ \"deviation\" ] , [ \"d\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ 0 ] ) ] ] ) ) \n        df = pd . concat ( [ angles_df , deviation_df ] , axis = 1 ) . sort_index ( axis = 1 ) \n        df [ \"stats\" , \"max_angle\" ] = df . angles . max ( axis = 1 ) \n        df [ \"stats\" , \"min_angle\" ] = df . angles . min ( axis = 1 ) \n        df [ \"stats\" , \"max_angular_deviation\" ] = df . deviation . max ( axis = 1 ) \n        df [ \"stats\" , \"min_angular_deviation\" ] = df . deviation . min ( axis = 1 ) \n        df [ \"stats\" , \"max_abs_angular_deviation\" ] = abs ( df . deviation ) . max ( axis = 1 ) \n        df = df . sort_index ( axis = 1 ) \n        out . append ( df ) \n    out = pd . concat ( out ) . sort_index ( axis = 1 ) \n    return out "}
{"12674": "\ndef node_set_to_surface ( self , tag ) : \n    nodes = self . nodes . copy ( ) \n    dummy = nodes . iloc [ 0 ] . copy ( ) \n    dummy [ \"coords\" ] = dummy [ \"coords\" ] * ( np . nan ) \n    dummy [ \"sets\" ] = True \n    nodes . loc [ 0 ] = dummy \n    element_surfaces = self . split ( \"surfaces\" ) . unstack ( ) \n    surf = pd . DataFrame ( nodes . sets [ tag ] . loc [ element_surfaces . values . flatten ( ) ] . values . reshape ( element_surfaces . shape ) . prod ( axis = 1 ) . astype ( np . bool ) , index = element_surfaces . index ) . unstack ( ) . fillna ( False ) \n    for k in surf . keys ( ) : \n        self . elements [ \"surfaces\" , tag , \"f{0}\" . format ( k [ 1 ] + 1 ) ] = surf . loc [ : , k ] "}
{"12681": "\ndef read_history_report ( path , steps , x_name = None ) : \n    data = pd . read_csv ( path , delim_whitespace = True ) \n    if x_name != None : \n        data [ x_name ] = data . X \n        del data [ \"X\" ] \n    data [ \"step\" ] = 0 \n    t = 0. \n    for i in range ( len ( steps ) ) : \n        dt = steps [ i ] . duration \n        loc = data [ data . t == t ] . index \n        if len ( loc ) == 2 : \n            data . loc [ loc [ 1 ] : , \"step\" ] = i \n        t = t + ( dt ) \n    return data "}
{"12683": "\ndef list_to_string ( l = range ( 200 ) , width = 40 , indent = \"  \" ) : \n    l = [ str ( v ) + \",\" for v in l ] \n    counter = 0 \n    out = \"\" + indent \n    for w in l : \n        s = len ( w ) \n        if counter + s > width : \n            out = out + ( \"\\n\" + indent ) \n            counter = 0 \n        out = out + ( w ) \n        counter = counter + ( s ) \n    return out . strip ( \",\" ) "}
{"12684": "\ndef _equation ( nodes = ( 1 , 2 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : \n    N = len ( nodes ) \n    if comment == None : \n        out = \"\" \n    else : \n        out = \"**EQUATION: {0}\\n\" . format ( comment ) \n    out = out + ( \"*EQUATION\\n  {0}\\n  \" . format ( N ) ) \n    out = out + ( \"\\n  \" . join ( [ \",\" . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) ) \n    return out "}
{"12689": "\ndef write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - 1 , frame = - 1 , sortItem = 'Node Label' ) : \n    stepKeys = get_steps ( odb ) \n    step = xrange ( len ( stepKeys ) ) [ step ] \n    frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] \n    nf = NumberFormat ( numDigits = 9 , precision = 0 , format = SCIENTIFIC ) \n    session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) \n    leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) \n    session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) \n    session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) \n    lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] \n    isdata = - 1 \n    data = [ ] \n    for line in lines : \n        if isdata == 1 : \n            if len ( line ) == 0 : \n                isdata = isdata - ( 1 ) \n            else : \n                data . append ( line ) \n        elif isdata < 1 : \n            if line . startswith ( \"--\" ) : \n                isdata = isdata + ( 1 ) \n    data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) \n    header = str ( output_position ) . lower ( ) + \",\" \n    header = header + ( \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + \"\\n\" ) \n    metadata = ( ( \"label\" , label ) , ( \"argiope_class\" , argiope_class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output_position ) , ( \"step_num\" , step ) , ( \"step_label\" , stepKeys [ step ] ) , ( \"frame\" , frame ) , ( \"frame_value\" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) \n    out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) \n    open ( path , \"w\" ) . write ( out ) "}
{"12707": "\ndef collect_words ( self , si ) : \n    counter = Counter ( ) \n    for tagger_id , sentences in si . body . sentences . iteritems ( ) : \n        if ( ( self . keyword_tagger_ids is not None and tagger_id not in self . keyword_tagger_ids ) ) : \n            continue \n        for sentence in sentences : \n            for token in sentence . tokens : \n                term = token . token \n                term = term . decode ( 'utf-8' ) \n                term = cleanse ( term ) \n                if ( ( self . keyword_size_limit is not None and len ( term ) > self . keyword_size_limit ) ) : \n                    continue \n                if term not in self . stop_words : \n                    counter [ term ] = counter [ term ] + ( 1 ) \n    return counter "}
{"12708": "\ndef index ( self , si ) : \n    if not si . body . clean_visible : \n        logger . warn ( 'stream item %s has no clean_visible part, ' 'skipping keyword indexing' , si . stream_id ) \n        return \n    hash_counts = defaultdict ( int ) \n    hash_counts [ DOCUMENT_HASH_KEY ] = 1 \n    hash_kw = defaultdict ( int ) \n    words = self . collect_words ( si ) \n    for tok , count in words . iteritems ( ) : \n        ( tok , tok_hash ) = self . make_hash_kw ( tok ) \n        hash_counts [ tok_hash ] = hash_counts [ tok_hash ] + ( count ) \n        hash_kw [ tok ] = tok_hash \n    if self . hash_docs : \n        ( k1 , k2 ) = key_for_stream_item ( si ) \n        kvps = [ ( ( h , k1 , k2 ) , n ) for ( h , n ) in hash_counts . iteritems ( ) if h != DOCUMENT_HASH_KEY ] \n        self . client . put ( HASH_TF_INDEX_TABLE , * kvps ) \n    if self . hash_frequencies : \n        kvps = [ ( ( h , ) , 1 ) for h in hash_counts . iterkeys ( ) ] \n        self . client . increment ( HASH_FREQUENCY_TABLE , * kvps ) \n    if self . hash_keywords : \n        kvps = [ ( ( h , t ) , 1 ) for ( t , h ) in hash_kw . iteritems ( ) ] \n        self . client . increment ( HASH_KEYWORD_INDEX_TABLE , * kvps ) "}
{"12759": "\ndef run ( self , i_str , start_count = 0 , start_chunk_time = None ) : \n    try : \n        if not os . path . exists ( self . tmp_dir_path ) : \n            os . makedirs ( self . tmp_dir_path ) \n        if start_chunk_time is None : \n            start_chunk_time = time . time ( ) \n        i_chunk = self . reader ( i_str ) \n        t_path = None \n        len_clean_visible = 0 \n        sources = set ( ) \n        next_idx = 0 \n        input_item_count = 0 \n        for si in i_chunk : \n            next_idx = next_idx + ( 1 ) \n            if gevent : \n                gevent . sleep ( 0 ) \n            if next_idx <= start_count : \n                continue \n            if next_idx % self . rate_log_interval == 0 : \n                elapsed = time . time ( ) - start_chunk_time \n                if elapsed > 0 : \n                    rate = float ( next_idx ) / elapsed \n                    logger . info ( '%d in %.1f --> %.1f per sec on ' '(pre-partial_commit) %s' , next_idx - start_count , elapsed , rate , i_str ) \n            if not self . t_chunk : \n                t_path = os . path . join ( self . tmp_dir_path , 't_chunk-%s' % uuid . uuid4 ( ) . hex ) \n                self . t_chunk = streamcorpus . Chunk ( path = t_path , mode = 'wb' ) \n                assert self . t_chunk . message == streamcorpus . StreamItem_v0_3_0 , self . t_chunk . message \n            si = self . _run_incremental_transforms ( si , self . incremental_transforms ) \n            if si : \n                sources . add ( si . source ) \n                if self . assert_single_source and len ( sources ) != 1 : \n                    raise InvalidStreamItem ( 'stream item %r had source %r, not %r ' '(set assert_single_source: false to suppress)' % ( si . stream_id , si . source , sources ) ) \n            if si and si . body and si . body . clean_visible : \n                len_clean_visible = len_clean_visible + ( len ( si . body . clean_visible ) ) \n            if ( ( self . output_chunk_max_count is not None and len ( self . t_chunk ) == self . output_chunk_max_count ) ) : \n                logger . info ( 'reached output_chunk_max_count (%d) at: %d' , len ( self . t_chunk ) , next_idx ) \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            elif ( self . output_max_clean_visible_bytes is not None and len_clean_visible >= self . output_chunk_max_clean_visible_bytes ) : \n                logger . info ( 'reached output_chunk_max_clean_visible_bytes ' '(%d) at: %d' , self . output_chunk_max_clean_visible_bytes , len_clean_visible ) \n                len_clean_visible = 0 \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            input_item_count = input_item_count + ( 1 ) \n            if ( ( ( self . input_item_limit is not None ) and ( input_item_count > self . input_item_limit ) ) ) : \n                break \n        if self . t_chunk is not None : \n            self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n        return next_idx \n    finally : \n        if self . t_chunk is not None : \n            self . t_chunk . close ( ) \n        for transform in self . batch_transforms : \n            transform . shutdown ( ) \n        if self . cleanup_tmp_files : \n            rmtree ( self . tmp_dir_path ) "}
{"12760": "\ndef _run_writers ( self , start_count , next_idx , sources , i_str , t_path ) : \n    name_info = dict ( first = start_count , source = sources . pop ( ) , ) \n    all_o_paths = [ ] \n    for writer in self . writers : \n        logger . debug ( 'running %r on %r: %r' , writer , i_str , name_info ) \n        o_paths = writer ( t_path , name_info , i_str ) \n        logger . debug ( 'loaded (%d, %d) of %r into %r' , start_count , next_idx - 1 , i_str , o_paths ) \n        all_o_paths = all_o_paths + ( o_paths ) \n    return all_o_paths "}
{"12764": "\ndef make_chains_with_names ( sentences ) : \n    fake_equiv_ids = - 2 \n    equiv_ids = collections . defaultdict ( lambda : ( set ( ) , set ( ) ) ) \n    for tagger_id , sents in sentences . items ( ) : \n        for sent in sents : \n            for tok in sent . tokens : \n                if tok . entity_type is not None : \n                    if tok . equiv_id == - 1 : \n                        eqid = fake_equiv_ids \n                        fake_equiv_ids = fake_equiv_ids - ( 1 ) \n                    else : \n                        eqid = tok . equiv_id \n                    equiv_ids [ eqid ] [ 0 ] . add ( cleanse ( tok . token . decode ( 'utf8' ) ) ) \n                    equiv_ids [ eqid ] [ 1 ] . add ( tok ) \n    return equiv_ids "}
{"12768": "\ndef multi_token_match ( stream_item , aligner_data ) : \n    tagger_id = _get_tagger_id ( stream_item , aligner_data ) \n    sentences = stream_item . body . sentences . get ( tagger_id ) \n    if not sentences : \n        return \n    tokens = map ( lambda tok : ( cleanse ( tok . token . decode ( 'utf8' ) ) . split ( ' ' ) , tok ) , itertools . chain ( * [ sent . tokens for sent in sentences ] ) ) \n    required_annotator_id = aligner_data [ 'annotator_id' ] \n    for annotator_id , ratings in stream_item . ratings . items ( ) : \n        if ( required_annotator_id is None ) or ( annotator_id == required_annotator_id ) : \n            for rating in ratings : \n                label = Label ( annotator = rating . annotator , target = rating . target ) \n                num_tokens_matched = 0 \n                for tok in look_ahead_match ( rating , tokens ) : \n                    if aligner_data . get ( 'update_labels' ) : \n                        tok . labels . pop ( annotator_id , None ) \n                    add_annotation ( tok , label ) \n                    num_tokens_matched = num_tokens_matched + ( 1 ) \n                if num_tokens_matched == 0 : \n                    logger . warning ( 'multi_token_match didn\\'t actually match ' 'entity %r in stream_id %r' , rating . target . target_id , stream_item . stream_id ) \n                else : \n                    logger . debug ( 'matched %d tokens for %r in %r' , num_tokens_matched , rating . target . target_id , stream_item . stream_id ) "}
{"12776": "\ndef make_sentences ( self , stream_item ) : \n    self . make_label_index ( stream_item ) \n    sentences = [ ] \n    token_num = 0 \n    new_mention_id = 0 \n    for sent_start , sent_end , sent_str in self . _sentences ( stream_item . body . clean_visible ) : \n        assert isinstance ( sent_str , unicode ) \n        sent = Sentence ( ) \n        sentence_pos = 0 \n        for start , end in self . word_tokenizer . span_tokenize ( sent_str ) : \n            token_str = sent_str [ start : end ] . encode ( 'utf8' ) \n            tok = Token ( token_num = token_num , token = token_str , sentence_pos = sentence_pos , ) \n            tok . offsets [ OffsetType . CHARS ] = Offset ( type = OffsetType . CHARS , first = sent_start + start , length = end - start , ) \n            try : \n                label = self . label_index . find_le ( sent_start + start ) \n            except ValueError : \n                label = None \n            if label : \n                off = label . offsets [ OffsetType . CHARS ] \n                if off . first + off . length > sent_start + start : \n                    streamcorpus . add_annotation ( tok , label ) \n                    logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target_id ) \n                    if label in self . label_to_mention_id : \n                        mention_id = self . label_to_mention_id [ label ] \n                    else : \n                        mention_id = new_mention_id \n                        new_mention_id = new_mention_id + ( 1 ) \n                        self . label_to_mention_id [ label ] = mention_id \n                    tok . mention_id = mention_id \n            token_num = token_num + ( 1 ) \n            sentence_pos = sentence_pos + ( 1 ) \n            sent . tokens . append ( tok ) \n        sentences . append ( sent ) \n    return sentences "}
{"12785": "\ndef re_based_make_clean_visible ( html ) : \n    text = '' \n    html = fix_emails ( html ) \n    for m in invisible . finditer ( html ) : \n        text = text + ( m . group ( 'before' ) ) \n        text = text + ( ' ' * len ( m . group ( 'invisible' ) ) ) \n    assert len ( html ) >= len ( text ) , '%d !>= %d' % ( len ( html ) , len ( text ) ) \n    tail = len ( html ) - len ( text ) \n    text = text + ( html [ - tail : ] ) \n    assert len ( html ) == len ( text ) , '%d != %d' % ( len ( html ) , len ( text ) ) \n    return text "}
{"12789": "\ndef main ( ) : \n    import argparse \n    import sys \n    parser = argparse . ArgumentParser ( ) \n    parser . add_argument ( 'path' ) \n    args = parser . parse_args ( ) \n    html = open ( args . path ) . read ( ) \n    html = html . decode ( 'utf8' ) \n    cursor = 0 \n    for s in non_tag_chars_from_raw ( html ) : \n        for c in s : \n            if c != ' ' and c != html [ cursor ] : \n                import pdb ; \n                pdb . set_trace ( ) \n            sys . stdout . write ( c . encode ( 'utf8' ) ) \n            sys . stdout . flush ( ) \n            cursor = cursor + ( 1 ) "}
{"12799": "\ndef get_random_available ( self , max_iter = 10000 ) : \n    c = 1 \n    keeper = None \n    for row in self . _available . get_range ( row_count = max_iter , read_consistency_level = pycassa . ConsistencyLevel . ALL ) : \n        logger . debug ( 'considering %r' % ( row , ) ) \n        if random . random ( ) < 1 / c : \n            keeper = row [ 0 ] \n        if c == max_iter : \n            break \n        c = c + ( 1 ) \n    return keeper "}
{"12800": "\ndef tokens ( self , sentence_dom ) : \n    self . sent_pos = 0 \n    mention_id = 0 \n    while len ( sentence_dom . childNodes ) > 0 : \n        node = sentence_dom . childNodes . pop ( 0 ) \n        if node . nodeType == node . TEXT_NODE : \n            for line in node . data . splitlines ( True ) : \n                self . _input_string = line \n                for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                    tok = self . _make_token ( start , end ) \n                    if tok : \n                        yield tok \n                if line . endswith ( '\\n' ) : \n                    self . line_idx = self . line_idx + ( 1 ) \n                self . byte_idx = self . byte_idx + ( len ( line . encode ( 'utf-8' ) ) ) \n        else : \n            assert node . nodeName == 'ENAMEX' , node . nodeName \n            chain_id = node . attributes . get ( 'ID' ) . value \n            entity_type = node . attributes . get ( 'TYPE' ) . value \n            for node in node . childNodes : \n                assert node . nodeType == node . TEXT_NODE , node . nodeType \n                for line in node . data . splitlines ( True ) : \n                    self . _input_string = line \n                    for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                        tok = self . _make_token ( start , end ) \n                        if tok : \n                            if entity_type in _PRONOUNS : \n                                tok . mention_type = MentionType . PRO \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                                attr = Attribute ( attribute_type = AttributeType . PER_GENDER , value = str ( _PRONOUNS [ entity_type ] ) ) \n                                self . attributes . append ( attr ) \n                            else : \n                                tok . mention_type = MentionType . NAME \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                            tok . equiv_id = int ( chain_id ) \n                            tok . mention_id = mention_id \n                            yield tok \n                    if line . endswith ( '\\n' ) : \n                        self . line_idx = self . line_idx + ( 1 ) \n                    self . byte_idx = self . byte_idx + ( len ( line . encode ( 'utf-8' ) ) ) \n            mention_id = mention_id + ( 1 ) "}
{"12802": "\ndef _retry ( func ) : \n    def retry_func ( self , * args , ** kwargs ) : \n        tries = 1 \n        while True : \n            try : \n                return func ( self , * args , ** kwargs ) \n                break \n            except OSError as exc : \n                logger . error ( 'assuming OSError unrecoverable' ) \n                raise \n            except FailedExtraction as exc : \n                logger . error ( 'FAIL(%d)' , tries , exc_info = True ) \n                raise \n            except FailedVerification as exc : \n                logger . warn ( 'FAIL(%d)' , tries , exc_info = True ) \n                if tries >= self . config [ 'tries' ] : \n                    if self . config . get ( 'suppress_failures' ) : \n                        logger . warn ( 'suppressing failure and breaking out of this loop; data may be corrupt, downstream will have to cope' ) \n                        break \n                    else : \n                        raise \n            except Exception as exc : \n                logger . warn ( 'FAIL(%d): having I/O trouble with S3' , tries , exc_info = True ) \n                if tries >= self . config [ 'tries' ] : \n                    raise \n            logger . warn ( 'RETRYING (%d left)' , self . config [ 'tries' ] - tries ) \n            time . sleep ( 3 * tries ) \n            tries = tries + ( 1 ) \n    return retry_func "}
{"12815": "\ndef char_offsets_to_xpaths ( html , char_offsets ) : \n    html = uni ( html ) \n    parser = XpathTextCollector ( ) \n    prev_end = 0 \n    prev_progress = True \n    for start , end in char_offsets : \n        if start == end : \n            yield None \n            continue \n        if not prev_progress : \n            for i in xrange ( prev_end , start ) : \n                parser . feed ( html [ i ] ) \n                prev_end = prev_end + ( 1 ) \n                if parser . made_progress : \n                    break \n            if not parser . made_progress : \n                yield None \n                continue \n        if prev_end < start : \n            parser . feed ( html [ prev_end : start ] ) \n            if not parser . made_progress : \n                parser . feed ( html [ start : end ] ) \n                prev_progress = parser . made_progress \n                prev_end = end \n                yield None \n                continue \n        xstart = parser . xpath_offset ( ) \n        parser . feed ( html [ start : end ] ) \n        xend = parser . xpath_offset ( ) \n        prev_end = end \n        if not parser . made_progress : \n            prev_progress = False \n            yield None \n        else : \n            prev_progress = True \n            yield XpathRange ( xstart [ 0 ] , xstart [ 1 ] , xend [ 0 ] , xend [ 1 ] ) \n    parser . feed ( html [ prev_end : ] ) \n    parser . close ( ) "}
{"12816": "\ndef add_element ( self , tag ) : \n    if tag is TextElement and self . last_tag is TextElement : \n        return \n    self . last_tag = tag \n    if tag not in self . tags : \n        self . tags [ tag ] = 1 \n    else : \n        self . tags [ tag ] = self . tags [ tag ] + ( 1 ) "}
{"12818": "\ndef text_index ( self ) : \n    i = self . tags . get ( TextElement , 0 ) \n    if self . last_tag is not TextElement : \n        i = i + ( 1 ) \n    return i "}
{"12825": "\ndef make_pretty ( elem , depth = 0 , indent = '  ' ) : \n    depth = depth + ( 1 ) \n    updated_child_list = [ ] \n    updated_child_ix = 0 \n    for child in elem . xml_children : \n        if isinstance ( child , element ) : \n            if updated_child_ix % 2 : \n                updated_child_list . append ( child ) \n                updated_child_ix = updated_child_ix + ( 1 ) \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_list . append ( child ) \n                updated_child_ix = updated_child_ix + ( 2 ) \n            make_pretty ( child , depth ) \n        else : \n            if child . xml_value . strip ( ) : \n                updated_child_list . append ( child ) \n                updated_child_ix = updated_child_ix + ( 1 ) \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_ix = updated_child_ix + ( 1 ) \n    if not ( updated_child_ix % 2 ) : \n        new_text = text ( '\\n' + indent * ( depth - 1 ) , elem ) \n        updated_child_list . append ( new_text ) \n    elem . xml_children = updated_child_list \n    return elem "}
{"12827": "\ndef inkscape_export ( input_file , output_file , export_flag = \"-A\" , dpi = 90 , inkscape_binpath = None ) : \n    if not os . path . exists ( input_file ) : \n        log . error ( 'File {} not found.' . format ( input_file ) ) \n        raise IOError ( ( 0 , 'File not found.' , input_file ) ) \n    if '=' not in export_flag : \n        export_flag = export_flag + ( ' ' ) \n    arg_strings = [ ] \n    arg_strings = arg_strings + ( [ '--without-gui' ] ) \n    arg_strings = arg_strings + ( [ '--export-text-to-path' ] ) \n    arg_strings = arg_strings + ( [ '{}\"{}\"' . format ( export_flag , output_file ) ] ) \n    arg_strings = arg_strings + ( [ '--export-dpi={}' . format ( dpi ) ] ) \n    arg_strings = arg_strings + ( [ '\"{}\"' . format ( input_file ) ] ) \n    return call_inkscape ( arg_strings , inkscape_binpath = inkscape_binpath ) "}
{"12844": "\ndef execute ( option ) : \n    namelist_option = [ ] \n    makefile_option = [ ] \n    flags = \"\" \n    for entry in option : \n        key = entry . keys ( ) [ 0 ] \n        if key == \"Problem Size\" : \n            namelist_option . append ( { \"SIZE\" : entry [ key ] } ) \n        elif key == \"F90\" : \n            makefile_option . append ( entry ) \n        else : \n            flags = flags + ( entry [ key ] + \" \" ) \n    makefile_option . append ( { \"F90FLAGS\" : flags } ) \n    namelist = create_input ( namelist_option , \"namelist\" , template_location = \"templates\" ) \n    makefile_include = create_input ( makefile_option , \"Makefile.include\" , template_location = \"templates\" ) \n    benchmark_base = \"shallow\" \n    location = benchmark_base + \"/original/namelist\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( namelist ) \n    my_file . flush ( ) \n    location = benchmark_base + \"/common/Makefile.include\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( makefile_include ) \n    my_file . flush ( ) \n    base_path = benchmark_base + \"/original\" \n    import subprocess \n    make_process = subprocess . Popen ( [ \"make\" , \"clean\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"make\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"./shallow_base\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    stdout = make_process . stdout . read ( ) \n    for line in stdout . split ( \"\\n\" ) : \n        if \"Time-stepping\" in line : \n            total_time = line . split ( ) [ 2 ] \n    return True , total_time "}
{"12853": "\ndef add_extension_if_needed ( filepath , ext , check_if_exists = False ) : \n    if not filepath . endswith ( ext ) : \n        filepath = filepath + ( ext ) \n    if check_if_exists : \n        if not os . path . exists ( filepath ) : \n            err = 'File not found: ' + filepath \n            log . error ( err ) \n            raise IOError ( err ) \n    return filepath "}
{"12896": "\ndef tex2pdf ( tex_file , output_file = None , output_format = 'pdf' ) : \n    if not os . path . exists ( tex_file ) : \n        raise IOError ( 'Could not find file {}.' . format ( tex_file ) ) \n    if output_format != 'pdf' and output_format != 'dvi' : \n        raise ValueError ( \"Invalid output format given {}. Can only accept 'pdf' or 'dvi'.\" . format ( output_format ) ) \n    cmd_name = 'pdflatex' \n    check_command ( cmd_name ) \n    args_strings = [ cmd_name ] \n    if output_file is not None : \n        args_strings = args_strings + ( [ '-output-directory=\"{}\" ' . format ( os . path . abspath ( os . path . dirname ( output_file ) ) ) ] ) \n    result_dir = os . path . dirname ( output_file ) if output_file else os . path . dirname ( tex_file ) \n    args_strings = args_strings + ( [ '-output-format=\"{}\"' . format ( output_format ) ] ) \n    args_strings = args_strings + ( [ '\"' + tex_file + '\"' ] ) \n    log . debug ( 'Calling command {} with args: {}.' . format ( cmd_name , args_strings ) ) \n    ret = simple_call ( args_strings ) \n    result_file = os . path . join ( result_dir , remove_ext ( os . path . basename ( tex_file ) ) + '.' + output_format ) \n    if os . path . exists ( result_file ) : \n        shutil . move ( result_file , output_file ) \n    else : \n        raise IOError ( 'Could not find PDFLatex result file.' ) \n    log . debug ( 'Cleaning *.aux and *.log files from folder {}.' . format ( result_dir ) ) \n    cleanup ( result_dir , 'aux' ) \n    cleanup ( result_dir , 'log' ) \n    return ret "}
{"12900": "\ndef expand ( self , other ) : \n    if len ( other ) == 2 : \n        other = other + ( other ) \n    mid = len ( other ) // 2 \n    self . ll = map ( min , self . ll , other [ : mid ] ) \n    self . ur = map ( max , self . ur , other [ mid : ] ) "}
{"12976": "\ndef _send_reliable_message ( self , msg ) : \n    result = False \n    max_retries = 15 \n    trans_id = next ( LWLink . transaction_id ) \n    msg = \"%d,%s\" % ( trans_id , msg ) \n    try : \n        with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as write_sock , socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as read_sock : \n            write_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n            read_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_BROADCAST , 1 ) \n            read_sock . settimeout ( self . SOCKET_TIMEOUT ) \n            read_sock . bind ( ( '0.0.0.0' , self . RX_PORT ) ) \n            while max_retries : \n                max_retries = max_retries - ( 1 ) \n                write_sock . sendto ( msg . encode ( 'UTF-8' ) , ( LWLink . link_ip , self . TX_PORT ) ) \n                result = False \n                while True : \n                    response , dummy = read_sock . recvfrom ( 1024 ) \n                    response = response . decode ( 'UTF-8' ) \n                    if \"Not yet registered.\" in response : \n                        _LOGGER . error ( \"Not yet registered\" ) \n                        self . register ( ) \n                        result = True \n                        break \n                    if response . startswith ( \"%d,OK\" % trans_id ) : \n                        result = True \n                        break \n                    if response . startswith ( \"%d,ERR\" % trans_id ) : \n                        _LOGGER . error ( response ) \n                        break \n                    _LOGGER . info ( response ) \n                if result : \n                    break \n                time . sleep ( 0.25 ) \n    except socket . timeout : \n        _LOGGER . error ( \"LW broker timeout!\" ) \n        return result \n    except Exception as ex : \n        _LOGGER . error ( ex ) \n        raise \n    if result : \n        _LOGGER . info ( \"LW broker OK!\" ) \n    else : \n        _LOGGER . error ( \"LW broker fail!\" ) \n    return result "}
{"13010": "\ndef _load_job ( self ) : \n    try : \n        next_job = next ( self . _jobs ) \n    except StopIteration : \n        self . _on_deck = None \n    else : \n        if not isinstance ( next_job , Job ) : \n            next_job = DefaultJob ( next_job ) \n        self . _on_deck = next_job \n        self . _active_jobs = self . _active_jobs + ( 1 ) "}
{"13012": "\ndef add_result ( self , result ) : \n    if self . _active_jobs == 0 : \n        return \n    self . _results . add ( result ) \n    self . _active_jobs = self . _active_jobs - ( 1 ) \n    if self . _active_jobs == 0 : \n        self . _done ( ) "}
{"13092": "\ndef copy_dir ( bucket_name , src_path , dest_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None , surrogate_key = None , cache_control = None , surrogate_control = None , create_directory_redirect_object = True ) : \n    if not src_path . endswith ( '/' ) : \n        src_path = src_path + ( '/' ) \n    if not dest_path . endswith ( '/' ) : \n        dest_path = dest_path + ( '/' ) \n    common_prefix = os . path . commonprefix ( [ src_path , dest_path ] ) \n    if common_prefix == src_path : \n        msg = 'Common prefix {0} is same as source dir {1}' . format ( common_prefix , src_path ) \n        raise RuntimeError ( msg ) \n    if common_prefix == dest_path : \n        msg = 'Common prefix {0} is same as dest dir {1}' . format ( common_prefix , dest_path ) \n        raise RuntimeError ( msg ) \n    delete_dir ( bucket_name , dest_path , aws_access_key_id , aws_secret_access_key ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key , profile_name = aws_profile ) \n    s3 = session . resource ( 's3' ) \n    bucket = s3 . Bucket ( bucket_name ) \n    for src_obj in bucket . objects . filter ( Prefix = src_path ) : \n        src_rel_path = os . path . relpath ( src_obj . key , start = src_path ) \n        dest_key_path = os . path . join ( dest_path , src_rel_path ) \n        head = s3 . meta . client . head_object ( Bucket = bucket_name , Key = src_obj . key ) \n        metadata = head [ 'Metadata' ] \n        content_type = head [ 'ContentType' ] \n        if cache_control is None and 'CacheControl' in head : \n            cache_control = head [ 'CacheControl' ] \n        if surrogate_control is not None : \n            metadata [ 'surrogate-control' ] = surrogate_control \n        if surrogate_key is not None : \n            metadata [ 'surrogate-key' ] = surrogate_key \n        s3 . meta . client . copy_object ( Bucket = bucket_name , Key = dest_key_path , CopySource = { 'Bucket' : bucket_name , 'Key' : src_obj . key } , MetadataDirective = 'REPLACE' , Metadata = metadata , ACL = 'public-read' , CacheControl = cache_control , ContentType = content_type ) \n    if create_directory_redirect_object : \n        dest_dirname = dest_path . rstrip ( '/' ) \n        obj = bucket . Object ( dest_dirname ) \n        metadata = { 'dir-redirect' : 'true' } \n        obj . put ( Body = '' , ACL = 'public-read' , Metadata = metadata , CacheControl = cache_control ) "}
{"13110": "\ndef hotspots ( self ) : \n    rooted_leaf_samples , _ = self . live_data_copy ( ) \n    line_samples = { } \n    for _ , counts in rooted_leaf_samples . items ( ) : \n        for key , count in counts . items ( ) : \n            line_samples . setdefault ( key , 0 ) \n            line_samples [ key ] = line_samples [ key ] + ( count ) \n    return sorted ( line_samples . items ( ) , key = lambda v : v [ 1 ] , reverse = True ) "}
{"13129": "\ndef _json_to_html ( self , slug , json_data ) : \n    html = '<div id=\"chart-' + slug + '\"></div>' \n    html = html + ( '<script>' ) \n    html = html + ( 'var s' + slug + ' = ' + json_data + ';' ) \n    html = html + ( 'vega.embed(\"#chart-' + slug + '\", s' + slug + ');' ) \n    html = html + ( '</script>' ) \n    return html "}
{"13138": "\ndef iter_attribute ( iterable_name ) -> Union [ Iterable , Callable ] : \n    def create_new_class ( decorated_class ) -> Union [ Iterable , Callable ] : \n        assert inspect . isclass ( decorated_class ) , 'You can only decorate class objects!' \n        assert isinstance ( iterable_name , str ) , 'Please provide attribute name string' \n        decorated_class . iterator_attr_index = 0 \n        def __iter__ ( instance ) -> Iterable : \n            return instance \n        def __next__ ( instance ) -> Any : \n            assert hasattr ( instance , iterable_name ) , 'Decorated object does not have attribute named {}' . format ( iterable_name ) \n            assert isinstance ( getattr ( instance , iterable_name ) , collections . Iterable ) , '{} of object {} is not iterable' . format ( iterable_name , instance . __class__ . __name__ ) \n            ind = instance . iterator_attr_index \n            while ind < len ( getattr ( instance , iterable_name ) ) : \n                val = getattr ( instance , iterable_name ) [ ind ] \n                instance . iterator_attr_index = instance . iterator_attr_index + ( 1 ) \n                return val \n            instance . iterator_attr_index = 0 \n            raise StopIteration \n        dct = dict ( decorated_class . __dict__ ) \n        dct [ '__iter__' ] = __iter__ \n        dct [ '__next__' ] = __next__ \n        dct [ 'iterator_attr_index' ] = decorated_class . iterator_attr_index \n        return type ( decorated_class . __name__ , ( collections . Iterable , ) , dct ) \n    return create_new_class "}
{"13148": "\ndef unique ( func , num_args = 0 , max_attempts = 100 , cache = None ) : \n    if cache is None : \n        cache = _cache_unique \n    \n    @ wraps ( func ) \n    def wrapper ( * args ) : \n        key = \"%s_%s\" % ( str ( func . __name__ ) , str ( args [ : num_args ] ) ) \n        attempt = 0 \n        while attempt < max_attempts : \n            attempt = attempt + ( 1 ) \n            drawn = cache . get ( key , [ ] ) \n            result = func ( * args ) \n            if result not in drawn : \n                drawn . append ( result ) \n                cache [ key ] = drawn \n                return result \n        raise MaxAttemptException ( ) \n    return wrapper "}
{"13173": "\ndef save ( self , fname = None ) : \n    if not fname : \n        fname = self . getname ( ) \n    assert fname != None , 'You must specify a filename to save to' \n    if not fname . endswith ( '.png' ) : \n        fname = fname + ( '.png' ) \n    try : \n        urlretrieve ( self . url , fname ) \n    except Exception : \n        raise IOError ( 'Problem saving %s to file' % fname ) \n    return fname "}
{"13191": "\ndef _letter_map ( word ) : \n    lmap = { } \n    for letter in word : \n        try : \n            lmap [ letter ] = lmap [ letter ] + ( 1 ) \n        except KeyError : \n            lmap [ letter ] = 1 \n    return lmap "}
{"13192": "\ndef anagrams_in_word ( word , sowpods = False , start = \"\" , end = \"\" ) : \n    input_letters , blanks , questions = blank_tiles ( word ) \n    for tile in start + end : \n        input_letters . append ( tile ) \n    for word in word_list ( sowpods , start , end ) : \n        lmap = _letter_map ( input_letters ) \n        used_blanks = 0 \n        for letter in word : \n            if letter in lmap : \n                lmap [ letter ] = lmap [ letter ] - ( 1 ) \n                if lmap [ letter ] < 0 : \n                    used_blanks = used_blanks + ( 1 ) \n                    if used_blanks > ( blanks + questions ) : \n                        break \n            else : \n                used_blanks = used_blanks + ( 1 ) \n                if used_blanks > ( blanks + questions ) : \n                    break \n        else : \n            yield ( word , word_score ( word , input_letters , questions ) ) "}
{"13268": "\ndef _cmdRegex ( self , cmd_grp = None ) : \n    cmd_grp = cmd_grp or \"cmd\" \n    help_opts = ( \"-h\" , \"--help\" ) \n    cmd = self . name ( ) \n    names = \"|\" . join ( [ re . escape ( cmd ) ] + [ re . escape ( a ) for a in self . aliases ( ) ] ) \n    opts = [ ] \n    for action in self . parser . _actions : \n        opts = opts + ( [ a for a in action . option_strings if a not in help_opts ] ) \n    opts_re = \"|\" . join ( [ re . escape ( o ) for o in opts ] ) \n    if opts_re : \n        opts_re = rf\"(\\s+(?P<{cmd_grp}_opts>{opts_re}))*\" \n    help_re = \"|\" . join ( [ re . escape ( o ) for o in help_opts ] ) \n    help_re = rf\"(\\s+(?P<HELP_OPTS>{help_re}))*\" \n    completers = { } \n    if opts_re : \n        completers [ f\"{cmd_grp}_opts\" ] = WordCompleter ( opts ) \n    return tuple ( [ rf\"\"\"(?P<{cmd_grp}>{names}){opts_re}{help_re}\"\"\" , completers ] ) "}
{"13273": "\ndef dump ( filename , dbname , username = None , password = None , host = None , port = None , tempdir = '/tmp' , pg_dump_path = 'pg_dump' , format = 'p' ) : \n    filepath = os . path . join ( tempdir , filename ) \n    cmd = pg_dump_path \n    cmd = cmd + ( ' --format %s' % format ) \n    cmd = cmd + ( ' --file ' + os . path . join ( tempdir , filename ) ) \n    if username : \n        cmd = cmd + ( ' --username %s' % username ) \n    if host : \n        cmd = cmd + ( ' --host %s' % host ) \n    if port : \n        cmd = cmd + ( ' --port %s' % port ) \n    cmd = cmd + ( ' ' + dbname ) \n    if password : \n        os . environ [ \"PGPASSWORD\" ] = password \n    return sh ( cmd ) "}
{"13288": "\ndef word_score ( word , input_letters , questions = 0 ) : \n    score = 0 \n    bingo = 0 \n    filled_by_blanks = [ ] \n    rack = list ( input_letters ) \n    for letter in word : \n        if letter in rack : \n            bingo = bingo + ( 1 ) \n            score = score + ( letter_score ( letter ) ) \n            rack . remove ( letter ) \n        else : \n            filled_by_blanks . append ( letter_score ( letter ) ) \n    for blank_score in sorted ( filled_by_blanks , reverse = True ) : \n        if questions > 0 : \n            score = score + ( blank_score ) \n            questions = questions - ( 1 ) \n    if bingo > 6 : \n        score = score + ( 50 ) \n    return score "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9 , \"b\" : 2 , \"c\" : 2 , \"d\" : 4 , \"e\" : 12 , \"f\" : 2 , \"g\" : 3 , \"h\" : 2 , \"i\" : 9 , \"j\" : 1 , \"k\" : 1 , \"l\" : 4 , \"m\" : 2 , \"n\" : 6 , \"o\" : 8 , \"p\" : 2 , \"q\" : 1 , \"r\" : 6 , \"s\" : 4 , \"t\" : 6 , \"u\" : 4 , \"v\" : 2 , \"w\" : 2 , \"x\" : 1 , \"y\" : 2 , \"z\" : 1 , \"_\" : 2 , } \n    for letter in word : \n        if letter == \"?\" : \n            continue \n        try : \n            letters_in_bag [ letter ] = letters_in_bag [ letter ] - ( 1 ) \n        except KeyError : \n            return False \n        if letters_in_bag [ letter ] < 0 : \n            letters_in_bag [ \"_\" ] = letters_in_bag [ \"_\" ] - ( 1 ) \n            if letters_in_bag [ \"_\" ] < 0 : \n                return False \n    return True "}
{"13331": "\ndef dump ( filename , dbname , username = None , password = None , host = None , port = None , tempdir = '/tmp' , mysqldump_path = 'mysqldump' ) : \n    filepath = os . path . join ( tempdir , filename ) \n    cmd = mysqldump_path \n    cmd = cmd + ( ' --result-file=' + os . path . join ( tempdir , filename ) ) \n    if username : \n        cmd = cmd + ( ' --user=%s' % username ) \n    if host : \n        cmd = cmd + ( ' --host=%s' % host ) \n    if port : \n        cmd = cmd + ( ' --port=%s' % port ) \n    if password : \n        cmd = cmd + ( ' --password=%s' % password ) \n    cmd = cmd + ( ' ' + dbname ) \n    return sh ( cmd ) "}
{"13369": "\ndef raise_ ( self , reply_socket , channel , exc_info = None ) : \n    if not reply_socket : \n        return \n    if exc_info is None : \n        exc_info = sys . exc_info ( ) \n    exc_type , exc , tb = exc_info \n    while tb . tb_next is not None : \n        tb = tb . tb_next \n    if issubclass ( exc_type , RemoteException ) : \n        exc_type = exc_type . exc_type \n    filename , lineno = tb . tb_frame . f_code . co_filename , tb . tb_lineno \n    val = ( exc_type , str ( exc ) , filename , lineno ) \n    try : \n        state = exc . __getstate__ ( ) \n    except AttributeError : \n        pass \n    else : \n        val = val + ( ( state , ) ) \n    self . send_reply ( reply_socket , RAISE , val , * channel ) "}
{"13371": "\ndef establish ( self , call_id , timeout , limit = None , retry = None , max_retries = None ) : \n    rejected = 0 \n    retried = 0 \n    results = [ ] \n    result_queue = self . result_queues [ call_id ] \n    try : \n        with Timeout ( timeout , False ) : \n            while True : \n                result = result_queue . get ( ) \n                if result is None : \n                    rejected = rejected + ( 1 ) \n                    if retry is not None : \n                        if retried == max_retries : \n                            break \n                        retry ( ) \n                        retried = retried + ( 1 ) \n                    continue \n                results . append ( result ) \n                if len ( results ) == limit : \n                    break \n    finally : \n        del result_queue \n        self . remove_result_queue ( call_id ) \n    if not results : \n        if rejected : \n            raise Rejected ( '%d workers rejected' % rejected if rejected != 1 else 'A worker rejected' ) \n        else : \n            raise WorkerNotFound ( 'failed to find worker' ) \n    return results "}
{"13393": "\ndef _range_filters ( self , * key_ranges ) : \n    filters = [ ] \n    for s , e in key_ranges : \n        if isinstance ( s , basestring ) : \n            s = eid ( s ) \n        if isinstance ( e , basestring ) : \n            e = e + ( u'\\U0010FFFF' ) \n            e = eid ( e ) \n        if s == ( ) and e == ( ) : \n            filters . append ( { 'match_all' : { } } ) \n        elif e == ( ) : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s } } } ) \n        elif s == ( ) : \n            filters . append ( { 'range' : { '_id' : { 'lte' : e } } } ) \n        else : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s , 'lte' : e } } } ) \n    if len ( filters ) == 0 : \n        return [ { 'match_all' : { } } ] \n    else : \n        return filters "}
{"13399": "\ndef fc_bytes ( self , fc_dict ) : \n    num_bytes = 0 \n    for _ , feat in fc_dict . iteritems ( ) : \n        num_bytes = num_bytes + ( len ( feat ) ) \n    return num_bytes "}
{"13400": "\ndef count_bytes ( self , filter_preds ) : \n    num_bytes = defaultdict ( int ) \n    for hit in self . _scan ( ) : \n        for filter_pred in filter_preds : \n            if filter_pred ( did ( hit [ '_id' ] ) ) : \n                num_bytes [ filter_pred ] = num_bytes [ filter_pred ] + ( self . fc_bytes ( hit [ '_source' ] [ 'fc' ] ) ) \n    return num_bytes "}
{"13404": "\ndef pretty_to_link ( inst , link ) : \n    values = '' \n    prefix = '' \n    metaclass = xtuml . get_metaclass ( inst ) \n    for name , ty in metaclass . attributes : \n        if name in link . key_map : \n            value = getattr ( inst , name ) \n            value = xtuml . serialize_value ( value , ty ) \n            name = link . key_map [ name ] \n            values = values + ( '%s%s=%s' % ( prefix , name , value ) ) \n            prefix = ', ' \n    return '%s(%s)' % ( link . kind , values ) "}
{"13405": "\ndef pretty_unique_identifier ( inst , identifier ) : \n    values = '' \n    prefix = '' \n    metaclass = xtuml . get_metaclass ( inst ) \n    for name , ty in metaclass . attributes : \n        if name in metaclass . identifying_attributes : \n            value = getattr ( inst , name ) \n            value = xtuml . serialize_value ( value , ty ) \n            values = values + ( '%s%s=%s' % ( prefix , name , value ) ) \n            prefix = ', ' \n    return '%s(%s)' % ( identifier , values ) "}
{"13406": "\ndef check_uniqueness_constraint ( m , kind = None ) : \n    if kind is None : \n        metaclasses = m . metaclasses . values ( ) \n    else : \n        metaclasses = [ m . find_metaclass ( kind ) ] \n    res = 0 \n    for metaclass in metaclasses : \n        id_map = dict ( ) \n        for identifier in metaclass . indices : \n            id_map [ identifier ] = dict ( ) \n        for inst in metaclass . select_many ( ) : \n            for name , ty in metaclass . attributes : \n                if name not in metaclass . identifying_attributes : \n                    continue \n                value = getattr ( inst , name ) \n                isnull = value is None \n                isnull |= ( ty == 'UNIQUE_ID' and not value ) \n                if isnull : \n                    res = res + ( 1 ) \n                    logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) \n            for identifier in metaclass . indices : \n                kwargs = dict ( ) \n                for name in metaclass . indices [ identifier ] : \n                    kwargs [ name ] = getattr ( inst , name ) \n                index_key = frozenset ( kwargs . items ( ) ) \n                if index_key in id_map [ identifier ] : \n                    res = res + ( 1 ) \n                    id_string = pretty_unique_identifier ( inst , identifier ) \n                    logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id_string ) ) \n                id_map [ identifier ] [ index_key ] = inst \n    return res "}
{"13407": "\ndef check_link_integrity ( m , link ) : \n    res = 0 \n    for inst in link . from_metaclass . select_many ( ) : \n        q_set = list ( link . navigate ( inst ) ) \n        if ( len ( q_set ) < 1 and not link . conditional ) or ( ( len ( q_set ) > 1 and not link . many ) ) : \n            res = res + ( 1 ) \n            logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty_from_link ( inst , link ) , link . rel_id , pretty_to_link ( inst , link ) ) ) \n    return res "}
{"13408": "\ndef check_subtype_integrity ( m , super_kind , rel_id ) : \n    if isinstance ( rel_id , int ) : \n        rel_id = 'R%d' % rel_id \n    res = 0 \n    for inst in m . select_many ( super_kind ) : \n        if not xtuml . navigate_subtype ( inst , rel_id ) : \n            res = res + ( 1 ) \n            logger . warning ( 'integrity violation across ' '%s[%s]' % ( super_kind , rel_id ) ) \n    return res "}
{"13463": "\ndef serialize_association ( ass ) : \n    s1 = '%s %s (%s)' % ( ass . source_link . cardinality , ass . source_link . to_metaclass . kind , ', ' . join ( ass . source_keys ) ) \n    if ass . target_link . phrase : \n        s1 = s1 + ( \" PHRASE '%s'\" % ass . target_link . phrase ) \n    s2 = '%s %s (%s)' % ( ass . target_link . cardinality , ass . target_link . to_metaclass . kind , ', ' . join ( ass . target_keys ) ) \n    if ass . source_link . phrase : \n        s2 = s2 + ( \" PHRASE '%s'\" % ass . source_link . phrase ) \n    return 'CREATE ROP REF_ID %s FROM %s TO %s;\\n' % ( ass . rel_id , s1 , s2 ) "}
{"13464": "\ndef serialize_class ( Cls ) : \n    metaclass = xtuml . get_metaclass ( Cls ) \n    attributes = [ '%s %s' % ( name , ty . upper ( ) ) for name , ty in metaclass . attributes ] \n    s = 'CREATE TABLE %s (\\n    ' % metaclass . kind \n    s = s + ( ',\\n    ' . join ( attributes ) ) \n    s = s + ( '\\n);\\n' ) \n    return s "}
{"13491": "\ndef _find_match ( self , position ) : \n    document = self . _text_edit . document ( ) \n    start_char = document . characterAt ( position ) \n    search_char = self . _opening_map . get ( start_char ) \n    if search_char : \n        increment = 1 \n    else : \n        search_char = self . _closing_map . get ( start_char ) \n        if search_char : \n            increment = - 1 \n        else : \n            return - 1 \n    char = start_char \n    depth = 0 \n    while position >= 0 and position < document . characterCount ( ) : \n        if char == start_char : \n            depth = depth + ( 1 ) \n        elif char == search_char : \n            depth = depth - ( 1 ) \n        if depth == 0 : \n            break \n        position = position + ( increment ) \n        char = document . characterAt ( position ) \n    else : \n        position = - 1 \n    return position "}
{"13505": "\ndef history_previous ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while index > 0 : \n        index = index - ( 1 ) \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13506": "\ndef history_next ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while self . _history_index < len ( self . _history ) : \n        index = index + ( 1 ) \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13541": "\ndef get_system_cpu_times ( ) : \n    user , system , idle = 0 , 0 , 0 \n    for cpu_time in _psutil_mswindows . get_system_cpu_times ( ) : \n        user = user + ( cpu_time [ 0 ] ) \n        system = system + ( cpu_time [ 1 ] ) \n        idle = idle + ( cpu_time [ 2 ] ) \n    return _cputimes_ntuple ( user , system , idle ) "}
{"13555": "\ndef indented_short_title ( self , item ) : \n    r = \"\" \n    if hasattr ( item , 'get_absolute_url' ) : \n        r = '<input type=\"hidden\" class=\"medialibrary_file_path\" value=\"%s\" />' % item . get_absolute_url ( ) \n    editable_class = '' \n    if not getattr ( item , 'feincms_editable' , True ) : \n        editable_class = ' tree-item-not-editable' \n    r = r + ( '<span id=\"page_marker-%d\" class=\"page_marker%s\" style=\"width: %dpx;\">&nbsp;</span>&nbsp;' % ( item . id , editable_class , 14 + item . level * 18 ) ) \n    if hasattr ( item , 'short_title' ) : \n        r = r + ( item . short_title ( ) ) \n    else : \n        r = r + ( unicode ( item ) ) \n    return mark_safe ( r ) "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if inisp < indent_spaces : \n        indent_spaces = inisp \n        if indent_spaces <= 0 : \n            full_dedent = True \n    if line . rstrip ( ) [ - 1 ] == ':' : \n        indent_spaces = indent_spaces + ( 4 ) \n    elif dedent_re . match ( line ) : \n        indent_spaces = indent_spaces - ( 4 ) \n        if indent_spaces <= 0 : \n            full_dedent = True \n    if indent_spaces < 0 : \n        indent_spaces = 0 \n    return indent_spaces , full_dedent "}
{"13613": "\ndef energy ( self , state = None ) : \n    state = self . state if state is None else state \n    route = state \n    e = 0 \n    if self . distance_matrix : \n        for i in range ( len ( route ) ) : \n            e = e + ( self . distance_matrix [ \"{},{}\" . format ( route [ i - 1 ] , route [ i ] ) ] ) \n    else : \n        for i in range ( len ( route ) ) : \n            e = e + ( distance ( self . cities [ route [ i - 1 ] ] , self . cities [ route [ i ] ] ) ) \n    return e "}
{"13620": "\ndef table ( rows ) : \n    output = '<table>' \n    for row in rows : \n        output = output + ( '<tr>' ) \n        for column in row : \n            output = output + ( '<td>{s}</td>' . format ( s = column ) ) \n        output = output + ( '</tr>' ) \n    output = output + ( '</table>' ) \n    return output "}
{"13621": "\ndef link ( url , text = '' , classes = '' , target = '' , get = \"\" , ** kwargs ) : \n    if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : \n        urlargs = { } \n        for arg , val in kwargs . items ( ) : \n            if arg [ : 4 ] == \"url_\" : \n                urlargs [ arg [ 4 : ] ] = val \n        url = reverse ( url , kwargs = urlargs ) \n        if get : \n            url = url + ( '?' + get ) \n    return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } ) "}
{"13635": "\ndef _find_url_name ( self , index_url , url_name , req ) : \n    if not index_url . url . endswith ( '/' ) : \n        index_url . url = index_url . url + ( '/' ) \n    page = self . _get_page ( index_url , req ) \n    if page is None : \n        logger . critical ( 'Cannot fetch index base URL %s' , index_url ) \n        return \n    norm_name = normalize_name ( req . url_name ) \n    for link in page . links : \n        base = posixpath . basename ( link . path . rstrip ( '/' ) ) \n        if norm_name == normalize_name ( base ) : \n            logger . debug ( 'Real name of requirement %s is %s' , url_name , base , ) \n            return base \n    return None "}
{"13695": "\ndef run_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    if silent : \n        store_history = False \n    self . input_splitter . push ( raw_cell ) \n    if self . input_splitter . cell_magic_parts : \n        self . _current_cell_magic_body = '' . join ( self . input_splitter . cell_magic_parts ) \n    cell = self . input_splitter . source_reset ( ) \n    with self . builtin_trap : \n        prefilter_failed = False \n        if len ( cell . splitlines ( ) ) == 1 : \n            try : \n                cell = self . prefilter_manager . prefilter_lines ( cell ) + '\\n' \n            except AliasError as e : \n                error ( e ) \n                prefilter_failed = True \n            except Exception : \n                self . showtraceback ( ) \n                prefilter_failed = True \n        if store_history : \n            self . history_manager . store_inputs ( self . execution_count , cell , raw_cell ) \n        if not silent : \n            self . logger . log ( cell , raw_cell ) \n        if not prefilter_failed : \n            cell_name = self . compile . cache ( cell , self . execution_count ) \n            with self . display_trap : \n                try : \n                    code_ast = self . compile . ast_parse ( cell , filename = cell_name ) \n                except IndentationError : \n                    self . showindentationerror ( ) \n                    if store_history : \n                        self . execution_count = self . execution_count + ( 1 ) \n                    return None \n                except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n                    self . showsyntaxerror ( ) \n                    if store_history : \n                        self . execution_count = self . execution_count + ( 1 ) \n                    return None \n                interactivity = \"none\" if silent else self . ast_node_interactivity \n                self . run_ast_nodes ( code_ast . body , cell_name , interactivity = interactivity ) \n                post_exec = [ ] if silent else self . _post_execute . iteritems ( ) \n                for func , status in post_exec : \n                    if self . disable_failing_post_execute and not status : \n                        continue \n                    try : \n                        func ( ) \n                    except KeyboardInterrupt : \n                        print >> io . stderr , \"\\nKeyboardInterrupt\" \n                    except Exception : \n                        self . _post_execute [ func ] = False \n                        self . showtraceback ( ) \n                        print >> io . stderr , '\\n' . join ( [ \"post-execution function %r produced an error.\" % func , \"If this problem persists, you can disable failing post-exec functions with:\" , \"\" , \"    get_ipython().disable_failing_post_execute = True\" ] ) \n    if store_history : \n        self . history_manager . store_output ( self . execution_count ) \n        self . execution_count = self . execution_count + ( 1 ) "}
{"13738": "\ndef highlight_text ( needles , haystack , cls_name = 'highlighted' , words = False , case = False ) : \n    if not needles : \n        return haystack \n    if not haystack : \n        return '' \n    if words : \n        pattern = r\"(%s)\" % \"|\" . join ( [ '\\\\b{}\\\\b' . format ( re . escape ( n ) ) for n in needles ] ) \n    else : \n        pattern = r\"(%s)\" % \"|\" . join ( [ re . escape ( n ) for n in needles ] ) \n    if case : \n        regex = re . compile ( pattern ) \n    else : \n        regex = re . compile ( pattern , re . I ) \n    i , out = 0 , \"\" \n    for m in regex . finditer ( haystack ) : \n        out = out + ( \"\" . join ( [ haystack [ i : m . start ( ) ] , '<span class=\"%s\">' % cls_name , haystack [ m . start ( ) : m . end ( ) ] , \"</span>\" ] ) ) \n        i = m . end ( ) \n    return mark_safe ( out + haystack [ i : ] ) "}
{"13757": "\ndef _default_pprint ( obj , p , cycle ) : \n    klass = getattr ( obj , '__class__' , None ) or type ( obj ) \n    if getattr ( klass , '__repr__' , None ) not in _baseclass_reprs : \n        p . text ( repr ( obj ) ) \n        return \n    p . begin_group ( 1 , '<' ) \n    p . pretty ( klass ) \n    p . text ( ' at 0x%x' % id ( obj ) ) \n    if cycle : \n        p . text ( ' ...' ) \n    elif p . verbose : \n        first = True \n        for key in dir ( obj ) : \n            if not key . startswith ( '_' ) : \n                try : \n                    value = getattr ( obj , key ) \n                except AttributeError : \n                    continue \n                if isinstance ( value , types . MethodType ) : \n                    continue \n                if not first : \n                    p . text ( ',' ) \n                p . breakable ( ) \n                p . text ( key ) \n                p . text ( '=' ) \n                step = len ( key ) + 1 \n                p . indentation = p . indentation + ( step ) \n                p . pretty ( value ) \n                p . indentation = p . indentation - ( step ) \n                first = False \n    p . end_group ( 1 , '>' ) "}
{"13767": "\ndef text ( self , obj ) : \n    width = len ( obj ) \n    if self . buffer : \n        text = self . buffer [ - 1 ] \n        if not isinstance ( text , Text ) : \n            text = Text ( ) \n            self . buffer . append ( text ) \n        text . add ( obj , width ) \n        self . buffer_width = self . buffer_width + ( width ) \n        self . _break_outer_groups ( ) \n    else : \n        self . output . write ( obj ) \n        self . output_width = self . output_width + ( width ) "}
{"13768": "\ndef breakable ( self , sep = ' ' ) : \n    width = len ( sep ) \n    group = self . group_stack [ - 1 ] \n    if group . want_break : \n        self . flush ( ) \n        self . output . write ( self . newline ) \n        self . output . write ( ' ' * self . indentation ) \n        self . output_width = self . indentation \n        self . buffer_width = 0 \n    else : \n        self . buffer . append ( Breakable ( sep , width , self ) ) \n        self . buffer_width = self . buffer_width + ( width ) \n        self . _break_outer_groups ( ) "}
{"13769": "\ndef end_group ( self , dedent = 0 , close = '' ) : \n    self . indentation = self . indentation - ( dedent ) \n    group = self . group_stack . pop ( ) \n    if not group . breakables : \n        self . group_queue . remove ( group ) \n    if close : \n        self . text ( close ) "}
{"13770": "\ndef flush ( self ) : \n    for data in self . buffer : \n        self . output_width = self . output_width + ( data . output ( self . output , self . output_width ) ) \n    self . buffer . clear ( ) \n    self . buffer_width = 0 "}
{"13825": "\ndef _function_magic_marker ( magic_kind ) : \n    validate_type ( magic_kind ) \n    def magic_deco ( arg ) : \n        call = lambda f , * a , ** k : f ( * a , ** k ) \n        caller = sys . _getframe ( 1 ) \n        for ns in [ 'f_locals' , 'f_globals' , 'f_builtins' ] : \n            get_ipython = getattr ( caller , ns ) . get ( 'get_ipython' ) \n            if get_ipython is not None : \n                break \n        else : \n            raise NameError ( 'Decorator can only run in context where ' '`get_ipython` exists' ) \n        ip = get_ipython ( ) \n        if callable ( arg ) : \n            func = arg \n            name = func . func_name \n            ip . register_magic_function ( func , magic_kind , name ) \n            retval = decorator ( call , func ) \n        elif isinstance ( arg , basestring ) : \n            name = arg \n            def mark ( func , * a , ** kw ) : \n                ip . register_magic_function ( func , magic_kind , name ) \n                return decorator ( call , func ) \n            retval = mark \n        else : \n            raise TypeError ( \"Decorator can only be called with \" \"string or function\" ) \n        return retval \n    ds = _docstring_template . format ( 'function' , magic_kind ) \n    ds = ds + ( dedent ( \"\"\"    Note: this decorator can only be used in a context where IPython is already    active, so that the `get_ipython()` call succeeds.  You can therefore use    it in your startup files loaded after IPython initializes, but *not* in the    IPython configuration file itself, which is executed before IPython is    fully up and running.  Any file located in the `startup` subdirectory of    your configuration profile will be OK in this sense.    \"\"\" ) ) \n    magic_deco . __doc__ = ds \n    return magic_deco "}
{"13838": "\ndef run ( self , message ) : \n    the_callable = self . func_from_info ( ) \n    try : \n        task_message = dict ( task = self , channel_message = message , ) \n        the_callable ( task_message ) \n    finally : \n        if self . end_running < self . next_run : \n            self . enabled = False \n            Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            return \n        if self . iterations == 0 : \n            return \n        else : \n            self . iterations = self . iterations - ( 1 ) \n            if self . iterations == 0 : \n                self . enabled = False \n                Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            self . save ( ) "}
{"13844": "\ndef timid ( ctxt , test , key = None , check = False , exts = None ) : \n    if exts is None : \n        exts = extensions . ExtensionSet ( ) \n    ctxt . emit ( 'Reading test steps from %s%s...' % ( test , '[%s]' % key if key else '' ) , debug = True ) \n    ctxt . steps = ctxt . steps + ( exts . read_steps ( ctxt , steps . Step . parse_file ( ctxt , test , key ) ) ) \n    if check : \n        return None \n    for idx , step in enumerate ( ctxt . steps ) : \n        ctxt . emit ( '[Step %d]: %s . . .' % ( idx , step . name ) ) \n        if exts . pre_step ( ctxt , step , idx ) : \n            ctxt . emit ( '[Step %d]: `- Step %s' % ( idx , steps . states [ steps . SKIPPED ] ) ) \n            continue \n        result = step ( ctxt ) \n        exts . post_step ( ctxt , step , idx , result ) \n        ctxt . emit ( '[Step %d]: `- Step %s%s' % ( idx , steps . states [ result . state ] , ' (ignored)' if result . ignore else '' ) ) \n        if not result : \n            msg = 'Test step failure' \n            if result . msg : \n                msg = msg + ( ': %s' % result . msg ) \n            return msg \n    return None "}
{"13863": "\ndef base_launch_kernel ( code , fname , stdin = None , stdout = None , stderr = None , executable = None , independent = False , extra_arguments = [ ] , cwd = None ) : \n    if executable is None : \n        executable = sys . executable \n    arguments = [ executable , '-c' , code , '-f' , fname ] \n    arguments . extend ( extra_arguments ) \n    redirect_in = True \n    _stdin = PIPE if stdin is None else stdin \n    redirect_out = sys . executable . endswith ( 'pythonw.exe' ) \n    if redirect_out : \n        _stdout = PIPE if stdout is None else stdout \n        _stderr = PIPE if stderr is None else stderr \n    else : \n        _stdout , _stderr = stdout , stderr \n    if sys . platform == 'win32' : \n        interrupt_event = ParentPollerWindows . create_interrupt_event ( ) \n        arguments = arguments + ( [ '--interrupt=%i' % interrupt_event ] ) \n        if executable . endswith ( 'pythonw.exe' ) : \n            if stdout is None : \n                arguments . append ( '--no-stdout' ) \n            if stderr is None : \n                arguments . append ( '--no-stderr' ) \n        if independent : \n            proc = Popen ( arguments , creationflags = 512 , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        else : \n            try : \n                from _winapi import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            except : \n                from _subprocess import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            pid = GetCurrentProcess ( ) \n            handle = DuplicateHandle ( pid , pid , pid , 0 , True , DUPLICATE_SAME_ACCESS ) \n            proc = Popen ( arguments + [ '--parent=%i' % int ( handle ) ] , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        proc . win32_interrupt_event = interrupt_event \n    else : \n        if independent : \n            proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n        else : \n            proc = Popen ( arguments + [ '--parent=1' ] , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    if redirect_in : \n        if stdin is None : \n            proc . stdin . close ( ) \n    if redirect_out : \n        if stdout is None : \n            proc . stdout . close ( ) \n        if stderr is None : \n            proc . stderr . close ( ) \n    return proc "}
{"13871": "\ndef report ( self , morfs , outfile = None ) : \n    outfile = outfile or sys . stdout \n    impl = xml . dom . minidom . getDOMImplementation ( ) \n    docType = impl . createDocumentType ( \"coverage\" , None , \"http://cobertura.sourceforge.net/xml/coverage-03.dtd\" ) \n    self . xml_out = impl . createDocument ( None , \"coverage\" , docType ) \n    xcoverage = self . xml_out . documentElement \n    xcoverage . setAttribute ( \"version\" , __version__ ) \n    xcoverage . setAttribute ( \"timestamp\" , str ( int ( time . time ( ) * 1000 ) ) ) \n    xcoverage . appendChild ( self . xml_out . createComment ( \" Generated by coverage.py: %s \" % __url__ ) ) \n    xpackages = self . xml_out . createElement ( \"packages\" ) \n    xcoverage . appendChild ( xpackages ) \n    self . packages = { } \n    self . report_files ( self . xml_file , morfs ) \n    lnum_tot , lhits_tot = 0 , 0 \n    bnum_tot , bhits_tot = 0 , 0 \n    for pkg_name in sorted ( self . packages . keys ( ) ) : \n        pkg_data = self . packages [ pkg_name ] \n        class_elts , lhits , lnum , bhits , bnum = pkg_data \n        xpackage = self . xml_out . createElement ( \"package\" ) \n        xpackages . appendChild ( xpackage ) \n        xclasses = self . xml_out . createElement ( \"classes\" ) \n        xpackage . appendChild ( xclasses ) \n        for class_name in sorted ( class_elts . keys ( ) ) : \n            xclasses . appendChild ( class_elts [ class_name ] ) \n        xpackage . setAttribute ( \"name\" , pkg_name . replace ( os . sep , '.' ) ) \n        xpackage . setAttribute ( \"line-rate\" , rate ( lhits , lnum ) ) \n        xpackage . setAttribute ( \"branch-rate\" , rate ( bhits , bnum ) ) \n        xpackage . setAttribute ( \"complexity\" , \"0\" ) \n        lnum_tot = lnum_tot + ( lnum ) \n        lhits_tot = lhits_tot + ( lhits ) \n        bnum_tot = bnum_tot + ( bnum ) \n        bhits_tot = bhits_tot + ( bhits ) \n    xcoverage . setAttribute ( \"line-rate\" , rate ( lhits_tot , lnum_tot ) ) \n    xcoverage . setAttribute ( \"branch-rate\" , rate ( bhits_tot , bnum_tot ) ) \n    outfile . write ( self . xml_out . toprettyxml ( ) ) \n    denom = lnum_tot + bnum_tot \n    if denom == 0 : \n        pct = 0.0 \n    else : \n        pct = 100.0 * ( lhits_tot + bhits_tot ) / denom \n    return pct "}
{"13872": "\ndef xml_file ( self , cu , analysis ) : \n    package_name = rpartition ( cu . name , \".\" ) [ 0 ] \n    className = cu . name \n    package = self . packages . setdefault ( package_name , [ { } , 0 , 0 , 0 , 0 ] ) \n    xclass = self . xml_out . createElement ( \"class\" ) \n    xclass . appendChild ( self . xml_out . createElement ( \"methods\" ) ) \n    xlines = self . xml_out . createElement ( \"lines\" ) \n    xclass . appendChild ( xlines ) \n    xclass . setAttribute ( \"name\" , className ) \n    filename = cu . file_locator . relative_filename ( cu . filename ) \n    xclass . setAttribute ( \"filename\" , filename . replace ( \"\\\\\" , \"/\" ) ) \n    xclass . setAttribute ( \"complexity\" , \"0\" ) \n    branch_stats = analysis . branch_stats ( ) \n    for line in sorted ( analysis . statements ) : \n        xline = self . xml_out . createElement ( \"line\" ) \n        xline . setAttribute ( \"number\" , str ( line ) ) \n        xline . setAttribute ( \"hits\" , str ( int ( line not in analysis . missing ) ) ) \n        if self . arcs : \n            if line in branch_stats : \n                total , taken = branch_stats [ line ] \n                xline . setAttribute ( \"branch\" , \"true\" ) \n                xline . setAttribute ( \"condition-coverage\" , \"%d%% (%d/%d)\" % ( 100 * taken / total , taken , total ) ) \n        xlines . appendChild ( xline ) \n    class_lines = len ( analysis . statements ) \n    class_hits = class_lines - len ( analysis . missing ) \n    if self . arcs : \n        class_branches = sum ( [ t for t , k in branch_stats . values ( ) ] ) \n        missing_branches = sum ( [ t - k for t , k in branch_stats . values ( ) ] ) \n        class_br_hits = class_branches - missing_branches \n    else : \n        class_branches = 0.0 \n        class_br_hits = 0.0 \n    xclass . setAttribute ( \"line-rate\" , rate ( class_hits , class_lines ) ) \n    xclass . setAttribute ( \"branch-rate\" , rate ( class_br_hits , class_branches ) ) \n    package [ 0 ] [ className ] = xclass \n    package [ 1 ] = package [ 1 ] + ( class_hits ) \n    package [ 2 ] = package [ 2 ] + ( class_lines ) \n    package [ 3 ] = package [ 3 ] + ( class_br_hits ) \n    package [ 4 ] = package [ 4 ] + ( class_branches ) "}
{"13874": "\ndef reduce_freqs ( freqlist ) : \n    allfreqs = np . zeros_like ( freqlist [ 0 ] ) \n    for f in freqlist : \n        allfreqs = allfreqs + ( f ) \n    return allfreqs "}
{"13877": "\ndef one_digit_freqs ( digits , normalize = False ) : \n    freqs = np . zeros ( 10 , dtype = 'i4' ) \n    for d in digits : \n        freqs [ int ( d ) ] = freqs [ int ( d ) ] + ( 1 ) \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13878": "\ndef two_digit_freqs ( digits , normalize = False ) : \n    freqs = np . zeros ( 100 , dtype = 'i4' ) \n    last = digits . next ( ) \n    this = digits . next ( ) \n    for d in digits : \n        index = int ( last + this ) \n        freqs [ index ] = freqs [ index ] + ( 1 ) \n        last = this \n        this = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13879": "\ndef n_digit_freqs ( digits , n , normalize = False ) : \n    freqs = np . zeros ( pow ( 10 , n ) , dtype = 'i4' ) \n    current = np . zeros ( n , dtype = int ) \n    for i in range ( n ) : \n        current [ i ] = digits . next ( ) \n    for d in digits : \n        index = int ( '' . join ( map ( str , current ) ) ) \n        freqs [ index ] = freqs [ index ] + ( 1 ) \n        current [ 0 : - 1 ] = current [ 1 : ] \n        current [ - 1 ] = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13905": "\ndef make_code_from_py ( filename ) : \n    try : \n        source_file = open_source ( filename ) \n    except IOError : \n        raise NoSource ( \"No file to run: %r\" % filename ) \n    try : \n        source = source_file . read ( ) \n    finally : \n        source_file . close ( ) \n    if not source or source [ - 1 ] != '\\n' : \n        source = source + ( '\\n' ) \n    code = compile ( source , filename , \"exec\" ) \n    return code "}
{"13954": "\ndef engine_count ( self ) : \n    count = 0 \n    for n in self . engines . itervalues ( ) : \n        if isinstance ( n , ( tuple , list ) ) : \n            n , args = n \n        count = count + ( n ) \n    return count "}
{"14033": "\ndef deprecated ( replacement = None ) : \n    def outer ( fun ) : \n        msg = \"psutil.%s is deprecated\" % fun . __name__ \n        if replacement is not None : \n            msg = msg + ( \"; use %s instead\" % replacement ) \n        if fun . __doc__ is None : \n            fun . __doc__ = msg \n        \n        @ wraps ( fun ) \n        def inner ( * args , ** kwargs ) : \n            warnings . warn ( msg , category = DeprecationWarning , stacklevel = 2 ) \n            return fun ( * args , ** kwargs ) \n        return inner \n    return outer "}
{"14082": "\ndef GOE ( N ) : \n    m = ra . standard_normal ( ( N , N ) ) \n    m = m + ( m . T ) \n    return m / 2 "}
{"14092": "\ndef write ( self , suffix = None ) : \n    if self . use_file : \n        filename = self . filename \n        if suffix : \n            filename = filename + ( \".\" + suffix ) \n        self . write_file ( filename ) "}
{"14117": "\ndef findsource ( object ) : \n    file = getsourcefile ( object ) or getfile ( object ) \n    globals_dict = None \n    if inspect . isframe ( object ) : \n        globals_dict = object . f_globals \n    else : \n        module = getmodule ( object , file ) \n        if module : \n            globals_dict = module . __dict__ \n    lines = linecache . getlines ( file , globals_dict ) \n    if not lines : \n        raise IOError ( 'could not get source code' ) \n    if ismodule ( object ) : \n        return lines , 0 \n    if isclass ( object ) : \n        name = object . __name__ \n        pat = re . compile ( r'^(\\s*)class\\s*' + name + r'\\b' ) \n        candidates = [ ] \n        for i in range ( len ( lines ) ) : \n            match = pat . match ( lines [ i ] ) \n            if match : \n                if lines [ i ] [ 0 ] == 'c' : \n                    return lines , i \n                candidates . append ( ( match . group ( 1 ) , i ) ) \n        if candidates : \n            candidates . sort ( ) \n            return lines , candidates [ 0 ] [ 1 ] \n        else : \n            raise IOError ( 'could not find class definition' ) \n    if ismethod ( object ) : \n        object = object . im_func \n    if isfunction ( object ) : \n        object = object . func_code \n    if istraceback ( object ) : \n        object = object . tb_frame \n    if isframe ( object ) : \n        object = object . f_code \n    if iscode ( object ) : \n        if not hasattr ( object , 'co_firstlineno' ) : \n            raise IOError ( 'could not find function definition' ) \n        pat = re . compile ( r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) \n        pmatch = pat . match \n        lnum = min ( object . co_firstlineno , len ( lines ) ) - 1 \n        while lnum > 0 : \n            if pmatch ( lines [ lnum ] ) : \n                break \n            lnum = lnum - ( 1 ) \n        return lines , lnum \n    raise IOError ( 'could not find code object' ) "}
{"14122": "\ndef _format_list ( self , extracted_list ) : \n    Colors = self . Colors \n    list = [ ] \n    for filename , lineno , name , line in extracted_list [ : - 1 ] : \n        item = '  File %s\"%s\"%s, line %s%d%s, in %s%s%s\\n' % ( Colors . filename , filename , Colors . Normal , Colors . lineno , lineno , Colors . Normal , Colors . name , name , Colors . Normal ) \n        if line : \n            item = item + ( '    %s\\n' % line . strip ( ) ) \n        list . append ( item ) \n    filename , lineno , name , line = extracted_list [ - 1 ] \n    item = '%s  File %s\"%s\"%s, line %s%d%s, in %s%s%s%s\\n' % ( Colors . normalEm , Colors . filenameEm , filename , Colors . normalEm , Colors . linenoEm , lineno , Colors . normalEm , Colors . nameEm , name , Colors . normalEm , Colors . Normal ) \n    if line : \n        item = item + ( '%s    %s%s\\n' % ( Colors . line , line . strip ( ) , Colors . Normal ) ) \n    list . append ( item ) \n    return list "}
{"14123": "\ndef _format_exception_only ( self , etype , value ) : \n    have_filedata = False \n    Colors = self . Colors \n    list = [ ] \n    stype = Colors . excName + etype . __name__ + Colors . Normal \n    if value is None : \n        list . append ( str ( stype ) + '\\n' ) \n    else : \n        if etype is SyntaxError : \n            have_filedata = True \n            if not value . filename : \n                value . filename = \"<string>\" \n            list . append ( '%s  File %s\"%s\"%s, line %s%d%s\\n' % ( Colors . normalEm , Colors . filenameEm , value . filename , Colors . normalEm , Colors . linenoEm , value . lineno , Colors . Normal ) ) \n            if value . text is not None : \n                i = 0 \n                while i < len ( value . text ) and value . text [ i ] . isspace ( ) : \n                    i = i + ( 1 ) \n                list . append ( '%s    %s%s\\n' % ( Colors . line , value . text . strip ( ) , Colors . Normal ) ) \n                if value . offset is not None : \n                    s = '    ' \n                    for c in value . text [ i : value . offset - 1 ] : \n                        if c . isspace ( ) : \n                            s = s + ( c ) \n                        else : \n                            s = s + ( ' ' ) \n                    list . append ( '%s%s^%s\\n' % ( Colors . caret , s , Colors . Normal ) ) \n        try : \n            s = value . msg \n        except Exception : \n            s = self . _some_str ( value ) \n        if s : \n            list . append ( '%s%s:%s %s\\n' % ( str ( stype ) , Colors . excName , Colors . Normal , s ) ) \n        else : \n            list . append ( '%s\\n' % str ( stype ) ) \n    if have_filedata : \n        ipinst = ipapi . get ( ) \n        if ipinst is not None : \n            ipinst . hooks . synchronize_with_editor ( value . filename , value . lineno , 0 ) \n    return list "}
{"14197": "\ndef pxrun_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    ipself = self . shell \n    with ipself . builtin_trap : \n        cell = ipself . prefilter_manager . prefilter_lines ( raw_cell ) \n        if store_history : \n            ipself . history_manager . store_inputs ( ipself . execution_count , cell , raw_cell ) \n        cell_name = ipself . compile . cache ( cell , ipself . execution_count ) \n        try : \n            ast . parse ( cell , filename = cell_name ) \n        except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n            ipself . showsyntaxerror ( ) \n            ipself . execution_count = ipself . execution_count + ( 1 ) \n            return None \n        except NameError : \n            pass \n    if store_history : \n        ipself . history_manager . store_output ( ipself . execution_count ) \n        ipself . execution_count = ipself . execution_count + ( 1 ) \n    if re . search ( r'get_ipython\\(\\)\\.magic\\(u?[\"\\']%?autopx' , cell ) : \n        self . _disable_autopx ( ) \n        return False \n    else : \n        try : \n            result = self . view . execute ( cell , silent = False , block = False ) \n        except : \n            ipself . showtraceback ( ) \n            return True \n        else : \n            if self . view . block : \n                try : \n                    result . get ( ) \n                except : \n                    self . shell . showtraceback ( ) \n                    return True \n                else : \n                    with ipself . builtin_trap : \n                        result . display_outputs ( ) \n            return False "}
{"14209": "\ndef report ( self , morfs , outfile = None ) : \n    self . find_code_units ( morfs ) \n    max_name = max ( [ len ( cu . name ) for cu in self . code_units ] + [ 5 ] ) \n    fmt_name = \"%%- %ds  \" % max_name \n    fmt_err = \"%s   %s: %s\\n\" \n    header = ( fmt_name % \"Name\" ) + \" Stmts   Miss\" \n    fmt_coverage = fmt_name + \"%6d %6d\" \n    if self . branches : \n        header = header + ( \" Branch BrMiss\" ) \n        fmt_coverage = fmt_coverage + ( \" %6d %6d\" ) \n    width100 = Numbers . pc_str_width ( ) \n    header = header + ( \"%*s\" % ( width100 + 4 , \"Cover\" ) ) \n    fmt_coverage = fmt_coverage + ( \"%%%ds%%%%\" % ( width100 + 3 , ) ) \n    if self . config . show_missing : \n        header = header + ( \"   Missing\" ) \n        fmt_coverage = fmt_coverage + ( \"   %s\" ) \n    rule = \"-\" * len ( header ) + \"\\n\" \n    header = header + ( \"\\n\" ) \n    fmt_coverage = fmt_coverage + ( \"\\n\" ) \n    if not outfile : \n        outfile = sys . stdout \n    outfile . write ( header ) \n    outfile . write ( rule ) \n    total = Numbers ( ) \n    for cu in self . code_units : \n        try : \n            analysis = self . coverage . _analyze ( cu ) \n            nums = analysis . numbers \n            args = ( cu . name , nums . n_statements , nums . n_missing ) \n            if self . branches : \n                args = args + ( ( nums . n_branches , nums . n_missing_branches ) ) \n            args = args + ( ( nums . pc_covered_str , ) ) \n            if self . config . show_missing : \n                args = args + ( ( analysis . missing_formatted ( ) , ) ) \n            outfile . write ( fmt_coverage % args ) \n            total = total + ( nums ) \n        except KeyboardInterrupt : \n            raise \n        except : \n            report_it = not self . config . ignore_errors \n            if report_it : \n                typ , msg = sys . exc_info ( ) [ : 2 ] \n                if typ is NotPython and not cu . should_be_python ( ) : \n                    report_it = False \n            if report_it : \n                outfile . write ( fmt_err % ( cu . name , typ . __name__ , msg ) ) \n    if total . n_files > 1 : \n        outfile . write ( rule ) \n        args = ( \"TOTAL\" , total . n_statements , total . n_missing ) \n        if self . branches : \n            args = args + ( ( total . n_branches , total . n_missing_branches ) ) \n        args = args + ( ( total . pc_covered_str , ) ) \n        if self . config . show_missing : \n            args = args + ( ( \"\" , ) ) \n        outfile . write ( fmt_coverage % args ) \n    return total . pc_covered "}
{"14219": "\ndef _try_passwordless_openssh ( server , keyfile ) : \n    if pexpect is None : \n        raise ImportError ( \"pexpect unavailable, use paramiko\" ) \n    cmd = 'ssh -f ' + server \n    if keyfile : \n        cmd = cmd + ( ' -i ' + keyfile ) \n    cmd = cmd + ( ' exit' ) \n    p = pexpect . spawn ( cmd ) \n    while True : \n        try : \n            p . expect ( '[Pp]assword:' , timeout = .1 ) \n        except pexpect . TIMEOUT : \n            continue \n        except pexpect . EOF : \n            return True \n        else : \n            return False "}
{"14220": "\ndef _try_passwordless_paramiko ( server , keyfile ) : \n    if paramiko is None : \n        msg = \"Paramiko unavaliable, \" \n        if sys . platform == 'win32' : \n            msg = msg + ( \"Paramiko is required for ssh tunneled connections on Windows.\" ) \n        else : \n            msg = msg + ( \"use OpenSSH.\" ) \n        raise ImportError ( msg ) \n    username , server , port = _split_server ( server ) \n    client = paramiko . SSHClient ( ) \n    client . load_system_host_keys ( ) \n    client . set_missing_host_key_policy ( paramiko . WarningPolicy ( ) ) \n    try : \n        client . connect ( server , port , username = username , key_filename = keyfile , look_for_keys = True ) \n    except paramiko . AuthenticationException : \n        return False \n    else : \n        client . close ( ) \n        return True "}
{"14223": "\ndef _stop_scheduling_tasks ( self ) : \n    self . _task_socket . close ( ) \n    self . _task_socket = None \n    msg = \"An engine has been unregistered, and we are using pure \" + \"ZMQ task scheduling.  Task farming will be disabled.\" \n    if self . outstanding : \n        msg = msg + ( \" If you were running tasks when this happened, \" + \"some `outstanding` msg_ids may never resolve.\" ) \n    warnings . warn ( msg , RuntimeWarning ) "}
{"14230": "\ndef _flush_control ( self , sock ) : \n    if self . _ignored_control_replies <= 0 : \n        return \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        self . _ignored_control_replies = self . _ignored_control_replies - ( 1 ) \n        if self . debug : \n            pprint ( msg ) \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14231": "\ndef _flush_ignored_control ( self ) : \n    while self . _ignored_control_replies > 0 : \n        self . session . recv ( self . _control_socket ) \n        self . _ignored_control_replies = self . _ignored_control_replies - ( 1 ) "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent = indent + ( 1 ) \n        elif toktype == token . DEDENT : \n            indent = indent - ( 1 ) \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + 1 ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14252": "\ndef exit_counts ( self ) : \n    excluded_lines = self . first_lines ( self . excluded ) \n    exit_counts = { } \n    for l1 , l2 in self . arcs ( ) : \n        if l1 < 0 : \n            continue \n        if l1 in excluded_lines : \n            continue \n        if l2 in excluded_lines : \n            continue \n        if l1 not in exit_counts : \n            exit_counts [ l1 ] = 0 \n        exit_counts [ l1 ] = exit_counts [ l1 ] + ( 1 ) \n    for l in self . classdefs : \n        if l in exit_counts : \n            exit_counts [ l ] = exit_counts [ l ] - ( 1 ) \n    return exit_counts "}
{"14254": "\ndef _bytes_lines ( self ) : \n    byte_increments = bytes_to_ints ( self . code . co_lnotab [ 0 : : 2 ] ) \n    line_increments = bytes_to_ints ( self . code . co_lnotab [ 1 : : 2 ] ) \n    last_line_num = None \n    line_num = self . code . co_firstlineno \n    byte_num = 0 \n    for byte_incr , line_incr in zip ( byte_increments , line_increments ) : \n        if byte_incr : \n            if line_num != last_line_num : \n                yield ( byte_num , line_num ) \n                last_line_num = line_num \n            byte_num = byte_num + ( byte_incr ) \n        line_num = line_num + ( line_incr ) \n    if line_num != last_line_num : \n        yield ( byte_num , line_num ) "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = 0 \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if bc . jump_to >= 0 : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = 0 \n    for bc in bytecodes : \n        start_new_chunk = False \n        first_chunk = False \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = True \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = True \n        elif bc . offset in jump_to : \n            start_new_chunk = True \n        elif bc . op in OPS_CHUNK_BEGIN : \n            start_new_chunk = True \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if bc . jump_to >= 0 and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch = ignore_branch - ( 1 ) \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if bc . op == OP_BREAK_LOOP : \n                chunk . exits . add ( block_stack [ - 1 ] [ 1 ] ) \n            chunk = None \n        if bc . op == OP_END_FINALLY : \n            for block in reversed ( block_stack ) : \n                if block [ 0 ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ 1 ] ) \n                    break \n        if bc . op == OP_COMPARE_OP and bc . arg == COMPARE_EXCEPTION : \n            ignore_branch = ignore_branch + ( 1 ) \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if penult . op == OP_LOAD_CONST and ult . op == OP_RETURN_VALUE : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if chunks [ - 1 ] . byte != penult . offset : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - 1 ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , False ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - 1 ] . length = bc . next_offset - chunks [ - 1 ] . byte \n        for i in range ( len ( chunks ) - 1 ) : \n            chunks [ i ] . length = chunks [ i + 1 ] . byte - chunks [ i ] . byte \n    return chunks "}
{"14288": "\ndef _edit ( self , filename , line = None ) : \n    if self . custom_edit : \n        self . custom_edit_requested . emit ( filename , line ) \n    elif not self . editor : \n        self . _append_plain_text ( 'No default editor available.\\n' 'Specify a GUI text editor in the `IPythonWidget.editor` ' 'configurable to enable the %edit magic' ) \n    else : \n        try : \n            filename = '\"%s\"' % filename \n            if line and self . editor_line : \n                command = self . editor_line . format ( filename = filename , line = line ) \n            else : \n                try : \n                    command = self . editor . format ( ) \n                except KeyError : \n                    command = self . editor . format ( filename = filename ) \n                else : \n                    command = command + ( ' ' + filename ) \n        except KeyError : \n            self . _append_plain_text ( 'Invalid editor command.\\n' ) \n        else : \n            try : \n                Popen ( command , shell = True ) \n            except OSError : \n                msg = 'Opening editor with command \"%s\" failed.\\n' \n                self . _append_plain_text ( msg % command ) "}
{"14337": "\ndef log_connection_info ( self ) : \n    basename = os . path . basename ( self . connection_file ) \n    if basename == self . connection_file or os . path . dirname ( self . connection_file ) == self . profile_dir . security_dir : \n        tail = basename \n        if self . profile != 'default' : \n            tail = tail + ( \" --profile %s\" % self . profile ) \n    else : \n        tail = self . connection_file \n    self . log . critical ( \"--existing %s\" , tail ) \n    self . ports = dict ( shell = self . shell_port , iopub = self . iopub_port , stdin = self . stdin_port , hb = self . hb_port ) "}
{"14354": "\ndef format_usage ( self , usage ) : \n    msg = 'Usage: %s' % usage \n    if self . parser . description : \n        msg = msg + ( '\\n' ) \n    return msg "}
{"14363": "\ndef setFormat ( self , start , count , format ) : \n    start = start + ( self . _current_offset ) \n    super ( FrontendHighlighter , self ) . setFormat ( start , count , format ) "}
{"14389": "\ndef _document_contents_change ( self , position , removed , added ) : \n    position = position + ( added ) \n    document = self . _control . document ( ) \n    if position == self . _get_cursor ( ) . position ( ) : \n        self . _call_tip ( ) "}
{"14430": "\ndef prefilter_line ( self , line , continue_prompt = False ) : \n    self . shell . _last_input_line = line \n    if not line : \n        return '' \n    if not continue_prompt or ( continue_prompt and self . multi_line_specials ) : \n        line = self . transform_line ( line , continue_prompt ) \n    line_info = LineInfo ( line , continue_prompt ) \n    stripped = line . strip ( ) \n    normal_handler = self . get_handler_by_name ( 'normal' ) \n    if not stripped : \n        if not continue_prompt : \n            self . shell . displayhook . prompt_count = self . shell . displayhook . prompt_count - ( 1 ) \n        return normal_handler . handle ( line_info ) \n    if continue_prompt and not self . multi_line_specials : \n        return normal_handler . handle ( line_info ) \n    prefiltered = self . prefilter_line_info ( line_info ) \n    return prefiltered "}
{"14453": "\ndef pwordfreq ( view , fnames ) : \n    assert len ( fnames ) == len ( view . targets ) \n    view . scatter ( 'fname' , fnames , flatten = True ) \n    ar = view . apply ( wordfreq , Reference ( 'fname' ) ) \n    freqs_list = ar . get ( ) \n    word_set = set ( ) \n    for f in freqs_list : \n        word_set . update ( f . keys ( ) ) \n    freqs = dict ( zip ( word_set , repeat ( 0 ) ) ) \n    for f in freqs_list : \n        for word , count in f . iteritems ( ) : \n            freqs [ word ] = freqs [ word ] + ( count ) \n    return freqs "}
{"14465": "\ndef get_color ( self , color , intensity = 0 ) : \n    if color is None : \n        return None \n    if color < 8 and intensity > 0 : \n        color = color + ( 8 ) \n    constructor = self . color_map . get ( color , None ) \n    if isinstance ( constructor , basestring ) : \n        return QtGui . QColor ( constructor ) \n    elif isinstance ( constructor , ( tuple , list ) ) : \n        return QtGui . QColor ( * constructor ) \n    return None "}
{"14475": "\ndef sizeHint ( self ) : \n    font_metrics = QtGui . QFontMetrics ( self . font ) \n    margin = ( self . _control . frameWidth ( ) + self . _control . document ( ) . documentMargin ( ) ) * 2 \n    style = self . style ( ) \n    splitwidth = style . pixelMetric ( QtGui . QStyle . PM_SplitterWidth ) \n    width = font_metrics . width ( ' ' ) * 81 + margin \n    width = width + ( style . pixelMetric ( QtGui . QStyle . PM_ScrollBarExtent ) ) \n    if self . paging == 'hsplit' : \n        width = width * 2 + splitwidth \n    height = font_metrics . height ( ) * 25 + margin \n    if self . paging == 'vsplit' : \n        height = height * 2 + splitwidth \n    return QtCore . QSize ( width , height ) "}
{"14480": "\ndef execute ( self , source = None , hidden = False , interactive = False ) : \n    if source is None : \n        source = self . input_buffer \n        if not hidden : \n            source = source + ( '\\n' ) \n    elif not hidden : \n        self . input_buffer = source \n    complete = self . _is_complete ( source , interactive ) \n    if hidden : \n        if complete : \n            self . _execute ( source , hidden ) \n        else : \n            error = 'Incomplete noninteractive input: \"%s\"' \n            raise RuntimeError ( error % source ) \n    else : \n        if complete : \n            self . _append_plain_text ( '\\n' ) \n            self . _input_buffer_executing = self . input_buffer \n            self . _executing = True \n            self . _prompt_finished ( ) \n            self . _control . document ( ) . setMaximumBlockCount ( self . buffer_size ) \n            self . _control . setUndoRedoEnabled ( False ) \n            self . _execute ( source , hidden ) \n        else : \n            cursor = self . _get_end_cursor ( ) \n            cursor . beginEditBlock ( ) \n            cursor . insertText ( '\\n' ) \n            self . _insert_continuation_prompt ( cursor ) \n            cursor . endEditBlock ( ) \n            self . _control . moveCursor ( QtGui . QTextCursor . End ) \n    return complete "}
{"14488": "\ndef _append_custom ( self , insert , input , before_prompt = False ) : \n    cursor = self . _control . textCursor ( ) \n    if before_prompt and ( self . _reading or not self . _executing ) : \n        cursor . setPosition ( self . _append_before_prompt_pos ) \n    else : \n        cursor . movePosition ( QtGui . QTextCursor . End ) \n    start_pos = cursor . position ( ) \n    result = insert ( cursor , input ) \n    if before_prompt and not self . _executing : \n        diff = cursor . position ( ) - start_pos \n        self . _append_before_prompt_pos = self . _append_before_prompt_pos + ( diff ) \n        self . _prompt_pos = self . _prompt_pos + ( diff ) \n    return result "}
{"14524": "\ndef raw_input_multi ( header = '' , ps1 = '==> ' , ps2 = '..> ' , terminate_str = '.' ) : \n    try : \n        if header : \n            header = header + ( '\\n' ) \n        lines = [ raw_input ( header + ps1 ) ] \n    except EOFError : \n        return [ ] \n    terminate = [ terminate_str ] \n    try : \n        while lines [ - 1 : ] != terminate : \n            new_line = raw_input ( ps1 ) \n            while new_line . endswith ( '\\\\' ) : \n                new_line = new_line [ : - 1 ] + raw_input ( ps2 ) \n            lines . append ( new_line ) \n        return lines [ : - 1 ] \n    except EOFError : \n        print ( ) \n        return lines "}
{"14532": "\ndef path_to_filename ( pathfile ) : \n    path = pathfile [ : pathfile . rfind ( '/' ) + 1 ] \n    if path == '' : \n        path = './' \n    filename = pathfile [ pathfile . rfind ( '/' ) + 1 : len ( pathfile ) ] \n    if '.' not in filename : \n        path = pathfile \n        filename = '' \n    if ( filename == '' ) and ( path [ len ( path ) - 1 ] != '/' ) : \n        path = path + ( '/' ) \n    return path , filename "}
{"14538": "\ndef nt_quote_arg ( arg ) : \n    result = [ ] \n    needquote = False \n    nb = 0 \n    needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) \n    if needquote : \n        result . append ( '\"' ) \n    for c in arg : \n        if c == '\\\\' : \n            nb = nb + ( 1 ) \n        elif c == '\"' : \n            result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) \n            nb = 0 \n        else : \n            if nb : \n                result . append ( '\\\\' * nb ) \n                nb = 0 \n            result . append ( c ) \n    if nb : \n        result . append ( '\\\\' * nb ) \n    if needquote : \n        result . append ( '\\\\' * nb ) \n        result . append ( '\"' ) \n    return '' . join ( result ) "}
{"14580": "\ndef get_session_info ( self , session = 0 ) : \n    if session <= 0 : \n        session = session + ( self . session_number ) \n    query = \"SELECT * from sessions where session == ?\" \n    return self . db . execute ( query , ( session , ) ) . fetchone ( ) "}
{"14581": "\ndef get_tail ( self , n = 10 , raw = True , output = False , include_latest = False ) : \n    self . writeout_cache ( ) \n    if not include_latest : \n        n = n + ( 1 ) \n    cur = self . _run_sql ( \"ORDER BY session DESC, line DESC LIMIT ?\" , ( n , ) , raw = raw , output = output ) \n    if not include_latest : \n        return reversed ( list ( cur ) [ 1 : ] ) \n    return reversed ( list ( cur ) ) "}
{"14586": "\ndef _get_range_session ( self , start = 1 , stop = None , raw = True , output = False ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if start < 0 : \n        start = start + ( n ) \n    if not stop or ( stop > n ) : \n        stop = n \n    elif stop < 0 : \n        stop = stop + ( n ) \n    for i in range ( start , stop ) : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( 0 , i , line ) "}
{"14590": "\ndef _get_num_cpus ( ) : \n    try : \n        return os . sysconf ( \"SC_NPROCESSORS_ONLN\" ) \n    except ValueError : \n        num = 0 \n        f = open ( '/proc/cpuinfo' , 'r' ) \n        try : \n            lines = f . readlines ( ) \n        finally : \n            f . close ( ) \n        for line in lines : \n            if line . lower ( ) . startswith ( 'processor' ) : \n                num = num + ( 1 ) \n    if num == 0 : \n        f = open ( '/proc/stat' , 'r' ) \n        try : \n            lines = f . readlines ( ) \n        finally : \n            f . close ( ) \n        search = re . compile ( 'cpu\\d' ) \n        for line in lines : \n            line = line . split ( ' ' ) [ 0 ] \n            if search . match ( line ) : \n                num = num + ( 1 ) \n    if num == 0 : \n        raise RuntimeError ( \"can't determine number of CPUs\" ) \n    return num "}
{"14595": "\ndef format_lines ( statements , lines ) : \n    pairs = [ ] \n    i = 0 \n    j = 0 \n    start = None \n    statements = sorted ( statements ) \n    lines = sorted ( lines ) \n    while i < len ( statements ) and j < len ( lines ) : \n        if statements [ i ] == lines [ j ] : \n            if start == None : \n                start = lines [ j ] \n            end = lines [ j ] \n            j = j + ( 1 ) \n        elif start : \n            pairs . append ( ( start , end ) ) \n            start = None \n        i = i + ( 1 ) \n    if start : \n        pairs . append ( ( start , end ) ) \n    ret = ', ' . join ( map ( nice_pair , pairs ) ) \n    return ret "}
{"14614": "\ndef _run_startup_files ( self ) : \n    startup_dir = self . profile_dir . startup_dir \n    startup_files = glob . glob ( os . path . join ( startup_dir , '*.py' ) ) \n    startup_files = startup_files + ( glob . glob ( os . path . join ( startup_dir , '*.ipy' ) ) ) \n    if not startup_files : \n        return \n    self . log . debug ( \"Running startup files from %s...\" , startup_dir ) \n    try : \n        for fname in sorted ( startup_files ) : \n            self . _exec_file ( fname ) \n    except : \n        self . log . warn ( \"Unknown error in handling startup files:\" ) \n        self . shell . showtraceback ( ) "}
{"14678": "\ndef edit ( self , index = None ) : \n    index = self . _get_index ( index ) \n    if index is None : \n        return \n    if index > 0 : \n        index = index - ( 1 ) \n    filename = self . shell . mktempfile ( self . src_blocks [ index ] ) \n    self . shell . hooks . editor ( filename , 1 ) \n    new_block = file_read ( filename ) \n    self . src_blocks [ index ] = new_block \n    self . src_blocks_colored [ index ] = self . ip_colorize ( new_block ) \n    self . block_index = index \n    self ( ) "}
{"14687": "\ndef tbsource ( tb , context = 6 ) : \n    lineno = tb . tb_lineno \n    frame = tb . tb_frame \n    if context > 0 : \n        start = lineno - 1 - context // 2 \n        log . debug ( \"lineno: %s start: %s\" , lineno , start ) \n        try : \n            lines , dummy = inspect . findsource ( frame ) \n        except IOError : \n            lines , index = [ '' ] , 0 \n        else : \n            all_lines = lines \n            start = max ( start , 1 ) \n            start = max ( 0 , min ( start , len ( lines ) - context ) ) \n            lines = lines [ start : start + context ] \n            index = lineno - 1 - start \n            if sys . version_info >= ( 2 , 5 ) and index > 0 : \n                while lines [ index - 1 ] . strip ( ) . endswith ( '\\\\' ) : \n                    start = start - ( 1 ) \n                    lines = all_lines [ start : start + context ] \n    else : \n        lines , index = [ '' ] , 0 \n    log . debug ( \"tbsource lines '''%s''' around index %s\" , lines , index ) \n    return ( lines , index ) "}
{"14689": "\ndef countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = False , progressbar_inversed = False , showpct = False ) : \n    end_date = dateparse . parse_datetime ( date ) \n    end = dateformat . format ( end_date , 'U' ) \n    content = '<div class=\"name\">' + name + '</div>' \n    content = content + ( '<div class=\"description\">' + description + '</div>' ) \n    if progressbar : \n        if not end : \n            raise Exception ( 'For progressbar, start date is requried.' ) \n        parsed_date = datetime . datetime . combine ( dateparse . parse_date ( start ) , datetime . time ( ) ) \n        start_date = dateparse . parse_datetime ( start ) or parsed_date \n        now = datetime . datetime . now ( ) \n        pct = ( now - start_date ) . total_seconds ( ) / ( end_date - start_date ) . total_seconds ( ) \n        pct = int ( pct * 100 ) \n        if progressbar_inversed : \n            pct = 100 - pct \n        bar = '<div class=\"progress progress-striped active\">' \n        bar = bar + ( '<div class=\"progress-bar\"  role=\"progressbar\" aria-valuenow=\"{pct}\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: {pct}%\">' ) \n        bar = bar + ( '<span class=\"sr-only\">{pct}% Complete</span>' ) \n        bar = bar + ( '</div>' ) \n        bar = bar + ( '</div>' ) \n        if showpct : \n            bar = bar + ( '<div class=\"percentage\">{pct}%</div>' ) \n        bar = bar . format ( pct = pct ) \n        content = content + ( bar ) \n    content = content + ( '<div class=\"counter\"></div>' ) \n    attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } \n    if id : \n        attr [ 'id' ] = id \n    return html . tag ( 'div' , content , attr ) "}
{"14702": "\ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = False \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules = modules + ( module_list ( path ) ) \n        if time ( ) - t >= TIMEOUT_STORAGE and not store : \n            store = True \n            print ( \"\\nCaching the list of root modules, please wait!\" ) \n            print ( \"(This will only be done once - type '%rehashx' to \" \"reset cache!)\\n\" ) \n            sys . stdout . flush ( ) \n        if time ( ) - t > TIMEOUT_GIVEUP : \n            print ( \"This is taking too long, we give up.\\n\" ) \n            ip . db [ 'rootmodules' ] = [ ] \n            return [ ] \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14710": "\ndef addError ( self , test , err , capt = None ) : \n    taken = self . _timeTaken ( ) \n    if issubclass ( err [ 0 ] , SkipTest ) : \n        type = 'skipped' \n        self . stats [ 'skipped' ] = self . stats [ 'skipped' ] + ( 1 ) \n    else : \n        type = 'error' \n        self . stats [ 'errors' ] = self . stats [ 'errors' ] + ( 1 ) \n    tb = '' . join ( traceback . format_exception ( * err ) ) \n    id = test . id ( ) \n    self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<%(type)s type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</%(type)s></testcase>' % { 'cls' : self . _quoteattr ( id_split ( id ) [ 0 ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - 1 ] ) , 'taken' : taken , 'type' : type , 'errtype' : self . _quoteattr ( nice_classname ( err [ 0 ] ) ) , 'message' : self . _quoteattr ( exc_message ( err ) ) , 'tb' : escape_cdata ( tb ) , } ) "}
{"14711": "\ndef addFailure ( self , test , err , capt = None , tb_info = None ) : \n    taken = self . _timeTaken ( ) \n    tb = '' . join ( traceback . format_exception ( * err ) ) \n    self . stats [ 'failures' ] = self . stats [ 'failures' ] + ( 1 ) \n    id = test . id ( ) \n    self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<failure type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</failure></testcase>' % { 'cls' : self . _quoteattr ( id_split ( id ) [ 0 ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - 1 ] ) , 'taken' : taken , 'errtype' : self . _quoteattr ( nice_classname ( err [ 0 ] ) ) , 'message' : self . _quoteattr ( exc_message ( err ) ) , 'tb' : escape_cdata ( tb ) , } ) "}
{"14712": "\ndef addSuccess ( self , test , capt = None ) : \n    taken = self . _timeTaken ( ) \n    self . stats [ 'passes' ] = self . stats [ 'passes' ] + ( 1 ) \n    id = test . id ( ) \n    self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s ' 'time=\"%(taken).3f\" />' % { 'cls' : self . _quoteattr ( id_split ( id ) [ 0 ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - 1 ] ) , 'taken' : taken , } ) "}
{"14714": "\ndef weighted ( loads ) : \n    weights = 1. / ( 1e-6 + numpy . array ( loads ) ) \n    sums = weights . cumsum ( ) \n    t = sums [ - 1 ] \n    x = random ( ) * t \n    y = random ( ) * t \n    idx = 0 \n    idy = 0 \n    while sums [ idx ] < x : \n        idx = idx + ( 1 ) \n    while sums [ idy ] < y : \n        idy = idy + ( 1 ) \n    if weights [ idy ] > weights [ idx ] : \n        return idy \n    else : \n        return idx "}
{"14772": "\ndef depth ( n , tree ) : \n    d = 0 \n    parent = tree [ n ] \n    while parent is not None : \n        d = d + ( 1 ) \n        parent = tree [ parent ] \n    return d "}
{"14801": "\ndef annotate_file ( self , cu , analysis ) : \n    if not cu . relative : \n        return \n    filename = cu . filename \n    source = cu . source_file ( ) \n    if self . directory : \n        dest_file = os . path . join ( self . directory , cu . flat_rootname ( ) ) \n        dest_file = dest_file + ( \".py,cover\" ) \n    else : \n        dest_file = filename + \",cover\" \n    dest = open ( dest_file , 'w' ) \n    statements = sorted ( analysis . statements ) \n    missing = sorted ( analysis . missing ) \n    excluded = sorted ( analysis . excluded ) \n    lineno = 0 \n    i = 0 \n    j = 0 \n    covered = True \n    while True : \n        line = source . readline ( ) \n        if line == '' : \n            break \n        lineno = lineno + ( 1 ) \n        while i < len ( statements ) and statements [ i ] < lineno : \n            i = i + ( 1 ) \n        while j < len ( missing ) and missing [ j ] < lineno : \n            j = j + ( 1 ) \n        if i < len ( statements ) and statements [ i ] == lineno : \n            covered = j >= len ( missing ) or missing [ j ] > lineno \n        if self . blank_re . match ( line ) : \n            dest . write ( '  ' ) \n        elif self . else_re . match ( line ) : \n            if i >= len ( statements ) and j >= len ( missing ) : \n                dest . write ( '! ' ) \n            elif i >= len ( statements ) or j >= len ( missing ) : \n                dest . write ( '> ' ) \n            elif statements [ i ] == missing [ j ] : \n                dest . write ( '! ' ) \n            else : \n                dest . write ( '> ' ) \n        elif lineno in excluded : \n            dest . write ( '- ' ) \n        elif covered : \n            dest . write ( '> ' ) \n        else : \n            dest . write ( '! ' ) \n        dest . write ( line ) \n    source . close ( ) \n    dest . close ( ) "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m = m + ( step ) \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2 : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14839": "\ndef load_all_modules_in_packages ( package_or_set_of_packages ) : \n    if isinstance ( package_or_set_of_packages , types . ModuleType ) : \n        packages = [ package_or_set_of_packages ] \n    elif isinstance ( package_or_set_of_packages , Iterable ) and not isinstance ( package_or_set_of_packages , ( dict , str ) ) : \n        packages = package_or_set_of_packages \n    else : \n        raise Exception ( \"This function only accepts a module reference, or an iterable of said objects\" ) \n    imported = packages . copy ( ) \n    for package in packages : \n        if not hasattr ( package , '__path__' ) : \n            raise Exception ( 'Package object passed in has no __path__ attribute. ' 'Make sure to pass in imported references to the packages in question.' ) \n        for module_finder , name , ispkg in pkgutil . walk_packages ( package . __path__ ) : \n            module_name = '{}.{}' . format ( package . __name__ , name ) \n            current_module = importlib . import_module ( module_name ) \n            imported . append ( current_module ) \n            if ispkg : \n                imported = imported + ( load_all_modules_in_packages ( current_module ) ) \n    for module in imported : \n        dir ( module ) \n    return list ( { module . __name__ : module for module in imported } . values ( ) ) "}
