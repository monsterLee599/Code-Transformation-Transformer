{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256 \n    for index , _ in enumerate ( data ) : \n        t = index \n        for i in range ( 8 ) : \n            t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) \n        e = o = r = - 1 \n        i , a = 0 , len ( path ) \n        while a > i : \n            e = ord ( path [ i ] ) \n            i += 1 \n            if 128 > e : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] \n            else : \n                if 2048 > e : \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n                else : \n                    if 55296 <= e < 57344 : \n                        e = ( 1023 & e ) + 64 \n                        i += 1 \n                        o = 1023 & t . url ( i ) \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] \n                    else : \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) \n    while 1 : \n        url = tmp ( ) \n        if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : \n            return url "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += 1 \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_workers_running > num_ready_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            elif num_workers_expected < num_workers_running : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                for _ in range ( excess ) : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess -= 1 \n                    wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n            elif num_workers_running == num_workers_expected : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10 ) \n                if num_workers_expected > len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"63": "\ndef print_stats ( self ) : \n    session = settings . Session ( ) \n    TI = TaskInstance \n    tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) \n    successful_tis = [ x for x in tis if x . state == State . SUCCESS ] \n    ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] \n    ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) \n    print ( 'Performance Results' ) \n    print ( '###################' ) \n    for dag_id in DAG_IDS : \n        print ( 'DAG {}' . format ( dag_id ) ) \n        print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) \n    print ( '###################' ) \n    if len ( successful_tis ) < len ( tis ) : \n        print ( \"WARNING!! The following task instances haven't completed\" ) \n        print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) \n    session . commit ( ) "}
{"64": "\ndef heartbeat ( self ) : \n    super ( SchedulerMetricsJob , self ) . heartbeat ( ) \n    session = settings . Session ( ) \n    TI = TaskInstance \n    successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) \n    session . commit ( ) \n    dagbag = DagBag ( SUBDIR ) \n    dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] \n    num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) \n    if ( len ( successful_tis ) == num_task_instances or MAX_RUNTIME_SECS < ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) ) : \n        if len ( successful_tis ) == num_task_instances : \n            self . log . info ( \"All tasks processed! Printing stats.\" ) \n        else : \n            self . log . info ( \"Test timeout reached. Printing available stats.\" ) \n        self . print_stats ( ) \n        set_dags_paused_state ( True ) \n        sys . exit ( ) "}
{"78": "\ndef provide_session ( func ) : \n    \n    @ wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        arg_session = 'session' \n        func_params = func . __code__ . co_varnames \n        session_in_args = arg_session in func_params and len ( args ) > func_params . index ( arg_session ) \n        session_in_kwargs = arg_session in kwargs \n        if session_in_kwargs or session_in_args : \n            return func ( * args , ** kwargs ) \n        else : \n            with create_session ( ) as session : \n                kwargs [ arg_session ] = session \n                return func ( * args , ** kwargs ) \n    return wrapper "}
{"107": "\ndef flush ( self ) : \n    if 0 < len ( self . _buffer ) : \n        self . logger . log ( self . level , self . _buffer ) \n        self . _buffer = str ( ) "}
{"115": "\ndef start_in_async ( self ) : \n    while True : \n        loop_start_time = time . time ( ) \n        if self . _signal_conn . poll ( ) : \n            agent_signal = self . _signal_conn . recv ( ) \n            if agent_signal == DagParsingSignal . TERMINATE_MANAGER : \n                self . terminate ( ) \n                break \n            elif agent_signal == DagParsingSignal . END_MANAGER : \n                self . end ( ) \n                sys . exit ( os . EX_OK ) \n        self . _refresh_dag_dir ( ) \n        simple_dags = self . heartbeat ( ) \n        for simple_dag in simple_dags : \n            self . _result_queue . put ( simple_dag ) \n        self . _print_stat ( ) \n        all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n        max_runs_reached = self . max_runs_reached ( ) \n        dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) \n        self . _stat_queue . put ( dag_parsing_stat ) \n        if max_runs_reached : \n            self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n            break \n        loop_duration = time . time ( ) - loop_start_time \n        if 1 > loop_duration : \n            sleep_length = 1 - loop_duration \n            self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep_length ) \n            time . sleep ( sleep_length ) "}
{"117": "\ndef _refresh_dag_dir ( self ) : \n    elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) \n    if self . dag_dir_list_interval < elapsed_time_since_refresh : \n        self . log . info ( \"Searching for files in %s\" , self . _dag_directory ) \n        self . _file_paths = list_py_file_paths ( self . _dag_directory ) \n        self . last_dag_dir_refresh_time = timezone . utcnow ( ) \n        self . log . info ( \"There are %s files in %s\" , len ( self . _file_paths ) , self . _dag_directory ) \n        self . set_file_paths ( self . _file_paths ) \n        try : \n            self . log . debug ( \"Removing old import errors\" ) \n            self . clear_nonexistent_import_errors ( ) \n        except Exception : \n            self . log . exception ( \"Error removing old import errors\" ) "}
{"118": "\ndef _print_stat ( self ) : \n    if ( self . print_stats_interval < ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) ) : \n        if 0 < len ( self . _file_paths ) : \n            self . _log_file_processing_stats ( self . _file_paths ) \n        self . last_stat_print_time = timezone . utcnow ( ) "}
{"123": "\ndef heartbeat ( self ) : \n    finished_processors = { } \n    running_processors = { } \n    for file_path , processor in self . _processors . items ( ) : \n        if processor . done : \n            self . log . debug ( \"Processor for %s finished\" , file_path ) \n            now = timezone . utcnow ( ) \n            finished_processors [ file_path ] = processor \n            self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) \n            self . _last_finish_time [ file_path ] = now \n            self . _run_count [ file_path ] += 1 \n        else : \n            running_processors [ file_path ] = processor \n    self . _processors = running_processors \n    self . log . debug ( \"%s/%s DAG parsing processes running\" , len ( self . _processors ) , self . _parallelism ) \n    self . log . debug ( \"%s file paths queued for processing\" , len ( self . _file_path_queue ) ) \n    simple_dags = [ ] \n    for file_path , processor in finished_processors . items ( ) : \n        if processor . result is None : \n            self . log . warning ( \"Processor for %s exited with return code %s.\" , processor . file_path , processor . exit_code ) \n        else : \n            for simple_dag in processor . result : \n                simple_dags . append ( simple_dag ) \n    if len ( self . _file_path_queue ) == 0 : \n        file_paths_in_progress = self . _processors . keys ( ) \n        now = timezone . utcnow ( ) \n        file_paths_recently_processed = [ ] \n        for file_path in self . _file_paths : \n            last_finish_time = self . get_last_finish_time ( file_path ) \n            if ( last_finish_time is not None and self . _file_process_interval > ( now - last_finish_time ) . total_seconds ( ) ) : \n                file_paths_recently_processed . append ( file_path ) \n        files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] \n        files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) \n        for file_path , processor in self . _processors . items ( ) : \n            self . log . debug ( \"File path %s is still being processed (started: %s)\" , processor . file_path , processor . start_time . isoformat ( ) ) \n        self . log . debug ( \"Queuing the following files for processing:\\n\\t%s\" , \"\\n\\t\" . join ( files_paths_to_queue ) ) \n        self . _file_path_queue . extend ( files_paths_to_queue ) \n    zombies = self . _find_zombies ( ) \n    while ( 0 < self . _parallelism - len ( self . _processors ) and 0 < len ( self . _file_path_queue ) ) : \n        file_path = self . _file_path_queue . pop ( 0 ) \n        processor = self . _processor_factory ( file_path , zombies ) \n        processor . start ( ) \n        self . log . debug ( \"Started a process (PID: %s) to generate tasks for %s\" , processor . pid , file_path ) \n        self . _processors [ file_path ] = processor \n    self . _run_count [ self . _heart_beat_key ] += 1 \n    return simple_dags "}
{"124": "\ndef end ( self ) : \n    pids_to_kill = self . get_all_pids ( ) \n    if 0 < len ( pids_to_kill ) : \n        this_process = psutil . Process ( os . getpid ( ) ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        for child in child_processes : \n            self . log . info ( \"Terminating child PID: %s\" , child . pid ) \n            child . terminate ( ) \n        timeout = 5 \n        self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) \n        try : \n            psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) \n        except psutil . TimeoutExpired : \n            self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        if 0 < len ( child_processes ) : \n            self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) \n            for child in child_processes : \n                self . log . info ( \"Killing child PID: %s\" , child . pid ) \n                child . kill ( ) \n                child . wait ( ) "}
{"134": "\ndef wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : \n    while 0 < timeout : \n        operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) \n        if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) \n        timeout -= TIME_TO_SLEEP_IN_SECONDS \n    raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" ) "}
{"164": "\ndef chunks ( items , chunk_size ) : \n    if 0 >= chunk_size : \n        raise ValueError ( 'Chunk size must be a positive integer' ) \n    for i in range ( 0 , len ( items ) , chunk_size ) : \n        yield items [ i : i + chunk_size ] "}
{"167": "\ndef pprinttable ( rows ) : \n    if not rows : \n        return \n    if hasattr ( rows [ 0 ] , '_fields' ) : \n        headers = rows [ 0 ] . _fields \n    else : \n        headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] \n    lens = [ len ( s ) for s in headers ] \n    for row in rows : \n        for i in range ( len ( rows [ 0 ] ) ) : \n            slenght = len ( \"{}\" . format ( row [ i ] ) ) \n            if lens [ i ] < slenght : \n                lens [ i ] = slenght \n    formats = [ ] \n    hformats = [ ] \n    for i in range ( len ( rows [ 0 ] ) ) : \n        if isinstance ( rows [ 0 ] [ i ] , int ) : \n            formats . append ( \"%%%dd\" % lens [ i ] ) \n        else : \n            formats . append ( \"%%-%ds\" % lens [ i ] ) \n        hformats . append ( \"%%-%ds\" % lens [ i ] ) \n    pattern = \" | \" . join ( formats ) \n    hpattern = \" | \" . join ( hformats ) \n    separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) \n    s = \"\" \n    s += separator + '\\n' \n    s += ( hpattern % tuple ( headers ) ) + '\\n' \n    s += separator + '\\n' \n    def f ( t ) : \n        return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t \n    for line in rows : \n        s += pattern % tuple ( f ( t ) for t in line ) + '\\n' \n    s += separator + '\\n' \n    return s "}
{"181": "\ndef _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : \n    if 0 >= interval : \n        raise ValueError ( \"Interval must be > 0\" ) \n    while True : \n        job = self . _get_job ( project_id , job_id ) \n        if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : \n            return job \n        time . sleep ( interval ) "}
{"206": "\ndef apply_defaults ( func ) : \n    sig_cache = signature ( func ) \n    non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if param . default == param . empty and param . name != 'self' and param . kind not in ( param . VAR_POSITIONAL , param . VAR_KEYWORD ) } \n    \n    @ wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        if 1 < len ( args ) : \n            raise AirflowException ( \"Use keyword arguments when initializing operators\" ) \n        dag_args = { } \n        dag_params = { } \n        dag = kwargs . get ( 'dag' , None ) or settings . CONTEXT_MANAGER_DAG \n        if dag : \n            dag_args = copy ( dag . default_args ) or { } \n            dag_params = copy ( dag . params ) or { } \n        params = { } \n        if 'params' in kwargs : \n            params = kwargs [ 'params' ] \n        dag_params . update ( params ) \n        default_args = { } \n        if 'default_args' in kwargs : \n            default_args = kwargs [ 'default_args' ] \n            if 'params' in default_args : \n                dag_params . update ( default_args [ 'params' ] ) \n                del default_args [ 'params' ] \n        dag_args . update ( default_args ) \n        default_args = dag_args \n        for arg in sig_cache . parameters : \n            if arg not in kwargs and arg in default_args : \n                kwargs [ arg ] = default_args [ arg ] \n        missing_args = list ( non_optional_args - set ( kwargs ) ) \n        if missing_args : \n            msg = \"Argument {0} is required\" . format ( missing_args ) \n            raise AirflowException ( msg ) \n        kwargs [ 'params' ] = dag_params \n        result = func ( * args , ** kwargs ) \n        return result \n    return wrapper "}
{"212": "\ndef get_previous_dagrun ( self , session = None ) : \n    return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , self . execution_date > DagRun . execution_date ) . order_by ( DagRun . execution_date . desc ( ) ) . first ( ) "}
{"215": "\ndef verify_integrity ( self , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    task_ids = [ ] \n    for ti in tis : \n        task_ids . append ( ti . task_id ) \n        task = None \n        try : \n            task = dag . get_task ( ti . task_id ) \n        except AirflowException : \n            if ti . state == State . REMOVED : \n                pass \n            elif self . state is not State . RUNNING and not dag . partial : \n                self . log . warning ( \"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\" . format ( ti , dag ) ) \n                Stats . incr ( \"task_removed_from_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) \n                ti . state = State . REMOVED \n        is_task_in_dag = task is not None \n        should_restore_task = is_task_in_dag and ti . state == State . REMOVED \n        if should_restore_task : \n            self . log . info ( \"Restoring task '{}' which was previously \" \"removed from DAG '{}'\" . format ( ti , dag ) ) \n            Stats . incr ( \"task_restored_to_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) \n            ti . state = State . NONE \n    for task in six . itervalues ( dag . task_dict ) : \n        if self . execution_date < task . start_date and not self . is_backfill : \n            continue \n        if task . task_id not in task_ids : \n            Stats . incr ( \"task_instance_created-{}\" . format ( task . __class__ . __name__ ) , 1 , 1 ) \n            ti = TaskInstance ( task , self . execution_date ) \n            session . add ( ti ) \n    session . commit ( ) "}
{"221": "\ndef get_dag ( self , dag_id ) : \n    from airflow . models . dag import DagModel \n    root_dag_id = dag_id \n    if dag_id in self . dags : \n        dag = self . dags [ dag_id ] \n        if dag . is_subdag : \n            root_dag_id = dag . parent_dag . dag_id \n    orm_dag = DagModel . get_current ( root_dag_id ) \n    if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and orm_dag . last_expired > dag . last_loaded ) ) : \n        found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = False ) \n        if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : \n            return self . dags [ dag_id ] \n        elif dag_id in self . dags : \n            del self . dags [ dag_id ] \n    return self . dags . get ( dag_id ) "}
{"240": "\ndef ready_for_retry ( self ) : \n    return ( self . state == State . UP_FOR_RETRY and timezone . utcnow ( ) > self . next_retry_datetime ( ) ) "}
{"241": "\ndef pool_full ( self , session ) : \n    if not self . task . pool : \n        return False \n    pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) \n    if not pool : \n        return False \n    open_slots = pool . open_slots ( session = session ) \n    return 0 >= open_slots "}
{"243": "\ndef xcom_push ( self , key , value , execution_date = None ) : \n    if execution_date and self . execution_date > execution_date : \n        raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) \n    XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date ) "}
{"297": "\ndef get_object_from_salesforce ( self , obj , fields ) : \n    query = \"SELECT {} FROM {}\" . format ( \",\" . join ( fields ) , obj ) \n    self . log . info ( \"Making query to Salesforce: %s\" , query if 30 > len ( query ) else \" ... \" . join ( [ query [ : 15 ] , query [ - 15 : ] ] ) ) \n    return self . make_query ( query ) "}
{"299": "\ndef write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = False , record_time_added = False ) : \n    fmt = fmt . lower ( ) \n    if fmt not in [ 'csv' , 'json' , 'ndjson' ] : \n        raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) \n    df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) \n    df . columns = [ column . lower ( ) for column in df . columns ] \n    if coerce_to_timestamp and 0 < df . shape [ 0 ] : \n        object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] \n        self . log . info ( \"Coercing timestamps for: %s\" , object_name ) \n        schema = self . describe_object ( object_name ) \n        possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] \n        df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) \n    if record_time_added : \n        fetched_time = time . time ( ) \n        df [ \"time_fetched_from_salesforce\" ] = fetched_time \n    if fmt == \"csv\" : \n        self . log . info ( \"Cleaning data and writing to CSV\" ) \n        possible_strings = df . columns [ df . dtypes == \"object\" ] \n        df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) \n        df . to_csv ( filename , index = False ) \n    elif fmt == \"json\" : \n        df . to_json ( filename , \"records\" , date_unit = \"s\" ) \n    elif fmt == \"ndjson\" : \n        df . to_json ( filename , \"records\" , lines = True , date_unit = \"s\" ) \n    return df "}
{"303": "\ndef has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = False ) : \n    mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) \n    return 0 < len ( mail_attachments ) "}
{"309": "\ndef _get_dep_statuses ( self , ti , session , dep_context ) : \n    if dep_context . ignore_in_reschedule_period : \n        yield self . _passing_status ( reason = \"The context specified that being in a reschedule period was \" \"permitted.\" ) \n        return \n    if ti . state not in self . RESCHEDULEABLE_STATES : \n        yield self . _passing_status ( reason = \"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\" ) \n        return \n    task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) \n    if not task_reschedules : \n        yield self . _passing_status ( reason = \"There is no reschedule request for this task instance.\" ) \n        return \n    now = timezone . utcnow ( ) \n    next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date \n    if next_reschedule_date <= now : \n        yield self . _passing_status ( reason = \"Task instance id ready for reschedule.\" ) \n        return \n    yield self . _failing_status ( reason = \"Task is not ready for reschedule yet but will be rescheduled \" \"automatically. Current date is {0} and task will be rescheduled \" \"at {1}.\" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) ) "}
{"314": "\ndef check_for_prefix ( self , container_name , prefix , ** kwargs ) : \n    matches = self . connection . list_blobs ( container_name , prefix , num_results = 1 , ** kwargs ) \n    return 0 < len ( list ( matches ) ) "}
{"338": "\ndef is_updated_after ( self , bucket_name , object_name , ts ) : \n    client = self . get_conn ( ) \n    bucket = storage . Bucket ( client = client , name = bucket_name ) \n    blob = bucket . get_blob ( blob_name = object_name ) \n    blob . reload ( ) \n    blob_update_time = blob . updated \n    if blob_update_time is not None : \n        import dateutil . tz \n        if not ts . tzinfo : \n            ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) \n        self . log . info ( \"Verify object date: %s > %s\" , blob_update_time , ts ) \n        if ts < blob_update_time : \n            return True \n    return False "}
{"346": "\ndef secondary_training_status_changed ( current_job_description , prev_job_description ) : \n    current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) \n    if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == 0 : \n        return False \n    prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None \n    last_message = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and 0 < len ( prev_job_secondary_status_transitions ) else '' \n    message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] \n    return message != last_message "}
{"356": "\ndef describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : \n    log_group = '/aws/sagemaker/TrainingJobs' \n    if instance_count > len ( stream_names ) : \n        logs_conn = self . get_log_conn ( ) \n        try : \n            streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) \n            stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] \n            positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) \n        except logs_conn . exceptions . ResourceNotFoundException : \n            pass \n    if 0 < len ( stream_names ) : \n        for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : \n            self . log . info ( event [ 'message' ] ) \n            ts , count = positions [ stream_names [ idx ] ] \n            if event [ 'timestamp' ] == ts : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) \n            else : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) \n    if state == LogState . COMPLETE : \n        return state , last_description , last_describe_job_call \n    if state == LogState . JOB_COMPLETE : \n        state = LogState . COMPLETE \n    elif 30 <= time . time ( ) - last_describe_job_call : \n        description = self . describe_training_job ( job_name ) \n        last_describe_job_call = time . time ( ) \n        if secondary_training_status_changed ( description , last_description ) : \n            self . log . info ( secondary_training_status_message ( description , last_description ) ) \n            last_description = description \n        status = description [ 'TrainingJobStatus' ] \n        if status not in self . non_terminal_states : \n            state = LogState . JOB_COMPLETE \n    return state , last_description , last_describe_job_call "}
{"357": "\ndef check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : \n    if not non_terminal_states : \n        non_terminal_states = self . non_terminal_states \n    sec = 0 \n    running = True \n    while running : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        try : \n            response = describe_function ( job_name ) \n            status = response [ key ] \n            self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) \n        except KeyError : \n            raise AirflowException ( 'Could not get status of the SageMaker job' ) \n        except ClientError : \n            raise AirflowException ( 'AWS request failed, check logs for more info' ) \n        if status in non_terminal_states : \n            running = True \n        elif status in self . failed_states : \n            raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) \n        else : \n            running = False \n        if max_ingestion_time and max_ingestion_time < sec : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    self . log . info ( 'SageMaker Job Compeleted' ) \n    response = describe_function ( job_name ) \n    return response "}
{"358": "\ndef check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : \n    sec = 0 \n    description = self . describe_training_job ( job_name ) \n    self . log . info ( secondary_training_status_message ( description , None ) ) \n    instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] \n    status = description [ 'TrainingJobStatus' ] \n    stream_names = [ ] \n    positions = { } \n    job_already_completed = status not in non_terminal_states \n    state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE \n    last_describe_job_call = time . time ( ) \n    last_description = description \n    while True : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) \n        if state == LogState . COMPLETE : \n            break \n        if max_ingestion_time and max_ingestion_time < sec : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    if wait_for_completion : \n        status = last_description [ 'TrainingJobStatus' ] \n        if status in failed_states : \n            reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) \n            raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) \n        billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) "}
{"391": "\ndef cancel_query ( self ) : \n    jobs = self . service . jobs ( ) \n    if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : \n        self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) \n        if self . location : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) \n        else : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) \n    else : \n        self . log . info ( 'No running BigQuery jobs to cancel.' ) \n        return \n    max_polling_attempts = 12 \n    polling_attempts = 0 \n    job_complete = False \n    while max_polling_attempts > polling_attempts and not job_complete : \n        polling_attempts = polling_attempts + 1 \n        job_complete = self . poll_job_complete ( self . running_job_id ) \n        if job_complete : \n            self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) \n        elif polling_attempts == max_polling_attempts : \n            self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) \n        else : \n            self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) \n            time . sleep ( 5 ) "}
{"415": "\ndef gzipped ( f ) : \n    \n    @ functools . wraps ( f ) \n    def view_func ( * args , ** kwargs ) : \n        \n        @ after_this_request \n        def zipper ( response ) : \n            accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) \n            if 'gzip' not in accept_encoding . lower ( ) : \n                return response \n            response . direct_passthrough = False \n            if ( 200 > response . status_code or 300 <= response . status_code or 'Content-Encoding' in response . headers ) : \n                return response \n            gzip_buffer = IO ( ) \n            gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) \n            gzip_file . write ( response . data ) \n            gzip_file . close ( ) \n            response . data = gzip_buffer . getvalue ( ) \n            response . headers [ 'Content-Encoding' ] = 'gzip' \n            response . headers [ 'Vary' ] = 'Accept-Encoding' \n            response . headers [ 'Content-Length' ] = len ( response . data ) \n            return response \n        return f ( * args , ** kwargs ) \n    return view_func "}
{"432": "\ndef poll_query_status ( self , query_execution_id , max_tries = None ) : \n    try_number = 1 \n    final_query_state = None \n    while True : \n        query_state = self . check_query_status ( query_execution_id ) \n        if query_state is None : \n            self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) \n        elif query_state in self . INTERMEDIATE_STATES : \n            self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) \n        else : \n            self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) \n            final_query_state = query_state \n            break \n        if max_tries and max_tries <= try_number : \n            final_query_state = query_state \n            break \n        try_number += 1 \n        sleep ( self . sleep_time ) \n    return final_query_state "}
{"446": "\ndef filter_for_filesize ( result , size = None ) : \n    if size : \n        log = LoggingMixin ( ) . log \n        log . debug ( 'Filtering for file size >= %s in files: %s' , size , map ( lambda x : x [ 'path' ] , result ) ) \n        size *= settings . MEGABYTE \n        result = [ x for x in result if size <= x [ 'length' ] ] \n        log . debug ( 'HdfsSensor.poke: after size filter result is %s' , result ) \n    return result "}
{"459": "\ndef _build_discord_payload ( self ) : \n    payload = { } \n    if self . username : \n        payload [ 'username' ] = self . username \n    if self . avatar_url : \n        payload [ 'avatar_url' ] = self . avatar_url \n    payload [ 'tts' ] = self . tts \n    if 2000 >= len ( self . message ) : \n        payload [ 'content' ] = self . message \n    else : \n        raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) \n    return json . dumps ( payload ) "}
{"479": "\ndef _process_task_instances ( self , dag , queue , session = None ) : \n    dag_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) \n    active_dag_runs = [ ] \n    for run in dag_runs : \n        self . log . info ( \"Examining DAG run %s\" , run ) \n        if timezone . utcnow ( ) < run . execution_date : \n            self . log . error ( \"Execution date is in future: %s\" , run . execution_date ) \n            continue \n        if dag . max_active_runs <= len ( active_dag_runs ) : \n            self . log . info ( \"Number of active dag runs reached max_active_run.\" ) \n            break \n        if run . is_backfill : \n            continue \n        run . dag = dag \n        run . verify_integrity ( session = session ) \n        run . update_state ( session = session ) \n        if run . state == State . RUNNING : \n            make_transient ( run ) \n            active_dag_runs . append ( run ) \n    for run in active_dag_runs : \n        self . log . debug ( \"Examining active DAG run: %s\" , run ) \n        tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) \n        for ti in tis : \n            task = dag . get_task ( ti . task_id ) \n            ti . task = task \n            if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True ) , session = session ) : \n                self . log . debug ( 'Queuing task: %s' , ti ) \n                queue . append ( ti . key ) "}
{"480": "\ndef _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : \n    tis_changed = 0 \n    query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) \n    if self . using_sqlite : \n        tis_to_change = query . with_for_update ( ) . all ( ) \n        for ti in tis_to_change : \n            ti . set_state ( new_state , session = session ) \n            tis_changed += 1 \n    else : \n        subq = query . subquery ( ) \n        tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) \n        session . commit ( ) \n    if 0 < tis_changed : \n        self . log . warning ( \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state ) "}
{"487": "\ndef process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : \n    self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) \n    simple_dags = [ ] \n    try : \n        dagbag = models . DagBag ( file_path , include_examples = False ) \n    except Exception : \n        self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) \n        Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) \n        return [ ] \n    if 0 < len ( dagbag . dags ) : \n        self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) \n    else : \n        self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) \n        self . update_import_errors ( session , dagbag ) \n        return [ ] \n    for dag in dagbag . dags . values ( ) : \n        dag . sync_to_db ( ) \n    paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] \n    for dag_id in dagbag . dags : \n        if dag_id not in paused_dag_ids : \n            dag = dagbag . get_dag ( dag_id ) \n            pickle_id = None \n            if pickle_dags : \n                pickle_id = dag . pickle ( session ) . id \n            simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) \n    if 0 < len ( self . dag_ids ) : \n        dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] \n    else : \n        dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] \n    ti_keys_to_schedule = [ ] \n    self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) \n    for ti_key in ti_keys_to_schedule : \n        dag = dagbag . dags [ ti_key [ 0 ] ] \n        task = dag . get_task ( ti_key [ 1 ] ) \n        ti = models . TaskInstance ( task , ti_key [ 2 ] ) \n        ti . refresh_from_db ( session = session , lock_for_update = True ) \n        dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) \n        if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : \n            ti . state = State . SCHEDULED \n        self . log . info ( \"Creating / updating %s in ORM\" , ti ) \n        session . merge ( ti ) \n    session . commit ( ) \n    try : \n        self . update_import_errors ( session , dagbag ) \n    except Exception : \n        self . log . exception ( \"Error logging import errors!\" ) \n    try : \n        dagbag . kill_zombies ( zombies ) \n    except Exception : \n        self . log . exception ( \"Error killing zombies!\" ) \n    return simple_dags "}
{"490": "\ndef _get_dag_run ( self , run_date , session = None ) : \n    run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) \n    respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) \n    current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) \n    run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) \n    if run is not None and 0 < len ( run ) : \n        run = run [ 0 ] \n        if run . state == State . RUNNING : \n            respect_dag_max_active_limit = False \n    else : \n        run = None \n    if ( respect_dag_max_active_limit and self . dag . max_active_runs <= current_active_dag_count ) : \n        return None \n    run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self . conf , ) \n    run . dag = self . dag \n    run . state = State . RUNNING \n    run . run_id = run_id \n    run . verify_integrity ( session = session ) \n    return run "}
{"494": "\ndef _execute ( self , session = None ) : \n    ti_status = BackfillJob . _DagRunTaskStatus ( ) \n    start_date = self . bf_start_date \n    run_dates = self . dag . get_run_dates ( start_date = start_date , end_date = self . bf_end_date ) \n    if self . run_backwards : \n        tasks_that_depend_on_past = [ t . task_id for t in self . dag . task_dict . values ( ) if t . depends_on_past ] \n        if tasks_that_depend_on_past : \n            raise AirflowException ( 'You cannot backfill backwards because one or more tasks depend_on_past: {}' . format ( \",\" . join ( tasks_that_depend_on_past ) ) ) \n        run_dates = run_dates [ : : - 1 ] \n    if len ( run_dates ) == 0 : \n        self . log . info ( \"No run dates were found for the given dates and dag interval.\" ) \n        return \n    pickle_id = None \n    if not self . donot_pickle and self . executor . __class__ not in ( executors . LocalExecutor , executors . SequentialExecutor ) : \n        pickle = DagPickle ( self . dag ) \n        session . add ( pickle ) \n        session . commit ( ) \n        pickle_id = pickle . id \n    executor = self . executor \n    executor . start ( ) \n    ti_status . total_runs = len ( run_dates ) \n    try : \n        remaining_dates = ti_status . total_runs \n        while 0 < remaining_dates : \n            dates_to_process = [ run_date for run_date in run_dates if run_date not in ti_status . executed_dag_run_dates ] \n            self . _execute_for_run_dates ( run_dates = dates_to_process , ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) \n            remaining_dates = ( ti_status . total_runs - len ( ti_status . executed_dag_run_dates ) ) \n            err = self . _collect_errors ( ti_status = ti_status , session = session ) \n            if err : \n                raise AirflowException ( err ) \n            if 0 < remaining_dates : \n                self . log . info ( \"max_active_runs limit for dag %s has been reached \" \" - waiting for other dag runs to finish\" , self . dag_id ) \n                time . sleep ( self . delay_on_limit_secs ) \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . log . warning ( \"Backfill terminated by user.\" ) \n        self . _set_unfinished_dag_runs_to_failed ( ti_status . active_runs ) \n    finally : \n        session . commit ( ) \n        executor . end ( ) \n    self . log . info ( \"Backfill done. Exiting.\" ) "}
{"516": "\ndef _wait_for_task_ended ( self ) : \n    try : \n        waiter = self . client . get_waiter ( 'job_execution_complete' ) \n        waiter . config . max_attempts = sys . maxsize \n        waiter . wait ( jobs = [ self . jobId ] ) \n    except ValueError : \n        retry = True \n        retries = 0 \n        while self . max_retries > retries and retry : \n            self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) \n            response = self . client . describe_jobs ( jobs = [ self . jobId ] ) \n            if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : \n                retry = False \n            sleep ( 1 + pow ( retries * 0.1 , 2 ) ) \n            retries += 1 "}
{"523": "\ndef apply_lineage ( func ) : \n    backend = _get_backend ( ) \n    \n    @ wraps ( func ) \n    def wrapper ( self , context , * args , ** kwargs ) : \n        self . log . debug ( \"Backend: %s, Lineage called with inlets: %s, outlets: %s\" , backend , self . inlets , self . outlets ) \n        ret_val = func ( self , context , * args , ** kwargs ) \n        outlets = [ x . as_dict ( ) for x in self . outlets ] \n        inlets = [ x . as_dict ( ) for x in self . inlets ] \n        if 0 < len ( self . outlets ) : \n            self . xcom_push ( context , key = PIPELINE_OUTLETS , value = outlets , execution_date = context [ 'ti' ] . execution_date ) \n        if 0 < len ( self . inlets ) : \n            self . xcom_push ( context , key = PIPELINE_INLETS , value = inlets , execution_date = context [ 'ti' ] . execution_date ) \n        if backend : \n            backend . send_lineage ( operator = self , inlets = self . inlets , outlets = self . outlets , context = context ) \n        return ret_val \n    return wrapper "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and end_date < start_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while end_date >= start_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if 0 < num : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if 0 < num : \n                    start_date += delta \n                else : \n                    start_date -= delta \n    return sorted ( dates ) "}
{"542": "\ndef poke ( self , context ) : \n    sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) \n    sqs_conn = sqs_hook . get_conn ( ) \n    self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) \n    messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) \n    self . log . info ( \"reveived message %s\" , str ( messages ) ) \n    if 'Messages' in messages and 0 < len ( messages [ 'Messages' ] ) : \n        entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] \n        result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) \n        if 'Successful' in result : \n            context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) \n            return True \n        else : \n            raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) \n    return False "}
{"543": "\ndef get_conn ( self ) : \n    effective_user = self . proxy_user \n    autoconfig = self . autoconfig \n    use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' \n    try : \n        connections = self . get_connections ( self . hdfs_conn_id ) \n        if not effective_user : \n            effective_user = connections [ 0 ] . login \n        if not autoconfig : \n            autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) \n        hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) \n    except AirflowException : \n        if not autoconfig : \n            raise \n    if autoconfig : \n        client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) \n    elif len ( connections ) == 1 : \n        client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    elif 1 < len ( connections ) : \n        nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] \n        client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    else : \n        raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) \n    return client "}
{"557": "\ndef health ( self , session = None ) : \n    BJ = jobs . BaseJob \n    payload = { } \n    scheduler_health_check_threshold = timedelta ( seconds = conf . getint ( 'scheduler' , 'scheduler_health_check_threshold' ) ) \n    latest_scheduler_heartbeat = None \n    payload [ 'metadatabase' ] = { 'status' : 'healthy' } \n    try : \n        latest_scheduler_heartbeat = session . query ( func . max ( BJ . latest_heartbeat ) ) . filter ( BJ . state == 'running' , BJ . job_type == 'SchedulerJob' ) . scalar ( ) \n    except Exception : \n        payload [ 'metadatabase' ] [ 'status' ] = 'unhealthy' \n    if not latest_scheduler_heartbeat : \n        scheduler_status = 'unhealthy' \n    else : \n        if scheduler_health_check_threshold >= timezone . utcnow ( ) - latest_scheduler_heartbeat : \n            scheduler_status = 'healthy' \n        else : \n            scheduler_status = 'unhealthy' \n    payload [ 'scheduler' ] = { 'status' : scheduler_status , 'latest_scheduler_heartbeat' : str ( latest_scheduler_heartbeat ) } \n    return wwwutils . json_response ( payload ) "}
{"564": "\ndef fallback_to_default_project_id ( func ) : \n    \n    @ functools . wraps ( func ) \n    def inner_wrapper ( self , * args , ** kwargs ) : \n        if 0 < len ( args ) : \n            raise AirflowException ( \"You must use keyword arguments in this methods rather than\" \" positional\" ) \n        if 'project_id' in kwargs : \n            kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) \n        else : \n            kwargs [ 'project_id' ] = self . _get_project_id ( None ) \n        if not kwargs [ 'project_id' ] : \n            raise AirflowException ( \"The project id must be passed either as \" \"keyword project_id parameter or as project_id extra \" \"in GCP connection definition. Both are not set!\" ) \n        return func ( self , * args , ** kwargs ) \n    return inner_wrapper "}
{"569": "\ndef resize ( img , size , interpolation = Image . BILINEAR ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not ( isinstance ( size , int ) or ( isinstance ( size , Iterable ) and len ( size ) == 2 ) ) : \n        raise TypeError ( 'Got inappropriate size arg: {}' . format ( size ) ) \n    if isinstance ( size , int ) : \n        w , h = img . size \n        if ( h >= w and w == size ) or ( w >= h and h == size ) : \n            return img \n        if h > w : \n            ow = size \n            oh = int ( size * h / w ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n        else : \n            oh = size \n            ow = int ( size * w / h ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n    else : \n        return img . resize ( size [ : : - 1 ] , interpolation ) "}
{"576": "\ndef five_crop ( img , size ) : \n    if isinstance ( size , numbers . Number ) : \n        size = ( int ( size ) , int ( size ) ) \n    else : \n        assert len ( size ) == 2 , \"Please provide only two dimensions (h, w) for size.\" \n    w , h = img . size \n    crop_h , crop_w = size \n    if w < crop_w or h < crop_h : \n        raise ValueError ( \"Requested crop size {} is bigger than input size {}\" . format ( size , ( h , w ) ) ) \n    tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) \n    tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) \n    bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) \n    br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) \n    center = center_crop ( img , ( crop_h , crop_w ) ) \n    return ( tl , tr , bl , br , center ) "}
{"581": "\ndef adjust_gamma ( img , gamma , gain = 1 ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if 0 > gamma : \n        raise ValueError ( 'Gamma should be a non-negative real number' ) \n    input_mode = img . mode \n    img = img . convert ( 'RGB' ) \n    gamma_map = [ 255 * gain * pow ( ele / 255. , gamma ) for ele in range ( 256 ) ] * 3 \n    img = img . point ( gamma_map ) \n    img = img . convert ( input_mode ) \n    return img "}
{"583": "\ndef affine ( img , angle , translate , scale , shear , resample = 0 , fillcolor = None ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    assert isinstance ( translate , ( tuple , list ) ) and len ( translate ) == 2 , \"Argument translate should be a list or tuple of length 2\" \n    assert 0.0 < scale , \"Argument scale should be positive\" \n    output_size = img . size \n    center = ( img . size [ 0 ] * 0.5 + 0.5 , img . size [ 1 ] * 0.5 + 0.5 ) \n    matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) \n    kwargs = { \"fillcolor\" : fillcolor } if PILLOW_VERSION [ 0 ] == '5' else { } \n    return img . transform ( output_size , Image . AFFINE , matrix , resample , ** kwargs ) "}
{"586": "\ndef _find_classes ( self , dir ) : \n    if ( 3 , 5 ) <= sys . version_info : \n        classes = [ d . name for d in os . scandir ( dir ) if d . is_dir ( ) ] \n    else : \n        classes = [ d for d in os . listdir ( dir ) if os . path . isdir ( os . path . join ( dir , d ) ) ] \n    classes . sort ( ) \n    class_to_idx = { classes [ i ] : i for i in range ( len ( classes ) ) } \n    return classes , class_to_idx "}
{"598": "\ndef get_params ( img , scale , ratio ) : \n    area = img . size [ 0 ] * img . size [ 1 ] \n    for attempt in range ( 10 ) : \n        target_area = random . uniform ( * scale ) * area \n        log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) \n        aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) \n        w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) \n        h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) \n        if img . size [ 0 ] >= w and img . size [ 1 ] >= h : \n            i = random . randint ( 0 , img . size [ 1 ] - h ) \n            j = random . randint ( 0 , img . size [ 0 ] - w ) \n            return i , j , h , w \n    in_ratio = img . size [ 0 ] / img . size [ 1 ] \n    if ( min ( ratio ) > in_ratio ) : \n        w = img . size [ 0 ] \n        h = w / min ( ratio ) \n    elif ( max ( ratio ) < in_ratio ) : \n        h = img . size [ 1 ] \n        w = h * max ( ratio ) \n    else : \n        w = img . size [ 0 ] \n        h = img . size [ 1 ] \n    i = ( img . size [ 1 ] - h ) // 2 \n    j = ( img . size [ 0 ] - w ) // 2 \n    return i , j , h , w "}
{"605": "\ndef autocompleter ( ) : \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    if PY3 : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , b'' ) , disabled_engines ) \n    else : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , u'' ) . encode ( 'utf-8' ) , disabled_engines ) \n    raw_text_query . parse_query ( ) \n    if not raw_text_query . getSearchQuery ( ) : \n        return '' , 400 \n    completer = autocomplete_backends . get ( request . preferences . get_value ( 'autocomplete' ) ) \n    raw_results = searx_bang ( raw_text_query ) \n    if 3 >= len ( raw_results ) and completer : \n        language = request . preferences . get_value ( 'language' ) \n        if not language or language == 'all' : \n            language = 'en' \n        else : \n            language = language . split ( '-' ) [ 0 ] \n        raw_results . extend ( completer ( raw_text_query . getSearchQuery ( ) , language ) ) \n    results = [ ] \n    for result in raw_results : \n        raw_text_query . changeSearchQuery ( result ) \n        results . append ( raw_text_query . getFullQuery ( ) ) \n    if request . form . get ( 'format' ) == 'x-suggestions' : \n        return Response ( json . dumps ( [ raw_text_query . query , results ] ) , mimetype = 'application/json' ) \n    return Response ( json . dumps ( results ) , mimetype = 'application/json' ) "}
{"606": "\ndef preferences ( ) : \n    if request . method == 'POST' : \n        resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) \n        try : \n            request . preferences . parse_form ( request . form ) \n        except ValidationException : \n            request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) \n            return resp \n        return request . preferences . save ( resp ) \n    image_proxy = request . preferences . get_value ( 'image_proxy' ) \n    lang = request . preferences . get_value ( 'language' ) \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    allowed_plugins = request . preferences . plugins . get_enabled ( ) \n    stats = { } \n    for c in categories : \n        for e in categories [ c ] : \n            stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } \n            if settings [ 'outgoing' ] [ 'request_timeout' ] < e . timeout : \n                stats [ e . name ] [ 'warn_timeout' ] = True \n            stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) \n    for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : \n        stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) \n        if settings [ 'outgoing' ] [ 'request_timeout' ] < engine_stat . get ( 'avg' ) : \n            stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True \n    return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True ) "}
{"608": "\ndef searx_bang ( full_query ) : \n    if len ( full_query . getSearchQuery ( ) ) == 0 : \n        return [ ] \n    results = [ ] \n    first_char = full_query . getSearchQuery ( ) [ 0 ] \n    if first_char == '!' or first_char == '?' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( first_char + \"images\" ) \n            results . append ( first_char + \"wikipedia\" ) \n            results . append ( first_char + \"osm\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for categorie in categories : \n                if categorie . startswith ( engine_query ) : \n                    results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) \n            for engine in engines : \n                if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) \n            for engine_shortcut in engine_shortcuts : \n                if engine_shortcut . startswith ( engine_query ) : \n                    results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) \n    elif first_char == ':' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( \":en\" ) \n            results . append ( \":en_us\" ) \n            results . append ( \":english\" ) \n            results . append ( \":united_kingdom\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for lc in language_codes : \n                lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) \n                if lang_id . startswith ( engine_query ) : \n                    if 2 >= len ( engine_query ) : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) \n                    else : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) \n                if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : \n                    results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) \n                if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) \n    result_set = set ( results ) \n    for query_part in full_query . query_parts : \n        if query_part in result_set : \n            result_set . remove ( query_part ) \n    return list ( result_set ) "}
{"621": "\ndef _max_mask_non_finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : \n    m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) \n    needs_masking = ~ np . isfinite ( m ) \n    if 0 < needs_masking . ndim : \n        m [ needs_masking ] = mask \n    elif needs_masking : \n        m = mask \n    return m "}
{"631": "\ndef toy_logistic_data ( num_examples , input_size = 2 , weights_prior_stddev = 5.0 ) : \n    random_weights = weights_prior_stddev * np . random . randn ( input_size ) \n    random_bias = np . random . randn ( ) \n    design_matrix = np . random . rand ( num_examples , input_size ) * 2 - 1 \n    logits = np . reshape ( np . dot ( design_matrix , random_weights ) + random_bias , ( - 1 , 1 ) ) \n    p_labels = 1. / ( 1 + np . exp ( - logits ) ) \n    labels = np . int32 ( np . random . rand ( num_examples , 1 ) < p_labels ) \n    return random_weights , random_bias , np . float32 ( design_matrix ) , labels "}
{"641": "\ndef metropolis_hastings_step ( current_state : State , proposed_state : State , energy_change : FloatTensor , seed = None ) -> Tuple [ State , tf . Tensor , tf . Tensor ] : \n    flat_current = tf . nest . flatten ( current_state ) \n    flat_proposed = nest . flatten_up_to ( current_state , proposed_state ) \n    flat_current = [ p if c is None else c for p , c in zip ( flat_proposed , flat_current ) ] \n    current_state = tf . nest . pack_sequence_as ( current_state , flat_current ) \n    current_state = tf . nest . map_structure ( tf . convert_to_tensor , current_state ) \n    proposed_state = tf . nest . map_structure ( tf . convert_to_tensor , proposed_state ) \n    energy_change = tf . convert_to_tensor ( value = energy_change ) \n    log_accept_ratio = - energy_change \n    log_uniform = tf . math . log ( tf . random . uniform ( shape = tf . shape ( input = log_accept_ratio ) , dtype = log_accept_ratio . dtype . base_dtype , seed = seed ) ) \n    is_accepted = log_accept_ratio > log_uniform \n    next_state = mcmc_util . choose ( is_accepted , proposed_state , current_state , name = 'choose_next_state' ) \n    return next_state , is_accepted , log_uniform "}
{"643": "\ndef sign_adaptation ( control : FloatNest , output : FloatTensor , set_point : FloatTensor , adaptation_rate : FloatTensor = 0.01 ) -> FloatNest : \n    def _get_new_control ( control , output , set_point ) : \n        new_control = mcmc_util . choose ( set_point < output , control * ( 1. + adaptation_rate ) , control / ( 1. + adaptation_rate ) ) \n        return new_control \n    output = maybe_broadcast_structure ( output , control ) \n    set_point = maybe_broadcast_structure ( set_point , control ) \n    return tf . nest . map_structure ( _get_new_control , control , output , set_point ) "}
{"653": "\ndef _ensure_tf_install ( ) : \n    try : \n        import tensorflow as tf \n    except ImportError : \n        print ( \"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \" \"installed by default when you install TensorFlow Probability. This \" \"is so that users can decide whether to install the GPU-enabled \" \"TensorFlow package. To use TensorFlow Probability, please install \" \"the most recent version of TensorFlow, by following instructions at \" \"https://tensorflow.org/install.\\n\\n\" ) \n        raise \n    import distutils . version \n    required_tensorflow_version = \"1.13\" \n    if ( distutils . version . LooseVersion ( required_tensorflow_version ) > distutils . version . LooseVersion ( tf . __version__ ) ) : \n        raise ImportError ( \"This version of TensorFlow Probability requires TensorFlow \" \"version >= {required}; Detected an installation of version {present}. \" \"Please upgrade TensorFlow to proceed.\" . format ( required = required_tensorflow_version , present = tf . __version__ ) ) "}
{"659": "\ndef _make_positive_axis ( axis , ndims ) : \n    axis = _make_list_or_1d_tensor ( axis ) \n    ndims = tf . convert_to_tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) \n    ndims_ = tf . get_static_value ( ndims ) \n    if _is_list_like ( axis ) and ndims_ is not None : \n        positive_axis = [ ] \n        for a in axis : \n            if 0 > a : \n                a = ndims_ + a \n            positive_axis . append ( a ) \n    else : \n        axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) \n        positive_axis = tf . where ( 0 <= axis , axis , axis + ndims ) \n    return positive_axis "}
{"665": "\ndef sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = True , seed = None , name = None ) : \n    if 1 > dim or _MAX_DIMENSION < dim : \n        raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) \n    if ( num_results is None ) == ( sequence_indices is None ) : \n        raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) \n    if not dtype . is_floating : \n        raise ValueError ( 'dtype must be of `float`-type' ) \n    with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : \n        if num_results is not None : \n            num_results = tf . convert_to_tensor ( value = num_results ) \n        if sequence_indices is not None : \n            sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) \n        indices = _get_indices ( num_results , sequence_indices , dtype ) \n        radixes = tf . constant ( _PRIMES [ 0 : dim ] , dtype = dtype , shape = [ dim , 1 ] ) \n        max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) \n        max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) \n        exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , 1 ] ) \n        weight_mask = max_sizes_by_axes <= exponents_by_axes \n        capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) \n        weights = radixes ** capped_exponents \n        coeffs = tf . math . floordiv ( indices , weights ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs %= radixes \n        if not randomized : \n            coeffs /= radixes \n            return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) \n        coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs /= radixes \n        base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        zero_correction = tf . random . uniform ( [ dim , 1 ] , seed = stream ( ) , dtype = dtype ) \n        zero_correction /= radixes ** max_sizes_by_axes \n        return base_values + tf . reshape ( zero_correction , [ - 1 ] ) "}
{"669": "\ndef _primes_less_than ( n ) : \n    small_primes = np . array ( ( 2 , 3 , 5 ) ) \n    if 6 >= n : \n        return small_primes [ n > small_primes ] \n    sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) \n    sieve [ 0 ] = False \n    m = int ( n ** 0.5 ) // 3 + 1 \n    for i in range ( m ) : \n        if not sieve [ i ] : \n            continue \n        k = 3 * i + 1 | 1 \n        sieve [ k ** 2 // 3 : : 2 * k ] = False \n        sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = False \n    return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ] "}
{"671": "\ndef hager_zhang ( value_and_gradients_function , initial_step_size = None , value_at_initial_step = None , value_at_zero = None , converged = None , threshold_use_approximate_wolfe_condition = 1e-6 , shrinkage_param = 0.66 , expansion_param = 5.0 , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , step_size_shrink_param = 0.1 , max_iterations = 50 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'hager_zhang' , [ initial_step_size , value_at_initial_step , value_at_zero , converged , threshold_use_approximate_wolfe_condition , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ] ) : \n        val_0 , val_initial , f_lim , prepare_evals = _prepare_args ( value_and_gradients_function , initial_step_size , value_at_initial_step , value_at_zero , threshold_use_approximate_wolfe_condition ) \n        valid_inputs = ( hzl . is_finite ( val_0 ) & ( 0 > val_0 . df ) & tf . math . is_finite ( val_initial . x ) & ( 0 < val_initial . x ) ) \n        if converged is None : \n            init_converged = tf . zeros_like ( valid_inputs ) \n        else : \n            init_converged = tf . convert_to_tensor ( value = converged ) \n        failed = ~ init_converged & ~ valid_inputs \n        active = ~ init_converged & valid_inputs \n        fix_step_evals , val_c , fix_failed = _fix_step_size ( value_and_gradients_function , val_initial , active , step_size_shrink_param ) \n        init_interval = HagerZhangLineSearchResult ( converged = init_converged , failed = failed | fix_failed , func_evals = prepare_evals + fix_step_evals , iterations = tf . convert_to_tensor ( value = 0 ) , left = val_0 , right = hzl . val_where ( init_converged , val_0 , val_c ) ) \n        def _apply_bracket_and_search ( ) : \n            return _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) \n        init_active = ~ init_interval . failed & ~ init_interval . converged \n        return prefer_static . cond ( tf . reduce_any ( input_tensor = init_active ) , _apply_bracket_and_search , lambda : init_interval ) "}
{"672": "\ndef _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : \n    iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) \n    def _cond ( i , val_c , to_fix ) : \n        del val_c \n        return ( iter_max > i ) & tf . reduce_any ( input_tensor = to_fix ) \n    def _body ( i , val_c , to_fix ) : \n        next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) \n        next_val_c = value_and_gradients_function ( next_c ) \n        still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) \n        return ( i + 1 , next_val_c , still_to_fix ) \n    to_fix = active & ~ hzl . is_finite ( val_c_input ) \n    return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( 0 , val_c_input , to_fix ) ) "}
{"674": "\ndef _line_search_after_bracketing ( value_and_gradients_function , search_interval , val_0 , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param ) : \n    def _loop_cond ( curr_interval ) : \n        active = ~ ( curr_interval . converged | curr_interval . failed ) \n        return ( max_iterations > curr_interval . iterations ) & tf . reduce_any ( input_tensor = active ) \n    def _loop_body ( curr_interval ) : \n        secant2_raw_result = hzl . secant2 ( value_and_gradients_function , val_0 , curr_interval , f_lim , sufficient_decrease_param , curvature_param ) \n        secant2_result = HagerZhangLineSearchResult ( converged = secant2_raw_result . converged , failed = secant2_raw_result . failed , iterations = curr_interval . iterations + 1 , func_evals = secant2_raw_result . num_evals , left = secant2_raw_result . left , right = secant2_raw_result . right ) \n        should_check_shrinkage = ~ ( secant2_result . converged | secant2_result . failed ) \n        def _do_check_shrinkage ( ) : \n            old_width = curr_interval . right . x - curr_interval . left . x \n            new_width = secant2_result . right . x - secant2_result . left . x \n            sufficient_shrinkage = old_width * shrinkage_param > new_width \n            func_is_flat = ( _very_close ( curr_interval . left . f , curr_interval . right . f ) & _very_close ( secant2_result . left . f , secant2_result . right . f ) ) \n            new_converged = ( should_check_shrinkage & sufficient_shrinkage & func_is_flat ) \n            needs_inner_bisect = should_check_shrinkage & ~ sufficient_shrinkage \n            inner_bisect_args = secant2_result . _replace ( converged = secant2_result . converged | new_converged ) \n            def _apply_inner_bisect ( ) : \n                return _line_search_inner_bisection ( value_and_gradients_function , inner_bisect_args , needs_inner_bisect , f_lim ) \n            return prefer_static . cond ( tf . reduce_any ( input_tensor = needs_inner_bisect ) , _apply_inner_bisect , lambda : inner_bisect_args ) \n        next_args = prefer_static . cond ( tf . reduce_any ( input_tensor = should_check_shrinkage ) , _do_check_shrinkage , lambda : secant2_result ) \n        interval_shrunk = ( ~ next_args . failed & _very_close ( next_args . left . x , next_args . right . x ) ) \n        return [ next_args . _replace ( converged = next_args . converged | interval_shrunk ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ search_interval ] , parallel_iterations = 1 ) [ 0 ] "}
{"690": "\ndef _marginal_hidden_probs ( self ) : \n    initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) \n    if 1 < self . _num_steps : \n        transition_log_probs = self . _log_trans \n        def forward_step ( log_probs , _ ) : \n            return _log_vector_matrix ( log_probs , transition_log_probs ) \n        dummy_index = tf . zeros ( self . _num_steps - 1 , dtype = tf . float32 ) \n        forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = \"forward_log_probs\" ) \n        forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) \n    else : \n        forward_log_probs = initial_log_probs [ tf . newaxis , ... ] \n    return tf . exp ( forward_log_probs ) "}
{"698": "\ndef _build_trainable_posterior ( param , initial_loc_fn ) : \n    loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) \n    scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) \n    q = tfd . Normal ( loc = loc , scale = scale ) \n    if ( param . prior . event_shape . ndims is None or 0 < param . prior . event_shape . ndims ) : \n        q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) \n    return tfd . TransformedDistribution ( q , param . bijector ) "}
{"700": "\ndef _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : \n    optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer \n    def train_loop_body ( step ) : \n        train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) \n        return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) \n    minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : num_steps > step , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = True ) [ 0 ] \n    return minimize_op "}
{"712": "\ndef _unify_call_signature ( i , dist_fn ) : \n    if distribution_util . is_distribution_instance ( dist_fn ) : \n        return ( lambda * _ : dist_fn ) , None \n    if not callable ( dist_fn ) : \n        raise TypeError ( '{} must be either `tfd.Distribution`-like or ' '`callable`.' . format ( dist_fn ) ) \n    args = _get_required_args ( dist_fn ) \n    if not args : \n        return ( lambda * _ : dist_fn ( ) ) , ( ) \n    \n    @ functools . wraps ( dist_fn ) \n    def dist_fn_wrapped ( * xs ) : \n        if i != len ( xs ) : \n            raise ValueError ( 'Internal Error: Unexpected number of inputs provided to {}-th ' 'distribution maker (dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , i , len ( xs ) ) ) \n        if len ( args ) > len ( xs ) : \n            raise ValueError ( 'Internal Error: Too few inputs provided to {}-th distribution maker ' '(dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , len ( args ) , len ( xs ) ) ) \n        return dist_fn ( * reversed ( xs [ - len ( args ) : ] ) ) \n    return dist_fn_wrapped , args "}
{"727": "\ndef call ( self , inputs , state ) : \n    original_shape = inputs . shape \n    if 2 > len ( original_shape ) : \n        inputs = tf . reshape ( inputs , [ 1 , - 1 ] ) \n    out , state = self . lstm_cell ( inputs , state ) \n    out = self . output_layer ( out ) \n    correct_shape = tf . concat ( ( original_shape [ : - 1 ] , tf . shape ( input = out ) [ - 1 : ] ) , 0 ) \n    out = tf . reshape ( out , correct_shape ) \n    loc = out [ ... , : self . dimensions ] \n    scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 \n    return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state "}
{"737": "\ndef _compute_min_event_ndims ( bijector_list , compute_forward = True ) : \n    min_event_ndims = 0 \n    rank_changed_adjusted_max_min_event_ndims = 0 \n    if compute_forward : \n        bijector_list = reversed ( bijector_list ) \n    for b in bijector_list : \n        if compute_forward : \n            current_min_event_ndims = b . forward_min_event_ndims \n            current_inverse_min_event_ndims = b . inverse_min_event_ndims \n        else : \n            current_min_event_ndims = b . inverse_min_event_ndims \n            current_inverse_min_event_ndims = b . forward_min_event_ndims \n        if current_min_event_ndims > rank_changed_adjusted_max_min_event_ndims : \n            min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) \n        number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions \n    return min_event_ndims "}
{"744": "\ndef log_ndtr ( x , series_order = 3 , name = \"log_ndtr\" ) : \n    if not isinstance ( series_order , int ) : \n        raise TypeError ( \"series_order must be a Python integer.\" ) \n    if 0 > series_order : \n        raise ValueError ( \"series_order must be non-negative.\" ) \n    if 30 < series_order : \n        raise ValueError ( \"series_order must be <= 30.\" ) \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        if dtype_util . base_equal ( x . dtype , tf . float64 ) : \n            lower_segment = LOGNDTR_FLOAT64_LOWER \n            upper_segment = LOGNDTR_FLOAT64_UPPER \n        elif dtype_util . base_equal ( x . dtype , tf . float32 ) : \n            lower_segment = LOGNDTR_FLOAT32_LOWER \n            upper_segment = LOGNDTR_FLOAT32_UPPER \n        else : \n            raise TypeError ( \"x.dtype=%s is not supported.\" % x . dtype ) \n        return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) ) "}
{"745": "\ndef _log_ndtr_asymptotic_series ( x , series_order ) : \n    npdt = dtype_util . as_numpy_dtype ( x . dtype ) \n    if 0 >= series_order : \n        return npdt ( 1 ) \n    x_2 = tf . square ( x ) \n    even_sum = tf . zeros_like ( x ) \n    odd_sum = tf . zeros_like ( x ) \n    x_2n = x_2 \n    for n in range ( 1 , series_order + 1 ) : \n        y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n \n        if n % 2 : \n            odd_sum += y \n        else : \n            even_sum += y \n        x_2n *= x_2 \n    return 1. + even_sum - odd_sum "}
{"747": "\ndef log_cdf_laplace ( x , name = \"log_cdf_laplace\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        lower_solution = - np . log ( 2. ) + x \n        safe_exp_neg_x = tf . exp ( - tf . abs ( x ) ) \n        upper_solution = tf . math . log1p ( - 0.5 * safe_exp_neg_x ) \n        return tf . where ( 0. > x , lower_solution , upper_solution ) "}
{"748": "\ndef text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : \n    alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) \n    rv_lambda = tfd . Exponential ( rate = alpha ) \n    rv_tau = tfd . Uniform ( ) \n    lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) >= tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) \n    rv_observation = tfd . Poisson ( rate = lambda_ ) \n    return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) ) "}
{"782": "\ndef _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : \n    with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : \n        states = tf . convert_to_tensor ( value = states , name = 'states' ) \n        dt = states . dtype \n        auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) \n        if filter_threshold is not None : \n            filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) \n            mask = filter_threshold > auto_corr \n            mask = tf . cast ( mask , dtype = dt ) \n            mask = tf . cumsum ( mask , axis = 0 ) \n            mask = tf . maximum ( 1. - mask , 0. ) \n            auto_corr *= mask \n        n = _axis_size ( states , axis = 0 ) \n        k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) \n        nk_factor = ( n - k ) / n \n        if auto_corr . shape . ndims is not None : \n            new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) \n        else : \n            new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) \n        nk_factor = tf . reshape ( nk_factor , new_shape ) \n        return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) ) "}
{"794": "\ndef slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        batch_shape = tf . shape ( input = x_initial ) \n        dtype = step_size . dtype . base_dtype \n        left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) \n        left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) \n        left_endpoints -= left_increments \n        right_endpoints = left_endpoints + widths \n        left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) \n        right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) \n        left_ok = log_slice_heights > left_ep_values \n        right_ok = log_slice_heights > right_ep_values \n        both_ok = left_ok & right_ok \n        both_ok_f = tf . reshape ( both_ok , [ max_doublings + 1 , - 1 ] ) \n        best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) \n        point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( input = best_interval_idx ) ) ] , axis = 1 , name = 'point_index_gather' ) \n        left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + 1 , - 1 ] ) \n        right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + 1 , - 1 ] ) \n        lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) \n        upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) \n        both_ok = tf . reduce_any ( input_tensor = both_ok , axis = 0 ) \n        return upper_bounds , lower_bounds , both_ok "}
{"795": "\ndef _sample_with_shrinkage ( x_initial , target_log_prob , log_slice_heights , step_size , lower_bounds , upper_bounds , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_with_shrinkage' , [ x_initial , log_slice_heights , step_size , lower_bounds , upper_bounds ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = '_sample_with_shrinkage' ) \n        found = tf . zeros_like ( x_initial , dtype = tf . bool ) \n        cond = lambda found , * ignored_args : ~ tf . reduce_all ( input_tensor = found ) \n        x_next = tf . identity ( x_initial ) \n        x_initial_shape = tf . shape ( input = x_initial ) \n        x_initial_dtype = x_initial . dtype . base_dtype \n        def _body ( found , left , right , x_next ) : \n            proportions = tf . random . uniform ( x_initial_shape , dtype = x_initial_dtype , seed = seed_gen ( ) ) \n            x_proposed = tf . where ( ~ found , left + proportions * ( right - left ) , x_next ) \n            accept_res = _test_acceptance ( x_initial , target_log_prob = target_log_prob , decided = found , log_slice_heights = log_slice_heights , x_proposed = x_proposed , step_size = step_size , lower_bounds = left , upper_bounds = right ) \n            boundary_test = target_log_prob ( x_proposed ) > log_slice_heights \n            can_accept = boundary_test & accept_res \n            next_found = found | can_accept \n            next_left = tf . where ( x_initial > x_proposed , x_proposed , left ) \n            next_right = tf . where ( x_initial <= x_proposed , x_proposed , right ) \n            return next_found , next_left , next_right , x_proposed \n        return tf . while_loop ( cond = cond , body = _body , loop_vars = ( found , lower_bounds , upper_bounds , x_next ) ) [ - 1 ] "}
{"801": "\ndef _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : \n    if depth == 0 : \n        [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) \n        next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) \n        num_states = tf . cast ( log_slice_sample < next_log_joint , dtype = tf . int32 ) \n        continue_trajectory = ( log_slice_sample - max_simulation_error < next_log_joint ) \n        return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] \n    seed_stream = tfd . SeedStream ( seed , \"build_tree\" ) \n    [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n    if continue_trajectory : \n        if 0 > direction : \n            [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        else : \n            [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        num_states += far_num_states \n        accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) \n        if accept_far_state : \n            next_state = far_state \n            next_target_log_prob = far_target_log_prob \n            next_grads_target_log_prob = far_grads_target_log_prob \n        has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) \n        continue_trajectory = far_continue_trajectory and has_no_u_turn \n    return [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] "}
{"803": "\ndef _has_no_u_turn ( state_one , state_two , momentum ) : \n    dot_product = sum ( [ tf . reduce_sum ( input_tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state_one , state_two , momentum ) ] ) \n    return 0 < dot_product "}
{"837": "\ndef _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : \n    ds = [ ] \n    values_out = [ ] \n    seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) \n    gen = self . _model ( ) \n    index = 0 \n    d = next ( gen ) \n    try : \n        while True : \n            actual_distribution = d . distribution if isinstance ( d , self . Root ) else d \n            ds . append ( actual_distribution ) \n            if ( value is not None and index < len ( value ) and value [ index ] is not None ) : \n                seed ( ) \n                next_value = value [ index ] \n            else : \n                next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) \n            values_out . append ( next_value ) \n            index += 1 \n            d = gen . send ( next_value ) \n    except StopIteration : \n        pass \n    return ds , values_out "}
{"844": "\ndef minimize ( grad_and_hessian_loss_fn , x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = 1 , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : \n    graph_deps = [ x_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] , \n    with tf . compat . v1 . name_scope ( name , 'minimize' , graph_deps ) : \n        def _loop_cond ( x_start , converged , iter_ ) : \n            del x_start \n            return tf . logical_and ( maximum_iterations > iter_ , tf . logical_not ( converged ) ) \n        def _loop_body ( x_start , converged , iter_ ) : \n            g , h_outer , h_middle = grad_and_hessian_loss_fn ( x_start ) \n            x_start , converged , _ = minimize_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = h_outer , hessian_unregularized_loss_middle = h_middle , x_start = x_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps_per_iteration , tolerance = tolerance , learning_rate = learning_rate ) \n            return x_start , converged , iter_ + 1 \n        return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ x_start , tf . zeros ( [ ] , np . bool , name = 'converged' ) , tf . zeros ( [ ] , np . int32 , name = 'iter' ) , ] ) "}
{"858": "\ndef minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = False , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = 1 , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : \n        ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) \n        domain_dtype = simplex . dtype \n        ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) \n        closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) \n        def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : \n            ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , ** closure_kwargs ) \n            return ( converged , iterations + 1 , next_simplex , next_objective , num_evaluations + evaluations ) \n        initial_args = ( False , 0 , simplex , objective_at_simplex , num_evaluations ) \n        def _is_converged ( converged , num_iterations , * ignored_args ) : \n            not_converged = tf . logical_not ( converged ) \n            return ( not_converged if max_iterations is None else ( not_converged & ( max_iterations > num_iterations ) ) ) \n        ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) \n        order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = True ) \n        best_index = order [ 0 ] \n        return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) \n        ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex \n        face_centroid /= tf . cast ( dim , domain_dtype ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = 1 \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return ( True , current_simplex , current_objective_values , 0 ) \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( second_worst_objective_value > objective_at_reflected ) & ( best_objective_value <= objective_at_reflected ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = best_objective_value > objective_at_reflected \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( worst_objective_value > objective_at_reflected ) & ( second_worst_objective_value <= objective_at_reflected ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) "}
{"861": "\ndef _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : \n    def _expand_and_maybe_replace ( ) : \n        expanded = face_centroid + expansion * ( reflected - face_centroid ) \n        expanded_objective_value = objective_function ( expanded ) \n        expanded_is_better = ( objective_at_reflected > expanded_objective_value ) \n        accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) \n        accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) \n        next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) \n        next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) \n        next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) \n        return False , next_simplex , next_objective_at_simplex , 1 \n    return _expand_and_maybe_replace "}
{"862": "\ndef _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : \n    def _contraction ( ) : \n        contracted = face_centroid + contraction * ( reflected - face_centroid ) \n        objective_at_contracted = objective_function ( contracted ) \n        is_contracted_acceptable = objective_at_reflected >= objective_at_contracted \n        def _accept_contraction ( ) : \n            next_simplex = _replace_at_index ( simplex , worst_index , contracted ) \n            objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) \n            return ( False , next_simplex , objective_at_next_simplex , 1 ) \n        def _reject_contraction ( ) : \n            return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) \n        return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) \n    return _contraction "}
{"865": "\ndef _check_convergence ( simplex , best_vertex , best_objective , worst_objective , func_tolerance , position_tolerance ) : \n    objective_convergence = func_tolerance > tf . abs ( worst_objective - best_objective ) \n    simplex_degeneracy = position_tolerance > tf . reduce_max ( input_tensor = tf . abs ( simplex - best_vertex ) ) \n    return objective_convergence | simplex_degeneracy "}
{"876": "\ndef _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : \n    df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) \n    df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) \n    denom = tf . where ( 2. < df , df - 2. , tf . ones_like ( df ) ) \n    statistic = statistic * df_factor_fn ( df / denom ) \n    inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) \n    result_where_defined = tf . where ( 2. < df , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = \"inf\" ) ) \n    if self . allow_nan_stats : \n        nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) \n        return tf . where ( 1. < df , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = \"nan\" ) ) \n    else : \n        with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + \" not defined for components of df <= 1\" ) , ] ) : \n            return tf . identity ( result_where_defined ) "}
{"887": "\ndef _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : \n    param_shape = tf . shape ( input = param ) \n    insert_ones = tf . ones ( [ tf . size ( input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) \n    new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = 0 ) \n    full_batch_param = tf . reshape ( param , new_param_shape ) \n    param_slices = [ ] \n    param_dim_idx = 0 \n    batch_dim_idx = 0 \n    for slc in slices : \n        if slc is tf . newaxis : \n            param_slices . append ( slc ) \n            continue \n        if slc is Ellipsis : \n            if 0 > batch_dim_idx : \n                raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) \n            param_slices . append ( slc ) \n            num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + 1 : ] ] ) \n            batch_dim_idx = - num_remaining_non_newaxis_slices \n            param_dim_idx = batch_dim_idx - param_event_ndims \n            continue \n        param_dim_size = new_param_shape [ param_dim_idx ] \n        batch_dim_size = dist_batch_shape [ batch_dim_idx ] \n        is_broadcast = param_dim_size < batch_dim_size \n        if isinstance ( slc , slice ) : \n            start , stop , step = slc . start , slc . stop , slc . step \n            if start is not None : \n                start = tf . where ( is_broadcast , 0 , start ) \n            if stop is not None : \n                stop = tf . where ( is_broadcast , 1 , stop ) \n            if step is not None : \n                step = tf . where ( is_broadcast , 1 , step ) \n            param_slices . append ( slice ( start , stop , step ) ) \n        else : \n            param_slices . append ( tf . where ( is_broadcast , 0 , slc ) ) \n        param_dim_idx += 1 \n        batch_dim_idx += 1 \n    param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) \n    return full_batch_param . __getitem__ ( param_slices ) "}
{"893": "\ndef convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : \n    def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : \n        relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous - model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) \n        return ( 0 < iter_ ) & tf . reduce_all ( input_tensor = tolerance > relative_euclidean_norm ) \n    return convergence_criteria_fn "}
{"910": "\ndef move_dimension ( x , source_idx , dest_idx ) : \n    ndims = prefer_static_rank ( x ) \n    dtype = dtype_util . common_dtype ( [ source_idx , dest_idx ] , preferred_dtype = tf . int32 ) \n    source_idx = tf . convert_to_tensor ( value = source_idx , dtype = dtype ) \n    dest_idx = tf . convert_to_tensor ( value = dest_idx , dtype = dtype ) \n    source_idx = pick_scalar_condition ( 0 > source_idx , ndims + source_idx , source_idx ) \n    dest_idx = pick_scalar_condition ( 0 > dest_idx , ndims + dest_idx , dest_idx ) \n    def move_left_permutation ( ) : \n        return prefer_static_value ( tf . concat ( [ tf . range ( 0 , dest_idx , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) \n    def move_right_permutation ( ) : \n        return prefer_static_value ( tf . concat ( [ tf . range ( 0 , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , dest_idx + 1 , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) \n    def x_permuted ( ) : \n        return tf . transpose ( a = x , perm = prefer_static . cond ( dest_idx > source_idx , move_right_permutation , move_left_permutation ) ) \n    return prefer_static . cond ( tf . equal ( source_idx , dest_idx ) , lambda : x , x_permuted ) "}
{"919": "\ndef embed_check_categorical_event_shape ( categorical_param , name = \"embed_check_categorical_event_shape\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = categorical_param , name = \"categorical_param\" ) \n        x_dtype = dtype_util . base_dtype ( x . dtype ) \n        max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else 0 ) \n        if max_event_size is 0 : \n            raise TypeError ( \"Unable to validate size of unrecognized dtype \" \"({}).\" . format ( dtype_util . name ( x_dtype ) ) ) \n        try : \n            x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) \n        except ValueError : \n            raise ValueError ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) \n        event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) \n        if event_size is not None : \n            if 2 > event_size : \n                raise ValueError ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) \n            if max_event_size < event_size : \n                raise ValueError ( \"Number of classes exceeds `dtype` precision, i.e., \" \"{} implies shape ({}) cannot exceed {}.\" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) \n            return x \n        else : \n            event_size = tf . shape ( input = x , out_type = tf . int64 , name = \"x_shape\" ) [ - 1 ] \n            return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2 , message = ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = \"Number of classes exceeds `dtype` precision, \" \"i.e., {} dtype cannot exceed {} shape.\" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x ) "}
{"921": "\ndef rotate_transpose ( x , shift , name = \"rotate_transpose\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        shift = tf . convert_to_tensor ( value = shift , name = \"shift\" ) \n        assert_util . assert_integer ( shift ) \n        shift_value_static = tf . get_static_value ( shift ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is not None and shift_value_static is not None : \n            if 2 > ndims : \n                return x \n            shift_value_static = np . sign ( shift_value_static ) * ( abs ( shift_value_static ) % ndims ) \n            if shift_value_static == 0 : \n                return x \n            perm = np . roll ( np . arange ( ndims ) , shift_value_static ) \n            return tf . transpose ( a = x , perm = perm ) \n        else : \n            ndims = tf . rank ( x ) \n            shift = tf . where ( tf . less ( shift , 0 ) , - shift % ndims , ndims - shift % ndims ) \n            first = tf . range ( 0 , shift ) \n            last = tf . range ( shift , ndims ) \n            perm = tf . concat ( [ last , first ] , 0 ) \n            return tf . transpose ( a = x , perm = perm ) "}
{"931": "\ndef _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : \n            raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) \n        if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : \n            if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != 0 : \n                raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] \n        rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) \n        msg = '`rightmost_transposed_ndims` must be non-negative.' \n        if rightmost_transposed_ndims_ is not None : \n            if 0 > rightmost_transposed_ndims_ : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] \n        return assertions "}
{"933": "\ndef _event_shape ( self , shape , static_perm_to_shape ) : \n    rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) \n    if tensorshape_util . rank ( shape ) is None or rightmost_ is None : \n        return tf . TensorShape ( None ) \n    if rightmost_ > tensorshape_util . rank ( shape ) : \n        raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) \n    perm_ = tf . get_static_value ( self . perm , partial = True ) \n    if perm_ is None : \n        return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) \n    if sum ( p is None for p in perm_ ) == 1 : \n        present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) \n        for i , p in enumerate ( present [ 1 : ] ) : \n            if i != p : \n                perm_ = [ i if p is None else p for p in perm_ ] \n                break \n    return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) ) "}
{"939": "\ndef _augment_sample_shape ( partial_batch_dist , full_sample_and_batch_shape , validate_args = False ) : \n    full_ndims = distribution_util . prefer_static_shape ( full_sample_and_batch_shape ) [ 0 ] \n    partial_batch_ndims = ( tensorshape_util . rank ( partial_batch_dist . batch_shape ) if tensorshape_util . rank ( partial_batch_dist . batch_shape ) is not None else distribution_util . prefer_static_shape ( partial_batch_dist . batch_shape_tensor ( ) ) [ 0 ] ) \n    num_broadcast_dims = full_ndims - partial_batch_ndims \n    expected_partial_batch_shape = ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    expected_partial_batch_shape_static = tf . get_static_value ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    num_broadcast_dims_static = tf . get_static_value ( num_broadcast_dims ) \n    if num_broadcast_dims_static is not None : \n        if 0 > num_broadcast_dims_static : \n            raise ValueError ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions\" . format ( partial_batch_dist ) ) \n    if ( expected_partial_batch_shape_static is not None and tensorshape_util . is_fully_defined ( partial_batch_dist . batch_shape ) ) : \n        if ( partial_batch_dist . batch_shape and any ( expected_partial_batch_shape_static != tensorshape_util . as_list ( partial_batch_dist . batch_shape ) ) ) : \n            raise NotImplementedError ( \"Broadcasting is not supported; \" \"unexpected batch shape \" \"(expected {}, saw {}).\" . format ( expected_partial_batch_shape_static , partial_batch_dist . batch_shape ) ) \n    runtime_assertions = [ ] \n    if validate_args : \n        runtime_assertions . append ( assert_util . assert_greater_equal ( tf . convert_to_tensor ( value = num_broadcast_dims , dtype = tf . int32 ) , tf . zeros ( ( ) , dtype = tf . int32 ) , message = ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions.\" . format ( partial_batch_dist ) ) ) ) \n        runtime_assertions . append ( assert_util . assert_equal ( expected_partial_batch_shape , partial_batch_dist . batch_shape_tensor ( ) , message = ( \"Broadcasting is not supported; \" \"unexpected batch shape.\" ) , name = \"assert_batch_shape_same\" ) ) \n    with tf . control_dependencies ( runtime_assertions ) : \n        return full_sample_and_batch_shape [ : num_broadcast_dims ] "}
{"956": "\ndef _log_normalization ( self ) : \n    event_dim = tf . compat . dimension_value ( self . event_shape [ 0 ] ) \n    if event_dim is None : \n        raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) \n    safe_conc = tf . where ( 0 < self . concentration , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_lognorm = ( ( event_dim / 2 - 1 ) * tf . math . log ( safe_conc ) - ( event_dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2 - 1 , safe_conc ) ) - tf . abs ( safe_conc ) ) \n    log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2 , self . dtype ) ) ) \n    return tf . where ( 0 < self . concentration , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) ) "}
{"959": "\ndef _sample_3d ( self , n , seed = None ) : \n    seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) \n    u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) \n    z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) \n    safe_conc = tf . where ( 0 < self . concentration , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_z = tf . where ( 0 < z , z , tf . ones_like ( z ) ) \n    safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = 0 ) / safe_conc \n    u = tf . where ( tf . zeros_like ( safe_u ) < self . concentration , safe_u , 2 * z - 1 ) \n    u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) \n    if not self . _allow_nan_stats : \n        u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) \n    return u [ ... , tf . newaxis ] "}
{"970": "\ndef smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : \n        loop_num_iter_ = tf . get_static_value ( loop_num_iter ) \n        if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : \n            loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) \n            return tf . while_loop ( cond = lambda i , * args : loop_num_iter > i , body = lambda i , * args : [ i + 1 ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( 0 ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ 1 : ] \n        result = initial_loop_vars \n        for _ in range ( loop_num_iter_ ) : \n            result = body_fn ( * result ) \n        return result "}
{"971": "\ndef trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : \n        if vs . caching_device is None and not tf . executing_eagerly ( ) : \n            vs . set_caching_device ( lambda op : op . device ) \n        initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) \n        elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) \n        static_length = elems . shape [ 0 ] \n        if tf . compat . dimension_value ( static_length ) is None : \n            length = tf . shape ( input = elems ) [ 0 ] \n        else : \n            length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) \n        elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ 1 : ] ) \n        elems_array = elems_array . unstack ( elems ) \n        trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) \n        def _body ( i , state , trace_arrays ) : \n            state = loop_fn ( state , elems_array . read ( i ) ) \n            trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) \n            return i + 1 , state , trace_arrays \n        _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : length > i , body = _body , loop_vars = ( 0 , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) \n        stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) \n        def _merge_static_length ( x ) : \n            x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ 1 : ] ) ) \n            return x \n        stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) \n        return final_state , stacked_trace "}
{"975": "\ndef _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : \n    output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) \n    validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) \n    if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : \n        with tf . control_dependencies ( validation_dependencies ) : \n            output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) \n        return output_shape , output_tensorshape \n    with tf . control_dependencies ( validation_dependencies ) : \n        event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) \n        input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) \n    additional_assertions = [ ] \n    if is_validated : \n        pass \n    elif validate_args : \n        mask = 0 <= event_shape_in \n        explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) \n        explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) \n        additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) \n    with tf . control_dependencies ( additional_assertions ) : \n        output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) \n    return output_shape , output_tensorshape "}
{"976": "\ndef _replace_event_shape_in_tensorshape ( input_tensorshape , event_shape_in , event_shape_out ) : \n    event_shape_in_ndims = tensorshape_util . num_elements ( event_shape_in . shape ) \n    if tensorshape_util . rank ( input_tensorshape ) is None or event_shape_in_ndims is None : \n        return tf . TensorShape ( None ) , False \n    input_non_event_ndims = tensorshape_util . rank ( input_tensorshape ) - event_shape_in_ndims \n    if 0 > input_non_event_ndims : \n        raise ValueError ( 'Input has fewer ndims ({}) than event shape ndims ({}).' . format ( tensorshape_util . rank ( input_tensorshape ) , event_shape_in_ndims ) ) \n    input_non_event_tensorshape = input_tensorshape [ : input_non_event_ndims ] \n    input_event_tensorshape = input_tensorshape [ input_non_event_ndims : ] \n    event_shape_in_ = tf . get_static_value ( event_shape_in ) \n    is_validated = ( tensorshape_util . is_fully_defined ( input_event_tensorshape ) and event_shape_in_ is not None ) \n    if is_validated : \n        input_event_shape_ = np . int32 ( input_event_tensorshape ) \n        mask = 0 <= event_shape_in_ \n        explicit_input_event_shape_ = input_event_shape_ [ mask ] \n        explicit_event_shape_in_ = event_shape_in_ [ mask ] \n        if not all ( explicit_input_event_shape_ == explicit_event_shape_in_ ) : \n            raise ValueError ( 'Input `event_shape` does not match `event_shape_in`. ' '({} vs {}).' . format ( input_event_shape_ , event_shape_in_ ) ) \n    event_tensorshape_out = tensorshape_util . constant_value_as_shape ( event_shape_out ) \n    if tensorshape_util . rank ( event_tensorshape_out ) is None : \n        output_tensorshape = tf . TensorShape ( None ) \n    else : \n        output_tensorshape = tensorshape_util . concatenate ( input_non_event_tensorshape , event_tensorshape_out ) \n    return output_tensorshape , is_validated "}
{"977": "\ndef _maybe_check_valid_shape ( shape , validate_args ) : \n    if not dtype_util . is_integer ( shape . dtype ) : \n        raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) \n    assertions = [ ] \n    message = '`{}` rank should be <= 1.' \n    if tensorshape_util . rank ( shape . shape ) is not None : \n        if 1 < tensorshape_util . rank ( shape . shape ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) \n    shape_ = tf . get_static_value ( shape ) \n    message = '`{}` elements must have at most one `-1`.' \n    if shape_ is not None : \n        if 1 < sum ( shape_ == - 1 ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) \n    message = '`{}` elements must be either positive integers or `-1`.' \n    if shape_ is not None : \n        if np . any ( - 1 > shape_ ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) \n    return assertions "}
{"979": "\ndef get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : \n    if control_inputs : \n        with tf . control_dependencies ( control_inputs ) : \n            f0 , df0 = value_and_gradients_function ( initial_position ) \n    else : \n        f0 , df0 = value_and_gradients_function ( initial_position ) \n    converged = grad_tolerance > norm ( df0 , dims = 1 ) \n    return dict ( converged = converged , failed = tf . zeros_like ( converged ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , num_objective_evaluations = tf . convert_to_tensor ( value = 1 ) , position = initial_position , objective_value = f0 , objective_gradient = df0 ) "}
{"983": "\ndef _check_convergence ( current_position , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : \n    grad_converged = grad_tolerance >= norm ( next_gradient , dims = 1 ) \n    x_converged = x_tolerance >= norm ( next_position - current_position , dims = 1 ) \n    f_converged = ( f_relative_tolerance * current_objective >= norm ( next_objective - current_objective , dims = 0 ) ) \n    return grad_converged | x_converged | f_converged "}
{"986": "\ndef default_exchange_proposed_fn ( prob_exchange ) : \n    def default_exchange_proposed_fn_ ( num_replica , seed = None ) : \n        seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) \n        zero_start = 0.5 < tf . random . uniform ( [ ] , seed = seed_stream ( ) ) \n        if num_replica % 2 == 0 : \n            def _exchange ( ) : \n                flat_exchange = tf . range ( num_replica ) \n                if 2 < num_replica : \n                    start = tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                    end = num_replica - start \n                    flat_exchange = flat_exchange [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) \n        else : \n            def _exchange ( ) : \n                start = tf . cast ( zero_start , dtype = tf . int32 ) \n                end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                flat_exchange = tf . range ( num_replica ) [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) \n        def _null_exchange ( ) : \n            return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ 0 , 2 ] ) \n        return tf . cond ( pred = prob_exchange > tf . random . uniform ( [ ] , seed = seed_stream ( ) ) , true_fn = _exchange , false_fn = _null_exchange ) \n    return default_exchange_proposed_fn_ "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        for replica in range ( self . num_replica ) : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n        target_log_probs = tf . stack ( target_log_probs , axis = 0 ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ 0 ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return exchange_proposed_n > i \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = log_accept_ratio > log_uniforms [ i ] \n            for k in range ( num_state_parts ) : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n            return i + 1 , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ] "}
{"996": "\ndef _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : \n    update_result = update ( value_and_gradients_function , initial_args . left , initial_args . right , val_c , f_lim , active = initial_args . active ) \n    active = initial_args . active & ~ update_result . failed \n    failed = initial_args . failed | update_result . failed \n    val_left = val_where ( active , update_result . left , initial_args . left ) \n    val_right = val_where ( active , update_result . right , initial_args . right ) \n    updated_left = active & tf . equal ( val_left . x , val_c . x ) \n    updated_right = active & tf . equal ( val_right . x , val_c . x ) \n    is_new = updated_left | updated_right \n    next_c = tf . where ( updated_left , _secant ( initial_args . left , val_left ) , val_c . x ) \n    next_c = tf . where ( updated_right , _secant ( initial_args . right , val_right ) , next_c ) \n    in_range = ( next_c >= val_left . x ) & ( val_right . x >= next_c ) \n    needs_extra_eval = tf . reduce_any ( input_tensor = in_range & is_new ) \n    num_evals = initial_args . num_evals + update_result . num_evals \n    num_evals = num_evals + tf . cast ( needs_extra_eval , num_evals . dtype ) \n    next_args = _Secant2Result ( active = active & in_range , converged = initial_args . converged , failed = failed , num_evals = num_evals , left = val_left , right = val_right ) \n    def _apply_inner_update ( ) : \n        next_val_c = prefer_static . cond ( needs_extra_eval , ( lambda : value_and_gradients_function ( next_c ) ) , ( lambda : val_c ) ) \n        return _secant2_inner_update ( value_and_gradients_function , next_args , val_0 , next_val_c , f_lim , sufficient_decrease_param , curvature_param ) \n    return prefer_static . cond ( tf . reduce_any ( input_tensor = next_args . active ) , _apply_inner_update , lambda : next_args ) "}
{"998": "\ndef update ( value_and_gradients_function , val_left , val_right , val_trial , f_lim , active = None ) : \n    within_range = ( val_trial . x > val_left . x ) & ( val_right . x > val_trial . x ) \n    if active is not None : \n        within_range = within_range & active \n    valid_left = ( 0 > val_trial . df ) & ( f_lim >= val_trial . f ) \n    needs_bisect = within_range & ( 0 > val_trial . df ) & ( f_lim < val_trial . f ) \n    left = val_where ( within_range & valid_left , val_trial , val_left ) \n    right = val_where ( within_range & ~ valid_left , val_trial , val_right ) \n    bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = ~ needs_bisect , failed = tf . zeros_like ( within_range ) , num_evals = tf . convert_to_tensor ( value = 0 ) , left = left , right = right ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"999": "\ndef bracket ( value_and_gradients_function , search_interval , f_lim , max_iterations , expansion_param = 5.0 ) : \n    already_stopped = search_interval . failed | search_interval . converged \n    bracketed = 0 <= search_interval . right . df \n    needs_bisect = ( 0 > search_interval . right . df ) & ( f_lim < search_interval . right . f ) \n    initial_args = _IntermediateResult ( iteration = search_interval . iterations , stopped = already_stopped | bracketed | needs_bisect , failed = search_interval . failed , num_evals = search_interval . func_evals , left = search_interval . left , right = search_interval . right ) \n    def _loop_cond ( curr ) : \n        return ( max_iterations > curr . iteration ) & ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        new_right = value_and_gradients_function ( expansion_param * curr . right . x ) \n        left = val_where ( curr . stopped , curr . left , curr . right ) \n        right = val_where ( curr . stopped , curr . right , new_right ) \n        failed = curr . failed | ~ is_finite ( right ) \n        bracketed = 0 <= right . df \n        needs_bisect = ( 0 > right . df ) & ( f_lim < right . f ) \n        return [ _IntermediateResult ( iteration = curr . iteration + 1 , stopped = curr . stopped | failed | bracketed | needs_bisect , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n    bracket_result = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] \n    needs_bisect = ( ( 0 > bracket_result . right . df ) & ( f_lim < bracket_result . right . f ) ) \n    stopped = already_stopped | bracket_result . failed | ~ needs_bisect \n    left = val_where ( stopped , bracket_result . left , search_interval . left ) \n    bisect_args = bracket_result . _replace ( stopped = stopped , left = left ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1000": "\ndef bisect ( value_and_gradients_function , initial_left , initial_right , f_lim ) : \n    failed = ~ is_finite ( initial_left , initial_right ) \n    needs_bisect = ( 0 > initial_right . df ) & ( f_lim < initial_right . f ) \n    bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = failed | ~ needs_bisect , failed = failed , num_evals = tf . convert_to_tensor ( value = 0 ) , left = initial_left , right = initial_right ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1001": "\ndef _bisect ( value_and_gradients_function , initial_args , f_lim ) : \n    def _loop_cond ( curr ) : \n        return ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) \n        failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) \n        to_update = ~ ( curr . stopped | failed ) \n        update_left = ( 0 > mid . df ) & ( f_lim >= mid . f ) \n        left = val_where ( to_update & update_left , mid , curr . left ) \n        right = val_where ( to_update & ~ update_left , mid , curr . right ) \n        stopped = curr . stopped | failed | ( 0 <= right . df ) \n        return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] "}
{"1003": "\ndef _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : \n    exact_wolfe_suff_dec = ( ( val_c . f - val_0 . f ) / val_c . x <= sufficient_decrease_param * val_0 . df ) \n    wolfe_curvature = curvature_param * val_0 . df <= val_c . df \n    exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature \n    approx_wolfe_applies = f_lim >= val_c . f \n    approx_wolfe_suff_dec = ( val_c . df <= ( 2 * sufficient_decrease_param - 1 ) * val_0 . df ) \n    approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature \n    is_satisfied = exact_wolfe | approx_wolfe \n    return is_satisfied "}
{"1005": "\ndef make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : \n    if step_counter is None and num_adaptation_steps is not None : \n        step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - 1 , dtype = np . int32 ) , dtype = tf . int32 , trainable = False , use_resource = True ) \n    def step_size_simple_update_fn ( step_size_var , kernel_results ) : \n        if kernel_results is None : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ tf . identity ( ss ) for ss in step_size_var ] \n            return tf . identity ( step_size_var ) \n        log_n = tf . math . log ( tf . cast ( tf . size ( input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) \n        log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n \n        adjustment = tf . where ( tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) > log_mean_accept_ratio , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) \n        def build_assign_op ( ) : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] \n            return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) \n        if num_adaptation_steps is None : \n            return build_assign_op ( ) \n        else : \n            with tf . control_dependencies ( [ step_counter . assign_add ( 1 ) ] ) : \n                return tf . cond ( pred = num_adaptation_steps > step_counter , true_fn = build_assign_op , false_fn = lambda : step_size_var ) \n    return step_size_simple_update_fn "}
{"1019": "\ndef load_nips2011_papers ( path ) : \n    path = os . path . expanduser ( path ) \n    filename = \"NIPS_1987-2015.csv\" \n    filepath = os . path . join ( path , filename ) \n    if not os . path . exists ( filepath ) : \n        url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) \n        if not tf . io . gfile . exists ( path ) : \n            tf . io . gfile . makedirs ( path ) \n        print ( \"Downloading %s to %s\" % ( url , filepath ) ) \n        urllib . request . urlretrieve ( url , filepath ) \n    with open ( filepath ) as f : \n        iterator = csv . reader ( f ) \n        documents = next ( iterator ) [ 1 : ] \n        words = [ ] \n        x_train = [ ] \n        for row in iterator : \n            words . append ( row [ 0 ] ) \n            x_train . append ( row [ 1 : ] ) \n    x_train = np . array ( x_train , dtype = np . int ) \n    doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] \n    documents = [ documents [ doc ] for doc in doc_idx ] \n    x_train = x_train [ : , doc_idx ] \n    word_idx = np . logical_and ( 2 <= np . sum ( x_train != 0 , 1 ) , 10 <= np . sum ( x_train , 1 ) ) \n    words = [ word for word , idx in zip ( words , word_idx ) if idx ] \n    bag_of_words = x_train [ word_idx , : ] . T \n    return bag_of_words , words "}
{"1021": "\ndef _registered_kl ( type_a , type_b ) : \n    hierarchy_a = tf_inspect . getmro ( type_a ) \n    hierarchy_b = tf_inspect . getmro ( type_b ) \n    dist_to_children = None \n    kl_fn = None \n    for mro_to_a , parent_a in enumerate ( hierarchy_a ) : \n        for mro_to_b , parent_b in enumerate ( hierarchy_b ) : \n            candidate_dist = mro_to_a + mro_to_b \n            candidate_kl_fn = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) \n            if not kl_fn or ( candidate_kl_fn and dist_to_children > candidate_dist ) : \n                dist_to_children = candidate_dist \n                kl_fn = candidate_kl_fn \n    return kl_fn "}
{"1024": "\ndef create_character ( skin , hair , top , pants ) : \n    dtype = skin . dtype \n    hair_mask = tf . cast ( 0 >= hair [ ... , - 1 : ] , dtype ) \n    top_mask = tf . cast ( 0 >= top [ ... , - 1 : ] , dtype ) \n    pants_mask = tf . cast ( 0 >= pants [ ... , - 1 : ] , dtype ) \n    char = ( skin * hair_mask ) + hair \n    char = ( char * top_mask ) + top \n    char = ( char * pants_mask ) + pants \n    return char "}
{"1033": "\ndef find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : \n        in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) \n        if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and 2 > tf . compat . dimension_value ( edges . shape [ 0 ] ) ) : \n            raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) \n        flattening_x = edges . shape . ndims == 1 and 1 < x . shape . ndims \n        if flattening_x : \n            x_orig_shape = tf . shape ( input = x ) \n            x = tf . reshape ( x , [ - 1 ] ) \n        if dtype is None : \n            dtype = in_type \n        dtype = tf . as_dtype ( dtype ) \n        x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) \n        edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) \n        searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None \n        almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) \n        almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) \n        bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( input = edges ) [ 0 ] - 2 , dtype ) ) \n        if not extend_lower_interval : \n            low_fill = np . nan if dtype . is_floating else - 1 \n            bins = tf . where ( tf . expand_dims ( edges [ 0 ] , 0 ) > x , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) \n        if not extend_upper_interval : \n            up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ 0 ] - 1 \n            bins = tf . where ( tf . expand_dims ( edges [ - 1 ] , 0 ) < x , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) \n        if flattening_x : \n            bins = tf . reshape ( bins , x_orig_shape ) \n        return bins "}
{"1036": "\ndef _get_static_ndims ( x , expect_static = False , expect_ndims = None , expect_ndims_no_more_than = None , expect_ndims_at_least = None ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        shape_const = tf . get_static_value ( tf . shape ( input = x ) ) \n        if shape_const is not None : \n            ndims = shape_const . ndim \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected argument `x` to have statically defined `ndims`.  Found: ' % x ) \n        return \n    if expect_ndims is not None : \n        ndims_message = ( 'Expected argument `x` to have ndims %s.  Found tensor %s' % ( expect_ndims , x ) ) \n        if ndims != expect_ndims : \n            raise ValueError ( ndims_message ) \n    if expect_ndims_at_least is not None : \n        ndims_at_least_message = ( 'Expected argument `x` to have ndims >= %d.  Found tensor %s' % ( expect_ndims_at_least , x ) ) \n        if expect_ndims_at_least > ndims : \n            raise ValueError ( ndims_at_least_message ) \n    if expect_ndims_no_more_than is not None : \n        ndims_no_more_than_message = ( 'Expected argument `x` to have ndims <= %d.  Found tensor %s' % ( expect_ndims_no_more_than , x ) ) \n        if expect_ndims_no_more_than < ndims : \n            raise ValueError ( ndims_no_more_than_message ) \n    return ndims "}
{"1048": "\ndef t_power ( logu , t , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"t_power\" , [ logu , t ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = \"t\" ) \n        fu = tf . math . expm1 ( t * logu ) \n        if self_normalized : \n            fu -= t * tf . math . expm1 ( logu ) \n        fu *= tf . where ( tf . logical_and ( t > 0. , 1. > t ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) \n        return fu "}
{"1055": "\ndef csiszar_vimco_helper ( logu , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"csiszar_vimco_helper\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( 1 ) [ 0 ] ) \n        if n is None : \n            n = tf . shape ( input = logu ) [ 0 ] \n            log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) \n            nm1 = tf . cast ( n - 1 , dtype = logu . dtype ) \n        else : \n            log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) \n            nm1 = np . asarray ( n - 1 , dtype = logu . dtype . as_numpy_dtype ) \n        log_max_u = tf . reduce_max ( input_tensor = logu , axis = 0 ) \n        log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = 0 ) \n        d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) \n        d_ok = tf . not_equal ( d , 0. ) \n        safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) \n        d_ok_result = logu + tfd . softplus_inverse ( safe_d ) \n        inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) \n        is_positive_and_largest = tf . logical_and ( 0. < logu , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) \n        log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( input = logu ) , - inf ) , logu ) , axis = 0 , keepdims = True ) \n        log_lomsum_u = tf . tile ( log_lomsum_u , multiples = 1 + tf . pad ( tensor = [ n - 1 ] , paddings = [ [ 0 , tf . rank ( logu ) - 1 ] ] ) ) \n        d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( input = d ) , - inf ) ) \n        log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) \n        looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = 0 ) - logu ) / nm1 \n        log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = 0 ) \n        log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n \n        log_sooavg_u = log_soosum_u - log_n \n        log_avg_u . set_shape ( logu . shape . with_rank_at_least ( 1 ) [ 1 : ] ) \n        log_sooavg_u . set_shape ( logu . shape ) \n        return log_avg_u , log_sooavg_u "}
{"1056": "\ndef _assert_ndims_statically ( x , expect_ndims = None , expect_ndims_at_least = None , expect_static = False ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) \n        return \n    if expect_ndims is not None and ndims != expect_ndims : \n        raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) \n    if expect_ndims_at_least is not None and expect_ndims_at_least > ndims : \n        raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) ) "}
{"1061": "\ndef minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        input_shape = distribution_util . prefer_static_shape ( initial_position ) \n        batch_shape , domain_size = input_shape [ : - 1 ] , input_shape [ - 1 ] \n        if stopping_condition is None : \n            stopping_condition = bfgs_utils . converged_all \n        control_inputs = None \n        if initial_inverse_hessian_estimate is None : \n            initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) \n        else : \n            initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) \n            control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) \n            hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , 0 ) \n            initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) \n        def _cond ( state ) : \n            return ( ( max_iterations > state . num_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( state ) : \n            search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) \n            derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) \n            needs_reset = ( ~ state . failed & ~ state . converged & ( 0 <= derivative_at_start_pt ) ) \n            search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) \n            actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) \n            actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) \n            current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            return [ _update_inv_hessian ( current_state , next_state ) ] \n        kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) \n        kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian \n        initial_state = BfgsOptimizerResults ( ** kwargs ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1068": "\ndef pad_shape_right_with_ones ( x , ndims ) : \n    if not ( isinstance ( ndims , int ) and 0 <= ndims ) : \n        raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) \n    if ndims == 0 : \n        return x \n    x = tf . convert_to_tensor ( value = x ) \n    original_shape = x . shape \n    new_shape = distribution_util . pad ( tf . shape ( input = x ) , axis = 0 , back = True , value = 1 , count = ndims ) \n    x = tf . reshape ( x , new_shape ) \n    x . set_shape ( original_shape . concatenate ( [ 1 ] * ndims ) ) \n    return x "}
{"1072": "\ndef minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10 , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    if initial_inverse_hessian_estimate is not None : \n        raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) \n    if stopping_condition is None : \n        stopping_condition = bfgs_utils . converged_all \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        def _cond ( state ) : \n            return ( ( max_iterations > state . num_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( current_state ) : \n            search_direction = _get_search_direction ( current_state ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            should_update = ~ ( next_state . converged | next_state . failed ) \n            state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) \n            return [ state_after_inv_hessian_update ] \n        initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1077": "\ndef _psd_mask ( x ) : \n    eigenvalues , _ = tf . linalg . eigh ( x ) \n    return tf . cast ( 0 <= tf . reduce_min ( input_tensor = eigenvalues , axis = - 1 ) , dtype = x . dtype ) "}
{"1078": "\ndef _det_large_enough_mask ( x , det_bounds ) : \n    return tf . cast ( det_bounds < tf . linalg . det ( x ) , dtype = x . dtype ) "}
{"1083": "\ndef _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : \n    num_terms = tf . cast ( num_terms , dtype = dtype ) \n    def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : \n        denominator = 2. * n / concentration + rn \n        ddenominator_dk = - 2. * n / concentration ** 2 + drn_dconcentration \n        rn = 1. / denominator \n        drn_dconcentration = - ddenominator_dk / denominator ** 2 \n        multiplier = tf . sin ( n * x ) / n + vn \n        vn = rn * multiplier \n        dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) \n        n -= 1. \n        return n , rn , drn_dconcentration , vn , dvn_dconcentration \n    ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : 0. < n , body = loop_body , loop_vars = ( num_terms , tf . zeros_like ( x , name = \"rn\" ) , tf . zeros_like ( x , name = \"drn_dconcentration\" ) , tf . zeros_like ( x , name = \"vn\" ) , tf . zeros_like ( x , name = \"dvn_dconcentration\" ) , ) , ) \n    cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi \n    dcdf_dconcentration = dvn_dconcentration / np . pi \n    cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) \n    dcdf_dconcentration *= tf . cast ( ( 0. <= cdf ) & ( 1. >= cdf ) , dtype ) \n    return cdf_clipped , dcdf_dconcentration "}
{"1085": "\ndef one_step ( objective_function , population , population_values = None , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'one_step' , [ population , population_values , differential_weight , crossover_prob ] ) : \n        population , _ = _ensure_list ( population ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        population_size = tf . shape ( input = population [ 0 ] ) [ 0 ] \n        seed_stream = distributions . SeedStream ( seed , salt = 'one_step' ) \n        mixing_indices = _get_mixing_indices ( population_size , seed = seed_stream ( ) ) \n        mutants = _get_mutants ( population , population_size , mixing_indices , differential_weight ) \n        candidates = _binary_crossover ( population , population_size , mutants , crossover_prob , seed = seed_stream ( ) ) \n        candidate_values = objective_function ( * candidates ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        infinity = tf . zeros_like ( population_values ) + np . inf \n        population_values = tf . where ( tf . math . is_nan ( population_values ) , x = infinity , y = population_values ) \n        to_replace = population_values > candidate_values \n        next_population = [ tf . where ( to_replace , x = candidates_part , y = population_part ) for candidates_part , population_part in zip ( candidates , population ) ] \n        next_values = tf . where ( to_replace , x = candidate_values , y = population_values ) \n    return next_population , next_values "}
{"1086": "\ndef minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    if initial_population is None and initial_position is None : \n        raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) \n    if initial_population is not None and initial_position is not None : \n        raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) \n    with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : \n        ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) \n        def evolve_body ( loop_vars ) : \n            next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) \n            converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) \n            failed = _check_failure ( next_population_values ) \n            return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] \n        def evolve_cond ( loop_vars ) : \n            should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and max_iterations <= loop_vars . num_iterations ) ) \n            return ~ should_stop \n        initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) \n        final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] \n        best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) \n        final_population = final_state . population \n        if not was_iterable : \n            final_population = final_population [ 0 ] \n            best_position = best_position [ 0 ] \n        return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations ) "}
{"1089": "\ndef _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : \n    value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) \n    value_converged = func_tolerance >= value_range \n    half_tol = position_tolerance / 2 \n    def part_converged ( part ) : \n        return half_tol >= tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ 0 ] ) ) \n    x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) \n    return value_converged | x_converged "}
{"1091": "\ndef _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : \n    sizes = [ tf . cast ( tf . size ( input = x ) , dtype = tf . float64 ) for x in population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) \n    force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , 1 ] , seed = seed_stream ( ) ) \n    recombinants = [ ] \n    for i , population_part in enumerate ( population ) : \n        pop_part_flat = tf . reshape ( population_part , [ population_size , - 1 ] ) \n        mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - 1 ] ) \n        part_size = tf . size ( input = population_part ) // population_size \n        force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = 0 , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = True , off_value = False , dtype = tf . bool ) \n        group_mask = tf . math . equal ( force_crossover_group , i ) \n        force_crossovers &= group_mask \n        do_binary_crossover = crossover_prob > tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) \n        do_binary_crossover |= force_crossovers \n        recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) \n        recombinant = tf . reshape ( recombinant_flat , tf . shape ( input = population_part ) ) \n        recombinants . append ( recombinant ) \n    return recombinants "}
{"1093": "\ndef _get_mixing_indices ( size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'get_mixing_indices' , values = [ size ] ) : \n        size = tf . convert_to_tensor ( value = size ) \n        dtype = size . dtype \n        seed_stream = distributions . SeedStream ( seed , salt = 'get_mixing_indices' ) \n        first = tf . random . uniform ( [ size ] , maxval = size - 1 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . random . uniform ( [ size ] , maxval = size - 2 , dtype = dtype , seed = seed_stream ( ) ) \n        third = tf . random . uniform ( [ size ] , maxval = size - 3 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . where ( second > first , x = second , y = second + 1 ) \n        smaller = tf . math . minimum ( first , second ) \n        larger = tf . math . maximum ( first , second ) \n        third = tf . where ( smaller > third , x = third , y = third + 1 ) \n        third = tf . where ( larger > third , x = third , y = third + 1 ) \n        sample = tf . stack ( [ first , second , third ] , axis = 1 ) \n        to_avoid = tf . expand_dims ( tf . range ( size ) , axis = - 1 ) \n        sample = tf . where ( to_avoid > sample , x = sample , y = sample + 1 ) \n        return sample "}
{"1105": "\ndef matrix_rank ( a , tol = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : \n        a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        s = tf . linalg . svd ( a , compute_uv = False ) \n        if tol is None : \n            if a . shape [ - 2 : ] . is_fully_defined ( ) : \n                m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) \n            else : \n                m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2 : ] ) \n            eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps \n            tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = True ) ) \n        return tf . reduce_sum ( input_tensor = tf . cast ( tol < s , tf . int32 ) , axis = - 1 ) "}
{"1106": "\ndef pinv ( a , rcond = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : \n        a = tf . convert_to_tensor ( value = a , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        dtype = a . dtype . as_numpy_dtype \n        if rcond is None : \n            def get_dim_size ( dim ) : \n                if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : \n                    return tf . compat . dimension_value ( a . shape [ dim ] ) \n                return tf . shape ( input = a ) [ dim ] \n            num_rows = get_dim_size ( - 2 ) \n            num_cols = get_dim_size ( - 1 ) \n            if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : \n                max_rows_cols = float ( max ( num_rows , num_cols ) ) \n            else : \n                max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) \n            rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps \n        rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) \n        [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) \n        cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) \n        singular_values = tf . where ( cutoff [ ... , tf . newaxis ] < singular_values , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) \n        a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) \n        if a . shape . ndims is not None : \n            a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2 ] ] ) ) \n        return a_pinv "}
{"1109": "\ndef _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : \n    assertions = [ ] \n    message = 'Input `lower_upper` must have at least 2 dimensions.' \n    if lower_upper . shape . ndims is not None : \n        if 2 > lower_upper . shape . ndims : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) \n    message = '`rank(lower_upper)` must equal `rank(perm) + 1`' \n    if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : \n        if lower_upper . shape . ndims != perm . shape . ndims + 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) \n    message = '`lower_upper` must be square.' \n    if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : \n        if lower_upper . shape [ - 2 ] != lower_upper . shape [ - 1 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) \n        assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) \n    return assertions "}
{"1110": "\ndef _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : \n    assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n    message = 'Input `rhs` must have at least 2 dimensions.' \n    if rhs . shape . ndims is not None : \n        if 2 > rhs . shape . ndims : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) \n    message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' \n    if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : \n        if lower_upper . shape [ - 1 ] != rhs . shape [ - 2 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) \n    return assertions "}
{"1112": "\ndef _maybe_validate_matrix ( a , validate_args ) : \n    assertions = [ ] \n    if not a . dtype . is_floating : \n        raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) \n    if a . shape . ndims is not None : \n        if 2 > a . shape . ndims : \n            raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) \n    return assertions "}
{"1113": "\ndef _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : \n    mean , variance , grad_mean = model ( linear_response ) \n    is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( 0. < variance ) ) \n    def _mask_if_invalid ( x , mask ) : \n        mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) \n        return tf . where ( is_valid , x , mask ) \n    v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) \n    grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) \n    fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) \n    return - grad_log_likelihood , fim_middle "}
{"1120": "\ndef _create_masks ( degrees ) : \n    return [ out >= inp [ : , np . newaxis ] for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ degrees [ 0 ] > degrees [ - 1 ] [ : , np . newaxis ] ] "}
{"1122": "\ndef build ( self , input_shape ) : \n    if self . _event_shape is None : \n        self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - 1 ] ) ] \n        self . _event_size = self . _event_shape [ - 1 ] \n        self . _event_ndims = len ( self . _event_shape ) \n    if input_shape [ - 1 ] != self . _event_shape [ - 1 ] : \n        raise ValueError ( \"Invalid final dimension of `input_shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . _event_shape [ - 1 ] , input_shape [ - 1 ] ) ) \n    self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) \n    self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) \n    self . _masks [ - 1 ] = np . reshape ( np . tile ( self . _masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . _params ] ) , [ self . _masks [ - 1 ] . shape [ 0 ] , self . _event_size * self . _params ] ) \n    self . _network = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) \n    layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] \n    for k in range ( len ( self . _masks ) ) : \n        self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if len ( self . _masks ) > k + 1 else None , use_bias = self . _use_bias , ** self . _kwargs ) ) \n    super ( AutoregressiveLayer , self ) . build ( input_shape ) "}
{"1128": "\ndef _depth ( g ) : \n    def _explore ( v ) : \n        if 0 > v . depth : \n            v . depth = ( ( 1 + max ( [ - 1 ] + [ _explore ( annotated_graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) \n        return v . depth \n    annotated_graph = { k : _Node ( k , v ) for k , v in g . items ( ) } \n    for v in annotated_graph . values ( ) : \n        _explore ( v ) \n    return annotated_graph "}
{"1129": "\ndef _best_order ( g ) : \n    def _explore ( u ) : \n        if 0 > u . depth : \n            return \n        if not u . parents : \n            result . append ( ( u . name , u . parents ) ) \n            u . depth = - 1 \n            return \n        b = ( u . name , [ ] ) \n        result . append ( b ) \n        u . depth = - 1 \n        d = 0 \n        for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : \n            n0 = len ( result ) \n            _explore ( v ) \n            n1 = len ( result ) \n            b [ 1 ] . extend ( [ '_' ] * d + [ v . name ] ) \n            d = n1 - n0 - 1 \n    g = _depth ( g ) \n    result = [ ] \n    for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : \n        _explore ( u ) \n    return tuple ( reversed ( result ) ) "}
{"1151": "\ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = 0 \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id += 1 \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if ( 3 , ) > sys . version_info : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    print ( 'Found %s texts.' % len ( texts ) ) \n    return texts "}
{"1152": "\ndef get_glove_w2v ( source_dir = \"./data/news20/\" , dim = 100 ) : \n    w2v_dir = download_glove_w2v ( source_dir ) \n    w2v_path = os . path . join ( w2v_dir , \"glove.6B.%sd.txt\" % dim ) \n    if ( 3 , ) > sys . version_info : \n        w2v_f = open ( w2v_path ) \n    else : \n        w2v_f = open ( w2v_path , encoding = 'latin-1' ) \n    pre_w2v = { } \n    for line in w2v_f . readlines ( ) : \n        items = line . split ( \" \" ) \n        pre_w2v [ items [ 0 ] ] = [ float ( i ) for i in items [ 1 : ] ] \n    w2v_f . close ( ) \n    return pre_w2v "}
{"1196": "\ndef is_spark_below_2_2 ( ) : \n    import pyspark \n    if ( hasattr ( pyspark , \"version\" ) ) : \n        full_version = pyspark . version . __version__ \n        parts = full_version . split ( \".\" ) \n        spark_version = parts [ 0 ] + \".\" + parts [ 1 ] \n        if ( 0 <= compare_version ( spark_version , \"2.2\" ) ) : \n            return False \n    return True "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if 0 < possible_matches : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if 0 < possible_matches_by_order [ i ] : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if 0 < min ( precisions ) : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if 1.0 < ratio : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1205": "\ndef _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : \n    if isinstance ( utterance , str ) : \n        pass \n    elif isinstance ( utterance , RichMessage ) : \n        utterance = utterance . json ( ) \n    elif isinstance ( utterance , ( list , dict ) ) : \n        utterance = jsonify_data ( utterance ) \n    else : \n        utterance = str ( utterance ) \n    dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id \n    if self . log_max_size * 1024 <= self . log_file . tell ( ) : \n        self . log_file . close ( ) \n        self . log_file = self . _get_log_file ( ) \n    else : \n        try : \n            log_msg = { } \n            log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) \n            log_msg [ 'dialog_id' ] = dialog_id \n            log_msg [ 'direction' ] = direction \n            log_msg [ 'message' ] = utterance \n            log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) \n            self . log_file . write ( f'{log_str}\\n' ) \n        except IOError : \n            log . error ( 'Failed to write dialog log.' ) "}
{"1207": "\ndef dump_weights ( tf_save_dir , outfile , options ) : \n    def _get_outname ( tf_name ) : \n        outname = re . sub ( ':0$' , '' , tf_name ) \n        outname = outname . lstrip ( 'lm/' ) \n        outname = re . sub ( '/rnn/' , '/RNN/' , outname ) \n        outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) \n        outname = re . sub ( '/cell_' , '/Cell' , outname ) \n        outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) \n        if '/RNN/' in outname : \n            if 'projection' in outname : \n                outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) \n            else : \n                outname = re . sub ( '/kernel' , '/W_0' , outname ) \n                outname = re . sub ( '/bias' , '/B' , outname ) \n        return outname \n    ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) \n    config = tf . ConfigProto ( allow_soft_placement = True ) \n    with tf . Graph ( ) . as_default ( ) : \n        with tf . Session ( config = config ) as sess : \n            with tf . variable_scope ( 'lm' ) : \n                LanguageModel ( options , False ) \n                loader = tf . train . Saver ( ) \n                loader . restore ( sess , ckpt_file ) \n            with h5py . File ( outfile , 'w' ) as fout : \n                for v in tf . trainable_variables ( ) : \n                    if 0 <= v . name . find ( 'softmax' ) : \n                        continue \n                    outname = _get_outname ( v . name ) \n                    shape = v . get_shape ( ) . as_list ( ) \n                    dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) \n                    values = sess . run ( [ v ] ) [ 0 ] \n                    dset [ ... ] = values "}
{"1218": "\ndef process_word ( word : str , to_lower : bool = False , append_case : Optional [ str ] = None ) -> Tuple [ str ] : \n    if all ( x . isupper ( ) for x in word ) and 1 < len ( word ) : \n        uppercase = \"<ALL_UPPER>\" \n    elif word [ 0 ] . isupper ( ) : \n        uppercase = \"<FIRST_UPPER>\" \n    else : \n        uppercase = None \n    if to_lower : \n        word = word . lower ( ) \n    if word . isdigit ( ) : \n        answer = [ \"<DIGIT>\" ] \n    elif word . startswith ( \"http://\" ) or word . startswith ( \"www.\" ) : \n        answer = [ \"<HTTP>\" ] \n    else : \n        answer = list ( word ) \n    if to_lower and uppercase is not None : \n        if append_case == \"first\" : \n            answer = [ uppercase ] + answer \n        elif append_case == \"last\" : \n            answer = answer + [ uppercase ] \n    return tuple ( answer ) "}
{"1232": "\ndef build ( self ) : \n    word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = \"int32\" ) \n    inputs = [ word_inputs ] \n    word_outputs = self . _build_word_cnn ( word_inputs ) \n    if 0 < len ( self . word_vectorizers ) : \n        additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = \"float32\" ) for input_dim , dense_dim in self . word_vectorizers ] \n        inputs . extend ( additional_word_inputs ) \n        additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] \n        word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) \n    outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) \n    compile_args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical_crossentropy\" , \"metrics\" : [ \"accuracy\" ] } \n    self . model_ = Model ( inputs , outputs ) \n    self . model_ . compile ( ** compile_args ) \n    if 0 < self . verbose : \n        self . model_ . summary ( print_fn = log . info ) \n    return self "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = 0 \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) \n        for _ in range ( self . char_conv_layers - 1 ) : \n            curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if 0.0 < self . conv_dropout : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n        curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ += curr_filters_number \n    if 1 < len ( conv_outputs ) : \n        conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ 0 ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) \n    if 0.0 < self . intermediate_dropout : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    for i in range ( self . char_highway_layers - 1 ) : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if 0.0 < self . highway_dropout : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1234": "\ndef _build_basic_network ( self , word_outputs ) : \n    if 0.0 < self . word_dropout : \n        lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) \n    else : \n        lstm_outputs = word_outputs \n    for j in range ( self . word_lstm_layers - 1 ) : \n        lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = \"softmax\" , activity_regularizer = self . regularizer ) , name = \"p\" ) ( lstm_outputs ) \n    return pre_outputs , lstm_outputs "}
{"1275": "\ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while True : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if 1 < len ( model . out_params ) : \n            pred = zip ( * pred ) \n        print ( '>>' , * pred ) "}
{"1277": "\ndef read_infile ( infile : Union [ Path , str ] , from_words = False , word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , tag_column : int = TAG_COLUMN , max_sents : int = - 1 , read_only_words : bool = False ) -> List [ Tuple [ List , Union [ List , None ] ] ] : \n    answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] \n    if from_words : \n        word_column , read_only_words = 0 , True \n    with open ( infile , \"r\" , encoding = \"utf8\" ) as fin : \n        for line in fin : \n            line = line . strip ( ) \n            if line . startswith ( \"#\" ) : \n                continue \n            if line == \"\" : \n                if 0 < len ( curr_word_sent ) : \n                    if read_only_words : \n                        curr_tag_sent = None \n                    answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n                curr_tag_sent , curr_word_sent = [ ] , [ ] \n                if len ( answer ) == max_sents : \n                    break \n                continue \n            splitted = line . split ( \"\\t\" ) \n            index = splitted [ 0 ] \n            if not from_words and not index . isdigit ( ) : \n                continue \n            curr_word_sent . append ( splitted [ word_column ] ) \n            if not read_only_words : \n                pos , tag = splitted [ pos_column ] , splitted [ tag_column ] \n                tag = pos if tag == \"_\" else \"{},{}\" . format ( pos , tag ) \n                curr_tag_sent . append ( tag ) \n        if 0 < len ( curr_word_sent ) : \n            if read_only_words : \n                curr_tag_sent = None \n            answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n    return answer "}
{"1282": "\ndef _get_best ( values : List [ float ] , losses : List [ float ] , max_loss_div : float = 0.9 , min_val_div : float = 10.0 ) -> float : \n    assert len ( values ) == len ( losses ) , \"lengths of values and losses should be equal\" \n    min_ind = np . argmin ( losses ) \n    for i in range ( min_ind - 1 , 0 , - 1 ) : \n        if ( losses [ min_ind ] < losses [ i ] * max_loss_div ) or ( values [ min_ind ] > values [ i ] * min_val_div ) : \n            return values [ i + 1 ] \n    return values [ min_ind ] / min_val_div "}
{"1301": "\ndef squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : \n    f1_total = 0.0 \n    for ground_truth , prediction in zip ( y_true , y_predicted ) : \n        prediction_tokens = normalize_answer ( prediction ) . split ( ) \n        f1s = [ ] \n        for gt in ground_truth : \n            gt_tokens = normalize_answer ( gt ) . split ( ) \n            if len ( gt_tokens ) == 0 or len ( prediction_tokens ) == 0 : \n                f1s . append ( float ( gt_tokens == prediction_tokens ) ) \n                continue \n            common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) \n            num_same = sum ( common . values ( ) ) \n            if num_same == 0 : \n                f1s . append ( 0.0 ) \n                continue \n            precision = 1.0 * num_same / len ( prediction_tokens ) \n            recall = 1.0 * num_same / len ( gt_tokens ) \n            f1 = ( 2 * precision * recall ) / ( precision + recall ) \n            f1s . append ( f1 ) \n        f1_total += max ( f1s ) \n    return 100 * f1_total / len ( y_true ) if 0 < len ( y_true ) else 0 "}
{"1310": "\ndef _refresh_valid_certs ( self ) -> None : \n    self . timer = Timer ( REFRESH_VALID_CERTS_PERIOD_SECS , self . _refresh_valid_certs ) \n    self . timer . start ( ) \n    expired_certificates = [ ] \n    for valid_cert_url , valid_cert in self . valid_certificates . items ( ) : \n        valid_cert : ValidatedCert = valid_cert \n        cert_expiration_time : datetime = valid_cert . expiration_timestamp \n        if cert_expiration_time < datetime . utcnow ( ) : \n            expired_certificates . append ( valid_cert_url ) \n    for expired_cert_url in expired_certificates : \n        del self . valid_certificates [ expired_cert_url ] \n        log . info ( f'Validation period of {expired_cert_url} certificate expired' ) "}
{"1312": "\ndef _handle_request ( self , request : dict ) -> dict : \n    request_body : bytes = request [ 'request_body' ] \n    signature_chain_url : str = request [ 'signature_chain_url' ] \n    signature : str = request [ 'signature' ] \n    alexa_request : dict = request [ 'alexa_request' ] \n    if not self . _verify_request ( signature_chain_url , signature , request_body ) : \n        return { 'error' : 'failed certificate/signature check' } \n    timestamp_str = alexa_request [ 'request' ] [ 'timestamp' ] \n    timestamp_datetime = datetime . strptime ( timestamp_str , '%Y-%m-%dT%H:%M:%SZ' ) \n    now = datetime . utcnow ( ) \n    delta = now - timestamp_datetime if timestamp_datetime <= now else timestamp_datetime - now \n    if REQUEST_TIMESTAMP_TOLERANCE_SECS < abs ( delta . seconds ) : \n        log . error ( f'Failed timestamp check for request: {request_body.decode(\"utf-8\", \"replace\")}' ) \n        return { 'error' : 'failed request timestamp check' } \n    conversation_key = alexa_request [ 'session' ] [ 'user' ] [ 'userId' ] \n    if conversation_key not in self . conversations . keys ( ) : \n        if self . config [ 'multi_instance' ] : \n            conv_agent = self . _init_agent ( ) \n            log . info ( 'New conversation instance level agent initiated' ) \n        else : \n            conv_agent = self . agent \n        self . conversations [ conversation_key ] = Conversation ( config = self . config , agent = conv_agent , conversation_key = conversation_key , self_destruct_callback = lambda : self . _del_conversation ( conversation_key ) ) \n        log . info ( f'Created new conversation, key: {conversation_key}' ) \n    conversation = self . conversations [ conversation_key ] \n    response = conversation . handle_request ( alexa_request ) \n    return response "}
{"1321": "\ndef show_status ( self , detailed = False ) : \n    if time . time ( ) > self . _retrieved_at + self . REFRESH_INTERVAL : \n        new_info = h2o . api ( \"GET /3/Cloud\" ) \n        self . _fill_from_h2ocluster ( new_info ) \n    ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) \n    allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) \n    free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) \n    unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) \n    status = \"locked\" if self . locked else \"accepting new members\" \n    if unhealthy_nodes == 0 : \n        status += \", healthy\" \n    else : \n        status += \", %d nodes are not healthy\" % unhealthy_nodes \n    api_extensions = self . list_api_extensions ( ) \n    H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) \n    if detailed : \n        keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] \n        header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] \n        table = [ [ k ] for k in keys ] \n        for node in self . nodes : \n            for i , k in enumerate ( keys ) : \n                table [ i ] . append ( node [ k ] ) \n        H2ODisplay ( table = table , header = header ) "}
{"1326": "\ndef stabilize ( self , test_func , error , timeoutSecs = 10 , retryDelaySecs = 0.5 ) : \n    start = time . time ( ) \n    numberOfRetries = 0 \n    while h2o_args . no_timeout or ( timeoutSecs > time . time ( ) - start ) : \n        if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : \n            break \n        time . sleep ( retryDelaySecs ) \n        numberOfRetries += 1 \n        if ( ( numberOfRetries % 50 ) == 0 ) : \n            check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) \n    else : \n        timeTakenSecs = time . time ( ) - start \n        if isinstance ( error , type ( '' ) ) : \n            raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) \n        else : \n            msg = error ( self , timeTakenSecs , numberOfRetries ) \n            raise Exception ( msg ) "}
{"1340": "\ndef wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : \n    log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) \n    for ip in ips : \n        if not skipAlive or not ssh_live ( ip , port ) : \n            log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) \n            count = 0 \n            while requiredsuccess > count : \n                if ssh_live ( ip , port ) : \n                    count += 1 \n                else : \n                    count = 0 \n                time . sleep ( 1 ) \n                h2o_cmd . dot ( ) "}
{"1342": "\ndef _find_function_from_code ( frame , code ) : \n    def find_code ( iterable , depth = 0 ) : \n        if 3 < depth : \n            return \n        for item in iterable : \n            if item is None : \n                continue \n            found = None \n            if hasattr ( item , \"__code__\" ) and item . __code__ == code : \n                found = item \n            elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : \n                try : \n                    found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) \n                except Exception : \n                    continue \n            elif isinstance ( item , ( list , tuple , set ) ) : \n                found = find_code ( item , depth + 1 ) \n            elif isinstance ( item , dict ) : \n                found = find_code ( item . values ( ) , depth + 1 ) \n            if found : \n                return found \n    return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) ) "}
{"1344": "\ndef _wrap ( text , wrap_at = 120 , indent = 4 ) : \n    out = \"\" \n    curr_line_length = indent \n    space_needed = False \n    for word in text . split ( ) : \n        if wrap_at < curr_line_length + len ( word ) : \n            out += \"\\n\" + \" \" * indent \n            curr_line_length = indent \n            space_needed = False \n        if space_needed : \n            out += \" \" \n            curr_line_length += 1 \n        out += word \n        curr_line_length += len ( word ) \n        space_needed = True \n    return out "}
{"1353": "\ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60 \n    while 0 < retries : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while 0 < len ( s ) : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( 1 ) \n                if size is not None : \n                    size = int ( size ) \n                    if size == nodes_per_cloud : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries -= 1 \n        if self . terminated : \n            return \n        time . sleep ( 1 ) \n    print ( \"\" ) \n    print ( \"ERROR: Too many retries starting cloud.\" ) \n    print ( \"\" ) \n    sys . exit ( 1 ) "}
{"1354": "\ndef stop ( self ) : \n    if 0 < self . pid : \n        print ( \"Killing JVM with PID {}\" . format ( self . pid ) ) \n        try : \n            self . child . terminate ( ) \n            self . child . wait ( ) \n        except OSError : \n            pass \n        self . pid = - 1 "}
{"1356": "\ndef get_ip ( self ) : \n    if 0 < len ( self . client_nodes ) : \n        node = self . client_nodes [ 0 ] \n    else : \n        node = self . nodes [ 0 ] \n    return node . get_ip ( ) "}
{"1357": "\ndef get_port ( self ) : \n    if 0 < len ( self . client_nodes ) : \n        node = self . client_nodes [ 0 ] \n    else : \n        node = self . nodes [ 0 ] \n    return node . get_port ( ) "}
{"1359": "\ndef _determine_vec_size ( self ) : \n    first_column = self . pre_trained . types [ self . pre_trained . columns [ 0 ] ] \n    if first_column != 'string' : \n        raise H2OValueError ( \"First column of given pre_trained model %s is required to be a String\" , self . pre_trained . frame_id ) \n    if 1 < list ( self . pre_trained . types . values ( ) ) . count ( 'string' ) : \n        raise H2OValueError ( \"There are multiple columns in given pre_trained model %s with a String type.\" , self . pre_trained . frame_id ) \n    self . vec_size = self . pre_trained . dim [ 1 ] - 1 ; "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2 ] \n        assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and fr . f_code . co_filename == this_filename : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                for i in range ( fr . f_lineno - 1 ) : \n                    next ( f ) \n                g = tokenize . generate_tokens ( f . readline ) \n                step = 0 \n                args_tokens = [ ] \n                level = 0 \n                for ttt in g : \n                    if step == 0 : \n                        if ttt [ 0 ] != tokenize . NAME : \n                            continue \n                        if not ttt [ 1 ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = 1 \n                    elif step == 1 : \n                        assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \"(\" \n                        args_tokens . append ( [ ] ) \n                        step = 2 \n                    elif step == 2 : \n                        if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \",\" : \n                            args_tokens . append ( [ ] ) \n                        elif level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \")\" : \n                            break \n                        else : \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \"([{\" : \n                                level += 1 \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \")]}\" : \n                                level -= 1 \n                            assert 0 <= level , \"Parse error: parentheses level became negative\" \n                            args_tokens [ - 1 ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1376": "\ndef execute ( self , progress_fn , print_verbose_info = None ) : \n    assert_is_type ( progress_fn , FunctionType , GeneratorType , MethodType ) \n    if isinstance ( progress_fn , GeneratorType ) : \n        progress_fn = ( lambda g : lambda : next ( g ) ) ( progress_fn ) \n    self . _next_poll_time = 0 \n    self . _t0 = time . time ( ) \n    self . _x0 = 0 \n    self . _v0 = 0.01 \n    self . _ve = 0.01 \n    progress = 0 \n    status = None \n    try : \n        while True : \n            now = time . time ( ) \n            if now >= self . _next_poll_time : \n                res = progress_fn ( ) \n                assert_is_type ( res , ( numeric , numeric ) , numeric ) \n                if not isinstance ( res , tuple ) : \n                    res = ( res , - 1 ) \n                now = time . time ( ) \n                self . _store_model_progress ( res , now ) \n                self . _recalculate_model_parameters ( now ) \n            progress = min ( self . _compute_progress_at_time ( now ) [ 0 ] , 1 ) \n            if progress == 1 and 1 <= self . _get_real_progress ( ) : \n                break \n            result = self . _widget . render ( progress ) \n            assert_is_type ( result , RenderResult ) \n            time0 = result . next_time \n            time1 = self . _get_time_at_progress ( result . next_progress ) \n            next_render_time = min ( time0 , time1 ) \n            self . _draw ( result . rendered ) \n            wait_time = min ( next_render_time , self . _next_poll_time ) - now \n            if 0 < wait_time : \n                time . sleep ( wait_time ) \n                if print_verbose_info is not None : \n                    print_verbose_info ( progress ) \n    except KeyboardInterrupt : \n        status = \"cancelled\" \n    except StopIteration as e : \n        status = str ( e ) \n    result = self . _widget . render ( progress = progress , status = status ) \n    self . _draw ( result . rendered , final = True ) \n    if status == \"cancelled\" : \n        raise StopIteration ( status ) "}
{"1377": "\ndef _store_model_progress ( self , res , now ) : \n    raw_progress , delay = res \n    raw_progress = clamp ( raw_progress , 0 , self . _maxval ) \n    self . _progress_data . append ( ( now , raw_progress ) ) \n    if 0 > delay : \n        delay = self . _guess_next_poll_interval ( ) \n    self . _next_poll_time = now + clamp ( delay , self . MIN_PROGRESS_CHECK_INTERVAL , self . MAX_PROGRESS_CHECK_INTERVAL ) "}
{"1378": "\ndef _recalculate_model_parameters ( self , now ) : \n    time_until_end = self . _estimate_progress_completion_time ( now ) - now \n    assert 0 <= time_until_end , \"Estimated progress completion cannot be in the past.\" \n    x_real = self . _get_real_progress ( ) \n    if x_real == 1 : \n        t0 , x0 , v0 , ve = now , 1 , 0 , 0 \n    else : \n        x0 , v0 = self . _compute_progress_at_time ( now ) \n        t0 = now \n        if 1 <= x0 : \n            t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 \n            time_until_end += now - t0 \n        z = self . BETA * time_until_end \n        max_speed = ( 1 - x_real ** 2 ) / self . FINISH_DELAY \n        ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) \n        if 0 > ve : \n            v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) \n            ve = 0 \n        if max_speed < ve : \n            ve = max_speed \n    self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve "}
{"1379": "\ndef _estimate_progress_completion_time ( self , now ) : \n    assert now <= self . _next_poll_time \n    tlast , wlast = self . _progress_data [ - 1 ] \n    if wlast == self . _maxval : \n        current_completion_time = ( 1 - self . _x0 ) / self . _v0 + self . _t0 \n        return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) \n    tacc , wacc = 0 , 0 \n    factor = self . GAMMA \n    for t , x in self . _progress_data [ - 2 : : - 1 ] : \n        tacc += factor * ( tlast - t ) \n        wacc += factor * ( wlast - x ) \n        factor *= self . GAMMA \n        if 1e-2 > factor : \n            break \n    if wacc == 0 : \n        return now + 300 \n    t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc \n    if self . _next_poll_time >= t_estimate : \n        t_estimate = self . _next_poll_time + self . FINISH_DELAY \n    return t_estimate "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    for _ in range ( 20 ) : \n        if v == 0 : \n            return 1e20 \n        t += ( x_target - x ) / v \n        x , v = self . _compute_progress_at_time ( t ) \n        if 1e-3 > abs ( x - x_target ) : \n            return t \n    return time . time ( ) + 100 "}
{"1384": "\ndef _compute_widget_sizes ( self ) : \n    wl = [ 0 ] * len ( self . _widgets ) \n    flex_count = 0 \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            flex_count += 1 \n        else : \n            wl [ i ] = widget . render ( 1 ) . length \n    remaining_width = self . _width - sum ( wl ) \n    remaining_width -= len ( self . _widgets ) - 1 \n    if 10 * flex_count > remaining_width : \n        if self . _file_mode : \n            remaining_width = 10 * flex_count \n        else : \n            widget0 = self . _widgets [ 0 ] \n            if isinstance ( widget0 , PBWString ) and 10 * flex_count <= remaining_width + widget0 . render ( 0 ) . length : \n                remaining_width += widget0 . render ( 0 ) . length + 1 \n                self . _to_render = widget0 . render ( 0 ) . rendered + \"\\n\" \n                self . _widgets = self . _widgets [ 1 : ] \n            if 10 * flex_count > remaining_width : \n                self . _file_mode = True \n                remaining_width = 10 * flex_count \n    remaining_width = max ( remaining_width , 10 * flex_count ) \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            target_length = int ( remaining_width / flex_count ) \n            result = widget . render ( 1 , target_length ) \n            wl [ i ] = result . length \n            remaining_width -= result . length \n            flex_count -= 1 \n    return wl "}
{"1413": "\ndef split_frame ( self , ratios = None , destination_frames = None , seed = None ) : \n    assert_is_type ( ratios , [ numeric ] , None ) \n    assert_is_type ( destination_frames , [ str ] , None ) \n    assert_is_type ( seed , int , None ) \n    if ratios is None : \n        ratios = [ 0.75 ] \n    if not ratios : \n        raise ValueError ( \"Ratios array may not be empty\" ) \n    if destination_frames is not None : \n        if len ( ratios ) + 1 != len ( destination_frames ) : \n            raise ValueError ( \"The number of provided destination_frames must be one more \" \"than the number of provided ratios\" ) \n    num_slices = len ( ratios ) + 1 \n    boundaries = [ ] \n    last_boundary = 0 \n    i = 0 \n    while num_slices - 1 > i : \n        ratio = ratios [ i ] \n        if 0 > ratio : \n            raise ValueError ( \"Ratio must be greater than 0\" ) \n        boundary = last_boundary + ratio \n        if 1.0 <= boundary : \n            raise ValueError ( \"Ratios must add up to less than 1.0\" ) \n        boundaries . append ( boundary ) \n        last_boundary = boundary \n        i += 1 \n    splits = [ ] \n    tmp_runif = self . runif ( seed ) \n    tmp_runif . frame_id = \"%s_splitter\" % _py_tmp_key ( h2o . connection ( ) . session_id ) \n    i = 0 \n    while num_slices > i : \n        if i == 0 : \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( upper_boundary >= tmp_runif ) , : ] \n        elif i == num_slices - 1 : \n            lower_boundary = boundaries [ i - 1 ] \n            tmp_slice = self [ ( lower_boundary < tmp_runif ) , : ] \n        else : \n            lower_boundary = boundaries [ i - 1 ] \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( ( lower_boundary < tmp_runif ) & ( upper_boundary >= tmp_runif ) ) , : ] \n        if destination_frames is None : \n            splits . append ( tmp_slice ) \n        else : \n            destination_frame_id = destination_frames [ i ] \n            tmp_slice . frame_id = destination_frame_id \n            splits . append ( tmp_slice ) \n        i += 1 \n    del tmp_runif \n    return splits "}
{"1432": "\ndef isax ( self , num_words , max_cardinality , optimize_card = False , ** kwargs ) : \n    if 0 >= num_words : \n        raise H2OValueError ( \"num_words must be greater than 0\" ) \n    if 0 >= max_cardinality : \n        raise H2OValueError ( \"max_cardinality must be greater than 0\" ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"isax\" , self , num_words , max_cardinality , optimize_card ) ) "}
{"1437": "\ndef difflag1 ( self ) : \n    if 1 < self . ncols : \n        raise H2OValueError ( \"Only single-column frames supported\" ) \n    if self . types [ self . columns [ 0 ] ] not in { \"real\" , \"int\" , \"bool\" } : \n        raise H2OValueError ( \"Numeric column expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"difflag1\" , self ) , cache = self . _ex . _cache ) \n    return fr "}
{"1477": "\ndef get_human_readable_bytes ( size ) : \n    if size == 0 : \n        return \"0\" \n    if size is None : \n        return \"\" \n    assert_is_type ( size , int ) \n    assert 0 <= size , \"`size` cannot be negative, got %d\" % size \n    suffixes = \"PTGMk\" \n    maxl = len ( suffixes ) \n    for i in range ( maxl + 1 ) : \n        shift = ( maxl - i ) * 10 \n        if size >> shift == 0 : \n            continue \n        ndigits = 0 \n        for nd in [ 3 , 2 , 1 ] : \n            if size >> ( shift + 12 - nd * 3 ) == 0 : \n                ndigits = nd \n                break \n        if ndigits == 0 or size == ( size >> shift ) << shift : \n            rounded_val = str ( size >> shift ) \n        else : \n            rounded_val = \"%.*f\" % ( ndigits , size / ( 1 << shift ) ) \n        return \"%s %sb\" % ( rounded_val , suffixes [ i ] if maxl > i else \"\" ) "}
{"1478": "\ndef normalize_slice ( s , total ) : \n    newstart = 0 if s . start is None else max ( 0 , s . start + total ) if 0 > s . start else min ( s . start , total ) \n    newstop = total if s . stop is None else max ( 0 , s . stop + total ) if 0 > s . stop else min ( s . stop , total ) \n    newstep = 1 if s . step is None else s . step \n    return slice ( newstart , newstop , newstep ) "}
{"1479": "\ndef slice_is_normalized ( s ) : \n    return ( s . start is not None and s . stop is not None and s . step is not None and s . stop >= s . start ) "}
{"1482": "\ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert 2 <= len ( stack ) and stack [ - 1 ] [ 2 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack \n            print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) \n            print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) \n            print ( \"        ^^^^ %s\" % message ) \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = True \n        return decorator_invisible \n    return deprecated_decorator "}
{"1496": "\ndef extractRunInto ( javaLogText ) : \n    global g_initialXY \n    global g_reguarlize_Y \n    global g_regularize_X_objective \n    global g_updateX \n    global g_updateY \n    global g_objective \n    global g_stepsize \n    global g_history \n    if os . path . isfile ( javaLogText ) : \n        run_result = dict ( ) \n        run_result [ \"total time (ms)\" ] = [ ] \n        run_result [ \"initialXY (ms)\" ] = [ ] \n        run_result [ \"regularize Y (ms)\" ] = [ ] \n        run_result [ \"regularize X and objective (ms)\" ] = [ ] \n        run_result [ \"update X (ms)\" ] = [ ] \n        run_result [ \"update Y (ms)\" ] = [ ] \n        run_result [ \"objective (ms)\" ] = [ ] \n        run_result [ \"step size (ms)\" ] = [ ] \n        run_result [ \"update history (ms)\" ] = [ ] \n        total_run_time = - 1 \n        val = 0.0 \n        with open ( javaLogText , 'r' ) as thefile : \n            for each_line in thefile : \n                temp_string = each_line . split ( ) \n                if 0 < len ( temp_string ) : \n                    val = temp_string [ - 1 ] . replace ( '\\\\' , '' ) \n                if g_initialXY in each_line : \n                    if 0 < total_run_time : \n                        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n                        total_run_time = 0.0 \n                    else : \n                        total_run_time = 0.0 \n                    run_result [ \"initialXY (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_reguarlize_Y in each_line : \n                    run_result [ \"regularize Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_regularize_X_objective in each_line : \n                    run_result [ \"regularize X and objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateX in each_line : \n                    run_result [ \"update X (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateY in each_line : \n                    run_result [ \"update Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_objective in each_line : \n                    run_result [ \"objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_stepsize in each_line : \n                    run_result [ \"step size (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_history in each_line : \n                    run_result [ \"update history (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n        print ( \"Run result summary: \\n {0}\" . format ( run_result ) ) \n    else : \n        print ( \"Cannot find your java log file.  Nothing is done.\\n\" ) "}
{"1497": "\ndef main ( argv ) : \n    global g_test_root_dir \n    global g_temp_filename \n    if 2 > len ( argv ) : \n        print ( \"invoke this script as python extractGLRMRuntimeJavaLog.py javatextlog.\\n\" ) \n        sys . exit ( 1 ) \n    else : \n        javaLogText = argv [ 1 ] \n        print ( \"your java text is {0}\" . format ( javaLogText ) ) \n        extractRunInto ( javaLogText ) "}
{"1508": "\ndef get_automl ( project_name ) : \n    automl_json = h2o . api ( \"GET /99/AutoML/%s\" % project_name ) \n    project_name = automl_json [ \"project_name\" ] \n    leaderboard_list = [ key [ \"name\" ] for key in automl_json [ 'leaderboard' ] [ 'models' ] ] \n    if leaderboard_list is not None and 0 < len ( leaderboard_list ) : \n        leader_id = leaderboard_list [ 0 ] \n    else : \n        leader_id = None \n    leader = h2o . get_model ( leader_id ) \n    is_progress = H2OJob . __PROGRESS_BAR__ \n    h2o . no_progress ( ) \n    try : \n        leaderboard = h2o . H2OFrame ( automl_json [ \"leaderboard_table\" ] . cell_values , column_names = automl_json [ \"leaderboard_table\" ] . col_header ) \n    except Exception as ex : \n        raise ex \n    finally : \n        if is_progress is True : \n            h2o . show_progress ( ) \n    leaderboard = leaderboard [ 1 : ] \n    automl_dict = { 'project_name' : project_name , \"leader\" : leader , \"leaderboard\" : leaderboard } \n    return automl_dict "}
{"1516": "\ndef find_git_hash_branch ( each_line , temp_func_list ) : \n    global g_git_hash_branch \n    global g_failed_test_info_dict \n    if g_git_hash_branch in each_line : \n        [ start , found , endstr ] = each_line . partition ( g_git_hash_branch ) \n        temp_strings = endstr . strip ( ) . split ( ) \n        if 1 < len ( temp_strings ) : \n            g_failed_test_info_dict [ \"4.git_hash\" ] = temp_strings [ 0 ] \n            g_failed_test_info_dict [ \"5.git_branch\" ] = temp_strings [ 1 ] \n        temp_func_list . remove ( find_git_hash_branch ) \n    return True "}
{"1518": "\ndef find_build_failure ( each_line , temp_func_list ) : \n    global g_build_success \n    global g_build_success_tests \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    global g_build_failed_message \n    for ind in range ( 0 , len ( g_build_failed_message ) ) : \n        if g_build_failed_message [ ind ] in each_line . lower ( ) : \n            if ( ( ind == 0 ) and ( 0 < len ( g_failed_jobs ) ) ) : \n                continue \n            else : \n                g_failure_occurred = True \n                g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' \n                temp_func_list . remove ( find_build_failure ) \n                return False \n    return True "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = False \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if 0 < len ( found ) : \n                    if 0 < len ( g_current_testname ) : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( 6 <= len ( temp_strings ) ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : \n                if g_toContinue == True : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = False \n            else : \n                if g_toContinue : \n                    tempMessage += each_line \n            if ( ( 5 < len ( temp_strings ) ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) \n                if found and ( 0 < len ( endStr . strip ( ) ) ) : \n                    tempMessage += endStr \n                    messageType = temp_strings [ 5 ] \n                    g_toContinue = True \n        java_file . close ( ) "}
{"1525": "\ndef write_java_message ( key , val , text_file ) : \n    text_file . write ( key ) \n    text_file . write ( '\\n' ) \n    if ( 0 < len ( val [ 0 ] ) ) and ( 3 <= len ( val ) ) : \n        for index in range ( len ( val [ 0 ] ) ) : \n            text_file . write ( \"Java Message Type: \" ) \n            text_file . write ( val [ 1 ] [ index ] ) \n            text_file . write ( '\\n' ) \n            text_file . write ( \"Java Message: \" ) \n            for jmess in val [ 2 ] [ index ] : \n                text_file . write ( jmess ) \n                text_file . write ( '\\n' ) \n        text_file . write ( '\\n \\n' ) "}
{"1529": "\ndef poll ( self , verbose_model_scoring_history = False ) : \n    try : \n        hidden = not H2OJob . __PROGRESS_BAR__ \n        pb = ProgressBar ( title = self . _job_type + \" progress\" , hidden = hidden ) \n        if verbose_model_scoring_history : \n            pb . execute ( self . _refresh_job_status , print_verbose_info = lambda x : self . _print_verbose_info ( ) if int ( x * 10 ) % 5 == 0 else \" \" ) \n        else : \n            pb . execute ( self . _refresh_job_status ) \n    except StopIteration as e : \n        if str ( e ) == \"cancelled\" : \n            h2o . api ( \"POST /3/Jobs/%s/cancel\" % self . job_key ) \n            self . status = \"CANCELLED\" \n    assert self . status in { \"DONE\" , \"CANCELLED\" , \"FAILED\" } or 0 >= self . _poll_count , \"Polling finished while the job has status %s\" % self . status \n    if self . warnings : \n        for w in self . warnings : \n            warnings . warn ( w ) \n    if self . status == \"CANCELLED\" : \n        raise H2OJobCancelled ( \"Job<%s> was cancelled by the user.\" % self . job_key ) \n    if self . status == \"FAILED\" : \n        if ( isinstance ( self . job , dict ) ) and ( \"stacktrace\" in list ( self . job ) ) : \n            raise EnvironmentError ( \"Job with key {} failed with an exception: {}\\nstacktrace: \" \"\\n{}\" . format ( self . job_key , self . exception , self . job [ \"stacktrace\" ] ) ) \n        else : \n            raise EnvironmentError ( \"Job with key %s failed with an exception: %s\" % ( self . job_key , self . exception ) ) \n    return self "}
{"1538": "\ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if self . algo == 'glm' or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : \n        if self . algo == 'glm' : \n            tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values \n            maxVal = 0 \n            sum = 0 \n            for item in tempvals : \n                sum = sum + item [ 1 ] \n                if maxVal < item [ 1 ] : \n                    maxVal = item [ 1 ] \n            vals = [ ] \n            for item in tempvals : \n                tempT = ( item [ 0 ] , item [ 1 ] , item [ 1 ] / maxVal , item [ 1 ] / sum ) \n                vals . append ( tempT ) \n            header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] \n        else : \n            vals = model [ \"variable_importances\" ] . cell_values \n            header = model [ \"variable_importances\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        print ( \"Warning: This model doesn't have variable importances\" ) "}
{"1551": "\ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        print ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            print ( self . _table_description ) \n    print ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if 20 < nr : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if 20 < nr and can_use_pandas ( ) : \n        print ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1552": "\ndef start ( jar_path = None , nthreads = - 1 , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and max_mem_size < min_mem_size : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( port [ - 1 ] == \"+\" and port [ : - 1 ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - 1 ] ) \n            port = 0 \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        print ( \"Attempting to start a local H2O server...\" ) \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        print ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1568": "\ndef extract_message_to_dict ( filename ) : \n    message_dict = { } \n    if os . path . isfile ( filename ) : \n        with open ( filename , 'r' ) as wfile : \n            key = \"\" \n            val = \"\" \n            startMess = False \n            while 1 : \n                each_line = wfile . readline ( ) \n                if not each_line : \n                    if startMess : \n                        add_to_dict ( val . strip ( ) , key , message_dict ) \n                    break \n                if \"keyname\" in each_line . lower ( ) : \n                    temp_strings = each_line . strip ( ) . split ( '=' ) \n                    if ( 1 < len ( temp_strings ) ) : \n                        if startMess : \n                            add_to_dict ( val . strip ( ) , key , message_dict ) \n                            val = \"\" \n                        key = temp_strings [ 1 ] . strip ( ) \n                        startMess = False \n                if ( 1 < len ( each_line ) ) and startMess : \n                    val += each_line \n                if \"ignoredmessage\" in each_line . lower ( ) : \n                    startMess = True \n                    temp_mess = each_line . split ( '=' ) \n                    if ( 1 < len ( temp_mess ) ) : \n                        val = temp_mess [ 1 ] \n    return message_dict "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if 2 > len ( argv ) : \n        usage ( ) \n    i = 1 \n    while ( len ( argv ) > i ) : \n        s = argv [ i ] \n        if ( s == \"--inputfileadd\" ) : \n            i += 1 \n            if ( len ( argv ) < i ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        elif ( s == \"--inputfilerm\" ) : \n            i += 1 \n            if ( len ( argv ) < i ) : \n                usage ( ) \n            g_old_messages_to_remove = argv [ i ] \n        elif ( s == \"--loadjavamessage\" ) : \n            i += 1 \n            if len ( argv ) < i : \n                usage ( ) \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == \"--savejavamessage\" ) : \n            i += 1 \n            if ( len ( argv ) < i ) : \n                usage ( ) \n            g_save_java_message_filename = argv [ i ] \n        elif ( s == '--printjavamessage' ) : \n            i += 1 \n            g_print_java_messages = True \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == '--help' ) : \n            usage ( ) \n        else : \n            unknown_arg ( s ) \n        i += 1 "}
{"1577": "\ndef summarizeFailedRuns ( ) : \n    global g_summary_dict_all \n    onlyFiles = [ x for x in listdir ( g_test_root_dir ) if isfile ( join ( g_test_root_dir , x ) ) ] \n    for f in onlyFiles : \n        for fileStart in g_file_start : \n            if ( fileStart in f ) and ( 10 < os . path . getsize ( f ) ) : \n                fFullPath = os . path . join ( g_test_root_dir , f ) \n                try : \n                    temp_dict = json . load ( open ( fFullPath , 'r' ) ) \n                    for ind in range ( len ( temp_dict [ \"TestName\" ] ) ) : \n                        addFailedTests ( g_summary_dict_all , temp_dict , ind ) \n                except : \n                    continue \n                break "}
{"1578": "\ndef extractPrintSaveIntermittens ( ) : \n    global g_summary_dict_intermittents \n    localtz = time . tzname [ 0 ] \n    for ind in range ( len ( g_summary_dict_all [ \"TestName\" ] ) ) : \n        if g_threshold_failure <= g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] : \n            addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) \n    if 0 < len ( g_summary_dict_intermittents [ \"TestName\" ] ) : \n        json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) \n        with open ( g_summary_csv_filename , 'w' ) as summaryFile : \n            for ind in range ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) ) : \n                testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] \n                numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] \n                firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ 0 ] ) \n                summaryFile . write ( eachTest ) \n                print ( \"Intermittent: {0}, Last failed: {1}, Failed {2} times since \" \"{3}\" . format ( testName , recentFailStr , numberFailure , firstFailedStr ) ) "}
{"1590": "\ndef check_not_bot ( self , user_id ) : \n    self . small_delay ( ) \n    user_id = self . convert_to_user_id ( user_id ) \n    if not user_id : \n        return False \n    if user_id in self . whitelist : \n        return True \n    if user_id in self . blacklist : \n        return False \n    user_info = self . get_user_info ( user_id ) \n    if not user_info : \n        return True \n    skipped = self . skipped_file \n    if \"following_count\" in user_info and self . max_following_to_block < user_info [ \"following_count\" ] : \n        msg = 'following_count > bot.max_following_to_block, skipping!' \n        self . console_print ( msg , 'red' ) \n        skipped . append ( user_id ) \n        return False \n    if search_stop_words_in_user ( self , user_info ) : \n        msg = '`bot.search_stop_words_in_user` found in user, skipping!' \n        skipped . append ( user_id ) \n        return False \n    return True "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if 3 > sys . version_info [ 0 ] : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        print ( str ( exception ) ) \n        return [ ] "}
{"1601": "\ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            print ( 'request:' ) \n            try : \n                print ( request . body ) \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        elif 0 < resp . length : \n            respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            print ( 'response:' ) \n            try : \n                print ( respbody ) \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if status == 307 : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if 300 <= status : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1621": "\ndef putheader ( self , name , value ) : \n    if ( 3 , ) > sys . version_info : \n        name = str ( name ) . decode ( 'utf-8' ) \n        value = str ( value ) . decode ( 'utf-8' ) \n    self . _httprequest . set_request_header ( name , value ) "}
{"1731": "\ndef get_receiver ( self , session = None , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if self . entity and not self . requires_session and session : \n        raise ValueError ( \"A session cannot be used with a non-sessionful entitiy.\" ) \n    if self . entity and self . requires_session and not session : \n        raise ValueError ( \"This entity requires a session.\" ) \n    if 0 > int ( prefetch ) or 50000 < int ( prefetch ) : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if session : \n        return SessionReceiver ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) \n    return Receiver ( handler_id , self . entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1732": "\ndef get_deadletter_receiver ( self , transfer_deadletter = False , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if 0 > int ( prefetch ) or 50000 < int ( prefetch ) : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if transfer_deadletter : \n        entity_uri = self . mgmt_client . format_transfer_dead_letter_queue_name ( self . entity_uri ) \n    else : \n        entity_uri = self . mgmt_client . format_dead_letter_queue_name ( self . entity_uri ) \n    return Receiver ( handler_id , entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1802": "\ndef _token_is_expired ( self , token ) : \n    time_pos_begin = token . find ( 'ExpiresOn=' ) + len ( 'ExpiresOn=' ) \n    time_pos_end = token . find ( '&' , time_pos_begin ) \n    token_expire_time = int ( token [ time_pos_begin : time_pos_end ] ) \n    time_now = time . mktime ( time . localtime ( ) ) \n    return 30 > ( token_expire_time - time_now ) "}
{"1810": "\ndef as_batch_body ( self ) : \n    if ( 3 , ) <= sys . version_info and isinstance ( self . body , bytes ) : \n        body = self . body . decode ( 'utf-8' ) \n    else : \n        body = self . body \n    result = { 'Body' : body } \n    if self . custom_properties : \n        result [ 'UserProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . custom_properties . items ( ) } \n    if self . broker_properties : \n        result [ 'BrokerProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . broker_properties . items ( ) } \n    return result "}
{"1835": "\ndef expired ( self ) : \n    if self . locked_until and datetime . datetime . now ( ) >= self . locked_until : \n        return True \n    return False "}
{"1840": "\ndef _bulk_add_tasks ( self , results_queue , chunk_tasks_to_add ) : \n    try : \n        add_collection_response = self . _original_add_collection ( self . _client , self . _job_id , chunk_tasks_to_add , self . _task_add_collection_options , self . _custom_headers , self . _raw ) \n    except BatchErrorException as e : \n        if e . error . code == \"RequestBodyTooLarge\" : \n            if len ( chunk_tasks_to_add ) == 1 : \n                failed_task = chunk_tasks_to_add . pop ( ) \n                self . errors . appendleft ( e ) \n                _LOGGER . error ( \"Failed to add task with ID %s due to the body\" \" exceeding the maximum request size\" , failed_task . id ) \n            else : \n                midpoint = int ( len ( chunk_tasks_to_add ) / 2 ) \n                with self . _max_tasks_lock : \n                    if self . _max_tasks_per_request > midpoint : \n                        self . _max_tasks_per_request = midpoint \n                        _LOGGER . info ( \"Amount of tasks per request reduced from %s to %s due to the\" \" request body being too large\" , str ( self . _max_tasks_per_request ) , str ( midpoint ) ) \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add [ midpoint : ] ) \n                self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add [ : midpoint ] ) \n        elif 500 <= e . response . status_code <= 599 : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        else : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n            self . errors . appendleft ( e ) \n    except Exception as e : \n        self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        self . errors . appendleft ( e ) \n    else : \n        try : \n            add_collection_response = add_collection_response . output \n        except AttributeError : \n            pass \n        for task_result in add_collection_response . value : \n            if task_result . status == TaskAddStatus . server_error : \n                with self . _pending_queue_lock : \n                    for task in chunk_tasks_to_add : \n                        if task . id == task_result . task_id : \n                            self . tasks_to_add . appendleft ( task ) \n            elif ( task_result . status == TaskAddStatus . client_error and not task_result . error . code == \"TaskExists\" ) : \n                self . failure_tasks . appendleft ( task_result ) \n            else : \n                results_queue . appendleft ( task_result ) "}
{"1841": "\ndef task_collection_thread_handler ( self , results_queue ) : \n    while self . tasks_to_add and not self . errors : \n        max_tasks = self . _max_tasks_per_request \n        chunk_tasks_to_add = [ ] \n        with self . _pending_queue_lock : \n            while max_tasks > len ( chunk_tasks_to_add ) and self . tasks_to_add : \n                chunk_tasks_to_add . append ( self . tasks_to_add . pop ( ) ) \n        if chunk_tasks_to_add : \n            self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add ) "}
{"1884": "\ndef get_cut_prefix ( value , max_len ) : \n    should_convert = isinstance ( value , bytes ) \n    if should_convert : \n        value = value . decode ( \"utf8\" , \"ignore\" ) \n    for i in range ( len ( value ) ) : \n        if max_len >= terminal_width ( value [ i : ] ) : \n            break \n    return value [ i : ] . encode ( \"utf8\" , \"ignore\" ) if should_convert else value [ i : ] "}
{"1886": "\ndef format_filesize ( size ) : \n    for suffix in ( \"bytes\" , \"KB\" , \"MB\" , \"GB\" , \"TB\" ) : \n        if 1024.0 > size : \n            if suffix in ( \"GB\" , \"TB\" ) : \n                return \"{0:3.2f} {1}\" . format ( size , suffix ) \n            else : \n                return \"{0:3.1f} {1}\" . format ( size , suffix ) \n        size /= 1024.0 "}
{"1887": "\ndef format_time ( elapsed ) : \n    hours = int ( elapsed / ( 60 * 60 ) ) \n    minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) \n    seconds = int ( elapsed % 60 ) \n    rval = \"\" \n    if hours : \n        rval += \"{0}h\" . format ( hours ) \n    if 60 < elapsed : \n        rval += \"{0}m\" . format ( minutes ) \n    rval += \"{0}s\" . format ( seconds ) \n    return rval "}
{"1888": "\ndef create_status_line ( ** params ) : \n    max_size = get_terminal_size ( ) . columns - 1 \n    for fmt in PROGRESS_FORMATS : \n        status = fmt . format ( ** params ) \n        if max_size >= len ( status ) : \n            break \n    return status "}
{"1889": "\ndef progress ( iterator , prefix ) : \n    if 25 < terminal_width ( prefix ) : \n        prefix = ( \"..\" + get_cut_prefix ( prefix , 23 ) ) \n    speed_updated = start = time ( ) \n    speed_written = written = 0 \n    speed_history = deque ( maxlen = 5 ) \n    for data in iterator : \n        yield data \n        now = time ( ) \n        elapsed = now - start \n        written += len ( data ) \n        speed_elapsed = now - speed_updated \n        if 0.5 <= speed_elapsed : \n            speed_history . appendleft ( ( written - speed_written , speed_updated , ) ) \n            speed_updated = now \n            speed_written = written \n            speed_history_written = sum ( h [ 0 ] for h in speed_history ) \n            speed_history_elapsed = now - speed_history [ - 1 ] [ 1 ] \n            speed = speed_history_written / speed_history_elapsed \n            status = create_status_line ( prefix = prefix , written = format_filesize ( written ) , elapsed = format_time ( elapsed ) , speed = format_filesize ( speed ) ) \n            print_inplace ( status ) \n    sys . stderr . write ( \"\\n\" ) \n    sys . stderr . flush ( ) "}
{"1895": "\ndef _pv_params ( cls , session , pvswf , pv , ** request_params ) : \n    try : \n        data , hdntl = pv . split ( \";\" ) \n    except ValueError : \n        data = pv \n        hdntl = \"\" \n    cache = Cache ( filename = \"stream.json\" ) \n    key = \"akamaihd-player:\" + pvswf \n    cached = cache . get ( key ) \n    request_params = deepcopy ( request_params ) \n    headers = request_params . pop ( \"headers\" , { } ) \n    if cached : \n        headers [ \"If-Modified-Since\" ] = cached [ \"modified\" ] \n    swf = session . http . get ( pvswf , headers = headers , ** request_params ) \n    if cached and swf . status_code == 304 : \n        hash = cached [ \"hash\" ] \n    else : \n        hash = sha256 ( ) \n        hash . update ( swfdecompress ( swf . content ) ) \n        hash = base64 . b64encode ( hash . digest ( ) ) . decode ( \"ascii\" ) \n        modified = swf . headers . get ( \"Last-Modified\" , \"\" ) \n        if 40 > len ( modified ) : \n            cache . set ( key , dict ( hash = hash , modified = modified ) ) \n    msg = \"st=0~exp=9999999999~acl=*~data={0}!{1}\" . format ( data , hash ) \n    auth = hmac . new ( AKAMAIHD_PV_KEY , msg . encode ( \"ascii\" ) , sha256 ) \n    pvtoken = \"{0}~hmac={1}\" . format ( msg , auth . hexdigest ( ) ) \n    params = [ ( \"pvtoken\" , pvtoken ) ] \n    params . extend ( parse_qsl ( hdntl , keep_blank_values = True ) ) \n    return params "}
{"1898": "\ndef parse_json ( data , name = \"JSON\" , exception = PluginError , schema = None ) : \n    try : \n        json_data = json . loads ( data ) \n    except ValueError as err : \n        snippet = repr ( data ) \n        if 35 < len ( snippet ) : \n            snippet = snippet [ : 35 ] + \" ...\" \n        else : \n            snippet = data \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        json_data = schema . validate ( json_data , name = name , exception = exception ) \n    return json_data "}
{"1899": "\ndef parse_xml ( data , name = \"XML\" , ignore_ns = False , exception = PluginError , schema = None , invalid_char_entities = False ) : \n    if is_py2 and isinstance ( data , unicode ) : \n        data = data . encode ( \"utf8\" ) \n    elif is_py3 and isinstance ( data , str ) : \n        data = bytearray ( data , \"utf8\" ) \n    if ignore_ns : \n        data = re . sub ( br\"[\\t ]xmlns=\\\"(.+?)\\\"\" , b\"\" , data ) \n    if invalid_char_entities : \n        data = re . sub ( br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)' , b'&amp;' , data ) \n    try : \n        tree = ET . fromstring ( data ) \n    except Exception as err : \n        snippet = repr ( data ) \n        if 35 < len ( snippet ) : \n            snippet = snippet [ : 35 ] + \" ...\" \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        tree = schema . validate ( tree , name = name , exception = exception ) \n    return tree "}
{"1902": "\ndef spawn ( self , parameters = None , arguments = None , stderr = None , timeout = None , short_option_prefix = \"-\" , long_option_prefix = \"--\" ) : \n    stderr = stderr or self . stderr \n    cmd = self . bake ( self . _check_cmd ( ) , parameters , arguments , short_option_prefix , long_option_prefix ) \n    log . debug ( \"Spawning command: {0}\" , subprocess . list2cmdline ( cmd ) ) \n    try : \n        process = subprocess . Popen ( cmd , stderr = stderr , stdout = subprocess . PIPE ) \n    except ( OSError , IOError ) as err : \n        raise StreamError ( \"Failed to start process: {0} ({1})\" . format ( self . _check_cmd ( ) , str ( err ) ) ) \n    if timeout : \n        elapsed = 0 \n        while timeout > elapsed and not process . poll ( ) : \n            time . sleep ( 0.25 ) \n            elapsed += 0.25 \n        if not process . poll ( ) : \n            try : \n                log . debug ( \"Process timeout expired ({0}s), killing process\" . format ( timeout ) ) \n                process . kill ( ) \n            except Exception : \n                pass \n        process . wait ( ) \n    return process "}
{"1904": "\ndef parse_manifest ( cls , session , url_or_manifest , ** args ) : \n    ret = { } \n    if url_or_manifest . startswith ( '<?xml' ) : \n        mpd = MPD ( parse_xml ( url_or_manifest , ignore_ns = True ) ) \n    else : \n        res = session . http . get ( url_or_manifest , ** args ) \n        url = res . url \n        urlp = list ( urlparse ( url ) ) \n        urlp [ 2 ] , _ = urlp [ 2 ] . rsplit ( \"/\" , 1 ) \n        mpd = MPD ( session . http . xml ( res , ignore_ns = True ) , base_url = urlunparse ( urlp ) , url = url ) \n    video , audio = [ ] , [ ] \n    for aset in mpd . periods [ 0 ] . adaptationSets : \n        if aset . contentProtection : \n            raise PluginError ( \"{} is protected by DRM\" . format ( url ) ) \n        for rep in aset . representations : \n            if rep . mimeType . startswith ( \"video\" ) : \n                video . append ( rep ) \n            elif rep . mimeType . startswith ( \"audio\" ) : \n                audio . append ( rep ) \n    if not video : \n        video = [ None ] \n    if not audio : \n        audio = [ None ] \n    locale = session . localization \n    locale_lang = locale . language \n    lang = None \n    available_languages = set ( ) \n    for aud in audio : \n        if aud and aud . lang : \n            available_languages . add ( aud . lang ) \n            try : \n                if locale . explicit and aud . lang and Language . get ( aud . lang ) == locale_lang : \n                    lang = aud . lang \n            except LookupError : \n                continue \n    if not lang : \n        lang = audio [ 0 ] and audio [ 0 ] . lang \n    log . debug ( \"Available languages for DASH audio streams: {0} (using: {1})\" . format ( \", \" . join ( available_languages ) or \"NONE\" , lang or \"n/a\" ) ) \n    if 1 < len ( available_languages ) : \n        audio = list ( filter ( lambda a : a . lang is None or a . lang == lang , audio ) ) \n    for vid , aud in itertools . product ( video , audio ) : \n        stream = DASHStream ( session , mpd , vid , aud , ** args ) \n        stream_name = [ ] \n        if vid : \n            stream_name . append ( \"{:0.0f}{}\" . format ( vid . height or vid . bandwidth_rounded , \"p\" if vid . height else \"k\" ) ) \n        if audio and 1 < len ( audio ) : \n            stream_name . append ( \"a{:0.0f}k\" . format ( aud . bandwidth ) ) \n        ret [ '+' . join ( stream_name ) ] = stream \n    return ret "}
{"1916": "\ndef iter_chunks ( self , fd = None , buf = None , skip_header = None ) : \n    timestamps = dict ( self . timestamps_add ) \n    tag_iterator = self . iter_tags ( fd = fd , buf = buf , skip_header = skip_header ) \n    if not self . flv_header_written : \n        analyzed_tags = self . analyze_tags ( tag_iterator ) \n    else : \n        analyzed_tags = [ ] \n    for tag in chain ( analyzed_tags , tag_iterator ) : \n        if not self . flv_header_written : \n            flv_header = Header ( has_video = self . has_video , has_audio = self . has_audio ) \n            yield flv_header . serialize ( ) \n            self . flv_header_written = True \n        if self . verify_tag ( tag ) : \n            self . adjust_tag_gap ( tag ) \n            self . adjust_tag_timestamp ( tag ) \n            if self . duration : \n                norm_timestamp = tag . timestamp / 1000 \n                if self . duration < norm_timestamp : \n                    break \n            yield tag . serialize ( ) \n            timestamps [ tag . type ] = tag . timestamp \n    if not self . flatten_timestamps : \n        self . timestamps_add = timestamps \n    self . tags = [ ] "}
{"1929": "\ndef fetch_streams_with_retry ( plugin , interval , count ) : \n    try : \n        streams = fetch_streams ( plugin ) \n    except PluginError as err : \n        log . error ( u\"{0}\" , err ) \n        streams = None \n    if not streams : \n        log . info ( \"Waiting for streams, retrying every {0} \" \"second(s)\" , interval ) \n    attempts = 0 \n    while not streams : \n        sleep ( interval ) \n        try : \n            streams = fetch_streams ( plugin ) \n        except FatalPluginError as err : \n            raise \n        except PluginError as err : \n            log . error ( u\"{0}\" , err ) \n        if 0 < count : \n            attempts += 1 \n            if count <= attempts : \n                break \n    return streams "}
{"1931": "\ndef format_valid_streams ( plugin , streams ) : \n    delimiter = \", \" \n    validstreams = [ ] \n    for name , stream in sorted ( streams . items ( ) , key = lambda stream : plugin . stream_weight ( stream [ 0 ] ) ) : \n        if name in STREAM_SYNONYMS : \n            continue \n        def synonymfilter ( n ) : \n            return stream is streams [ n ] and n is not name \n        synonyms = list ( filter ( synonymfilter , streams . keys ( ) ) ) \n        if 0 < len ( synonyms ) : \n            joined = delimiter . join ( synonyms ) \n            name = \"{0} ({1})\" . format ( name , joined ) \n        validstreams . append ( name ) \n    return delimiter . join ( validstreams ) "}
{"1974": "\ndef outputFormatter ( s ) : \n    result = '' \n    def formatSubString ( s ) : \n        for c in s : \n            if c == 32 : \n                yield ' ' \n            else : \n                yield outputCharFormatter ( c ) \n    if 200 > len ( result ) : \n        return '' . join ( formatSubString ( s ) ) \n    else : \n        return '' . join ( formatSubString ( s [ : 100 ] ) ) + '...' + '' . join ( formatSubString ( s [ - 100 : ] ) ) "}
{"1983": "\ndef value ( self , index , extra ) : \n    lower , upper = self . span ( index ) \n    value = lower + ( extra or 0 ) \n    if upper < value : \n        raise ValueError ( 'value: extra out of range' ) \n    return value "}
{"1985": "\ndef value ( self , index , extra ) : \n    index = index \n    if index == 0 : \n        return 1 , 0 \n    if self . RLEMAX >= index : \n        return ( 1 << index ) + extra , 0 \n    return 1 , index - self . RLEMAX "}
{"1986": "\ndef mnemonic ( self , index ) : \n    i , c , d0 = self . splitSymbol ( index ) \n    iLower , _ = i . code . span ( i . index ) \n    iExtra = i . extraBits ( ) \n    cLower , _ = c . code . span ( c . index ) \n    cExtra = c . extraBits ( ) \n    return 'I{}{}{}C{}{}{}{}' . format ( iLower , '+' if iExtra else '' , 'x' * iExtra if 6 > iExtra else '[{}*x]' . format ( iExtra ) , cLower , '+' if cExtra else '' , 'x' * cExtra if 6 > cExtra else '[{}*x]' . format ( cExtra ) , '&D=0' if d0 else '' ) "}
{"1987": "\ndef mnemonic ( self , index , verbose = False ) : \n    if 16 > index : \n        return [ 'last' , '2last' , '3last' , '4last' , 'last-1' , 'last+1' , 'last-2' , 'last+2' , 'last-3' , 'last+3' , '2last-1' , '2last+1' , '2last-2' , '2last+2' , '2last-3' , '2last+3' ] [ index ] \n    if 16 + self . NDIRECT > index : \n        return str ( index - 16 ) \n    index -= self . NDIRECT + 16 \n    hcode = index >> self . NPOSTFIX \n    lcode = index & ( 1 << self . NPOSTFIX ) - 1 \n    if self . NPOSTFIX : \n        formatString = '1{0}{1}{2:0{3}b}{4:+d}' \n    else : \n        formatString = '1{0}{1}{4:+d}' \n    return formatString . format ( hcode & 1 , 'x' * ( 2 + hcode >> 1 ) if 13 > hcode or verbose else '[{}*x]' . format ( 2 + hcode >> 1 ) , lcode , self . NPOSTFIX , self . NDIRECT + 1 - ( 4 << self . NPOSTFIX ) ) "}
{"1994": "\ndef blockType ( self , kind ) : \n    NBLTYPES = self . verboseRead ( TypeCountAlphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) \n    self . numberOfBlockTypes [ kind ] = NBLTYPES \n    if 2 <= NBLTYPES : \n        self . blockTypeCodes [ kind ] = self . readPrefixCode ( BlockTypeAlphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) \n        self . blockCountCodes [ kind ] = self . readPrefixCode ( BlockCountAlphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) \n        blockCount = self . verboseRead ( self . blockCountCodes [ kind ] ) \n    else : \n        blockCount = 1 << 24 \n    self . currentBlockCounts [ kind ] = blockCount "}
{"2076": "\ndef set_current_row ( self , value ) : \n    if ( value is not None ) and ( ( 0 > value ) or ( len ( self ) <= value ) ) : \n        raise IndexError ( \"index %d out of range [0,%d]\" % ( value , len ( self ) ) ) \n    self . _current_row = value \n    self . signal_pick . emit ( self , value ) "}
{"2083": "\ndef split ( self , frac ) : \n    self = self . extract ( ) \n    if _issequence ( frac ) : \n        total = sum ( frac ) \n        frac = [ k / total for k in frac ] \n    else : \n        assert 1 >= frac , \"fraction should be <= 1\" \n        frac = [ frac , 1 - frac ] \n    offsets = np . round ( np . cumsum ( frac ) * len ( self ) ) . astype ( np . int64 ) \n    start = 0 \n    for offset in offsets : \n        yield self [ start : offset ] \n        start = offset "}
{"2088": "\ndef selection_can_redo ( self , name = \"default\" ) : \n    return len ( self . selection_histories [ name ] ) > ( self . selection_history_indices [ name ] + 1 ) "}
{"2094": "\ndef select_circle ( self , x , y , xc , yc , r , mode = \"replace\" , name = \"default\" , inclusive = True ) : \n    if inclusive : \n        expr = r ** 2 >= ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 \n    else : \n        expr = r ** 2 > ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2095": "\ndef select_ellipse ( self , x , y , xc , yc , width , height , angle = 0 , mode = \"replace\" , name = \"default\" , radians = False , inclusive = True ) : \n    if radians : \n        pass \n    else : \n        alpha = np . deg2rad ( angle ) \n    xr = width / 2 \n    yr = height / 2 \n    r = max ( xr , yr ) \n    a = xr / r \n    b = yr / r \n    expr = \"(({x}-{xc})*cos({alpha})+({y}-{yc})*sin({alpha}))**2/{a}**2 + (({x}-{xc})*sin({alpha})-({y}-{yc})*cos({alpha}))**2/{b}**2 <= {r}**2\" . format ( ** locals ( ) ) \n    if inclusive : \n        expr = r ** 2 >= ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 \n    else : \n        expr = r ** 2 > ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2103": "\ndef categorize ( self , column , labels = None , check = True ) : \n    column = _ensure_string_from_expression ( column ) \n    if check : \n        vmin , vmax = self . minmax ( column ) \n        if labels is None : \n            N = int ( vmax + 1 ) \n            labels = list ( map ( str , range ( N ) ) ) \n        if len ( labels ) <= ( vmax - vmin ) : \n            raise ValueError ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) \n    self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) ) "}
{"2145": "\ndef get_autotype ( arr ) : \n    try : \n        narr = arr . astype ( 'float' ) \n        if ( sys . maxsize > narr ) . all ( ) and ( narr % 1 ) . sum ( ) == 0 : \n            return narr . astype ( 'int' ) \n        else : \n            return narr \n    except ValueError : \n        return arr "}
{"2161": "\ndef hz_to_mel ( frequencies , htk = False ) : \n    frequencies = np . asanyarray ( frequencies ) \n    if htk : \n        return 2595.0 * np . log10 ( 1.0 + frequencies / 700.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    mels = ( frequencies - f_min ) / f_sp \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if frequencies . ndim : \n        log_t = ( min_log_hz <= frequencies ) \n        mels [ log_t ] = min_log_mel + np . log ( frequencies [ log_t ] / min_log_hz ) / logstep \n    elif min_log_hz <= frequencies : \n        mels = min_log_mel + np . log ( frequencies / min_log_hz ) / logstep \n    return mels "}
{"2162": "\ndef mel_to_hz ( mels , htk = False ) : \n    mels = np . asanyarray ( mels ) \n    if htk : \n        return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    freqs = f_min + f_sp * mels \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if mels . ndim : \n        log_t = ( min_log_mel <= mels ) \n        freqs [ log_t ] = min_log_hz * np . exp ( logstep * ( mels [ log_t ] - min_log_mel ) ) \n    elif min_log_mel <= mels : \n        freqs = min_log_hz * np . exp ( logstep * ( mels - min_log_mel ) ) \n    return freqs "}
{"2169": "\ndef hybrid_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' , res_type = None ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) \n    lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , window = window ) \n    pseudo_filters = 2 * hop_length > 2.0 ** np . ceil ( np . log2 ( lengths ) ) \n    n_bins_pseudo = int ( np . sum ( pseudo_filters ) ) \n    n_bins_full = n_bins - n_bins_pseudo \n    cqt_resp = [ ] \n    if 0 < n_bins_pseudo : \n        fmin_pseudo = np . min ( freqs [ pseudo_filters ] ) \n        cqt_resp . append ( pseudo_cqt ( y , sr , hop_length = hop_length , fmin = fmin_pseudo , n_bins = n_bins_pseudo , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode ) ) \n    if 0 < n_bins_full : \n        cqt_resp . append ( np . abs ( cqt ( y , sr , hop_length = hop_length , fmin = fmin , n_bins = n_bins_full , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode , res_type = res_type ) ) ) \n    return __trim_stack ( cqt_resp , n_bins ) "}
{"2171": "\ndef icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if min ( lengths ) < hop_length : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    for octave in range ( n_octaves - 1 , - 1 , - 1 ) : \n        slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , - ( octave ) * bins_per_octave - 1 ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] \n        oct_hop = hop_length // 2 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , 1 , 2 , scale = True , res_type = res_type , fix = False ) \n            y [ : len ( y_oct ) ] += y_oct \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2172": "\ndef __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : \n    basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = True , window = window ) \n    n_fft = basis . shape [ 1 ] \n    if ( hop_length is not None and 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) > n_fft ) : \n        n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) \n    basis *= lengths [ : , np . newaxis ] / float ( n_fft ) \n    fft = get_fftlib ( ) \n    fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2 ) + 1 ] \n    fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) \n    return fft_basis , n_fft , lengths "}
{"2176": "\ndef __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : \n    downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) \n    if 0 < downsample_count and res_type == 'kaiser_fast' : \n        downsample_factor = 2 ** ( downsample_count ) \n        hop_length //= downsample_factor \n        if downsample_factor > len ( y ) : \n            raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) \n        new_sr = sr / float ( downsample_factor ) \n        y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = True ) \n        if not scale : \n            y *= np . sqrt ( downsample_factor ) \n        sr = new_sr \n    return y , sr , hop_length "}
{"2177": "\ndef __dtw_calc_accu_cost ( C , D , D_steps , step_sizes_sigma , weights_mul , weights_add , max_0 , max_1 ) : \n    for cur_n in range ( max_0 , D . shape [ 0 ] ) : \n        for cur_m in range ( max_1 , D . shape [ 1 ] ) : \n            for cur_step_idx , cur_w_add , cur_w_mul in zip ( range ( step_sizes_sigma . shape [ 0 ] ) , weights_add , weights_mul ) : \n                cur_D = D [ cur_n - step_sizes_sigma [ cur_step_idx , 0 ] , cur_m - step_sizes_sigma [ cur_step_idx , 1 ] ] \n                cur_C = cur_w_mul * C [ cur_n - max_0 , cur_m - max_1 ] \n                cur_C += cur_w_add \n                cur_cost = cur_D + cur_C \n                if D [ cur_n , cur_m ] > cur_cost : \n                    D [ cur_n , cur_m ] = cur_cost \n                    D_steps [ cur_n , cur_m ] = cur_step_idx \n    return D , D_steps "}
{"2178": "\ndef __dtw_backtracking ( D_steps , step_sizes_sigma ) : \n    wp = [ ] \n    cur_idx = ( D_steps . shape [ 0 ] - 1 , D_steps . shape [ 1 ] - 1 ) \n    wp . append ( ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ) \n    while 0 < cur_idx [ 0 ] : \n        cur_step_idx = D_steps [ ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ] \n        cur_idx = ( cur_idx [ 0 ] - step_sizes_sigma [ cur_step_idx ] [ 0 ] , cur_idx [ 1 ] - step_sizes_sigma [ cur_step_idx ] [ 1 ] ) \n        wp . append ( ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ) \n    return wp "}
{"2180": "\ndef viterbi_discriminative ( prob , transition , p_state = None , p_init = None , return_logp = False ) : \n    n_states , n_steps = prob . shape \n    if transition . shape != ( n_states , n_states ) : \n        raise ParameterError ( 'transition.shape={}, must be ' '(n_states, n_states)={}' . format ( transition . shape , ( n_states , n_states ) ) ) \n    if np . any ( 0 > transition ) or not np . allclose ( transition . sum ( axis = 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid transition matrix: must be non-negative ' 'and sum to 1 on each row.' ) \n    if np . any ( 0 > prob ) or not np . allclose ( prob . sum ( axis = 0 ) , 1 ) : \n        raise ParameterError ( 'Invalid probability values: each column must ' 'sum to 1 and be non-negative' ) \n    states = np . zeros ( n_steps , dtype = int ) \n    values = np . zeros ( ( n_steps , n_states ) , dtype = float ) \n    ptr = np . zeros ( ( n_steps , n_states ) , dtype = int ) \n    epsilon = np . finfo ( prob . dtype ) . tiny \n    if p_state is None : \n        p_state = np . empty ( n_states ) \n        p_state . fill ( 1. / n_states ) \n    elif p_state . shape != ( n_states , ) : \n        raise ParameterError ( 'Marginal distribution p_state must have shape (n_states,). ' 'Got p_state.shape={}' . format ( p_state . shape ) ) \n    elif np . any ( 0 > p_state ) or not np . allclose ( p_state . sum ( axis = - 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid marginal state distribution: ' 'p_state={}' . format ( p_state ) ) \n    log_trans = np . log ( transition + epsilon ) \n    log_marginal = np . log ( p_state + epsilon ) \n    log_prob = np . log ( prob . T + epsilon ) - log_marginal \n    if p_init is None : \n        p_init = np . empty ( n_states ) \n        p_init . fill ( 1. / n_states ) \n    elif np . any ( 0 > p_init ) or not np . allclose ( p_init . sum ( ) , 1 ) : \n        raise ParameterError ( 'Invalid initial state distribution: ' 'p_init={}' . format ( p_init ) ) \n    log_p_init = np . log ( p_init + epsilon ) \n    _viterbi ( log_prob , log_trans , log_p_init , states , values , ptr ) \n    if return_logp : \n        return states , values [ - 1 , states [ - 1 ] ] \n    return states "}
{"2181": "\ndef transition_uniform ( n_states ) : \n    if not isinstance ( n_states , int ) or 0 >= n_states : \n        raise ParameterError ( 'n_states={} must be a positive integer' ) \n    transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) \n    transition . fill ( 1. / n_states ) \n    return transition "}
{"2182": "\ndef transition_loop ( n_states , prob ) : \n    if not isinstance ( n_states , int ) or 1 >= n_states : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) \n    prob = np . asarray ( prob , dtype = np . float ) \n    if prob . ndim == 0 : \n        prob = np . tile ( prob , n_states ) \n    if prob . shape != ( n_states , ) : \n        raise ParameterError ( 'prob={} must have length equal to n_states={}' . format ( prob , n_states ) ) \n    if np . any ( 0 > prob ) or np . any ( 1 < prob ) : \n        raise ParameterError ( 'prob={} must have values in the range [0, 1]' . format ( prob ) ) \n    for i , prob_i in enumerate ( prob ) : \n        transition [ i ] = ( 1. - prob_i ) / ( n_states - 1 ) \n        transition [ i , i ] = prob_i \n    return transition "}
{"2183": "\ndef transition_cycle ( n_states , prob ) : \n    if not isinstance ( n_states , int ) or 1 >= n_states : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    prob = np . asarray ( prob , dtype = np . float ) \n    if prob . ndim == 0 : \n        prob = np . tile ( prob , n_states ) \n    if prob . shape != ( n_states , ) : \n        raise ParameterError ( 'prob={} must have length equal to n_states={}' . format ( prob , n_states ) ) \n    if np . any ( 0 > prob ) or np . any ( 1 < prob ) : \n        raise ParameterError ( 'prob={} must have values in the range [0, 1]' . format ( prob ) ) \n    for i , prob_i in enumerate ( prob ) : \n        transition [ i , np . mod ( i + 1 , n_states ) ] = 1. - prob_i \n        transition [ i , i ] = prob_i \n    return transition "}
{"2184": "\ndef transition_local ( n_states , width , window = 'triangle' , wrap = False ) : \n    if not isinstance ( n_states , int ) or 1 >= n_states : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    width = np . asarray ( width , dtype = int ) \n    if width . ndim == 0 : \n        width = np . tile ( width , n_states ) \n    if width . shape != ( n_states , ) : \n        raise ParameterError ( 'width={} must have length equal to n_states={}' . format ( width , n_states ) ) \n    if np . any ( 1 > width ) : \n        raise ParameterError ( 'width={} must be at least 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    for i , width_i in enumerate ( width ) : \n        trans_row = pad_center ( get_window ( window , width_i , fftbins = False ) , n_states ) \n        trans_row = np . roll ( trans_row , n_states // 2 + i + 1 ) \n        if not wrap : \n            trans_row [ min ( n_states , i + width_i // 2 + 1 ) : ] = 0 \n            trans_row [ : max ( 0 , i - width_i // 2 ) ] = 0 \n        transition [ i ] = trans_row \n    transition /= transition . sum ( axis = 1 , keepdims = True ) \n    return transition "}
{"2187": "\ndef onset_backtrack ( events , energy ) : \n    minima = np . flatnonzero ( ( energy [ : - 2 ] >= energy [ 1 : - 1 ] ) & ( energy [ 2 : ] > energy [ 1 : - 1 ] ) ) \n    minima = util . fix_frames ( 1 + minima , x_min = 0 ) \n    return minima [ util . match_events ( events , minima , right = False ) ] "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = False , center = True , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if 1 > lag or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if 1 > max_size or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048 ) \n    hop_length = kwargs . get ( 'hop_length' , 512 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if max_size == 1 : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) \n    elif ref . shape != S . shape : \n        raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = True \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = False \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = 0 ) \n    pad_width = lag \n    if center : \n        pad_width += n_fft // ( 2 * hop_length ) \n    onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - 1 ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ 1 ] ] \n    return onset_env "}
{"2190": "\ndef write_wav ( path , y , sr , norm = False ) : \n    util . valid_audio ( y , mono = False ) \n    if norm and np . issubdtype ( y . dtype , np . floating ) : \n        wav = util . normalize ( y , norm = np . inf , axis = None ) \n    else : \n        wav = y \n    if 1 < wav . ndim and wav . shape [ 0 ] == 2 : \n        wav = wav . T \n    scipy . io . wavfile . write ( path , sr , wav ) "}
{"2191": "\ndef cmap ( data , robust = True , cmap_seq = 'magma' , cmap_bool = 'gray_r' , cmap_div = 'coolwarm' ) : \n    data = np . atleast_1d ( data ) \n    if data . dtype == 'bool' : \n        return get_cmap ( cmap_bool ) \n    data = data [ np . isfinite ( data ) ] \n    if robust : \n        min_p , max_p = 2 , 98 \n    else : \n        min_p , max_p = 0 , 100 \n    max_val = np . percentile ( data , max_p ) \n    min_val = np . percentile ( data , min_p ) \n    if 0 <= min_val or 0 >= max_val : \n        return get_cmap ( cmap_seq ) \n    return get_cmap ( cmap_div ) "}
{"2192": "\ndef waveplot ( y , sr = 22050 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , max_sr = 1000 , ax = None , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( isinstance ( max_sr , int ) and 0 < max_sr ) : \n        raise ParameterError ( 'max_sr must be a non-negative integer' ) \n    target_sr = sr \n    hop_length = 1 \n    if max_points is not None : \n        if 0 >= max_points : \n            raise ParameterError ( 'max_points must be strictly positive' ) \n        if y . shape [ - 1 ] > max_points : \n            target_sr = min ( max_sr , ( sr * y . shape [ - 1 ] ) // max_points ) \n        hop_length = sr // target_sr \n        if y . ndim == 1 : \n            y = __envelope ( y , hop_length ) \n        else : \n            y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) \n    if 1 < y . ndim : \n        y_top = y [ 0 ] \n        y_bottom = - y [ 1 ] \n    else : \n        y_top = y \n        y_bottom = - y \n    axes = __check_axes ( ax ) \n    kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) \n    locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , sr = sr , hop_length = hop_length ) \n    out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) \n    axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) \n    if x_axis == 'time' : \n        axes . xaxis . set_major_formatter ( TimeFormatter ( lag = False ) ) \n        axes . xaxis . set_label_text ( 'Time' ) \n    elif x_axis is None or x_axis in [ 'off' , 'none' ] : \n        axes . set_xticks ( [ ] ) \n    else : \n        raise ParameterError ( 'Unknown x_axis value: {}' . format ( x_axis ) ) \n    return out "}
{"2194": "\ndef __mesh_coords ( ax_type , coords , n , ** kwargs ) : \n    if coords is not None : \n        if n > len ( coords ) : \n            raise ParameterError ( 'Coordinate shape mismatch: ' '{}<{}' . format ( len ( coords ) , n ) ) \n        return coords \n    coord_map = { 'linear' : __coord_fft_hz , 'hz' : __coord_fft_hz , 'log' : __coord_fft_hz , 'mel' : __coord_mel_hz , 'cqt' : __coord_cqt_hz , 'cqt_hz' : __coord_cqt_hz , 'cqt_note' : __coord_cqt_hz , 'chroma' : __coord_chroma , 'time' : __coord_time , 's' : __coord_time , 'ms' : __coord_time , 'lag' : __coord_time , 'lag_s' : __coord_time , 'lag_ms' : __coord_time , 'tonnetz' : __coord_n , 'off' : __coord_n , 'tempo' : __coord_tempo , 'frames' : __coord_n , None : __coord_n } \n    if ax_type not in coord_map : \n        raise ParameterError ( 'Unknown axis type: {}' . format ( ax_type ) ) \n    return coord_map [ ax_type ] ( n , ** kwargs ) "}
{"2202": "\ndef estimate_tuning ( y = None , sr = 22050 , S = None , n_fft = 2048 , resolution = 0.01 , bins_per_octave = 12 , ** kwargs ) : \n    pitch , mag = piptrack ( y = y , sr = sr , S = S , n_fft = n_fft , ** kwargs ) \n    pitch_mask = 0 < pitch \n    if pitch_mask . any ( ) : \n        threshold = np . median ( mag [ pitch_mask ] ) \n    else : \n        threshold = 0.0 \n    return pitch_tuning ( pitch [ ( threshold <= mag ) & pitch_mask ] , resolution = resolution , bins_per_octave = bins_per_octave ) "}
{"2203": "\ndef piptrack ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = None , fmin = 150.0 , fmax = 4000.0 , threshold = 0.1 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , ref = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    S = np . abs ( S ) \n    fmin = np . maximum ( fmin , 0 ) \n    fmax = np . minimum ( fmax , float ( sr ) / 2 ) \n    fft_freqs = time_frequency . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    avg = 0.5 * ( S [ 2 : ] - S [ : - 2 ] ) \n    shift = 2 * S [ 1 : - 1 ] - S [ 2 : ] - S [ : - 2 ] \n    shift = avg / ( shift + ( util . tiny ( shift ) > np . abs ( shift ) ) ) \n    avg = np . pad ( avg , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    shift = np . pad ( shift , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    dskew = 0.5 * avg * shift \n    pitches = np . zeros_like ( S ) \n    mags = np . zeros_like ( S ) \n    freq_mask = ( ( fft_freqs >= fmin ) & ( fmax > fft_freqs ) ) . reshape ( ( - 1 , 1 ) ) \n    if ref is None : \n        ref = np . max \n    if six . callable ( ref ) : \n        ref_value = threshold * ref ( S , axis = 0 ) \n    else : \n        ref_value = np . abs ( ref ) \n    idx = np . argwhere ( freq_mask & util . localmax ( S * ( ref_value < S ) ) ) \n    pitches [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( ( idx [ : , 0 ] + shift [ idx [ : , 0 ] , idx [ : , 1 ] ] ) * float ( sr ) / n_fft ) \n    mags [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( S [ idx [ : , 0 ] , idx [ : , 1 ] ] + dskew [ idx [ : , 0 ] , idx [ : , 1 ] ] ) \n    return pitches , mags "}
{"2207": "\ndef time_stretch ( y , rate ) : \n    if 0 >= rate : \n        raise ParameterError ( 'rate must be a positive number' ) \n    stft = core . stft ( y ) \n    stft_stretch = core . phase_vocoder ( stft , rate ) \n    y_stretch = core . istft ( stft_stretch , dtype = y . dtype ) \n    return y_stretch "}
{"2208": "\ndef pitch_shift ( y , sr , n_steps , bins_per_octave = 12 , res_type = 'kaiser_best' ) : \n    if 1 > bins_per_octave or not np . issubdtype ( type ( bins_per_octave ) , np . integer ) : \n        raise ParameterError ( 'bins_per_octave must be a positive integer.' ) \n    rate = 2.0 ** ( - float ( n_steps ) / bins_per_octave ) \n    y_shift = core . resample ( time_stretch ( y , rate ) , float ( sr ) / rate , sr , res_type = res_type ) \n    return util . fix_length ( y_shift , len ( y ) ) "}
{"2210": "\ndef _signal_to_frame_nonsilent ( y , frame_length = 2048 , hop_length = 512 , top_db = 60 , ref = np . max ) : \n    y_mono = core . to_mono ( y ) \n    mse = feature . rms ( y = y_mono , frame_length = frame_length , hop_length = hop_length ) ** 2 \n    return ( - top_db < core . power_to_db ( mse . squeeze ( ) , ref = ref , top_db = None ) ) "}
{"2211": "\ndef trim ( y , top_db = 60 , ref = np . max , frame_length = 2048 , hop_length = 512 ) : \n    non_silent = _signal_to_frame_nonsilent ( y , frame_length = frame_length , hop_length = hop_length , ref = ref , top_db = top_db ) \n    nonzero = np . flatnonzero ( non_silent ) \n    if 0 < nonzero . size : \n        start = int ( core . frames_to_samples ( nonzero [ 0 ] , hop_length ) ) \n        end = min ( y . shape [ - 1 ] , int ( core . frames_to_samples ( nonzero [ - 1 ] + 1 , hop_length ) ) ) \n    else : \n        start , end = 0 , 0 \n    full_index = [ slice ( None ) ] * y . ndim \n    full_index [ - 1 ] = slice ( start , end ) \n    return y [ tuple ( full_index ) ] , np . asarray ( [ start , end ] ) "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = False , norm = 1 , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2 \n    if norm is not None and norm != 1 and norm != np . inf : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    for i in range ( n_mels ) : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2 ] / fdiff [ i + 1 ] \n        weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) \n    if norm == 1 : \n        enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) \n        weights *= enorm [ : , np . newaxis ] \n    if not np . all ( ( mel_f [ : - 2 ] == 0 ) | ( 0 < weights . max ( axis = 1 ) ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2222": "\ndef __float_window ( window_spec ) : \n    def _wrap ( n , * args , ** kwargs ) : \n        n_min , n_max = int ( np . floor ( n ) ) , int ( np . ceil ( n ) ) \n        window = get_window ( window_spec , n_min ) \n        if n_max > len ( window ) : \n            window = np . pad ( window , [ ( 0 , n_max - len ( window ) ) ] , mode = 'constant' ) \n        window [ n_min : ] = 0.0 \n        return window \n    return _wrap "}
{"2224": "\ndef constant_q_lengths ( sr , fmin , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , window = 'hann' , filter_scale = 1 ) : \n    if 0 >= fmin : \n        raise ParameterError ( 'fmin must be positive' ) \n    if 0 >= bins_per_octave : \n        raise ParameterError ( 'bins_per_octave must be positive' ) \n    if 0 >= filter_scale : \n        raise ParameterError ( 'filter_scale must be positive' ) \n    if 0 >= n_bins or not isinstance ( n_bins , int ) : \n        raise ParameterError ( 'n_bins must be a positive integer' ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - 1 ) \n    freq = fmin * ( 2.0 ** ( np . arange ( n_bins , dtype = float ) / bins_per_octave ) ) \n    if sr / 2.0 < freq [ - 1 ] * ( 1 + 0.5 * window_bandwidth ( window ) / Q ) : \n        raise ParameterError ( 'Filter pass-band lies beyond Nyquist' ) \n    lengths = Q * sr / freq \n    return lengths "}
{"2233": "\ndef spectral_centroid ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , freq = None , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with real-valued input' ) \n    elif np . any ( 0 > S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    return np . sum ( freq * util . normalize ( S , norm = 1 , axis = 0 ) , axis = 0 , keepdims = True ) "}
{"2234": "\ndef spectral_rolloff ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , freq = None , roll_percent = 0.85 ) : \n    if not 0.0 < roll_percent < 1.0 : \n        raise ParameterError ( 'roll_percent must lie in the range (0, 1)' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with real-valued input' ) \n    elif np . any ( 0 > S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    total_energy = np . cumsum ( S , axis = 0 ) \n    threshold = roll_percent * total_energy [ - 1 ] \n    ind = np . where ( threshold > total_energy , np . nan , 1 ) \n    return np . nanmin ( ind * freq , axis = 0 , keepdims = True ) "}
{"2235": "\ndef spectral_flatness ( y = None , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , amin = 1e-10 , power = 2.0 ) : \n    if 0 >= amin : \n        raise ParameterError ( 'amin must be strictly positive' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 1. , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with real-valued input' ) \n    elif np . any ( 0 > S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with non-negative energies' ) \n    S_thresh = np . maximum ( amin , S ** power ) \n    gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = 0 , keepdims = True ) ) \n    amean = np . mean ( S_thresh , axis = 0 , keepdims = True ) \n    return gmean / amean "}
{"2239": "\ndef chroma_cqt ( y = None , sr = 22050 , C = None , hop_length = 512 , fmin = None , norm = np . inf , threshold = 0.0 , tuning = None , n_chroma = 12 , n_octaves = 7 , window = None , bins_per_octave = None , cqt_mode = 'full' ) : \n    cqt_func = { 'full' : cqt , 'hybrid' : hybrid_cqt } \n    if bins_per_octave is None : \n        bins_per_octave = n_chroma \n    if C is None : \n        C = np . abs ( cqt_func [ cqt_mode ] ( y , sr = sr , hop_length = hop_length , fmin = fmin , n_bins = n_octaves * bins_per_octave , bins_per_octave = bins_per_octave , tuning = tuning ) ) \n    cq_to_chr = filters . cq_to_chroma ( C . shape [ 0 ] , bins_per_octave = bins_per_octave , n_chroma = n_chroma , fmin = fmin , window = window ) \n    chroma = cq_to_chr . dot ( C ) \n    if threshold is not None : \n        chroma [ threshold > chroma ] = 0.0 \n    if norm is not None : \n        chroma = util . normalize ( chroma , norm = norm , axis = 0 ) \n    return chroma "}
{"2241": "\ndef __jaccard ( int_a , int_b ) : \n    ends = [ int_a [ 1 ] , int_b [ 1 ] ] \n    if ends [ 0 ] > ends [ 1 ] : \n        ends . reverse ( ) \n    starts = [ int_a [ 0 ] , int_b [ 0 ] ] \n    if starts [ 0 ] > starts [ 1 ] : \n        starts . reverse ( ) \n    intersection = ends [ 0 ] - starts [ 1 ] \n    if 0 > intersection : \n        intersection = 0. \n    union = ends [ 1 ] - starts [ 0 ] \n    if 0 < union : \n        return intersection / union \n    return 0.0 "}
{"2242": "\ndef __match_interval_overlaps ( query , intervals_to , candidates ) : \n    best_score = - 1 \n    best_idx = - 1 \n    for idx in candidates : \n        score = __jaccard ( query , intervals_to [ idx ] ) \n        if best_score < score : \n            best_score , best_idx = score , idx \n    return best_idx "}
{"2243": "\ndef __match_intervals ( intervals_from , intervals_to , strict = True ) : \n    start_index = np . argsort ( intervals_to [ : , 0 ] ) \n    end_index = np . argsort ( intervals_to [ : , 1 ] ) \n    start_sorted = intervals_to [ start_index , 0 ] \n    end_sorted = intervals_to [ end_index , 1 ] \n    search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) \n    search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) \n    output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) \n    for i in range ( len ( intervals_from ) ) : \n        query = intervals_from [ i ] \n        after_query = search_ends [ i ] \n        before_query = search_starts [ i ] \n        candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) \n        if 0 < len ( candidates ) : \n            output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) \n        elif strict : \n            raise ParameterError \n        else : \n            dist_before = np . inf \n            dist_after = np . inf \n            if 0 < search_starts [ i ] : \n                dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] \n            if len ( intervals_to ) > search_ends [ i ] + 1 : \n                dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] \n            if dist_after > dist_before : \n                output [ i ] = end_index [ search_starts [ i ] - 1 ] \n            else : \n                output [ i ] = start_index [ search_ends [ i ] + 1 ] \n    return output "}
{"2245": "\ndef match_events ( events_from , events_to , left = True , right = True ) : \n    if len ( events_from ) == 0 or len ( events_to ) == 0 : \n        raise ParameterError ( 'Attempting to match empty event list' ) \n    if not ( left or right ) and not np . all ( np . in1d ( events_from , events_to ) ) : \n        raise ParameterError ( 'Cannot match events with left=right=False ' 'and events_from is not contained ' 'in events_to' ) \n    if ( not left ) and max ( events_from ) > max ( events_to ) : \n        raise ParameterError ( 'Cannot match events with left=False ' 'and max(events_to) < max(events_from)' ) \n    if ( not right ) and min ( events_from ) < min ( events_to ) : \n        raise ParameterError ( 'Cannot match events with right=False ' 'and min(events_to) > min(events_from)' ) \n    output = np . empty_like ( events_from , dtype = np . int ) \n    return __match_events_helper ( output , events_from , events_to , left , right ) "}
{"2251": "\ndef __audioread_load ( path , offset , duration , dtype ) : \n    y = [ ] \n    with audioread . audio_open ( path ) as input_file : \n        sr_native = input_file . samplerate \n        n_channels = input_file . channels \n        s_start = int ( np . round ( sr_native * offset ) ) * n_channels \n        if duration is None : \n            s_end = np . inf \n        else : \n            s_end = s_start + ( int ( np . round ( sr_native * duration ) ) * n_channels ) \n        n = 0 \n        for frame in input_file : \n            frame = util . buf_to_float ( frame , dtype = dtype ) \n            n_prev = n \n            n = n + len ( frame ) \n            if s_start > n : \n                continue \n            if n_prev > s_end : \n                break \n            if n > s_end : \n                frame = frame [ : s_end - n_prev ] \n            if n_prev <= s_start <= n : \n                frame = frame [ ( s_start - n_prev ) : ] \n            y . append ( frame ) \n    if y : \n        y = np . concatenate ( y ) \n        if 1 < n_channels : \n            y = y . reshape ( ( - 1 , n_channels ) ) . T \n    else : \n        y = np . empty ( 0 , dtype = dtype ) \n    return y , sr_native "}
{"2252": "\ndef to_mono ( y ) : \n    util . valid_audio ( y , mono = False ) \n    if 1 < y . ndim : \n        y = np . mean ( y , axis = 0 ) \n    return y "}
{"2255": "\ndef lpc ( y , order ) : \n    if not isinstance ( order , int ) or 1 > order : \n        raise ParameterError ( \"order must be an integer > 0\" ) \n    util . valid_audio ( y , mono = True ) \n    return __lpc ( y , order ) "}
{"2256": "\ndef clicks ( times = None , frames = None , sr = 22050 , hop_length = 512 , click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : \n    if times is None : \n        if frames is None : \n            raise ParameterError ( 'either \"times\" or \"frames\" must be provided' ) \n        positions = frames_to_samples ( frames , hop_length = hop_length ) \n    else : \n        positions = time_to_samples ( times , sr = sr ) \n    if click is not None : \n        util . valid_audio ( click , mono = True ) \n    else : \n        if 0 >= click_duration : \n            raise ParameterError ( 'click_duration must be strictly positive' ) \n        if 0 >= click_freq : \n            raise ParameterError ( 'click_freq must be strictly positive' ) \n        angular_freq = 2 * np . pi * click_freq / float ( sr ) \n        click = np . logspace ( 0 , - 10 , num = int ( np . round ( sr * click_duration ) ) , base = 2.0 ) \n        click *= np . sin ( angular_freq * np . arange ( len ( click ) ) ) \n    if length is None : \n        length = positions . max ( ) + click . shape [ 0 ] \n    else : \n        if 1 > length : \n            raise ParameterError ( 'length must be a positive integer' ) \n        positions = positions [ length > positions ] \n    click_signal = np . zeros ( length , dtype = np . float32 ) \n    for start in positions : \n        end = start + click . shape [ 0 ] \n        if length <= end : \n            click_signal [ start : ] += click [ : length - start ] \n        else : \n            click_signal [ start : end ] += click \n    return click_signal "}
{"2264": "\ndef __beat_tracker ( onset_envelope , bpm , fft_res , tightness , trim ) : \n    if 0 >= bpm : \n        raise ParameterError ( 'bpm must be strictly positive' ) \n    period = round ( 60.0 * fft_res / bpm ) \n    localscore = __beat_local_score ( onset_envelope , period ) \n    backlink , cumscore = __beat_track_dp ( localscore , period , tightness ) \n    beats = [ __last_beat ( cumscore ) ] \n    while 0 <= backlink [ beats [ - 1 ] ] : \n        beats . append ( backlink [ beats [ - 1 ] ] ) \n    beats = np . array ( beats [ : : - 1 ] , dtype = int ) \n    beats = __trim_beats ( localscore , beats , trim ) \n    return beats "}
{"2266": "\ndef __beat_track_dp ( localscore , period , tightness ) : \n    backlink = np . zeros_like ( localscore , dtype = int ) \n    cumscore = np . zeros_like ( localscore ) \n    window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) \n    if 0 >= tightness : \n        raise ParameterError ( 'tightness must be strictly positive' ) \n    txwt = - tightness * ( np . log ( - window / period ) ** 2 ) \n    first_beat = True \n    for i , score_i in enumerate ( localscore ) : \n        z_pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) \n        candidates = txwt . copy ( ) \n        candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] \n        beat_location = np . argmax ( candidates ) \n        cumscore [ i ] = score_i + candidates [ beat_location ] \n        if first_beat and 0.01 * localscore . max ( ) > score_i : \n            backlink [ i ] = - 1 \n        else : \n            backlink [ i ] = window [ beat_location ] \n            first_beat = False \n        window = window + 1 \n    return backlink , cumscore "}
{"2267": "\ndef __last_beat ( cumscore ) : \n    maxes = util . localmax ( cumscore ) \n    med_score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) \n    return np . argwhere ( ( med_score < cumscore * maxes * 2 ) ) . max ( ) "}
{"2271": "\ndef subsegment ( data , frames , n_segments = 4 , axis = - 1 ) : \n    frames = util . fix_frames ( frames , x_min = 0 , x_max = data . shape [ axis ] , pad = True ) \n    if 1 > n_segments : \n        raise ParameterError ( 'n_segments must be a positive integer' ) \n    boundaries = [ ] \n    idx_slices = [ slice ( None ) ] * data . ndim \n    for seg_start , seg_end in zip ( frames [ : - 1 ] , frames [ 1 : ] ) : \n        idx_slices [ axis ] = slice ( seg_start , seg_end ) \n        boundaries . extend ( seg_start + agglomerative ( data [ tuple ( idx_slices ) ] , min ( seg_end - seg_start , n_segments ) , axis = axis ) ) \n    return np . ascontiguousarray ( boundaries ) "}
{"2273": "\ndef path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7 , zero_mean = False , clip = True , ** kwargs ) : \n    if min_ratio is None : \n        min_ratio = 1. / max_ratio \n    elif max_ratio < min_ratio : \n        raise ParameterError ( 'min_ratio={} cannot exceed max_ratio={}' . format ( min_ratio , max_ratio ) ) \n    R_smooth = None \n    for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2 ) : \n        kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) \n        if R_smooth is None : \n            R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) \n        else : \n            np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , out = R_smooth ) \n    if clip : \n        np . clip ( R_smooth , 0 , None , out = R_smooth ) \n    return R_smooth "}
{"2275": "\ndef frame ( y , frame_length = 2048 , hop_length = 512 ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'Input must be of type numpy.ndarray, ' 'given type(y)={}' . format ( type ( y ) ) ) \n    if y . ndim != 1 : \n        raise ParameterError ( 'Input must be one-dimensional, ' 'given y.ndim={}' . format ( y . ndim ) ) \n    if frame_length > len ( y ) : \n        raise ParameterError ( 'Buffer is too short (n={:d})' ' for frame_length={:d}' . format ( len ( y ) , frame_length ) ) \n    if 1 > hop_length : \n        raise ParameterError ( 'Invalid hop_length: {:d}' . format ( hop_length ) ) \n    if not y . flags [ 'C_CONTIGUOUS' ] : \n        raise ParameterError ( 'Input buffer must be contiguous.' ) \n    n_frames = 1 + int ( ( len ( y ) - frame_length ) / hop_length ) \n    y_frames = as_strided ( y , shape = ( frame_length , n_frames ) , strides = ( y . itemsize , hop_length * y . itemsize ) ) \n    return y_frames "}
{"2276": "\ndef valid_audio ( y , mono = True ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and y . ndim != 1 : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    elif 2 < y . ndim or y . ndim == 0 : \n        raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return True "}
{"2278": "\ndef fix_length ( data , size , axis = - 1 , ** kwargs ) : \n    kwargs . setdefault ( 'mode' , 'constant' ) \n    n = data . shape [ axis ] \n    if size < n : \n        slices = [ slice ( None ) ] * data . ndim \n        slices [ axis ] = slice ( 0 , size ) \n        return data [ tuple ( slices ) ] \n    elif size > n : \n        lengths = [ ( 0 , 0 ) ] * data . ndim \n        lengths [ axis ] = ( 0 , size - n ) \n        return np . pad ( data , lengths , ** kwargs ) \n    return data "}
{"2280": "\ndef normalize ( S , norm = np . inf , axis = 0 , threshold = None , fill = None ) : \n    if threshold is None : \n        threshold = tiny ( S ) \n    elif 0 >= threshold : \n        raise ParameterError ( 'threshold={} must be strictly ' 'positive' . format ( threshold ) ) \n    if fill not in [ None , False , True ] : \n        raise ParameterError ( 'fill={} must be None or boolean' . format ( fill ) ) \n    if not np . all ( np . isfinite ( S ) ) : \n        raise ParameterError ( 'Input must be finite' ) \n    mag = np . abs ( S ) . astype ( np . float ) \n    fill_norm = 1 \n    if norm == np . inf : \n        length = np . max ( mag , axis = axis , keepdims = True ) \n    elif norm == - np . inf : \n        length = np . min ( mag , axis = axis , keepdims = True ) \n    elif norm == 0 : \n        if fill is True : \n            raise ParameterError ( 'Cannot normalize with norm=0 and fill=True' ) \n        length = np . sum ( 0 < mag , axis = axis , keepdims = True , dtype = mag . dtype ) \n    elif np . issubdtype ( type ( norm ) , np . number ) and 0 < norm : \n        length = np . sum ( mag ** norm , axis = axis , keepdims = True ) ** ( 1. / norm ) \n        if axis is None : \n            fill_norm = mag . size ** ( - 1. / norm ) \n        else : \n            fill_norm = mag . shape [ axis ] ** ( - 1. / norm ) \n    elif norm is None : \n        return S \n    else : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    small_idx = threshold > length \n    Snorm = np . empty_like ( S ) \n    if fill is None : \n        length [ small_idx ] = 1.0 \n        Snorm [ : ] = S / length \n    elif fill : \n        length [ small_idx ] = np . nan \n        Snorm [ : ] = S / length \n        Snorm [ np . isnan ( Snorm ) ] = fill_norm \n    else : \n        length [ small_idx ] = np . inf \n        Snorm [ : ] = S / length \n    return Snorm "}
{"2281": "\ndef localmax ( x , axis = 0 ) : \n    paddings = [ ( 0 , 0 ) ] * x . ndim \n    paddings [ axis ] = ( 1 , 1 ) \n    x_pad = np . pad ( x , paddings , mode = 'edge' ) \n    inds1 = [ slice ( None ) ] * x . ndim \n    inds1 [ axis ] = slice ( 0 , - 2 ) \n    inds2 = [ slice ( None ) ] * x . ndim \n    inds2 [ axis ] = slice ( 2 , x_pad . shape [ axis ] ) \n    return ( x_pad [ tuple ( inds1 ) ] < x ) & ( x_pad [ tuple ( inds2 ) ] <= x ) "}
{"2282": "\ndef peak_pick ( x , pre_max , post_max , pre_avg , post_avg , delta , wait ) : \n    if 0 > pre_max : \n        raise ParameterError ( 'pre_max must be non-negative' ) \n    if 0 > pre_avg : \n        raise ParameterError ( 'pre_avg must be non-negative' ) \n    if 0 > delta : \n        raise ParameterError ( 'delta must be non-negative' ) \n    if 0 > wait : \n        raise ParameterError ( 'wait must be non-negative' ) \n    if 0 >= post_max : \n        raise ParameterError ( 'post_max must be positive' ) \n    if 0 >= post_avg : \n        raise ParameterError ( 'post_avg must be positive' ) \n    if x . ndim != 1 : \n        raise ParameterError ( 'input array must be one-dimensional' ) \n    pre_max = valid_int ( pre_max , cast = np . ceil ) \n    post_max = valid_int ( post_max , cast = np . ceil ) \n    pre_avg = valid_int ( pre_avg , cast = np . ceil ) \n    post_avg = valid_int ( post_avg , cast = np . ceil ) \n    wait = valid_int ( wait , cast = np . ceil ) \n    max_length = pre_max + post_max \n    max_origin = np . ceil ( 0.5 * ( pre_max - post_max ) ) \n    mov_max = scipy . ndimage . filters . maximum_filter1d ( x , int ( max_length ) , mode = 'constant' , origin = int ( max_origin ) , cval = x . min ( ) ) \n    avg_length = pre_avg + post_avg \n    avg_origin = np . ceil ( 0.5 * ( pre_avg - post_avg ) ) \n    mov_avg = scipy . ndimage . filters . uniform_filter1d ( x , int ( avg_length ) , mode = 'nearest' , origin = int ( avg_origin ) ) \n    n = 0 \n    while 0 > n - pre_avg and x . shape [ 0 ] > n : \n        start = n - pre_avg \n        start = start if 0 < start else 0 \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n += 1 \n    n = x . shape [ 0 ] - post_avg \n    n = n if 0 < n else 0 \n    while x . shape [ 0 ] > n : \n        start = n - pre_avg \n        start = start if 0 < start else 0 \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n += 1 \n    detections = x * ( x == mov_max ) \n    detections = detections * ( ( mov_avg + delta ) <= detections ) \n    peaks = [ ] \n    last_onset = - np . inf \n    for i in np . nonzero ( detections ) [ 0 ] : \n        if last_onset + wait < i : \n            peaks . append ( i ) \n            last_onset = i \n    return np . array ( peaks ) "}
{"2283": "\ndef sparsify_rows ( x , quantile = 0.01 ) : \n    if x . ndim == 1 : \n        x = x . reshape ( ( 1 , - 1 ) ) \n    elif 2 < x . ndim : \n        raise ParameterError ( 'Input must have 2 or fewer dimensions. ' 'Provided x.shape={}.' . format ( x . shape ) ) \n    if not 0.0 <= quantile < 1 : \n        raise ParameterError ( 'Invalid quantile {:.2f}' . format ( quantile ) ) \n    x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    mags = np . abs ( x ) \n    norms = np . sum ( mags , axis = 1 , keepdims = True ) \n    mag_sort = np . sort ( mags , axis = 1 ) \n    cumulative_mag = np . cumsum ( mag_sort / norms , axis = 1 ) \n    threshold_idx = np . argmin ( quantile > cumulative_mag , axis = 1 ) \n    for i , j in enumerate ( threshold_idx ) : \n        idx = np . where ( mag_sort [ i , j ] <= mags [ i ] ) \n        x_sparse [ i , idx ] = x [ i , idx ] \n    return x_sparse . tocsr ( ) "}
{"2288": "\ndef softmask ( X , X_ref , power = 1 , split_zeros = False ) : \n    if X . shape != X_ref . shape : \n        raise ParameterError ( 'Shape mismatch: {}!={}' . format ( X . shape , X_ref . shape ) ) \n    if np . any ( 0 > X ) or np . any ( 0 > X_ref ) : \n        raise ParameterError ( 'X and X_ref must be non-negative' ) \n    if 0 >= power : \n        raise ParameterError ( 'power must be strictly positive' ) \n    dtype = X . dtype \n    if not np . issubdtype ( dtype , np . floating ) : \n        dtype = np . float32 \n    Z = np . maximum ( X , X_ref ) . astype ( dtype ) \n    bad_idx = ( np . finfo ( dtype ) . tiny > Z ) \n    Z [ bad_idx ] = 1 \n    if np . isfinite ( power ) : \n        mask = ( X / Z ) ** power \n        ref_mask = ( X_ref / Z ) ** power \n        good_idx = ~ bad_idx \n        mask [ good_idx ] /= mask [ good_idx ] + ref_mask [ good_idx ] \n        if split_zeros : \n            mask [ bad_idx ] = 0.5 \n        else : \n            mask [ bad_idx ] = 0.0 \n    else : \n        mask = X_ref < X \n    return mask "}
{"2292": "\ndef get_frame ( self , frame_id ) : \n    if 0 > frame_id or self . _frame_cnt <= frame_id : \n        raise IndexError ( '\"frame_id\" must be between 0 and {}' . format ( self . _frame_cnt - 1 ) ) \n    if frame_id == self . _position : \n        return self . read ( ) \n    if self . _cache : \n        img = self . _cache . get ( frame_id ) \n        if img is not None : \n            self . _position = frame_id + 1 \n            return img \n    self . _set_real_position ( frame_id ) \n    ret , img = self . _vcap . read ( ) \n    if ret : \n        if self . _cache : \n            self . _cache . put ( self . _position , img ) \n        self . _position += 1 \n    return img "}
{"2293": "\ndef cvt2frames ( self , frame_dir , file_start = 0 , filename_tmpl = '{:06d}.jpg' , start = 0 , max_num = 0 , show_progress = True ) : \n    mkdir_or_exist ( frame_dir ) \n    if max_num == 0 : \n        task_num = self . frame_cnt - start \n    else : \n        task_num = min ( self . frame_cnt - start , max_num ) \n    if 0 >= task_num : \n        raise ValueError ( 'start must be less than total frame number' ) \n    if 0 < start : \n        self . _set_real_position ( start ) \n    def write_frame ( file_idx ) : \n        img = self . read ( ) \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        cv2 . imwrite ( filename , img ) \n    if show_progress : \n        track_progress ( write_frame , range ( file_start , file_start + task_num ) ) \n    else : \n        for i in range ( task_num ) : \n            img = self . read ( ) \n            if img is None : \n                break \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i + file_start ) ) \n            cv2 . imwrite ( filename , img ) "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50 , chunksize = 1 , skip_first = False , keep_order = True ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num -= nproc * chunksize * int ( skip_first ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if nproc * chunksize > len ( results ) : \n                continue \n            elif len ( results ) == nproc * chunksize : \n                prog_bar . start ( ) \n                continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2300": "\ndef imcrop ( img , bboxes , scale = 1.0 , pad_fill = None ) : \n    chn = 1 if img . ndim == 2 else img . shape [ 2 ] \n    if pad_fill is not None : \n        if isinstance ( pad_fill , ( int , float ) ) : \n            pad_fill = [ pad_fill for _ in range ( chn ) ] \n        assert len ( pad_fill ) == chn \n    _bboxes = bboxes [ None , ... ] if bboxes . ndim == 1 else bboxes \n    scaled_bboxes = bbox_scaling ( _bboxes , scale ) . astype ( np . int32 ) \n    clipped_bbox = bbox_clip ( scaled_bboxes , img . shape ) \n    patches = [ ] \n    for i in range ( clipped_bbox . shape [ 0 ] ) : \n        x1 , y1 , x2 , y2 = tuple ( clipped_bbox [ i , : ] ) \n        if pad_fill is None : \n            patch = img [ y1 : y2 + 1 , x1 : x2 + 1 , ... ] \n        else : \n            _x1 , _y1 , _x2 , _y2 = tuple ( scaled_bboxes [ i , : ] ) \n            if chn == 2 : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 ) \n            else : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 , chn ) \n            patch = np . array ( pad_fill , dtype = img . dtype ) * np . ones ( patch_shape , dtype = img . dtype ) \n            x_start = 0 if 0 <= _x1 else - _x1 \n            y_start = 0 if 0 <= _y1 else - _y1 \n            w = x2 - x1 + 1 \n            h = y2 - y1 + 1 \n            patch [ y_start : y_start + h , x_start : x_start + w , ... ] = img [ y1 : y1 + h , x1 : x1 + w , ... ] \n        patches . append ( patch ) \n    if bboxes . ndim == 1 : \n        return patches [ 0 ] \n    else : \n        return patches "}
{"2301": "\ndef impad ( img , shape , pad_val = 0 ) : \n    if not isinstance ( pad_val , ( int , float ) ) : \n        assert len ( pad_val ) == img . shape [ - 1 ] \n    if len ( img . shape ) > len ( shape ) : \n        shape = shape + ( img . shape [ - 1 ] , ) \n    assert len ( shape ) == len ( img . shape ) \n    for i in range ( len ( shape ) - 1 ) : \n        assert img . shape [ i ] <= shape [ i ] \n    pad = np . empty ( shape , dtype = img . dtype ) \n    pad [ ... ] = pad_val \n    pad [ : img . shape [ 0 ] , : img . shape [ 1 ] , ... ] = img \n    return pad "}
{"2306": "\ndef imrescale ( img , scale , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    if isinstance ( scale , ( float , int ) ) : \n        if 0 >= scale : \n            raise ValueError ( 'Invalid scale {}, must be positive.' . format ( scale ) ) \n        scale_factor = scale \n    elif isinstance ( scale , tuple ) : \n        max_long_edge = max ( scale ) \n        max_short_edge = min ( scale ) \n        scale_factor = min ( max_long_edge / max ( h , w ) , max_short_edge / min ( h , w ) ) \n    else : \n        raise TypeError ( 'Scale must be a number or tuple of int, but got {}' . format ( type ( scale ) ) ) \n    new_size = _scale_size ( ( w , h ) , scale_factor ) \n    rescaled_img = imresize ( img , new_size , interpolation = interpolation ) \n    if return_scale : \n        return rescaled_img , scale_factor \n    else : \n        return rescaled_img "}
{"2308": "\ndef get_priority ( priority ) : \n    if isinstance ( priority , int ) : \n        if 0 > priority or 100 < priority : \n            raise ValueError ( 'priority must be between 0 and 100' ) \n        return priority \n    elif isinstance ( priority , Priority ) : \n        return priority . value \n    elif isinstance ( priority , str ) : \n        return Priority [ priority . upper ( ) ] . value \n    else : \n        raise TypeError ( 'priority must be an integer or Priority enum value' ) "}
{"2309": "\ndef dequantize ( arr , min_val , max_val , levels , dtype = np . float64 ) : \n    if not ( isinstance ( levels , int ) and 1 < levels ) : \n        raise ValueError ( 'levels must be a positive integer, but got {}' . format ( levels ) ) \n    if max_val <= min_val : \n        raise ValueError ( 'min_val ({}) must be smaller than max_val ({})' . format ( min_val , max_val ) ) \n    dequantized_arr = ( arr + 0.5 ) . astype ( dtype ) * ( max_val - min_val ) / levels + min_val \n    return dequantized_arr "}
{"2311": "\ndef imshow_bboxes ( img , bboxes , colors = 'green' , top_k = - 1 , thickness = 1 , show = True , win_name = '' , wait_time = 0 , out_file = None ) : \n    img = imread ( img ) \n    if isinstance ( bboxes , np . ndarray ) : \n        bboxes = [ bboxes ] \n    if not isinstance ( colors , list ) : \n        colors = [ colors for _ in range ( len ( bboxes ) ) ] \n    colors = [ color_val ( c ) for c in colors ] \n    assert len ( bboxes ) == len ( colors ) \n    for i , _bboxes in enumerate ( bboxes ) : \n        _bboxes = _bboxes . astype ( np . int32 ) \n        if 0 >= top_k : \n            _top_k = _bboxes . shape [ 0 ] \n        else : \n            _top_k = min ( top_k , _bboxes . shape [ 0 ] ) \n        for j in range ( _top_k ) : \n            left_top = ( _bboxes [ j , 0 ] , _bboxes [ j , 1 ] ) \n            right_bottom = ( _bboxes [ j , 2 ] , _bboxes [ j , 3 ] ) \n            cv2 . rectangle ( img , left_top , right_bottom , colors [ i ] , thickness = thickness ) \n    if show : \n        imshow ( img , win_name , wait_time ) \n    if out_file is not None : \n        imwrite ( img , out_file ) "}
{"2322": "\ndef register_hook ( self , hook , priority = 'NORMAL' ) : \n    assert isinstance ( hook , Hook ) \n    if hasattr ( hook , 'priority' ) : \n        raise ValueError ( '\"priority\" is a reserved attribute for hooks' ) \n    priority = get_priority ( priority ) \n    hook . priority = priority \n    inserted = False \n    for i in range ( len ( self . _hooks ) - 1 , - 1 , - 1 ) : \n        if self . _hooks [ i ] . priority <= priority : \n            self . _hooks . insert ( i + 1 , hook ) \n            inserted = True \n            break \n    if not inserted : \n        self . _hooks . insert ( 0 , hook ) "}
{"2323": "\ndef run ( self , data_loaders , workflow , max_epochs , ** kwargs ) : \n    assert isinstance ( data_loaders , list ) \n    assert mmcv . is_list_of ( workflow , tuple ) \n    assert len ( data_loaders ) == len ( workflow ) \n    self . _max_epochs = max_epochs \n    work_dir = self . work_dir if self . work_dir is not None else 'NONE' \n    self . logger . info ( 'Start running, host: %s, work_dir: %s' , get_host_info ( ) , work_dir ) \n    self . logger . info ( 'workflow: %s, max: %d epochs' , workflow , max_epochs ) \n    self . call_hook ( 'before_run' ) \n    while max_epochs > self . epoch : \n        for i , flow in enumerate ( workflow ) : \n            mode , epochs = flow \n            if isinstance ( mode , str ) : \n                if not hasattr ( self , mode ) : \n                    raise ValueError ( 'runner has no method named \"{}\" to run an epoch' . format ( mode ) ) \n                epoch_runner = getattr ( self , mode ) \n            elif callable ( mode ) : \n                epoch_runner = mode \n            else : \n                raise TypeError ( 'mode in workflow must be a str or ' 'callable function, not {}' . format ( type ( mode ) ) ) \n            for _ in range ( epochs ) : \n                if mode == 'train' and max_epochs <= self . epoch : \n                    return \n                epoch_runner ( data_loaders [ i ] , ** kwargs ) \n    time . sleep ( 1 ) \n    self . call_hook ( 'after_run' ) "}
{"2329": "\ndef list_from_file ( filename , prefix = '' , offset = 0 , max_num = 0 ) : \n    cnt = 0 \n    item_list = [ ] \n    with open ( filename , 'r' ) as f : \n        for _ in range ( offset ) : \n            f . readline ( ) \n        for line in f : \n            if 0 < max_num and max_num <= cnt : \n                break \n            item_list . append ( prefix + line . rstrip ( '\\n' ) ) \n            cnt += 1 \n    return item_list "}
{"2330": "\ndef dict_from_file ( filename , key_type = str ) : \n    mapping = { } \n    with open ( filename , 'r' ) as f : \n        for line in f : \n            items = line . rstrip ( '\\n' ) . split ( ) \n            assert 2 <= len ( items ) \n            key = key_type ( items [ 0 ] ) \n            val = items [ 1 : ] if 2 < len ( items ) else items [ 1 ] \n            mapping [ key ] = val \n    return mapping "}
{"2342": "\ndef average ( self , n = 0 ) : \n    assert 0 <= n \n    for key in self . val_history : \n        values = np . array ( self . val_history [ key ] [ - n : ] ) \n        nums = np . array ( self . n_history [ key ] [ - n : ] ) \n        avg = np . sum ( values * nums ) / np . sum ( nums ) \n        self . output [ key ] = avg \n    self . ready = True "}
{"2343": "\ndef scatter ( input , devices , streams = None ) : \n    if streams is None : \n        streams = [ None ] * len ( devices ) \n    if isinstance ( input , list ) : \n        chunk_size = ( len ( input ) - 1 ) // len ( devices ) + 1 \n        outputs = [ scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) ] \n        return outputs \n    elif isinstance ( input , torch . Tensor ) : \n        output = input . contiguous ( ) \n        stream = streams [ 0 ] if 0 < output . numel ( ) else None \n        with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : \n            output = output . cuda ( devices [ 0 ] , non_blocking = True ) \n        return output \n    else : \n        raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) ) "}
{"2344": "\ndef color_val ( color ) : \n    if is_str ( color ) : \n        return Color [ color ] . value \n    elif isinstance ( color , Color ) : \n        return color . value \n    elif isinstance ( color , tuple ) : \n        assert len ( color ) == 3 \n        for channel in color : \n            assert 0 <= channel and 255 >= channel \n        return color \n    elif isinstance ( color , int ) : \n        assert 0 <= color and 255 >= color \n        return color , color , color \n    elif isinstance ( color , np . ndarray ) : \n        assert color . ndim == 1 and color . size == 3 \n        assert np . all ( ( 0 <= color ) & ( 255 >= color ) ) \n        color = color . astype ( np . uint8 ) \n        return tuple ( color ) \n    else : \n        raise TypeError ( 'Invalid type for color: {}' . format ( type ( color ) ) ) "}
{"2350": "\ndef flow2rgb ( flow , color_wheel = None , unknown_thr = 1e6 ) : \n    assert flow . ndim == 3 and flow . shape [ - 1 ] == 2 \n    if color_wheel is None : \n        color_wheel = make_color_wheel ( ) \n    assert color_wheel . ndim == 2 and color_wheel . shape [ 1 ] == 3 \n    num_bins = color_wheel . shape [ 0 ] \n    dx = flow [ : , : , 0 ] . copy ( ) \n    dy = flow [ : , : , 1 ] . copy ( ) \n    ignore_inds = ( np . isnan ( dx ) | np . isnan ( dy ) | ( unknown_thr < np . abs ( dx ) ) | ( unknown_thr < np . abs ( dy ) ) ) \n    dx [ ignore_inds ] = 0 \n    dy [ ignore_inds ] = 0 \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    if np . any ( np . finfo ( float ) . eps < rad ) : \n        max_rad = np . max ( rad ) \n        dx /= max_rad \n        dy /= max_rad \n    [ h , w ] = dx . shape \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    angle = np . arctan2 ( - dy , - dx ) / np . pi \n    bin_real = ( angle + 1 ) / 2 * ( num_bins - 1 ) \n    bin_left = np . floor ( bin_real ) . astype ( int ) \n    bin_right = ( bin_left + 1 ) % num_bins \n    w = ( bin_real - bin_left . astype ( np . float32 ) ) [ ... , None ] \n    flow_img = ( 1 - w ) * color_wheel [ bin_left , : ] + w * color_wheel [ bin_right , : ] \n    small_ind = 1 >= rad \n    flow_img [ small_ind ] = 1 - rad [ small_ind , None ] * ( 1 - flow_img [ small_ind ] ) \n    flow_img [ np . logical_not ( small_ind ) ] *= 0.75 \n    flow_img [ ignore_inds , : ] = 0 \n    return flow_img "}
{"2353": "\ndef scatter ( inputs , target_gpus , dim = 0 ) : \n    def scatter_map ( obj ) : \n        if isinstance ( obj , torch . Tensor ) : \n            return OrigScatter . apply ( target_gpus , None , dim , obj ) \n        if isinstance ( obj , DataContainer ) : \n            if obj . cpu_only : \n                return obj . data \n            else : \n                return Scatter . forward ( target_gpus , obj . data ) \n        if isinstance ( obj , tuple ) and 0 < len ( obj ) : \n            return list ( zip ( * map ( scatter_map , obj ) ) ) \n        if isinstance ( obj , list ) and 0 < len ( obj ) : \n            out = list ( map ( list , zip ( * map ( scatter_map , obj ) ) ) ) \n            return out \n        if isinstance ( obj , dict ) and 0 < len ( obj ) : \n            out = list ( map ( type ( obj ) , zip ( * map ( scatter_map , obj . items ( ) ) ) ) ) \n            return out \n        return [ obj for targets in target_gpus ] \n    try : \n        return scatter_map ( inputs ) \n    finally : \n        scatter_map = None "}
{"2354": "\ndef scatter_kwargs ( inputs , kwargs , target_gpus , dim = 0 ) : \n    inputs = scatter ( inputs , target_gpus , dim ) if inputs else [ ] \n    kwargs = scatter ( kwargs , target_gpus , dim ) if kwargs else [ ] \n    if len ( kwargs ) > len ( inputs ) : \n        inputs . extend ( [ ( ) for _ in range ( len ( kwargs ) - len ( inputs ) ) ] ) \n    elif len ( inputs ) > len ( kwargs ) : \n        kwargs . extend ( [ { } for _ in range ( len ( inputs ) - len ( kwargs ) ) ] ) \n    inputs = tuple ( inputs ) \n    kwargs = tuple ( kwargs ) \n    return inputs , kwargs "}
{"2355": "\nasync def fetch ( self ) -> Response : \n    if 0 < self . request_config . get ( 'DELAY' , 0 ) : \n        await asyncio . sleep ( self . request_config [ 'DELAY' ] ) \n    timeout = self . request_config . get ( 'TIMEOUT' , 10 ) \n    try : \n        async with async_timeout . timeout ( timeout ) : \n            resp = await self . _make_request ( ) \n        try : \n            resp_data = await resp . text ( encoding = self . encoding ) \n        except UnicodeDecodeError : \n            resp_data = await resp . read ( ) \n        response = Response ( url = self . url , method = self . method , encoding = resp . get_encoding ( ) , html = resp_data , metadata = self . metadata , cookies = resp . cookies , headers = resp . headers , history = resp . history , status = resp . status , aws_json = resp . json , aws_text = resp . text , aws_read = resp . read ) \n        aws_valid_response = self . request_config . get ( 'VALID' ) \n        if aws_valid_response and iscoroutinefunction ( aws_valid_response ) : \n            response = await aws_valid_response ( response ) \n        if response . ok : \n            return response \n        else : \n            return await self . _retry ( error_msg = 'request url failed!' ) \n    except asyncio . TimeoutError : \n        return await self . _retry ( error_msg = 'timeout' ) \n    except Exception as e : \n        return await self . _retry ( error_msg = e ) \n    finally : \n        await self . _close_request_session ( ) "}
{"2363": "\ndef parse_yaml_linenumbers ( data , filename ) : \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + 1 \n        return node \n    def construct_mapping ( node , deep = False ) : \n        if 2 > ANSIBLE_VERSION : \n            mapping = Constructor . construct_mapping ( loader , node , deep = deep ) \n        else : \n            mapping = AnsibleConstructor . construct_mapping ( loader , node , deep = deep ) \n        if hasattr ( node , '__line__' ) : \n            mapping [ LINE_NUMBER_KEY ] = node . __line__ \n        else : \n            mapping [ LINE_NUMBER_KEY ] = mapping . _line_number \n        mapping [ FILENAME_KEY ] = filename \n        return mapping \n    try : \n        if 2 > ANSIBLE_VERSION : \n            loader = yaml . Loader ( data ) \n        else : \n            import inspect \n            kwargs = { } \n            if 'vault_password' in inspect . getargspec ( AnsibleLoader . __init__ ) . args : \n                kwargs [ 'vault_password' ] = DEFAULT_VAULT_PASSWORD \n            loader = AnsibleLoader ( data , ** kwargs ) \n        loader . compose_node = compose_node \n        loader . construct_mapping = construct_mapping \n        data = loader . get_single_data ( ) \n    except ( yaml . parser . ParserError , yaml . scanner . ScannerError ) as e : \n        raise SystemExit ( \"Failed to parse YAML in %s: %s\" % ( filename , str ( e ) ) ) \n    return data "}
{"2375": "\nasync def read ( self , keys : List [ str ] ) -> dict : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        if 0 < len ( keys ) : \n            parameters = [ { 'name' : f'@id{i}' , 'value' : f'{self.__sanitize_key(key)}' } for i , key in enumerate ( keys ) ] \n            parameter_sequence = ',' . join ( param . get ( 'name' ) for param in parameters ) \n            query = { \"query\" : f\"SELECT c.id, c.realId, c.document, c._etag \\FROM c WHERE c.id in ({parameter_sequence})\" , \"parameters\" : parameters } \n            options = { 'enableCrossPartitionQuery' : True } \n            results = list ( self . client . QueryItems ( self . __container_link , query , options ) ) \n            return { r . get ( 'realId' ) : self . __create_si ( r ) for r in results } \n        else : \n            raise Exception ( 'cosmosdb_storage.read(): \\provide at least one key' ) \n    except TypeError as e : \n        raise e "}
{"2376": "\nasync def write ( self , changes : Dict [ str , StoreItem ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for ( key , change ) in changes . items ( ) : \n            e_tag = change . e_tag \n            doc = { 'id' : self . __sanitize_key ( key ) , 'realId' : key , 'document' : self . __create_dict ( change ) } \n            if ( e_tag == '*' or not e_tag ) : \n                self . client . UpsertItem ( database_or_Container_link = self . __container_link , document = doc , options = { 'disableAutomaticIdGeneration' : True } ) \n            elif ( 0 < len ( e_tag ) ) : \n                access_condition = { 'type' : 'IfMatch' , 'condition' : e_tag } \n                self . client . ReplaceItem ( document_link = self . __item_link ( self . __sanitize_key ( key ) ) , new_document = doc , options = { 'accessCondition' : access_condition } ) \n            else : \n                raise Exception ( 'cosmosdb_storage.write(): etag missing' ) \n    except Exception as e : \n        raise e "}
{"2382": "\ndef __get_or_create_database ( self , doc_client , id ) -> str : \n    dbs = list ( doc_client . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : id } ] } ) ) \n    if 0 < len ( dbs ) : \n        return dbs [ 0 ] [ 'id' ] \n    else : \n        res = doc_client . CreateDatabase ( { 'id' : id } ) \n        return res [ 'id' ] "}
{"2383": "\ndef __get_or_create_container ( self , doc_client , container ) -> str : \n    containers = list ( doc_client . QueryContainers ( self . __database_link , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : container } ] } ) ) \n    if 0 < len ( containers ) : \n        return containers [ 0 ] [ 'id' ] \n    else : \n        res = doc_client . CreateContainer ( self . __database_link , { 'id' : container } ) \n        return res [ 'id' ] "}
{"2384": "\ndef fill_qna_event ( self , query_results : [ QueryResult ] , turn_context : TurnContext , telemetry_properties : Dict [ str , str ] = None , telemetry_metrics : Dict [ str , float ] = None ) -> EventData : \n    properties : Dict [ str , str ] = dict ( ) \n    metrics : Dict [ str , float ] = dict ( ) \n    properties [ QnATelemetryConstants . knowledge_base_id_property ] = self . _endpoint . knowledge_base_id \n    text : str = turn_context . activity . text \n    userName : str = turn_context . activity . from_property . name \n    if self . log_personal_information : \n        if text : \n            properties [ QnATelemetryConstants . question_property ] = text \n        if userName : \n            properties [ QnATelemetryConstants . username_property ] = userName \n    if 0 < len ( query_results ) : \n        query_result = query_results [ 0 ] \n        result_properties = { QnATelemetryConstants . matched_question_property : json . dumps ( query_result . questions ) , QnATelemetryConstants . question_id_property : str ( query_result . id ) , QnATelemetryConstants . answer_property : query_result . answer , QnATelemetryConstants . score_metric : query_result . score , QnATelemetryConstants . article_found_property : 'true' } \n        properties . update ( result_properties ) \n    else : \n        no_match_properties = { QnATelemetryConstants . matched_question_property : 'No Qna Question matched' , QnATelemetryConstants . question_id_property : 'No Qna Question Id matched' , QnATelemetryConstants . answer_property : 'No Qna Answer matched' , QnATelemetryConstants . article_found_property : 'false' } \n        properties . update ( no_match_properties ) \n    if telemetry_properties : \n        properties . update ( telemetry_properties ) \n    if telemetry_metrics : \n        metrics . update ( telemetry_metrics ) \n    return EventData ( properties = properties , metrics = metrics ) "}
{"2387": "\ndef supports_suggested_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : \n    max_actions = { Channels . facebook : 10 , Channels . skype : 10 , Channels . line : 13 , Channels . kik : 20 , Channels . telegram : 100 , Channels . slack : 100 , Channels . emulator : 100 , Channels . direct_line : 100 , Channels . webchat : 100 , } \n    return max_actions [ channel_id ] >= button_cnt if channel_id in max_actions else False "}
{"2388": "\ndef supports_card_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : \n    max_actions = { Channels . facebook : 3 , Channels . skype : 3 , Channels . ms_teams : 3 , Channels . line : 99 , Channels . slack : 100 , Channels . emulator : 100 , Channels . direct_line : 100 , Channels . webchat : 100 , Channels . cortana : 100 , } \n    return max_actions [ channel_id ] >= button_cnt if channel_id in max_actions else False "}
{"2395": "\ndef c_if ( self , classical , val ) : \n    if not isinstance ( classical , ClassicalRegister ) : \n        raise QiskitError ( \"c_if must be used with a classical register\" ) \n    if 0 > val : \n        raise QiskitError ( \"control value should be non-negative\" ) \n    self . control = ( classical , val ) \n    return self "}
{"2409": "\ndef _einsum_matmul_index_helper ( gate_indices , number_of_qubits ) : \n    if 26 < len ( gate_indices ) + number_of_qubits : \n        raise QiskitError ( \"Total number of free indexes limited to 26\" ) \n    tens_in = ascii_lowercase [ : number_of_qubits ] \n    tens_out = list ( tens_in ) \n    mat_left = \"\" \n    mat_right = \"\" \n    for pos , idx in enumerate ( reversed ( gate_indices ) ) : \n        mat_left += ascii_lowercase [ - 1 - pos ] \n        mat_right += tens_in [ - 1 - idx ] \n        tens_out [ - 1 - idx ] = ascii_lowercase [ - 1 - pos ] \n    tens_out = \"\" . join ( tens_out ) \n    return mat_left , mat_right , tens_in , tens_out "}
{"2418": "\ndef yzy_to_zyz ( xi , theta1 , theta2 , eps = 1e-9 ) : \n    quaternion_yzy = quaternion_from_euler ( [ theta1 , xi , theta2 ] , 'yzy' ) \n    euler = quaternion_yzy . to_zyz ( ) \n    quaternion_zyz = quaternion_from_euler ( euler , 'zyz' ) \n    out_angles = ( euler [ 1 ] , euler [ 0 ] , euler [ 2 ] ) \n    abs_inner = abs ( quaternion_zyz . data . dot ( quaternion_yzy . data ) ) \n    if not np . allclose ( abs_inner , 1 , eps ) : \n        raise TranspilerError ( 'YZY and ZYZ angles do not give same rotation matrix.' ) \n    out_angles = tuple ( 0 if _CHOP_THRESHOLD > np . abs ( angle ) else angle for angle in out_angles ) \n    return out_angles "}
{"2421": "\ndef _get_gate_span ( qregs , instruction ) : \n    min_index = len ( qregs ) \n    max_index = 0 \n    for qreg in instruction . qargs : \n        index = qregs . index ( qreg ) \n        if min_index > index : \n            min_index = index \n        if max_index < index : \n            max_index = index \n    if instruction . cargs : \n        return qregs [ min_index : ] \n    return qregs [ min_index : max_index + 1 ] "}
{"2422": "\ndef circuit_to_instruction ( circuit ) : \n    instruction = Instruction ( name = circuit . name , num_qubits = sum ( [ qreg . size for qreg in circuit . qregs ] ) , num_clbits = sum ( [ creg . size for creg in circuit . cregs ] ) , params = [ ] ) \n    instruction . control = None \n    def find_bit_position ( bit ) : \n        if isinstance ( bit [ 0 ] , QuantumRegister ) : \n            ordered_regs = circuit . qregs \n        else : \n            ordered_regs = circuit . cregs \n        reg_index = ordered_regs . index ( bit [ 0 ] ) \n        return sum ( [ reg . size for reg in ordered_regs [ : reg_index ] ] ) + bit [ 1 ] \n    definition = circuit . data . copy ( ) \n    if 0 < instruction . num_qubits : \n        q = QuantumRegister ( instruction . num_qubits , 'q' ) \n    if 0 < instruction . num_clbits : \n        c = ClassicalRegister ( instruction . num_clbits , 'c' ) \n    definition = list ( map ( lambda x : ( x [ 0 ] , list ( map ( lambda y : ( q , find_bit_position ( y ) ) , x [ 1 ] ) ) , list ( map ( lambda y : ( c , find_bit_position ( y ) ) , x [ 2 ] ) ) ) , definition ) ) \n    instruction . definition = definition \n    return instruction "}
{"2423": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if self . coupling_map . size ( ) < num_dag_qubits : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    best_sub = self . _best_subset ( num_dag_qubits ) \n    layout = Layout ( ) \n    map_iter = 0 \n    for qreg in dag . qregs . values ( ) : \n        for i in range ( qreg . size ) : \n            layout [ ( qreg , i ) ] = int ( best_sub [ map_iter ] ) \n            map_iter += 1 \n    self . property_set [ 'layout' ] = layout "}
{"2424": "\ndef _best_subset ( self , n_qubits ) : \n    if n_qubits == 1 : \n        return np . array ( [ 0 ] ) \n    device_qubits = self . coupling_map . size ( ) \n    cmap = np . asarray ( self . coupling_map . get_edges ( ) ) \n    data = np . ones_like ( cmap [ : , 0 ] ) \n    sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , 0 ] , cmap [ : , 1 ] ) ) , shape = ( device_qubits , device_qubits ) ) . tocsr ( ) \n    best = 0 \n    best_map = None \n    for k in range ( sp_cmap . shape [ 0 ] ) : \n        bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = False , return_predecessors = False ) \n        connection_count = 0 \n        sub_graph = [ ] \n        for i in range ( n_qubits ) : \n            node_idx = bfs [ i ] \n            for j in range ( sp_cmap . indptr [ node_idx ] , sp_cmap . indptr [ node_idx + 1 ] ) : \n                node = sp_cmap . indices [ j ] \n                for counter in range ( n_qubits ) : \n                    if node == bfs [ counter ] : \n                        connection_count += 1 \n                        sub_graph . append ( [ node_idx , node ] ) \n                        break \n        if best < connection_count : \n            best = connection_count \n            best_map = bfs [ 0 : n_qubits ] \n            mapping = { } \n            for edge in range ( best_map . shape [ 0 ] ) : \n                mapping [ best_map [ edge ] ] = edge \n            new_cmap = [ [ mapping [ c [ 0 ] ] , mapping [ c [ 1 ] ] ] for c in sub_graph ] \n            rows = [ edge [ 0 ] for edge in new_cmap ] \n            cols = [ edge [ 1 ] for edge in new_cmap ] \n            data = [ 1 ] * len ( rows ) \n            sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , shape = ( n_qubits , n_qubits ) ) . tocsr ( ) \n            perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) \n            best_map = best_map [ perm ] \n    return best_map "}
{"2428": "\ndef _process_custom_unitary ( self , node ) : \n    name = node . name \n    if node . arguments is not None : \n        args = self . _process_node ( node . arguments ) \n    else : \n        args = [ ] \n    bits = [ self . _process_bit_id ( node_element ) for node_element in node . bitlist . children ] \n    if name in self . gates : \n        gargs = self . gates [ name ] [ \"args\" ] \n        gbits = self . gates [ name ] [ \"bits\" ] \n        maxidx = max ( map ( len , bits ) ) \n        for idx in range ( maxidx ) : \n            self . arg_stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) \n            element = [ idx * x for x in [ 1 < len ( bits [ j ] ) for j in range ( len ( bits ) ) ] ] \n            self . bit_stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) \n            self . _create_dag_op ( name , [ self . arg_stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit_stack [ - 1 ] [ s ] for s in gbits ] ) \n            self . arg_stack . pop ( ) \n            self . bit_stack . pop ( ) \n    else : \n        raise QiskitError ( \"internal error undefined gate:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) "}
{"2429": "\ndef _process_gate ( self , node , opaque = False ) : \n    self . gates [ node . name ] = { } \n    de_gate = self . gates [ node . name ] \n    de_gate [ \"print\" ] = True \n    de_gate [ \"opaque\" ] = opaque \n    de_gate [ \"n_args\" ] = node . n_args ( ) \n    de_gate [ \"n_bits\" ] = node . n_bits ( ) \n    if 0 < node . n_args ( ) : \n        de_gate [ \"args\" ] = [ element . name for element in node . arguments . children ] \n    else : \n        de_gate [ \"args\" ] = [ ] \n    de_gate [ \"bits\" ] = [ c . name for c in node . bitlist . children ] \n    if opaque : \n        de_gate [ \"body\" ] = None \n    else : \n        de_gate [ \"body\" ] = node . body "}
{"2430": "\ndef _process_cnot ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ 0 ] ) \n    id1 = self . _process_bit_id ( node . children [ 1 ] ) \n    if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : \n        raise QiskitError ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) \n    for idx in range ( maxidx ) : \n        if 1 < len ( id0 ) and 1 < len ( id1 ) : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) \n        elif 1 < len ( id0 ) : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) \n        else : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition ) "}
{"2451": "\ndef chop ( array , epsilon = 1e-10 ) : \n    ret = np . array ( array ) \n    if np . isrealobj ( ret ) : \n        ret [ epsilon > abs ( ret ) ] = 0.0 \n    else : \n        ret . real [ epsilon > abs ( ret . real ) ] = 0.0 \n        ret . imag [ epsilon > abs ( ret . imag ) ] = 0.0 \n    return ret "}
{"2457": "\ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if state . ndim == 2 and len ( state ) == 4 and d0 == 2 and d1 == 2 : \n        return __eof_qubit ( state ) \n    elif state . ndim == 1 : \n        if d1 > d0 : \n            tr = [ 1 ] \n        else : \n            tr = [ 0 ] \n        state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n        return entropy ( state ) \n    else : \n        print ( 'Input must be a state-vector or 2-qubit density matrix.' ) \n    return None "}
{"2473": "\ndef to_zyz ( self ) : \n    mat = self . to_matrix ( ) \n    euler = np . zeros ( 3 , dtype = float ) \n    if 1 > mat [ 2 , 2 ] : \n        if - 1 < mat [ 2 , 2 ] : \n            euler [ 0 ] = math . atan2 ( mat [ 1 , 2 ] , mat [ 0 , 2 ] ) \n            euler [ 1 ] = math . acos ( mat [ 2 , 2 ] ) \n            euler [ 2 ] = math . atan2 ( mat [ 2 , 1 ] , - mat [ 2 , 0 ] ) \n        else : \n            euler [ 0 ] = - math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) \n            euler [ 1 ] = np . pi \n    else : \n        euler [ 0 ] = math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) \n    return euler "}
{"2477": "\ndef check_range ( self , j ) : \n    if isinstance ( j , int ) : \n        if 0 > j or self . size <= j : \n            raise QiskitIndexError ( \"register index out of range\" ) \n        elif isinstance ( j , slice ) : \n            if 0 > j . start or self . size <= j . stop or ( j . step is not None and 0 >= j . step ) : \n                raise QiskitIndexError ( \"register index slice out of range\" ) "}
{"2482": "\ndef is_positive_semidefinite_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    if not is_hermitian_matrix ( mat , rtol = rtol , atol = atol ) : \n        return False \n    vals = np . linalg . eigvalsh ( mat ) \n    for v in vals : \n        if - atol > v : \n            return False \n    return True "}
{"2497": "\ndef _choi_to_kraus ( data , input_dim , output_dim , atol = ATOL_DEFAULT ) : \n    if is_hermitian_matrix ( data , atol = atol ) : \n        w , v = la . eigh ( data ) \n        if len ( w [ - atol > w ] ) == 0 : \n            kraus = [ ] \n            for val , vec in zip ( w , v . T ) : \n                if atol < abs ( val ) : \n                    k = np . sqrt ( val ) * vec . reshape ( ( output_dim , input_dim ) , order = 'F' ) \n                    kraus . append ( k ) \n            if not kraus : \n                kraus . append ( np . zeros ( ( output_dim , input_dim ) , dtype = complex ) ) \n            return ( kraus , None ) \n    mat_u , svals , mat_vh = la . svd ( data ) \n    kraus_l = [ ] \n    kraus_r = [ ] \n    for val , vec_l , vec_r in zip ( svals , mat_u . T , mat_vh . conj ( ) ) : \n        kraus_l . append ( np . sqrt ( val ) * vec_l . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n        kraus_r . append ( np . sqrt ( val ) * vec_r . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n    return ( kraus_l , kraus_r ) "}
{"2535": "\ndef gaussian_square ( times : np . ndarray , amp : complex , center : float , width : float , sigma : float , zeroed_width : Union [ None , float ] = None ) -> np . ndarray : \n    square_start = center - width / 2 \n    square_stop = center + width / 2 \n    if zeroed_width : \n        zeroed_width = min ( width , zeroed_width ) \n        gauss_zeroed_width = zeroed_width - width \n    else : \n        gauss_zeroed_width = None \n    funclist = [ functools . partial ( gaussian , amp = amp , center = square_start , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = True ) , functools . partial ( gaussian , amp = amp , center = square_stop , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = True ) , functools . partial ( constant , amp = amp ) ] \n    condlist = [ square_start >= times , square_stop <= times ] \n    return np . piecewise ( times . astype ( np . complex_ ) , condlist , funclist ) "}
{"2553": "\ndef num_connected_components ( self , unitary_only = False ) : \n    reg_offset = 0 \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset += reg . size \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( 1 if instr . control else 0 ) \n        if 2 <= num_qargs and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = 0 \n            if instr . control and not unitary_only : \n                creg = instr . control [ 0 ] \n                creg_int = reg_map [ creg . name ] \n                for coff in range ( creg . size ) : \n                    temp_int = creg_int + coff \n                    for k in range ( num_sub_graphs ) : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            for item in args : \n                reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] \n                for k in range ( num_sub_graphs ) : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            if 1 < num_touched : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                for idx in range ( num_sub_graphs ) : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs -= ( num_touched - 1 ) \n        if num_sub_graphs == 1 : \n            break \n    return num_sub_graphs "}
{"2554": "\ndef bind_parameters ( self , value_dict ) : \n    new_circuit = self . copy ( ) \n    if self . parameters < value_dict . keys ( ) : \n        raise QiskitError ( 'Cannot bind parameters ({}) not present in the circuit.' . format ( [ str ( p ) for p in value_dict . keys ( ) - self . parameters ] ) ) \n    for parameter , value in value_dict . items ( ) : \n        new_circuit . _bind_parameter ( parameter , value ) \n    for parameter in value_dict : \n        del new_circuit . _parameter_table [ parameter ] \n    return new_circuit "}
{"2557": "\ndef _search_forward_n_swaps ( layout , gates , coupling_map , depth = SEARCH_DEPTH , width = SEARCH_WIDTH ) : \n    gates_mapped , gates_remaining = _map_free_gates ( layout , gates , coupling_map ) \n    base_step = { 'layout' : layout , 'swaps_added' : 0 , 'gates_mapped' : gates_mapped , 'gates_remaining' : gates_remaining } \n    if not gates_remaining or depth == 0 : \n        return base_step \n    possible_swaps = coupling_map . get_edges ( ) \n    def _score_swap ( swap ) : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        return _calc_layout_distance ( gates , coupling_map , trial_layout ) \n    ranked_swaps = sorted ( possible_swaps , key = _score_swap ) \n    best_swap , best_step = None , None \n    for swap in ranked_swaps [ : width ] : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        next_step = _search_forward_n_swaps ( trial_layout , gates_remaining , coupling_map , depth - 1 , width ) \n        if best_swap is None or _score_step ( best_step ) < _score_step ( next_step ) : \n            best_swap , best_step = swap , next_step \n    best_swap_gate = _swap_ops_from_edge ( best_swap , layout ) \n    return { 'layout' : best_step [ 'layout' ] , 'swaps_added' : 1 + best_step [ 'swaps_added' ] , 'gates_remaining' : best_step [ 'gates_remaining' ] , 'gates_mapped' : gates_mapped + best_swap_gate + best_step [ 'gates_mapped' ] , } "}
{"2584": "\ndef _bloch_angles ( pair_of_complex ) : \n    [ a_complex , b_complex ] = pair_of_complex \n    a_complex = complex ( a_complex ) \n    b_complex = complex ( b_complex ) \n    mag_a = np . absolute ( a_complex ) \n    final_r = float ( np . sqrt ( mag_a ** 2 + np . absolute ( b_complex ) ** 2 ) ) \n    if _EPS > final_r : \n        theta = 0 \n        phi = 0 \n        final_r = 0 \n        final_t = 0 \n    else : \n        theta = float ( 2 * np . arccos ( mag_a / final_r ) ) \n        a_arg = np . angle ( a_complex ) \n        b_arg = np . angle ( b_complex ) \n        final_t = a_arg + b_arg \n        phi = b_arg - a_arg \n    return final_r * np . exp ( 1.J * final_t / 2 ) , theta , phi "}
{"2585": "\ndef _multiplex ( self , target_gate , list_of_angles ) : \n    list_len = len ( list_of_angles ) \n    local_num_qubits = int ( math . log2 ( list_len ) ) + 1 \n    q = QuantumRegister ( local_num_qubits ) \n    circuit = QuantumCircuit ( q , name = \"multiplex\" + local_num_qubits . __str__ ( ) ) \n    lsb = q [ 0 ] \n    msb = q [ local_num_qubits - 1 ] \n    if local_num_qubits == 1 : \n        circuit . append ( target_gate ( list_of_angles [ 0 ] ) , [ q [ 0 ] ] ) \n        return circuit \n    angle_weight = scipy . kron ( [ [ 0.5 , 0.5 ] , [ 0.5 , - 0.5 ] ] , np . identity ( 2 ** ( local_num_qubits - 2 ) ) ) \n    list_of_angles = angle_weight . dot ( np . array ( list_of_angles ) ) . tolist ( ) \n    multiplex_1 = self . _multiplex ( target_gate , list_of_angles [ 0 : ( list_len // 2 ) ] ) \n    circuit . append ( multiplex_1 . to_instruction ( ) , q [ 0 : - 1 ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    multiplex_2 = self . _multiplex ( target_gate , list_of_angles [ ( list_len // 2 ) : ] ) \n    if 1 < list_len : \n        circuit . append ( multiplex_2 . to_instruction ( ) . mirror ( ) , q [ 0 : - 1 ] ) \n    else : \n        circuit . append ( multiplex_2 . to_instruction ( ) , q [ 0 : - 1 ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    return circuit "}
{"2594": "\ndef _layer_update ( self , i , first_layer , best_layout , best_depth , best_circuit , layer_list ) : \n    layout = best_layout \n    logger . debug ( \"layer_update: layout = %s\" , pformat ( layout ) ) \n    logger . debug ( \"layer_update: self.initial_layout = %s\" , pformat ( self . initial_layout ) ) \n    dagcircuit_output = DAGCircuit ( ) \n    for register in layout . get_virtual_bits ( ) . keys ( ) : \n        if register [ 0 ] not in dagcircuit_output . qregs . values ( ) : \n            dagcircuit_output . add_qreg ( register [ 0 ] ) \n    if first_layer : \n        logger . debug ( \"layer_update: first multi-qubit gate layer\" ) \n        for j in range ( i + 1 ) : \n            edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n            for bit in dagcircuit_output . clbits ( ) : \n                edge_map [ bit ] = bit \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , edge_map ) \n    else : \n        if 0 < best_depth : \n            logger . debug ( \"layer_update: there are swaps in this layer, \" \"depth %d\" , best_depth ) \n            dagcircuit_output . extend_back ( best_circuit ) \n        else : \n            logger . debug ( \"layer_update: there are no swaps in this layer\" ) \n        edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n        for bit in dagcircuit_output . clbits ( ) : \n            edge_map [ bit ] = bit \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , edge_map ) \n    return dagcircuit_output "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if 5 > number_of_qubits : \n        temp_set = [ ] \n        if case == 'weight' : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) \n        elif case == 'tensor' : \n            for k in range ( 4 ** number_of_qubits ) : \n                z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                for j in range ( number_of_qubits ) : \n                    element = ( k // ( 4 ** j ) ) % 4 \n                    if element == 1 : \n                        x [ j ] = True \n                    elif element == 2 : \n                        z [ j ] = True \n                        x [ j ] = True \n                    elif element == 3 : \n                        z [ j ] = True \n                temp_set . append ( Pauli ( z , x ) ) \n            return temp_set \n        else : \n            raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2608": "\ndef _get_measure_outcome ( self , qubit ) : \n    axis = list ( range ( self . _number_of_qubits ) ) \n    axis . remove ( self . _number_of_qubits - 1 - qubit ) \n    probabilities = np . sum ( np . abs ( self . _statevector ) ** 2 , axis = tuple ( axis ) ) \n    random_number = self . _local_random . rand ( ) \n    if probabilities [ 0 ] > random_number : \n        return '0' , probabilities [ 0 ] \n    return '1' , probabilities [ 1 ] "}
{"2614": "\ndef _get_statevector ( self ) : \n    vec = np . reshape ( self . _statevector , 2 ** self . _number_of_qubits ) \n    vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) \n    vec [ self . _chop_threshold > abs ( vec ) ] = 0.0 \n    return vec "}
{"2615": "\ndef _validate_measure_sampling ( self , experiment ) : \n    if 1 >= self . _shots : \n        self . _sample_measure = False \n        return \n    if hasattr ( experiment . config , 'allows_measure_sampling' ) : \n        self . _sample_measure = experiment . config . allows_measure_sampling \n    else : \n        measure_flag = False \n        for instruction in experiment . instructions : \n            if instruction . name == \"reset\" : \n                self . _sample_measure = False \n                return \n            if measure_flag : \n                if instruction . name not in [ \"measure\" , \"barrier\" , \"id\" , \"u0\" ] : \n                    self . _sample_measure = False \n                    return \n            elif instruction . name == \"measure\" : \n                measure_flag = True \n        self . _sample_measure = True "}
{"2618": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if max_qubits < n_qubits : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if experiment . config . memory_slots == 0 : \n            logger . warning ( 'No classical registers in circuit \"%s\", ' 'counts will be empty.' , name ) \n        elif 'measure' not in [ op . name for op in experiment . instructions ] : \n            logger . warning ( 'No measurements in circuit \"%s\", ' 'classical register will remain all zeros.' , name ) "}
{"2621": "\ndef _get_unitary ( self ) : \n    unitary = np . reshape ( self . _unitary , 2 * [ 2 ** self . _number_of_qubits ] ) \n    unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) \n    unitary [ self . _chop_threshold > abs ( unitary ) ] = 0.0 \n    return unitary "}
{"2623": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if max_qubits < n_qubits : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    if hasattr ( qobj . config , 'shots' ) and qobj . config . shots != 1 : \n        logger . info ( '\"%s\" only supports 1 shot. Setting shots=1.' , self . name ( ) ) \n        qobj . config . shots = 1 \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if getattr ( experiment . config , 'shots' , 1 ) != 1 : \n            logger . info ( '\"%s\" only supports 1 shot. ' 'Setting shots=1 for circuit \"%s\".' , self . name ( ) , name ) \n            experiment . config . shots = 1 \n        for operation in experiment . instructions : \n            if operation . name in [ 'measure' , 'reset' ] : \n                raise BasicAerError ( 'Unsupported \"%s\" instruction \"%s\" ' + 'in circuit \"%s\" ' , self . name ( ) , operation . name , name ) "}
{"2624": "\ndef _is_bit ( obj ) : \n    if isinstance ( obj , tuple ) and len ( obj ) == 2 : \n        if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and len ( obj [ 0 ] ) > obj [ 1 ] : \n            return True \n    return False "}
{"2625": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if self . coupling_map . size ( ) < num_dag_qubits : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    self . property_set [ 'layout' ] = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) "}
{"2626": "\ndef has_overlap ( self , interval : 'Interval' ) -> bool : \n    if interval . end > self . begin and self . end > interval . begin : \n        return True \n    return False "}
{"2639": "\ndef _initialize_backend_prop ( self ) : \n    backend_prop = self . backend_prop \n    for ginfo in backend_prop . gates : \n        if ginfo . gate == 'cx' : \n            for item in ginfo . parameters : \n                if item . name == 'gate_error' : \n                    g_reliab = 1.0 - item . value \n                    break \n                else : \n                    g_reliab = 1.0 \n            swap_reliab = - math . log ( pow ( g_reliab , 3 ) ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap_reliab ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap_reliab ) \n            self . cx_errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g_reliab \n            self . gate_list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) \n    idx = 0 \n    for q in backend_prop . qubits : \n        for nduv in q : \n            if nduv . name == 'readout_error' : \n                self . readout_errors [ idx ] = 1.0 - nduv . value \n                self . available_hw_qubits . append ( idx ) \n        idx += 1 \n    for edge in self . cx_errors : \n        self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ 0 ] ] * self . readout_errors [ edge [ 1 ] ] \n    self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) \n    for i in swap_costs_temp : \n        self . swap_costs [ i ] = { } \n        for j in swap_costs_temp [ i ] : \n            if ( i , j ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] \n            elif ( j , i ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] \n            else : \n                best_reliab = 0.0 \n                for n in self . swap_graph . neighbors ( j ) : \n                    if ( n , j ) in self . cx_errors : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] \n                    else : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] \n                    if best_reliab < reliab : \n                        best_reliab = reliab \n                self . swap_costs [ i ] [ j ] = best_reliab "}
{"2642": "\ndef _select_best_remaining_cx ( self ) : \n    candidates = [ ] \n    for gate in self . gate_list : \n        chk1 = gate [ 0 ] in self . available_hw_qubits \n        chk2 = gate [ 1 ] in self . available_hw_qubits \n        if chk1 and chk2 : \n            candidates . append ( gate ) \n    best_reliab = 0 \n    best_item = None \n    for item in candidates : \n        if best_reliab < self . gate_cost [ item ] : \n            best_reliab = self . gate_cost [ item ] \n            best_item = item \n    return best_item "}
{"2643": "\ndef _select_best_remaining_qubit ( self , prog_qubit ) : \n    reliab_store = { } \n    for hw_qubit in self . available_hw_qubits : \n        reliab = 1 \n        for n in self . prog_graph . neighbors ( prog_qubit ) : \n            if n in self . prog2hw : \n                reliab *= self . swap_costs [ self . prog2hw [ n ] ] [ hw_qubit ] \n        reliab *= self . readout_errors [ hw_qubit ] \n        reliab_store [ hw_qubit ] = reliab \n    max_reliab = 0 \n    best_hw_qubit = None \n    for hw_qubit in reliab_store : \n        if max_reliab < reliab_store [ hw_qubit ] : \n            max_reliab = reliab_store [ hw_qubit ] \n            best_hw_qubit = hw_qubit \n    return best_hw_qubit "}
{"2644": "\ndef run ( self , dag ) : \n    self . _initialize_backend_prop ( ) \n    num_qubits = self . _create_program_graph ( dag ) \n    if len ( self . swap_graph ) < num_qubits : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : \n        self . pending_program_edges . append ( ( end1 , end2 ) ) \n    while self . pending_program_edges : \n        edge = self . _select_next_edge ( ) \n        q1_mapped = edge [ 0 ] in self . prog2hw \n        q2_mapped = edge [ 1 ] in self . prog2hw \n        if ( not q1_mapped ) and ( not q2_mapped ) : \n            best_hw_edge = self . _select_best_remaining_cx ( ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] \n            self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] \n            self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) \n            self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) \n        elif not q1_mapped : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        else : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) \n            self . prog2hw [ edge [ 1 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        new_edges = [ x for x in self . pending_program_edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] \n        self . pending_program_edges = new_edges \n    for qid in self . qarg_to_id . values ( ) : \n        if qid not in self . prog2hw : \n            self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] \n            self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) \n    layout = Layout ( ) \n    for q in dag . qubits ( ) : \n        pid = self . _qarg_to_id ( q ) \n        hwid = self . prog2hw [ pid ] \n        layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid \n    self . property_set [ 'layout' ] = layout "}
{"2655": "\ndef swap_mapper_layer_update ( self , i , first_layer , best_layout , best_d , best_circ , layer_list ) : \n    layout = best_layout \n    dagcircuit_output = DAGCircuit ( ) \n    QR = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) \n    dagcircuit_output . add_qreg ( QR ) \n    identity_wire_map = { ( QR , j ) : ( QR , j ) for j in range ( self . coupling_map . size ( ) ) } \n    if first_layer : \n        for j in range ( i + 1 ) : \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , layout ) \n    else : \n        if 0 < best_d : \n            dagcircuit_output . compose_back ( best_circ , identity_wire_map ) \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , layout ) \n    return dagcircuit_output "}
{"2671": "\ndef iplot_state_qsphere ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"qsphere_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            data = $data;            qVisualizations.plotState(\"qsphere_$divNumber\",                                      \"qsphere\",                                      data,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] } \n    qspheres_data = [ ] \n    num = int ( np . log2 ( len ( rho ) ) ) \n    weig , stateall = linalg . eigh ( rho ) \n    for _ in range ( 2 ** num ) : \n        probmix = weig . max ( ) \n        prob_location = weig . argmax ( ) \n        if 0.001 < probmix : \n            state = stateall [ : , prob_location ] \n            loc = np . absolute ( state ) . argmax ( ) \n            for j in range ( 2 ** num ) : \n                test = np . absolute ( np . absolute ( state [ j ] ) - np . absolute ( state [ loc ] ) ) \n                if 0.001 > test : \n                    loc = j \n                    break \n            angles = ( np . angle ( state [ loc ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n            angleset = np . exp ( - 1j * angles ) \n            state = angleset * state \n            state . flatten ( ) \n            spherepoints = [ ] \n            for i in range ( 2 ** num ) : \n                element = bin ( i ) [ 2 : ] . zfill ( num ) \n                weight = element . count ( \"1\" ) \n                number_of_divisions = n_choose_k ( num , weight ) \n                weight_order = bit_string_index ( element ) \n                angle = weight_order * 2 * np . pi / number_of_divisions \n                zvalue = - 2 * weight / num + 1 \n                xvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . cos ( angle ) \n                yvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . sin ( angle ) \n                prob = np . real ( np . dot ( state [ i ] , state [ i ] . conj ( ) ) ) \n                angles = ( np . angle ( state [ i ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n                qpoint = { 'x' : xvalue , 'y' : yvalue , 'z' : zvalue , 'prob' : prob , 'phase' : angles } \n                spherepoints . append ( qpoint ) \n            sphere = { 'points' : spherepoints , 'eigenvalue' : probmix } \n            qspheres_data . append ( sphere ) \n            weig [ prob_location ] = 0 \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : qspheres_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2725": "\ndef collect_runs ( self , namelist ) : \n    group_list = [ ] \n    topo_ops = list ( self . topological_op_nodes ( ) ) \n    nodes_seen = dict ( zip ( topo_ops , [ False ] * len ( topo_ops ) ) ) \n    for node in topo_ops : \n        if node . name in namelist and node . condition is None and not nodes_seen [ node ] : \n            group = [ node ] \n            nodes_seen [ node ] = True \n            s = list ( self . _multi_graph . successors ( node ) ) \n            while len ( s ) == 1 and s [ 0 ] . type == \"op\" and s [ 0 ] . name in namelist : \n                group . append ( s [ 0 ] ) \n                nodes_seen [ s [ 0 ] ] = True \n                s = list ( self . _multi_graph . successors ( s [ 0 ] ) ) \n            if 1 <= len ( group ) : \n                group_list . append ( tuple ( group ) ) \n    return set ( group_list ) "}
{"2740": "\ndef __wizard ( rho , epsilon = None ) : \n    if epsilon is None : \n        epsilon = 0. \n    dim = len ( rho ) \n    rho_wizard = np . zeros ( [ dim , dim ] ) \n    v , w = np . linalg . eigh ( rho ) \n    for j in range ( dim ) : \n        if epsilon > v [ j ] : \n            tmp = v [ j ] \n            v [ j ] = 0. \n            x = 0. \n            for k in range ( j + 1 , dim ) : \n                x += tmp / ( dim - ( j + 1 ) ) \n                v [ k ] = v [ k ] + tmp / ( dim - ( j + 1 ) ) \n    for j in range ( dim ) : \n        rho_wizard = rho_wizard + v [ j ] * outer ( w [ : , j ] ) \n    return rho_wizard "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if msg_len > len ( msg ) : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif msg_len < len ( msg ) : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2745": "\ndef euler_angles_1q ( unitary_matrix ) : \n    if unitary_matrix . shape != ( 2 , 2 ) : \n        raise QiskitError ( \"euler_angles_1q: expected 2x2 matrix\" ) \n    phase = la . det ( unitary_matrix ) ** ( - 1.0 / 2.0 ) \n    U = phase * unitary_matrix \n    if _CUTOFF_PRECISION < abs ( U [ 0 , 0 ] ) : \n        theta = 2 * math . acos ( abs ( U [ 0 , 0 ] ) ) \n    else : \n        theta = 2 * math . asin ( abs ( U [ 1 , 0 ] ) ) \n    phase11 = 0.0 \n    phase10 = 0.0 \n    if _CUTOFF_PRECISION < abs ( math . cos ( theta / 2.0 ) ) : \n        phase11 = U [ 1 , 1 ] / math . cos ( theta / 2.0 ) \n    if _CUTOFF_PRECISION < abs ( math . sin ( theta / 2.0 ) ) : \n        phase10 = U [ 1 , 0 ] / math . sin ( theta / 2.0 ) \n    phiplambda = 2 * math . atan2 ( np . imag ( phase11 ) , np . real ( phase11 ) ) \n    phimlambda = 2 * math . atan2 ( np . imag ( phase10 ) , np . real ( phase10 ) ) \n    phi = 0.0 \n    if _CUTOFF_PRECISION < abs ( U [ 0 , 0 ] ) and _CUTOFF_PRECISION < abs ( U [ 1 , 0 ] ) : \n        phi = ( phiplambda + phimlambda ) / 2.0 \n        lamb = ( phiplambda - phimlambda ) / 2.0 \n    else : \n        if _CUTOFF_PRECISION > abs ( U [ 0 , 0 ] ) : \n            lamb = - phimlambda \n        else : \n            lamb = phiplambda \n    Rzphi = np . array ( [ [ np . exp ( - 1j * phi / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * phi / 2.0 ) ] ] , dtype = complex ) \n    Rytheta = np . array ( [ [ np . cos ( theta / 2.0 ) , - np . sin ( theta / 2.0 ) ] , [ np . sin ( theta / 2.0 ) , np . cos ( theta / 2.0 ) ] ] , dtype = complex ) \n    Rzlambda = np . array ( [ [ np . exp ( - 1j * lamb / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * lamb / 2.0 ) ] ] , dtype = complex ) \n    V = np . dot ( Rzphi , np . dot ( Rytheta , Rzlambda ) ) \n    if _CUTOFF_PRECISION < la . norm ( V - U ) : \n        raise QiskitError ( \"euler_angles_1q: incorrect result\" ) \n    return theta , phi , lamb "}
{"2746": "\ndef simplify_U ( theta , phi , lam ) : \n    gate = U3Gate ( theta , phi , lam ) \n    if _CUTOFF_PRECISION > abs ( gate . params [ 0 ] % ( 2.0 * math . pi ) ) : \n        gate = U1Gate ( gate . params [ 0 ] + gate . params [ 1 ] + gate . params [ 2 ] ) \n    if isinstance ( gate , U3Gate ) : \n        if _CUTOFF_PRECISION > abs ( ( gate . params [ 0 ] - math . pi / 2 ) % ( 2.0 * math . pi ) ) : \n            gate = U2Gate ( gate . params [ 1 ] , gate . params [ 2 ] + ( gate . params [ 0 ] - math . pi / 2 ) ) \n        if _CUTOFF_PRECISION > abs ( ( gate . params [ 0 ] + math . pi / 2 ) % ( 2.0 * math . pi ) ) : \n            gate = U2Gate ( gate . params [ 1 ] + math . pi , gate . params [ 2 ] - math . pi + ( gate . params [ 0 ] + math . pi / 2 ) ) \n    if isinstance ( gate , U1Gate ) and _CUTOFF_PRECISION > abs ( gate . params [ 0 ] % ( 4.0 * math . pi ) ) : \n        gate = IdGate ( ) \n    return gate "}
{"2771": "\ndef convert_acquire ( self , shift , instruction ) : \n    meas_level = self . _run_config . get ( 'meas_level' , 2 ) \n    command_dict = { 'name' : 'acquire' , 't0' : shift + instruction . start_time , 'duration' : instruction . duration , 'qubits' : [ q . index for q in instruction . acquires ] , 'memory_slot' : [ m . index for m in instruction . mem_slots ] } \n    if meas_level == 2 : \n        if instruction . command . discriminator : \n            command_dict . update ( { 'discriminators' : [ QobjMeasurementOption ( name = instruction . command . discriminator . name , params = instruction . command . discriminator . params ) ] } ) \n        command_dict . update ( { 'register_slot' : [ regs . index for regs in instruction . reg_slots ] } ) \n    if 1 <= meas_level : \n        if instruction . command . kernel : \n            command_dict . update ( { 'kernels' : [ QobjMeasurementOption ( name = instruction . command . kernel . name , params = instruction . command . kernel . params ) ] } ) \n    return self . _qobj_model ( ** command_dict ) "}
{"2785": "\ndef verify_as_gate ( self , obj , bitlist , arglist = None ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( \"Cannot find gate definition for '\" + obj . name + \"', line\" , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if not ( g_sym . type == 'gate' or g_sym . type == 'opaque' ) : \n        raise QasmError ( \"'\" + obj . name + \"' is used as a gate \" + \"or opaque call but the symbol is neither;\" + \" it is a '\" + g_sym . type + \"' line\" , str ( obj . line ) , 'file' , obj . file ) \n    if g_sym . n_bits ( ) != bitlist . size ( ) : \n        raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( bitlist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_bits ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    if arglist : \n        if g_sym . n_args ( ) != arglist . size ( ) : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( arglist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    else : \n        if 0 < g_sym . n_args ( ) : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' has no arguments but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) "}
{"2786": "\ndef verify_reg ( self , obj , object_type ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( 'Cannot find definition for' , object_type , \"'\" + obj . name + \"'\" , 'at line' , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if g_sym . type != object_type : \n        raise QasmError ( \"Type for '\" + g_sym . name + \"' should be '\" + object_type + \"' but was found to be '\" + g_sym . type + \"'\" , \"line\" , str ( obj . line ) , \"file\" , obj . file ) \n    if obj . type == 'indexed_id' : \n        bound = g_sym . index \n        ndx = obj . index \n        if 0 > ndx or bound <= ndx : \n            raise QasmError ( \"Register index for '\" + g_sym . name + \"' out of bounds. Index is\" , str ( ndx ) , \"bound is 0 <= index <\" , str ( bound ) , \"at line\" , str ( obj . line ) , \"file\" , obj . file ) "}
{"2788": "\ndef find_column ( self , input_ , token ) : \n    if token is None : \n        return 0 \n    last_cr = input_ . rfind ( '\\n' , 0 , token . lexpos ) \n    if 0 > last_cr : \n        last_cr = 0 \n    column = ( token . lexpos - last_cr ) + 1 \n    return column "}
{"2794": "\ndef basis_state ( str_state , num ) : \n    n = int ( str_state , 2 ) \n    if len ( str_state ) <= num : \n        state = np . zeros ( 1 << num , dtype = complex ) \n        state [ n ] = 1 \n        return state \n    else : \n        raise QiskitError ( 'size of bitstring is greater than num.' ) "}
{"2802": "\ndef get_backend ( self , name = None , ** kwargs ) : \n    backends = self . backends ( name , ** kwargs ) \n    if 1 < len ( backends ) : \n        raise QiskitBackendNotFoundError ( 'More than one backend matches the criteria' ) \n    elif not backends : \n        raise QiskitBackendNotFoundError ( 'No backend matches the criteria' ) \n    return backends [ 0 ] "}
{"2808": "\ndef _get_beamer_page ( self ) : \n    PIL_limit = 40000 \n    beamer_limit = 550 \n    aspect_ratio = self . sum_row_heights / self . sum_column_widths \n    margin_factor = 1.5 \n    height = min ( self . sum_row_heights * margin_factor , beamer_limit ) \n    width = min ( self . sum_column_widths * margin_factor , beamer_limit ) \n    if PIL_limit < height * width : \n        height = min ( np . sqrt ( PIL_limit * aspect_ratio ) , beamer_limit ) \n        width = min ( np . sqrt ( PIL_limit / aspect_ratio ) , beamer_limit ) \n    height = max ( height , 10 ) \n    width = max ( width , 10 ) \n    return ( height , width , self . scale ) "}
{"2818": "\ndef random_unitary ( dim , seed = None ) : \n    if dim == 0 or not math . log2 ( dim ) . is_integer ( ) : \n        raise QiskitError ( \"Desired unitary dimension not a positive power of 2.\" ) \n    matrix = np . zeros ( [ dim , dim ] , dtype = complex ) \n    for j in range ( dim ) : \n        if j == 0 : \n            a = random_state ( dim , seed ) \n        else : \n            a = random_state ( dim ) \n        matrix [ : , j ] = np . copy ( a ) \n        i = j - 1 \n        while 0 <= i : \n            dc = np . vdot ( matrix [ : , i ] , a ) \n            matrix [ : , j ] = matrix [ : , j ] - dc * matrix [ : , i ] \n            i = i - 1 \n        matrix [ : , j ] = matrix [ : , j ] * ( 1.0 / np . sqrt ( np . vdot ( matrix [ : , j ] , matrix [ : , j ] ) ) ) \n    return Operator ( matrix ) "}
{"2834": "\ndef _atol ( self , atol ) : \n    max_tol = self . __class__ . MAX_TOL \n    if 0 > atol : \n        raise QiskitError ( \"Invalid atol: must be non-negative.\" ) \n    if max_tol < atol : \n        raise QiskitError ( \"Invalid atol: must be less than {}.\" . format ( max_tol ) ) \n    self . __class__ . ATOL = atol "}
{"2835": "\ndef _rtol ( self , rtol ) : \n    max_tol = self . __class__ . MAX_TOL \n    if 0 > rtol : \n        raise QiskitError ( \"Invalid rtol: must be non-negative.\" ) \n    if max_tol < rtol : \n        raise QiskitError ( \"Invalid rtol: must be less than {}.\" . format ( max_tol ) ) \n    self . __class__ . RTOL = rtol "}
{"2840": "\ndef power ( self , n ) : \n    if not isinstance ( n , ( int , np . integer ) ) or 1 > n : \n        raise QiskitError ( \"Can only power with positive integer powers.\" ) \n    if self . _input_dim != self . _output_dim : \n        raise QiskitError ( \"Can only power with input_dim = output_dim.\" ) \n    ret = self . copy ( ) \n    for _ in range ( 1 , n ) : \n        ret = ret . compose ( self ) \n    return ret "}
{"2866": "\ndef _exc_to_net ( param1 , success ) : \n    if 3 >= len ( param1 ) : \n        if success : \n            return 0 \n        else : \n            return 314 \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return KNOWN_EXC [ exc ] \n    else : \n        logger . warning ( \"Unknown Java exception, consider adding it to dictionary: %s\" , param1 ) \n        return 41 "}
{"2867": "\ndef _exc_to_http ( param1 ) : \n    if 3 >= len ( param1 ) : \n        try : \n            int ( param1 ) \n        except BaseException : \n            logger . error ( \"JMeter wrote some strange data into codes column: %s\" , param1 ) \n        else : \n            return int ( param1 ) \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return 0 \n    else : \n        logger . warning ( \"Unknown Java exception. %s\" , param1 ) \n        return 0 "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n        self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if 120000 < self . timeout : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl' "}
{"2870": "\ndef get_info ( self ) : \n    result = copy . copy ( self . streams [ 0 ] ) \n    result . stat_log = self . stat_log \n    result . steps = [ ] \n    result . ammo_file = '' \n    result . rps_schedule = None \n    result . ammo_count = 0 \n    result . duration = 0 \n    result . instances = 0 \n    result . loadscheme = [ ] \n    result . loop_count = 0 \n    for stream in self . streams : \n        sec_no = 0 \n        logger . debug ( \"Steps: %s\" , stream . stepper_wrapper . steps ) \n        for item in stream . stepper_wrapper . steps : \n            for x in range ( 0 , item [ 1 ] ) : \n                if sec_no < len ( result . steps ) : \n                    result . steps [ sec_no ] [ 0 ] += item [ 0 ] \n                else : \n                    result . steps . append ( [ item [ 0 ] , 1 ] ) \n                sec_no += 1 \n        if result . rps_schedule : \n            result . rps_schedule = [ ] \n        else : \n            result . rps_schedule = stream . stepper_wrapper . loadscheme \n        if result . loadscheme : \n            result . loadscheme = '' \n        else : \n            result . loadscheme = '' \n        if result . loop_count : \n            result . loop_count = u'0' \n        else : \n            result . loop_count = stream . stepper_wrapper . loop_count \n        result . ammo_file += '{} ' . format ( stream . stepper_wrapper . ammo_file ) \n        result . ammo_count += stream . stepper_wrapper . ammo_count \n        result . duration = max ( result . duration , stream . stepper_wrapper . duration ) \n        result . instances += stream . instances \n    if not result . ammo_count : \n        raise ValueError ( \"Total ammo count cannot be zero\" ) \n    return result "}
{"2880": "\ndef create ( rps_schedule ) : \n    if 1 < len ( rps_schedule ) : \n        lp = Composite ( [ StepFactory . produce ( step_config ) for step_config in rps_schedule ] ) \n    else : \n        lp = StepFactory . produce ( rps_schedule [ 0 ] ) \n    info . status . publish ( 'duration' , lp . get_duration ( ) / 1000 ) \n    info . status . publish ( 'steps' , lp . get_rps_list ( ) ) \n    info . status . lp_len = len ( lp ) \n    return lp "}
{"2893": "\ndef _collect_data ( self , end = False ) : \n    data = get_nowait_from_queue ( self . results ) \n    stats = get_nowait_from_queue ( self . stats_results ) \n    logger . debug ( \"Data timestamps: %s\" % [ d . get ( 'ts' ) for d in data ] ) \n    logger . debug ( \"Stats timestamps: %s\" % [ d . get ( 'ts' ) for d in stats ] ) \n    for item in data : \n        ts = item [ 'ts' ] \n        if ts in self . stat_cache : \n            data_item = item \n            stat_item = self . stat_cache . pop ( ts ) \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . data_cache [ ts ] = item \n    for item in stats : \n        ts = item [ 'ts' ] \n        if ts in self . data_cache : \n            data_item = self . data_cache . pop ( ts ) \n            stat_item = item \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . stat_cache [ ts ] = item \n    if end and 0 < len ( self . data_cache ) : \n        logger . info ( 'Timestamps without stats:' ) \n        for ts , data_item in sorted ( self . data_cache . items ( ) , key = lambda i : i [ 0 ] ) : \n            logger . info ( ts ) \n            self . __notify_listeners ( data_item , StatsReader . stats_item ( ts , 0 , 0 ) ) "}
{"2899": "\ndef __discover_jmeter_udp_port ( self ) : \n    r = re . compile ( self . DISCOVER_PORT_PATTERN ) \n    with open ( self . process_stderr . name , 'r' ) as f : \n        cnt = 0 \n        while self . process . pid and 10 > cnt : \n            line = f . readline ( ) \n            m = r . match ( line ) \n            if m is None : \n                cnt += 1 \n                time . sleep ( 1 ) \n            else : \n                port = int ( m . group ( 'port' ) ) \n                return port \n        else : \n            logger . warning ( 'JMeter UDP port wasn\\'t discovered' ) \n            return None "}
{"2900": "\ndef __add_jmeter_components ( self , jmx , jtl , variables ) : \n    logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) \n    with open ( jmx , 'r' ) as src_jmx : \n        source_lines = src_jmx . readlines ( ) \n    try : \n        closing = source_lines . pop ( - 1 ) \n        if \"WorkBenchGui\" in source_lines [ - 5 ] : \n            logger . info ( \"WorkBench checkbox enabled...bypassing\" ) \n            last_string_count = 6 \n        else : \n            last_string_count = 2 \n        while 0 < last_string_count : \n            closing = source_lines . pop ( - 1 ) + closing \n            last_string_count -= 1 \n        logger . debug ( \"Closing statement: %s\" , closing ) \n    except Exception as exc : \n        raise RuntimeError ( \"Failed to find the end of JMX XML: %s\" % exc ) \n    udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) \n    udv_set = [ ] \n    for var_name , var_value in variables . iteritems ( ) : \n        udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) \n    udv = \"\\n\" . join ( udv_set ) \n    if 2.13 <= self . jmeter_ver : \n        save_connect = '<connectTime>true</connectTime>' \n    else : \n        save_connect = '' \n    if self . ext_log in [ 'errors' , 'all' ] : \n        level_map = { 'errors' : 'true' , 'all' : 'false' } \n        tpl_resource = 'jmeter_writer_ext.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'ext_log' : self . ext_log_file , 'ext_level' : level_map [ self . ext_log ] , 'save_connect' : save_connect } \n    else : \n        tpl_resource = 'jmeter_writer.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'save_connect' : save_connect } \n    tpl = resource_string ( __name__ , 'config/' + tpl_resource ) \n    try : \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) \n    except OSError as exc : \n        logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) \n    logger . debug ( \"Modified JMX: %s\" , new_jmx ) \n    with open ( new_jmx , \"wb\" ) as fh : \n        fh . write ( '' . join ( source_lines ) ) \n        fh . write ( tpl % tpl_args ) \n        fh . write ( closing ) \n    return new_jmx "}
{"2901": "\ndef __terminate ( self ) : \n    if self . __stderr_file : \n        self . __stderr_file . close ( ) \n    if not self . __process : \n        return \n    waitfor = time . time ( ) + _PROCESS_KILL_TIMEOUT \n    while waitfor > time . time ( ) : \n        try : \n            self . __process . terminate ( ) \n        except EnvironmentError as e : \n            if e . errno != errno . ESRCH : \n                _LOGGER . warning ( \"Failed to terminate process '{}': {}\" . format ( self . __cmd , e ) ) \n            return \n        time . sleep ( 0.1 ) \n    try : \n        self . __process . kill ( ) \n    except EnvironmentError as e : \n        if e . errno != errno . ESRCH : \n            _LOGGER . warning ( \"Failed to kill process '{}': {}\" . format ( self . __cmd , e ) ) \n        return "}
{"2902": "\ndef _read_data ( self , lines ) : \n    results = [ ] \n    for line in lines : \n        timestamp , rps , instances = line . split ( \"\\t\" ) \n        curr_ts = int ( float ( timestamp ) ) \n        if curr_ts > self . __last_ts : \n            self . __last_ts = curr_ts \n            results . append ( self . stats_item ( self . __last_ts , float ( rps ) , float ( instances ) ) ) \n    return results "}
{"2906": "\ndef __check_disk ( self ) : \n    cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" \n    cmd += self . core . artifacts_base_dir \n    cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" \n    res = execute ( cmd , True , 0.1 , True ) \n    logging . debug ( \"Result: %s\" , res ) \n    if not len ( res [ 1 ] ) : \n        self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) \n        return \n    disk_free = res [ 1 ] \n    self . log . debug ( \"Disk free space: %s/%s\" , disk_free . strip ( ) , self . disk_limit ) \n    if self . disk_limit > int ( disk_free . strip ( ) ) : \n        raise RuntimeError ( \"Not enough local resources: disk space less than %sMB in %s: %sMB\" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) ) "}
{"2907": "\ndef __check_mem ( self ) : \n    mem_free = psutil . virtual_memory ( ) . available / 2 ** 20 \n    self . log . debug ( \"Memory free: %s/%s\" , mem_free , self . mem_limit ) \n    if self . mem_limit > mem_free : \n        raise RuntimeError ( \"Not enough resources: free memory less \" \"than %sMB: %sMB\" % ( self . mem_limit , mem_free ) ) "}
{"2909": "\ndef __get_right_line ( self , widget_output ) : \n    right_line = '' \n    if widget_output : \n        right_line = widget_output . pop ( 0 ) \n        if self . right_panel_width < len ( right_line ) : \n            right_line_plain = self . markup . clean_markup ( right_line ) \n            if self . right_panel_width < len ( right_line_plain ) : \n                right_line = right_line [ : self . right_panel_width ] + self . markup . RESET \n    return right_line "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ True if i == ' ' else False for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( True ) \n            elif is_space ( chunk ) : \n                result . append ( True ) \n            else : \n                result . append ( False ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result += chunk \n        else : \n            if 0 < left : \n                if left >= len ( chunk ) : \n                    result += chunk \n                    left -= len ( chunk ) \n                else : \n                    leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] \n                    was_cut = not is_empty ( leftover , markups ) \n                    if was_cut : \n                        result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' \n                    else : \n                        result += chunk [ : left ] \n                    left = 0 \n    return result "}
{"2912": "\ndef render_screen ( self ) : \n    self . term_width , self . term_height = get_terminal_size ( ) \n    self . log . debug ( \"Terminal size: %sx%s\" , self . term_width , self . term_height ) \n    self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100 ) ) - 1 \n    if 0 < self . right_panel_width : \n        self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 \n    else : \n        self . right_panel_width = 0 \n        self . left_panel_width = self . term_width - 1 \n    self . log . debug ( \"Left/right panels width: %s/%s\" , self . left_panel_width , self . right_panel_width ) \n    widget_output = [ ] \n    if self . right_panel_width : \n        widget_output = [ ] \n        self . log . debug ( \"There are %d info widgets\" % len ( self . info_widgets ) ) \n        for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : \n            self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) \n            widget_out = widget . render ( self ) . strip ( ) \n            if widget_out : \n                widget_output += widget_out . split ( \"\\n\" ) \n                widget_output += [ \"\" ] \n    left_lines = self . __render_left_panel ( ) \n    self . log . debug ( \"Composing final screen output\" ) \n    output = [ ] \n    for line_no in range ( 1 , self . term_height ) : \n        line = \" \" \n        if 1 < line_no and left_lines : \n            left_line = left_lines . pop ( 0 ) \n            left_line_plain = self . markup . clean_markup ( left_line ) \n            left_line += ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) \n            line += left_line \n        else : \n            line += ' ' * self . left_panel_width \n        if self . right_panel_width : \n            line += self . markup . RESET \n            line += self . markup . WHITE \n            line += self . RIGHT_PANEL_SEPARATOR \n            line += self . markup . RESET \n            right_line = self . __get_right_line ( widget_output ) \n            line += right_line \n        output . append ( line ) \n    return self . markup . new_line . join ( output ) + self . markup . new_line "}
{"2921": "\ndef get_plugin_of_type ( self , plugin_class ) : \n    logger . debug ( \"Searching for plugin: %s\" , plugin_class ) \n    matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin_class ) ] \n    if matches : \n        if 1 < len ( matches ) : \n            logger . debug ( \"More then one plugin of type %s found. Using first one.\" , plugin_class ) \n        return matches [ - 1 ] \n    else : \n        raise KeyError ( \"Requested plugin type not found: %s\" % plugin_class ) "}
{"2933": "\ndef poll ( self ) : \n    start_time = time . time ( ) \n    for agent in self . agents : \n        for collect in agent . reader : \n            if not collect : \n                return 0 \n            for chunk in collect : \n                ts , prepared_results = chunk \n                if self . load_start_time and self . load_start_time <= int ( ts ) : \n                    ready_to_send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash_hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared_results } } } \n                    self . __collected_data . append ( ready_to_send ) \n    logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000 ) \n    collected_data_length = len ( self . __collected_data ) \n    if not self . first_data_received and self . __collected_data : \n        self . first_data_received = True \n        logger . info ( \"Monitoring received first data.\" ) \n    else : \n        self . send_collected_data ( ) \n    return collected_data_length "}
{"2936": "\ndef __handle_data_items ( self , host , data ) : \n    for metric , value in data . iteritems ( ) : \n        if value == '' : \n            self . sign [ host ] [ metric ] = - 1 \n            self . data [ host ] [ metric ] = value \n        else : \n            if not self . data [ host ] . get ( metric , None ) : \n                self . sign [ host ] [ metric ] = 1 \n            elif float ( self . data [ host ] [ metric ] ) < float ( value ) : \n                self . sign [ host ] [ metric ] = 1 \n            elif float ( self . data [ host ] [ metric ] ) > float ( value ) : \n                self . sign [ host ] [ metric ] = - 1 \n            else : \n                self . sign [ host ] [ metric ] = 0 \n            self . data [ host ] [ metric ] = \"%.2f\" % float ( value ) "}
{"2938": "\nasync def subscribe ( self , channels ) : \n    ws_channels = [ ] \n    nats_channels = [ ] \n    for c in channels : \n        if c . startswith ( ( 'Q.' , 'T.' , 'A.' , 'AM.' , ) ) : \n            nats_channels . append ( c ) \n        else : \n            ws_channels . append ( c ) \n    if 0 < len ( ws_channels ) : \n        await self . _ensure_ws ( ) \n        await self . _ws . send ( json . dumps ( { 'action' : 'listen' , 'data' : { 'streams' : ws_channels , } } ) ) \n    if 0 < len ( nats_channels ) : \n        await self . _ensure_nats ( ) \n        await self . polygon . subscribe ( nats_channels ) "}
{"2941": "\ndef _one_request ( self , method , url , opts , retry ) : \n    retry_codes = self . _retry_codes \n    resp = self . _session . request ( method , url , ** opts ) \n    try : \n        resp . raise_for_status ( ) \n    except HTTPError as http_error : \n        if resp . status_code in retry_codes and 0 < retry : \n            raise RetryException ( ) \n        if 'code' in resp . text : \n            error = resp . json ( ) \n            if 'code' in error : \n                raise APIError ( error , http_error ) \n        else : \n            raise \n    if resp . text != '' : \n        return resp . json ( ) \n    return None "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if 2 > num_part : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if len ( sizes ) < num_part : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if num_part == 2 : \n        total_nfps , u = min ( ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( 0 , len ( sizes ) - 1 ) ) \n        return [ ( sizes [ 0 ] , sizes [ u ] ) , ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2 ) ) \n    p2i = lambda p : p - 2 \n    for p in range ( 2 , num_part ) : \n        for u in range ( p - 1 , len ( sizes ) ) : \n            if p == 2 : \n                cost [ u , p2i ( p ) ] = min ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , u ] for u1 in range ( ( p - 1 ) - 1 , u ) ) \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , len ( sizes ) - 1 ) ) \n    partitions = [ ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] \n    p -= 1 \n    while 1 < p : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + 1 , u ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , u ) ) \n        partitions . insert ( 0 , ( sizes [ u1_best + 1 ] , sizes [ u ] ) ) \n        u = u1_best \n        p -= 1 \n    partitions . insert ( 0 , ( sizes [ 0 ] , sizes [ u ] ) ) \n    return [ partitions , total_nfps , cost ] "}
{"3002": "\ndef optimal_partitions ( sizes , counts , num_part ) : \n    if 2 > num_part : \n        return [ ( sizes [ 0 ] , sizes [ - 1 ] ) ] \n    if len ( sizes ) <= num_part : \n        partitions = [ ( x , x ) for x in sizes ] \n        return partitions \n    nfps = _compute_nfps_real ( counts , sizes ) \n    partitions , _ , _ = _compute_best_partitions ( num_part , sizes , nfps ) \n    return partitions "}
{"3006": "\ndef serialize ( self , buf , byteorder = '@' ) : \n    if self . bytesize ( ) > len ( buf ) : \n        raise ValueError ( \"The buffer does not have enough space\\                    for holding this MinHash.\" ) \n    fmt = \"%sqi%dI\" % ( byteorder , len ( self ) ) \n    struct . pack_into ( fmt , buf , 0 , self . seed , len ( self ) , * self . hashvalues ) "}
{"3010": "\ndef union ( cls , * mhs ) : \n    if 2 > len ( mhs ) : \n        raise ValueError ( \"Cannot union less than 2 MinHash\" ) \n    num_perm = len ( mhs [ 0 ] ) \n    seed = mhs [ 0 ] . seed \n    if any ( ( seed != m . seed or num_perm != len ( m ) ) for m in mhs ) : \n        raise ValueError ( \"The unioning MinHash must have the\\                    same seed and number of permutation functions\" ) \n    hashvalues = np . minimum . reduce ( [ m . hashvalues for m in mhs ] ) \n    permutations = mhs [ 0 ] . permutations \n    return cls ( num_perm = num_perm , seed = seed , hashvalues = hashvalues , permutations = permutations ) "}
{"3011": "\ndef index ( self , entries ) : \n    if not self . is_empty ( ) : \n        raise ValueError ( \"Cannot call index again on a non-empty index\" ) \n    if not isinstance ( entries , list ) : \n        queue = deque ( [ ] ) \n        for key , minhash , size in entries : \n            if 0 >= size : \n                raise ValueError ( \"Set size must be positive\" ) \n            queue . append ( ( key , minhash , size ) ) \n        entries = list ( queue ) \n    if len ( entries ) == 0 : \n        raise ValueError ( \"entries is empty\" ) \n    sizes , counts = np . array ( sorted ( Counter ( e [ 2 ] for e in entries ) . most_common ( ) ) ) . T \n    partitions = optimal_partitions ( sizes , counts , len ( self . indexes ) ) \n    for i , ( lower , upper ) in enumerate ( partitions ) : \n        self . lowers [ i ] , self . uppers [ i ] = lower , upper \n    entries . sort ( key = lambda e : e [ 2 ] ) \n    curr_part = 0 \n    for key , minhash , size in entries : \n        if self . uppers [ curr_part ] < size : \n            curr_part += 1 \n        for r in self . indexes [ curr_part ] : \n            self . indexes [ curr_part ] [ r ] . insert ( key , minhash ) "}
{"3016": "\ndef count ( self ) : \n    e = self . alpha * float ( self . m ** 2 ) / np . sum ( 2.0 ** ( - self . reg ) ) \n    if ( 5.0 / 2.0 ) * self . m >= e : \n        num_zero = self . m - np . count_nonzero ( self . reg ) \n        return self . _linearcounting ( num_zero ) \n    if ( 1.0 / 30.0 ) * ( 1 << 32 ) >= e : \n        return e \n    return self . _largerange_correction ( e ) "}
{"3019": "\ndef apk ( actual , predicted , k = 10 ) : \n    if k < len ( predicted ) : \n        predicted = predicted [ : k ] \n    score = 0.0 \n    num_hits = 0.0 \n    for i , p in enumerate ( predicted ) : \n        if p in actual and p not in predicted [ : i ] : \n            num_hits += 1.0 \n            score += num_hits / ( i + 1.0 ) \n    if len ( actual ) == 0 : \n        return 0.0 \n    return score / min ( len ( actual ) , k ) "}
{"3022": "\ndef query ( self , minhash , k ) : \n    if 0 >= k : \n        raise ValueError ( \"k must be positive\" ) \n    if self . k * self . l > len ( minhash ) : \n        raise ValueError ( \"The num_perm of MinHash out of range\" ) \n    results = set ( ) \n    r = self . k \n    while 0 < r : \n        for key in self . _query ( minhash , r , self . l ) : \n            results . add ( key ) \n            if k <= len ( results ) : \n                return list ( results ) \n        r -= 1 \n    return list ( results ) "}
{"3050": "\ndef operative_config_str ( max_line_length = 80 , continuation_indent = 4 ) : \n    def format_binding ( key , value ) : \n        formatted_val = pprint . pformat ( value , width = ( max_line_length - continuation_indent ) ) \n        formatted_val_lines = formatted_val . split ( '\\n' ) \n        if ( len ( formatted_val_lines ) == 1 and max_line_length >= len ( key + formatted_val ) ) : \n            output = '{} = {}' . format ( key , formatted_val ) \n        else : \n            indented_formatted_val = '\\n' . join ( [ ' ' * continuation_indent + line for line in formatted_val_lines ] ) \n            output = '{} = \\\\\\n{}' . format ( key , indented_formatted_val ) \n        return output \n    def sort_key ( key_tuple ) : \n        scope , selector = key_tuple [ 0 ] \n        parts = selector . lower ( ) . split ( '.' ) [ : : - 1 ] + scope . lower ( ) . split ( '/' ) [ : : - 1 ] \n        return '/' . join ( parts ) \n    formatted_statements = [ 'import {}' . format ( module ) for module in sorted ( _IMPORTED_MODULES ) ] \n    if formatted_statements : \n        formatted_statements . append ( '' ) \n    macros = { } \n    for ( scope , selector ) , config in six . iteritems ( _OPERATIVE_CONFIG ) : \n        if _REGISTRY [ selector ] . fn_or_cls == macro : \n            macros [ scope , selector ] = config \n    if macros : \n        formatted_statements . append ( '# Macros:' ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2 ) ) \n    for ( name , _ ) , config in sorted ( macros . items ( ) , key = sort_key ) : \n        binding = format_binding ( name , config [ 'value' ] ) \n        formatted_statements . append ( binding ) \n    if macros : \n        formatted_statements . append ( '' ) \n    sorted_items = sorted ( _OPERATIVE_CONFIG . items ( ) , key = sort_key ) \n    for ( scope , selector ) , config in sorted_items : \n        configurable_ = _REGISTRY [ selector ] \n        fn = configurable_ . fn_or_cls \n        if fn == macro or fn == _retrieve_constant : \n            continue \n        minimal_selector = _REGISTRY . minimal_selector ( configurable_ . selector ) \n        scoped_selector = ( scope + '/' if scope else '' ) + minimal_selector \n        parameters = [ ( k , v ) for k , v in six . iteritems ( config ) if _is_literally_representable ( v ) ] \n        formatted_statements . append ( '# Parameters for {}:' . format ( scoped_selector ) ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2 ) ) \n        for arg , val in sorted ( parameters ) : \n            binding = format_binding ( '{}.{}' . format ( scoped_selector , arg ) , val ) \n            formatted_statements . append ( binding ) \n        if not parameters : \n            formatted_statements . append ( '# None.' ) \n        formatted_statements . append ( '' ) \n    return '\\n' . join ( formatted_statements ) "}
{"3063": "\ndef minimal_selector ( self , complete_selector ) : \n    if complete_selector not in self . _selector_map : \n        raise KeyError ( \"No value with selector '{}'.\" . format ( complete_selector ) ) \n    selector_components = complete_selector . split ( '.' ) \n    node = self . _selector_tree \n    start = None \n    for i , component in enumerate ( reversed ( selector_components ) ) : \n        if len ( node ) == 1 : \n            if start is None : \n                start = - i \n        else : \n            start = None \n        node = node [ component ] \n    if 1 < len ( node ) : \n        return complete_selector \n    return '.' . join ( selector_components [ start : ] ) "}
{"3069": "\ndef get_thing ( self , idx ) : \n    try : \n        idx = int ( idx ) \n    except ValueError : \n        return None \n    if 0 > idx or len ( self . things ) <= idx : \n        return None \n    return self . things [ idx ] "}
{"3109": "\ndef update ( self , ** fields ) : \n    self . _for_write = True \n    if ( 2 , 0 ) <= django . VERSION : \n        query = self . query . chain ( UpdateQuery ) \n    else : \n        query = self . query . clone ( UpdateQuery ) \n    query . _annotations = None \n    query . add_update_values ( fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresReturningUpdateCompiler ( query , connection , self . db ) \n    with transaction . atomic ( using = self . db , savepoint = False ) : \n        rows = compiler . execute_sql ( CURSOR ) \n    self . _result_cache = None \n    for row in rows : \n        signals . update . send ( self . model , pk = row [ 0 ] ) \n    return len ( rows ) "}
{"3129": "\ndef create_sql ( self , model , schema_editor , using = '' ) : \n    if ( 2 , 0 ) <= django . VERSION : \n        statement = super ( ) . create_sql ( model , schema_editor , using ) \n        statement . template = self . sql_create_index \n        statement . parts [ 'condition' ] = self . condition \n        return statement \n    else : \n        sql_create_index = self . sql_create_index \n        sql_parameters = { ** Index . get_sql_create_template_values ( self , model , schema_editor , using ) , 'condition' : self . condition } \n        return sql_create_index % sql_parameters "}
{"3150": "\ndef tdist95conf_level ( df ) : \n    df = int ( round ( df ) ) \n    highest_table_df = len ( _T_DIST_95_CONF_LEVELS ) \n    if 200 <= df : \n        return 1.960 \n    if 100 <= df : \n        return 1.984 \n    if 80 <= df : \n        return 1.990 \n    if 60 <= df : \n        return 2.000 \n    if 50 <= df : \n        return 2.009 \n    if 40 <= df : \n        return 2.021 \n    if highest_table_df <= df : \n        return _T_DIST_95_CONF_LEVELS [ highest_table_df - 1 ] \n    return _T_DIST_95_CONF_LEVELS [ df ] "}
{"3153": "\ndef is_significant ( sample1 , sample2 ) : \n    deg_freedom = len ( sample1 ) + len ( sample2 ) - 2 \n    critical_value = tdist95conf_level ( deg_freedom ) \n    t_score = tscore ( sample1 , sample2 ) \n    return ( critical_value <= abs ( t_score ) , t_score ) "}
{"3159": "\ndef filter_benchmarks ( benchmarks , bench_funcs , base_ver ) : \n    for bm in list ( benchmarks ) : \n        func = bench_funcs [ bm ] \n        if getattr ( func , '_python2_only' , False ) and base_ver >= ( 3 , 0 ) : \n            benchmarks . discard ( bm ) \n            logging . info ( \"Skipping Python2-only benchmark %s; \" \"not compatible with Python %s\" % ( bm , base_ver ) ) \n            continue \n    return benchmarks "}
{"3164": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) \n    fetching = True \n    page = 0 \n    nposts = 0 \n    since = int ( from_date . timestamp ( ) * 1000 ) \n    while fetching : \n        raw_posts = self . client . posts ( self . channel , page = page ) \n        posts_before = nposts \n        for post in self . _parse_posts ( raw_posts ) : \n            if since > post [ 'update_at' ] : \n                fetching = False \n                break \n            user_id = post [ 'user_id' ] \n            user = self . _get_or_fetch_user ( user_id ) \n            post [ 'user_data' ] = user \n            yield post \n            nposts += 1 \n        if fetching : \n            if posts_before == nposts : \n                fetching = False \n            else : \n                page += 1 \n    logger . info ( \"Fetch process completed: %s posts fetched\" , nposts ) "}
{"3186": "\ndef calculate_time_to_reset ( self ) : \n    time_to_reset = self . rate_limit_reset_ts - ( datetime_utcnow ( ) . replace ( microsecond = 0 ) . timestamp ( ) + 1 ) \n    if 0 > time_to_reset : \n        time_to_reset = 0 \n    return time_to_reset "}
{"3206": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    logger . info ( \"Fetching events of '%s' group from %s to %s\" , self . group , str ( from_date ) , str ( to_date ) if to_date else '--' ) \n    to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None \n    nevents = 0 \n    stop_fetching = False \n    ev_pages = self . client . events ( self . group , from_date = from_date ) \n    for evp in ev_pages : \n        events = [ event for event in self . parse_json ( evp ) ] \n        for event in events : \n            event_id = event [ 'id' ] \n            event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) \n            event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) \n            event_ts = self . metadata_updated_on ( event ) \n            if to_date_ts and to_date_ts <= event_ts : \n                stop_fetching = True \n                continue \n            yield event \n            nevents += 1 \n        if stop_fetching : \n            break \n    logger . info ( \"Fetch process completed: %s events fetched\" , nevents ) "}
{"3222": "\ndef _fetch_gerrit28 ( self , from_date = DEFAULT_DATETIME ) : \n    from_ut = datetime_to_utc ( from_date ) \n    from_ut = from_ut . timestamp ( ) \n    filter_open = \"status:open\" \n    filter_closed = \"status:closed\" \n    last_item_open = self . client . next_retrieve_group_item ( ) \n    last_item_closed = self . client . next_retrieve_group_item ( ) \n    reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n    reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n    last_nreviews_open = len ( reviews_open ) \n    last_nreviews_closed = len ( reviews_closed ) \n    while reviews_open or reviews_closed : \n        if reviews_open and reviews_closed : \n            if reviews_closed [ 0 ] [ 'lastUpdated' ] <= reviews_open [ 0 ] [ 'lastUpdated' ] : \n                review_open = reviews_open . pop ( 0 ) \n                review = review_open \n            else : \n                review_closed = reviews_closed . pop ( 0 ) \n                review = review_closed \n        elif reviews_closed : \n            review_closed = reviews_closed . pop ( 0 ) \n            review = review_closed \n        else : \n            review_open = reviews_open . pop ( 0 ) \n            review = review_open \n        updated = review [ 'lastUpdated' ] \n        if from_ut >= updated : \n            logger . debug ( \"No more updates for %s\" % ( self . hostname ) ) \n            break \n        else : \n            yield review \n        if not reviews_open and self . max_reviews <= last_nreviews_open : \n            last_item_open = self . client . next_retrieve_group_item ( last_item_open , review_open ) \n            reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n            last_nreviews_open = len ( reviews_open ) \n        if not reviews_closed and self . max_reviews <= last_nreviews_closed : \n            last_item_closed = self . client . next_retrieve_group_item ( last_item_closed , review_closed ) \n            reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n            last_nreviews_closed = len ( reviews_closed ) "}
{"3225": "\ndef next_retrieve_group_item ( self , last_item = None , entry = None ) : \n    next_item = None \n    gerrit_version = self . version \n    if gerrit_version [ 0 ] == 2 and 9 < gerrit_version [ 1 ] : \n        if last_item is None : \n            next_item = 0 \n        else : \n            next_item = last_item \n    elif gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] == 9 : \n        cause = \"Gerrit 2.9.0 does not support pagination\" \n        raise BackendError ( cause = cause ) \n    else : \n        if entry is not None : \n            next_item = entry [ 'sortKey' ] \n    return next_item "}
{"3228": "\ndef __execute_from_remote ( self , cmd ) : \n    result = None \n    retries = 0 \n    while self . MAX_RETRIES > retries : \n        try : \n            result = subprocess . check_output ( cmd , shell = True ) \n            break \n        except subprocess . CalledProcessError as ex : \n            logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) \n            time . sleep ( self . RETRY_WAIT * retries ) \n            retries += 1 \n    if result is None : \n        result = RuntimeError ( cmd + \" failed \" + str ( self . MAX_RETRIES ) + \" times. Giving up!\" ) \n    if self . archive : \n        cmd = self . sanitize_for_archive ( cmd ) \n        self . archive . store ( cmd , None , None , result ) \n    if isinstance ( result , RuntimeError ) : \n        raise result \n    return result "}
{"3259": "\ndef _fetch_and_parse_messages ( self , mailing_list , from_date ) : \n    from_date = datetime_to_utc ( from_date ) \n    nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) \n    for mbox in mailing_list . mboxes : \n        tmp_path = None \n        try : \n            tmp_path = self . _copy_mbox ( mbox ) \n            for message in self . parse_mbox ( tmp_path ) : \n                tmsgs += 1 \n                if not self . _validate_message ( message ) : \n                    imsgs += 1 \n                    continue \n                dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) \n                if from_date > dt : \n                    logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from_date ) ) \n                    tmsgs -= 1 \n                    continue \n                message = self . _casedict_to_dict ( message ) \n                nmsgs += 1 \n                logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) \n                yield message \n        except ( OSError , EOFError ) as e : \n            logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) \n        except Exception as e : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n            raise e \n        finally : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n    logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs ) "}
{"3278": "\ndef _fetch_pack ( self ) : \n    def prepare_refs ( refs ) : \n        return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] \n    def determine_wants ( refs ) : \n        remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) \n        local_refs = prepare_refs ( self . _discover_refs ( ) ) \n        wants = [ ref for ref in remote_refs if ref not in local_refs ] \n        return wants \n    client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) \n    repo = dulwich . repo . Repo ( self . dirpath ) \n    fd = io . BytesIO ( ) \n    local_refs = self . _discover_refs ( ) \n    graph_walker = _GraphWalker ( local_refs ) \n    result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) \n    refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] \n    if 0 < len ( fd . getvalue ( ) ) : \n        fd . seek ( 0 ) \n        pack = repo . object_store . add_thin_pack ( fd . read , None ) \n        pack_name = pack . name ( ) . decode ( 'utf-8' ) \n    else : \n        pack_name = None \n    return ( pack_name , refs ) "}
{"3296": "\ndef __fetch_pull_requests ( self , from_date , to_date ) : \n    raw_pulls = self . client . pulls ( from_date = from_date ) \n    for raw_pull in raw_pulls : \n        pull = json . loads ( raw_pull ) \n        if to_date < str_to_datetime ( pull [ 'updated_at' ] ) : \n            return \n        self . __init_extra_pull_fields ( pull ) \n        for field in TARGET_PULL_FIELDS : \n            if not pull [ field ] : \n                continue \n            if field == 'user' : \n                pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) \n            elif field == 'merged_by' : \n                pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) \n            elif field == 'review_comments' : \n                pull [ field + '_data' ] = self . __get_pull_review_comments ( pull [ 'number' ] ) \n            elif field == 'requested_reviewers' : \n                pull [ field + '_data' ] = self . __get_pull_requested_reviewers ( pull [ 'number' ] ) \n            elif field == 'commits' : \n                pull [ field + '_data' ] = self . __get_pull_commits ( pull [ 'number' ] ) \n        yield pull "}
{"3316": "\ndef _choose_best_api_token ( self ) : \n    if self . n_tokens == 0 : \n        return \n    token_idx = 0 \n    if 1 < self . n_tokens : \n        remainings = self . _get_tokens_rate_limits ( ) \n        token_idx = remainings . index ( max ( remainings ) ) \n        logger . debug ( \"Remaining API points: {}, choosen index: {}\" . format ( remainings , token_idx ) ) \n    self . current_token = self . tokens [ token_idx ] \n    self . session . headers . update ( { 'Authorization' : 'token ' + self . current_token } ) \n    self . _update_current_rate_limit ( ) "}
{"3317": "\ndef _need_check_tokens ( self ) : \n    if 1 >= self . n_tokens or self . rate_limit is None : \n        return False \n    elif self . last_rate_limit_checked is None : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 \n    if approaching_limit >= self . rate_limit : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) \n    if 1.0 - TOKEN_USAGE_BEFORE_SWITCH > ratio : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    elif 1.0 < ratio : \n        self . last_rate_limit_checked = self . rate_limit \n        return False \n    else : \n        return False "}
{"3324": "\ndef _verify_archive ( self ) : \n    nentries = self . _count_table_rows ( self . ARCHIVE_TABLE ) \n    nmetadata = self . _count_table_rows ( self . METADATA_TABLE ) \n    if 1 < nmetadata : \n        msg = \"archive %s metadata corrupted; multiple metadata entries\" % ( self . archive_path ) \n        raise ArchiveError ( cause = msg ) \n    if nmetadata == 0 and 0 < nentries : \n        msg = \"archive %s metadata is empty but %s entries were achived\" % ( self . archive_path ) \n        raise ArchiveError ( cause = msg ) \n    logger . debug ( \"Integrity of archive %s OK; entries: %s rows, metadata: %s rows\" , self . archive_path , nentries , nmetadata ) "}
{"3330": "\ndef _search_archives ( self , origin , backend_name , category , archived_after ) : \n    for archive_path in self . _search_files ( ) : \n        try : \n            archive = Archive ( archive_path ) \n        except ArchiveError : \n            continue \n        match = archive . origin == origin and archive . backend_name == backend_name and archive . category == category and archived_after <= archive . created_on \n        if not match : \n            continue \n        yield archive_path , archive . created_on "}
{"3335": "\ndef remove_invalid_xml_chars ( raw_xml ) : \n    illegal_unichrs = [ ( 0x00 , 0x08 ) , ( 0x0B , 0x1F ) , ( 0x7F , 0x84 ) , ( 0x86 , 0x9F ) ] \n    illegal_ranges = [ '%s-%s' % ( chr ( low ) , chr ( high ) ) for ( low , high ) in illegal_unichrs if sys . maxunicode > low ] \n    illegal_xml_re = re . compile ( '[%s]' % '' . join ( illegal_ranges ) ) \n    purged_xml = '' \n    for c in raw_xml : \n        if illegal_xml_re . search ( c ) is not None : \n            c = ' ' \n        purged_xml += c \n    return purged_xml "}
{"3348": "\ndef get_items ( self , from_date , url , expand_fields = True ) : \n    start_at = 0 \n    req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n    issues = req . text \n    data = req . json ( ) \n    titems = data [ 'total' ] \n    nitems = data [ 'maxResults' ] \n    start_at += min ( nitems , titems ) \n    self . __log_status ( start_at , titems , url ) \n    while issues : \n        yield issues \n        issues = None \n        if titems > data [ 'startAt' ] + nitems : \n            req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n            data = req . json ( ) \n            start_at += nitems \n            issues = req . text \n            self . __log_status ( start_at , titems , url ) "}
{"3358": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    reviews_api = kwargs [ 'reviews_api' ] \n    mediawiki_version = self . client . get_version ( ) \n    logger . info ( \"MediaWiki version: %s\" , mediawiki_version ) \n    if reviews_api : \n        if ( ( mediawiki_version [ 0 ] == 1 and 27 <= mediawiki_version [ 1 ] ) or 1 < mediawiki_version [ 0 ] ) : \n            fetcher = self . __fetch_1_27 ( from_date ) \n        else : \n            logger . warning ( \"Reviews API only available in MediaWiki >= 1.27\" ) \n            logger . warning ( \"Using the Pages API instead\" ) \n            fetcher = self . __fetch_pre1_27 ( from_date ) \n    else : \n        fetcher = self . __fetch_pre1_27 ( from_date ) \n    for page_reviews in fetcher : \n        yield page_reviews "}
{"3359": "\ndef __get_max_date ( self , reviews ) : \n    max_ts = 0 \n    for review in reviews : \n        ts = str_to_datetime ( review [ 'timestamp' ] ) \n        ts = datetime_to_utc ( ts ) \n        if max_ts < ts . timestamp ( ) : \n            max_ts = ts . timestamp ( ) \n    return max_ts "}
{"3367": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Fetching articles of '%s' group on '%s' offset %s\" , self . group , self . host , str ( offset ) ) \n    narts , iarts , tarts = ( 0 , 0 , 0 ) \n    _ , _ , first , last , _ = self . client . group ( self . group ) \n    if last >= offset : \n        first = max ( first , offset ) \n        _ , overview = self . client . over ( ( first , last ) ) \n    else : \n        overview = [ ] \n    tarts = len ( overview ) \n    logger . debug ( \"Total number of articles to fetch: %s\" , tarts ) \n    for article_id , _ in overview : \n        try : \n            article_raw = self . client . article ( article_id ) \n            article = self . __parse_article ( article_raw ) \n        except ParseError : \n            logger . warning ( \"Error parsing %s article; skipping\" , article_id ) \n            iarts += 1 \n            continue \n        except nntplib . NNTPTemporaryError as e : \n            logger . warning ( \"Error '%s' fetching article %s; skipping\" , e . response , article_id ) \n            iarts += 1 \n            continue \n        yield article \n        narts += 1 "}
{"3375": "\ndef setup_rate_limit_handler ( self , sleep_for_rate = False , min_rate_to_sleep = MIN_RATE_LIMIT , rate_limit_header = RATE_LIMIT_HEADER , rate_limit_reset_header = RATE_LIMIT_RESET_HEADER ) : \n    self . rate_limit = None \n    self . rate_limit_reset_ts = None \n    self . sleep_for_rate = sleep_for_rate \n    self . rate_limit_header = rate_limit_header \n    self . rate_limit_reset_header = rate_limit_reset_header \n    if self . MAX_RATE_LIMIT < min_rate_to_sleep : \n        msg = \"Minimum rate to sleep value exceeded (%d).\" \n        msg += \"High values might cause the client to sleep forever.\" \n        msg += \"Reset to %d.\" \n        self . min_rate_to_sleep = self . MAX_RATE_LIMIT \n        logger . warning ( msg , min_rate_to_sleep , self . MAX_RATE_LIMIT ) \n    else : \n        self . min_rate_to_sleep = min_rate_to_sleep "}
{"3376": "\ndef sleep_for_rate_limit ( self ) : \n    if self . rate_limit is not None and self . min_rate_to_sleep >= self . rate_limit : \n        seconds_to_reset = self . calculate_time_to_reset ( ) \n        if 0 > seconds_to_reset : \n            logger . warning ( \"Value of sleep for rate limit is negative, reset it to 0\" ) \n            seconds_to_reset = 0 \n        cause = \"Rate limit exhausted.\" \n        if self . sleep_for_rate : \n            logger . info ( \"%s Waiting %i secs for rate limit reset.\" , cause , seconds_to_reset ) \n            time . sleep ( seconds_to_reset ) \n        else : \n            raise RateLimitError ( cause = cause , seconds_to_reset = seconds_to_reset ) "}
{"3379": "\ndef __retrieve_archives ( self , from_date ) : \n    archives = [ ] \n    candidates = self . __list_supybot_archives ( ) \n    for candidate in candidates : \n        dt = self . __parse_date_from_filepath ( candidate ) \n        if from_date . date ( ) <= dt . date ( ) : \n            archives . append ( ( dt , candidate ) ) \n        else : \n            logger . debug ( \"Archive %s stored before %s; skipped\" , candidate , str ( from_date ) ) \n    archives . sort ( key = lambda x : x [ 0 ] ) \n    return [ archive [ 1 ] for archive in archives ] "}
{"3468": "\ndef erfcc ( x ) : \n    z = abs ( x ) \n    t = 1 / ( 1 + 0.5 * z ) \n    r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) \n    if ( 0. <= x ) : \n        return r \n    else : \n        return 2. - r "}
{"3477": "\ndef atomic_sa ( self , i ) : \n    sa = 4.0 * np . pi * self . rads2 [ i ] \n    neighbors = self . neighbors . get ( i ) \n    if neighbors is None : \n        return sa \n    XYZi = self . xyzs [ i , np . newaxis ] . T \n    sphere = self . sphere * self . rads [ i ] + XYZi \n    N = sphere . shape [ 1 ] \n    for j , _ in neighbors : \n        XYZj = self . xyzs [ j , np . newaxis ] . T \n        d2 = ( sphere - XYZj ) ** 2 \n        mask = self . rads2 [ j ] < ( d2 [ 0 ] + d2 [ 1 ] + d2 [ 2 ] ) \n        sphere = np . compress ( mask , sphere , axis = 1 ) \n    return sa * sphere . shape [ 1 ] / N "}
{"3501": "\ndef processed ( self ) : \n    self . processed_tasks += 1 \n    qsize = self . tasks . qsize ( ) \n    if 0 < qsize : \n        progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed_tasks , qsize , len ( self . workers ) ) \n    else : \n        progress ( '[%d task(s) completed, %d thread(s)]' , self . processed_tasks , len ( self . workers ) ) "}
{"3517": "\ndef get_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if os . path . isdir ( target ) : \n        for src in source : \n            self . get_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) ) \n    else : \n        if 1 < len ( source ) : \n            raise Failure ( 'Target \"%s\" is not a directory.' % target ) \n        elif len ( source ) == 1 : \n            self . get_single_file ( pool , source [ 0 ] , target ) \n        else : \n            pass \n    pool . join ( ) "}
{"3519": "\ndef cp_files ( self , source , target , delete_source = False ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if target [ - 1 ] == PATH_SEP : \n        for src in source : \n            self . cp_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) , delete_source ) \n    else : \n        if 1 < len ( source ) : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n        elif len ( source ) == 1 : \n            self . cp_single_file ( pool , source [ 0 ] , target , delete_source ) \n        else : \n            pass \n    pool . join ( ) "}
{"3527": "\ndef partial_match ( self , path , filter_path ) : \n    if not path or not filter_path : \n        return True \n    if path [ - 1 ] == PATH_SEP : \n        path = path [ 0 : - 1 ] \n    if filter_path [ - 1 ] == PATH_SEP : \n        filter_path += '*' \n    pi = path . split ( PATH_SEP ) \n    fi = filter_path . split ( PATH_SEP ) \n    min_len = min ( len ( pi ) , len ( fi ) ) \n    matched = fnmatch . fnmatch ( PATH_SEP . join ( pi [ 0 : min_len ] ) , PATH_SEP . join ( fi [ 0 : min_len ] ) ) \n    return matched and ( self . opt . recursive or len ( fi ) >= len ( pi ) ) "}
{"3529": "\ndef conditional ( self , result , obj ) : \n    fileonly = ( self . opt . last_modified_before is not None ) or ( self . opt . last_modified_after is not None ) \n    if obj [ 'is_dir' ] : \n        if not fileonly : \n            result . append ( obj ) \n        return \n    if ( self . opt . last_modified_before is not None ) and self . opt . last_modified_before <= obj [ 'last_modified' ] : \n        return \n    if ( self . opt . last_modified_after is not None ) and self . opt . last_modified_after >= obj [ 'last_modified' ] : \n        return \n    result . append ( obj ) "}
{"3533": "\ndef upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : \n    s3url = S3URL ( target ) \n    obj = self . lookup ( s3url ) \n    if not mpi : \n        fsize = os . path . getsize ( source ) \n        md5cache = LocalMD5Cache ( source ) \n        if self . opt . dry_run : \n            message ( '%s => %s' , source , target ) \n            return \n        elif self . opt . sync_check and self . sync_check ( md5cache , obj ) : \n            message ( '%s => %s (synced)' , source , target ) \n            return \n        elif not self . opt . force and obj : \n            raise Failure ( 'File already exists: %s' % target ) \n        if self . opt . max_singlepart_upload_size > fsize : \n            data = self . read_file_chunk ( source , 0 , fsize ) \n            self . s3 . put_object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n            message ( '%s => %s' , source , target ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . upload ( * args ) \n        return \n    data = self . read_file_chunk ( source , pos , chunk ) \n    response = self . s3 . upload_part ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , Body = data , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            message ( '%s => %s' , source , target ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Upload failed: Unable to complete upload %s.' % source ) "}
{"3536": "\ndef copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete_source = False ) : \n    if self . opt . dry_run : \n        message ( '%s => %s' % ( source , target ) ) \n        return \n    source_url = S3URL ( source ) \n    target_url = S3URL ( target ) \n    if not mpi : \n        obj = self . lookup ( source_url ) \n        fsize = int ( obj [ 'ContentLength' ] ) \n        if self . opt . max_singlepart_copy_size > fsize : \n            self . s3 . copy_object ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } ) \n            message ( '%s => %s' % ( source , target ) ) \n            if delete_source : \n                self . delete ( source ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , Metadata = obj [ 'Metadata' ] ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . copy ( * args , delete_source = delete_source ) \n        return \n    response = self . s3 . upload_part_copy ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } , CopySourceRange = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , UploadId = mpi . id , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'CopyPartResult' ] [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            if delete_source : \n                self . delete ( source ) \n            message ( '%s => %s' % ( source , target ) ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = source_url . bucket , Key = source_url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Copy failed: Unable to complete copy %s.' % source ) "}
{"3539": "\ndef pretty_print ( self , objlist ) : \n    def normalize_time ( timestamp ) : \n        if timestamp is None : \n            return ' ' * 16 \n        return TIMESTAMP_FORMAT % ( timestamp . year , timestamp . month , timestamp . day , timestamp . hour , timestamp . minute ) \n    cwidth = [ 0 , 0 , 0 ] \n    format = '%%%ds %%%ds %%-%ds' \n    result = [ ] \n    for obj in objlist : \n        last_modified = normalize_time ( obj [ 'last_modified' ] ) \n        size = str ( obj [ 'size' ] ) if not obj [ 'is_dir' ] else 'DIR' \n        name = obj [ 'name' ] \n        item = ( last_modified , size , name ) \n        for i , value in enumerate ( item ) : \n            if len ( value ) > cwidth [ i ] : \n                cwidth [ i ] = len ( value ) \n        result . append ( item ) \n    for item in result : \n        text = ( format % tuple ( cwidth ) ) % item \n        message ( '%s' , text . rstrip ( ) ) "}
{"3542": "\ndef put_handler ( self , args ) : \n    if 3 > len ( args ) : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) \n    source = args [ 1 : - 1 ] \n    target = args [ - 1 ] \n    self . s3handler ( ) . put_files ( source , target ) "}
{"3578": "\ndef decompose ( hangul_letter ) : \n    from . import checker \n    if 1 > len ( hangul_letter ) : \n        raise NotLetterException ( '' ) \n    elif not checker . is_hangul ( hangul_letter ) : \n        raise NotHangulException ( '' ) \n    if hangul_letter in CHO : \n        return hangul_letter , '' , '' \n    if hangul_letter in JOONG : \n        return '' , hangul_letter , '' \n    if hangul_letter in JONG : \n        return '' , '' , hangul_letter \n    code = hangul_index ( hangul_letter ) \n    cho , joong , jong = decompose_index ( code ) \n    if 0 > cho : \n        cho = 0 \n    try : \n        return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] \n    except : \n        print ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) \n        print ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) \n        raise Exception ( ) "}
{"3579": "\ndef has_jongsung ( letter ) : \n    if len ( letter ) != 1 : \n        raise Exception ( 'The target string must be one letter.' ) \n    if not is_hangul ( letter ) : \n        raise NotHangulException ( 'The target string must be Hangul' ) \n    code = lt . hangul_index ( letter ) \n    return 0 < code % NUM_JONG "}
{"3604": "\ndef node_type ( node : astroid . node_classes . NodeNG ) -> Optional [ type ] : \n    types = set ( ) \n    try : \n        for var_type in node . infer ( ) : \n            if var_type == astroid . Uninferable or is_none ( var_type ) : \n                continue \n            types . add ( var_type ) \n            if 1 < len ( types ) : \n                return None \n    except astroid . InferenceError : \n        return None \n    return types . pop ( ) if types else None "}
{"3611": "\ndef visit_import ( self , node ) : \n    self . _check_reimport ( node ) \n    self . _check_import_as_rename ( node ) \n    modnode = node . root ( ) \n    names = [ name for name , _ in node . names ] \n    if 2 <= len ( names ) : \n        self . add_message ( \"multiple-imports\" , args = \", \" . join ( names ) , node = node ) \n    for name in names : \n        self . _check_deprecated_module ( node , name ) \n        self . _check_preferred_module ( node , name ) \n        imported_module = self . _get_imported_module ( node , name ) \n        if isinstance ( node . parent , astroid . Module ) : \n            self . _check_position ( node ) \n        if isinstance ( node . scope ( ) , astroid . Module ) : \n            self . _record_import ( node , imported_module ) \n        if imported_module is None : \n            continue \n        self . _check_relative_import ( modnode , node , imported_module , name ) \n        self . _add_imported_module ( node , imported_module . name ) "}
{"3614": "\ndef _record_import ( self , node , importedmodnode ) : \n    if isinstance ( node , astroid . ImportFrom ) : \n        importedname = node . modname \n    else : \n        importedname = importedmodnode . name if importedmodnode else None \n    if not importedname : \n        importedname = node . names [ 0 ] [ 0 ] . split ( \".\" ) [ 0 ] \n    if isinstance ( node , astroid . ImportFrom ) and 1 <= ( node . level or 0 ) : \n        importedname = \".\" + importedname \n    self . _imports_stack . append ( ( node , importedname ) ) "}
{"3642": "\ndef class_diagram ( self , project , klass ) : \n    self . classdiagram = ClassDiagram ( klass , self . config . mode ) \n    if 1 < len ( project . modules ) : \n        module , klass = klass . rsplit ( \".\" , 1 ) \n        module = project . get_module ( module ) \n    else : \n        module = project . modules [ 0 ] \n        klass = klass . split ( \".\" ) [ - 1 ] \n    klass = next ( module . ilookup ( klass ) ) \n    anc_level , association_level = self . _get_levels ( ) \n    self . extract_classes ( klass , anc_level , association_level ) \n    return self . classdiagram "}
{"3645": "\ndef _similar_names ( owner , attrname , distance_threshold , max_choices ) : \n    possible_names = [ ] \n    names = _node_names ( owner ) \n    for name in names : \n        if name == attrname : \n            continue \n        distance = _string_distance ( attrname , name ) \n        if distance_threshold >= distance : \n            possible_names . append ( ( name , distance ) ) \n    picked = [ name for ( name , _ ) in heapq . nsmallest ( max_choices , possible_names , key = operator . itemgetter ( 1 ) ) ] \n    return sorted ( picked ) "}
{"3661": "\ndef visit_importfrom ( self , node ) : \n    basename = node . modname \n    context_file = node . root ( ) . file \n    if context_file is not None : \n        relative = modutils . is_relative ( basename , context_file ) \n    else : \n        relative = False \n    for name in node . names : \n        if name [ 0 ] == \"*\" : \n            continue \n        fullname = \"%s.%s\" % ( basename , name [ 0 ] ) \n        if - 1 < fullname . find ( \".\" ) : \n            try : \n                fullname = modutils . get_module_part ( fullname , context_file ) \n            except ImportError : \n                continue \n        if fullname != basename : \n            self . _imported_module ( node , fullname , relative ) "}
{"3673": "\ndef _check_new_format ( self , node , func ) : \n    if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : \n        return \n    if node . starargs or node . kwargs : \n        return \n    try : \n        strnode = next ( func . bound . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : \n        return \n    try : \n        call_site = CallSite . from_call ( node ) \n    except astroid . InferenceError : \n        return \n    try : \n        fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) \n    except utils . IncompleteFormatString : \n        self . add_message ( \"bad-format-string\" , node = node ) \n        return \n    positional_arguments = call_site . positional_arguments \n    named_arguments = call_site . keyword_arguments \n    named_fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } \n    if num_args and manual_pos : \n        self . add_message ( \"format-combined-specification\" , node = node ) \n        return \n    check_args = False \n    num_args += sum ( 1 for field in named_fields if field == \"\" ) \n    if named_fields : \n        for field in named_fields : \n            if field and field not in named_arguments : \n                self . add_message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) \n        for field in named_arguments : \n            if field not in named_fields : \n                self . add_message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) \n        num_args = num_args or manual_pos \n        if positional_arguments or num_args : \n            empty = any ( True for field in named_fields if field == \"\" ) \n            if named_arguments or empty : \n                check_args = True \n    else : \n        check_args = True \n    if check_args : \n        num_args = num_args or manual_pos \n        if num_args < len ( positional_arguments ) : \n            self . add_message ( \"too-many-format-args\" , node = node ) \n        elif num_args > len ( positional_arguments ) : \n            self . add_message ( \"too-few-format-args\" , node = node ) \n    self . _detect_vacuous_formatting ( node , positional_arguments ) \n    self . _check_new_format_specifiers ( node , fields , named_arguments ) "}
{"3696": "\ndef register_checker ( self , checker ) : \n    assert 0 >= checker . priority , \"checker priority can't be >= 0\" \n    self . _checkers [ checker . name ] . append ( checker ) \n    for r_id , r_title , r_cb in checker . reports : \n        self . register_report ( r_id , r_title , r_cb , checker ) \n    self . register_options_provider ( checker ) \n    if hasattr ( checker , \"msgs\" ) : \n        self . msgs_store . register_messages_from_checker ( checker ) \n    checker . load_defaults ( ) \n    if not getattr ( checker , \"enabled\" , True ) : \n        self . disable ( checker . name ) "}
{"3723": "\ndef get_table_content ( self , table ) : \n    result = [ [ ] ] \n    cols = table . cols \n    for cell in self . compute_content ( table ) : \n        if cols == 0 : \n            result . append ( [ ] ) \n            cols = table . cols \n        cols -= 1 \n        result [ - 1 ] . append ( cell ) \n    while cols > len ( result [ - 1 ] ) : \n        result [ - 1 ] . append ( \"\" ) \n    return result "}
{"3741": "\ndef _detect_global_scope ( node , frame , defframe ) : \n    def_scope = scope = None \n    if frame and frame . parent : \n        scope = frame . parent . scope ( ) \n    if defframe and defframe . parent : \n        def_scope = defframe . parent . scope ( ) \n    if isinstance ( frame , astroid . FunctionDef ) : \n        if not isinstance ( node . parent , ( astroid . FunctionDef , astroid . Arguments ) ) : \n            return False \n    elif any ( not isinstance ( f , ( astroid . ClassDef , astroid . Module ) ) for f in ( frame , defframe ) ) : \n        return False \n    break_scopes = [ ] \n    for s in ( scope , def_scope ) : \n        parent_scope = s \n        while parent_scope : \n            if not isinstance ( parent_scope , ( astroid . ClassDef , astroid . Module ) ) : \n                break_scopes . append ( parent_scope ) \n                break \n            if parent_scope . parent : \n                parent_scope = parent_scope . parent . scope ( ) \n            else : \n                break \n    if break_scopes and len ( set ( break_scopes ) ) != 1 : \n        return False \n    return defframe . lineno > frame . lineno "}
{"3751": "\ndef run ( self ) : \n    install_lib . install_lib . run ( self ) \n    if include_dirs : \n        for directory in include_dirs : \n            dest = join ( self . install_dir , directory ) \n            if ( 3 , 0 ) <= sys . version_info : \n                exclude = { \"invalid_encoded_data*\" , \"unknown_encoding*\" } \n            else : \n                exclude = set ( ) \n            shutil . rmtree ( dest , ignore_errors = True ) \n            shutil . copytree ( directory , dest , ignore = shutil . ignore_patterns ( * exclude ) ) "}
{"3757": "\ndef _find_common ( self , lineset1 , lineset2 ) : \n    lines1 = lineset1 . enumerate_stripped \n    lines2 = lineset2 . enumerate_stripped \n    find = lineset2 . find \n    index1 = 0 \n    min_lines = self . min_lines \n    while len ( lineset1 ) > index1 : \n        skip = 1 \n        num = 0 \n        for index2 in find ( lineset1 [ index1 ] ) : \n            non_blank = 0 \n            for num , ( ( _ , line1 ) , ( _ , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : \n                if line1 != line2 : \n                    if min_lines < non_blank : \n                        yield num , lineset1 , index1 , lineset2 , index2 \n                    skip = max ( skip , num ) \n                    break \n                if line1 : \n                    non_blank += 1 \n            else : \n                num += 1 \n                if min_lines < non_blank : \n                    yield num , lineset1 , index1 , lineset2 , index2 \n                skip = max ( skip , num ) \n        index1 += skip "}
{"3775": "\ndef _check_accessed_members ( self , node , accessed ) : \n    excs = ( \"AttributeError\" , \"Exception\" , \"BaseException\" ) \n    for attr , nodes in accessed . items ( ) : \n        try : \n            node . local_attr ( attr ) \n            continue \n        except astroid . NotFoundError : \n            pass \n        try : \n            next ( node . instance_attr_ancestors ( attr ) ) \n            continue \n        except StopIteration : \n            pass \n        try : \n            defstmts = node . instance_attr ( attr ) \n        except astroid . NotFoundError : \n            pass \n        else : \n            defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] \n            if not defstmts : \n                continue \n            scope = defstmts [ 0 ] . scope ( ) \n            defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if i == 0 or stmt . scope ( ) is not scope ] \n            if len ( defstmts ) == 1 : \n                defstmt = defstmts [ 0 ] \n                frame = defstmt . frame ( ) \n                lno = defstmt . fromlineno \n                for _node in nodes : \n                    if ( _node . frame ( ) is frame and lno > _node . fromlineno and not astroid . are_exclusive ( _node . statement ( ) , defstmt , excs ) ) : \n                        self . add_message ( \"access-member-before-definition\" , node = _node , args = ( attr , lno ) , ) "}
{"3777": "\ndef _check_signature ( self , method1 , refmethod , class_type , cls ) : \n    if not ( isinstance ( method1 , astroid . FunctionDef ) and isinstance ( refmethod , astroid . FunctionDef ) ) : \n        self . add_message ( \"method-check-failed\" , args = ( method1 , refmethod ) , node = method1 ) \n        return \n    instance = cls . instantiate_class ( ) \n    method1 = function_to_method ( method1 , instance ) \n    refmethod = function_to_method ( refmethod , instance ) \n    if method1 . args . args is None or refmethod . args . args is None : \n        return \n    if is_attr_private ( method1 . name ) : \n        return \n    if method1 . decorators : \n        for decorator in method1 . decorators . nodes : \n            if ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname == \"setter\" ) : \n                return \n    if _different_parameters ( refmethod , method1 , dummy_parameter_regex = self . _dummy_rgx ) : \n        self . add_message ( \"arguments-differ\" , args = ( class_type , method1 . name ) , node = method1 ) \n    elif len ( refmethod . args . defaults ) > len ( method1 . args . defaults ) : \n        self . add_message ( \"signature-differs\" , args = ( class_type , method1 . name ) , node = method1 ) "}
{"3781": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    klass = node . parent . frame ( ) \n    for stmt in node . nodes_of_class ( astroid . Call ) : \n        if node_frame_class ( stmt ) != node_frame_class ( node ) : \n            continue \n        expr = stmt . func \n        if not isinstance ( expr , astroid . Attribute ) : \n            continue \n        call = expr . expr \n        if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == \"super\" ) : \n            continue \n        if not klass . newstyle and has_known_bases ( klass ) : \n            continue \n        else : \n            if not call . args : \n                if sys . version_info [ 0 ] == 3 : \n                    continue \n                else : \n                    self . add_message ( \"missing-super-argument\" , node = call ) \n                    continue \n            arg0 = call . args [ 0 ] \n            if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == \"type\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) \n                continue \n            if ( 2 <= len ( call . args ) and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == \"self\" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == \"__class__\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"self.__class__\" , ) ) \n                continue \n            try : \n                supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) \n            except astroid . InferenceError : \n                continue \n            if klass is not supcls : \n                name = None \n                if supcls : \n                    name = supcls . name \n                elif call . args and hasattr ( call . args [ 0 ] , \"name\" ) : \n                    name = call . args [ 0 ] . name \n                if name : \n                    self . add_message ( \"bad-super-call\" , node = call , args = ( name , ) ) "}
{"3787": "\ndef visit_classdef ( self , node ) : \n    nb_parents = len ( list ( node . ancestors ( ) ) ) \n    if self . config . max_parents < nb_parents : \n        self . add_message ( \"too-many-ancestors\" , node = node , args = ( nb_parents , self . config . max_parents ) , ) \n    if self . config . max_attributes < len ( node . instance_attrs ) : \n        self . add_message ( \"too-many-instance-attributes\" , node = node , args = ( len ( node . instance_attrs ) , self . config . max_attributes ) , ) "}
{"3788": "\ndef leave_classdef ( self , node ) : \n    my_methods = sum ( 1 for method in node . mymethods ( ) if not method . name . startswith ( \"_\" ) ) \n    if self . config . max_public_methods < my_methods : \n        self . add_message ( \"too-many-public-methods\" , node = node , args = ( my_methods , self . config . max_public_methods ) , ) \n    if ( node . type != \"class\" or _is_enum_class ( node ) or _is_dataclass ( node ) or _is_typing_namedtuple ( node ) ) : \n        return \n    all_methods = _count_methods_in_class ( node ) \n    if self . config . min_public_methods > all_methods : \n        self . add_message ( \"too-few-public-methods\" , node = node , args = ( all_methods , self . config . min_public_methods ) , ) "}
{"3789": "\ndef visit_if ( self , node ) : \n    self . _check_boolean_expressions ( node ) \n    branches = 1 \n    if node . orelse and ( 1 < len ( node . orelse ) or not isinstance ( node . orelse [ 0 ] , If ) ) : \n        branches += 1 \n    self . _inc_branch ( node , branches ) \n    self . _inc_all_stmts ( branches ) "}
{"3790": "\ndef _check_boolean_expressions ( self , node ) : \n    condition = node . test \n    if not isinstance ( condition , BoolOp ) : \n        return \n    nb_bool_expr = _count_boolean_expressions ( condition ) \n    if self . config . max_bool_expr < nb_bool_expr : \n        self . add_message ( \"too-many-boolean-expressions\" , node = condition , args = ( nb_bool_expr , self . config . max_bool_expr ) , ) "}
{"3798": "\ndef _check_raising_stopiteration_in_generator_next_call ( self , node ) : \n    def _looks_like_infinite_iterator ( param ) : \n        inferred = utils . safe_infer ( param ) \n        if inferred : \n            return inferred . qname ( ) in KNOWN_INFINITE_ITERATORS \n        return False \n    if isinstance ( node . func , astroid . Attribute ) : \n        return \n    inferred = utils . safe_infer ( node . func ) \n    if getattr ( inferred , \"name\" , \"\" ) == \"next\" : \n        frame = node . frame ( ) \n        has_sentinel_value = 1 < len ( node . args ) \n        if ( isinstance ( frame , astroid . FunctionDef ) and frame . is_generator ( ) and not has_sentinel_value and not utils . node_ignores_exception ( node , StopIteration ) and not _looks_like_infinite_iterator ( node . args [ 0 ] ) ) : \n            self . add_message ( \"stop-iteration-return\" , node = node ) "}
{"3799": "\ndef _check_nested_blocks ( self , node ) : \n    if not isinstance ( node . scope ( ) , astroid . FunctionDef ) : \n        return \n    nested_blocks = self . _nested_blocks [ : ] \n    if node . parent == node . scope ( ) : \n        self . _nested_blocks = [ node ] \n    else : \n        for ancestor_node in reversed ( self . _nested_blocks ) : \n            if ancestor_node == node . parent : \n                break \n            self . _nested_blocks . pop ( ) \n        if isinstance ( node , astroid . If ) and self . _is_actual_elif ( node ) : \n            if self . _nested_blocks : \n                self . _nested_blocks . pop ( ) \n        self . _nested_blocks . append ( node ) \n    if len ( self . _nested_blocks ) < len ( nested_blocks ) : \n        self . _emit_nested_blocks_message_if_needed ( nested_blocks ) "}
{"3802": "\ndef _check_chained_comparison ( self , node ) : \n    if node . op != \"and\" or 2 > len ( node . values ) : \n        return \n    def _find_lower_upper_bounds ( comparison_node , uses ) : \n        left_operand = comparison_node . left \n        for operator , right_operand in comparison_node . ops : \n            for operand in ( left_operand , right_operand ) : \n                value = None \n                if isinstance ( operand , astroid . Name ) : \n                    value = operand . name \n                elif isinstance ( operand , astroid . Const ) : \n                    value = operand . value \n                if value is None : \n                    continue \n                if operator in ( \"<\" , \"<=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n                    elif operand is right_operand : \n                        uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                elif operator in ( \">\" , \">=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                    elif operand is right_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n            left_operand = right_operand \n    uses = collections . defaultdict ( lambda : { \"lower_bound\" : set ( ) , \"upper_bound\" : set ( ) } ) \n    for comparison_node in node . values : \n        if isinstance ( comparison_node , astroid . Compare ) : \n            _find_lower_upper_bounds ( comparison_node , uses ) \n    for _ , bounds in uses . items ( ) : \n        num_shared = len ( bounds [ \"lower_bound\" ] . intersection ( bounds [ \"upper_bound\" ] ) ) \n        num_lower_bounds = len ( bounds [ \"lower_bound\" ] ) \n        num_upper_bounds = len ( bounds [ \"upper_bound\" ] ) \n        if num_lower_bounds > num_shared and num_upper_bounds > num_shared : \n            self . add_message ( \"chained-comparison\" , node = node ) \n            break "}
{"3806": "\ndef visit_for ( self , node ) : \n    if not isinstance ( node . iter , astroid . Call ) : \n        return \n    if not self . _is_builtin ( node . iter . func , \"range\" ) : \n        return \n    if len ( node . iter . args ) == 2 and not _is_constant_zero ( node . iter . args [ 0 ] ) : \n        return \n    if 2 < len ( node . iter . args ) : \n        return \n    if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : \n        return \n    second_func = node . iter . args [ - 1 ] . func \n    if not self . _is_builtin ( second_func , \"len\" ) : \n        return \n    len_args = node . iter . args [ - 1 ] . args \n    if not len_args or len ( len_args ) != 1 : \n        return \n    iterating_object = len_args [ 0 ] \n    if not isinstance ( iterating_object , astroid . Name ) : \n        return \n    scope = node . scope ( ) \n    if iterating_object . name == \"self\" and scope . name == \"__iter__\" : \n        return \n    for child in node . body : \n        for subscript in child . nodes_of_class ( astroid . Subscript ) : \n            if not isinstance ( subscript . value , astroid . Name ) : \n                continue \n            if not isinstance ( subscript . slice , astroid . Index ) : \n                continue \n            if not isinstance ( subscript . slice . value , astroid . Name ) : \n                continue \n            if subscript . slice . value . name != node . target . name : \n                continue \n            if iterating_object . name != subscript . value . name : \n                continue \n            if subscript . value . scope ( ) != node . scope ( ) : \n                continue \n            self . add_message ( \"consider-using-enumerate\" , node = node ) \n            return "}
{"3813": "\ndef may_be_emitted ( self ) : \n    if self . minversion is not None and sys . version_info < self . minversion : \n        return False \n    if self . maxversion is not None and sys . version_info >= self . maxversion : \n        return False \n    return True "}
{"3831": "\ndef _get_indent_hint_line ( bar_positions , bad_position ) : \n    if not bar_positions : \n        return ( \"\" , \"\" ) \n    bar_positions = [ _get_indent_length ( indent ) for indent in bar_positions ] \n    bad_position = _get_indent_length ( bad_position ) \n    delta_message = \"\" \n    markers = [ ( pos , \"|\" ) for pos in bar_positions ] \n    if len ( markers ) == 1 : \n        expected_position = markers [ 0 ] [ 0 ] \n        delta = abs ( expected_position - bad_position ) \n        direction = \"add\" if bad_position < expected_position else \"remove\" \n        delta_message = _CONTINUATION_HINT_MESSAGE % ( direction , delta , \"s\" if 1 < delta else \"\" , ) \n    markers . append ( ( bad_position , \"^\" ) ) \n    markers . sort ( ) \n    line = [ \" \" ] * ( markers [ - 1 ] [ 0 ] + 1 ) \n    for position , marker in markers : \n        line [ position ] = marker \n    return ( \"\" . join ( line ) , delta_message ) "}
{"3833": "\ndef handle_line_start ( self , pos ) : \n    if - 1 < self . _line_start : \n        return \n    check_token_position = pos \n    if self . _tokens . token ( pos ) == _ASYNC_TOKEN : \n        check_token_position += 1 \n    self . _is_block_opener = ( self . _tokens . token ( check_token_position ) in _CONTINUATION_BLOCK_OPENERS ) \n    self . _line_start = pos "}
{"3845": "\ndef check_lines ( self , lines , i ) : \n    max_chars = self . config . max_line_length \n    ignore_long_line = self . config . ignore_long_lines \n    def check_line ( line , i ) : \n        if not line . endswith ( \"\\n\" ) : \n            self . add_message ( \"missing-final-newline\" , line = i ) \n        else : \n            stripped_line = line . rstrip ( \"\\t\\n\\r\\v \" ) \n            if not stripped_line and _EMPTY_LINE in self . config . no_space_check : \n                pass \n            elif line [ len ( stripped_line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : \n                self . add_message ( \"trailing-whitespace\" , line = i , col_offset = len ( stripped_line ) ) \n            line = stripped_line \n        mobj = OPTION_RGX . search ( line ) \n        if mobj and \"=\" in line : \n            front_of_equal , _ , back_of_equal = mobj . group ( 1 ) . partition ( \"=\" ) \n            if front_of_equal . strip ( ) == \"disable\" : \n                if \"line-too-long\" in { _msg_id . strip ( ) for _msg_id in back_of_equal . split ( \",\" ) } : \n                    return None \n                line = line . rsplit ( \"#\" , 1 ) [ 0 ] . rstrip ( ) \n        if max_chars < len ( line ) and not ignore_long_line . search ( line ) : \n            self . add_message ( \"line-too-long\" , line = i , args = ( len ( line ) , max_chars ) ) \n        return i + 1 \n    unsplit_ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } \n    unsplit = [ ] \n    for line in lines . splitlines ( True ) : \n        if line [ - 1 ] in unsplit_ends : \n            unsplit . append ( line ) \n            continue \n        if unsplit : \n            unsplit . append ( line ) \n            line = \"\" . join ( unsplit ) \n            unsplit = [ ] \n        i = check_line ( line , i ) \n        if i is None : \n            break \n    if unsplit : \n        check_line ( \"\" . join ( unsplit ) , i ) "}
{"3847": "\ndef _in_iterating_context ( node ) : \n    parent = node . parent \n    if isinstance ( parent , astroid . For ) : \n        return True \n    if isinstance ( parent , astroid . Comprehension ) : \n        if parent . iter == node : \n            return True \n    elif isinstance ( parent , astroid . Call ) : \n        if isinstance ( parent . func , astroid . Name ) : \n            parent_scope = parent . func . lookup ( parent . func . name ) [ 0 ] \n            if _is_builtin ( parent_scope ) and parent . func . name in _ACCEPTS_ITERATOR : \n                return True \n        elif isinstance ( parent . func , astroid . Attribute ) : \n            if parent . func . attrname in ATTRIBUTES_ACCEPTS_ITERATOR : \n                return True \n        inferred = utils . safe_infer ( parent . func ) \n        if inferred : \n            if inferred . qname ( ) in _BUILTIN_METHOD_ACCEPTS_ITERATOR : \n                return True \n            root = inferred . root ( ) \n            if root and root . name == \"itertools\" : \n                return True \n    elif isinstance ( parent , astroid . Assign ) and isinstance ( parent . targets [ 0 ] , ( astroid . List , astroid . Tuple ) ) : \n        if 1 < len ( parent . targets [ 0 ] . elts ) : \n            return True \n    elif ( isinstance ( parent , astroid . Compare ) and len ( parent . ops ) == 1 and parent . ops [ 0 ] [ 0 ] == \"in\" ) : \n        return True \n    elif isinstance ( parent , astroid . YieldFrom ) : \n        return True \n    if isinstance ( parent , astroid . Starred ) : \n        return True \n    return False "}
{"3852": "\ndef visit_excepthandler ( self , node ) : \n    def _is_used_in_except_block ( node ) : \n        scope = node . scope ( ) \n        current = node \n        while ( current and current != scope and not isinstance ( current , astroid . ExceptHandler ) ) : \n            current = current . parent \n        return isinstance ( current , astroid . ExceptHandler ) and current . type != node \n    if isinstance ( node . name , ( astroid . Tuple , astroid . List ) ) : \n        self . add_message ( \"unpacking-in-except\" , node = node ) \n        return \n    if not node . name : \n        return \n    scope = node . parent . scope ( ) \n    scope_names = scope . nodes_of_class ( astroid . Name , skip_klass = astroid . FunctionDef ) \n    scope_names = list ( scope_names ) \n    potential_leaked_names = [ scope_name for scope_name in scope_names if scope_name . name == node . name . name and node . lineno < scope_name . lineno and not _is_used_in_except_block ( scope_name ) ] \n    reassignments_for_same_name = { assign_name . lineno for assign_name in scope . nodes_of_class ( astroid . AssignName , skip_klass = astroid . FunctionDef ) if assign_name . name == node . name . name } \n    for leaked_name in potential_leaked_names : \n        if any ( node . lineno < elem < leaked_name . lineno for elem in reassignments_for_same_name ) : \n            continue \n        self . add_message ( \"exception-escape\" , node = leaked_name ) "}
{"3858": "\ndef register_options_provider ( self , provider , own_group = True ) : \n    assert 0 >= provider . priority , \"provider's priority can't be >= 0\" \n    for i in range ( len ( self . options_providers ) ) : \n        if self . options_providers [ i ] . priority < provider . priority : \n            self . options_providers . insert ( i , provider ) \n            break \n    else : \n        self . options_providers . append ( provider ) \n    non_group_spec_options = [ option for option in provider . options if \"group\" not in option [ 1 ] ] \n    groups = getattr ( provider , \"option_groups\" , ( ) ) \n    if own_group and non_group_spec_options : \n        self . add_option_group ( provider . name . upper ( ) , provider . __doc__ , non_group_spec_options , provider , ) \n    else : \n        for opt , optdict in non_group_spec_options : \n            self . add_optik_option ( provider , self . cmdline_parser , opt , optdict ) \n    for gname , gdoc in groups : \n        gname = gname . upper ( ) \n        goptions = [ option for option in provider . options if option [ 1 ] . get ( \"group\" , \"\" ) . upper ( ) == gname ] \n        self . add_option_group ( gname , gdoc , goptions , provider ) "}
{"3872": "\ndef visit_module ( self , node ) : \n    self . _logging_names = set ( ) \n    logging_mods = self . config . logging_modules \n    self . _format_style = self . config . logging_format_style \n    self . _logging_modules = set ( logging_mods ) \n    self . _from_imports = { } \n    for logging_mod in logging_mods : \n        parts = logging_mod . rsplit ( \".\" , 1 ) \n        if 1 < len ( parts ) : \n            self . _from_imports [ parts [ 0 ] ] = parts [ 1 ] "}
{"3876": "\ndef _check_format_string ( self , node , format_arg ) : \n    num_args = _count_supplied_tokens ( node . args [ format_arg + 1 : ] ) \n    if not num_args : \n        return \n    format_string = node . args [ format_arg ] . value \n    if not isinstance ( format_string , str ) : \n        required_num_args = 0 \n    else : \n        try : \n            if self . _format_style == \"old\" : \n                keyword_args , required_num_args , _ , _ = utils . parse_format_string ( format_string ) \n                if keyword_args : \n                    return \n            elif self . _format_style == \"new\" : \n                keyword_arguments , implicit_pos_args , explicit_pos_args = utils . parse_format_method_string ( format_string ) \n                keyword_args_cnt = len ( set ( k for k , l in keyword_arguments if not isinstance ( k , int ) ) ) \n                required_num_args = ( keyword_args_cnt + implicit_pos_args + explicit_pos_args ) \n        except utils . UnsupportedFormatCharacter as ex : \n            char = format_string [ ex . index ] \n            self . add_message ( \"logging-unsupported-format\" , node = node , args = ( char , ord ( char ) , ex . index ) , ) \n            return \n        except utils . IncompleteFormatString : \n            self . add_message ( \"logging-format-truncated\" , node = node ) \n            return \n    if required_num_args < num_args : \n        self . add_message ( \"logging-too-many-args\" , node = node ) \n    elif required_num_args > num_args : \n        self . add_message ( \"logging-too-few-args\" , node = node ) "}
{"3900": "\ndef _check_docstring ( self , node_type , node , report_missing = True , confidence = interfaces . HIGH ) : \n    docstring = node . doc \n    if docstring is None : \n        if not report_missing : \n            return \n        lines = utils . get_node_last_lineno ( node ) - node . lineno \n        if node_type == \"module\" and not lines : \n            return \n        max_lines = self . config . docstring_min_length \n        if node_type != \"module\" and - 1 < max_lines and max_lines > lines : \n            return \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : \n            func = utils . safe_infer ( node . body [ 0 ] . value . func ) \n            if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : \n                if PY3K and func . bound . name == \"str\" : \n                    return \n                if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : \n                    return \n        self . add_message ( \"missing-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) \n    elif not docstring . strip ( ) : \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        self . add_message ( \"empty-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) "}
{"3904": "\ndef visit_module ( self , node ) : \n    visitor = PathGraphingAstVisitor ( ) \n    for child in node . body : \n        visitor . preorder ( child , visitor ) \n    for graph in visitor . graphs . values ( ) : \n        complexity = graph . complexity ( ) \n        node = graph . root \n        if hasattr ( node , \"name\" ) : \n            node_name = \"'%s'\" % node . name \n        else : \n            node_name = \"This '%s'\" % node . __class__ . __name__ . lower ( ) \n        if self . config . max_complexity >= complexity : \n            continue \n        self . add_message ( \"too-complex\" , node = node , confidence = HIGH , args = ( node_name , complexity ) ) "}
{"3974": "\ndef validate_bearer_token ( self , token , scopes , request ) : \n    log . debug ( 'Validate bearer token %r' , token ) \n    tok = self . _tokengetter ( access_token = token ) \n    if not tok : \n        msg = 'Bearer token not found.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    if tok . expires is not None and tok . expires < datetime . datetime . utcnow ( ) : \n        msg = 'Bearer token is expired.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    if scopes and not set ( tok . scopes ) & set ( scopes ) : \n        msg = 'Bearer token scope not valid.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    request . access_token = tok \n    request . user = tok . user \n    request . scopes = scopes \n    if hasattr ( tok , 'client' ) : \n        request . client = tok . client \n    elif hasattr ( tok , 'client_id' ) : \n        request . client = self . _clientgetter ( tok . client_id ) \n    return True "}
{"3976": "\ndef validate_code ( self , client_id , code , client , request , * args , ** kwargs ) : \n    client = client or self . _clientgetter ( client_id ) \n    log . debug ( 'Validate code for client %r and code %r' , client . client_id , code ) \n    grant = self . _grantgetter ( client_id = client . client_id , code = code ) \n    if not grant : \n        log . debug ( 'Grant not found.' ) \n        return False \n    if hasattr ( grant , 'expires' ) and grant . expires < datetime . datetime . utcnow ( ) : \n        log . debug ( 'Grant is expired.' ) \n        return False \n    request . state = kwargs . get ( 'state' ) \n    request . user = grant . user \n    request . scopes = grant . scopes \n    return True "}
{"4032": "\ndef bio_read ( self , bufsiz ) : \n    if self . _from_ssl is None : \n        raise TypeError ( \"Connection sock was not None\" ) \n    if not isinstance ( bufsiz , integer_types ) : \n        raise TypeError ( \"bufsiz must be an integer\" ) \n    buf = _no_zero_allocator ( \"char[]\" , bufsiz ) \n    result = _lib . BIO_read ( self . _from_ssl , buf , bufsiz ) \n    if 0 >= result : \n        self . _handle_bio_errors ( self . _from_ssl , result ) \n    return _ffi . buffer ( buf , result ) [ : ] "}
{"4034": "\ndef shutdown ( self ) : \n    result = _lib . SSL_shutdown ( self . _ssl ) \n    if 0 > result : \n        self . _raise_ssl_error ( self . _ssl , result ) \n    elif 0 < result : \n        return True \n    else : \n        return False "}
{"4038": "\ndef server_random ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if session == _ffi . NULL : \n        return None \n    length = _lib . SSL_get_server_random ( self . _ssl , _ffi . NULL , 0 ) \n    assert 0 < length \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_get_server_random ( self . _ssl , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4039": "\ndef client_random ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if session == _ffi . NULL : \n        return None \n    length = _lib . SSL_get_client_random ( self . _ssl , _ffi . NULL , 0 ) \n    assert 0 < length \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_get_client_random ( self . _ssl , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4040": "\ndef master_key ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if session == _ffi . NULL : \n        return None \n    length = _lib . SSL_SESSION_get_master_key ( session , _ffi . NULL , 0 ) \n    assert 0 < length \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_SESSION_get_master_key ( session , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4057": "\ndef sign ( pkey , data , digest ) : \n    data = _text_to_bytes_and_warn ( \"data\" , data ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    md_ctx = _lib . Cryptography_EVP_MD_CTX_new ( ) \n    md_ctx = _ffi . gc ( md_ctx , _lib . Cryptography_EVP_MD_CTX_free ) \n    _lib . EVP_SignInit ( md_ctx , digest_obj ) \n    _lib . EVP_SignUpdate ( md_ctx , data , len ( data ) ) \n    length = _lib . EVP_PKEY_size ( pkey . _pkey ) \n    _openssl_assert ( 0 < length ) \n    signature_buffer = _ffi . new ( \"unsigned char[]\" , length ) \n    signature_length = _ffi . new ( \"unsigned int *\" ) \n    final_result = _lib . EVP_SignFinal ( md_ctx , signature_buffer , signature_length , pkey . _pkey ) \n    _openssl_assert ( final_result == 1 ) \n    return _ffi . buffer ( signature_buffer , signature_length [ 0 ] ) [ : ] "}
{"4061": "\ndef generate_key ( self , type , bits ) : \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if not isinstance ( bits , int ) : \n        raise TypeError ( \"bits must be an integer\" ) \n    if type == TYPE_RSA : \n        if 0 >= bits : \n            raise ValueError ( \"Invalid number of bits\" ) \n        exponent = _lib . BN_new ( ) \n        exponent = _ffi . gc ( exponent , _lib . BN_free ) \n        _lib . BN_set_word ( exponent , _lib . RSA_F4 ) \n        rsa = _lib . RSA_new ( ) \n        result = _lib . RSA_generate_key_ex ( rsa , bits , exponent , _ffi . NULL ) \n        _openssl_assert ( result == 1 ) \n        result = _lib . EVP_PKEY_assign_RSA ( self . _pkey , rsa ) \n        _openssl_assert ( result == 1 ) \n    elif type == TYPE_DSA : \n        dsa = _lib . DSA_new ( ) \n        _openssl_assert ( dsa != _ffi . NULL ) \n        dsa = _ffi . gc ( dsa , _lib . DSA_free ) \n        res = _lib . DSA_generate_parameters_ex ( dsa , bits , _ffi . NULL , 0 , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n        _openssl_assert ( res == 1 ) \n        _openssl_assert ( _lib . DSA_generate_key ( dsa ) == 1 ) \n        _openssl_assert ( _lib . EVP_PKEY_set1_DSA ( self . _pkey , dsa ) == 1 ) \n    else : \n        raise Error ( \"No such key type\" ) \n    self . _initialized = True "}
{"4066": "\ndef der ( self ) : \n    result_buffer = _ffi . new ( 'unsigned char**' ) \n    encode_result = _lib . i2d_X509_NAME ( self . _name , result_buffer ) \n    _openssl_assert ( 0 <= encode_result ) \n    string_result = _ffi . buffer ( result_buffer [ 0 ] , encode_result ) [ : ] \n    _lib . OPENSSL_free ( result_buffer [ 0 ] ) \n    return string_result "}
{"4076": "\ndef verify ( self , pkey ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    result = _lib . X509_REQ_verify ( self . _req , pkey . _pkey ) \n    if 0 >= result : \n        _raise_current_error ( ) \n    return result "}
{"4081": "\ndef sign ( self , pkey , digest ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    if pkey . _only_public : \n        raise ValueError ( \"Key only has public part\" ) \n    if not pkey . _initialized : \n        raise ValueError ( \"Key is uninitialized\" ) \n    evp_md = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if evp_md == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    sign_result = _lib . X509_sign ( self . _x509 , pkey . _pkey , evp_md ) \n    _openssl_assert ( 0 < sign_result ) "}
{"4088": "\ndef has_expired ( self ) : \n    time_string = _native ( self . get_notAfter ( ) ) \n    not_after = datetime . datetime . strptime ( time_string , \"%Y%m%d%H%M%SZ\" ) \n    return datetime . datetime . utcnow ( ) > not_after "}
{"4098": "\ndef _init ( self ) : \n    ret = _lib . X509_STORE_CTX_init ( self . _store_ctx , self . _store . _store , self . _cert . _x509 , _ffi . NULL ) \n    if 0 >= ret : \n        _raise_current_error ( ) "}
{"4100": "\ndef verify_certificate ( self ) : \n    self . _cleanup ( ) \n    self . _init ( ) \n    ret = _lib . X509_verify_cert ( self . _store_ctx ) \n    self . _cleanup ( ) \n    if 0 >= ret : \n        raise self . _exception_from_context ( ) "}
{"4102": "\ndef get_serial ( self ) : \n    bio = _new_mem_buf ( ) \n    asn1_int = _lib . X509_REVOKED_get0_serialNumber ( self . _revoked ) \n    _openssl_assert ( asn1_int != _ffi . NULL ) \n    result = _lib . i2a_ASN1_INTEGER ( bio , asn1_int ) \n    _openssl_assert ( 0 <= result ) \n    return _bio_to_string ( bio ) "}
{"4114": "\ndef sign ( self , pkey , digest ) : \n    if pkey . _only_public : \n        raise ValueError ( \"Key has only public part\" ) \n    if not pkey . _initialized : \n        raise ValueError ( \"Key is uninitialized\" ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    sign_result = _lib . NETSCAPE_SPKI_sign ( self . _spki , pkey . _pkey , digest_obj ) \n    _openssl_assert ( 0 < sign_result ) "}
{"4115": "\ndef verify ( self , key ) : \n    answer = _lib . NETSCAPE_SPKI_verify ( self . _spki , key . _pkey ) \n    if 0 >= answer : \n        _raise_current_error ( ) \n    return True "}
{"4134": "\ndef aggregate_issues ( conf , main_section , debug ) : \n    log . info ( \"Starting to aggregate remote issues.\" ) \n    targets = aslist ( conf . get ( main_section , 'targets' ) ) \n    queue = multiprocessing . Queue ( ) \n    log . info ( \"Spawning %i workers.\" % len ( targets ) ) \n    processes = [ ] \n    if debug : \n        for target in targets : \n            _aggregate_issues ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) \n    else : \n        for target in targets : \n            proc = multiprocessing . Process ( target = _aggregate_issues , args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) ) \n            proc . start ( ) \n            processes . append ( proc ) \n            time . sleep ( 1 ) \n    currently_running = len ( targets ) \n    while 0 < currently_running : \n        issue = queue . get ( True ) \n        if isinstance ( issue , tuple ) : \n            completion_type , args = issue \n            if completion_type == SERVICE_FINISHED_ERROR : \n                target , e = args \n                log . info ( \"Terminating workers\" ) \n                for process in processes : \n                    process . terminate ( ) \n                raise RuntimeError ( \"critical error in target '{}'\" . format ( target ) ) \n            currently_running -= 1 \n            continue \n        yield issue \n    log . info ( \"Done aggregating remote issues.\" ) "}
{"4145": "\ndef find_local_uuid ( tw , keys , issue , legacy_matching = False ) : \n    if not issue [ 'description' ] : \n        raise ValueError ( 'Issue %s has no description.' % issue ) \n    possibilities = set ( [ ] ) \n    if legacy_matching : \n        legacy_description = issue . get_default_description ( ) . rsplit ( '..' , 1 ) [ 0 ] \n        legacy_description = legacy_description . split ( \"'\" ) [ 0 ] \n        results = tw . filter_tasks ( { 'description.startswith' : legacy_description , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n        possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    for service , key_list in six . iteritems ( keys ) : \n        if any ( [ key in issue for key in key_list ] ) : \n            results = tw . filter_tasks ( { 'and' : [ ( \"%s.is\" % key , issue [ key ] ) for key in key_list ] , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n            possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    if len ( possibilities ) == 1 : \n        return possibilities . pop ( ) \n    if 1 < len ( possibilities ) : \n        raise MultipleMatches ( \"Issue %s matched multiple IDs: %s\" % ( issue [ 'description' ] , possibilities ) ) \n    raise NotFound ( \"No issue was found matching %s\" % issue ) "}
{"4146": "\ndef merge_left ( field , local_task , remote_issue , hamming = False ) : \n    local_field = local_task . get ( field , [ ] ) \n    remote_field = remote_issue . get ( field , [ ] ) \n    if field not in local_task : \n        local_task [ field ] = [ ] \n    new_count = 0 \n    for remote in remote_field : \n        for local in local_field : \n            if ( ( hamming and get_annotation_hamming_distance ( remote , local ) == 0 ) or ( remote == local ) ) : \n                break \n        else : \n            log . debug ( \"%s not found in %r\" % ( remote , local_field ) ) \n            local_task [ field ] . append ( remote ) \n            new_count += 1 \n    if 0 < new_count : \n        log . debug ( 'Added %s new values to %s (total: %s)' % ( new_count , field , len ( local_task [ field ] ) , ) ) "}
{"4151": "\ndef calc_pvalues ( query , gene_sets , background = 20000 , ** kwargs ) : \n    k = len ( query ) \n    query = set ( query ) \n    vals = [ ] \n    if isinstance ( background , set ) : \n        bg = len ( background ) \n        query = query . intersection ( background ) \n    elif isinstance ( background , int ) : \n        bg = background \n    else : \n        raise ValueError ( \"background should be set or int object\" ) \n    subsets = sorted ( gene_sets . keys ( ) ) \n    for s in subsets : \n        category = gene_sets . get ( s ) \n        m = len ( category ) \n        hits = query . intersection ( set ( category ) ) \n        x = len ( hits ) \n        if 1 > x : \n            continue \n        vals . append ( ( s , hypergeom . sf ( x - 1 , bg , m , k ) , x , m , hits ) ) \n    return zip ( * vals ) "}
{"4152": "\ndef fdrcorrection ( pvals , alpha = 0.05 ) : \n    pvals = np . asarray ( pvals ) \n    pvals_sortind = np . argsort ( pvals ) \n    pvals_sorted = np . take ( pvals , pvals_sortind ) \n    ecdffactor = _ecdf ( pvals_sorted ) \n    reject = ecdffactor * alpha >= pvals_sorted \n    if reject . any ( ) : \n        rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) \n        reject [ : rejectmax ] = True \n    pvals_corrected_raw = pvals_sorted / ecdffactor \n    pvals_corrected = np . minimum . accumulate ( pvals_corrected_raw [ : : - 1 ] ) [ : : - 1 ] \n    del pvals_corrected_raw \n    pvals_corrected [ 1 < pvals_corrected ] = 1 \n    pvals_corrected_ = np . empty_like ( pvals_corrected ) \n    pvals_corrected_ [ pvals_sortind ] = pvals_corrected \n    del pvals_corrected \n    reject_ = np . empty_like ( reject ) \n    reject_ [ pvals_sortind ] = reject \n    return reject_ , pvals_corrected_ "}
{"4160": "\ndef enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = 1 , nperm = 1000 , rs = np . random . RandomState ( ) , single = False , scale = False ) : \n    N = len ( gene_list ) \n    tag_indicator = np . in1d ( gene_list , gene_set , assume_unique = True ) . astype ( int ) \n    if weighted_score_type == 0 : \n        correl_vector = np . repeat ( 1 , N ) \n    else : \n        correl_vector = np . abs ( correl_vector ) ** weighted_score_type \n    hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) \n    axis = 1 \n    tag_indicator = np . tile ( tag_indicator , ( nperm + 1 , 1 ) ) \n    correl_vector = np . tile ( correl_vector , ( nperm + 1 , 1 ) ) \n    for i in range ( nperm ) : \n        rs . shuffle ( tag_indicator [ i ] ) \n    Nhint = tag_indicator . sum ( axis = axis , keepdims = True ) \n    sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = True ) \n    no_tag_indicator = 1 - tag_indicator \n    Nmiss = N - Nhint \n    norm_tag = 1.0 / sum_correl_tag \n    norm_no_tag = 1.0 / Nmiss \n    RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) \n    if scale : \n        RES = RES / N \n    if single : \n        es_vec = RES . sum ( axis = axis ) \n    else : \n        max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) \n        es_vec = np . where ( np . abs ( min_ES ) < np . abs ( max_ES ) , max_ES , min_ES ) \n    es , esnull , RES = es_vec [ - 1 ] , es_vec [ : - 1 ] , RES [ - 1 , : ] \n    return es , esnull , hit_ind , RES "}
{"4163": "\ndef gsea_pval ( es , esnull ) : \n    condlist = [ 0 > es , 0 <= es ] \n    choicelist = [ np . sum ( es . reshape ( len ( es ) , 1 ) > esnull , axis = 1 ) / np . sum ( 0 > esnull , axis = 1 ) , np . sum ( es . reshape ( len ( es ) , 1 ) <= esnull , axis = 1 ) / np . sum ( 0 <= esnull , axis = 1 ) ] \n    pval = np . select ( condlist , choicelist ) \n    return pval "}
{"4164": "\ndef gsea_significance ( enrichment_scores , enrichment_nulls ) : \n    np . seterr ( divide = 'ignore' , invalid = 'ignore' ) \n    es = np . array ( enrichment_scores ) \n    esnull = np . array ( enrichment_nulls ) \n    logging . debug ( \"Start to compute pvals..................................\" ) \n    enrichmentPVals = gsea_pval ( es , esnull ) . tolist ( ) \n    logging . debug ( \"Compute nes and nesnull.................................\" ) \n    esnull_pos = ( esnull * ( 0 <= esnull ) ) . mean ( axis = 1 ) \n    esnull_neg = ( esnull * ( 0 > esnull ) ) . mean ( axis = 1 ) \n    nEnrichmentScores = np . where ( 0 <= es , es / esnull_pos , - es / esnull_neg ) \n    nEnrichmentNulls = np . where ( 0 <= esnull , esnull / esnull_pos [ : , np . newaxis ] , - esnull / esnull_neg [ : , np . newaxis ] ) \n    logging . debug ( \"start to compute fdrs..................................\" ) \n    nvals = np . sort ( nEnrichmentNulls . flatten ( ) ) \n    nnes = np . sort ( nEnrichmentScores ) \n    fdrs = [ ] \n    for i in range ( len ( enrichment_scores ) ) : \n        nes = nEnrichmentScores [ i ] \n        if 0 <= nes : \n            allPos = int ( len ( nvals ) - np . searchsorted ( nvals , 0 , side = \"left\" ) ) \n            allHigherAndPos = int ( len ( nvals ) - np . searchsorted ( nvals , nes , side = \"left\" ) ) \n            nesPos = len ( nnes ) - int ( np . searchsorted ( nnes , 0 , side = \"left\" ) ) \n            nesHigherAndPos = len ( nnes ) - int ( np . searchsorted ( nnes , nes , side = \"left\" ) ) \n        else : \n            allPos = int ( np . searchsorted ( nvals , 0 , side = \"left\" ) ) \n            allHigherAndPos = int ( np . searchsorted ( nvals , nes , side = \"right\" ) ) \n            nesPos = int ( np . searchsorted ( nnes , 0 , side = \"left\" ) ) \n            nesHigherAndPos = int ( np . searchsorted ( nnes , nes , side = \"right\" ) ) \n        try : \n            pi_norm = allHigherAndPos / float ( allPos ) \n            pi_obs = nesHigherAndPos / float ( nesPos ) \n            fdr = pi_norm / pi_obs \n            fdrs . append ( fdr if 1 > fdr else 1.0 ) \n        except : \n            fdrs . append ( 1000000000.0 ) \n    logging . debug ( \"Statistical testing finished.............................\" ) \n    return zip ( enrichment_scores , nEnrichmentScores , enrichmentPVals , fdrs ) "}
{"4174": "\ndef _set_cores ( self ) : \n    cpu_num = cpu_count ( ) - 1 \n    if cpu_num < self . _processes : \n        cores = cpu_num \n    elif 1 > self . _processes : \n        cores = 1 \n    else : \n        cores = self . _processes \n    self . _processes = int ( cores ) "}
{"4178": "\ndef _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : \n    width = len ( classes ) if 6 <= len ( classes ) else 5 \n    cls_booA = list ( map ( lambda x : True if x == pheno_pos else False , classes ) ) \n    cls_booB = list ( map ( lambda x : True if x == pheno_neg else False , classes ) ) \n    datA = df . loc [ : , cls_booA ] \n    datB = df . loc [ : , cls_booB ] \n    datAB = pd . concat ( [ datA , datB ] , axis = 1 ) \n    self . _width = width \n    self . heatmat = datAB \n    return "}
{"4179": "\ndef _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : \n    res = OrderedDict ( ) \n    for gs , gseale , ind , RES in zipdata : \n        rdict = OrderedDict ( ) \n        rdict [ 'es' ] = gseale [ 0 ] \n        rdict [ 'nes' ] = gseale [ 1 ] \n        rdict [ 'pval' ] = gseale [ 2 ] \n        rdict [ 'fdr' ] = gseale [ 3 ] \n        rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) \n        rdict [ 'matched_size' ] = len ( ind ) \n        _genes = rank_metric . index . values [ ind ] \n        rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) \n        if self . module != 'ssgsea' : \n            if 0 < rdict [ 'es' ] : \n                idx = RES . argmax ( ) \n                ldg_pos = list ( filter ( lambda x : idx >= x , ind ) ) \n            elif 0 > rdict [ 'es' ] : \n                idx = RES . argmin ( ) \n                ldg_pos = list ( filter ( lambda x : idx <= x , ind ) ) \n            else : \n                ldg_pos = ind \n            rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) \n        rdict [ 'RES' ] = RES \n        rdict [ 'hits_indices' ] = ind \n        res [ gs ] = rdict \n    self . results = res \n    res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) \n    res_df . index . name = 'Term' \n    res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = True ) \n    res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) \n    self . res2d = res_df \n    if self . _outdir is None : \n        return \n    out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) \n    if self . module == 'ssgsea' : \n        out = out . replace ( \".csv\" , \".txt\" ) \n        with open ( out , 'a' ) as f : \n            f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\\n' ) \n            f . write ( \"# might not proper for publication\\n\" ) \n            res_df . to_csv ( f , sep = '\\t' ) \n    else : \n        res_df . to_csv ( out ) \n    return "}
{"4180": "\ndef load_data ( self , cls_vec ) : \n    if isinstance ( self . data , pd . DataFrame ) : \n        exprs = self . data . copy ( ) \n        if exprs . index . dtype == 'O' : \n            exprs = exprs . reset_index ( ) \n    elif os . path . isfile ( self . data ) : \n        if self . data . endswith ( \"gct\" ) : \n            exprs = pd . read_csv ( self . data , skiprows = 1 , comment = '#' , sep = \"\\t\" ) \n        else : \n            exprs = pd . read_csv ( self . data , comment = '#' , sep = \"\\t\" ) \n    else : \n        raise Exception ( 'Error parsing gene expression DataFrame!' ) \n    if 0 < exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) : \n        self . _logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) \n        exprs . drop_duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) \n    if 0 < exprs . isnull ( ) . any ( ) . sum ( ) : \n        self . _logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) \n        exprs . dropna ( how = 'all' , inplace = True ) \n        exprs = exprs . fillna ( 0 ) \n    exprs . set_index ( keys = exprs . columns [ 0 ] , inplace = True ) \n    df = exprs . select_dtypes ( include = [ np . number ] ) \n    df_std = df . groupby ( by = cls_vec , axis = 1 ) . std ( ) \n    df = df [ ~ df_std . isin ( [ 0 ] ) . any ( axis = 1 ) ] \n    df = df + 0.00001 \n    return df "}
{"4181": "\ndef run ( self ) : \n    assert self . permutation_type in [ \"phenotype\" , \"gene_set\" ] \n    assert self . max_size >= self . min_size \n    self . _logger . info ( \"Parsing data files for GSEA.............................\" ) \n    phenoPos , phenoNeg , cls_vector = gsea_cls_parser ( self . classes ) \n    dat = self . load_data ( cls_vector ) \n    assert 1 < len ( dat ) \n    dat2 = ranking_metric ( df = dat , method = self . method , pos = phenoPos , neg = phenoNeg , classes = cls_vector , ascending = self . ascending ) \n    self . ranking = dat2 \n    gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) \n    self . _logger . info ( \"%04d gene_sets used for further statistical testing.....\" % len ( gmt ) ) \n    self . _logger . info ( \"Start to run GSEA...Might take a while..................\" ) \n    self . _set_cores ( ) \n    dataset = dat if self . permutation_type == 'phenotype' else dat2 \n    gsea_results , hit_ind , rank_ES , subsets = gsea_compute_tensor ( data = dataset , gmt = gmt , n = self . permutation_num , weighted_score_type = self . weighted_score_type , permutation_type = self . permutation_type , method = self . method , pheno_pos = phenoPos , pheno_neg = phenoNeg , classes = cls_vector , ascending = self . ascending , processes = self . _processes , seed = self . seed ) \n    self . _logger . info ( \"Start to generate GSEApy reports and figures............\" ) \n    res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n    self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = self . permutation_type ) \n    self . _heatmat ( df = dat . loc [ dat2 . index ] , classes = cls_vector , pheno_pos = phenoPos , pheno_neg = phenoNeg ) \n    if not self . _noplot : \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format , pheno_pos = phenoPos , pheno_neg = phenoNeg ) \n    self . _logger . info ( \"Congratulations. GSEApy ran successfully.................\\n\" ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4182": "\ndef run ( self ) : \n    assert self . max_size >= self . min_size \n    dat2 = self . _load_ranking ( self . rnk ) \n    assert 1 < len ( dat2 ) \n    self . _set_cores ( ) \n    self . _logger . info ( \"Parsing data files for GSEA.............................\" ) \n    gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) \n    self . _logger . info ( \"%04d gene_sets used for further statistical testing.....\" % len ( gmt ) ) \n    self . _logger . info ( \"Start to run GSEA...Might take a while..................\" ) \n    gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed ) \n    self . _logger . info ( \"Start to generate gseapy reports, and produce figures...\" ) \n    res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n    self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = \"gene_sets\" ) \n    if not self . _noplot : \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format , pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg ) \n    self . _logger . info ( \"Congratulations. GSEApy runs successfully................\\n\" ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4183": "\ndef runSamplesPermu ( self , df , gmt = None ) : \n    assert self . max_size >= self . min_size \n    mkdirs ( self . outdir ) \n    self . resultsOnSamples = OrderedDict ( ) \n    outdir = self . outdir \n    for name , ser in df . iteritems ( ) : \n        self . outdir = os . path . join ( outdir , str ( name ) ) \n        self . _logger . info ( \"Run Sample: %s \" % name ) \n        mkdirs ( self . outdir ) \n        dat2 = ser . sort_values ( ascending = self . ascending ) \n        gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = '' , pheno_neg = '' , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed , single = True , scale = self . scale ) \n        res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n        self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = \"gene_sets\" ) \n        self . resultsOnSamples [ name ] = self . res2d . es \n        if self . _noplot : \n            continue \n        self . _logger . info ( \"Plotting Sample: %s \\n\" % name ) \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format ) \n    self . _save ( outdir ) \n    return "}
{"4186": "\ndef run ( self ) : \n    assert self . max_size >= self . min_size \n    assert 0 < self . fignum \n    import glob \n    from bs4 import BeautifulSoup \n    try : \n        results_path = glob . glob ( self . indir + '*/edb/results.edb' ) [ 0 ] \n        rank_path = glob . glob ( self . indir + '*/edb/*.rnk' ) [ 0 ] \n        gene_set_path = glob . glob ( self . indir + '*/edb/gene_sets.gmt' ) [ 0 ] \n    except IndexError as e : \n        sys . stderr . write ( \"Could not locate GSEA files in the given directory!\" ) \n        sys . exit ( 1 ) \n    cls_path = glob . glob ( self . indir + '*/edb/*.cls' ) \n    if cls_path : \n        pos , neg , classes = gsea_cls_parser ( cls_path [ 0 ] ) \n    else : \n        pos , neg = '' , '' \n    self . gene_sets = gene_set_path \n    gene_set_dict = self . parse_gmt ( gmt = gene_set_path ) \n    rank_metric = self . _load_ranking ( rank_path ) \n    correl_vector = rank_metric . values \n    gene_list = rank_metric . index . values \n    database = BeautifulSoup ( open ( results_path ) , features = 'xml' ) \n    length = len ( database . findAll ( 'DTG' ) ) \n    fig_num = self . fignum if length >= self . fignum else length \n    for idx in range ( fig_num ) : \n        enrich_term , hit_ind , nes , pval , fdr = gsea_edb_parser ( results_path , index = idx ) \n        gene_set = gene_set_dict . get ( enrich_term ) \n        RES = enrichment_score ( gene_list = gene_list , correl_vector = correl_vector , gene_set = gene_set , weighted_score_type = self . weighted_score_type , nperm = 0 ) [ - 1 ] \n        term = enrich_term . replace ( '/' , '_' ) . replace ( \":\" , \"_\" ) \n        outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) \n        gseaplot ( rank_metric = rank_metric , term = enrich_term , hits_indices = hit_ind , nes = nes , pval = pval , fdr = fdr , RES = RES , pheno_pos = pos , pheno_neg = neg , figsize = self . figsize , ofname = outfile ) \n    self . _logger . info ( \"Congratulations! Your plots have been reproduced successfully!\\n\" ) "}
{"4189": "\ndef parse_genelists ( self ) : \n    if isinstance ( self . gene_list , list ) : \n        genes = self . gene_list \n    elif isinstance ( self . gene_list , pd . DataFrame ) : \n        if 3 <= self . gene_list . shape [ 1 ] : \n            genes = self . gene_list . iloc [ : , : 3 ] . apply ( lambda x : \"\\t\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n        elif self . gene_list . shape [ 1 ] == 2 : \n            genes = self . gene_list . apply ( lambda x : \",\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n        else : \n            genes = self . gene_list . squeeze ( ) . tolist ( ) \n    elif isinstance ( self . gene_list , pd . Series ) : \n        genes = self . gene_list . squeeze ( ) . tolist ( ) \n    else : \n        genes = [ ] \n        with open ( self . gene_list ) as f : \n            for gene in f : \n                genes . append ( gene . strip ( ) ) \n    self . _isezid = all ( map ( self . _is_entrez_id , genes ) ) \n    if self . _isezid : \n        self . _gls = set ( map ( int , self . _gls ) ) \n    else : \n        self . _gls = genes \n    return '\\n' . join ( genes ) "}
{"4193": "\ndef run ( self ) : \n    self . get_organism ( ) \n    genes_list = self . parse_genelists ( ) \n    gss = self . parse_genesets ( ) \n    self . _logger . info ( \"Connecting to Enrichr Server to get latest library names\" ) \n    if 1 > len ( gss ) : \n        sys . stderr . write ( \"Not validated Enrichr library name provided\\n\" ) \n        sys . stdout . write ( \"Hint: use get_library_name() to view full list of supported names\" ) \n        sys . exit ( 1 ) \n    self . results = pd . DataFrame ( ) \n    for g in gss : \n        if isinstance ( g , dict ) : \n            res = self . enrich ( g ) \n            shortID , self . _gs = str ( id ( g ) ) , \"CUSTOM%s\" % id ( g ) \n            if res is None : \n                self . _logger . info ( \"No hits return, for gene set: Custom%s\" % shortID ) \n                continue \n        else : \n            self . _gs = str ( g ) \n            self . _logger . debug ( \"Start Enrichr using library: %s\" % ( self . _gs ) ) \n            self . _logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . _gs ) ) \n            shortID , res = self . get_results ( genes_list ) \n        res . insert ( 0 , \"Gene_set\" , self . _gs ) \n        self . results = self . results . append ( res , ignore_index = True , sort = True ) \n        self . res2d = res \n        if self . _outdir is None : \n            continue \n        self . _logger . info ( 'Save file of enrichment results: Job Id:' + str ( shortID ) ) \n        outfile = \"%s/%s.%s.%s.reports.txt\" % ( self . outdir , self . _gs , self . descriptions , self . module ) \n        self . res2d . to_csv ( outfile , index = False , encoding = 'utf-8' , sep = \"\\t\" ) \n        if not self . __no_plot : \n            msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top_term = self . __top_term , color = 'salmon' , title = self . _gs , ofname = outfile . replace ( \"txt\" , self . format ) ) \n            if msg is not None : \n                self . _logger . warning ( msg ) \n        self . _logger . info ( 'Done.\\n' ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4211": "\ndef function_cyl_co ( script , r_func = 'r' , theta_func = 'theta' , z_func = 'z' ) : \n    r = 'sqrt(x^2+y^2)' \n    if isinstance ( script , FilterScript ) and '2016.12' <= script . ml_version : \n        theta = 'atan2(y, x)' \n    else : \n        theta = mp_func . mp_atan2 ( 'y' , 'x' ) \n    r_func = re . sub ( r\"\\br\\b\" , r , r_func ) . replace ( 'theta' , theta ) \n    theta_func = re . sub ( r\"\\br\\b\" , r , theta_func ) . replace ( 'theta' , theta ) \n    z_func = re . sub ( r\"\\br\\b\" , r , z_func ) . replace ( 'theta' , theta ) \n    x_func = '(r)*cos(theta)' . replace ( 'r' , r_func ) . replace ( 'theta' , theta_func ) \n    y_func = '(r)*sin(theta)' . replace ( 'r' , r_func ) . replace ( 'theta' , theta_func ) \n    vert_function ( script , x_func , y_func , z_func ) \n    return None "}
{"4234": "\ndef del_layer ( self , layer_num ) : \n    del self . layer_stack [ layer_num ] \n    if self . current_layer ( ) > layer_num : \n        self . set_current_layer ( self . current_layer ( ) - 1 ) \n    return None "}
{"4277": "\ndef code_events ( self ) : \n    if self . _resulting_events : \n        return self . _resulting_events \n    for i , ( lineno , mem , func , fname ) in enumerate ( self . _events_list ) : \n        mem_in_mb = float ( mem - self . mem_overhead ) / _BYTES_IN_MB \n        if ( self . _resulting_events and self . _resulting_events [ - 1 ] [ 0 ] == lineno and self . _resulting_events [ - 1 ] [ 2 ] == func and self . _resulting_events [ - 1 ] [ 3 ] == fname and mem_in_mb > self . _resulting_events [ - 1 ] [ 1 ] ) : \n            self . _resulting_events [ - 1 ] [ 1 ] = mem_in_mb \n        else : \n            self . _resulting_events . append ( [ i + 1 , lineno , mem_in_mb , func , fname ] ) \n    return self . _resulting_events "}
{"4322": "\ndef run_profilers ( run_object , prof_config , verbose = False ) : \n    if len ( set ( prof_config ) ) < len ( prof_config ) : \n        raise AmbiguousConfigurationError ( 'Profiler configuration %s is ambiguous' % prof_config ) \n    available_profilers = { opt for opt , _ in _PROFILERS } \n    for option in prof_config : \n        if option not in available_profilers : \n            raise BadOptionError ( 'Unknown option: %s' % option ) \n    run_stats = OrderedDict ( ) \n    present_profilers = ( ( o , p ) for o , p in _PROFILERS if o in prof_config ) \n    for option , prof in present_profilers : \n        curr_profiler = prof ( run_object ) \n        if verbose : \n            print ( 'Running %s...' % curr_profiler . __class__ . __name__ ) \n        run_stats [ option ] = curr_profiler . run ( ) \n    return run_stats "}
{"4329": "\ndef _limit_features ( self , X , vocabulary , high = None , low = None , limit = None ) : \n    if high is None and low is None and limit is None : \n        return X , set ( ) \n    dfs = X . map ( _document_frequency ) . sum ( ) \n    tfs = X . map ( lambda x : np . asarray ( x . sum ( axis = 0 ) ) ) . sum ( ) . ravel ( ) \n    mask = np . ones ( len ( dfs ) , dtype = bool ) \n    if high is not None : \n        mask &= high >= dfs \n    if low is not None : \n        mask &= low <= dfs \n    if limit is not None and limit < mask . sum ( ) : \n        mask_inds = ( - tfs [ mask ] ) . argsort ( ) [ : limit ] \n        new_mask = np . zeros ( len ( dfs ) , dtype = bool ) \n        new_mask [ np . where ( mask ) [ 0 ] [ mask_inds ] ] = True \n        mask = new_mask \n    new_indices = np . cumsum ( mask ) - 1 \n    removed_terms = set ( ) \n    for term , old_index in list ( six . iteritems ( vocabulary ) ) : \n        if mask [ old_index ] : \n            vocabulary [ term ] = new_indices [ old_index ] \n        else : \n            del vocabulary [ term ] \n            removed_terms . add ( term ) \n    kept_indices = np . where ( mask ) [ 0 ] \n    if len ( kept_indices ) == 0 : \n        raise ValueError ( \"After pruning, no terms remain. Try a lower\" \" min_df or a higher max_df.\" ) \n    return kept_indices , removed_terms "}
{"4330": "\ndef fit_transform ( self , Z ) : \n    self . _validate_vocabulary ( ) \n    analyze = self . build_analyzer ( ) \n    A = Z . transform ( lambda X : list ( map ( analyze , X ) ) , column = 'X' ) . persist ( ) \n    X = A [ : , 'X' ] if isinstance ( A , DictRDD ) else A \n    self . vocabulary_ = self . _init_vocab ( X ) \n    mapper = self . broadcast ( self . _count_vocab , A . context ) \n    Z = A . transform ( mapper , column = 'X' , dtype = sp . spmatrix ) \n    if not self . fixed_vocabulary_ : \n        X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n        max_df = self . max_df \n        min_df = self . min_df \n        max_features = self . max_features \n        n_doc = X . shape [ 0 ] \n        max_doc_count = ( max_df if isinstance ( max_df , numbers . Integral ) else max_df * n_doc ) \n        min_doc_count = ( min_df if isinstance ( min_df , numbers . Integral ) else min_df * n_doc ) \n        if min_doc_count > max_doc_count : \n            raise ValueError ( \"max_df corresponds to < documents than min_df\" ) \n        kept_indices , self . stop_words_ = self . _limit_features ( X , self . vocabulary_ , max_doc_count , min_doc_count , max_features ) \n        map_index = self . _sort_features ( self . vocabulary_ ) \n        mask = kept_indices [ map_index ] \n        Z = Z . transform ( lambda x : x [ : , mask ] , column = 'X' , dtype = sp . spmatrix ) \n    A . unpersist ( ) \n    return Z "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if 0 < self . verbose : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( 0 , n_fits , n_folds ) : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4346": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    def mapper ( X ) : \n        X = check_array ( X , ( 'csr' , 'csc' ) , dtype = np . float64 ) \n        if hasattr ( X , \"toarray\" ) : \n            mean , var = mean_variance_axis ( X , axis = 0 ) \n        else : \n            mean , var = np . mean ( X , axis = 0 ) , np . var ( X , axis = 0 ) \n        return X . shape [ 0 ] , mean , var \n    def reducer ( a , b ) : \n        n_a , mean_a , var_a = a \n        n_b , mean_b , var_b = b \n        n_ab = n_a + n_b \n        mean_ab = ( ( mean_a * n_a ) + ( mean_b * n_b ) ) / n_ab \n        var_ab = ( ( ( n_a * var_a ) + ( n_b * var_b ) ) / n_ab ) + ( ( n_a * n_b ) * ( ( mean_b - mean_a ) / n_ab ) ** 2 ) \n        return ( n_ab , mean_ab , var_ab ) \n    _ , _ , self . variances_ = X . map ( mapper ) . treeReduce ( reducer ) \n    if np . all ( self . threshold >= self . variances_ ) : \n        msg = \"No feature in X meets the variance threshold {0:.5f}\" \n        if X . shape [ 0 ] == 1 : \n            msg += \" (X contains only one sample)\" \n        raise ValueError ( msg . format ( self . threshold ) ) \n    return self "}
{"4349": "\ndef _block_collection ( iterator , dtype , bsize = - 1 ) : \n    i = 0 \n    accumulated = [ ] \n    for a in iterator : \n        if ( 0 < bsize ) and ( bsize <= i ) : \n            yield _pack_accumulated ( accumulated , dtype ) \n            accumulated = [ ] \n            i = 0 \n        accumulated . append ( a ) \n        i += 1 \n    if 0 < i : \n        yield _pack_accumulated ( accumulated , dtype ) "}
{"4350": "\ndef _block_tuple ( iterator , dtypes , bsize = - 1 ) : \n    i = 0 \n    blocked_tuple = None \n    for tuple_i in iterator : \n        if blocked_tuple is None : \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n        if ( 0 < bsize ) and ( bsize <= i ) : \n            yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n            i = 0 \n        for x_j , x in zip ( tuple_i , blocked_tuple ) : \n            x . append ( x_j ) \n        i += 1 \n    if 0 < i : \n        yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) "}
{"4361": "\ndef execute_cmd ( cmd , cwd = None , timeout = 5 ) : \n    p = subprocess . Popen ( cmd , cwd = cwd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    try : \n        p . wait ( timeout = timeout ) \n    except subprocess . TimeoutExpired : \n        return None \n    else : \n        stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) \n        if ( 3 , ) <= sys . version_info : \n            stdout , stderr = stdout . decode ( 'utf-8' , errors = 'ignore' ) , stderr . decode ( 'utf-8' , errors = 'ignore' ) \n        if p . returncode : \n            raise ExecuteError ( 'Error running command {}: The error code {} has returned. Stderr: {}' . format ( ' ' . join ( cmd ) , p . returncode , stderr ) ) \n        else : \n            return stdout , stderr "}
{"4362": "\ndef execute_over_ssh ( cmd , ssh , cwd = None , shell = 'bash' ) : \n    port = None \n    parts = ssh . split ( ':' , 1 ) \n    if 1 < len ( parts ) and not parts [ 1 ] . isdigit ( ) : \n        raise InvalidConfig ( extra_body = 'Invalid port number on ssh config: {}' . format ( parts [ 1 ] ) ) \n    elif 1 < len ( parts ) : \n        port = parts [ 1 ] \n    quoted_cmd = ' ' . join ( [ x . replace ( \"'\" , \"\"\"'\"'\"'\"\"\" ) for x in cmd . split ( ' ' ) ] ) \n    remote_cmd = ' ' . join ( [ ' ' . join ( get_shell ( shell ) ) , ' ' . join ( [ EXECUTE_SHELL_PARAM , \"'\" , ' ' . join ( ( [ 'cd' , cwd , ';' ] if cwd else [ ] ) + [ quoted_cmd ] ) , \"'\" ] ) ] , ) \n    return [ 'ssh' , parts [ 0 ] ] + ( [ '-p' , port ] if port else [ ] ) + [ '-C' ] + [ remote_cmd ] "}
{"4373": "\ndef on_push ( self , device ) : \n    src = device . src . lower ( ) \n    if time . time ( ) < last_execution [ src ] + self . settings . get ( 'delay' , DEFAULT_DELAY ) : \n        return \n    last_execution [ src ] = time . time ( ) \n    self . execute ( device ) "}
{"4394": "\ndef setCodeVersion ( self , newVersion , callback = None ) : \n    assert isinstance ( newVersion , int ) \n    if self . __selfCodeVersion < newVersion : \n        raise Exception ( 'wrong version, current version is %d, requested version is %d' % ( self . __selfCodeVersion , newVersion ) ) \n    if self . __enabledCodeVersion > newVersion : \n        raise Exception ( 'wrong version, enabled version is %d, requested version is %d' % ( self . __enabledCodeVersion , newVersion ) ) \n    self . _applyCommand ( pickle . dumps ( newVersion ) , callback , _COMMAND_TYPE . VERSION ) "}
{"4398": "\ndef _maybeBind ( self ) : \n    if self . _ready or self . _selfIsReadonlyNode or self . _lastBindAttemptTime + self . _syncObj . conf . bindRetryTime > time . time ( ) : \n        return \n    self . _lastBindAttemptTime = time . time ( ) \n    try : \n        self . _server . bind ( ) \n    except Exception as e : \n        self . _bindAttempts += 1 \n        if self . _syncObj . conf . maxBindRetries and self . _syncObj . conf . maxBindRetries <= self . _bindAttempts : \n            self . _bindOverEvent . set ( ) \n            raise TransportNotReadyError \n    else : \n        self . _ready = True \n        self . _bindOverEvent . set ( ) "}
{"4402": "\ndef _shouldConnect ( self , node ) : \n    return isinstance ( node , TCPNode ) and node not in self . _preventConnectNodes and ( self . _selfIsReadonlyNode or node . address < self . _selfNode . address ) "}
{"4403": "\ndef _connectIfNecessarySingle ( self , node ) : \n    if node in self . _connections and self . _connections [ node ] . state != CONNECTION_STATE . DISCONNECTED : \n        return True \n    if not self . _shouldConnect ( node ) : \n        return False \n    assert node in self . _connections \n    if node in self . _lastConnectAttempt and self . _syncObj . conf . connectionRetryTime > time . time ( ) - self . _lastConnectAttempt [ node ] : \n        return False \n    self . _lastConnectAttempt [ node ] = time . time ( ) \n    return self . _connections [ node ] . connect ( node . ip , node . port ) "}
{"4411": "\ndef put ( self , item ) : \n    if self . __maxsize and self . __maxsize <= len ( self . __data ) : \n        return False \n    self . __data . append ( item ) \n    return True "}
{"4412": "\ndef put ( self , item ) : \n    if self . __maxsize and self . __maxsize <= len ( self . __data ) : \n        return False \n    heapq . heappush ( self . __data , item ) \n    return True "}
{"4423": "\ndef save_aggregate_reports_to_kafka ( self , aggregate_reports , aggregate_topic ) : \n    if ( type ( aggregate_reports ) == dict or type ( aggregate_reports ) == OrderedDict ) : \n        aggregate_reports = [ aggregate_reports ] \n    if 1 > len ( aggregate_reports ) : \n        return \n    for report in aggregate_reports : \n        report [ 'date_range' ] = self . generate_daterange ( report ) \n        report = self . strip_metadata ( report ) \n        for slice in report [ 'records' ] : \n            slice [ 'date_range' ] = report [ 'date_range' ] \n            slice [ 'org_name' ] = report [ 'org_name' ] \n            slice [ 'org_email' ] = report [ 'org_email' ] \n            slice [ 'policy_published' ] = report [ 'policy_published' ] \n            slice [ 'report_id' ] = report [ 'report_id' ] \n            logger . debug ( \"Sending slice.\" ) \n            try : \n                logger . debug ( \"Saving aggregate report to Kafka\" ) \n                self . producer . send ( aggregate_topic , slice ) \n            except UnknownTopicOrPartitionError : \n                raise KafkaError ( \"Kafka error: Unknown topic or partition on broker\" ) \n            except Exception as e : \n                raise KafkaError ( \"Kafka error: {0}\" . format ( e . __str__ ( ) ) ) \n            try : \n                self . producer . flush ( ) \n            except Exception as e : \n                raise KafkaError ( \"Kafka error: {0}\" . format ( e . __str__ ( ) ) ) "}
{"4432": "\ndef save_aggregate_reports_to_splunk ( self , aggregate_reports ) : \n    logger . debug ( \"Saving aggregate reports to Splunk\" ) \n    if type ( aggregate_reports ) == dict : \n        aggregate_reports = [ aggregate_reports ] \n    if 1 > len ( aggregate_reports ) : \n        return \n    data = self . _common_data . copy ( ) \n    json_str = \"\" \n    for report in aggregate_reports : \n        for record in report [ \"records\" ] : \n            new_report = dict ( ) \n            for metadata in report [ \"report_metadata\" ] : \n                new_report [ metadata ] = report [ \"report_metadata\" ] [ metadata ] \n            new_report [ \"published_policy\" ] = report [ \"policy_published\" ] \n            new_report [ \"source_ip_address\" ] = record [ \"source\" ] [ \"ip_address\" ] \n            new_report [ \"source_country\" ] = record [ \"source\" ] [ \"country\" ] \n            new_report [ \"source_reverse_dns\" ] = record [ \"source\" ] [ \"reverse_dns\" ] \n            new_report [ \"source_base_domain\" ] = record [ \"source\" ] [ \"base_domain\" ] \n            new_report [ \"message_count\" ] = record [ \"count\" ] \n            new_report [ \"disposition\" ] = record [ \"policy_evaluated\" ] [ \"disposition\" ] \n            new_report [ \"spf_aligned\" ] = record [ \"alignment\" ] [ \"spf\" ] \n            new_report [ \"dkim_aligned\" ] = record [ \"alignment\" ] [ \"dkim\" ] \n            new_report [ \"passed_dmarc\" ] = record [ \"alignment\" ] [ \"dmarc\" ] \n            new_report [ \"header_from\" ] = record [ \"identifiers\" ] [ \"header_from\" ] \n            new_report [ \"envelope_from\" ] = record [ \"identifiers\" ] [ \"envelope_from\" ] \n            if \"dkim\" in record [ \"auth_results\" ] : \n                new_report [ \"dkim_results\" ] = record [ \"auth_results\" ] [ \"dkim\" ] \n            if \"spf\" in record [ \"auth_results\" ] : \n                new_report [ \"spf_results\" ] = record [ \"auth_results\" ] [ \"spf\" ] \n            data [ \"sourcetype\" ] = \"dmarc:aggregate\" \n            timestamp = human_timestamp_to_timestamp ( new_report [ \"begin_date\" ] ) \n            data [ \"time\" ] = timestamp \n            data [ \"event\" ] = new_report . copy ( ) \n            json_str += \"{0}\\n\" . format ( json . dumps ( data ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if response [ \"code\" ] != 0 : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4433": "\ndef save_forensic_reports_to_splunk ( self , forensic_reports ) : \n    logger . debug ( \"Saving forensic reports to Splunk\" ) \n    if type ( forensic_reports ) == dict : \n        forensic_reports = [ forensic_reports ] \n    if 1 > len ( forensic_reports ) : \n        return \n    json_str = \"\" \n    for report in forensic_reports : \n        data = self . _common_data . copy ( ) \n        data [ \"sourcetype\" ] = \"dmarc:forensic\" \n        timestamp = human_timestamp_to_timestamp ( report [ \"arrival_date_utc\" ] ) \n        data [ \"time\" ] = timestamp \n        data [ \"event\" ] = report . copy ( ) \n        json_str += \"{0}\\n\" . format ( json . dumps ( data ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if response [ \"code\" ] != 0 : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4435": "\ndef get_base_domain ( domain , use_fresh_psl = False ) : \n    psl_path = os . path . join ( tempdir , \"public_suffix_list.dat\" ) \n    def download_psl ( ) : \n        url = \"https://publicsuffix.org/list/public_suffix_list.dat\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        fresh_psl = requests . get ( url , headers = headers ) . text \n        with open ( psl_path , \"w\" , encoding = \"utf-8\" ) as fresh_psl_file : \n            fresh_psl_file . write ( fresh_psl ) \n    if use_fresh_psl : \n        if not os . path . exists ( psl_path ) : \n            download_psl ( ) \n        else : \n            psl_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( psl_path ) . st_mtime ) \n            if timedelta ( hours = 24 ) < psl_age : \n                try : \n                    download_psl ( ) \n                except Exception as error : \n                    logger . warning ( \"Failed to download an updated PSL {0}\" . format ( error ) ) \n        with open ( psl_path , encoding = \"utf-8\" ) as psl_file : \n            psl = publicsuffix2 . PublicSuffixList ( psl_file ) \n        return psl . get_public_suffix ( domain ) \n    else : \n        return publicsuffix2 . get_public_suffix ( domain ) "}
{"4438": "\ndef get_ip_address_country ( ip_address , parallel = False ) : \n    def download_country_database ( location = \"GeoLite2-Country.mmdb\" ) : \n        if parallel : \n            logging . warning ( \"Cannot download GeoIP database in parallel mode\" ) \n            return \n        url = \"https://geolite.maxmind.com/download/geoip/database/\" \"GeoLite2-Country.tar.gz\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        original_filename = \"GeoLite2-Country.mmdb\" \n        try : \n            response = requests . get ( url , headers = headers ) \n            response . raise_for_status ( ) \n            tar_bytes = response . content \n            tar_file = tarfile . open ( fileobj = BytesIO ( tar_bytes ) , mode = \"r:gz\" ) \n            tar_dir = tar_file . getnames ( ) [ 0 ] \n            tar_path = \"{0}/{1}\" . format ( tar_dir , original_filename ) \n            tar_file . extract ( tar_path ) \n            shutil . move ( tar_path , location ) \n            shutil . rmtree ( tar_dir ) \n        except Exception as e : \n            logger . warning ( \"Error downloading {0}: {1}\" . format ( url , e . __str__ ( ) ) ) \n    system_paths = [ \"GeoLite2-Country.mmdb\" , \"/usr/local/share/GeoIP/GeoLite2-Country.mmdb\" , \"/usr/share/GeoIP/GeoLite2-Country.mmdb\" , \"/var/lib/GeoIP/GeoLite2-Country.mmdb\" , \"/var/local/lib/GeoIP/GeoLite2-Country.mmdb\" , \"C:\\\\GeoIP\\\\GeoLite2-Country.mmdb\" ] \n    db_path = None \n    for system_path in system_paths : \n        if os . path . exists ( system_path ) : \n            db_path = system_path \n            break \n    if db_path is None : \n        db_path = os . path . join ( tempdir , \"GeoLite2-Country.mmdb\" ) \n        if not os . path . exists ( db_path ) : \n            download_country_database ( db_path ) \n            if not os . path . exists ( db_path ) : \n                return None \n        else : \n            db_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( db_path ) . st_mtime ) \n            if timedelta ( days = 7 ) < db_age : \n                download_country_database ( ) \n        db_path = db_path \n    db_reader = geoip2 . database . Reader ( db_path ) \n    country = None \n    try : \n        country = db_reader . country ( ip_address ) . country . iso_code \n    except geoip2 . errors . AddressNotFoundError : \n        pass \n    return country "}
{"4444": "\ndef publish ( self , subject , payload ) : \n    if self . is_closed : \n        raise ErrConnectionClosed \n    if self . is_draining_pubs : \n        raise ErrConnectionDraining \n    payload_size = len ( payload ) \n    if self . _max_payload < payload_size : \n        raise ErrMaxPayload \n    yield from self . _publish ( subject , _EMPTY_ , payload , payload_size ) "}
{"4445": "\ndef publish_request ( self , subject , reply , payload ) : \n    if self . is_closed : \n        raise ErrConnectionClosed \n    if self . is_draining_pubs : \n        raise ErrConnectionDraining \n    payload_size = len ( payload ) \n    if self . _max_payload < payload_size : \n        raise ErrMaxPayload \n    yield from self . _publish ( subject , reply . encode ( ) , payload , payload_size ) "}
{"4449": "\ndef flush ( self , timeout = 60 ) : \n    if 0 >= timeout : \n        raise ErrBadTimeout \n    if self . is_closed : \n        raise ErrConnectionClosed \n    future = asyncio . Future ( loop = self . _loop ) \n    try : \n        yield from self . _send_ping ( future ) \n        yield from asyncio . wait_for ( future , timeout , loop = self . _loop ) \n    except asyncio . TimeoutError : \n        future . cancel ( ) \n        raise ErrTimeout "}
{"4450": "\ndef _select_next_server ( self ) : \n    while True : \n        if len ( self . _server_pool ) == 0 : \n            self . _current_server = None \n            raise ErrNoServers \n        now = time . monotonic ( ) \n        s = self . _server_pool . pop ( 0 ) \n        if 0 < self . options [ \"max_reconnect_attempts\" ] : \n            if self . options [ \"max_reconnect_attempts\" ] < s . reconnects : \n                continue \n        self . _server_pool . append ( s ) \n        if s . last_attempt is not None and s . last_attempt + self . options [ \"reconnect_time_wait\" ] > now : \n            yield from asyncio . sleep ( self . options [ \"reconnect_time_wait\" ] , loop = self . _loop ) \n        try : \n            s . last_attempt = time . monotonic ( ) \n            r , w = yield from asyncio . open_connection ( s . uri . hostname , s . uri . port , loop = self . _loop , limit = DEFAULT_BUFFER_SIZE ) \n            self . _current_server = s \n            self . _bare_io_reader = self . _io_reader = r \n            self . _bare_io_writer = self . _io_writer = w \n            break \n        except Exception as e : \n            s . last_attempt = time . monotonic ( ) \n            s . reconnects += 1 \n            self . _err = e \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            continue "}
{"4454": "\ndef _process_pong ( self ) : \n    if 0 < len ( self . _pongs ) : \n        future = self . _pongs . pop ( 0 ) \n        future . set_result ( True ) \n        self . _pongs_received += 1 \n        self . _pings_outstanding -= 1 "}
{"4455": "\ndef _process_msg ( self , sid , subject , reply , data ) : \n    payload_size = len ( data ) \n    self . stats [ 'in_msgs' ] += 1 \n    self . stats [ 'in_bytes' ] += payload_size \n    sub = self . _subs . get ( sid ) \n    if sub is None : \n        return \n    sub . received += 1 \n    if 0 < sub . max_msgs and sub . max_msgs <= sub . received : \n        self . _subs . pop ( sid , None ) \n    msg = self . _build_message ( subject , reply , data ) \n    if sub . future is not None : \n        if sub . future . cancelled ( ) : \n            return \n        sub . future . set_result ( msg ) \n        return \n    try : \n        sub . pending_size += payload_size \n        if sub . pending_bytes_limit <= sub . pending_size : \n            sub . pending_size -= payload_size \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) \n            return \n        sub . pending_queue . put_nowait ( msg ) \n    except asyncio . QueueFull : \n        if self . _error_cb is not None : \n            yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) "}
{"4458": "\ndef _flusher ( self ) : \n    while True : \n        if not self . is_connected or self . is_connecting : \n            break \n        try : \n            yield from self . _flush_queue . get ( ) \n            if 0 < self . _pending_data_size : \n                self . _io_writer . writelines ( self . _pending [ : ] ) \n                self . _pending = [ ] \n                self . _pending_data_size = 0 \n                yield from self . _io_writer . drain ( ) \n        except OSError as e : \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            yield from self . _process_op_err ( e ) \n            break \n        except asyncio . CancelledError : \n            break "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = True ) : \n    if masker is not None : \n        masker = masker \n    else : \n        if isinstance ( dataset , Dataset ) : \n            masker = dataset . masker \n        else : \n            if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n                raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = False ) \n    if regions . ndim == 2 : \n        m = regions \n        for i in range ( regions . shape [ 1 ] ) : \n            _nz = np . nonzero ( m [ : , i ] ) [ 0 ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        for i in range ( n_regions ) : \n            if isinstance ( threshold , int ) : \n                m [ regions == labels [ i ] , i ] = 1.0 \n            else : \n                m [ regions == labels [ i ] , i ] = 1.0 / np . sum ( regions == labels [ i ] ) \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ threshold > result ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4479": "\ndef fdr ( p , q = .05 ) : \n    s = np . sort ( p ) \n    nvox = p . shape [ 0 ] \n    null = np . array ( range ( 1 , nvox + 1 ) , dtype = 'float' ) * q / nvox \n    below = np . where ( null >= s ) [ 0 ] \n    return s [ max ( below ) ] if len ( below ) else - 1 "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( activation_threshold < prop_mask_active ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( 5.5 < dists ) & ( 6.5 > dists ) ) [ 0 ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ r >= dists ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    elif return_type == 'data' : \n        return self . get_image_data ( ids ) "}
{"4485": "\ndef get_feature_counts ( self , threshold = 0.001 ) : \n    counts = np . sum ( threshold <= self . get_feature_data ( ) , 0 ) \n    return dict ( zip ( self . get_feature_names ( ) , list ( counts ) ) ) "}
{"4491": "\ndef get_ids ( self , features , threshold = 0.0 , func = np . sum , get_weights = False ) : \n    if isinstance ( features , str ) : \n        features = [ features ] \n    features = self . search_features ( features ) \n    feature_weights = self . data . ix [ : , features ] \n    weights = feature_weights . apply ( func , 1 ) \n    above_thresh = weights [ threshold <= weights ] \n    return above_thresh if get_weights else list ( above_thresh . index ) "}
{"4529": "\nasync def get_all_albums ( self , * , market = 'US' ) -> List [ Album ] : \n    from . album import Album \n    albums = [ ] \n    offset = 0 \n    total = await self . total_albums ( market = market ) \n    while total > len ( albums ) : \n        data = await self . __client . http . artist_albums ( self . id , limit = 50 , offset = offset , market = market ) \n        offset += 50 \n        albums += list ( Album ( self . __client , item ) for item in data [ 'items' ] ) \n    return albums "}
{"4541": "\nasync def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : \n    tracks = [ ] \n    offset = 0 \n    total = self . total_tracks or None \n    while True : \n        data = await self . __client . http . album_tracks ( self . id , limit = 50 , offset = offset , market = market ) \n        if total is None : \n            total = data [ 'total' ] \n        offset += 50 \n        tracks += list ( Track ( self . __client , item ) for item in data [ 'items' ] ) \n        if total <= len ( tracks ) : \n            break \n    return tracks "}
{"4557": "\nasync def get_all_tracks ( self ) -> List [ PlaylistTrack ] : \n    if isinstance ( self . _tracks , PartialTracks ) : \n        return await self . _tracks . build ( ) \n    _tracks = [ ] \n    offset = 0 \n    while self . total_tracks > len ( self . tracks ) : \n        data = await self . __client . http . get_playlist_tracks ( self . owner . id , self . id , limit = 50 , offset = offset ) \n        _tracks += [ PlaylistTrack ( self . __client , item ) for item in data [ 'items' ] ] \n        offset += 50 \n    self . total_tracks = len ( self . _tracks ) \n    return list ( self . _tracks ) "}
{"4566": "\ndef _does_require_deprecation ( self ) : \n    for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2 ] ) : \n        if self . version_yaml [ index ] < version_number : \n            return True \n    return False "}
{"4588": "\ndef colorify_logo ( cls , home = False ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        elif 50 <= PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n        else : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        print ( \"\\n\" . join ( to_print ) ) "}
{"4589": "\ndef _format_domain ( cls , extracted_domain ) : \n    if not extracted_domain . startswith ( \"#\" ) : \n        if \"#\" in extracted_domain : \n            extracted_domain = extracted_domain [ : extracted_domain . find ( \"#\" ) ] . strip ( ) \n        if \" \" in extracted_domain or \"\\t\" in extracted_domain : \n            splited_line = extracted_domain . split ( ) \n            index = 1 \n            while len ( splited_line ) > index : \n                if splited_line [ index ] : \n                    break \n                index += 1 \n            return splited_line [ index ] \n        return extracted_domain \n    return \"\" "}
{"4608": "\ndef check_versions ( cls , local , upstream ) : \n    status = [ None , None , None ] \n    for index , version_number in enumerate ( local ) : \n        if int ( upstream [ index ] ) > int ( version_number ) : \n            status [ index ] = True \n        elif int ( upstream [ index ] ) < int ( version_number ) : \n            status [ index ] = False \n    if False in status : \n        return False \n    if True in status : \n        return True \n    return None "}
{"4636": "\ndef _header_constructor ( cls , data_to_print , header_separator = \"-\" , column_separator = \" \" ) : \n    header_data = [ ] \n    header_size = \"\" \n    before_size = \"%-\" \n    after_size = \"s\" \n    if header_separator : \n        header_separator_data = [ ] \n    length_data_to_print = len ( data_to_print ) - 1 \n    i = 0 \n    for data in data_to_print : \n        size = data_to_print [ data ] \n        header_data . append ( data ) \n        header_size += before_size + str ( size ) + after_size \n        if length_data_to_print > i : \n            header_size += column_separator \n        if header_separator : \n            header_separator_data . append ( header_separator * size ) \n        i += 1 \n    if header_separator : \n        return [ header_size % tuple ( header_data ) , header_size % tuple ( header_separator_data ) , ] \n    return [ header_size % tuple ( header_data ) ] "}
{"4668": "\ndef log ( self ) : \n    if ( PyFunceble . CONFIGURATION [ \"show_percentage\" ] and 0 < PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] ) : \n        output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) \n        File ( output ) . delete ( ) \n        self . _calculate ( ) \n        if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n            print ( \"\\n\" ) \n            Prints ( None , \"Percentage\" , output ) . header ( ) \n            lines_to_print = [ [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] \n            if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n                lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] \n                del lines_to_print [ 1 ] \n            for to_print in lines_to_print : \n                Prints ( to_print , \"Percentage\" , output ) . data ( ) \n    elif 0 < PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : \n        self . _calculate ( ) "}
{"4676": "\ndef _timestamp ( self ) : \n    if PyFunceble . CONFIGURATION [ \"inactive_database\" ] : \n        if ( \"inactive_db\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"inactive_db\" ] and PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] ) : \n            database_keys = [ x for x in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] . keys ( ) if x . isdigit ( ) ] \n            if database_keys : \n                recent_date = max ( database_keys ) \n            else : \n                return int ( PyFunceble . time ( ) ) \n            if int ( recent_date ) + self . one_day_in_seconds < int ( PyFunceble . time ( ) ) : \n                return int ( PyFunceble . time ( ) ) \n            if int ( recent_date ) + self . days_in_seconds > int ( PyFunceble . time ( ) ) : \n                return int ( recent_date ) \n    return int ( PyFunceble . time ( ) ) "}
{"4682": "\ndef is_time_older ( self ) : \n    if ( self . _authorization ( ) and self . is_in_database ( ) and int ( PyFunceble . time ( ) ) > int ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) ) : \n        return True \n    return False "}
{"4684": "\ndef add ( self ) : \n    if self . _authorization ( ) : \n        if int ( PyFunceble . time ( ) ) > self . epoch : \n            state = \"past\" \n        else : \n            state = \"future\" \n        if self . is_in_database ( ) : \n            if ( str ( self . epoch ) != PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration_date\" : self . expiration_date , } ) \n            elif self . is_time_older ( ) : \n                if ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"state\" ] != \"past\" ) : \n                    PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"state\" : \"past\" } ) \n            elif ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"state\" ] != \"future\" ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"state\" : \"future\" } ) \n        else : \n            if ( not PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] = { } \n            PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] . update ( { PyFunceble . INTERN [ \"to_test\" ] : { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration_date\" : self . expiration_date , } } ) \n        self . _backup ( ) "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = False \n            try : \n                time_autorisation = int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) <= int ( PyFunceble . time ( ) ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                print ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"4707": "\ndef filter_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False , ) : \n    imports = SAFE_IMPORTS \n    if additional_imports : \n        imports |= frozenset ( additional_imports ) \n    del additional_imports \n    messages = check ( source ) \n    if ignore_init_module_imports : \n        marked_import_line_numbers = frozenset ( ) \n    else : \n        marked_import_line_numbers = frozenset ( unused_import_line_numbers ( messages ) ) \n    marked_unused_module = collections . defaultdict ( lambda : [ ] ) \n    for line_number , module_name in unused_import_module_name ( messages ) : \n        marked_unused_module [ line_number ] . append ( module_name ) \n    if expand_star_imports and not ( re . search ( r'\\b__all__\\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : \n        marked_star_import_line_numbers = frozenset ( star_import_used_line_numbers ( messages ) ) \n        if 1 < len ( marked_star_import_line_numbers ) : \n            marked_star_import_line_numbers = frozenset ( ) \n        else : \n            undefined_names = [ ] \n            for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : \n                undefined_names . append ( undefined_name ) \n            if not undefined_names : \n                marked_star_import_line_numbers = frozenset ( ) \n    else : \n        marked_star_import_line_numbers = frozenset ( ) \n    if remove_unused_variables : \n        marked_variable_line_numbers = frozenset ( unused_variable_line_numbers ( messages ) ) \n    else : \n        marked_variable_line_numbers = frozenset ( ) \n    if remove_duplicate_keys : \n        marked_key_line_numbers = frozenset ( duplicate_key_line_numbers ( messages , source ) ) \n    else : \n        marked_key_line_numbers = frozenset ( ) \n    line_messages = get_messages_by_line ( messages ) \n    sio = io . StringIO ( source ) \n    previous_line = '' \n    for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : \n        if '#' in line : \n            yield line \n        elif line_number in marked_import_line_numbers : \n            yield filter_unused_import ( line , unused_module = marked_unused_module [ line_number ] , remove_all_unused_imports = remove_all_unused_imports , imports = imports , previous_line = previous_line ) \n        elif line_number in marked_variable_line_numbers : \n            yield filter_unused_variable ( line ) \n        elif line_number in marked_key_line_numbers : \n            yield filter_duplicate_key ( line , line_messages [ line_number ] , line_number , marked_key_line_numbers , source ) \n        elif line_number in marked_star_import_line_numbers : \n            yield filter_star_import ( line , undefined_names ) \n        else : \n            yield line \n        previous_line = line "}
{"4743": "\ndef process_request ( self , request , credential = None ) : \n    self . _client_identity = [ None , None ] \n    header = request . request_header \n    self . _set_protocol_version ( header . protocol_version ) \n    max_response_size = None \n    if header . maximum_response_size : \n        max_response_size = header . maximum_response_size . value \n    now = int ( time . time ( ) ) \n    if header . time_stamp : \n        then = header . time_stamp . value \n        if ( then <= now ) and ( 60 > ( now - then ) ) : \n            self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( then ) ) ) ) \n        else : \n            if then > now : \n                self . _logger . warning ( \"Received request with future timestamp. Received \" \"timestamp: {0}, Current timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Future request rejected by server.\" ) \n            else : \n                self . _logger . warning ( \"Received request with old timestamp. Possible \" \"replay attack. Received timestamp: {0}, Current \" \"timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Stale request rejected by server.\" ) \n    else : \n        self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( now ) ) ) ) \n    self . is_asynchronous = False \n    if header . asynchronous_indicator is not None : \n        self . is_asynchronous = header . asynchronous_indicator . value \n    if self . is_asynchronous : \n        raise exceptions . InvalidMessage ( \"Asynchronous operations are not supported.\" ) \n    if header . authentication : \n        if header . authentication . credentials : \n            auth_credentials = header . authentication . credentials [ 0 ] \n        else : \n            auth_credentials = None \n    else : \n        auth_credentials = None \n    self . _verify_credential ( auth_credentials , credential ) \n    batch_error_option = enums . BatchErrorContinuationOption . STOP \n    if header . batch_error_cont_option is not None : \n        batch_error_option = header . batch_error_cont_option . value \n    if batch_error_option == enums . BatchErrorContinuationOption . UNDO : \n        raise exceptions . InvalidMessage ( \"Undo option for batch handling is not supported.\" ) \n    batch_order_option = False \n    if header . batch_order_option : \n        batch_order_option = header . batch_order_option . value \n    response_batch = self . _process_batch ( request . batch_items , batch_error_option , batch_order_option ) \n    response = self . _build_response ( header . protocol_version , response_batch ) \n    return response , max_response_size , header . protocol_version "}
{"4745": "\ndef _process_template_attribute ( self , template_attribute ) : \n    attributes = { } \n    if 0 < len ( template_attribute . names ) : \n        raise exceptions . ItemNotFound ( \"Attribute templates are not supported.\" ) \n    for attribute in template_attribute . attributes : \n        name = attribute . attribute_name . value \n        if not self . _attribute_policy . is_attribute_supported ( name ) : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( name ) ) \n        if self . _attribute_policy . is_attribute_multivalued ( name ) : \n            values = attributes . get ( name , list ( ) ) \n            if ( not attribute . attribute_index ) and 0 < len ( values ) : \n                raise exceptions . InvalidField ( \"Attribute index missing from multivalued attribute.\" ) \n            values . append ( attribute . attribute_value ) \n            attributes . update ( [ ( name , values ) ] ) \n        else : \n            if attribute . attribute_index : \n                if attribute . attribute_index . value != 0 : \n                    raise exceptions . InvalidField ( \"Non-zero attribute index found for \" \"single-valued attribute.\" ) \n            value = attributes . get ( name , None ) \n            if value : \n                raise exceptions . IndexOutOfBounds ( \"Cannot set multiple instances of the \" \"{0} attribute.\" . format ( name ) ) \n            else : \n                attributes . update ( [ ( name , attribute . attribute_value ) ] ) \n    return attributes "}
{"4749": "\ndef _set_attribute_on_managed_object ( self , managed_object , attribute ) : \n    attribute_name = attribute [ 0 ] \n    attribute_value = attribute [ 1 ] \n    if self . _attribute_policy . is_attribute_multivalued ( attribute_name ) : \n        if attribute_name == 'Name' : \n            managed_object . names . extend ( [ x . name_value . value for x in attribute_value ] ) \n            for name in managed_object . names : \n                if 1 < managed_object . names . count ( name ) : \n                    raise exceptions . InvalidField ( \"Cannot set duplicate name values.\" ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) \n    else : \n        field = None \n        value = attribute_value . value \n        if attribute_name == 'Cryptographic Algorithm' : \n            field = 'cryptographic_algorithm' \n        elif attribute_name == 'Cryptographic Length' : \n            field = 'cryptographic_length' \n        elif attribute_name == 'Cryptographic Usage Mask' : \n            field = 'cryptographic_usage_masks' \n            value = list ( ) \n            for e in enums . CryptographicUsageMask : \n                if e . value & attribute_value . value : \n                    value . append ( e ) \n        elif attribute_name == 'Operation Policy Name' : \n            field = 'operation_policy_name' \n        if field : \n            existing_value = getattr ( managed_object , field ) \n            if existing_value : \n                if existing_value != value : \n                    raise exceptions . InvalidField ( \"Cannot overwrite the {0} attribute.\" . format ( attribute_name ) ) \n            else : \n                setattr ( managed_object , field , value ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) "}
{"4759": "\ndef validate ( self ) : \n    if self . value is not None : \n        if not isinstance ( self . value , six . integer_types ) : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        else : \n            if LongInteger . MAX < self . value : \n                raise ValueError ( 'long integer value greater than accepted max' ) \n            elif LongInteger . MIN > self . value : \n                raise ValueError ( 'long integer value less than accepted min' ) "}
{"4761": "\ndef write ( self , ostream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    binary = \"{0:b}\" . format ( abs ( self . value ) ) \n    binary = ( \"0\" * ( 64 - ( len ( binary ) % 64 ) ) ) + binary \n    if 0 > self . value : \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    hexadecimal = b'' \n    for i in range ( 0 , len ( binary ) , 8 ) : \n        byte = binary [ i : i + 8 ] \n        byte = int ( byte , 2 ) \n        hexadecimal += struct . pack ( '!B' , byte ) \n    self . length = len ( hexadecimal ) \n    super ( BigInteger , self ) . write ( ostream , kmip_version = kmip_version ) \n    ostream . write ( hexadecimal ) "}
{"4763": "\ndef validate ( self ) : \n    if not isinstance ( self . enum , enumeration . EnumMeta ) : \n        raise TypeError ( 'enumeration type {0} must be of type EnumMeta' . format ( self . enum ) ) \n    if self . value is not None : \n        if not isinstance ( self . value , self . enum ) : \n            raise TypeError ( 'enumeration {0} must be of type {1}' . format ( self . value , self . enum ) ) \n        if type ( self . value . value ) not in six . integer_types : \n            raise TypeError ( 'enumeration value must be an int' ) \n        else : \n            if Enumeration . MAX < self . value . value : \n                raise ValueError ( 'enumeration value greater than accepted max' ) \n            elif Enumeration . MIN > self . value . value : \n                raise ValueError ( 'enumeration value less than accepted min' ) "}
{"4769": "\ndef validate ( self ) : \n    if self . value is not None : \n        if type ( self . value ) not in six . integer_types : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        else : \n            if Interval . MAX < self . value : \n                raise ValueError ( 'interval value greater than accepted max' ) \n            elif Interval . MIN > self . value : \n                raise ValueError ( 'interval value less than accepted min' ) "}
{"4780": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateKeyPairRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . COMMON_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _common_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n            self . _common_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . is_tag_next ( enums . Tags . COMMON_ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( tag = enums . Tags . COMMON_ATTRIBUTES ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            self . _common_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _private_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _private_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( tag = enums . Tags . PRIVATE_KEY_ATTRIBUTES ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            self . _private_key_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _public_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _public_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( tag = enums . Tags . PUBLIC_KEY_ATTRIBUTES ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            self . _public_key_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    self . is_oversized ( local_buffer ) "}
{"4781": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _common_template_attribute is not None : \n            self . _common_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _common_template_attribute is not None : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _common_template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _private_key_template_attribute is not None : \n            self . _private_key_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _private_key_template_attribute is not None : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _private_key_template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _public_key_template_attribute is not None : \n            self . _public_key_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _public_key_template_attribute is not None : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _public_key_template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CreateKeyPairRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4782": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateKeyPairResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . PRIVATE_KEY_UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _private_key_unique_identifier = primitives . TextString ( tag = enums . Tags . PRIVATE_KEY_UNIQUE_IDENTIFIER ) \n        self . _private_key_unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The CreateKeyPair response payload encoding is missing the \" \"private key unique identifier.\" ) \n    if self . is_tag_next ( enums . Tags . PUBLIC_KEY_UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _public_key_unique_identifier = primitives . TextString ( tag = enums . Tags . PUBLIC_KEY_UNIQUE_IDENTIFIER ) \n        self . _public_key_unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The CreateKeyPair response payload encoding is missing the \" \"public key unique identifier.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _private_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _private_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _public_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _public_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    self . is_oversized ( local_buffer ) "}
{"4786": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributeListResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        self . _unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is missing \" \"the unique identifier.\" ) \n    names = list ( ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_NAME , local_buffer ) : \n            name = primitives . TextString ( tag = enums . Tags . ATTRIBUTE_NAME ) \n            name . read ( local_buffer , kmip_version = kmip_version ) \n            names . append ( name ) \n        if len ( names ) == 0 : \n            raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is \" \"missing the attribute names.\" ) \n        self . _attribute_names = names \n    else : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_REFERENCE , local_buffer ) : \n            if self . is_type_next ( enums . Types . STRUCTURE , local_buffer ) : \n                reference = objects . AttributeReference ( ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                names . append ( primitives . TextString ( value = reference . attribute_name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            elif self . is_type_next ( enums . Types . ENUMERATION , local_buffer ) : \n                reference = primitives . Enumeration ( enums . Tags , tag = enums . Tags . ATTRIBUTE_REFERENCE ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                name = enums . convert_attribute_tag_to_name ( reference . value ) \n                names . append ( primitives . TextString ( value = name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            else : \n                raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding \" \"contains an invalid AttributeReference type.\" ) \n        self . _attribute_names = names \n    self . is_oversized ( local_buffer ) "}
{"4787": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributeList response payload is missing the unique \" \"identifier field.\" ) \n    if self . _attribute_names : \n        if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n            for attribute_name in self . _attribute_names : \n                attribute_name . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            for attribute_name in self . _attribute_names : \n                t = enums . convert_attribute_name_to_tag ( attribute_name . value ) \n                e = primitives . Enumeration ( enums . Tags , value = t , tag = enums . Tags . ATTRIBUTE_REFERENCE ) \n                e . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributeList response payload is missing the \" \"attribute names field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( GetAttributeListResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4789": "\ndef scan_policies ( self ) : \n    policy_files = get_json_files ( self . policy_directory ) \n    for f in set ( policy_files ) - set ( self . policy_files ) : \n        self . file_timestamps [ f ] = 0 \n    for f in set ( self . policy_files ) - set ( policy_files ) : \n        self . logger . info ( \"Removing policies for file: {}\" . format ( f ) ) \n        self . file_timestamps . pop ( f , None ) \n        for p in self . policy_cache . keys ( ) : \n            self . disassociate_policy_and_file ( p , f ) \n        for p in [ k for k , v in self . policy_map . items ( ) if v == f ] : \n            self . restore_or_delete_policy ( p ) \n    self . policy_files = policy_files \n    for f in sorted ( self . file_timestamps . keys ( ) ) : \n        t = os . path . getmtime ( f ) \n        if self . file_timestamps [ f ] < t : \n            self . logger . info ( \"Loading policies for file: {}\" . format ( f ) ) \n            self . file_timestamps [ f ] = t \n            old_p = [ k for k , v in self . policy_map . items ( ) if v == f ] \n            try : \n                new_p = operation_policy . read_policy_from_file ( f ) \n            except ValueError : \n                self . logger . error ( \"Failure loading file: {}\" . format ( f ) ) \n                self . logger . debug ( \"\" , exc_info = True ) \n                continue \n            for p in new_p . keys ( ) : \n                self . logger . info ( \"Loading policy: {}\" . format ( p ) ) \n                if p in self . reserved_policies : \n                    self . logger . warning ( \"Policy '{}' overwrites a reserved policy and \" \"will be thrown out.\" . format ( p ) ) \n                    continue \n                if p in sorted ( self . policy_store . keys ( ) ) : \n                    self . logger . debug ( \"Policy '{}' overwrites an existing \" \"policy.\" . format ( p ) ) \n                    if f != self . policy_map . get ( p ) : \n                        self . policy_cache . get ( p ) . append ( ( time . time ( ) , self . policy_map . get ( p ) , self . policy_store . get ( p ) ) ) \n                else : \n                    self . policy_cache [ p ] = [ ] \n                self . policy_store [ p ] = new_p . get ( p ) \n                self . policy_map [ p ] = f \n            for p in set ( old_p ) - set ( new_p . keys ( ) ) : \n                self . disassociate_policy_and_file ( p , f ) \n                self . restore_or_delete_policy ( p ) "}
{"4794": "\ndef get_client_identity_from_certificate ( certificate ) : \n    client_ids = get_common_names_from_certificate ( certificate ) \n    if 0 < len ( client_ids ) : \n        if 1 < len ( client_ids ) : \n            raise exceptions . PermissionDenied ( \"Multiple client identities found.\" ) \n        return client_ids [ 0 ] \n    else : \n        raise exceptions . PermissionDenied ( \"The certificate does not define any subject common names. \" \"Client identity unavailable.\" ) "}
{"4795": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the object \" \"type.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the \" \"template attribute.\" ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            value = objects . convert_attributes_to_template_attribute ( attributes ) \n            self . _template_attribute = value \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the \" \"attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4796": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create request payload is missing the object type field.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Create request payload is missing the template \" \"attribute field.\" ) \n    else : \n        if self . _template_attribute : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Create request payload is missing the template \" \"attribute field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( CreateRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4797": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create response payload encoding is missing the object \" \"type.\" ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        self . _unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create response payload encoding is missing the unique \" \"identifier.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    self . is_oversized ( local_buffer ) "}
{"4798": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create response payload is missing the object type field.\" ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create response payload is missing the unique identifier \" \"field.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CreateResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4801": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( DeriveKeyRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the object \" \"type.\" ) \n    unique_identifiers = [ ] \n    while self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n        unique_identifiers . append ( unique_identifier ) \n    if not unique_identifiers : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the unique \" \"identifiers.\" ) \n    else : \n        self . _unique_identifiers = unique_identifiers \n    if self . is_tag_next ( enums . Tags . DERIVATION_METHOD , local_buffer ) : \n        self . _derivation_method = primitives . Enumeration ( enums . DerivationMethod , tag = enums . Tags . DERIVATION_METHOD ) \n        self . _derivation_method . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"derivation method.\" ) \n    if self . is_tag_next ( enums . Tags . DERIVATION_PARAMETERS , local_buffer ) : \n        self . _derivation_parameters = attributes . DerivationParameters ( ) \n        self . _derivation_parameters . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"derivation parameters.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"template attribute.\" ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attrs = objects . Attributes ( ) \n            attrs . read ( local_buffer , kmip_version = kmip_version ) \n            value = objects . convert_attributes_to_template_attribute ( attrs ) \n            self . _template_attribute = value \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4802": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the object type \" \"field.\" ) \n    if self . _unique_identifiers : \n        for unique_identifier in self . _unique_identifiers : \n            unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the unique \" \"identifiers field.\" ) \n    if self . _derivation_method : \n        self . _derivation_method . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the derivation \" \"method field.\" ) \n    if self . _derivation_parameters : \n        self . _derivation_parameters . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the derivation \" \"parameters field.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the template \" \"attribute field.\" ) \n    else : \n        if self . _template_attribute : \n            attrs = objects . convert_template_attribute_to_attributes ( self . _template_attribute ) \n            attrs . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the template \" \"attribute field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( DeriveKeyRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4803": "\ndef is_attribute_supported ( self , attribute ) : \n    if attribute not in self . _attribute_rule_sets . keys ( ) : \n        return False \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if rule_set . version_added <= self . _version : \n        return True \n    else : \n        return False "}
{"4804": "\ndef is_attribute_deprecated ( self , attribute ) : \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if rule_set . version_deprecated : \n        if rule_set . version_deprecated <= self . _version : \n            return True \n        else : \n            return False \n    else : \n        return False "}
{"4810": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the AttributeReference \" \"object.\" . format ( kmip_version . value ) ) \n    super ( AttributeReference , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . VENDOR_IDENTIFICATION , local_buffer ) : \n        self . _vendor_identification = primitives . TextString ( tag = enums . Tags . VENDOR_IDENTIFICATION ) \n        self . _vendor_identification . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The AttributeReference encoding is missing the vendor \" \"identification string.\" ) \n    if self . is_tag_next ( enums . Tags . ATTRIBUTE_NAME , local_buffer ) : \n        self . _attribute_name = primitives . TextString ( tag = enums . Tags . ATTRIBUTE_NAME ) \n        self . _attribute_name . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The AttributeReference encoding is missing the attribute \" \"name string.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4811": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the AttributeReference \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _vendor_identification : \n        self . _vendor_identification . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The AttributeReference is missing the vendor identification \" \"field.\" ) \n    if self . _attribute_name : \n        self . _attribute_name . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The AttributeReference is missing the attribute name field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( AttributeReference , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4812": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    super ( Attributes , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    while True : \n        if 3 > len ( local_stream ) : \n            break \n        tag = struct . unpack ( '!I' , b'\\x00' + local_stream . peek ( 3 ) ) [ 0 ] \n        if enums . is_enum_value ( enums . Tags , tag ) : \n            tag = enums . Tags ( tag ) \n            if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n                raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n            value = self . _factory . create_attribute_value_by_enum ( tag , None ) \n            value . read ( local_stream , kmip_version = kmip_version ) \n            self . _attributes . append ( value ) \n        else : \n            break \n    self . is_oversized ( local_stream ) "}
{"4813": "\ndef write ( self , output_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    local_stream = BytearrayStream ( ) \n    for attribute in self . _attributes : \n        tag = attribute . tag \n        if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n            raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n        attribute . write ( local_stream , kmip_version = kmip_version ) \n    self . length = local_stream . length ( ) \n    super ( Attributes , self ) . write ( output_stream , kmip_version = kmip_version ) \n    output_stream . write ( local_stream . buffer ) "}
{"4834": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ObjectDefaults object.\" . format ( kmip_version . value ) ) \n    super ( ObjectDefaults , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The ObjectDefaults encoding is missing the object type \" \"enumeration.\" ) \n    if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n        self . _attributes = Attributes ( ) \n        self . _attributes . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The ObjectDefaults encoding is missing the attributes \" \"structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4835": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ObjectDefaults object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ObjectDefaults structure is missing the object type \" \"field.\" ) \n    if self . _attributes : \n        self . _attributes . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ObjectDefaults structure is missing the attributes field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( ObjectDefaults , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4836": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the DefaultsInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( DefaultsInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    object_defaults = [ ] \n    while self . is_tag_next ( enums . Tags . OBJECT_DEFAULTS , local_buffer ) : \n        object_default = ObjectDefaults ( ) \n        object_default . read ( local_buffer , kmip_version = kmip_version ) \n        object_defaults . append ( object_default ) \n    if len ( object_defaults ) == 0 : \n        raise exceptions . InvalidKmipEncoding ( \"The DefaultsInformation encoding is missing the object \" \"defaults structure.\" ) \n    else : \n        self . _object_defaults = object_defaults \n    self . is_oversized ( local_buffer ) "}
{"4837": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the DefaultsInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _object_defaults : \n        for object_default in self . _object_defaults : \n            object_default . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DefaultsInformation structure is missing the object \" \"defaults field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( DefaultsInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4838": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the RNGParameters object.\" . format ( kmip_version . value ) ) \n    super ( RNGParameters , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . RNG_ALGORITHM , local_buffer ) : \n        rng_algorithm = primitives . Enumeration ( enums . RNGAlgorithm , tag = enums . Tags . RNG_ALGORITHM ) \n        rng_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _rng_algorithm = rng_algorithm \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The RNGParameters encoding is missing the RNG algorithm.\" ) \n    if self . is_tag_next ( enums . Tags . CRYPTOGRAPHIC_ALGORITHM , local_buffer ) : \n        cryptographic_algorithm = primitives . Enumeration ( enums . CryptographicAlgorithm , tag = enums . Tags . CRYPTOGRAPHIC_ALGORITHM ) \n        cryptographic_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _cryptographic_algorithm = cryptographic_algorithm \n    if self . is_tag_next ( enums . Tags . CRYPTOGRAPHIC_LENGTH , local_buffer ) : \n        cryptographic_length = primitives . Integer ( tag = enums . Tags . CRYPTOGRAPHIC_LENGTH ) \n        cryptographic_length . read ( local_buffer , kmip_version = kmip_version ) \n        self . _cryptographic_length = cryptographic_length \n    if self . is_tag_next ( enums . Tags . HASHING_ALGORITHM , local_buffer ) : \n        hashing_algorithm = primitives . Enumeration ( enums . HashingAlgorithm , tag = enums . Tags . HASHING_ALGORITHM ) \n        hashing_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _hashing_algorithm = hashing_algorithm \n    if self . is_tag_next ( enums . Tags . DRBG_ALGORITHM , local_buffer ) : \n        drbg_algorithm = primitives . Enumeration ( enums . DRBGAlgorithm , tag = enums . Tags . DRBG_ALGORITHM ) \n        drbg_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _drbg_algorithm = drbg_algorithm \n    if self . is_tag_next ( enums . Tags . RECOMMENDED_CURVE , local_buffer ) : \n        recommended_curve = primitives . Enumeration ( enums . RecommendedCurve , tag = enums . Tags . RECOMMENDED_CURVE ) \n        recommended_curve . read ( local_buffer , kmip_version = kmip_version ) \n        self . _recommended_curve = recommended_curve \n    if self . is_tag_next ( enums . Tags . FIPS186_VARIATION , local_buffer ) : \n        fips186_variation = primitives . Enumeration ( enums . FIPS186Variation , tag = enums . Tags . FIPS186_VARIATION ) \n        fips186_variation . read ( local_buffer , kmip_version = kmip_version ) \n        self . _fips186_variation = fips186_variation \n    if self . is_tag_next ( enums . Tags . PREDICTION_RESISTANCE , local_buffer ) : \n        prediction_resistance = primitives . Boolean ( tag = enums . Tags . PREDICTION_RESISTANCE ) \n        prediction_resistance . read ( local_buffer , kmip_version = kmip_version ) \n        self . _prediction_resistance = prediction_resistance \n    self . is_oversized ( local_buffer ) "}
{"4839": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the RNGParameters object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _rng_algorithm : \n        self . _rng_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The RNGParameters structure is missing the RNG algorithm \" \"field.\" ) \n    if self . _cryptographic_algorithm : \n        self . _cryptographic_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _cryptographic_length : \n        self . _cryptographic_length . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _hashing_algorithm : \n        self . _hashing_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _drbg_algorithm : \n        self . _drbg_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _recommended_curve : \n        self . _recommended_curve . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _fips186_variation : \n        self . _fips186_variation . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _prediction_resistance : \n        self . _prediction_resistance . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( RNGParameters , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4840": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ProfileInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( ProfileInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . PROFILE_NAME , local_buffer ) : \n        profile_name = primitives . Enumeration ( enums . ProfileName , tag = enums . Tags . PROFILE_NAME ) \n        profile_name . read ( local_buffer , kmip_version = kmip_version ) \n        self . _profile_name = profile_name \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The ProfileInformation encoding is missing the profile name.\" ) \n    if self . is_tag_next ( enums . Tags . SERVER_URI , local_buffer ) : \n        server_uri = primitives . TextString ( tag = enums . Tags . SERVER_URI ) \n        server_uri . read ( local_buffer , kmip_version = kmip_version ) \n        self . _server_uri = server_uri \n    if self . is_tag_next ( enums . Tags . SERVER_PORT , local_buffer ) : \n        server_port = primitives . Integer ( tag = enums . Tags . SERVER_PORT ) \n        server_port . read ( local_buffer , kmip_version = kmip_version ) \n        self . _server_port = server_port \n    self . is_oversized ( local_buffer ) "}
{"4841": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ProfileInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _profile_name : \n        self . _profile_name . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ProfileInformation structure is missing the profile \" \"name field.\" ) \n    if self . _server_uri : \n        self . _server_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _server_port : \n        self . _server_port . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( ProfileInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4842": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ValidationInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _validation_authority_type : \n        self . _validation_authority_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation authority type field.\" ) \n    if self . _validation_authority_country : \n        self . _validation_authority_country . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_authority_uri : \n        self . _validation_authority_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_version_major : \n        self . _validation_version_major . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation version major field.\" ) \n    if self . _validation_version_minor : \n        self . _validation_version_minor . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_type : \n        self . _validation_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation type field.\" ) \n    if self . _validation_level : \n        self . _validation_level . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation level field.\" ) \n    if self . _validation_certificate_identifier : \n        self . _validation_certificate_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_certificate_uri : \n        self . _validation_certificate_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_vendor_uri : \n        self . _validation_vendor_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_profiles : \n        for validation_profile in self . _validation_profiles : \n            validation_profile . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( ValidationInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4843": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the CapabilityInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( CapabilityInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . STREAMING_CAPABILITY , local_buffer ) : \n        streaming_capability = primitives . Boolean ( tag = enums . Tags . STREAMING_CAPABILITY ) \n        streaming_capability . read ( local_buffer , kmip_version = kmip_version ) \n        self . _streaming_capability = streaming_capability \n    if self . is_tag_next ( enums . Tags . ASYNCHRONOUS_CAPABILITY , local_buffer ) : \n        asynchronous_capability = primitives . Boolean ( tag = enums . Tags . ASYNCHRONOUS_CAPABILITY ) \n        asynchronous_capability . read ( local_buffer , kmip_version = kmip_version ) \n        self . _asynchronous_capability = asynchronous_capability \n    if self . is_tag_next ( enums . Tags . ATTESTATION_CAPABILITY , local_buffer ) : \n        attestation_capability = primitives . Boolean ( tag = enums . Tags . ATTESTATION_CAPABILITY ) \n        attestation_capability . read ( local_buffer , kmip_version = kmip_version ) \n        self . _attestation_capability = attestation_capability \n    if enums . KMIPVersion . KMIP_1_4 <= kmip_version : \n        if self . is_tag_next ( enums . Tags . BATCH_UNDO_CAPABILITY , local_buffer ) : \n            batch_undo_capability = primitives . Boolean ( tag = enums . Tags . BATCH_UNDO_CAPABILITY ) \n            batch_undo_capability . read ( local_buffer , kmip_version = kmip_version ) \n            self . _batch_continue_capability = batch_undo_capability \n        if self . is_tag_next ( enums . Tags . BATCH_CONTINUE_CAPABILITY , local_buffer ) : \n            batch_continue_capability = primitives . Boolean ( tag = enums . Tags . BATCH_CONTINUE_CAPABILITY ) \n            batch_continue_capability . read ( local_buffer , kmip_version = kmip_version ) \n            self . _batch_continue_capability = batch_continue_capability \n    if self . is_tag_next ( enums . Tags . UNWRAP_MODE , local_buffer ) : \n        unwrap_mode = primitives . Enumeration ( enums . UnwrapMode , tag = enums . Tags . UNWRAP_MODE ) \n        unwrap_mode . read ( local_buffer , kmip_version = kmip_version ) \n        self . _unwrap_mode = unwrap_mode \n    if self . is_tag_next ( enums . Tags . DESTROY_ACTION , local_buffer ) : \n        destroy_action = primitives . Enumeration ( enums . DestroyAction , tag = enums . Tags . DESTROY_ACTION ) \n        destroy_action . read ( local_buffer , kmip_version = kmip_version ) \n        self . _destroy_action = destroy_action \n    if self . is_tag_next ( enums . Tags . SHREDDING_ALGORITHM , local_buffer ) : \n        shredding_algorithm = primitives . Enumeration ( enums . ShreddingAlgorithm , tag = enums . Tags . SHREDDING_ALGORITHM ) \n        shredding_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _shredding_algorithm = shredding_algorithm \n    if self . is_tag_next ( enums . Tags . RNG_MODE , local_buffer ) : \n        rng_mode = primitives . Enumeration ( enums . RNGMode , tag = enums . Tags . RNG_MODE ) \n        rng_mode . read ( local_buffer , kmip_version = kmip_version ) \n        self . _rng_mode = rng_mode \n    self . is_oversized ( local_buffer ) "}
{"4844": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if enums . KMIPVersion . KMIP_1_3 > kmip_version : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the CapabilityInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _streaming_capability : \n        self . _streaming_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _asynchronous_capability : \n        self . _asynchronous_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _attestation_capability : \n        self . _attestation_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_1_4 <= kmip_version : \n        if self . _batch_undo_capability : \n            self . _batch_undo_capability . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _batch_continue_capability : \n            self . _batch_continue_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _unwrap_mode : \n        self . _unwrap_mode . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _destroy_action : \n        self . _destroy_action . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _shredding_algorithm : \n        self . _shredding_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _rng_mode : \n        self . _rng_mode . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CapabilityInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4847": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( LocateRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . MAXIMUM_ITEMS , local_buffer ) : \n        self . _maximum_items = primitives . Integer ( tag = enums . Tags . MAXIMUM_ITEMS ) \n        self . _maximum_items . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . OFFSET_ITEMS , local_buffer ) : \n        self . _offset_items = primitives . Integer ( tag = enums . Tags . OFFSET_ITEMS ) \n        self . _offset_items . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . STORAGE_STATUS_MASK , local_buffer ) : \n        self . _storage_status_mask = primitives . Integer ( tag = enums . Tags . STORAGE_STATUS_MASK ) \n        self . _storage_status_mask . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . OBJECT_GROUP_MEMBER , local_buffer ) : \n        self . _object_group_member = primitives . Enumeration ( enums . ObjectGroupMember , tag = enums . Tags . OBJECT_GROUP_MEMBER ) \n        self . _object_group_member . read ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE , local_buffer ) : \n            attribute = objects . Attribute ( ) \n            attribute . read ( local_buffer , kmip_version = kmip_version ) \n            self . _attributes . append ( attribute ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            temp_attr = objects . convert_attributes_to_template_attribute ( attributes ) \n            self . _attributes = temp_attr . attributes \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Locate request payload encoding is missing the \" \"attributes structure.\" ) "}
{"4848": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _maximum_items : \n        self . _maximum_items . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _offset_items : \n        self . _offset_items . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _storage_status_mask : \n        self . _storage_status_mask . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _object_group_member : \n        self . _object_group_member . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        if self . _attributes : \n            for attribute in self . attributes : \n                attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _attributes : \n            template_attribute = objects . TemplateAttribute ( attributes = self . attributes ) \n            attributes = objects . convert_template_attribute_to_attributes ( template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Locate request payload is missing the attributes \" \"list.\" ) \n    self . length = local_buffer . length ( ) \n    super ( LocateRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4886": "\ndef create ( self , algorithm , length , operation_policy_name = None , name = None , cryptographic_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or 0 >= length : \n        raise TypeError ( \"length must be a positive integer\" ) \n    if cryptographic_usage_mask is not None : \n        if not isinstance ( cryptographic_usage_mask , list ) or all ( isinstance ( item , enums . CryptographicUsageMask ) for item in cryptographic_usage_mask ) is False : \n            raise TypeError ( \"cryptographic_usage_mask must be a list of \" \"CryptographicUsageMask enumerations\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    key_attributes = self . _build_key_attributes ( algorithm , length , cryptographic_usage_mask ) \n    key_attributes . extend ( common_attributes ) \n    if name : \n        key_attributes . extend ( self . _build_name_attribute ( name ) ) \n    template = cobjects . TemplateAttribute ( attributes = key_attributes ) \n    result = self . proxy . create ( enums . ObjectType . SYMMETRIC_KEY , template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        return result . uuid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4887": "\ndef create_key_pair ( self , algorithm , length , operation_policy_name = None , public_name = None , public_usage_mask = None , private_name = None , private_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or 0 >= length : \n        raise TypeError ( \"length must be a positive integer\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    algorithm_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_ALGORITHM , algorithm ) \n    length_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_LENGTH , length ) \n    common_attributes . extend ( [ algorithm_attribute , length_attribute ] ) \n    template = cobjects . TemplateAttribute ( attributes = common_attributes , tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n    public_template = None \n    names = None \n    if public_name : \n        names = self . _build_name_attribute ( name = public_name ) \n    attrs = [ ] \n    if public_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , public_usage_mask ) ] \n    if names or attrs : \n        public_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n    private_template = None \n    names = None \n    if private_name : \n        names = self . _build_name_attribute ( name = private_name ) \n    attrs = [ ] \n    if private_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , private_usage_mask ) ] \n    if names or attrs : \n        private_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n    result = self . proxy . create_key_pair ( common_template_attribute = template , private_key_template_attribute = private_template , public_key_template_attribute = public_template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        public_uid = result . public_key_uuid \n        private_uid = result . private_key_uuid \n        return public_uid , private_uid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4906": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _operations : \n        for operation in self . _operations : \n            operation . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _object_types : \n        for object_type in self . _object_types : \n            object_type . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _vendor_identification : \n        self . _vendor_identification . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _server_information : \n        self . _server_information . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _application_namespaces : \n        for application_namespace in self . _application_namespaces : \n            application_namespace . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_1_1 <= kmip_version : \n        if self . _extension_information : \n            for extension_information in self . _extension_information : \n                extension_information . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_1_2 <= kmip_version : \n        if self . _attestation_types : \n            for attestation_type in self . _attestation_types : \n                attestation_type . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_1_3 <= kmip_version : \n        if self . _rng_parameters : \n            for rng_parameters in self . _rng_parameters : \n                rng_parameters . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _profile_information : \n            for profile_information in self . _profile_information : \n                profile_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _validation_information : \n            for validation_information in self . _validation_information : \n                validation_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _capability_information : \n            for capability_information in self . _capability_information : \n                capability_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _client_registration_methods : \n            for client_reg_method in self . _client_registration_methods : \n                client_reg_method . write ( local_buffer , kmip_version = kmip_version ) \n    if enums . KMIPVersion . KMIP_2_0 <= kmip_version : \n        if self . _defaults_information : \n            self . _defaults_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _storage_protection_masks : \n            for storage_protection_mask in self . _storage_protection_masks : \n                storage_protection_mask . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( QueryResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4907": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributesResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n        self . unique_identifier = unique_identifier . value \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributes response payload encoding is missing the \" \"unique identifier.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        self . _attributes = list ( ) \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE , local_buffer ) : \n            attribute = objects . Attribute ( ) \n            attribute . read ( local_buffer , kmip_version = kmip_version ) \n            self . _attributes . append ( attribute ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            temp_attr = objects . convert_attributes_to_template_attribute ( attributes ) \n            self . _attributes = temp_attr . attributes \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The GetAttributes response payload encoding is missing \" \"the attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4908": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributes response payload is missing the unique \" \"identifier field.\" ) \n    if enums . KMIPVersion . KMIP_2_0 > kmip_version : \n        for attribute in self . _attributes : \n            attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _attributes : \n            template_attribute = objects . TemplateAttribute ( attributes = self . attributes ) \n            attributes = objects . convert_template_attribute_to_attributes ( template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The GetAttributes response payload is missing the \" \"attributes list.\" ) \n    self . length = local_buffer . length ( ) \n    super ( GetAttributesResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4923": "\ndef timesince ( value ) : \n    if not value : \n        return \"\" \n    if not isinstance ( value , datetime . date ) : \n        return value \n    now = datetime . datetime . now ( ) \n    delta = now - value \n    if now < value : \n        return \"right now\" \n    elif 365 < delta . days : \n        return '%d years ago' % ( delta . days / 365 ) \n    elif 30 < delta . days : \n        return '%d months ago' % ( delta . days / 30 ) \n    elif 0 < delta . days : \n        return '%d days ago' % delta . days \n    elif 3600 < delta . seconds : \n        return '%d hours ago' % ( delta . seconds / 3600 ) \n    elif 60 < delta . seconds : \n        return '%d minutes ago' % ( delta . seconds / 60 ) \n    else : \n        return 'right now' "}
{"4959": "\ndef find_globals ( code ) : \n    cur_byte = 0 \n    byte_code = code . co_code \n    names = set ( ) \n    while len ( byte_code ) > cur_byte : \n        op = ord ( byte_code [ cur_byte ] ) \n        if dis . HAVE_ARGUMENT <= op : \n            if op == _LOAD_GLOBAL : \n                oparg = ord ( byte_code [ cur_byte + 1 ] ) + ( ord ( byte_code [ cur_byte + 2 ] ) << 8 ) \n                name = code . co_names [ oparg ] \n                names . add ( name ) \n            cur_byte += 2 \n        cur_byte += 1 \n    return names "}
{"4963": "\ndef _cubic_bernstein_extrema ( p0 , p1 , p2 , p3 ) : \n    a = 3. * ( p3 - p0 + 3. * ( p1 - p2 ) ) \n    b = 6. * ( p0 + p2 - 2. * p1 ) \n    c = 3. * ( p1 - p0 ) \n    if a == 0 : \n        if b == 0 : \n            return ( ) \n        return ( - c / b , ) \n    d = b * b - 4. * a * c \n    if 0 > d : \n        return ( ) \n    k = - 2. * a \n    if d == 0 : \n        return ( b / k , ) \n    r = math . sqrt ( d ) \n    return ( ( b + r ) / k , ( b - r ) / k ) "}
{"4965": "\ndef _build_choices ( self ) : \n    tree_token = u'sitetree_tree from \"%s\" template \"%s\"' % ( self . tree , self . template ) \n    context_kwargs = { 'current_app' : 'admin' } \n    context = template . Context ( context_kwargs ) if ( 1 , 8 ) <= VERSION else template . Context ( ** context_kwargs ) \n    context . update ( { 'request' : object ( ) } ) \n    choices_str = sitetree_tree ( Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = tree_token ) ) . render ( context ) \n    tree_choices = [ ( ITEMS_FIELD_ROOT_ID , self . root_title ) ] \n    for line in choices_str . splitlines ( ) : \n        if line . strip ( ) : \n            splitted = line . split ( ':::' ) \n            tree_choices . append ( ( splitted [ 0 ] , mark_safe ( splitted [ 1 ] ) ) ) \n    return tree_choices "}
{"4966": "\ndef options_getter ( command_options ) : \n    def get_options ( option_func = None ) : \n        from optparse import make_option \n        from django . core . management . base import BaseCommand \n        func = option_func or make_option \n        options = tuple ( [ func ( * option . args , ** option . kwargs ) for option in command_options ] ) \n        if option_func is None : \n            if ( 1 , 8 ) > VERSION : \n                result = BaseCommand . option_list + options \n            else : \n                result = [ ] \n        else : \n            result = options \n        return result \n    return get_options "}
{"4982": "\ndef get_ancestor_level ( self , current_item , depth = 1 ) : \n    if current_item . parent is None : \n        return current_item \n    if 1 >= depth : \n        return current_item . parent \n    return self . get_ancestor_level ( current_item . parent , depth = depth - 1 ) "}
{"4989": "\ndef update_has_children ( self , tree_alias , tree_items , navigation_type ) : \n    get_children = self . get_children \n    filter_items = self . filter_items \n    apply_hook = self . apply_hook \n    for tree_item in tree_items : \n        children = get_children ( tree_alias , tree_item ) \n        children = filter_items ( children , navigation_type ) \n        children = apply_hook ( children , '%s.has_children' % navigation_type ) \n        tree_item . has_children = 0 < len ( children ) "}
{"4999": "\ndef for_tag ( cls , parser , token , preposition , error_hint ) : \n    tokens = token . split_contents ( ) \n    if 3 <= len ( tokens ) and tokens [ 1 ] == preposition : \n        as_var = cls . get_as_var ( tokens ) \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        return cls ( tree_alias , as_var ) \n    raise template . TemplateSyntaxError ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error_hint ) ) "}
{"5020": "\ndef create_attrs_for_span ( sample_rate = 100.0 , trace_id = None , span_id = None , use_128bit_trace_id = False , ) : \n    if trace_id is None : \n        if use_128bit_trace_id : \n            trace_id = generate_random_128bit_string ( ) \n        else : \n            trace_id = generate_random_64bit_string ( ) \n    if span_id is None : \n        span_id = generate_random_64bit_string ( ) \n    if sample_rate == 0.0 : \n        is_sampled = False \n    else : \n        is_sampled = sample_rate > ( random . random ( ) * 100 ) \n    return ZipkinAttrs ( trace_id = trace_id , span_id = span_id , parent_span_id = None , flags = '0' , is_sampled = is_sampled , ) "}
{"5032": "\ndef create_protobuf_span ( span ) : \n    pb_kwargs = { } \n    pb_kwargs [ 'trace_id' ] = _hex_to_bytes ( span . trace_id ) \n    if span . parent_id : \n        pb_kwargs [ 'parent_id' ] = _hex_to_bytes ( span . parent_id ) \n    pb_kwargs [ 'id' ] = _hex_to_bytes ( span . span_id ) \n    pb_kind = _get_protobuf_kind ( span . kind ) \n    if pb_kind : \n        pb_kwargs [ 'kind' ] = pb_kind \n    if span . name : \n        pb_kwargs [ 'name' ] = span . name \n    if span . timestamp : \n        pb_kwargs [ 'timestamp' ] = int ( span . timestamp * 1000 * 1000 ) \n    if span . duration : \n        pb_kwargs [ 'duration' ] = int ( span . duration * 1000 * 1000 ) \n    if span . local_endpoint : \n        pb_kwargs [ 'local_endpoint' ] = _convert_endpoint ( span . local_endpoint ) \n    if span . remote_endpoint : \n        pb_kwargs [ 'remote_endpoint' ] = _convert_endpoint ( span . remote_endpoint ) \n    if 0 < len ( span . annotations ) : \n        pb_kwargs [ 'annotations' ] = _convert_annotations ( span . annotations ) \n    if 0 < len ( span . tags ) : \n        pb_kwargs [ 'tags' ] = span . tags \n    if span . debug : \n        pb_kwargs [ 'debug' ] = span . debug \n    if span . shared : \n        pb_kwargs [ 'shared' ] = span . shared \n    return zipkin_pb2 . Span ( ** pb_kwargs ) "}
{"5033": "\ndef _hex_to_bytes ( hex_id ) : \n    if 16 >= len ( hex_id ) : \n        int_id = unsigned_hex_to_signed_int ( hex_id ) \n        return struct . pack ( '>q' , int_id ) \n    else : \n        high_id = unsigned_hex_to_signed_int ( hex_id [ : - 16 ] ) \n        high_bin = struct . pack ( '>q' , high_id ) \n        low_id = unsigned_hex_to_signed_int ( hex_id [ - 16 : ] ) \n        low_bin = struct . pack ( '>q' , low_id ) \n        return high_bin + low_bin "}
{"5043": "\ndef create_span ( span_id , parent_span_id , trace_id , span_name , annotations , binary_annotations , timestamp_s , duration_s , ) : \n    trace_id_length = len ( trace_id ) \n    trace_id_high = None \n    if 16 < trace_id_length : \n        assert trace_id_length == 32 \n        trace_id , trace_id_high = trace_id [ 16 : ] , trace_id [ : 16 ] \n    if trace_id_high : \n        trace_id_high = unsigned_hex_to_signed_int ( trace_id_high ) \n    span_dict = { 'trace_id' : unsigned_hex_to_signed_int ( trace_id ) , 'name' : span_name , 'id' : unsigned_hex_to_signed_int ( span_id ) , 'annotations' : annotations , 'binary_annotations' : binary_annotations , 'timestamp' : int ( timestamp_s * 1000000 ) if timestamp_s else None , 'duration' : int ( duration_s * 1000000 ) if duration_s else None , 'trace_id_high' : trace_id_high , } \n    if parent_span_id : \n        span_dict [ 'parent_id' ] = unsigned_hex_to_signed_int ( parent_span_id ) \n    return zipkin_core . Span ( ** span_dict ) "}
{"5046": "\ndef detect_span_version_and_encoding ( message ) : \n    if isinstance ( message , six . string_types ) : \n        if six . PY2 : \n            message = six . b ( message ) \n        else : \n            message = message . encode ( 'utf-8' ) \n    if 2 > len ( message ) : \n        raise ZipkinError ( \"Invalid span format. Message too short.\" ) \n    if 16 >= six . byte2int ( message ) : \n        if six . byte2int ( message ) == 10 and six . byte2int ( message [ 1 : 2 ] ) != 0 : \n            return Encoding . V2_PROTO3 \n        return Encoding . V1_THRIFT \n    str_msg = message . decode ( 'utf-8' ) \n    if str_msg [ 0 ] == '[' : \n        span_list = json . loads ( str_msg ) \n        if 0 < len ( span_list ) : \n            for span in span_list : \n                if any ( word in span for word in _V2_ATTRIBUTES ) : \n                    return Encoding . V2_JSON \n                elif ( 'binaryAnnotations' in span or ( 'annotations' in span and 'endpoint' in span [ 'annotations' ] ) ) : \n                    return Encoding . V1_JSON \n            return Encoding . V2_JSON \n    raise ZipkinError ( \"Unknown or unsupported span encoding\" ) "}
{"5060": "\ndef date_fixup_pre_processor ( transactions , tag , tag_dict , * args ) : \n    if tag_dict [ 'month' ] == '02' : \n        year = int ( tag_dict [ 'year' ] , 10 ) \n        _ , max_month_day = calendar . monthrange ( year , 2 ) \n        if max_month_day < int ( tag_dict [ 'day' ] , 10 ) : \n            tag_dict [ 'day' ] = str ( max_month_day ) \n    return tag_dict "}
{"5071": "\nasync def get_bots ( self , limit , offset ) : \n    if 500 < limit : \n        limit = 50 \n    return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) ) "}
{"5072": "\ndef read ( self ) : \n    packet = self . packet \n    with self . __read_lock : \n        buffer = self . __buffer \n        while packet > len ( buffer ) : \n            buffer += self . _read_data ( ) \n        length = self . __unpack ( buffer [ : packet ] ) [ 0 ] + packet \n        while length > len ( buffer ) : \n            buffer += self . _read_data ( ) \n        term , self . __buffer = decode ( buffer [ packet : ] ) \n    return term "}
{"5075": "\ndef decode ( string ) : \n    if not string : \n        raise IncompleteData ( string ) \n    if string [ 0 ] != 131 : \n        raise ValueError ( \"unknown protocol version: %r\" % string [ 0 ] ) \n    if string [ 1 : 2 ] == b'P' : \n        if 16 > len ( string ) : \n            raise IncompleteData ( string ) \n        d = decompressobj ( ) \n        term_string = d . decompress ( string [ 6 : ] ) + d . flush ( ) \n        uncompressed_size , = _int4_unpack ( string [ 2 : 6 ] ) \n        if len ( term_string ) != uncompressed_size : \n            raise ValueError ( \"invalid compressed tag, \" \"%d bytes but got %d\" % ( uncompressed_size , len ( term_string ) ) ) \n        term , _tail = decode_term ( term_string ) \n        return term , d . unused_data \n    return decode_term ( string [ 1 : ] ) "}
{"5076": "\ndef encode ( term , compressed = False ) : \n    encoded_term = encode_term ( term ) \n    if compressed : \n        if compressed is True : \n            compressed = 6 \n        elif 0 > compressed or 9 < compressed : \n            raise ValueError ( \"invalid compression level: %r\" % ( compressed , ) ) \n        zlib_term = compress ( encoded_term , compressed ) \n        ln = len ( encoded_term ) \n        if ln >= len ( zlib_term ) + 5 : \n            return b\"\\x83P\" + _int4_pack ( ln ) + zlib_term \n    return b\"\\x83\" + encoded_term "}
{"5102": "\ndef absorb ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % HASH_LENGTH ) or HASH_LENGTH ) \n    trits += [ 0 ] * ( HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) \n    if 1 > length : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while length > offset : \n        start = offset \n        stop = min ( start + HASH_LENGTH , length ) \n        self . _state [ 0 : stop - start ] = trits [ start : stop ] \n        self . _transform ( ) \n        offset += HASH_LENGTH "}
{"5103": "\ndef squeeze ( self , trits , offset = 0 , length = HASH_LENGTH ) : \n    if length % HASH_LENGTH != 0 : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze`.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } ) \n    trits . extend ( [ 0 ] * max ( 0 , length - len ( trits ) ) ) \n    if HASH_LENGTH > len ( trits ) - offset : \n        raise with_context ( exc = ValueError ( 'Invalid offset passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length } , ) \n    while HASH_LENGTH <= length : \n        trits [ offset : offset + HASH_LENGTH ] = self . _state [ 0 : HASH_LENGTH ] \n        self . _transform ( ) \n        offset += HASH_LENGTH \n        length -= HASH_LENGTH "}
{"5104": "\ndef _transform ( self ) : \n    state_length = STATE_LENGTH \n    truth_table = TRUTH_TABLE \n    prev_state = self . _state [ : ] \n    new_state = prev_state [ : ] \n    index = 0 \n    for _ in range ( NUMBER_OF_ROUNDS ) : \n        prev_trit = prev_state [ index ] \n        for pos in range ( state_length ) : \n            index += ( 364 if 365 > index else - 365 ) \n            new_trit = prev_state [ index ] \n            new_state [ pos ] = truth_table [ prev_trit + ( 3 * new_trit ) + 4 ] \n            prev_trit = new_trit \n        prev_state = new_state \n        new_state = new_state [ : ] \n    self . _state = new_state "}
{"5110": "\ndef _add_trits ( left , right ) : \n    res = left + right \n    return res if - 2 < res < 2 else ( 0 > res ) - ( 0 < res ) "}
{"5145": "\ndef get_messages ( self , errors = 'drop' ) : \n    decode_errors = 'strict' if errors == 'drop' else errors \n    messages = [ ] \n    for group in self . group_transactions ( ) : \n        if 0 > group [ 0 ] . value : \n            continue \n        message_trytes = TryteString ( b'' ) \n        for txn in group : \n            message_trytes += txn . signature_message_fragment \n        if message_trytes : \n            try : \n                messages . append ( message_trytes . decode ( decode_errors ) ) \n            except ( TrytesDecodeError , UnicodeDecodeError ) : \n                if errors != 'drop' : \n                    raise \n    return messages "}
{"5154": "\ndef _create_validator ( self ) : \n    grouped_transactions = self . bundle . group_transactions ( ) \n    bundle_hash = self . bundle . hash \n    last_index = len ( self . bundle ) - 1 \n    balance = 0 \n    counter = 0 \n    for group in grouped_transactions : \n        for txn in group : \n            balance += txn . value \n            if txn . bundle_hash != bundle_hash : \n                yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) \n            if txn . current_index != counter : \n                yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current_index , i = counter , ) ) \n            if txn . last_index != last_index : \n                yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last_index , expected = last_index , i = counter , ) ) \n            counter += 1 \n    if balance != 0 : \n        yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) \n    if not self . _errors : \n        signature_validation_queue = [ ] \n        for group in grouped_transactions : \n            if 0 <= group [ 0 ] . value : \n                continue \n            validate_group_signature = True \n            for j , txn in enumerate ( group ) : \n                if ( 0 < j ) and ( txn . value != 0 ) : \n                    yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current_index , ) ) \n                    validate_group_signature = False \n                    continue \n            if validate_group_signature : \n                signature_validation_queue . append ( group ) \n        if signature_validation_queue : \n            for error in self . _get_bundle_signature_errors ( signature_validation_queue ) : \n                yield error "}
{"5161": "\ndef sign_input_transactions ( self , bundle , start_index ) : \n    if not bundle . hash : \n        raise with_context ( exc = ValueError ( 'Cannot sign inputs without a bundle hash!' ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n    from iota . crypto . signing import SignatureFragmentGenerator \n    signature_fragment_generator = ( SignatureFragmentGenerator ( self , bundle . hash ) ) \n    for j in range ( self . security_level ) : \n        try : \n            txn = bundle [ start_index + j ] \n        except IndexError as e : \n            raise with_context ( exc = e , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'current_index' : start_index + j , } , ) \n        if 0 < txn . value : \n            raise with_context ( exc = ValueError ( 'Attempting to sign non-input transaction #{i} ' '(value={value}).' . format ( i = txn . current_index , value = txn . value , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        if txn . signature_message_fragment : \n            raise with_context ( exc = ValueError ( 'Attempting to sign input transaction #{i}, ' 'but it has a non-empty fragment ' '(is it already signed?).' . format ( i = txn . current_index , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        txn . signature_message_fragment = next ( signature_fragment_generator ) "}
{"5163": "\ndef absorb ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits += [ 0 ] * ( TRIT_HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) \n    if 1 > length : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while length > offset : \n        stop = min ( offset + TRIT_HASH_LENGTH , length ) \n        if stop - offset == TRIT_HASH_LENGTH : \n            trits [ stop - 1 ] = 0 \n        signed_nums = conv . convertToBytes ( trits [ offset : stop ] ) \n        unsigned_bytes = bytearray ( conv . convert_sign ( b ) for b in signed_nums ) \n        self . k . update ( unsigned_bytes ) \n        offset += TRIT_HASH_LENGTH "}
{"5164": "\ndef squeeze ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits += [ 0 ] * ( TRIT_HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) or TRIT_HASH_LENGTH \n    if 1 > length : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while length > offset : \n        unsigned_hash = self . k . digest ( ) \n        if PY2 : \n            unsigned_hash = map ( ord , unsigned_hash ) \n        signed_hash = [ conv . convert_sign ( b ) for b in unsigned_hash ] \n        trits_from_hash = conv . convertToTrits ( signed_hash ) \n        trits_from_hash [ TRIT_HASH_LENGTH - 1 ] = 0 \n        stop = min ( TRIT_HASH_LENGTH , length - offset ) \n        trits [ offset : offset + stop ] = trits_from_hash [ 0 : stop ] \n        flipped_bytes = bytearray ( conv . convert_sign ( ~ b ) for b in unsigned_hash ) \n        self . reset ( ) \n        self . k . update ( flipped_bytes ) \n        offset += TRIT_HASH_LENGTH "}
{"5169": "\ndef add_transaction ( self , transaction ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if 0 > transaction . value : \n        raise ValueError ( 'Use ``add_inputs`` to add inputs to the bundle.' ) \n    self . _transactions . append ( ProposedTransaction ( address = transaction . address , value = transaction . value , tag = transaction . tag , message = transaction . message [ : Fragment . LEN ] , timestamp = transaction . timestamp , ) ) \n    fragment = transaction . message [ Fragment . LEN : ] \n    while fragment : \n        self . _transactions . append ( ProposedTransaction ( address = transaction . address , value = 0 , tag = transaction . tag , message = fragment [ : Fragment . LEN ] , timestamp = transaction . timestamp , ) ) \n        fragment = fragment [ Fragment . LEN : ] "}
{"5170": "\ndef finalize ( self ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not self : \n        raise ValueError ( 'Bundle has no transactions.' ) \n    balance = self . balance \n    if 0 > balance : \n        if self . change_address : \n            self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) \n        else : \n            raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) \n    elif 0 < balance : \n        raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) \n    while True : \n        sponge = Kerl ( ) \n        last_index = len ( self ) - 1 \n        for i , txn in enumerate ( self ) : \n            txn . current_index = i \n            txn . last_index = last_index \n            sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) \n        bundle_hash_trits = [ 0 ] * HASH_LENGTH \n        sponge . squeeze ( bundle_hash_trits ) \n        bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) \n        if any ( 13 in part for part in normalize ( bundle_hash ) ) : \n            tail_transaction = ( self . tail_transaction ) \n            tail_transaction . increment_legacy_tag ( ) \n        else : \n            break \n    for txn in self : \n        txn . bundle_hash = bundle_hash \n        txn . signature_message_fragment = Fragment ( txn . message or b'' ) "}
{"5171": "\ndef sign_inputs ( self , key_generator ) : \n    if not self . hash : \n        raise RuntimeError ( 'Cannot sign inputs until bundle is finalized.' ) \n    i = 0 \n    while len ( self ) > i : \n        txn = self [ i ] \n        if 0 > txn . value : \n            if txn . address . key_index is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``key_index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            if txn . address . security_level is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``security_level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            self . sign_input_at ( i , key_generator . get_key_for ( txn . address ) ) \n            i += txn . address . security_level \n        else : \n            i += 1 "}
{"5176": "\ndef prime_field_inv ( a : int , n : int ) -> int : \n    if a == 0 : \n        return 0 \n    lm , hm = 1 , 0 \n    low , high = a % n , n \n    while 1 < low : \n        r = high // low \n        nm , new = hm - lm * r , high - low * r \n        lm , low , hm , high = nm , new , lm , low \n    return lm % n "}
{"5178": "\ndef find_word_groups ( self , text , category , proximity = 2 ) : \n    f = re . IGNORECASE \n    words = getattr ( self , category ) \n    regex = re . compile ( r'(\\b' + r'\\b|\\b' . join ( words ) + r'\\b)' , flags = f ) \n    candidates = regex . finditer ( text ) \n    starts , ends = [ ] , [ ] \n    groups = [ ] \n    for item in candidates : \n        starts . append ( item . span ( ) [ 0 ] ) \n        ends . append ( item . span ( ) [ 1 ] ) \n        groups . append ( item . group ( ) . lower ( ) ) \n    new_starts = [ ] \n    new_groups = [ ] \n    skip = False \n    for i , g in enumerate ( groups ) : \n        if skip : \n            skip = False \n            continue \n        if ( len ( groups ) - 1 > i ) and ( proximity >= starts [ i + 1 ] - ends [ i ] ) : \n            if g [ - 1 ] == '-' : \n                sep = '' \n            else : \n                sep = ' ' \n            new_groups . append ( g + sep + groups [ i + 1 ] ) \n            new_starts . append ( starts [ i ] ) \n            skip = True \n        else : \n            if g not in new_groups : \n                new_groups . append ( g ) \n                new_starts . append ( starts [ i ] ) \n            skip = False \n    return new_groups "}
{"5194": "\ndef get_component ( self , colour , tolerance = 0 , default = None ) : \n    if not ( 0 <= tolerance <= np . sqrt ( 195075 ) ) : \n        raise LegendError ( 'Tolerance must be between 0 and 441.67' ) \n    for decor in self . __list : \n        if colour . lower ( ) == decor . colour : \n            return decor . component \n    r1 , g1 , b1 = utils . hex_to_rgb ( colour ) \n    best_match = '#000000' \n    best_match_dist = np . sqrt ( r1 ** 2. + g1 ** 2. + b1 ** 2. ) \n    for decor in self . __list : \n        r2 , g2 , b2 = decor . rgb \n        distance = np . sqrt ( ( r2 - r1 ) ** 2. + ( g2 - g1 ) ** 2. + ( b2 - b1 ) ** 2. ) \n        if best_match_dist > distance : \n            best_match = decor . component \n            best_match_dist = distance \n            best_match_colour = decor . colour \n    if tolerance >= best_match_dist : \n        return best_match \n    else : \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( \"always\" ) \n            w = \"No match found for {0} \" . format ( colour . lower ( ) ) \n            w += \"with tolerance of {0}. Best match is \" . format ( tolerance ) \n            w += \"{0}, {1}\" . format ( best_match . summary ( ) , best_match_colour ) \n            w += \", d={0}\" . format ( best_match_dist ) \n            warnings . warn ( w ) \n        return default "}
{"5200": "\ndef parse_canstrat ( text ) : \n    result = { } \n    for row in text . split ( '\\n' ) : \n        if not row : \n            continue \n        if 8 > len ( row ) : \n            continue \n        row_header = _process_row ( row , columns_ ) or { 'card' : None } \n        card = row_header [ 'card' ] \n        if card is not None : \n            item = _process_row ( row , columns [ card ] ) \n        this_list = result . get ( card , [ ] ) \n        this_list . append ( item ) \n        result [ card ] = this_list \n    for c , d in result . items ( ) : \n        if len ( d ) == 1 : \n            result [ c ] = d [ 0 ] \n    return result "}
{"5201": "\ndef __strict ( self ) : \n    def conc ( a , b ) : \n        return a + b \n    b = np . array ( reduce ( conc , [ [ i . top . z , i . base . z ] for i in self ] ) ) \n    return all ( 0 <= np . diff ( b ) ) "}
{"5208": "\ndef from_image ( cls , filename , start , stop , legend , source = \"Image\" , col_offset = 0.1 , row_offset = 2 , tolerance = 0 ) : \n    rgb = utils . loglike_from_image ( filename , col_offset ) \n    loglike = np . array ( [ utils . rgb_to_hex ( t ) for t in rgb ] ) \n    tops , hexes = utils . tops_from_loglike ( loglike , offset = row_offset ) \n    nonconsecutive = np . append ( np . diff ( tops ) , 2 ) \n    tops = tops [ 1 < nonconsecutive ] \n    hexes = hexes [ 1 < nonconsecutive ] \n    hexes_reduced = list ( set ( hexes ) ) \n    components = [ legend . get_component ( h , tolerance = tolerance ) for h in hexes_reduced ] \n    values = [ hexes_reduced . index ( i ) for i in hexes ] \n    basis = np . linspace ( start , stop , loglike . size ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components ) \n    return cls ( list_of_Intervals , source = \"Image\" ) "}
{"5209": "\ndef from_log ( cls , log , cutoff = None , components = None , legend = None , legend_field = None , field = None , right = False , basis = None , source = 'Log' ) : \n    if ( components is None ) and ( legend is None ) and ( field is None ) : \n        m = 'You must provide a list of components, and legend, or a field.' \n        raise StriplogError ( m ) \n    if ( legend is not None ) and ( legend_field is None ) : \n        try : \n            components = [ deepcopy ( decor . component ) for decor in legend ] \n        except AttributeError : \n            pass \n    if legend_field is not None : \n        field_values = [ getattr ( d , legend_field , 0 ) for d in legend ] \n        components = [ Component ( ) for i in range ( int ( max ( field_values ) + 1 ) ) ] \n        for i , decor in enumerate ( legend ) : \n            components [ i ] = deepcopy ( decor . component ) \n    if cutoff is not None : \n        try : \n            n = len ( cutoff ) \n        except TypeError : \n            n = 1 \n        if n + 1 > len ( components ) : \n            m = 'For n cutoffs, you need to provide at least' \n            m += 'n+1 components.' \n            raise StriplogError ( m ) \n        try : \n            a = np . digitize ( log , cutoff , right ) \n        except ValueError : \n            a = np . digitize ( log , [ cutoff ] , right ) \n    else : \n        a = np . copy ( log ) \n    tops , values = utils . tops_from_loglike ( a ) \n    if basis is None : \n        m = 'You must provide a depth or elevation basis.' \n        raise StriplogError ( m ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components , field = field ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5221": "\ndef prune ( self , limit = None , n = None , percentile = None , keep_ends = False ) : \n    strip = self . copy ( ) \n    if not ( limit or n or percentile ) : \n        m = \"You must provide a limit or n or percentile for pruning.\" \n        raise StriplogError ( m ) \n    if limit : \n        prune = [ i for i , iv in enumerate ( strip ) if limit > iv . thickness ] \n    if n : \n        prune = strip . thinnest ( n = n , index = True ) \n    if percentile : \n        n = np . floor ( len ( strip ) * percentile / 100 ) \n        prune = strip . thinnest ( n = n , index = True ) \n    if keep_ends : \n        first , last = 0 , len ( strip ) - 1 \n        if first in prune : \n            prune . remove ( first ) \n        if last in prune : \n            prune . remove ( last ) \n    del strip [ prune ] \n    return strip "}
{"5231": "\ndef hex_to_name ( hexx ) : \n    for n , h in defaults . COLOURS . items ( ) : \n        if ( 1 < len ( n ) ) and ( h == hexx . upper ( ) ) : \n            return n . lower ( ) \n    return None "}
{"5232": "\ndef loglike_from_image ( filename , offset ) : \n    im = plt . imread ( filename ) \n    if 1 > offset : \n        col = int ( im . shape [ 1 ] * offset ) \n    else : \n        col = offset \n    return im [ : , col , : 3 ] "}
{"5273": "\ndef get_output ( self , stdin_content , stdin ) : \n    try : \n        if stdin : \n            if ( 3 , 0 ) <= sys . version_info : \n                self . process . stdin . write ( bytes ( stdin_content , \"utf-8\" ) ) \n            else : \n                self . process . stdin . write ( stdin_content ) \n        self . _out = self . process . communicate ( ) [ 0 ] \n    except ( error , IOError ) : \n        self . _out = self . _in \n        pass "}
{"5274": "\ndef finish_read ( self , timeout = 2 , stdin_content = \"\" , stdin = False ) : \n    process = Thread ( target = self . get_output , args = ( stdin_content , stdin ) ) \n    process . start ( ) \n    if 0 < timeout : \n        process . join ( timeout ) \n    else : \n        process . join ( ) \n    if process . is_alive ( ) : \n        self . close ( ) \n        self . return_code = - signal . SIGHUP \n    else : \n        self . return_code = self . process . returncode "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = True ) : \n            if type ( d ) == dict : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = False ) \n                    elif type ( d [ k ] ) == str : \n                        if _decode : \n                            d [ k ] = decode ( d [ k ] ) \n                        else : \n                            d [ k ] = encode ( d [ k ] ) \n            elif type ( d ) == list : \n                arr = [ ] \n                for e in d : \n                    if type ( e ) == str : \n                        if _decode : \n                            arr . append ( decode ( e ) ) \n                        else : \n                            arr . append ( encode ( e ) ) \n                    elif type ( e ) in [ dict , list ] : \n                        if _decode : \n                            arr . append ( encode_decode_all ( e ) ) \n                        else : \n                            arr . append ( encode_decode_all ( e , _decode = False ) ) \n                    else : \n                        arr . append ( e ) \n                return arr \n            else : \n                if _decode : \n                    return decode ( d ) \n                else : \n                    return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if ( 3 , 0 ) <= sys . version_info : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if ( 3 , 0 ) <= sys . version_info : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5 ) ) , _decode = False ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = False ) \n    return func_wrapper "}
{"5284": "\ndef build ( self , pre = None , shortest = False ) : \n    global REF_LEVEL \n    REF_LEVEL += 1 \n    try : \n        if pre is None : \n            pre = [ ] \n        definition = self . fuzzer . get_ref ( self . cat , self . refname ) \n        res = utils . val ( definition , pre , shortest = ( shortest or self . max_recursion <= REF_LEVEL ) ) \n        return res \n    finally : \n        REF_LEVEL -= 1 "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == False : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = 0 \n    while num > total_gend : \n        if 0 < len ( self . _last_pref_keys ) and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            print ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += 1 \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5293": "\ndef fuzz_elements ( self , element ) : \n    try : \n        if type ( element ) == dict : \n            tmp_element = { } \n            for key in element : \n                if 0 < len ( self . config . parameters ) : \n                    if self . config . exclude_parameters : \n                        fuzz = key not in self . config . parameters \n                    else : \n                        fuzz = key in self . config . parameters \n                else : \n                    fuzz = True \n                if fuzz : \n                    if type ( element [ key ] ) == dict : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    elif type ( element [ key ] ) == list : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    else : \n                        tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) \n                else : \n                    tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n            element = tmp_element \n            del tmp_element \n        elif type ( element ) == list : \n            arr = [ ] \n            for key in element : \n                if type ( key ) == dict : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                elif type ( key ) == list : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                else : \n                    if 0 >= len ( self . config . parameters ) : \n                        arr . append ( self . mutator . fuzz ( key ) ) \n                    else : \n                        arr . append ( key ) \n            element = arr \n            del arr \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) \n    return element "}
{"5294": "\ndef fuzzed ( self ) : \n    try : \n        if self . config . strong_fuzz : \n            fuzzer = PJFMutators ( self . config ) \n            if self . config . url_encode : \n                if ( 3 , 0 ) <= sys . version_info : \n                    return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n                else : \n                    return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n            else : \n                if type ( self . config . json ) in [ list , dict ] : \n                    return fuzzer . fuzz ( json . dumps ( self . config . json ) ) \n                else : \n                    return fuzzer . fuzz ( self . config . json ) \n        else : \n            if self . config . url_encode : \n                if ( 3 , 0 ) <= sys . version_info : \n                    return urllib . parse . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n                else : \n                    return urllib . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n            else : \n                return self . get_fuzzed ( self . config . indent , self . config . utf8 ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5307": "\ndef contact ( self , id ) : \n    try : \n        json = self . skype . conn ( \"POST\" , \"{0}/users/batch/profiles\" . format ( SkypeConnection . API_USER ) , json = { \"usernames\" : [ id ] } , auth = SkypeConnection . Auth . SkypeToken ) . json ( ) \n        contact = SkypeContact . fromRaw ( self . skype , json [ 0 ] ) \n        if contact . id not in self . contactIds : \n            self . contactIds . append ( contact . id ) \n        return self . merge ( contact ) \n    except SkypeApiException as e : \n        if 2 <= len ( e . args ) and getattr ( e . args [ 1 ] , \"status_code\" , None ) == 403 : \n            return None \n        raise "}
{"5317": "\ndef readToken ( self ) : \n    if not self . tokenFile : \n        raise SkypeAuthException ( \"No token file specified\" ) \n    try : \n        with open ( self . tokenFile , \"r\" ) as f : \n            lines = f . read ( ) . splitlines ( ) \n    except OSError : \n        raise SkypeAuthException ( \"Token file doesn't exist or not readable\" ) \n    try : \n        user , skypeToken , skypeExpiry , regToken , regExpiry , msgsHost = lines \n        skypeExpiry = datetime . fromtimestamp ( int ( skypeExpiry ) ) \n        regExpiry = datetime . fromtimestamp ( int ( regExpiry ) ) \n    except ValueError : \n        raise SkypeAuthException ( \"Token file is malformed\" ) \n    if skypeExpiry <= datetime . now ( ) : \n        raise SkypeAuthException ( \"Token file has expired\" ) \n    self . userId = user \n    self . tokens [ \"skype\" ] = skypeToken \n    self . tokenExpiry [ \"skype\" ] = skypeExpiry \n    if regExpiry > datetime . now ( ) : \n        self . tokens [ \"reg\" ] = regToken \n        self . tokenExpiry [ \"reg\" ] = regExpiry \n        self . msgsHost = msgsHost \n    else : \n        self . getRegToken ( ) "}
{"5319": "\ndef verifyToken ( self , auth ) : \n    if auth in ( self . Auth . SkypeToken , self . Auth . Authorize ) : \n        if \"skype\" not in self . tokenExpiry or self . tokenExpiry [ \"skype\" ] <= datetime . now ( ) : \n            if not hasattr ( self , \"getSkypeToken\" ) : \n                raise SkypeAuthException ( \"Skype token expired, and no password specified\" ) \n            self . getSkypeToken ( ) \n    elif auth == self . Auth . RegToken : \n        if \"reg\" not in self . tokenExpiry or self . tokenExpiry [ \"reg\" ] <= datetime . now ( ) : \n            self . getRegToken ( ) "}
{"5344": "\ndef sublists ( self , i : int = None , pattern : str = None ) -> List [ 'WikiList' ] : \n    patterns = ( r'\\#' , r'\\*' , '[:;]' ) if pattern is None else ( pattern , ) \n    self_pattern = self . pattern \n    lists = self . lists \n    sublists = [ ] \n    sublists_append = sublists . append \n    if i is None : \n        for pattern in patterns : \n            for lst in lists ( self_pattern + pattern ) : \n                sublists_append ( lst ) \n        return sublists \n    match = self . _match \n    fullitem_spans = match . spans ( 'fullitem' ) \n    ss = self . _span [ 0 ] \n    ms = match . start ( ) \n    s , e = fullitem_spans [ i ] \n    e -= ms - ss \n    s -= ms - ss \n    for pattern in patterns : \n        for lst in lists ( self_pattern + pattern ) : \n            ls , le = lst . _span \n            if ls > s and e >= le : \n                sublists_append ( lst ) \n    return sublists "}
{"5350": "\ndef _check_index ( self , key : Union [ slice , int ] ) -> ( int , int ) : \n    ss , se = self . _span \n    if isinstance ( key , int ) : \n        if 0 > key : \n            key += se - ss \n            if 0 > key : \n                raise IndexError ( 'index out of range' ) \n        elif se - ss <= key : \n            raise IndexError ( 'index out of range' ) \n        start = ss + key \n        return start , start + 1 \n    if key . step is not None : \n        raise NotImplementedError ( 'step is not implemented for string setter.' ) \n    start , stop = key . start or 0 , key . stop \n    if 0 > start : \n        start += se - ss \n        if 0 > start : \n            raise IndexError ( 'start index out of range' ) \n    if stop is None : \n        stop = se - ss \n    elif 0 > stop : \n        stop += se - ss \n    if stop < start : \n        raise IndexError ( 'stop index out of range or start is after the stop' ) \n    return start + ss , stop + ss "}
{"5351": "\ndef insert ( self , index : int , string : str ) -> None : \n    ss , se = self . _span \n    lststr = self . _lststr \n    lststr0 = lststr [ 0 ] \n    if 0 > index : \n        index += se - ss \n        if 0 > index : \n            index = 0 \n    elif se - ss < index : \n        index = se - ss \n    index += ss \n    lststr [ 0 ] = lststr0 [ : index ] + string + lststr0 [ index : ] \n    string_len = len ( string ) \n    self . _insert_update ( index = index , length = string_len ) \n    type_to_spans = self . _type_to_spans \n    for type_ , spans in parse_to_spans ( bytearray ( string , 'ascii' , 'replace' ) ) . items ( ) : \n        for s , e in spans : \n            insort ( type_to_spans [ type_ ] , [ index + s , index + e ] ) "}
{"5354": "\ndef _shrink_update ( self , rmstart : int , rmstop : int ) -> None : \n    for spans in self . _type_to_spans . values ( ) : \n        i = len ( spans ) - 1 \n        while 0 <= i : \n            s , e = span = spans [ i ] \n            if s >= rmstop : \n                rmlength = rmstop - rmstart \n                span [ : ] = s - rmlength , e - rmlength \n                i -= 1 \n                continue \n            break \n        else : \n            continue \n        while True : \n            if s >= rmstart : \n                if e > rmstop : \n                    span [ : ] = rmstart , e + rmstart - rmstop \n                    i -= 1 \n                    if 0 > i : \n                        break \n                    s , e = span = spans [ i ] \n                    continue \n                spans . pop ( i ) [ : ] = - 1 , - 1 \n                i -= 1 \n                if 0 > i : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            break \n        while 0 <= i : \n            if rmstart >= e : \n                i -= 1 \n                if 0 > i : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            span [ 1 ] -= rmstop - rmstart \n            i -= 1 \n            if 0 > i : \n                break \n            s , e = span = spans [ i ] \n            continue "}
{"5355": "\ndef _insert_update ( self , index : int , length : int ) -> None : \n    ss , se = self . _span \n    for spans in self . _type_to_spans . values ( ) : \n        for span in spans : \n            if span [ 1 ] > index or span [ 1 ] == index == se : \n                span [ 1 ] += length \n                if span [ 0 ] > index or span [ 0 ] == index != ss : \n                    span [ 0 ] += length "}
{"5356": "\ndef nesting_level ( self ) -> int : \n    ss , se = self . _span \n    level = 0 \n    type_to_spans = self . _type_to_spans \n    for type_ in ( 'Template' , 'ParserFunction' ) : \n        spans = type_to_spans [ type_ ] \n        for s , e in spans [ : bisect ( spans , [ ss + 1 ] ) ] : \n            if e >= se : \n                level += 1 \n    return level "}
{"5359": "\ndef _pp_type_to_spans ( self ) -> Dict [ str , List [ List [ int ] ] ] : \n    ss , se = self . _span \n    if ss == 0 and se == len ( self . _lststr [ 0 ] ) : \n        return deepcopy ( self . _type_to_spans ) \n    return { type_ : [ [ s - ss , e - ss ] for s , e in spans [ bisect ( spans , [ ss ] ) : ] if se >= e ] for type_ , spans in self . _type_to_spans . items ( ) } "}
{"5367": "\ndef sections ( self ) -> List [ 'Section' ] : \n    sections = [ ] \n    sections_append = sections . append \n    type_to_spans = self . _type_to_spans \n    lststr = self . _lststr \n    ss , se = _span = self . _span \n    type_spans = type_to_spans . setdefault ( 'Section' , [ ] ) \n    full_match = SECTIONS_FULLMATCH ( self . _shadow ) \n    section_spans = full_match . spans ( 'section' ) \n    levels = [ len ( eq ) for eq in full_match . captures ( 'equals' ) ] \n    if not type_spans : \n        spans_append = type_spans . append \n        for current_index , ( current_level , ( s , e ) ) in enumerate ( zip ( levels , section_spans ) , 1 ) : \n            for section_index , section_level in enumerate ( levels [ current_index : ] , current_index ) : \n                if current_level and current_level < section_level : \n                    e = section_spans [ section_index ] [ 1 ] \n                else : \n                    break \n            span = [ ss + s , ss + e ] \n            spans_append ( span ) \n            sections_append ( Section ( lststr , type_to_spans , span , 'Section' ) ) \n        return sections \n    span_tuple_to_span = { ( s [ 0 ] , s [ 1 ] ) : s for s in type_spans } . get \n    for current_index , ( current_level , ( s , e ) ) in enumerate ( zip ( levels , section_spans ) , 1 ) : \n        for section_index , section_level in enumerate ( levels [ current_index : ] , current_index ) : \n            if current_level and current_level < section_level : \n                e = section_spans [ section_index ] [ 1 ] \n            else : \n                break \n        s , e = ss + s , ss + e \n        old_span = span_tuple_to_span ( ( s , e ) ) \n        if old_span is None : \n            span = [ s , e ] \n            insort ( type_spans , span ) \n        else : \n            span = old_span \n        sections_append ( Section ( lststr , type_to_spans , span , 'Section' ) ) \n    return sections "}
{"5371": "\ndef _subspans ( self , _type : str ) -> Generator [ int , None , None ] : \n    ss , se = self . _span \n    spans = self . _type_to_spans [ _type ] \n    b = bisect ( spans , [ ss ] ) \n    for span in spans [ b : bisect ( spans , [ se ] , b ) ] : \n        if se >= span [ 1 ] : \n            yield span "}
{"5372": "\ndef ancestors ( self , type_ : Optional [ str ] = None ) -> List [ 'WikiText' ] : \n    if type_ is None : \n        types = SPAN_PARSER_TYPES \n    else : \n        types = type_ , lststr = self . _lststr \n    type_to_spans = self . _type_to_spans \n    ss , se = self . _span \n    ancestors = [ ] \n    ancestors_append = ancestors . append \n    for type_ in types : \n        cls = globals ( ) [ type_ ] \n        spans = type_to_spans [ type_ ] \n        for span in spans [ : bisect ( spans , [ ss ] ) ] : \n            if span [ 1 ] > se : \n                ancestors_append ( cls ( lststr , type_to_spans , span , type_ ) ) \n    return sorted ( ancestors , key = lambda i : ss - i . _span [ 0 ] ) "}
{"5425": "\ndef _read_compressed_points_data ( self , laszip_vlr , point_format ) : \n    offset_to_chunk_table = struct . unpack ( \"<q\" , self . stream . read ( 8 ) ) [ 0 ] \n    size_of_point_data = offset_to_chunk_table - self . stream . tell ( ) \n    if 0 >= offset_to_chunk_table : \n        logger . warning ( \"Strange offset to chunk table: {}, ignoring it..\" . format ( offset_to_chunk_table ) ) \n        size_of_point_data = - 1 \n    points = record . PackedPointRecord . from_compressed_buffer ( self . stream . read ( size_of_point_data ) , point_format , self . header . point_count , laszip_vlr , ) \n    return points "}
{"5431": "\ndef create_from_header ( header ) : \n    header = copy . copy ( header ) \n    header . point_count = 0 \n    points = record . PackedPointRecord . empty ( PointFormat ( header . point_format_id ) ) \n    if \"1.4\" <= header . version : \n        return las14 . LasData ( header = header , points = points ) \n    return las12 . LasData ( header = header , points = points ) "}
{"5432": "\ndef create_las ( * , point_format_id = 0 , file_version = None ) : \n    if file_version is not None : \n        dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) \n    else : \n        file_version = dims . min_file_version_for_point_format ( point_format_id ) \n    header = headers . HeaderFactory . new ( file_version ) \n    header . point_format_id = point_format_id \n    if \"1.4\" <= file_version : \n        return las14 . LasData ( header = header ) \n    return las12 . LasData ( header = header ) "}
{"5433": "\ndef convert ( source_las , * , point_format_id = None , file_version = None ) : \n    if point_format_id is None : \n        point_format_id = source_las . points_data . point_format . id \n    if file_version is None : \n        file_version = max ( source_las . header . version , dims . min_file_version_for_point_format ( point_format_id ) , ) \n    else : \n        file_version = str ( file_version ) \n        dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) \n    header = headers . HeaderFactory . convert_header ( source_las . header , file_version ) \n    header . point_format_id = point_format_id \n    point_format = PointFormat ( point_format_id , source_las . points_data . point_format . extra_dims ) \n    points = record . PackedPointRecord . from_point_record ( source_las . points_data , point_format ) \n    try : \n        evlrs = source_las . evlrs \n    except ValueError : \n        evlrs = [ ] \n    if \"1.4\" <= file_version : \n        las = las14 . LasData ( header = header , vlrs = source_las . vlrs , points = points , evlrs = evlrs ) \n    else : \n        if evlrs : \n            logger . warning ( \"The source contained {} EVLRs,\" \" they will be lost as version {} doest not support them\" . format ( len ( evlrs ) , file_version ) ) \n        las = las12 . LasData ( header = header , vlrs = source_las . vlrs , points = points ) \n    return las "}
{"5447": "\ndef pack ( array , sub_field_array , mask , inplace = False ) : \n    lsb = least_significant_bit ( mask ) \n    max_value = int ( mask >> lsb ) \n    if max_value < sub_field_array . max ( ) : \n        raise OverflowError ( \"value ({}) is greater than allowed (max: {})\" . format ( sub_field_array . max ( ) , max_value ) ) \n    if inplace : \n        array [ : ] = array & ~ mask \n        array [ : ] = array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) \n    else : \n        array = array & ~ mask \n        return array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) "}
{"5461": "\nasync def disarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending disarm command.\" ) \n    while 16 > len ( code ) : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x84' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5462": "\nasync def clear_alarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending clear the alarm command.\" ) \n    while 16 > len ( code ) : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x85' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5463": "\nasync def set_output ( self , code , output_id , state ) : \n    _LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output_id , code ) \n    while 16 > len ( code ) : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    mode_command = 0x88 if state else 0x89 \n    data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) + code_bytes + output_bytes ( output_id ) ) \n    await self . _send_data ( data ) "}
{"5500": "\ndef _generate_notebooks ( table , timestamp_column , engine , crypto_factory , min_dt , max_dt , logger ) : \n    where_conds = [ ] \n    if min_dt is not None : \n        where_conds . append ( min_dt <= timestamp_column ) \n    if max_dt is not None : \n        where_conds . append ( max_dt > timestamp_column ) \n    if table is files : \n        where_conds . append ( files . c . name . like ( u'%.ipynb' ) ) \n    query = select ( [ table ] ) . order_by ( timestamp_column ) \n    for cond in where_conds : \n        query = query . where ( cond ) \n    result = engine . execute ( query ) \n    for nb_row in result : \n        try : \n            user_id = nb_row [ 'user_id' ] \n            decrypt_func = crypto_factory ( user_id ) . decrypt \n            nb_dict = to_dict_with_content ( table . c , nb_row , decrypt_func ) \n            if table is files : \n                nb_dict [ 'path' ] = nb_dict [ 'parent_name' ] + nb_dict [ 'name' ] \n                nb_dict [ 'last_modified' ] = nb_dict [ 'created_at' ] \n            yield { 'id' : nb_dict [ 'id' ] , 'user_id' : user_id , 'path' : to_api_path ( nb_dict [ 'path' ] ) , 'last_modified' : nb_dict [ 'last_modified' ] , 'content' : reads_base64 ( nb_dict [ 'content' ] ) , } \n        except CorruptedFile : \n            if logger is not None : \n                logger . warning ( 'Corrupted file with id %d in table %s.' % ( nb_row [ 'id' ] , table . name ) ) "}
{"5572": "\ndef signup ( request , uuid = None ) : \n    invite = get_object_or_404 ( Invite . objects . all ( ) , id = uuid ) \n    if timezone . now ( ) > invite . expiration_date : \n        invite . delete ( ) \n        raise Http404 ( 'This page does not exist.' ) \n    if request . method == 'POST' : \n        form = SignUpForm ( request . POST ) \n        if form . is_valid ( ) : \n            user = form . save ( commit = False ) \n            user . email = invite . email \n            user . person = invite . person \n            user . save ( ) \n            if invite . permissions == 'admin' : \n                group = Group . objects . get ( name = 'Admin' ) \n                user . groups . add ( group ) \n            invite . delete ( ) \n            return redirect ( 'dispatch-admin' ) \n        else : \n            return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) \n    else : \n        form = SignUpForm ( ) \n    return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) "}
{"5580": "\ndef save_thumbnail ( self , image , size , name , label , file_type ) : \n    width , height = size \n    ( imw , imh ) = image . size \n    if ( width < imw ) or ( height < imh ) : \n        image . thumbnail ( size , Img . ANTIALIAS ) \n    name = \"%s-%s.jpg\" % ( name , label ) \n    if file_type in self . JPG_FORMATS : \n        file_type = 'JPEG' \n    image_io = StringIO . StringIO ( ) \n    image . save ( image_io , format = file_type , quality = 75 ) \n    thumb_file = InMemoryUploadedFile ( image_io , None , name , 'image/jpeg' , image_io . len , None ) \n    default_storage . save ( name , thumb_file ) "}
{"5583": "\ndef read ( self , amount ) : \n    if not self . _bandwidth_limiting_enabled : \n        return self . _fileobj . read ( amount ) \n    self . _bytes_seen += amount \n    if self . _bytes_threshold > self . _bytes_seen : \n        return self . _fileobj . read ( amount ) \n    self . _consume_through_leaky_bucket ( ) \n    return self . _fileobj . read ( amount ) "}
{"5604": "\ndef request_writes ( self , offset , data ) : \n    if self . _next_offset > offset : \n        return [ ] \n    writes = [ ] \n    if offset in self . _pending_offsets : \n        return [ ] \n    heapq . heappush ( self . _writes , ( offset , data ) ) \n    self . _pending_offsets . add ( offset ) \n    while self . _writes and self . _writes [ 0 ] [ 0 ] == self . _next_offset : \n        next_write = heapq . heappop ( self . _writes ) \n        writes . append ( { 'offset' : next_write [ 0 ] , 'data' : next_write [ 1 ] } ) \n        self . _pending_offsets . remove ( next_write [ 0 ] ) \n        self . _next_offset += len ( next_write [ 1 ] ) \n    return writes "}
{"5613": "\ndef _read ( self , fileobj , amount , truncate = True ) : \n    if len ( self . _initial_data ) == 0 : \n        return fileobj . read ( amount ) \n    if len ( self . _initial_data ) >= amount : \n        data = self . _initial_data [ : amount ] \n        if truncate : \n            self . _initial_data = self . _initial_data [ amount : ] \n        return data \n    amount_to_read = amount - len ( self . _initial_data ) \n    data = self . _initial_data + fileobj . read ( amount_to_read ) \n    if truncate : \n        self . _initial_data = b'' \n    return data "}
{"5627": "\ndef upload_file ( self , filename , bucket , key , callback = None , extra_args = None ) : \n    if extra_args is None : \n        extra_args = { } \n    self . _validate_all_known_args ( extra_args , self . ALLOWED_UPLOAD_ARGS ) \n    events = self . _client . meta . events \n    events . register_first ( 'request-created.s3' , disable_upload_callbacks , unique_id = 's3upload-callback-disable' ) \n    events . register_last ( 'request-created.s3' , enable_upload_callbacks , unique_id = 's3upload-callback-enable' ) \n    if self . _config . multipart_threshold <= self . _osutil . get_file_size ( filename ) : \n        self . _multipart_upload ( filename , bucket , key , callback , extra_args ) \n    else : \n        self . _put_object ( filename , bucket , key , callback , extra_args ) "}
{"5720": "\ndef _translate_glob_part ( pat ) : \n    if pat == '**' : \n        return '.*' \n    i , n = 0 , len ( pat ) \n    res = [ ] \n    while n > i : \n        c = pat [ i ] \n        i = i + 1 \n        if c == '*' : \n            res . append ( '[^%s]*' % SEPARATORS ) \n        elif c == '?' : \n            res . append ( '[^%s]?' % SEPARATORS ) \n        elif c == '[' : \n            j = i \n            if n > j and pat [ j ] == '!' : \n                j = j + 1 \n            if n > j and pat [ j ] == ']' : \n                j = j + 1 \n            while n > j and pat [ j ] != ']' : \n                j = j + 1 \n            if n <= j : \n                res . append ( '\\\\[' ) \n            else : \n                stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) \n                i = j + 1 \n                if stuff [ 0 ] == '!' : \n                    stuff = '^' + stuff [ 1 : ] \n                elif stuff [ 0 ] == '^' : \n                    stuff = '\\\\' + stuff \n                res . append ( '[%s]' % stuff ) \n        else : \n            res . append ( re . escape ( c ) ) \n    return '' . join ( res ) "}
{"5733": "\ndef start ( self , block = False , timeout = None , retry_interval = 0.5 , extra_predicate = None ) : \n    start = time . time ( ) \n    while 1 : \n        task_handler = self . _dequeue_task ( extra_predicate ) \n        if task_handler is None and block : \n            if timeout is not None and timeout < ( time . time ( ) - start ) : \n                break \n            time . sleep ( retry_interval * ( random . random ( ) + 0.1 ) ) \n        else : \n            break \n    return task_handler "}
{"5737": "\ndef get ( self , query , * parameters , ** kwparameters ) : \n    rows = self . _query ( query , parameters , kwparameters ) \n    if not rows : \n        return None \n    elif not isinstance ( rows , list ) : \n        raise MySQLError ( \"Query is not a select query\" ) \n    elif 1 < len ( rows ) : \n        raise MySQLError ( \"Multiple rows returned for Database.get() query\" ) \n    else : \n        return rows [ 0 ] "}
{"5795": "\ndef _onConnect ( self , mqttc , userdata , flags , rc ) : \n    if rc == 0 : \n        self . connectEvent . set ( ) \n        self . logger . info ( \"Connected successfully: %s\" % ( self . clientId ) ) \n        with self . _subLock : \n            if 0 < len ( self . _subscriptions ) : \n                for subscription in self . _subscriptions : \n                    ( result , mid ) = self . client . subscribe ( subscription , qos = self . _subscriptions [ subscription ] ) \n                    if result != paho . MQTT_ERR_SUCCESS : \n                        self . _logAndRaiseException ( ConnectionException ( \"Unable to subscribe to %s\" % subscription ) ) \n                self . logger . debug ( \"Restored %s previous subscriptions\" % len ( self . _subscriptions ) ) \n    elif rc == 1 : \n        self . _logAndRaiseException ( ConnectionException ( \"Incorrect protocol version\" ) ) \n    elif rc == 2 : \n        self . _logAndRaiseException ( ConnectionException ( \"Invalid client identifier\" ) ) \n    elif rc == 3 : \n        self . _logAndRaiseException ( ConnectionException ( \"Server unavailable\" ) ) \n    elif rc == 4 : \n        self . _logAndRaiseException ( ConnectionException ( \"Bad username or password: (%s, %s)\" % ( self . username , self . password ) ) ) \n    elif rc == 5 : \n        self . _logAndRaiseException ( ConnectionException ( \"Not authorized: s (%s, %s, %s)\" % ( self . clientId , self . username , self . password ) ) ) \n    else : \n        self . _logAndRaiseException ( ConnectionException ( \"Unexpected connection failure: %s\" % ( rc ) ) ) "}
{"5822": "\ndef crop_on_centerpoint ( self , image , width , height , ppoi = ( 0.5 , 0.5 ) ) : \n    ppoi_x_axis = int ( image . size [ 0 ] * ppoi [ 0 ] ) \n    ppoi_y_axis = int ( image . size [ 1 ] * ppoi [ 1 ] ) \n    center_pixel_coord = ( ppoi_x_axis , ppoi_y_axis ) \n    orig_aspect_ratio = float ( image . size [ 0 ] ) / float ( image . size [ 1 ] ) \n    crop_aspect_ratio = float ( width ) / float ( height ) \n    if crop_aspect_ratio <= orig_aspect_ratio : \n        orig_crop_width = int ( ( crop_aspect_ratio * float ( image . size [ 1 ] ) ) + 0.5 ) \n        orig_crop_height = image . size [ 1 ] \n        crop_boundary_top = 0 \n        crop_boundary_bottom = orig_crop_height \n        crop_boundary_left = center_pixel_coord [ 0 ] - ( orig_crop_width // 2 ) \n        crop_boundary_right = crop_boundary_left + orig_crop_width \n        if 0 > crop_boundary_left : \n            crop_boundary_left = 0 \n            crop_boundary_right = crop_boundary_left + orig_crop_width \n        elif image . size [ 0 ] < crop_boundary_right : \n            crop_boundary_right = image . size [ 0 ] \n            crop_boundary_left = image . size [ 0 ] - orig_crop_width \n    else : \n        orig_crop_width = image . size [ 0 ] \n        orig_crop_height = int ( ( float ( image . size [ 0 ] ) / crop_aspect_ratio ) + 0.5 ) \n        crop_boundary_left = 0 \n        crop_boundary_right = orig_crop_width \n        crop_boundary_top = center_pixel_coord [ 1 ] - ( orig_crop_height // 2 ) \n        crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        if 0 > crop_boundary_top : \n            crop_boundary_top = 0 \n            crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        elif image . size [ 1 ] < crop_boundary_bottom : \n            crop_boundary_bottom = image . size [ 1 ] \n            crop_boundary_top = image . size [ 1 ] - orig_crop_height \n    cropped_image = image . crop ( ( crop_boundary_left , crop_boundary_top , crop_boundary_right , crop_boundary_bottom ) ) \n    return cropped_image . resize ( ( width , height ) , Image . ANTIALIAS ) "}
{"5832": "\ndef value_to_string ( self , obj ) : \n    if ( 1 , 9 ) < DJANGO_VERSION : \n        value = self . value_from_object ( obj ) \n    else : \n        value = self . _get_val_from_obj ( obj ) \n    return self . get_prep_value ( value ) "}
{"5909": "\ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ 0 ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                elif 3 <= len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) : \n                    if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                        matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            print ( '\\n' . join ( matching_phone_number_list ) ) \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    elif all_phone_numbers_list : \n        if parsable : \n            print ( '\\n' . join ( all_phone_numbers_list ) ) \n        else : \n            list_phone_numbers ( all_phone_numbers_list ) \n    else : \n        if not parsable : \n            print ( \"Found no phone numbers\" ) \n        sys . exit ( 1 ) "}
{"5928": "\ndef _search_all ( self , query ) : \n    regexp = re . compile ( query , re . IGNORECASE | re . DOTALL ) \n    for contact in self . contacts . values ( ) : \n        contact_details = contact . print_vcard ( ) \n        if regexp . search ( contact_details ) is not None : \n            yield contact \n        else : \n            clean_contact_details = re . sub ( \"[^a-zA-Z0-9\\n]\" , \"\" , contact_details ) \n            if regexp . search ( clean_contact_details ) is not None and 3 <= len ( re . sub ( \"\\D\" , \"\" , query ) ) : \n                yield contact "}
{"5940": "\ndef dispatch ( parser , argv = None , add_help_command = True , completion = True , pre_call = None , output_file = sys . stdout , errors_file = sys . stderr , raw_output = False , namespace = None , skip_unknown_args = False ) : \n    if completion : \n        autocomplete ( parser ) \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    if add_help_command : \n        if argv and argv [ 0 ] == 'help' : \n            argv . pop ( 0 ) \n            argv . append ( '--help' ) \n    if skip_unknown_args : \n        parse_args = parser . parse_known_args \n    else : \n        parse_args = parser . parse_args \n    if not namespace : \n        namespace = ArghNamespace ( ) \n    namespace_obj = parse_args ( argv , namespace = namespace ) \n    function = _get_function_from_namespace_obj ( namespace_obj ) \n    if function : \n        lines = _execute_command ( function , namespace_obj , errors_file , pre_call = pre_call ) \n    else : \n        lines = [ parser . format_usage ( ) ] \n    if output_file is None : \n        if ( 3 , 0 ) > sys . version_info : \n            f = compat . BytesIO ( ) \n        else : \n            f = compat . StringIO ( ) \n    else : \n        f = output_file \n    for line in lines : \n        io . dump ( line , f ) \n        if not raw_output : \n            io . dump ( '\\n' , f ) \n    if output_file is None : \n        f . seek ( 0 ) \n        return f . read ( ) "}
{"5941": "\ndef safe_input ( prompt ) : \n    if ( 3 , 0 ) > sys . version_info : \n        if isinstance ( prompt , compat . text_type ) : \n            encoding = locale . getpreferredencoding ( ) or 'utf-8' \n            prompt = prompt . encode ( encoding ) \n    else : \n        if not isinstance ( prompt , compat . text_type ) : \n            prompt = prompt . decode ( ) \n    return _input ( prompt ) "}
{"5942": "\ndef encode_output ( value , output_file ) : \n    if ( 3 , 0 ) < sys . version_info : \n        return compat . text_type ( value ) \n    else : \n        stream_encoding = getattr ( output_file , 'encoding' , None ) \n        if stream_encoding : \n            if stream_encoding . upper ( ) == 'UTF-8' : \n                return compat . text_type ( value ) \n            else : \n                return value . encode ( stream_encoding , 'ignore' ) \n        else : \n            if isinstance ( value , compat . text_type ) : \n                return value . encode ( 'utf-8' ) \n            else : \n                return str ( value ) "}
{"5947": "\ndef confirm ( action , default = None , skip = False ) : \n    MAX_ITERATIONS = 3 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , True : ( 'Y' , 'n' ) , False : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = 1 \n                while not choice and MAX_ITERATIONS > cnt : \n                    choice = safe_input ( prompt ) \n                    cnt += 1 \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return True \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return False \n    if default is not None : \n        return default \n    return None "}
{"5950": "\ndef cached_result ( self , timeout ) : \n    if not ( self . _filters or self . _order_by ) : \n        raise QueryError ( \"You are missing filter or order criteria\" ) \n    timeout = int ( timeout ) \n    if 1 > timeout : \n        raise QueryError ( \"You must specify a timeout >= 1, you gave %r\" % timeout ) \n    return self . _model . _gindex . search ( _connect ( self . _model ) , self . _filters , self . _order_by , timeout = timeout ) "}
{"5952": "\ndef delete ( self , blocksize = 100 ) : \n    from . columns import MODELS_REFERENCED \n    if not self . _model . _no_fk or self . _model . _namespace in MODELS_REFERENCED : \n        raise QueryError ( \"Can't delete entities of models with foreign key relationships\" ) \n    de = [ ] \n    i = 0 \n    for result in self . iter_result ( pagesize = blocksize ) : \n        de . append ( result ) \n        i += 1 \n        if blocksize <= i : \n            session . delete ( de ) \n            del de [ : ] \n            i = 0 \n    if de : \n        session . delete ( de ) "}
{"5956": "\ndef search ( self , conn , filters , order_by , offset = None , count = None , timeout = None ) : \n    pipe , intersect , temp_id = self . _prepare ( conn , filters ) \n    if order_by : \n        reverse = order_by and order_by . startswith ( '-' ) \n        order_clause = '%s:%s:idx' % ( self . namespace , order_by . lstrip ( '-' ) ) \n        intersect ( temp_id , { temp_id : 0 , order_clause : - 1 if reverse else 1 } ) \n    if timeout is not None : \n        pipe . expire ( temp_id , timeout ) \n        pipe . execute ( ) \n        return temp_id \n    offset = offset if offset is not None else 0 \n    end = ( offset + count - 1 ) if count and 0 < count else - 1 \n    pipe . zrange ( temp_id , offset , end ) \n    pipe . delete ( temp_id ) \n    return pipe . execute ( ) [ - 2 ] "}
{"5961": "\ndef clean_old_index ( model , block_size = 100 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2 ] ) ) \n    has_hscan = [ 2 , 8 ] <= version \n    pipe = conn . pipeline ( True ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10 ) \n    force_hscan = kwargs . get ( 'force_hscan' , False ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = 0 \n        while cursor != b'0' : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , 0 , 0 ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned += block_size \n            if max_id < scanned : \n                max_id = scanned + 1 \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while cursor != b'0' : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , 1 , 0 ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned += block_size \n                if max_id < scanned : \n                    max_id = scanned + 1 \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        for i in range ( 1 , max_id + 1 , block_size ) : \n            ids = list ( range ( i , min ( i + block_size , max_id + 1 ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - 1 ) , max_id \n    yield max_id , max_id "}
{"5968": "\ndef register ( cls , type , reduce_func ) : \n    if ( 3 , ) > sys . version_info : \n        def dispatcher ( cls , obj ) : \n            reduced = reduce_func ( obj ) \n            cls . save_reduce ( obj = obj , * reduced ) \n        cls . dispatch_table [ type ] = dispatcher \n    else : \n        cls . dispatch_table [ type ] = reduce_func "}
{"5970": "\ndef cpu_count ( ) : \n    import math \n    try : \n        cpu_count_mp = mp . cpu_count ( ) \n    except NotImplementedError : \n        cpu_count_mp = 1 \n    cpu_count_affinity = cpu_count_mp \n    if hasattr ( os , 'sched_getaffinity' ) : \n        try : \n            cpu_count_affinity = len ( os . sched_getaffinity ( 0 ) ) \n        except NotImplementedError : \n            pass \n    cpu_count_cfs = cpu_count_mp \n    cfs_quota_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\" \n    cfs_period_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_period_us\" \n    if os . path . exists ( cfs_quota_fname ) and os . path . exists ( cfs_period_fname ) : \n        with open ( cfs_quota_fname , 'r' ) as fh : \n            cfs_quota_us = int ( fh . read ( ) ) \n        with open ( cfs_period_fname , 'r' ) as fh : \n            cfs_period_us = int ( fh . read ( ) ) \n        if 0 < cfs_quota_us and 0 < cfs_period_us : \n            cpu_count_cfs = int ( math . ceil ( cfs_quota_us / cfs_period_us ) ) \n    cpu_count_loky = int ( os . environ . get ( 'LOKY_MAX_CPU_COUNT' , cpu_count_mp ) ) \n    aggregate_cpu_count = min ( cpu_count_mp , cpu_count_affinity , cpu_count_cfs , cpu_count_loky ) \n    return max ( aggregate_cpu_count , 1 ) "}
{"5972": "\ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = True ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while True : \n        try : \n            call_item = call_queue . get ( block = True , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = False ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                print ( previous_tb ) \n            sys . exit ( 1 ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if _MEMORY_LEAK_CHECK_DELAY < time ( ) - _last_memory_leak_check : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if _MAX_MEMORY_LEAK_SIZE > mem_usage - _process_reference_size : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                if _MAX_MEMORY_LEAK_SIZE > mem_usage - _process_reference_size : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        else : \n            if ( ( _last_memory_leak_check is None ) or ( _MEMORY_LEAK_CHECK_DELAY < time ( ) - _last_memory_leak_check ) ) : \n                gc . collect ( ) \n                _last_memory_leak_check = time ( ) "}
{"5977": "\ndef DupFd ( fd ) : \n    popen_obj = get_spawning_popen ( ) \n    if popen_obj is not None : \n        return popen_obj . DupFd ( popen_obj . duplicate_for_child ( fd ) ) \n    elif HAVE_SEND_HANDLE and ( 3 , 3 ) < sys . version_info [ : 2 ] : \n        from multiprocessing import resource_sharer \n        return resource_sharer . DupFd ( fd ) \n    else : \n        raise TypeError ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' ) "}
{"5978": "\ndef get_reusable_executor ( max_workers = None , context = None , timeout = 10 , kill_workers = False , reuse = \"auto\" , job_reducers = None , result_reducers = None , initializer = None , initargs = ( ) ) : \n    with _executor_lock : \n        global _executor , _executor_kwargs \n        executor = _executor \n        if max_workers is None : \n            if reuse is True and executor is not None : \n                max_workers = executor . _max_workers \n            else : \n                max_workers = cpu_count ( ) \n        elif 0 >= max_workers : \n            raise ValueError ( \"max_workers must be greater than 0, got {}.\" . format ( max_workers ) ) \n        if isinstance ( context , STRING_TYPE ) : \n            context = get_context ( context ) \n        if context is not None and context . get_start_method ( ) == \"fork\" : \n            raise ValueError ( \"Cannot use reusable executor with the 'fork' \" \"context\" ) \n        kwargs = dict ( context = context , timeout = timeout , job_reducers = job_reducers , result_reducers = result_reducers , initializer = initializer , initargs = initargs ) \n        if executor is None : \n            mp . util . debug ( \"Create a executor with max_workers={}.\" . format ( max_workers ) ) \n            executor_id = _get_next_executor_id ( ) \n            _executor_kwargs = kwargs \n            _executor = executor = _ReusablePoolExecutor ( _executor_lock , max_workers = max_workers , executor_id = executor_id , ** kwargs ) \n        else : \n            if reuse == 'auto' : \n                reuse = kwargs == _executor_kwargs \n            if ( executor . _flags . broken or executor . _flags . shutdown or not reuse ) : \n                if executor . _flags . broken : \n                    reason = \"broken\" \n                elif executor . _flags . shutdown : \n                    reason = \"shutdown\" \n                else : \n                    reason = \"arguments have changed\" \n                mp . util . debug ( \"Creating a new executor with max_workers={} as the \" \"previous instance cannot be reused ({}).\" . format ( max_workers , reason ) ) \n                executor . shutdown ( wait = True , kill_workers = kill_workers ) \n                _executor = executor = _executor_kwargs = None \n                return get_reusable_executor ( max_workers = max_workers , ** kwargs ) \n            else : \n                mp . util . debug ( \"Reusing existing executor with max_workers={}.\" . format ( executor . _max_workers ) ) \n                executor . _resize ( max_workers ) \n    return executor "}
{"5979": "\ndef _wait_job_completion ( self ) : \n    if 0 < len ( self . _pending_work_items ) : \n        warnings . warn ( \"Trying to resize an executor with running jobs: \" \"waiting for jobs completion before resizing.\" , UserWarning ) \n        mp . util . debug ( \"Executor {} waiting for jobs completion before\" \" resizing\" . format ( self . executor_id ) ) \n    while 0 < len ( self . _pending_work_items ) : \n        time . sleep ( 1e-3 ) "}
{"5980": "\ndef get_preparation_data ( name , init_main_module = True ) : \n    _check_not_importing_main ( ) \n    d = dict ( log_to_stderr = util . _log_to_stderr , authkey = bytes ( process . current_process ( ) . authkey ) , ) \n    if util . _logger is not None : \n        d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) \n        if 0 < len ( util . _logger . handlers ) : \n            h = util . _logger . handlers [ 0 ] \n            d [ 'log_fmt' ] = h . formatter . _fmt \n    sys_path = [ p for p in sys . path ] \n    try : \n        i = sys_path . index ( '' ) \n    except ValueError : \n        pass \n    else : \n        sys_path [ i ] = process . ORIGINAL_DIR \n    d . update ( name = name , sys_path = sys_path , sys_argv = sys . argv , orig_dir = process . ORIGINAL_DIR , dir = os . getcwd ( ) ) \n    if sys . platform != \"win32\" : \n        from . import semaphore_tracker \n        semaphore_tracker . ensure_running ( ) \n        d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid \n    if init_main_module : \n        main_module = sys . modules [ '__main__' ] \n        try : \n            main_mod_name = getattr ( main_module . __spec__ , \"name\" , None ) \n        except BaseException : \n            main_mod_name = None \n        if main_mod_name is not None : \n            d [ 'init_main_from_name' ] = main_mod_name \n        elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : \n            main_path = getattr ( main_module , '__file__' , None ) \n            if main_path is not None : \n                if ( not os . path . isabs ( main_path ) and process . ORIGINAL_DIR is not None ) : \n                    main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) \n                d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) \n                d [ 'main_path' ] = d [ 'init_main_from_path' ] \n    return d "}
{"5985": "\ndef get_exitcodes_terminated_worker ( processes ) : \n    patience = 5 \n    exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n    while len ( exitcodes ) == 0 and 0 < patience : \n        patience -= 1 \n        exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n        time . sleep ( .05 ) \n    return _format_exitcodes ( exitcodes ) "}
{"5988": "\ndef ensure_running ( self ) : \n    with self . _lock : \n        if self . _fd is not None : \n            if self . _check_alive ( ) : \n                return \n            os . close ( self . _fd ) \n            try : \n                os . waitpid ( self . _pid , 0 ) \n            except OSError : \n                pass \n            self . _fd = None \n            self . _pid = None \n            warnings . warn ( 'semaphore_tracker: process died unexpectedly, ' 'relaunching.  Some semaphores might leak.' ) \n        fds_to_pass = [ ] \n        try : \n            fds_to_pass . append ( sys . stderr . fileno ( ) ) \n        except Exception : \n            pass \n        r , w = os . pipe ( ) \n        cmd = 'from {} import main; main({}, {})' . format ( main . __module__ , r , VERBOSE ) \n        try : \n            fds_to_pass . append ( r ) \n            exe = spawn . get_executable ( ) \n            args = [ exe ] + util . _args_from_interpreter_flags ( ) \n            if ( 3 , 3 ) >= sys . version_info [ : 2 ] : \n                import re \n                for i in range ( 1 , len ( args ) ) : \n                    args [ i ] = re . sub ( \"-R+\" , \"-R\" , args [ i ] ) \n            args += [ '-c' , cmd ] \n            util . debug ( \"launching Semaphore tracker: {}\" . format ( args ) ) \n            try : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_BLOCK , _IGNORED_SIGNALS ) \n                pid = spawnv_passfds ( exe , args , fds_to_pass ) \n            finally : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n        except BaseException : \n            os . close ( w ) \n            raise \n        else : \n            self . _fd = w \n            self . _pid = pid \n        finally : \n            os . close ( r ) "}
{"5997": "\ndef get_int ( self , arg , min_value = 0 , default = 1 , cmdname = None , at_most = None ) : \n    if arg is None : \n        return default \n    default = self . get_int_noerr ( arg ) \n    if default is None : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer; \" + \"got: %s.\" ) % ( cmdname , str ( arg ) ) ) \n        else : \n            self . errmsg ( 'Expecting a positive integer, got: %s' % str ( arg ) ) \n            pass \n        return None \n        pass \n    if min_value > default : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at least\" + ' %d; got: %d.' ) % ( cmdname , min_value , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting a positive integer at least\" + ' %d; got: %d' ) % ( min_value , default ) ) \n            pass \n        return None \n    elif at_most and at_most < default : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at most\" + ' %d; got: %d.' ) % ( cmdname , at_most , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting an integer at most %d; got: %d\" ) % ( at_most , default ) ) \n            pass \n        pass \n    return default "}
{"5998": "\ndef process_commands ( self ) : \n    if self . core . execution_status != 'No program' : \n        self . setup ( ) \n        self . location ( ) \n        pass \n    leave_loop = run_hooks ( self , self . preloop_hooks ) \n    self . continue_running = False \n    while not leave_loop : \n        try : \n            run_hooks ( self , self . precmd_hooks ) \n            leave_loop = self . process_command ( ) \n            if leave_loop or self . continue_running : \n                break \n        except EOFError : \n            if 1 < len ( self . debugger . intf ) : \n                del self . debugger . intf [ - 1 ] \n                self . last_command = '' \n            else : \n                if self . debugger . intf [ - 1 ] . output : \n                    self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) \n                    raise Mexcept . DebuggerQuit \n                    pass \n                break \n            pass \n        pass \n    return run_hooks ( self , self . postcmd_hooks ) "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - 1 , cur_line = 0 , start_line = - 1 , end_line = None , relative_pos = False , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    statement_count = 10000 \n    if end_line is None : \n        end_line = 10000 \n    elif relative_pos : \n        end_line += start_line - 1 \n        pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if cur_line < start_line : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and end_offset < offset : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( cur_line < start_line ) or start_offset and offset < start_offset ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count -= 1 \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( end_line < cur_line ) or ( end_offset and end_offset < offset ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and offset >= start_offset : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if offset == lasti : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6007": "\ndef get_call_function_name ( frame ) : \n    f_back = frame . f_back \n    if not f_back : \n        return None \n    if 'CALL_FUNCTION' != Mbytecode . op_at_frame ( f_back ) : \n        return None \n    co = f_back . f_code \n    code = co . co_code \n    linestarts = dict ( dis . findlinestarts ( co ) ) \n    offset = f_back . f_lasti \n    while 0 <= offset : \n        if offset in linestarts : \n            op = code [ offset ] \n            offset += 1 \n            arg = code [ offset ] \n            extended_arg = 0 \n            while True : \n                if 3.6 <= PYTHON_VERSION : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 8 ) \n                        continue \n                    arg = code [ offset ] + extended_arg \n                else : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 256 ) \n                        continue \n                    arg = code [ offset ] + code [ offset + 1 ] * 256 + extended_arg \n                break \n            return co . co_names [ arg ] \n        offset -= 1 \n        pass \n    return None "}
{"6009": "\ndef lookup ( self , subcmd_prefix ) : \n    for subcmd_name in list ( self . subcmds . keys ( ) ) : \n        if subcmd_name . startswith ( subcmd_prefix ) and self . subcmds [ subcmd_name ] . __class__ . min_abbrev <= len ( subcmd_prefix ) : \n            return self . subcmds [ subcmd_name ] \n        pass \n    return None "}
{"6029": "\ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        print ( \"midpoint\" , midpoint , 'vs' , ( 16 * 5 + 16 * g + 16 * b ) ) \n        midpoint = 383 if term and term == 'xterm-256color' else 117963 \n    if ( midpoint > ( 16 * 5 + 16 * g + 16 * b ) ) : \n        return True \n    else : \n        return False "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return True \n    args = arg . split ( ) \n    signame = args [ 0 ] \n    signame = self . is_name_or_number ( args [ 0 ] ) \n    if not signame : \n        return \n    if len ( args ) == 1 : \n        self . info_signal ( [ signame ] ) \n        return True \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ 1 : ] : \n        if attr . startswith ( 'no' ) : \n            on = False \n            attr = attr [ 2 : ] \n        else : \n            on = True \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        elif 'print' . startswith ( attr ) and 2 <= len ( attr ) : \n            self . handle_print ( signame , on ) \n        elif 'pass' . startswith ( attr ) : \n            self . handle_pass ( signame , on ) \n        elif 'ignore' . startswith ( attr ) : \n            self . handle_ignore ( signame , on ) \n        elif 'stack' . startswith ( attr ) : \n            self . handle_print_stack ( signame , on ) \n        else : \n            self . dbgr . intf [ - 1 ] . errmsg ( 'Invalid arguments' ) \n            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6057": "\ndef write ( self , msg ) : \n    if self . state != 'connected' : \n        self . wait_for_connect ( ) \n        pass \n    buffer = Mtcpfns . pack_msg ( msg ) \n    while Mtcpfns . TCP_MAX_PACKET < len ( buffer ) : \n        self . conn . send ( buffer [ : Mtcpfns . TCP_MAX_PACKET ] ) \n        buffer = buffer [ Mtcpfns . TCP_MAX_PACKET : ] \n    return self . conn . send ( buffer ) "}
{"6064": "\ndef is_stop_here ( self , frame , event , arg ) : \n    lineno = frame . f_lineno \n    filename = frame . f_code . co_filename \n    if self . different_line and event == 'line' : \n        if self . last_lineno == lineno and self . last_filename == filename : \n            return False \n        pass \n    self . last_lineno = lineno \n    self . last_filename = filename \n    if self . stop_level is not None : \n        if frame != self . last_frame : \n            self . last_level = Mstack . count_frames ( frame ) \n            self . last_frame = frame \n            pass \n        if self . stop_level < self . last_level : \n            return False \n        elif self . last_level == self . stop_level and self . stop_on_finish and event in [ 'return' , 'c_return' ] : \n            self . stop_level = None \n            self . stop_reason = \"in return for 'finish' command\" \n            return True \n        pass \n    if self . _is_step_next_stop ( event ) : \n        self . stop_reason = 'at a stepping statement' \n        return True \n    return False "}
{"6067": "\ndef run ( self , args ) : \n    if len ( args ) == 0 : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return False \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ 0 ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m += \" cached in debugger\" \n        if filename_cache [ filename ] != filename : \n            m += ' as:' \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m += '.' \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( 1 < len ( matches ) ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        elif len ( matches ) == 1 : \n            canonic_name = pyficache . unmap_file ( matches [ 0 ] ) \n            m += \" matched debugger cache file:\\n  \" + canonic_name \n            self . msg ( m ) \n        else : \n            self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if name == v ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ 1 : ] : \n        processed_arg = False \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = False , arrange_vertical = False , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = True \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6096": "\ndef _request ( self , req_type , url , ** kwargs ) : \n    logger . debug ( '%s %s' % ( req_type , url ) ) \n    result = self . session . request ( req_type , url , ** kwargs ) \n    try : \n        result . raise_for_status ( ) \n    except requests . HTTPError : \n        error = result . text \n        try : \n            error = json . loads ( error ) \n        except ValueError : \n            pass \n        if result . status_code in ( 401 , 403 ) : \n            error_class = LuminosoAuthError \n        elif result . status_code in ( 400 , 404 , 405 ) : \n            error_class = LuminosoClientError \n        elif 500 <= result . status_code : \n            error_class = LuminosoServerError \n        else : \n            error_class = LuminosoError \n        raise error_class ( error ) \n    return result "}
{"6098": "\ndef wait_for_build ( self , interval = 5 , path = None ) : \n    path = path or '' \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) [ 'last_build_info' ] \n        if not response : \n            raise ValueError ( 'This project is not building!' ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if next_log < elapsed : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6106": "\ndef wait_for ( self , job_id , base_path = None , interval = 5 ) : \n    if base_path is None : \n        base_path = 'jobs/id' \n    path = '%s%d' % ( ensure_trailing_slash ( base_path ) , job_id ) \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if next_log < elapsed : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        print ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == 0 : \n            print ( 'No such project exists!' ) \n            return \n        if 1 < len ( projects ) : \n            print ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ 0 ] [ 'project_id' ] \n        print ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000 ) : \n        counter += 1 \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        print ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        print ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6164": "\ndef append_item ( self , item ) : \n    did_remove = self . remove_exit ( ) \n    item . menu = self \n    self . items . append ( item ) \n    if did_remove : \n        self . add_exit ( ) \n    if self . screen : \n        max_row , max_cols = self . screen . getmaxyx ( ) \n        if 6 + len ( self . items ) > max_row : \n            self . screen . resize ( 6 + len ( self . items ) , max_cols ) \n        self . draw ( ) "}
{"6166": "\ndef draw ( self ) : \n    self . screen . border ( 0 ) \n    if self . title is not None : \n        self . screen . addstr ( 2 , 2 , self . title , curses . A_STANDOUT ) \n    if self . subtitle is not None : \n        self . screen . addstr ( 4 , 2 , self . subtitle , curses . A_BOLD ) \n    for index , item in enumerate ( self . items ) : \n        if self . current_option == index : \n            text_style = self . highlight \n        else : \n            text_style = self . normal \n        self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text_style ) \n    screen_rows , screen_cols = CursesMenu . stdscr . getmaxyx ( ) \n    top_row = 0 \n    if screen_rows < 6 + len ( self . items ) : \n        if 6 + len ( self . items ) > screen_rows + self . current_option : \n            top_row = self . current_option \n        else : \n            top_row = 6 + len ( self . items ) - screen_rows \n    self . screen . refresh ( top_row , 0 , 0 , 0 , screen_rows - 1 , screen_cols - 1 ) "}
{"6167": "\ndef process_user_input ( self ) : \n    user_input = self . get_input ( ) \n    go_to_max = ord ( \"9\" ) if 9 <= len ( self . items ) else ord ( str ( len ( self . items ) ) ) \n    if ord ( '1' ) <= user_input <= go_to_max : \n        self . go_to ( user_input - ord ( '0' ) - 1 ) \n    elif user_input == curses . KEY_DOWN : \n        self . go_down ( ) \n    elif user_input == curses . KEY_UP : \n        self . go_up ( ) \n    elif user_input == ord ( \"\\n\" ) : \n        self . select ( ) \n    return user_input "}
{"6170": "\ndef top ( df , value : str , limit : int , order : str = 'asc' , group : Union [ str , List [ str ] ] = None ) : \n    ascending = order != 'desc' \n    limit = int ( limit ) \n    filter_func = 'nlargest' if ( 0 < limit ) ^ ascending else 'nsmallest' \n    def _top ( df ) : \n        return getattr ( df , filter_func ) ( abs ( limit ) , value ) . sort_values ( by = value , ascending = ascending ) \n    if group is None : \n        df = _top ( df ) \n    else : \n        df = df . groupby ( group ) . apply ( _top ) \n    return df "}
{"6185": "\ndef add_missing_row ( df : pd . DataFrame , id_cols : List [ str ] , reference_col : str , complete_index : Union [ Dict [ str , str ] , List [ str ] ] = None , method : str = None , cols_to_keep : List [ str ] = None ) -> pd . DataFrame : \n    if cols_to_keep is None : \n        cols_for_index = [ reference_col ] \n    else : \n        cols_for_index = [ reference_col ] + cols_to_keep \n    check_params_columns_duplicate ( id_cols + cols_for_index ) \n    if method == 'between' or method == 'between_and_after' : \n        df [ 'start' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( min ) \n        id_cols += [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        df [ 'end' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( max ) \n        id_cols += [ 'end' ] \n    names = id_cols + cols_for_index \n    new_df = df . set_index ( names ) \n    index_values = df . groupby ( id_cols ) . sum ( ) . index . values \n    if complete_index is None : \n        complete_index = df . groupby ( cols_for_index ) . sum ( ) . index . values \n    elif isinstance ( complete_index , dict ) : \n        if complete_index [ 'type' ] == 'date' : \n            freq = complete_index [ 'freq' ] \n            date_format = complete_index [ 'format' ] \n            start = complete_index [ 'start' ] \n            end = complete_index [ 'end' ] \n            if isinstance ( freq , dict ) : \n                freq = pd . DateOffset ( ** { k : int ( v ) for k , v in freq . items ( ) } ) \n            complete_index = pd . date_range ( start = start , end = end , freq = freq ) \n            complete_index = complete_index . strftime ( date_format ) \n        else : \n            raise ParamsValueError ( f'Unknown complete index type: ' f'{complete_index[\"type\"]}' ) \n    if not isinstance ( index_values [ 0 ] , tuple ) : \n        index_values = [ ( x , ) for x in index_values ] \n    if not isinstance ( complete_index [ 0 ] , tuple ) : \n        complete_index = [ ( x , ) for x in complete_index ] \n    new_tuples_index = [ x + y for x in index_values for y in complete_index ] \n    new_index = pd . MultiIndex . from_tuples ( new_tuples_index , names = names ) \n    new_df = new_df . reindex ( new_index ) . reset_index ( ) \n    if method == 'between' or method == 'between_and_after' : \n        new_df = new_df [ new_df [ 'start' ] <= new_df [ reference_col ] ] \n        del new_df [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        new_df = new_df [ new_df [ 'end' ] >= new_df [ reference_col ] ] \n        del new_df [ 'end' ] \n    return new_df "}
{"6194": "\ndef clean_cachedir_old_entries ( cachedir : StoreBackendBase , func_name : str , limit : int ) -> int : \n    if 1 > limit : \n        raise ValueError ( \"'limit' must be greater or equal to 1\" ) \n    cache_entries = get_cachedir_entries ( cachedir , func_name ) \n    cache_entries = sorted ( cache_entries , key = lambda e : e . last_access , reverse = True ) \n    cache_entries_to_remove = cache_entries [ limit : ] \n    for entry in cache_entries_to_remove : \n        shutil . rmtree ( entry . path , ignore_errors = True ) \n    return len ( cache_entries_to_remove ) "}
{"6200": "\ndef add_months ( dateobj , nb_months : int ) : \n    nb_years , nb_months = divmod ( nb_months , 12 ) \n    month = dateobj . month + nb_months \n    if 12 < month : \n        nb_years += 1 \n        month -= 12 \n    year = dateobj . year + nb_years \n    lastday = monthrange ( year , month ) [ 1 ] \n    return dateobj . replace ( year = year , month = month , day = min ( lastday , dateobj . day ) ) "}
{"6203": "\ndef filter_by_date ( df , date_col : str , date_format : str = '%Y-%m-%d' , start : str = None , stop : str = None , atdate : str = None ) : \n    mask = None \n    if start is None and stop is None and atdate is None : \n        raise TypeError ( 'either \"start\", \"stop\" or \"atdate\" must be specified' ) \n    if start is not None and atdate is not None : \n        raise TypeError ( '\"start\" and \"atdate\" are mutually exclusive' ) \n    if stop is not None and atdate is not None : \n        raise TypeError ( '\"stop\" and \"atdate\" are mutually exclusive' ) \n    filtercol = str ( uuid4 ( ) ) \n    df [ filtercol ] = pd . to_datetime ( df [ date_col ] , format = date_format ) \n    if atdate is not None : \n        mask = df [ filtercol ] == parse_date ( atdate , date_format ) \n    elif start is not None and stop is not None : \n        mask = ( ( parse_date ( start , date_format ) <= df [ filtercol ] ) & ( parse_date ( stop , date_format ) > df [ filtercol ] ) ) \n    elif stop is None : \n        mask = parse_date ( start , date_format ) <= df [ filtercol ] \n    elif start is None : \n        mask = parse_date ( stop , date_format ) > df [ filtercol ] \n    return df [ mask ] . drop ( filtercol , axis = 1 ) "}
{"6230": "\ndef load_params ( self , path , exclude_free_params = False ) : \n    self . network . load_params ( path , exclude_free_params = exclude_free_params ) \n    self . best_params = self . copy_params ( ) \n    if 0 < self . network . train_logger . progress ( ) or 0 < self . network . train_logger . epoch ( ) : \n        self . skip ( self . network . train_logger . progress ( ) , self . network . train_logger . epoch ( ) - 1 ) "}
{"6231": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    self . _epoch = 0 \n    while True : \n        if 0 < self . _skip_epochs : \n            logging . info ( \"skipping one epoch ...\" ) \n            self . _skip_epochs -= 1 \n            self . _epoch += 1 \n            yield None \n            continue \n        if not self . _epoch % self . config . test_frequency and test_set : \n            try : \n                self . _run_test ( self . _epoch , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not self . _epoch % self . validation_frequency and valid_set : \n            try : \n                if not self . _run_valid ( self . _epoch , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        try : \n            costs = self . _run_train ( self . _epoch , train_set , train_size ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if np . isnan ( costs [ 0 ] [ 1 ] ) : \n            logging . info ( \"NaN detected in costs, rollback to last parameters\" ) \n            self . set_params ( * self . checkpoint ) \n        else : \n            self . _epoch += 1 \n            self . network . epoch_callback ( ) \n        yield dict ( costs ) \n    if valid_set and self . config . get ( \"save_best_parameters\" , True ) : \n        self . set_params ( * self . best_params ) \n    if test_set : \n        self . _run_test ( - 1 , test_set ) "}
{"6233": "\ndef _run_valid ( self , epoch , valid_set , dry_run = False , save_path = None ) : \n    costs = self . valid_step ( valid_set ) \n    _ , J = costs [ 0 ] \n    new_best = False \n    if self . best_cost * self . min_improvement < self . best_cost - J : \n        self . best_params = self . copy_params ( ) \n        new_best = True \n        if not dry_run : \n            self . best_cost = J \n            self . best_epoch = epoch \n        self . save_checkpoint ( save_path ) \n    self . report ( dict ( costs ) , type = \"valid\" , epoch = 0 if dry_run else epoch , new_best = new_best ) \n    self . last_run_costs = costs \n    return self . patience > epoch - self . best_epoch "}
{"6234": "\ndef report ( self , score_map , type = \"valid\" , epoch = - 1 , new_best = False ) : \n    type_str = type \n    if 5 > len ( type_str ) : \n        type_str += \" \" * ( 5 - len ( type_str ) ) \n    info = \" \" . join ( \"%s=%.2f\" % el for el in score_map . items ( ) ) \n    current_epoch = epoch if 0 < epoch else self . current_epoch ( ) \n    epoch_str = \"epoch={}\" . format ( current_epoch + 1 ) \n    if 0 > epoch : \n        epoch_str = \"dryrun\" \n        sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) \n    marker = \" *\" if new_best else \"\" \n    message = \"{} ({}) {}{}\" . format ( type_str , epoch_str , info , marker ) \n    self . network . train_logger . record ( message ) \n    logging . info ( message ) "}
{"6258": "\ndef multiple_l2_norm ( tensors ) : \n    flattened = [ T . as_tensor_variable ( t ) . flatten ( ) for t in tensors ] \n    flattened = [ ( t if 0 < t . ndim else t . dimshuffle ( 'x' ) ) for t in flattened ] \n    joined = T . join ( 0 , * flattened ) \n    return T . sqrt ( T . sqr ( joined ) . sum ( ) ) "}
{"6272": "\ndef save ( self ) : \n    if self . data : \n        cookie = self . create_cookie ( ) \n        cookie_len = len ( cookie ) \n        if 4093 < cookie_len : \n            raise SessionError ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie_len ) ) \n        self . adapter . set_header ( 'Set-Cookie' , cookie ) \n        self . _data = { } "}
{"6277": "\ndef valid ( self ) : \n    if self . expiration_time : \n        return int ( time . time ( ) ) < self . expiration_time \n    else : \n        return True "}
{"6278": "\ndef expire_soon ( self , seconds ) : \n    if self . expiration_time : \n        return int ( time . time ( ) ) + int ( seconds ) > self . expiration_time \n    else : \n        return False "}
{"6290": "\ndef _http_status_in_category ( status , category ) : \n    assert 10 > category , 'HTTP status category must be a one-digit int!' \n    cat = category * 100 \n    return cat <= status and cat + 100 > status "}
{"6292": "\ndef cross_origin ( app , * args , ** kwargs ) : \n    _options = kwargs \n    _real_decorator = cors . decorate ( app , * args , run_middleware = False , with_context = False , ** kwargs ) \n    def wrapped_decorator ( f ) : \n        spf = SanicPluginsFramework ( app ) \n        try : \n            plugin = spf . register_plugin ( cors , skip_reg = True ) \n        except ValueError as e : \n            assert e . args and 1 < len ( e . args ) \n            plugin = e . args [ 1 ] \n        context = cors . get_context_from_spf ( spf ) \n        log = context . log \n        log ( logging . DEBUG , \"Enabled {:s} for cross_origin using options: {}\" . format ( str ( f ) , str ( _options ) ) ) \n        return _real_decorator ( f ) \n    return wrapped_decorator "}
{"6297": "\ndef isclose ( a , b , * , rel_tol = 1e-09 , abs_tol = 0.0 ) : \n    try : \n        return math . isclose ( a , b , rel_tol = rel_tol , abs_tol = abs_tol ) \n    except AttributeError : \n        if ( 0.0 > rel_tol ) or ( 0.0 > abs_tol ) : \n            raise ValueError ( \"Tolerances must be non-negative, but are rel_tol: {} and abs_tol: {}\" . format ( rel_tol , abs_tol ) ) \n        if math . isnan ( a ) or math . isnan ( b ) : \n            return False \n        if ( a == b ) : \n            return True \n        if math . isinf ( a ) or math . isinf ( b ) : \n            return False \n        diff = abs ( a - b ) \n        return ( rel_tol * abs ( b ) >= diff ) or ( rel_tol * abs ( a ) >= diff ) or ( abs_tol >= diff ) "}
{"6304": "\ndef fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = False ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample can be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples can be specified.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    if duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    end_sample = start_sample + num_samples \n    if len ( self . get_array_of_samples ( ) ) < end_sample and not zero_pad : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    elif len ( self . get_array_of_samples ( ) ) < end_sample and zero_pad : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n        zeros = np . zeros ( end_sample - len ( arr ) ) \n        arr = np . append ( arr , zeros ) \n    else : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n    audioslice = np . array ( arr [ start_sample : end_sample ] ) \n    fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2 ) ) + 1 ) ] \n    step_size = self . frame_rate / num_samples \n    bins = np . arange ( 0 , int ( round ( num_samples / 2 ) ) + 1 , 1.0 ) * step_size \n    return bins , fft_result "}
{"6305": "\ndef generate_frames ( self , frame_duration_ms , zero_pad = True ) : \n    Frame = collections . namedtuple ( \"Frame\" , \"bytes timestamp duration\" ) \n    bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000 ) * self . sample_width ) \n    offset = 0 \n    timestamp = 0.0 \n    frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width \n    while len ( self . raw_data ) > offset + bytes_per_frame : \n        yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) \n        timestamp += frame_duration_s \n        offset += bytes_per_frame \n    if zero_pad : \n        rest = self . raw_data [ offset : ] \n        zeros = bytes ( bytes_per_frame - len ( rest ) ) \n        yield Frame ( rest + zeros , timestamp , frame_duration_s ) "}
{"6306": "\ndef normalize_spl_by_average ( self , db ) : \n    arr = self . to_numpy_array ( ) . copy ( ) \n    if len ( arr ) == 0 : \n        raise ValueError ( \"Cannot normalize the SPL of an empty AudioSegment\" ) \n    def rms ( x ) : \n        return np . sqrt ( np . mean ( np . square ( x ) ) ) \n    desired_rms = P_REF_PCM * ( ( 10 ** ( db / 20.0 ) ) - 1E-9 ) \n    max_ntries = 50 \n    res_rms = 0.0 \n    ntries = 0 \n    factor = 0.1 \n    left = 0.0 \n    right = desired_rms \n    while ( max_ntries > ntries ) and not util . isclose ( res_rms , desired_rms , abs_tol = 0.1 ) : \n        res_rms = rms ( arr * factor ) \n        if desired_rms > res_rms : \n            left = factor \n        else : \n            right = factor \n        factor = 0.5 * ( left + right ) \n        ntries += 1 \n    dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ self . sample_width ] \n    new_seg = from_numpy_array ( np . array ( arr * factor , dtype = dtype ) , self . frame_rate ) \n    return new_seg "}
{"6310": "\ndef spectrogram ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , window_length_s = None , window_length_samples = None , overlap = 0.5 , window = ( 'tukey' , 0.25 ) ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample may be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples may be specified.\" ) \n    if window_length_s is not None and window_length_samples is not None : \n        raise ValueError ( \"Only one of window_length_s and window_length_samples may be specified.\" ) \n    if window_length_s is None and window_length_samples is None : \n        raise ValueError ( \"You must specify a window length, either in window_length_s or in window_length_samples.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    elif start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    elif duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if window_length_s is not None : \n        window_length_samples = int ( round ( window_length_s * self . frame_rate ) ) \n    if len ( self . get_array_of_samples ( ) ) < start_sample + num_samples : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    arr = self . to_numpy_array ( ) [ start_sample : start_sample + num_samples ] \n    fs , ts , sxx = signal . spectrogram ( arr , self . frame_rate , scaling = 'spectrum' , nperseg = window_length_samples , noverlap = int ( round ( overlap * window_length_samples ) ) , mode = 'magnitude' , window = window ) \n    return fs , ts , sxx "}
{"6312": "\ndef _get_offset_front_id_after_onset_sample_idx ( onset_sample_idx , offset_fronts ) : \n    offset_front_ids = [ i for i in np . unique ( offset_fronts ) if i != 0 ] \n    best_id_so_far = - 1 \n    closest_offset_sample_idx = sys . maxsize \n    for offset_front_id in offset_front_ids : \n        offset_front_idxs = _get_front_idxs_from_id ( offset_fronts , offset_front_id ) \n        offset_front_sample_idxs = [ s for _f , s in offset_front_idxs ] \n        min_sample_idx = min ( offset_front_sample_idxs ) \n        if onset_sample_idx < min_sample_idx and closest_offset_sample_idx > min_sample_idx : \n            closest_offset_sample_idx = min_sample_idx \n            best_id_so_far = offset_front_id \n    assert 1 < best_id_so_far or best_id_so_far == - 1 \n    return best_id_so_far "}
{"6316": "\ndef _update_segmentation_mask ( segmentation_mask , onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) : \n    onset_front_overlap , offset_front_overlap = _get_consecutive_and_overlapping_fronts ( onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) \n    onset_front = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) \n    offset_front = _get_front_idxs_from_id ( offset_fronts , offset_front_id_most_overlap ) \n    msg = \"Onset front {} and offset front {} result in consecutive overlapping portions of (on) {} and (off) {}, one of which is empty\" . format ( onset_front , offset_front , onset_front_overlap , offset_front_overlap ) \n    assert onset_front_overlap , msg \n    assert offset_front_overlap , msg \n    onset_front = onset_front_overlap \n    offset_front = offset_front_overlap \n    flow_on , _slow_on = onset_front [ 0 ] \n    fhigh_on , _shigh_on = onset_front [ - 1 ] \n    flow_off , _slow_off = offset_front [ 0 ] \n    fhigh_off , _shigh_off = offset_front [ - 1 ] \n    flow = max ( flow_on , flow_off ) \n    fhigh = min ( fhigh_on , fhigh_off ) \n    for fidx , _freqchan in enumerate ( segmentation_mask [ flow : fhigh + 1 , : ] , start = flow ) : \n        assert flow <= fidx , \"Frequency index is {}, but we should have started at {}\" . format ( fidx , flow ) \n        assert len ( onset_front ) > ( fidx - flow ) , \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in onset front {}\" . format ( fidx , flow , len ( onset_front ) , onset_front ) \n        assert len ( offset_front ) > ( fidx - flow ) , \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in offset front {}\" . format ( fidx , flow , len ( offset_front ) , offset_front ) \n        _ , beg = onset_front [ fidx - flow ] \n        _ , end = offset_front [ fidx - flow ] \n        if end < beg : \n            end , beg = beg , end \n        assert beg <= end \n        segmentation_mask [ fidx , beg : end + 1 ] = onset_front_id \n        onset_fronts [ fidx , ( beg + 1 ) : ( end + 1 ) ] = 0 \n        offset_fronts [ fidx , ( beg + 1 ) : ( end + 1 ) ] = 0 \n    nfreqs_used_in_onset_front = ( fidx - flow ) + 1 \n    indexes = np . arange ( flow , fhigh + 1 , 1 , dtype = np . int64 ) \n    onset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in onset_front ] ) \n    onset_front_sample_idxs_across_freqs_up_to_break = onset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] \n    offset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in offset_front ] ) \n    offset_front_sample_idxs_across_freqs_up_to_break = offset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] \n    offset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , offset_front_sample_idxs_across_freqs_up_to_break ] = 0 \n    onset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , onset_front_sample_idxs_across_freqs_up_to_break ] = 0 \n    whole_onset_front_matched = onset_front_id not in np . unique ( onset_fronts ) \n    return whole_onset_front_matched "}
{"6321": "\ndef _remove_fronts_that_are_too_small ( fronts , size ) : \n    ids = np . unique ( fronts ) \n    for id in ids : \n        if id == 0 or id == - 1 : \n            continue \n        front = _get_front_idxs_from_id ( fronts , id ) \n        if size > len ( front ) : \n            indexes = ( [ f for f , _ in front ] , [ s for _ , s in front ] ) \n            fronts [ indexes ] = 0 "}
{"6322": "\ndef _break_poorly_matched_fronts ( fronts , threshold = 0.1 , threshold_overlap_samples = 3 ) : \n    assert 0 < threshold_overlap_samples , \"Number of samples of overlap must be greater than zero\" \n    breaks_after = { } \n    for front_id in _get_front_ids_one_at_a_time ( fronts ) : \n        front = _get_front_idxs_from_id ( fronts , front_id ) \n        for i , ( f , s ) in enumerate ( front ) : \n            if len ( front ) - 1 > i : \n                next_f , next_s = front [ i + 1 ] \n                low_s = min ( s , next_s ) \n                high_s = max ( s , next_s ) \n                sig_this_f = fronts [ f , low_s : high_s ] \n                sig_next_f = fronts [ next_f , low_s : high_s ] \n                assert len ( sig_next_f ) == len ( sig_this_f ) \n                if threshold_overlap_samples < len ( sig_next_f ) : \n                    correlation = signal . correlate ( sig_this_f , sig_next_f , mode = 'same' ) \n                    assert 0 < len ( correlation ) \n                    correlation = correlation / max ( correlation + 1E-9 ) \n                    similarity = np . sum ( correlation ) / len ( correlation ) \n                    if threshold > similarity : \n                        if front_id in breaks_after : \n                            breaks_after [ front_id ] . append ( ( f , s ) ) \n                        else : \n                            breaks_after [ front_id ] = [ ] \n    taken_ids = sorted ( np . unique ( fronts ) ) \n    next_id = taken_ids [ - 1 ] + 1 \n    for id in breaks_after . keys ( ) : \n        for f , s in breaks_after [ id ] : \n            fidxs , sidxs = np . where ( fronts == id ) \n            idxs_greater_than_f = [ fidx for fidx in fidxs if f < fidx ] \n            start = len ( sidxs ) - len ( idxs_greater_than_f ) \n            indexes = ( idxs_greater_than_f , sidxs [ start : ] ) \n            fronts [ indexes ] = next_id \n            next_id += 1 \n    _remove_fronts_that_are_too_small ( fronts , 3 ) "}
{"6325": "\ndef _downsample_one_or_the_other ( mask , mask_indexes , stft , stft_indexes ) : \n    assert len ( mask . shape ) == 2 , \"Expected a two-dimensional `mask`, but got one of {} dimensions.\" . format ( len ( mask . shape ) ) \n    assert len ( stft . shape ) == 2 , \"Expected a two-dimensional `stft`, but got one of {} dimensions.\" . format ( len ( stft . shape ) ) \n    if stft . shape [ 1 ] < mask . shape [ 1 ] : \n        downsample_factor = mask . shape [ 1 ] / stft . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( mask , downsample_factor ) \n        mask = mask [ : , indexes ] \n        mask_indexes = np . array ( indexes ) \n    elif stft . shape [ 1 ] > mask . shape [ 1 ] : \n        downsample_factor = stft . shape [ 1 ] / mask . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( stft , downsample_factor ) \n        stft = stft [ : , indexes ] \n        stft_indexes = np . array ( indexes ) \n    return mask , mask_indexes , stft , stft_indexes "}
{"6326": "\ndef _asa_task ( q , masks , stft , sample_width , frame_rate , nsamples_for_each_fft ) : \n    for mask in masks : \n        mask = np . where ( 0 < mask , 1 , 0 ) \n    masks = [ mask * stft for mask in masks ] \n    nparrs = [ ] \n    dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ sample_width ] \n    for m in masks : \n        _times , nparr = signal . istft ( m , frame_rate , nperseg = nsamples_for_each_fft ) \n        nparr = nparr . astype ( dtype ) \n        nparrs . append ( nparr ) \n    for m in nparrs : \n        q . put ( m ) \n    q . put ( \"DONE\" ) "}
{"6332": "\ndef group_audit_ranks ( filenames , measurer , similarity_bound = 0.05 ) : \n    def _partition_groups ( feature_scores ) : \n        groups = [ ] \n        for feature , score in feature_scores : \n            added_to_group = False \n            for i , group in enumerate ( groups ) : \n                mean_score , group_feature_scores = group \n                if similarity_bound > abs ( mean_score - score ) : \n                    groups [ i ] [ 1 ] . append ( ( feature , score ) ) \n                    groups [ i ] [ 0 ] = sum ( [ s for _ , s in group_feature_scores ] ) / len ( group_feature_scores ) \n                    added_to_group = True \n                    break \n            if not added_to_group : \n                groups . append ( [ score , [ ( feature , score ) ] ] ) \n        return [ [ feature for feature , score in group ] for _ , group in groups ] \n    score_dict = { } \n    features = [ ] \n    for filename in filenames : \n        with open ( filename ) as audit_file : \n            header_line = audit_file . readline ( ) [ : - 1 ] \n            feature = header_line [ header_line . index ( \":\" ) + 1 : ] \n            features . append ( feature ) \n        confusion_matrices = load_audit_confusion_matrices ( filename ) \n        for rep_level , matrix in confusion_matrices : \n            score = measurer ( matrix ) \n            if rep_level not in score_dict : \n                score_dict [ rep_level ] = { } \n            score_dict [ rep_level ] [ feature ] = score \n    score_keys = sorted ( score_dict . keys ( ) ) \n    groups = [ features ] \n    while score_keys : \n        key = score_keys . pop ( ) \n        new_groups = [ ] \n        for group in groups : \n            group_features = [ ( f , score_dict [ key ] [ f ] ) for f in group ] \n            sub_groups = _partition_groups ( group_features ) \n            new_groups . extend ( sub_groups ) \n        groups = new_groups \n    return groups "}
{"6340": "\ndef handle_error ( errcode ) : \n    if type ( errcode ) is c_int : \n        errcode = errcode . value \n    if errcode == 0 : \n        pass \n    elif errcode == - 1 : \n        raise TimeoutError ( \"the operation failed due to a timeout.\" ) \n    elif errcode == - 2 : \n        raise LostError ( \"the stream has been lost.\" ) \n    elif errcode == - 3 : \n        raise InvalidArgumentError ( \"an argument was incorrectly specified.\" ) \n    elif errcode == - 4 : \n        raise InternalError ( \"an internal error has occurred.\" ) \n    elif 0 > errcode : \n        raise RuntimeError ( \"an unknown error has occurred.\" ) "}
{"6362": "\ndef make_fuzzy ( word , max = 1 ) : \n    neighbors = [ ] \n    for i in range ( 0 , len ( word ) - 1 ) : \n        neighbor = list ( word ) \n        neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] \n        neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( 0 , len ( word ) ) : \n            neighbor = list ( word ) \n            if letter != neighbor [ i ] : \n                neighbor [ i ] = letter \n                neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( 0 , len ( word ) + 1 ) : \n            neighbor = list ( word ) \n            neighbor . insert ( i , letter ) \n            neighbors . append ( '' . join ( neighbor ) ) \n    if 3 < len ( word ) : \n        for i in range ( 0 , len ( word ) ) : \n            neighbor = list ( word ) \n            del neighbor [ i ] \n            neighbors . append ( '' . join ( neighbor ) ) \n    return neighbors "}
{"6376": "\ndef getBits_from_array ( array , wordWidth , start , end , reinterpretElmToType = None ) : \n    inPartOffset = 0 \n    value = Bits ( end - start , None ) . fromPy ( None ) \n    while start != end : \n        assert end > start , ( start , end ) \n        dataWordIndex = start // wordWidth \n        v = array [ dataWordIndex ] \n        if reinterpretElmToType is not None : \n            v = v . _reinterpret_cast ( reinterpretElmToType ) \n        endOfWord = ( dataWordIndex + 1 ) * wordWidth \n        width = min ( end , endOfWord ) - start \n        offset = start % wordWidth \n        val = selectBitRange ( v . val , offset , width ) \n        vldMask = selectBitRange ( v . vldMask , offset , width ) \n        updateTime = v . updateTime \n        m = mask ( width ) \n        value . val |= ( val & m ) << inPartOffset \n        value . vldMask |= ( vldMask & m ) << inPartOffset \n        value . updateMask = max ( value . updateTime , updateTime ) \n        inPartOffset += width \n        start += width \n    return value "}
{"6395": "\ndef flatten ( iterables , level = inf ) : \n    if 0 <= level and isinstance ( iterables , ( list , tuple , GeneratorType , map , zip ) ) : \n        level -= 1 \n        for i in iterables : \n            yield from flatten ( i , level = level ) \n    else : \n        yield iterables "}
{"6419": "\ndef fullWordCnt ( self , start : int , end : int ) : \n    assert start <= end , ( start , end ) \n    gap = max ( 0 , ( end - start ) - ( start % self . wordWidth ) ) \n    return gap // self . wordWidth "}
{"6420": "\ndef groupByWordIndex ( self , transaction : 'TransTmpl' , offset : int ) : \n    actualW = None \n    partsInWord = [ ] \n    wordWidth = self . wordWidth \n    for item in self . splitOnWords ( transaction , offset ) : \n        _actualW = item . startOfPart // wordWidth \n        if actualW is None : \n            actualW = _actualW \n            partsInWord . append ( item ) \n        elif actualW < _actualW : \n            yield ( actualW , partsInWord ) \n            actualW = _actualW \n            partsInWord = [ item , ] \n        else : \n            partsInWord . append ( item ) \n    if partsInWord : \n        yield ( actualW , partsInWord ) "}
{"6422": "\ndef framesFromTransTmpl ( transaction : 'TransTmpl' , wordWidth : int , maxFrameLen : Union [ int , float ] = inf , maxPaddingWords : Union [ int , float ] = inf , trimPaddingWordsOnStart : bool = False , trimPaddingWordsOnEnd : bool = False ) -> Generator [ 'FrameTmpl' , None , None ] : \n    isFirstInFrame = True \n    partsPending = False \n    startOfThisFrame = 0 \n    assert 0 < maxFrameLen \n    assert 0 <= maxPaddingWords \n    if inf > maxPaddingWords : \n        assert trimPaddingWordsOnStart or trimPaddingWordsOnEnd , \"Padding has to be cut off somewhere\" \n    it = TransTmplWordIterator ( wordWidth ) \n    lastWordI = 0 \n    endOfThisFrame = maxFrameLen \n    parts = [ ] \n    for wordI , word in it . groupByWordIndex ( transaction , 0 ) : \n        if endOfThisFrame <= wordI * wordWidth : \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnEnd and maxPaddingWords < paddingWords : \n                _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            else : \n                _endOfThisFrame = wordI * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI \n        if ( not isFirstInFrame and trimPaddingWordsOnEnd and 1 < wordI - lastWordI ) : \n            _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI - 1 \n        if isFirstInFrame : \n            partsPending = True \n            isFirstInFrame = False \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnStart and maxPaddingWords < paddingWords : \n                startOfThisFrame += paddingWords * wordWidth \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n        parts . extend ( word ) \n        lastWordI = wordI \n    endOfThisFrame = transaction . bitAddrEnd \n    withPadding = not ( trimPaddingWordsOnEnd or trimPaddingWordsOnStart ) \n    if partsPending or ( withPadding and endOfThisFrame != startOfThisFrame ) : \n        endOfLastWord = ( lastWordI + 1 ) * wordWidth \n        if endOfLastWord > endOfThisFrame : \n            endOfThisFrame = endOfLastWord \n        else : \n            paddingWords = it . fullWordCnt ( endOfLastWord , endOfThisFrame ) \n            if trimPaddingWordsOnEnd and maxPaddingWords < paddingWords : \n                endOfThisFrame -= paddingWords * wordWidth \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , endOfThisFrame ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , parts ) \n        parts = [ ] \n        startOfThisFrame = endOfThisFrame \n    while withPadding and transaction . bitAddrEnd > startOfThisFrame : \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , transaction . bitAddrEnd ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , [ ] ) \n        startOfThisFrame = endOfThisFrame "}
{"6423": "\ndef walkWords ( self , showPadding : bool = False ) : \n    wIndex = 0 \n    lastEnd = self . startBitAddr \n    parts = [ ] \n    for p in self . parts : \n        end = p . startOfPart \n        if showPadding and end != lastEnd : \n            while end != lastEnd : \n                assert lastEnd <= end , ( end , lastEnd ) \n                endOfWord = ceil ( ( lastEnd + 1 ) / self . wordWidth ) * self . wordWidth \n                endOfPadding = min ( endOfWord , end ) \n                _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n                parts . append ( _p ) \n                if endOfWord <= endOfPadding : \n                    yield ( wIndex , parts ) \n                    wIndex += 1 \n                    parts = [ ] \n                lastEnd = endOfPadding \n        if self . _wordIndx ( lastEnd ) != self . _wordIndx ( p . startOfPart ) : \n            yield ( wIndex , parts ) \n            wIndex += 1 \n            parts = [ ] \n            lastEnd = p . endOfPart \n        parts . append ( p ) \n        lastEnd = p . endOfPart \n        if lastEnd % self . wordWidth == 0 : \n            yield ( wIndex , parts ) \n            wIndex += 1 \n            parts = [ ] \n    if showPadding and ( parts or lastEnd != self . endBitAddr or lastEnd % self . wordWidth != 0 ) : \n        end = ceil ( self . endBitAddr / self . wordWidth ) * self . wordWidth \n        while end != lastEnd : \n            assert lastEnd <= end , ( end , lastEnd ) \n            endOfWord = ( ( lastEnd // self . wordWidth ) + 1 ) * self . wordWidth \n            endOfPadding = min ( endOfWord , end ) \n            _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n            _p . parent = self \n            parts . append ( _p ) \n            if endOfWord <= endOfPadding : \n                yield ( wIndex , parts ) \n                wIndex += 1 \n                parts = [ ] \n            lastEnd = endOfPadding \n        if parts : \n            yield ( wIndex , parts ) "}
{"6443": "\ndef HStruct_unpack ( structT , data , getDataFn = None , dataWidth = None ) : \n    if getDataFn is None : \n        assert dataWidth is not None \n        def _getDataFn ( x ) : \n            return toHVal ( x ) . _auto_cast ( Bits ( dataWidth ) ) \n        getDataFn = _getDataFn \n    val = structT . fromPy ( None ) \n    fData = iter ( data ) \n    actualOffset = 0 \n    actual = None \n    for v in walkFlattenFields ( val , skipPadding = False ) : \n        required = v . _dtype . bit_length ( ) \n        if actual is None : \n            actualOffset = 0 \n            try : \n                actual = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            if dataWidth is None : \n                dataWidth = actual . _dtype . bit_length ( ) \n            actuallyHave = dataWidth \n        else : \n            actuallyHave = actual . _dtype . bit_length ( ) - actualOffset \n        while required > actuallyHave : \n            try : \n                d = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            actual = d . _concat ( actual ) \n            actuallyHave += dataWidth \n        if required <= actuallyHave : \n            _v = actual [ ( required + actualOffset ) : actualOffset ] \n            _v = _v . _auto_cast ( v . _dtype ) \n            v . val = _v . val \n            v . vldMask = _v . vldMask \n            v . updateTime = _v . updateTime \n            actuallyHave -= required \n            actualOffset += required \n        if actuallyHave == 0 : \n            actual = None \n    if actual is not None : \n        assert dataWidth > actual . _dtype . bit_length ( ) - actualOffset , \"It should be just a padding at the end of frame\" \n    return val "}
{"6449": "\ndef mkArrayUpdater ( nextItemVal : Value , indexes : Tuple [ Value ] , invalidate : bool ) : \n    def updater ( currentVal ) : \n        if 1 < len ( indexes ) : \n            raise NotImplementedError ( \"[TODO] implement for more indexes\" ) \n        _nextItemVal = nextItemVal . clone ( ) \n        if invalidate : \n            _nextItemVal . vldMask = 0 \n        index = indexes [ 0 ] \n        change = valueHasChanged ( currentVal . _getitem__val ( index ) , _nextItemVal ) \n        currentVal . _setitem__val ( index , _nextItemVal ) \n        return ( change , currentVal ) \n    return updater "}
{"6451": "\ndef HWProcess ( cls , proc : HWProcess , ctx : ResourceContext ) -> None : \n    seen = ctx . seen \n    for stm in proc . statements : \n        encl = stm . _enclosed_for \n        full_ev_dep = stm . _is_completly_event_dependent \n        now_ev_dep = stm . _now_is_event_dependent \n        ev_dep = full_ev_dep or now_ev_dep \n        out_mux_dim = count_mux_inputs_for_outputs ( stm ) \n        for o in stm . _outputs : \n            if o in seen : \n                continue \n            i = out_mux_dim [ o ] \n            if isinstance ( o . _dtype , HArray ) : \n                assert i == 1 , ( o , i , \" only one ram port per HWProcess\" ) \n                for a in walk_assignments ( stm , o ) : \n                    assert len ( a . indexes ) == 1 , \"one address per RAM port\" \n                    addr = a . indexes [ 0 ] \n                ctx . registerRAM_write_port ( o , addr , ev_dep ) \n            elif ev_dep : \n                ctx . registerFF ( o ) \n                if 1 < i : \n                    ctx . registerMUX ( stm , o , i ) \n            elif o not in encl : \n                ctx . registerLatch ( o ) \n                if 1 < i : \n                    ctx . registerMUX ( stm , o , i ) \n            elif 1 < i : \n                ctx . registerMUX ( stm , o , i ) \n            else : \n                continue \n        if isinstance ( stm , SwitchContainer ) : \n            caseEqs = set ( [ stm . switchOn . _eq ( c [ 0 ] ) for c in stm . cases ] ) \n            inputs = chain ( [ sig for sig in stm . _inputs if sig not in caseEqs ] , [ stm . switchOn ] ) \n        else : \n            inputs = stm . _inputs \n        for i in inputs : \n            if not i . hidden or i in seen : \n                continue \n            cls . HWProcess_operators ( i , ctx , ev_dep ) "}
{"6481": "\ndef signFix ( val , width ) : \n    if 0 < val : \n        msb = 1 << ( width - 1 ) \n        if val & msb : \n            val -= mask ( width ) + 1 \n    return val "}
{"6532": "\ndef average_hash ( image_path , hash_size = 8 ) : \n    with open ( image_path , 'rb' ) as f : \n        image = Image . open ( f ) . resize ( ( hash_size , hash_size ) , Image . ANTIALIAS ) . convert ( 'L' ) \n        pixels = list ( image . getdata ( ) ) \n    avg = sum ( pixels ) / len ( pixels ) \n    bits = \"\" . join ( map ( lambda pixel : '1' if avg < pixel else '0' , pixels ) ) \n    hashformat = \"0{hashlength}x\" . format ( hashlength = hash_size ** 2 // 4 ) \n    return int ( bits , 2 ) . __format__ ( hashformat ) "}
{"6539": "\ndef set_volume_level ( self , volume ) : \n    if self . _volume_level is not None : \n        if self . _volume_level < volume : \n            num = int ( self . _max_volume * ( volume - self . _volume_level ) ) \n            self . _volume_level = volume \n            self . _device . vol_up ( num = num ) \n        elif self . _volume_level > volume : \n            num = int ( self . _max_volume * ( self . _volume_level - volume ) ) \n            self . _volume_level = volume \n            self . _device . vol_down ( num = num ) "}
{"6548": "\ndef is_fourfold_repetition ( self ) : \n    zobrist_hash = self . zobrist_hash ( ) \n    if 4 > self . transpositions [ zobrist_hash ] : \n        return False \n    return True "}
{"6550": "\ndef sfen ( self ) : \n    sfen = [ ] \n    empty = 0 \n    for square in SQUARES : \n        piece = self . piece_at ( square ) \n        if not piece : \n            empty += 1 \n        else : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            sfen . append ( piece . symbol ( ) ) \n        if BB_SQUARES [ square ] & BB_FILE_1 : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            if square != I1 : \n                sfen . append ( '/' ) \n    sfen . append ( ' ' ) \n    if self . turn == WHITE : \n        sfen . append ( 'w' ) \n    else : \n        sfen . append ( 'b' ) \n    sfen . append ( ' ' ) \n    pih_len = 0 \n    for color in COLORS : \n        p = self . pieces_in_hand [ color ] \n        pih_len += len ( p ) \n        for piece_type in sorted ( p . keys ( ) , reverse = True ) : \n            if 1 <= p [ piece_type ] : \n                if 1 < p [ piece_type ] : \n                    sfen . append ( str ( p [ piece_type ] ) ) \n                piece = Piece ( piece_type , color ) \n                sfen . append ( piece . symbol ( ) ) \n    if pih_len == 0 : \n        sfen . append ( '-' ) \n    sfen . append ( ' ' ) \n    sfen . append ( str ( self . move_number ) ) \n    return '' . join ( sfen ) "}
{"6571": "\ndef _get_rate ( self , currency , date ) : \n    if currency == self . ref_currency : \n        return 1.0 \n    if date not in self . _rates [ currency ] : \n        first_date , last_date = self . bounds [ currency ] \n        if not self . fallback_on_wrong_date : \n            raise RateNotFoundError ( '{0} not in {1} bounds {2}/{3}' . format ( date , currency , first_date , last_date ) ) \n        if first_date > date : \n            fallback_date = first_date \n        elif last_date < date : \n            fallback_date = last_date \n        else : \n            raise AssertionError ( 'Should never happen, bug in the code!' ) \n        if self . verbose : \n            print ( r'/!\\ {0} not in {1} bounds {2}/{3}, falling back to {4}' . format ( date , currency , first_date , last_date , fallback_date ) ) \n        date = fallback_date \n    rate = self . _rates [ currency ] [ date ] \n    if rate is None : \n        raise RateNotFoundError ( '{0} has no rate for {1}' . format ( currency , date ) ) \n    return rate "}
{"6577": "\ndef map_words ( self , start , end ) : \n    i , j = 8 * start - 8 , 8 * end \n    try : \n        fileno = self . file . fileno ( ) \n    except ( AttributeError , io . UnsupportedOperation ) : \n        fileno = None \n    if fileno is None : \n        skip = 0 \n        self . file . seek ( i ) \n        m = self . file . read ( j - i ) \n    else : \n        skip = i % mmap . ALLOCATIONGRANULARITY \n        r = mmap . ACCESS_READ \n        m = mmap . mmap ( fileno , length = j - i + skip , access = r , offset = i - skip ) \n    if ( 3 , ) < sys . version_info : \n        m = memoryview ( m ) \n    return m , skip "}
{"6579": "\ndef add_array ( self , name , values , array ) : \n    f = self . file \n    scs = self . summary_control_struct \n    record_number = self . bward \n    data = bytearray ( self . read_record ( record_number ) ) \n    next_record , previous_record , n_summaries = scs . unpack ( data [ : 24 ] ) \n    if self . summaries_per_record > n_summaries : \n        summary_record = record_number \n        name_record = summary_record + 1 \n        data [ : 24 ] = scs . pack ( next_record , previous_record , n_summaries + 1 ) \n        self . write_record ( summary_record , data ) \n    else : \n        summary_record = ( ( self . free - 1 ) * 8 + 1023 ) // 1024 + 1 \n        name_record = summary_record + 1 \n        free_record = summary_record + 2 \n        n_summaries = 0 \n        data [ : 24 ] = scs . pack ( summary_record , previous_record , n_summaries ) \n        self . write_record ( record_number , data ) \n        summaries = scs . pack ( 0 , record_number , 1 ) . ljust ( 1024 , b'\\0' ) \n        names = b'\\0' * 1024 \n        self . write_record ( summary_record , summaries ) \n        self . write_record ( name_record , names ) \n        self . bward = summary_record \n        self . free = ( free_record - 1 ) * 1024 // 8 + 1 \n    start_word = self . free \n    f . seek ( ( start_word - 1 ) * 8 ) \n    array = numpy_array ( array ) \n    f . write ( array . view ( ) ) \n    end_word = f . tell ( ) // 8 \n    self . free = end_word + 1 \n    self . write_file_record ( ) \n    values = values [ : self . nd + self . ni - 2 ] + ( start_word , end_word ) \n    base = 1024 * ( summary_record - 1 ) \n    offset = int ( n_summaries ) * self . summary_step \n    f . seek ( base + scs . size + offset ) \n    f . write ( self . summary_struct . pack ( * values ) ) \n    f . seek ( base + 1024 + offset ) \n    f . write ( name [ : self . summary_length ] . ljust ( self . summary_step , b' ' ) ) "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = True ) : \n    scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( 0 > index ) . any ( ) or ( n < index ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( index == n ) \n    index [ omegas ] -= 1 \n    offset [ omegas ] += interval_length \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ 0 ] = 1.0 \n    T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    for i in range ( 2 , coefficient_count ) : \n        T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2 ] \n    components = ( T . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        components = components [ : , 0 ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ 0 ] = 0.0 \n    dT [ 1 ] = 1.0 \n    if 2 < coefficient_count : \n        dT [ 2 ] = twot1 + twot1 \n        for i in range ( 3 , coefficient_count ) : \n            dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2 ] + T [ i - 1 ] + T [ i - 1 ] \n    dT *= 2.0 \n    dT /= interval_length \n    rates = ( dT . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        rates = rates [ : , 0 ] \n    return components , rates "}
{"6585": "\ndef visit_Call ( self , node ) : \n    if self . within_logging_statement ( ) : \n        if self . within_logging_argument ( ) and self . is_format_call ( node ) : \n            self . violations . append ( ( node , STRING_FORMAT_VIOLATION ) ) \n            super ( LoggingVisitor , self ) . generic_visit ( node ) \n            return \n    logging_level = self . detect_logging_level ( node ) \n    if logging_level and self . current_logging_level is None : \n        self . current_logging_level = logging_level \n    if logging_level is None : \n        super ( LoggingVisitor , self ) . generic_visit ( node ) \n        return \n    self . current_logging_call = node \n    if logging_level == \"warn\" : \n        self . violations . append ( ( node , WARN_VIOLATION ) ) \n    self . check_exc_info ( node ) \n    for index , child in enumerate ( iter_child_nodes ( node ) ) : \n        if index == 1 : \n            self . current_logging_argument = child \n        if 1 <= index : \n            self . check_exception_arg ( child ) \n        if 1 < index and isinstance ( child , keyword ) and child . arg == \"extra\" : \n            self . current_extra_keyword = child \n        super ( LoggingVisitor , self ) . visit ( child ) \n        self . current_logging_argument = None \n        self . current_extra_keyword = None \n    self . current_logging_call = None \n    self . current_logging_level = None "}
{"6588": "\ndef visit_JoinedStr ( self , node ) : \n    if ( 3 , 6 ) <= version_info : \n        if self . within_logging_statement ( ) : \n            if any ( isinstance ( i , FormattedValue ) for i in node . values ) : \n                if self . within_logging_argument ( ) : \n                    self . violations . append ( ( node , FSTRING_VIOLATION ) ) \n                    super ( LoggingVisitor , self ) . generic_visit ( node ) "}
{"6592": "\ndef get_except_handler_name ( self , node ) : \n    name = node . name \n    if not name : \n        return None \n    if ( 3 , ) > version_info : \n        return name . id \n    return name "}
{"6631": "\ndef parse ( self , symbol : str ) -> ( str , str ) : \n    symbol_parts = symbol . split ( \":\" ) \n    namespace = None \n    mnemonic = symbol \n    if 1 < len ( symbol_parts ) : \n        namespace = symbol_parts [ 0 ] \n        mnemonic = symbol_parts [ 1 ] \n    self . namespace = namespace \n    self . mnemonic = mnemonic \n    return namespace , mnemonic "}
{"6653": "\ndef has_chosen ( state , correct , msgs ) : \n    if not issubclass ( type ( correct ) , int ) : \n        raise InstructorError ( \"Inside `has_chosen()`, the argument `correct` should be an integer.\" ) \n    student_process = state . student_process \n    if not isDefinedInProcess ( MC_VAR_NAME , student_process ) : \n        raise InstructorError ( \"Option not available in the student process\" ) \n    else : \n        selected_option = getOptionFromProcess ( student_process , MC_VAR_NAME ) \n        if not issubclass ( type ( selected_option ) , int ) : \n            raise InstructorError ( \"selected_option should be an integer\" ) \n        if 1 > selected_option or 1 > correct : \n            raise InstructorError ( \"selected_option and correct should be greater than zero\" ) \n        if len ( msgs ) < selected_option or len ( msgs ) < correct : \n            raise InstructorError ( \"there are not enough feedback messages defined\" ) \n        feedback_msg = msgs [ selected_option - 1 ] \n        state . reporter . success_msg = msgs [ correct - 1 ] \n        state . do_test ( EqualTest ( selected_option , correct , feedback_msg ) ) "}
{"6675": "\ndef authenticate_search_bind ( self , username , password ) : \n    connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) , ) \n    try : \n        connection . bind ( ) \n        log . debug ( \"Successfully bound to LDAP as '{0}' for search_bind method\" . format ( self . config . get ( 'LDAP_BIND_USER_DN' ) or 'Anonymous' ) ) \n    except Exception as e : \n        self . destroy_connection ( connection ) \n        log . error ( e ) \n        return AuthenticationResponse ( ) \n    user_filter = '({search_attr}={username})' . format ( search_attr = self . config . get ( 'LDAP_USER_LOGIN_ATTR' ) , username = username ) \n    search_filter = '(&{0}{1})' . format ( self . config . get ( 'LDAP_USER_OBJECT_FILTER' ) , user_filter , ) \n    log . debug ( \"Performing an LDAP Search using filter '{0}', base '{1}', \" \"and scope '{2}'\" . format ( search_filter , self . full_user_search_dn , self . config . get ( 'LDAP_USER_SEARCH_SCOPE' ) ) ) \n    connection . search ( search_base = self . full_user_search_dn , search_filter = search_filter , search_scope = getattr ( ldap3 , self . config . get ( 'LDAP_USER_SEARCH_SCOPE' ) ) , attributes = self . config . get ( 'LDAP_GET_USER_ATTRIBUTES' ) ) \n    response = AuthenticationResponse ( ) \n    if len ( connection . response ) == 0 or ( self . config . get ( 'LDAP_FAIL_AUTH_ON_MULTIPLE_FOUND' ) and 1 < len ( connection . response ) ) : \n        log . debug ( \"Authentication was not successful for user '{0}'\" . format ( username ) ) \n    else : \n        for user in connection . response : \n            if 'type' not in user or user . get ( 'type' ) != 'searchResEntry' : \n                continue \n            user_connection = self . _make_connection ( bind_user = user [ 'dn' ] , bind_password = password ) \n            log . debug ( \"Directly binding a connection to a server with \" \"user:'{0}'\" . format ( user [ 'dn' ] ) ) \n            try : \n                user_connection . bind ( ) \n                log . debug ( \"Authentication was successful for user '{0}'\" . format ( username ) ) \n                response . status = AuthenticationResponseStatus . success \n                user [ 'attributes' ] [ 'dn' ] = user [ 'dn' ] \n                response . user_info = user [ 'attributes' ] \n                response . user_id = username \n                response . user_dn = user [ 'dn' ] \n                if self . config . get ( 'LDAP_SEARCH_FOR_GROUPS' ) : \n                    response . user_groups = self . get_user_groups ( dn = user [ 'dn' ] , _connection = connection ) \n                self . destroy_connection ( user_connection ) \n                break \n            except ldap3 . core . exceptions . LDAPInvalidCredentialsResult : \n                log . debug ( \"Authentication was not successful for \" \"user '{0}'\" . format ( username ) ) \n                response . status = AuthenticationResponseStatus . fail \n            except Exception as e : \n                log . error ( e ) \n                response . status = AuthenticationResponseStatus . fail \n            self . destroy_connection ( user_connection ) \n    self . destroy_connection ( connection ) \n    return response "}
{"6679": "\ndef get_object ( self , dn , filter , attributes , _connection = None ) : \n    connection = _connection \n    if not connection : \n        connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) ) \n        connection . bind ( ) \n    connection . search ( search_base = dn , search_filter = filter , attributes = attributes , ) \n    data = None \n    if 0 < len ( connection . response ) : \n        data = connection . response [ 0 ] [ 'attributes' ] \n        data [ 'dn' ] = connection . response [ 0 ] [ 'dn' ] \n    if not _connection : \n        self . destroy_connection ( connection ) \n    return data "}
{"6702": "\ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    print ( results ) \n    if name is not None : \n        for result in results : \n            matches = False \n            if name in result . name : \n                matches = True \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = True \n            if matches is True : \n                content = self . _print_log ( result . name ) \n    else : \n        if 0 < len ( results ) : \n            latest = results [ 0 ] \n            for result in results : \n                if latest . time_created <= result . time_created : \n                    latest = result \n            content = self . _print_log ( result . name ) \n    return content "}
{"6713": "\ndef basic_auth_header ( username , password ) : \n    s = \"%s:%s\" % ( username , password ) \n    if 3 <= sys . version_info [ 0 ] : \n        s = bytes ( s , 'utf-8' ) \n        credentials = base64 . b64encode ( s ) . decode ( 'utf-8' ) \n    else : \n        credentials = base64 . b64encode ( s ) \n    auth = { \"Authorization\" : \"Basic %s\" % credentials } \n    return auth "}
{"6720": "\ndef remove ( self , image , force = False ) : \n    q = parse_image_name ( remove_uri ( image ) ) \n    if q [ 'registry' ] == None : \n        q [ 'registry' ] = self . base \n    q = self . _add_https ( q ) \n    url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"delete\" , names = q ) \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    self . _update_headers ( fields = headers ) \n    continue_delete = True \n    if force is False : \n        response = input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) \n        while 1 > len ( response ) or response [ 0 ] . lower ( ) . strip ( ) not in \"ynyesno\" : \n            response = input ( \"Please answer yes or no: \" ) \n        if response [ 0 ] . lower ( ) . strip ( ) in \"no\" : \n            continue_delete = False \n    if continue_delete is True : \n        response = self . _delete ( url ) \n        message = self . _read_response ( response ) \n        bot . info ( \"Response %s, %s\" % ( response . status_code , message ) ) \n    else : \n        bot . info ( \"Delete cancelled.\" ) "}
{"6731": "\ndef kill ( args ) : \n    from sregistry . main import Client as cli \n    if 0 < len ( args . commands ) : \n        for name in args . commands : \n            cli . destroy ( name ) \n    sys . exit ( 0 ) "}
{"6732": "\ndef list_logs ( args , container_name = None ) : \n    from sregistry . main import Client as cli \n    if 0 < len ( args . commands ) : \n        container_name = args . commands . pop ( 0 ) \n    cli . logs ( container_name ) \n    sys . exit ( 0 ) "}
{"6750": "\ndef load_templates ( self , name ) : \n    configs = self . _get_templates ( ) \n    templates = [ ] \n    matches = [ x for x in configs [ 'data' ] if name in x [ 'name' ] ] \n    if 0 < len ( matches ) : \n        for match in matches : \n            response = self . _get ( match [ 'id' ] ) \n            templates . append ( response ) \n        return templates \n    bot . info ( 'No matches found for %s' % name ) "}
{"6761": "\ndef list_endpoint ( self , endpoint , query = None ) : \n    if not hasattr ( self , 'transfer_client' ) : \n        self . _init_transfer_client ( ) \n    endpoint , path = self . _parse_endpoint_name ( endpoint ) \n    try : \n        result = self . transfer_client . operation_ls ( endpoint , path = path ) \n    except TransferAPIError as err : \n        bot . custom ( prefix = 'ERROR' , message = err , color = 'RED' ) \n        sys . exit ( 1 ) \n    rows = [ ] \n    for filey in result : \n        name = filey [ 'name' ] \n        if query is None or query in name : \n            if name . endswith ( 'img' ) : \n                name = bot . addColor ( 'PURPLE' , name ) \n            rows . append ( [ filey [ 'type' ] , filey [ 'permissions' ] , str ( filey [ 'size' ] ) , name ] ) \n    if 0 < len ( rows ) : \n        rows = [ [ \"type\" , \"[perm]\" , \"[size]\" , \"[name]\" ] ] + rows \n        bot . custom ( prefix = \"Endpoint Listing %s\" % path , message = '' , color = \"CYAN\" ) \n        bot . table ( rows ) \n    else : \n        bot . info ( 'No content was found at the selected endpoint.' ) \n    return rows "}
{"6781": "\ndef images ( self , query = None ) : \n    from sregistry . database . models import Collection , Container \n    rows = [ ] \n    if query is not None : \n        like = \"%\" + query + \"%\" \n        containers = Container . query . filter ( or_ ( Container . name == query , Container . tag . like ( like ) , Container . uri . like ( like ) , Container . name . like ( like ) ) ) . all ( ) \n    else : \n        containers = Container . query . all ( ) \n    if 0 < len ( containers ) : \n        message = \"  [date]   [client]\\t[uri]\" \n        bot . custom ( prefix = 'Containers:' , message = message , color = \"RED\" ) \n        for c in containers : \n            uri = c . get_uri ( ) \n            created_at = c . created_at . strftime ( '%B %d, %Y' ) \n            rows . append ( [ created_at , \"   [%s]\" % c . client , uri ] ) \n        bot . table ( rows ) \n    return containers "}
{"6788": "\ndef parse_header ( recipe , header = \"from\" , remove_header = True ) : \n    parsed_header = None \n    fromline = [ x for x in recipe . split ( '\\n' ) if \"%s:\" % header in x . lower ( ) ] \n    if len ( fromline ) == 0 : \n        return \"\" \n    if 0 < len ( fromline ) : \n        fromline = fromline [ 0 ] \n        parsed_header = fromline . strip ( ) \n    if remove_header is True : \n        parsed_header = fromline . split ( ':' , 1 ) [ - 1 ] . strip ( ) \n    return parsed_header "}
{"6789": "\ndef find_single_recipe ( filename , pattern = \"Singularity\" , manifest = None ) : \n    if pattern is None : \n        pattern = \"Singularity*\" \n    recipe = None \n    file_basename = os . path . basename ( filename ) \n    if fnmatch . fnmatch ( file_basename , pattern ) : \n        recipe = { 'path' : os . path . abspath ( filename ) , 'modified' : os . path . getmtime ( filename ) } \n    if manifest is not None and recipe is not None : \n        container_uri = '/' . join ( filename . split ( '/' ) [ - 2 : ] ) \n        if container_uri in manifest : \n            if os . path . getmtime ( filename ) > manifest [ container_uri ] [ 'modified' ] : \n                manifest [ container_uri ] = recipe \n        else : \n            manifest [ container_uri ] = recipe \n        return manifest \n    return recipe "}
{"6815": "\ndef _apply_transform ( self , mol , rule ) : \n    mols = [ mol ] \n    for n in six . moves . range ( 20 ) : \n        products = { } \n        for mol in mols : \n            for product in [ x [ 0 ] for x in rule . RunReactants ( ( mol , ) ) ] : \n                if Chem . SanitizeMol ( product , catchErrors = True ) == 0 : \n                    products [ Chem . MolToSmiles ( product , isomericSmiles = True ) ] = product \n        if products : \n            mols = [ products [ s ] for s in sorted ( products ) ] \n        else : \n            return mols [ 0 ] if 0 < n else None "}
{"6816": "\ndef canonicalize ( self , mol ) : \n    tautomers = self . _enumerate_tautomers ( mol ) \n    if len ( tautomers ) == 1 : \n        return tautomers [ 0 ] \n    highest = None \n    for t in tautomers : \n        smiles = Chem . MolToSmiles ( t , isomericSmiles = True ) \n        log . debug ( 'Tautomer: %s' , smiles ) \n        score = 0 \n        ssr = Chem . GetSymmSSSR ( t ) \n        for ring in ssr : \n            btypes = { t . GetBondBetweenAtoms ( * pair ) . GetBondType ( ) for pair in pairwise ( ring ) } \n            elements = { t . GetAtomWithIdx ( idx ) . GetAtomicNum ( ) for idx in ring } \n            if btypes == { BondType . AROMATIC } : \n                log . debug ( 'Score +100 (aromatic ring)' ) \n                score += 100 \n                if elements == { 6 } : \n                    log . debug ( 'Score +150 (carbocyclic aromatic ring)' ) \n                    score += 150 \n        for tscore in self . scores : \n            for match in t . GetSubstructMatches ( tscore . smarts ) : \n                log . debug ( 'Score %+d (%s)' , tscore . score , tscore . name ) \n                score += tscore . score \n        for atom in t . GetAtoms ( ) : \n            if atom . GetAtomicNum ( ) in { 15 , 16 , 34 , 52 } : \n                hs = atom . GetTotalNumHs ( ) \n                if hs : \n                    log . debug ( 'Score %+d (%s-H bonds)' , - hs , atom . GetSymbol ( ) ) \n                    score -= hs \n        if not highest or score > highest [ 'score' ] or ( highest [ 'score' ] == score and highest [ 'smiles' ] > smiles ) : \n            log . debug ( 'New highest tautomer: %s (%s)' , smiles , score ) \n            highest = { 'smiles' : smiles , 'tautomer' : t , 'score' : score } \n    return highest [ 'tautomer' ] "}
{"6830": "\ndef remove ( self , mol ) : \n    log . debug ( 'Running FragmentRemover' ) \n    for frag in self . fragments : \n        if mol . GetNumAtoms ( ) == 0 or ( self . leave_last and 1 >= len ( Chem . GetMolFrags ( mol ) ) ) : \n            break \n        removed = Chem . DeleteSubstructs ( mol , frag . smarts , onlyFrags = True ) \n        if not mol . GetNumAtoms ( ) == removed . GetNumAtoms ( ) : \n            log . info ( 'Removed fragment: %s' , frag . name ) \n        if self . leave_last and removed . GetNumAtoms ( ) == 0 : \n            break \n        mol = removed \n    return mol "}
{"6831": "\ndef choose ( self , mol ) : \n    log . debug ( 'Running LargestFragmentChooser' ) \n    fragments = Chem . GetMolFrags ( mol , asMols = True ) \n    largest = None \n    for f in fragments : \n        smiles = Chem . MolToSmiles ( f , isomericSmiles = True ) \n        log . debug ( 'Fragment: %s' , smiles ) \n        organic = is_organic ( f ) \n        if self . prefer_organic : \n            if largest and largest [ 'organic' ] and not organic : \n                continue \n            if largest and organic and not largest [ 'organic' ] : \n                largest = None \n        atoms = 0 \n        for a in f . GetAtoms ( ) : \n            atoms += 1 + a . GetTotalNumHs ( ) \n        if largest and largest [ 'atoms' ] > atoms : \n            continue \n        weight = rdMolDescriptors . CalcExactMolWt ( f ) \n        if largest and atoms == largest [ 'atoms' ] and largest [ 'weight' ] > weight : \n            continue \n        if largest and atoms == largest [ 'atoms' ] and weight == largest [ 'weight' ] and largest [ 'smiles' ] < smiles : \n            continue \n        log . debug ( 'New largest fragment: %s (%s)' , smiles , atoms ) \n        largest = { 'smiles' : smiles , 'fragment' : f , 'atoms' : atoms , 'weight' : weight , 'organic' : organic } \n    return largest [ 'fragment' ] "}
{"6832": "\ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : \n    f , j = get_f_and_j ( mu ) \n    if 1 < nt : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    if verbose : \n        print ( nfo ) \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , 1 ] , 'g--' ) \n        plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) \n        if savefig == 'None' : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6839": "\ndef get_issues ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_issues = repo . iter_issues ( state = 'all' ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if 2 < len ( files ) : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_issues = repo . iter_issues ( state = 'all' ) \n                is_only_today = True \n        if not is_only_today : \n            all_issues = repo . iter_issues ( since = date , state = 'all' ) \n    for issue in all_issues : \n        self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) \n    closed_issues = 0 \n    for issue in repo . iter_issues ( state = 'closed' ) : \n        if issue is not None : \n            closed_issues += 1 \n    return closed_issues "}
{"6842": "\ndef get_commits ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_commits = repo . iter_commits ( ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if 2 < len ( files ) : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_commits = repo . iter_commits ( ) \n                is_only_today = True \n        if not is_only_today : \n            all_commits = repo . iter_commits ( since = date ) \n    for commit in all_commits : \n        self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) \n    count = 0 \n    for commit in repo . iter_commits ( ) : \n        count += 1 \n    return count "}
{"6850": "\ndef _check_api_limits ( gh_session , api_required = 250 , sleep_time = 15 ) : \n    api_rates = gh_session . rate_limit ( ) \n    api_remaining = api_rates [ 'rate' ] [ 'remaining' ] \n    api_reset = api_rates [ 'rate' ] [ 'reset' ] \n    logger . debug ( 'Rate Limit - %d requests remaining' , api_remaining ) \n    if api_required < api_remaining : \n        return \n    now_time = time . time ( ) \n    time_to_reset = int ( api_reset - now_time ) \n    logger . warn ( 'Rate Limit Depleted - Sleeping for %d seconds' , time_to_reset ) \n    while api_reset > now_time : \n        time . sleep ( 10 ) \n        now_time = time . time ( ) \n    return "}
{"6855": "\ndef from_gitlab ( klass , repository , labor_hours = True ) : \n    if not isinstance ( repository , gitlab . v4 . objects . Project ) : \n        raise TypeError ( 'Repository must be a gitlab Repository object' ) \n    project = klass ( ) \n    logger . debug ( 'GitLab: repository_id=%d path_with_namespace=%s' , repository . id , repository . path_with_namespace , ) \n    project [ 'name' ] = repository . name \n    project [ 'repositoryURL' ] = repository . http_url_to_repo \n    project [ 'description' ] = repository . description \n    project [ 'permissions' ] [ 'licenses' ] = None \n    web_url = repository . web_url \n    public_server = web_url . startswith ( 'https://gitlab.com' ) \n    if repository . visibility in ( 'public' ) and public_server : \n        project [ 'permissions' ] [ 'usageType' ] = 'openSource' \n    elif POLICY_START_DATE > date_parse ( repository . created_at ) : \n        project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' \n    if labor_hours : \n        project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) \n    else : \n        project [ 'laborHours' ] = 0 \n    project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list \n    project [ 'contact' ] = { 'email' : '' , 'URL' : web_url , } \n    project [ 'organization' ] = repository . namespace [ 'name' ] \n    project [ 'status' ] = 'Development' \n    project [ 'vcs' ] = 'git' \n    project [ 'homepageURL' ] = repository . web_url \n    api_url = repository . manager . gitlab . _url \n    archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) \n    project [ 'downloadURL' ] = api_url + archive_suffix \n    project [ 'date' ] = { 'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 'metadataLastUpdated' : '' , } \n    _prune_dict_null_str ( project ) \n    return project "}
{"6880": "\ndef queryGitHubFromFile ( self , filePath , gitvars = { } , verbosity = 0 , ** kwargs ) : \n    gitquery = self . _readGQL ( filePath , verbose = ( 0 <= verbosity ) ) \n    return self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , ** kwargs ) "}
{"6881": "\ndef _submitQuery ( self , gitquery , gitvars = { } , verbose = False , rest = False ) : \n    errOut = DEVNULL if not verbose else None \n    authhead = 'Authorization: bearer ' + self . __githubApiToken \n    bashcurl = 'curl -iH TMPauthhead -X POST -d TMPgitquery https://api.github.com/graphql' if not rest else 'curl -iH TMPauthhead https://api.github.com' + gitquery \n    bashcurl_list = bashcurl . split ( ) \n    bashcurl_list [ 2 ] = authhead \n    if not rest : \n        gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) \n        bashcurl_list [ 6 ] = gitqueryJSON \n    fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) \n    _vPrint ( verbose , \"\\n\" + fullResponse ) \n    fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) \n    heads = fullResponse [ 0 ] . split ( '\\r\\n' ) \n    if 1 < len ( fullResponse ) : \n        result = fullResponse [ 1 ] \n    else : \n        result = \"\" \n    http = heads [ 0 ] . split ( ) \n    statusNum = int ( http [ 1 ] ) \n    headDict = { } \n    headDict [ \"http\" ] = heads [ 0 ] \n    for header in heads [ 1 : ] : \n        h = header . split ( ': ' ) \n        headDict [ h [ 0 ] ] = h [ 1 ] \n    linkDict = None \n    if \"Link\" in headDict : \n        linkProperties = headDict [ \"Link\" ] . split ( ', ' ) \n        propDict = { } \n        for item in linkProperties : \n            divided = re . split ( r'<https://api.github.com|>; rel=\"|\"' , item ) \n            propDict [ divided [ 2 ] ] = divided [ 1 ] \n        linkDict = propDict \n    return { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } "}
{"6883": "\ndef _countdown ( self , waitTime = 0 , printString = \"Waiting %*d seconds...\" , verbose = True ) : \n    if 0 >= waitTime : \n        waitTime = self . __retryDelay \n    for remaining in range ( waitTime , 0 , - 1 ) : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , remaining ) , end = \"\" , flush = True ) \n        time . sleep ( 1 ) \n    if verbose : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , 0 ) ) "}
{"6904": "\ndef rollup ( self ) : \n    now = time . time ( ) \n    if self . next_rollup > now : \n        return \n    self . next_rollup = now + self . flush_interval \n    for key , values in sorted ( self . incr_stats . items ( ) ) : \n        self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush_interval ) \n        self . incr_stats [ key ] = [ ] \n    for key , values in sorted ( self . gauge_stats . items ( ) ) : \n        if values : \n            self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) \n        else : \n            self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) \n        self . gauge_stats [ key ] = [ ] \n    for key , values in sorted ( self . histogram_stats . items ( ) ) : \n        if values : \n            self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) \n        else : \n            self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) \n        self . histogram_stats [ key ] = [ ] "}
{"6911": "\ndef mean ( self ) : \n    if 0 < self . counter . value : \n        return self . sum . value / self . counter . value \n    return 0.0 "}
{"6914": "\ndef mark ( self , value = 1 ) : \n    last = self . last . get_and_set ( value ) \n    if value >= last : \n        value = value - last \n    super ( Derive , self ) . mark ( value ) "}
{"6918": "\ndef _buffered_send_metric ( self , metric_str ) : \n    self . batch_count += 1 \n    self . batch_buffer += metric_str \n    if self . batch_size <= self . batch_count : \n        self . _send ( ) "}
{"6938": "\ndef _setup_watch ( self , alias , path , flags ) : \n    assert alias not in self . descriptors , \"Registering alias %s twice!\" % alias \n    wd = LibC . inotify_add_watch ( self . _fd , path , flags ) \n    if 0 > wd : \n        raise IOError ( \"Error setting up watch on %s with flags %s: wd=%s\" % ( path , flags , wd ) ) \n    self . descriptors [ alias ] = wd \n    self . aliases [ wd ] = alias "}
{"6945": "\ndef is_starved ( self ) : \n    for conn in itervalues ( self . conns ) : \n        if 0 < conn . in_flight and ( conn . last_rdy * 0.85 ) <= conn . in_flight : \n            return True \n    return False "}
{"6946": "\ndef connect_to_nsqd ( self , host , port ) : \n    assert isinstance ( host , string_types ) \n    assert isinstance ( port , int ) \n    conn = AsyncConn ( host , port , ** self . conn_kwargs ) \n    conn . on ( 'identify' , self . _on_connection_identify ) \n    conn . on ( 'identify_response' , self . _on_connection_identify_response ) \n    conn . on ( 'auth' , self . _on_connection_auth ) \n    conn . on ( 'auth_response' , self . _on_connection_auth_response ) \n    conn . on ( 'error' , self . _on_connection_error ) \n    conn . on ( 'close' , self . _on_connection_close ) \n    conn . on ( 'ready' , self . _on_connection_ready ) \n    conn . on ( 'message' , self . _on_message ) \n    conn . on ( 'heartbeat' , self . _on_heartbeat ) \n    conn . on ( 'backoff' , functools . partial ( self . _on_backoff_resume , success = False ) ) \n    conn . on ( 'resume' , functools . partial ( self . _on_backoff_resume , success = True ) ) \n    conn . on ( 'continue' , functools . partial ( self . _on_backoff_resume , success = None ) ) \n    if conn . id in self . conns : \n        return \n    now = time . time ( ) \n    last_connect_attempt = self . connection_attempts . get ( conn . id ) \n    if last_connect_attempt and now - 10 < last_connect_attempt : \n        return \n    self . connection_attempts [ conn . id ] = now \n    logger . info ( '[%s:%s] connecting to nsqd' , conn . id , self . name ) \n    conn . connect ( ) \n    return conn "}
{"6948": "\ndef set_max_in_flight ( self , max_in_flight ) : \n    assert isinstance ( max_in_flight , int ) \n    self . max_in_flight = max_in_flight \n    if max_in_flight == 0 : \n        for conn in itervalues ( self . conns ) : \n            if 0 < conn . rdy : \n                logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) \n                self . _send_rdy ( conn , 0 ) \n        self . total_rdy = 0 \n    else : \n        self . need_rdy_redistributed = True \n        self . _redistribute_rdy_state ( ) "}
{"6956": "\ndef theta ( self , s ) : \n    s = np . where ( - 709 > s , - 709 , s ) \n    return 1 / ( 1 + np . exp ( ( - 1 ) * s ) ) "}
{"6962": "\ndef main ( depth_file , json_dict , cutoff , sample_id ) : \n    logger . debug ( \"Cutoff value: {}. Type: {}\" . format ( cutoff , type ( cutoff ) ) ) \n    try : \n        cutoff_val = float ( cutoff ) \n        if 0.4 > cutoff_val : \n            logger . warning ( \"This cutoff value will generate a high volume of \" \"plot data. Therefore '.report.json' can be too big\" ) \n    except ValueError : \n        logger . error ( \"Cutoff value should be a string such as: '0.6'. \" \"The outputted value: {}. Make sure to provide an \" \"appropriate value for --cov_cutoff\" . format ( cutoff ) ) \n        sys . exit ( 1 ) \n    plasmid_length = json . load ( open ( json_dict ) ) \n    if plasmid_length : \n        logger . info ( \"Loaded dictionary of plasmid lengths\" ) \n    else : \n        logger . error ( \"Something went wrong and plasmid lengths dictionary\" \"could not be loaded. Check if process received this\" \"param successfully.\" ) \n        sys . exit ( 1 ) \n    depth_file_in = open ( depth_file ) \n    logger . info ( \"Reading depth file and creating dictionary to dump.\" ) \n    depth_dic_coverage = depth_file_reader ( depth_file_in ) \n    percentage_bases_covered , dict_cov = generate_jsons ( depth_dic_coverage , plasmid_length , cutoff_val ) \n    if percentage_bases_covered and dict_cov : \n        logger . info ( \"percentage_bases_covered length: {}\" . format ( str ( len ( percentage_bases_covered ) ) ) ) \n        logger . info ( \"dict_cov length: {}\" . format ( str ( len ( dict_cov ) ) ) ) \n    else : \n        logger . error ( \"Both dicts that dump to JSON file or .report.json are \" \"empty.\" ) \n    logger . info ( \"Dumping to {}\" . format ( \"{}_mapping.json\" . format ( depth_file ) ) ) \n    with open ( \"{}_mapping.json\" . format ( depth_file ) , \"w\" ) as output_json : \n        output_json . write ( json . dumps ( percentage_bases_covered ) ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mapping\" , \"table\" : \"plasmids\" , \"patlas_mapping\" : percentage_bases_covered , \"value\" : len ( percentage_bases_covered ) } ] } ] , \"sample\" : sample_id , \"patlas_mapping\" : percentage_bases_covered , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMappingSliding\" : dict_cov } , } ] } \n    logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( json_dic ) / 1024 ) ) \n    logger . info ( \"Writing to .report.json\" ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"6984": "\ndef inner_fork_insanity_checks ( pipeline_string ) : \n    list_of_forks = [ ] \n    left_indexes = [ ] \n    for pos , char in enumerate ( pipeline_string ) : \n        if char == FORK_TOKEN : \n            left_indexes . append ( pos ) \n        elif char == CLOSE_TOKEN and 0 < len ( left_indexes ) : \n            list_of_forks . append ( pipeline_string [ left_indexes [ - 1 ] + 1 : pos ] ) \n            left_indexes = left_indexes [ : - 1 ] \n    list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = True ) \n    for fork in list_of_forks : \n        for subfork in list_of_forks : \n            if subfork in list_of_forks and subfork != fork : \n                fork_simplified = fork . replace ( \"({})\" . format ( subfork ) , \"\" ) \n            else : \n                fork_simplified = fork \n        if not 1 < len ( fork_simplified . split ( LANE_TOKEN ) ) : \n            raise SanityError ( \"One of the forks doesn't have '|' \" \"separator between the processes to fork. This is\" \" the prime suspect: '({})'\" . format ( fork ) ) "}
{"6988": "\ndef get_lanes ( lanes_str ) : \n    logger . debug ( \"Parsing lanes from raw string: {}\" . format ( lanes_str ) ) \n    parsed_lanes = \"\" \n    infork = 0 \n    for i in lanes_str : \n        if i == FORK_TOKEN : \n            infork += 1 \n        if i == CLOSE_TOKEN : \n            infork -= 1 \n        if 0 > infork : \n            break \n        if infork == 0 : \n            if i not in [ FORK_TOKEN , CLOSE_TOKEN ] : \n                parsed_lanes += i \n    return [ x . split ( ) for x in parsed_lanes . split ( LANE_TOKEN ) ] "}
{"7001": "\ndef _assess_resource_warnings ( self , process , vals ) : \n    cpu_warnings = { } \n    mem_warnings = { } \n    for i in vals : \n        try : \n            expected_load = float ( i [ \"cpus\" ] ) * 100 \n            cpu_load = float ( i [ \"%cpu\" ] . replace ( \",\" , \".\" ) . replace ( \"%\" , \"\" ) ) \n            if expected_load * 0.9 > cpu_load > expected_load * 1.10 : \n                cpu_warnings [ i [ \"tag\" ] ] = { \"expected\" : expected_load , \"value\" : cpu_load } \n        except ( ValueError , KeyError ) : \n            pass \n        try : \n            rss = self . _size_coverter ( i [ \"rss\" ] ) \n            mem_allocated = self . _size_coverter ( i [ \"memory\" ] ) \n            if mem_allocated * 1.10 < rss : \n                mem_warnings [ i [ \"tag\" ] ] = { \"expected\" : mem_allocated , \"value\" : rss } \n        except ( ValueError , KeyError ) : \n            pass \n    return cpu_warnings , mem_warnings "}
{"7006": "\ndef _updown ( self , direction ) : \n    if direction == \"up\" and self . top_line != 0 : \n        self . top_line -= 1 \n    elif direction == \"down\" and self . content_lines + 3 >= self . screen . getmaxyx ( ) [ 0 ] + self . top_line : \n        self . top_line += 1 "}
{"7007": "\ndef _rightleft ( self , direction ) : \n    if direction == \"left\" and self . padding != 0 : \n        self . padding -= 1 \n    if direction == \"right\" and self . max_width > self . screen . getmaxyx ( ) [ 1 ] + self . padding : \n        self . padding += 1 "}
{"7013": "\ndef main ( sample_id , assembly , min_size ) : \n    logger . info ( \"Starting script\" ) \n    f_open = open ( assembly , \"rU\" ) \n    entry = ( x [ 1 ] for x in groupby ( f_open , lambda line : line [ 0 ] == \">\" ) ) \n    success = 0 \n    for header in entry : \n        headerStr = header . __next__ ( ) [ 1 : ] . strip ( ) \n        seq = \"\" . join ( s . strip ( ) for s in entry . __next__ ( ) ) \n        if min_size <= len ( seq ) : \n            with open ( sample_id + '_' + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + '.fasta' , \"w\" ) as output_file : \n                output_file . write ( \">\" + sample_id + \"_\" + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + \"\\\\n\" + seq + \"\\\\n\" ) \n                success += 1 \n    f_open . close ( ) \n    logger . info ( \"{} sequences sucessfully splitted.\" . format ( success ) ) "}
{"7019": "\ndef build_upstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 1 ] is not None : \n            if 1 < len ( process_descriptions [ task ] [ 1 ] . split ( \"|\" ) ) : \n                local_forks = process_descriptions [ task ] [ 1 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 1 ] in total_tasks : \n                    task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_upstream ( process_descriptions , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                else : \n                    logger . error ( colored_print ( \"{} not in provided protocols as \" \"input for {}\" . format ( process_descriptions [ task ] [ 1 ] , task ) , \"red_bold\" ) ) \n                    sys . exit ( ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7020": "\ndef build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 2 ] is not None : \n            if 1 < len ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) ) : \n                local_forks = process_descriptions [ task ] [ 2 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 2 ] in total_tasks : \n                    task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7021": "\ndef define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : \n    tasks_array = tasks . split ( ) \n    for task_unsplit in tasks_array : \n        task = task_unsplit . split ( \"=\" ) [ 0 ] \n        if task not in process_descriptions . keys ( ) : \n            logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) \n            sys . exit ( ) \n        else : \n            process_split = task_unsplit . split ( \"=\" ) \n            if 1 < len ( process_split ) : \n                self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] \n        if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            task_pipeline = [ ] \n            if task in process_descriptions : \n                if check_upstream : \n                    task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n                task_pipeline . append ( task ) \n                if check_downstream : \n                    task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n            forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) \n        elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            for fork in forks : \n                if task not in fork : \n                    try : \n                        dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) \n                        fork . insert ( dependent_index , task ) \n                    except ValueError : \n                        continue \n    for i in range ( 0 , len ( forks ) ) : \n        for j in range ( 0 , len ( forks [ i ] ) ) : \n            try : \n                if 1 < len ( forks [ i ] [ j ] . split ( \"|\" ) ) : \n                    forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) \n                    tmp_fork = [ ] \n                    for s in forks [ i ] [ j ] : \n                        if s in total_tasks : \n                            tmp_fork . append ( s ) \n                    forks [ i ] [ j ] = tmp_fork \n            except AttributeError as e : \n                continue \n    return forks "}
{"7036": "\ndef get_summary_stats ( self , output_csv = None ) : \n    contig_size_list = [ ] \n    self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) \n    for contig_id , sequence in self . contigs . items ( ) : \n        logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) \n        contig_len = len ( sequence ) \n        contig_size_list . append ( contig_len ) \n        self . summary_info [ \"total_len\" ] += contig_len \n        self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) \n        self . summary_info [ \"missing_data\" ] += sequence . count ( \"N\" ) \n    logger . debug ( \"Getting average contig size\" ) \n    self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) \n    logger . debug ( \"Getting average GC content\" ) \n    self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) \n    logger . debug ( \"Getting N50\" ) \n    cum_size = 0 \n    for l in sorted ( contig_size_list , reverse = True ) : \n        cum_size += l \n        if self . summary_info [ \"total_len\" ] / 2 <= cum_size : \n            self . summary_info [ \"n50\" ] = l \n            break \n    if output_csv : \n        logger . debug ( \"Writing report to csv\" ) \n        with open ( output_csv , \"w\" ) as fh : \n            summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) \n            fh . write ( summary_line ) "}
{"7064": "\ndef render_pipeline ( self ) : \n    dict_viz = { \"name\" : \"root\" , \"children\" : [ ] } \n    last_of_us = { } \n    f_tree = self . _fork_tree if self . _fork_tree else { 1 : [ 1 ] } \n    for x , ( k , v ) in enumerate ( f_tree . items ( ) ) : \n        for p in self . processes [ 1 : ] : \n            if x == 0 and p . lane not in [ k ] + v : \n                continue \n            if 0 < x and p . lane not in v : \n                continue \n            if not p . parent_lane : \n                lst = dict_viz [ \"children\" ] \n            else : \n                lst = last_of_us [ p . parent_lane ] \n            tooltip = { \"name\" : \"{}_{}\" . format ( p . template , p . pid ) , \"process\" : { \"pid\" : p . pid , \"input\" : p . input_type , \"output\" : p . output_type if p . output_type else \"None\" , \"lane\" : p . lane , } , \"children\" : [ ] } \n            dir_var = \"\" \n            for k2 , v2 in p . directives . items ( ) : \n                dir_var += k2 \n                for d in v2 : \n                    try : \n                        directive = v2 [ d ] . replace ( \"'\" , \"\" ) . replace ( '\"' , '' ) if isinstance ( v2 [ d ] , str ) else v2 [ d ] \n                        dir_var += \"{}: {}\" . format ( d , directive ) \n                    except KeyError : \n                        pass \n            if dir_var : \n                tooltip [ \"process\" ] [ \"directives\" ] = dir_var \n            else : \n                tooltip [ \"process\" ] [ \"directives\" ] = \"N/A\" \n            lst . append ( tooltip ) \n            last_of_us [ p . lane ] = lst [ - 1 ] [ \"children\" ] \n    self . dag_to_file ( dict_viz ) \n    with open ( os . path . join ( dirname ( self . nf_file ) , \".forkTree.json\" ) , \"w\" ) as fh : \n        fh . write ( json . dumps ( self . _fork_tree ) ) \n    return self . _render_config ( \"pipeline_graph.html\" , { \"data\" : dict_viz } ) "}
{"7068": "\ndef fetch_docker_tags ( self ) : \n    dict_of_parsed = { } \n    terminal_width = shutil . get_terminal_size ( ) . columns - 3 \n    center_string = \" Selected container tags \" \n    tags_list = [ [ \"=\" * int ( terminal_width / 4 ) , \"{0}{1}{0}\" . format ( \"=\" * int ( ( ( terminal_width / 2 - len ( center_string ) ) / 2 ) ) , center_string ) , \"{}\\n\" . format ( \"=\" * int ( terminal_width / 4 ) ) ] , [ \"component\" , \"container\" , \"tags\" ] , [ \"=\" * int ( terminal_width / 4 ) , \"=\" * int ( terminal_width / 2 ) , \"=\" * int ( terminal_width / 4 ) ] ] \n    for p in self . processes [ 1 : ] : \n        template = p . template \n        if template in dict_of_parsed : \n            continue \n        dict_of_parsed [ template ] = { \"container\" : [ ] } \n        for directives in p . directives . values ( ) : \n            try : \n                repo = directives [ \"container\" ] \n                default_version = directives [ \"version\" ] \n            except KeyError : \n                repo = \"flowcraft/flowcraft_base\" \n                default_version = \"1.0.0-1\" \n            repo_version = repo + default_version \n            if repo_version not in dict_of_parsed [ template ] [ \"container\" ] : \n                r = requests . get ( \"https://hub.docker.com/v2/repositories/{}/tags/\" . format ( repo ) ) \n                if r . status_code != 404 : \n                    r_content = json . loads ( r . content ) [ \"results\" ] \n                    for version in r_content : \n                        printed_version = ( version [ \"name\" ] + \"*\" ) if version [ \"name\" ] == default_version else version [ \"name\" ] \n                        tags_list . append ( [ template , repo , printed_version ] ) \n                else : \n                    tags_list . append ( [ template , repo , \"No DockerHub tags\" ] ) \n            dict_of_parsed [ template ] [ \"container\" ] . append ( repo_version ) \n    for x , entry in enumerate ( tags_list ) : \n        color = \"blue_bold\" if 3 > x else ( \"white\" if x % 2 != 0 else \"0;37;40m\" ) \n        final_width = [ int ( terminal_width / 4 ) , int ( terminal_width / 2 ) , int ( terminal_width / 4 ) ] \n        sys . stdout . write ( colored_print ( \"\\n {0: <{3}} {1: ^{4}} {2: >{5}}\" . format ( * entry , * final_width ) , color ) ) \n    sys . stdout . write ( \"\\n{0: >{1}}\\n\" . format ( \"(* = default)\" , terminal_width + 3 ) ) "}
{"7070": "\ndef set_kmers ( kmer_opt , max_read_len ) : \n    logger . debug ( \"Kmer option set to: {}\" . format ( kmer_opt ) ) \n    if kmer_opt == \"auto\" : \n        if 175 <= max_read_len : \n            kmers = [ 55 , 77 , 99 , 113 , 127 ] \n        else : \n            kmers = [ 21 , 33 , 55 , 67 , 77 ] \n        logger . debug ( \"Kmer range automatically selected based on max read\" \"length of {}: {}\" . format ( max_read_len , kmers ) ) \n    elif 1 < len ( kmer_opt . split ( ) ) : \n        kmers = kmer_opt . split ( ) \n        logger . debug ( \"Kmer range manually set to: {}\" . format ( kmers ) ) \n    else : \n        kmers = [ ] \n        logger . debug ( \"Kmer range set to empty (will be automatically \" \"determined by SPAdes\" ) \n    return kmers "}
{"7081": "\ndef main ( mash_output , hash_cutoff , sample_id , assembly_file ) : \n    input_f = open ( mash_output , \"r\" ) \n    master_dict = { } \n    for line in input_f : \n        tab_split = line . split ( \"\\t\" ) \n        current_seq = tab_split [ 1 ] . strip ( ) \n        ref_accession = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3 ] ) \n        mash_dist = tab_split [ 2 ] . strip ( ) \n        hashes_list = tab_split [ - 1 ] . strip ( ) . split ( \"/\" ) \n        perc_hashes = float ( hashes_list [ 0 ] ) / float ( hashes_list [ 1 ] ) \n        if ref_accession in master_dict . keys ( ) : \n            current_seq += \", {}\" . format ( master_dict [ ref_accession ] [ - 1 ] ) \n        if float ( hash_cutoff ) < perc_hashes : \n            master_dict [ ref_accession ] = [ round ( 1 - float ( mash_dist ) , 2 ) , round ( perc_hashes , 2 ) , current_seq ] \n    send_to_output ( master_dict , mash_output , sample_id , assembly_file ) "}
{"7083": "\ndef main ( mash_output , sample_id ) : \n    logger . info ( \"Reading file : {}\" . format ( mash_output ) ) \n    read_mash_output = open ( mash_output ) \n    dic = { } \n    median_list = [ ] \n    filtered_dic = { } \n    logger . info ( \"Generating dictionary and list to pre-process the final json\" ) \n    for line in read_mash_output : \n        tab_split = line . split ( \"\\t\" ) \n        identity = tab_split [ 0 ] \n        median_multiplicity = tab_split [ 2 ] \n        query_id = tab_split [ 4 ] \n        dic [ query_id ] = [ identity , median_multiplicity ] \n        median_list . append ( float ( median_multiplicity ) ) \n    output_json = open ( \" \" . join ( mash_output . split ( \".\" ) [ : - 1 ] ) + \".json\" , \"w\" ) \n    if 0 < len ( median_list ) : \n        median_cutoff = median ( median_list ) \n        logger . info ( \"Generating final json to dump to a file\" ) \n        for k , v in dic . items ( ) : \n            copy_number = int ( float ( v [ 1 ] ) / median_cutoff ) \n            if median_cutoff < float ( v [ 1 ] ) : \n                filtered_dic [ \"_\" . join ( k . split ( \"_\" ) [ 0 : 3 ] ) ] = [ round ( float ( v [ 0 ] ) , 2 ) , copy_number ] \n        logger . info ( \"Exported dictionary has {} entries\" . format ( len ( filtered_dic ) ) ) \n    else : \n        logger . error ( \"No matches were found using mash screen for the queried reads\" ) \n    output_json . write ( json . dumps ( filtered_dic ) ) \n    output_json . close ( ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Screen\" , \"table\" : \"plasmids\" , \"patlas_mashscreen\" : filtered_dic , \"value\" : len ( filtered_dic ) } ] } ] , } \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7084": "\ndef colored_print ( msg , color_label = \"white_bold\" ) : \n    if sys . stdout . encoding != \"UTF-8\" : \n        msg = \"\" . join ( [ i if 128 > ord ( i ) else \"\" for i in msg ] ) \n    try : \n        col = COLORS [ color_label ] \n    except KeyError : \n        col = color_label \n    return \"\\x1b[{}{}\\x1b[0m\" . format ( col , msg ) "}
{"7089": "\ndef get_encodings_in_range ( rmin , rmax ) : \n    valid_encodings = [ ] \n    valid_phred = [ ] \n    for encoding , ( phred , ( emin , emax ) ) in RANGES . items ( ) : \n        if emin <= rmin and emax >= rmax : \n            valid_encodings . append ( encoding ) \n            valid_phred . append ( phred ) \n    return valid_encodings , valid_phred "}
{"7091": "\ndef filter_assembly ( assembly_file , minimum_coverage , coverage_info , output_file ) : \n    write_flag = False \n    with open ( assembly_file ) as fh , open ( output_file , \"w\" ) as out_fh : \n        for line in fh : \n            if line . startswith ( \">\" ) : \n                write_flag = False \n                header = line . strip ( ) [ 1 : ] \n                contig_cov = coverage_info [ header ] [ \"cov\" ] \n                if minimum_coverage <= contig_cov : \n                    write_flag = True \n                    out_fh . write ( line ) \n            elif write_flag : \n                out_fh . write ( line ) "}
{"7092": "\ndef filter_bam ( coverage_info , bam_file , min_coverage , output_bam ) : \n    contig_list = [ x for x , vals in coverage_info . items ( ) if min_coverage <= vals [ \"cov\" ] ] \n    cli = [ \"samtools\" , \"view\" , \"-bh\" , \"-F\" , \"4\" , \"-o\" , output_bam , \"-@\" , \"1\" , bam_file , ] \n    cli += contig_list \n    logger . debug ( \"Runnig samtools view subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished samtools view subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished samtools view subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished samtools view with return code: {}\" . format ( p . returncode ) ) \n    if not p . returncode : \n        cli = [ \"samtools\" , \"index\" , output_bam ] \n        logger . debug ( \"Runnig samtools index subprocess with command: \" \"{}\" . format ( cli ) ) \n        p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n        stdout , stderr = p . communicate ( ) \n        try : \n            stderr = stderr . decode ( \"utf8\" ) \n            stdout = stdout . decode ( \"utf8\" ) \n        except ( UnicodeDecodeError , AttributeError ) : \n            stderr = str ( stderr ) \n            stdout = str ( stdout ) \n        logger . info ( \"Finished samtools index subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n        logger . info ( \"Fished samtools index subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n        logger . info ( \"Finished samtools index with return code: {}\" . format ( p . returncode ) ) "}
{"7093": "\ndef evaluate_min_coverage ( coverage_opt , assembly_coverage , assembly_size ) : \n    if coverage_opt == \"auto\" : \n        min_coverage = ( assembly_coverage / assembly_size ) * .3 \n        logger . info ( \"Minimum assembly coverage automatically set to: \" \"{}\" . format ( min_coverage ) ) \n        if 10 > min_coverage : \n            logger . info ( \"Minimum assembly coverage cannot be set to lower\" \" that 10. Setting to 10\" ) \n            min_coverage = 10 \n    else : \n        min_coverage = int ( coverage_opt ) \n        logger . info ( \"Minimum assembly coverage manually set to: {}\" . format ( min_coverage ) ) \n    return min_coverage "}
{"7099": "\ndef quickhull ( sample ) : \n    link = lambda a , b : np . concatenate ( ( a , b [ 1 : ] ) ) \n    edge = lambda a , b : np . concatenate ( ( [ a ] , [ b ] ) ) \n    def dome ( sample , base ) : \n        h , t = base \n        dists = np . dot ( sample - h , np . dot ( ( ( 0 , - 1 ) , ( 1 , 0 ) ) , ( t - h ) ) ) \n        outer = np . repeat ( sample , 0 < dists , axis = 0 ) \n        if len ( outer ) : \n            pivot = sample [ np . argmax ( dists ) ] \n            return link ( dome ( outer , edge ( h , pivot ) ) , dome ( outer , edge ( pivot , t ) ) ) \n        else : \n            return base \n    if 2 < len ( sample ) : \n        axis = sample [ : , 0 ] \n        base = np . take ( sample , [ np . argmin ( axis ) , np . argmax ( axis ) ] , axis = 0 ) \n        return link ( dome ( sample , base ) , dome ( sample , base [ : : - 1 ] ) ) \n    else : \n        return sample "}
{"7114": "\ndef get_feat_segments ( F , bound_idxs ) : \n    assert 0 < len ( bound_idxs ) , \"Boundaries can't be empty\" \n    bound_idxs = np . sort ( bound_idxs ) \n    assert 0 <= bound_idxs [ 0 ] and F . shape [ 0 ] > bound_idxs [ - 1 ] , \"Boundaries are not correct for the given feature dimensions.\" \n    feat_segments = [ ] \n    for i in range ( len ( bound_idxs ) - 1 ) : \n        feat_segments . append ( F [ bound_idxs [ i ] : bound_idxs [ i + 1 ] , : ] ) \n    return feat_segments "}
{"7115": "\ndef feat_segments_to_2dfmc_max ( feat_segments , offset = 4 ) : \n    if len ( feat_segments ) == 0 : \n        return [ ] \n    max_len = max ( [ feat_segment . shape [ 0 ] for feat_segment in feat_segments ] ) \n    fmcs = [ ] \n    for feat_segment in feat_segments : \n        X = np . zeros ( ( max_len , feat_segment . shape [ 1 ] ) ) \n        if offset >= feat_segment . shape [ 0 ] or offset == 0 : \n            X [ : feat_segment . shape [ 0 ] , : ] = feat_segment \n        else : \n            X [ : feat_segment . shape [ 0 ] - offset , : ] = feat_segment [ offset // 2 : - offset // 2 , : ] \n        try : \n            fmcs . append ( utils2d . compute_ffmc2d ( X ) ) \n        except : \n            logging . warning ( \"Couldn't compute the 2D Fourier Transform\" ) \n            fmcs . append ( np . zeros ( ( X . shape [ 0 ] * X . shape [ 1 ] ) // 2 + 1 ) ) \n    return np . asarray ( fmcs ) "}
{"7116": "\ndef compute_similarity ( F , bound_idxs , dirichlet = False , xmeans = False , k = 5 , offset = 4 ) : \n    feat_segments = get_feat_segments ( F , bound_idxs ) \n    fmcs = feat_segments_to_2dfmc_max ( feat_segments , offset ) \n    if len ( fmcs ) == 0 : \n        return np . arange ( len ( bound_idxs ) - 1 ) \n    if dirichlet : \n        k_init = np . min ( [ fmcs . shape [ 0 ] , k ] ) \n        if 500 < fmcs . shape [ 1 ] : \n            labels_est = compute_labels_kmeans ( fmcs , k = k ) \n        else : \n            dpgmm = mixture . DPGMM ( n_components = k_init , covariance_type = 'full' ) \n            dpgmm . fit ( fmcs ) \n            k = len ( dpgmm . means_ ) \n            labels_est = dpgmm . predict ( fmcs ) \n    if xmeans : \n        xm = XMeans ( fmcs , plot = False ) \n        k = xm . estimate_K_knee ( th = 0.01 , maxK = 8 ) \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    else : \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    return labels_est "}
{"7118": "\ndef partial_fit ( self , X , Y ) : \n    for ( xi , yi ) in itertools . izip ( X , Y ) : \n        prev_mean = None \n        prev_length = None \n        if self . scatter_within_ is None : \n            d , n = xi . shape \n            if 0 < yi [ 0 ] : \n                yi = np . concatenate ( [ np . array ( [ 0 ] ) , yi ] ) \n            if n > yi [ - 1 ] : \n                yi = np . concatenate ( [ yi , np . array ( [ n ] ) ] ) \n            self . scatter_within_ = self . sigma * np . eye ( d ) \n            self . scatter_ordinal_ = np . zeros ( d ) \n        for ( seg_start , seg_end ) in zip ( yi [ : - 1 ] , yi [ 1 : ] ) : \n            seg_length = seg_end - seg_start \n            if 2 > seg_length : \n                continue \n            seg_mean = np . mean ( xi [ : , seg_start : seg_end ] , axis = 1 , keepdims = True ) \n            seg_cov = np . cov ( xi [ : , seg_start : seg_end ] ) \n            self . scatter_within_ = self . scatter_within_ + seg_length * seg_cov \n            if prev_mean is not None : \n                diff_ord = seg_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + seg_length * np . dot ( diff_ord , diff_ord . T ) \n                diff_ord = prev_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + prev_length * np . dot ( diff_ord , diff_ord . T ) \n            prev_mean = seg_mean \n            prev_length = seg_length \n    e_vals , e_vecs = scipy . linalg . eig ( self . scatter_ordinal_ , self . scatter_within_ ) \n    self . e_vals_ = e_vals \n    self . e_vecs_ = e_vecs \n    self . components_ = e_vecs . T \n    return self "}
{"7120": "\ndef find_estimation ( jam , boundaries_id , labels_id , params ) : \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jam . search ( namespace = namespace ) . search ( ** { \"Sandbox.boundaries_id\" : boundaries_id } ) . search ( ** { \"Sandbox.labels_id\" : lambda x : ( isinstance ( x , six . string_types ) and re . match ( labels_id , x ) is not None ) or x is None } ) \n    for key , val in zip ( params . keys ( ) , params . values ( ) ) : \n        if isinstance ( val , six . string_types ) : \n            ann = ann . search ( ** { \"Sandbox.%s\" % key : val } ) \n        else : \n            ann = ann . search ( ** { \"Sandbox.%s\" % key : lambda x : x == val } ) \n    if 1 < len ( ann ) : \n        logging . warning ( \"More than one estimation with same parameters.\" ) \n    if 0 < len ( ann ) : \n        ann = ann [ 0 ] \n    if not ann : \n        ann = None \n    return ann "}
{"7129": "\ndef align_segmentation ( beat_times , song ) : \n    try : \n        segment_times , segment_labels = msaf . io . read_references ( song ) \n    except : \n        return None , None , None \n    segment_times = np . asarray ( segment_times ) \n    segment_intervals = msaf . utils . times_to_intervals ( segment_times ) \n    beat_intervals = np . asarray ( zip ( beat_times [ : - 1 ] , beat_times [ 1 : ] ) ) \n    beat_segment_ids = librosa . util . match_intervals ( beat_intervals , segment_intervals ) \n    segment_beats = [ ] \n    segment_times_out = [ ] \n    segment_labels_out = [ ] \n    for i in range ( segment_times . shape [ 0 ] ) : \n        hits = np . argwhere ( beat_segment_ids == i ) \n        if 0 < len ( hits ) and len ( segment_intervals ) > i and len ( segment_labels ) > i : \n            segment_beats . extend ( hits [ 0 ] ) \n            segment_times_out . append ( segment_intervals [ i , : ] ) \n            segment_labels_out . append ( segment_labels [ i ] ) \n    segment_beats = list ( segment_beats ) \n    segment_times_out = segment_times \n    return segment_beats , segment_times_out , segment_labels_out "}
{"7130": "\ndef estimate_beats ( self ) : \n    if self . _audio_percussive is None : \n        self . _audio_harmonic , self . _audio_percussive = self . compute_HPSS ( ) \n    tempo , frames = librosa . beat . beat_track ( y = self . _audio_percussive , sr = self . sr , hop_length = self . hop_length ) \n    times = librosa . frames_to_time ( frames , sr = self . sr , hop_length = self . hop_length ) \n    if 0 < len ( times ) and times [ 0 ] == 0 : \n        times = times [ 1 : ] \n        frames = frames [ 1 : ] \n    return times , frames "}
{"7131": "\ndef read_ann_beats ( self ) : \n    times , frames = ( None , None ) \n    if os . path . isfile ( self . file_struct . ref_file ) : \n        try : \n            jam = jams . load ( self . file_struct . ref_file ) \n        except TypeError : \n            logging . warning ( \"Can't read JAMS file %s. Maybe it's not \" \"compatible with current JAMS version?\" % self . file_struct . ref_file ) \n            return times , frames \n        beat_annot = jam . search ( namespace = \"beat.*\" ) \n        if 0 < len ( beat_annot ) : \n            beats_inters , _ = beat_annot [ 0 ] . to_interval_values ( ) \n            times = beats_inters [ : , 0 ] \n            frames = librosa . time_to_frames ( times , sr = self . sr , hop_length = self . hop_length ) \n    return times , frames "}
{"7147": "\ndef get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) : \n    utils . ensure_dir ( msaf . config . results_dir ) \n    file_name = os . path . join ( msaf . config . results_dir , \"results\" ) \n    file_name += \"_boundsE%s_labelsE%s\" % ( boundaries_id , labels_id ) \n    file_name += \"_annotatorE%d\" % ( annotator_id ) \n    sorted_keys = sorted ( config . keys ( ) , key = str . lower ) \n    for key in sorted_keys : \n        file_name += \"_%sE%s\" % ( key , str ( config [ key ] ) . replace ( \"/\" , \"_\" ) ) \n    if 255 - len ( msaf . config . results_ext ) < len ( file_name ) : \n        file_name = file_name [ : 255 - len ( msaf . config . results_ext ) ] \n    return file_name + msaf . config . results_ext "}
{"7149": "\ndef AddConfigVar ( name , doc , configparam , root = config ) : \n    if root is config : \n        configparam . fullname = name \n    sections = name . split ( '.' ) \n    if 1 < len ( sections ) : \n        if not hasattr ( root , sections [ 0 ] ) : \n            class SubObj ( object ) : \n                _i_am_a_config_class = True \n            setattr ( root . __class__ , sections [ 0 ] , SubObj ( ) ) \n        newroot = getattr ( root , sections [ 0 ] ) \n        if ( not getattr ( newroot , '_i_am_a_config_class' , False ) or isinstance ( newroot , type ) ) : \n            raise TypeError ( 'Internal config nodes must be config class instances' , newroot ) \n        return AddConfigVar ( '.' . join ( sections [ 1 : ] ) , doc , configparam , root = newroot ) \n    else : \n        if hasattr ( root , name ) : \n            raise AttributeError ( 'This name is already taken' , configparam . fullname ) \n        configparam . doc = doc \n        if not callable ( configparam . default ) : \n            configparam . __get__ ( root , type ( root ) , delete_key = True ) \n        else : \n            try : \n                fetch_val_for_key ( configparam . fullname ) \n                configparam . __get__ ( root , type ( root ) , delete_key = True ) \n            except KeyError : \n                pass \n        setattr ( root . __class__ , sections [ 0 ] , configparam ) \n        _config_var_list . append ( configparam ) "}
{"7152": "\ndef gaussian_cost ( X ) : \n    d , n = X . shape \n    if 2 > n : \n        return 0 \n    sigma = np . var ( X , axis = 1 , ddof = 1 ) \n    cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) \n    return cost "}
{"7153": "\ndef lognormalize ( F , floor = 0.1 , min_db = - 80 ) : \n    assert 0 > min_db \n    F = min_max_normalize ( F , floor = floor ) \n    F = np . abs ( min_db ) * np . log10 ( F ) \n    return F "}
{"7157": "\ndef remove_empty_segments ( times , labels ) : \n    assert len ( times ) - 1 == len ( labels ) \n    inters = times_to_intervals ( times ) \n    new_inters = [ ] \n    new_labels = [ ] \n    for inter , label in zip ( inters , labels ) : \n        if inter [ 1 ] > inter [ 0 ] : \n            new_inters . append ( inter ) \n            new_labels . append ( label ) \n    return intervals_to_times ( np . asarray ( new_inters ) ) , new_labels "}
{"7161": "\ndef align_end_hierarchies ( hier1 , hier2 , thres = 0.5 ) : \n    dur_h1 = hier1 [ 0 ] [ - 1 ] \n    for hier in hier1 : \n        assert hier [ - 1 ] == dur_h1 , \"hier1 is not correctly \" \"formatted {} {}\" . format ( hier [ - 1 ] , dur_h1 ) \n    dur_h2 = hier2 [ 0 ] [ - 1 ] \n    for hier in hier2 : \n        assert hier [ - 1 ] == dur_h2 , \"hier2 is not correctly formatted\" \n    if thres < abs ( dur_h1 - dur_h2 ) : \n        return \n    for hier in hier1 : \n        hier [ - 1 ] = dur_h2 "}
{"7162": "\ndef _distance ( self , idx ) : \n    if scipy . sparse . issparse ( self . data ) : \n        step = self . data . shape [ 1 ] \n    else : \n        step = 50000 \n    d = np . zeros ( ( self . data . shape [ 1 ] ) ) \n    if idx == - 1 : \n        vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) \n        if scipy . sparse . issparse ( self . data ) : \n            vec = scipy . sparse . csc_matrix ( vec ) \n    else : \n        vec = self . data [ : , idx : idx + 1 ] \n    self . _logger . info ( 'compute distance to node ' + str ( idx ) ) \n    for idx_start in range ( 0 , self . data . shape [ 1 ] , step ) : \n        if self . data . shape [ 1 ] < idx_start + step : \n            idx_end = self . data . shape [ 1 ] \n        else : \n            idx_end = idx_start + step \n        d [ idx_start : idx_end ] = self . _distfunc ( self . data [ : , idx_start : idx_end ] , vec ) \n        self . _logger . info ( 'completed:' + str ( idx_end / ( self . data . shape [ 1 ] / 100.0 ) ) + \"%\" ) \n    return d "}
{"7163": "\ndef estimate_K_knee ( self , th = .015 , maxK = 12 ) : \n    if maxK > self . X . shape [ 0 ] : \n        maxK = self . X . shape [ 0 ] \n    if 2 > maxK : \n        maxK = 2 \n    K = np . arange ( 1 , maxK ) \n    bics = [ ] \n    for k in K : \n        means , labels = self . run_kmeans ( self . X , k ) \n        bic = self . compute_bic ( self . X , means , labels , K = k , R = self . X . shape [ 0 ] ) \n        bics . append ( bic ) \n    diff_bics = np . diff ( bics ) \n    finalK = K [ - 1 ] \n    if len ( bics ) == 1 : \n        finalK = 2 \n    else : \n        bics = np . asarray ( bics ) \n        bics -= bics . min ( ) \n        diff_bics -= diff_bics . min ( ) \n        for i in range ( len ( K [ : - 1 ] ) ) : \n            if th > diff_bics [ i ] and K [ i ] != 1 : \n                finalK = K [ i ] \n                break \n    if self . plot : \n        plt . subplot ( 2 , 1 , 1 ) \n        plt . plot ( K , bics , label = \"BIC\" ) \n        plt . plot ( K [ : - 1 ] , diff_bics , label = \"BIC diff\" ) \n        plt . legend ( loc = 2 ) \n        plt . subplot ( 2 , 1 , 2 ) \n        plt . scatter ( self . X [ : , 0 ] , self . X [ : , 1 ] ) \n        plt . show ( ) \n    return finalK "}
{"7173": "\ndef compute_labels ( X , rank , R , bound_idxs , niter = 300 ) : \n    try : \n        F , G = cnmf ( X , rank , niter = niter , hull = False ) \n    except : \n        return [ 1 ] \n    label_frames = filter_activation_matrix ( G . T , R ) \n    label_frames = np . asarray ( label_frames , dtype = int ) \n    labels = [ ] \n    bound_inters = zip ( bound_idxs [ : - 1 ] , bound_idxs [ 1 : ] ) \n    for bound_inter in bound_inters : \n        if 0 >= bound_inter [ 1 ] - bound_inter [ 0 ] : \n            labels . append ( np . max ( label_frames ) + 1 ) \n        else : \n            labels . append ( most_frequent ( label_frames [ bound_inter [ 0 ] : bound_inter [ 1 ] ] ) ) \n    return labels "}
{"7179": "\ndef run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = 0 ) : \n    if msaf . config . minimum_frames >= config [ \"features\" ] . features . shape [ 0 ] : \n        logging . warning ( \"Audio file too short, or too many few beats \" \"estimated. Returning empty estimations.\" ) \n        return np . asarray ( [ 0 , config [ \"features\" ] . dur ] ) , np . asarray ( [ 0 ] , dtype = int ) \n    bounds_module = get_boundaries_module ( boundaries_id ) \n    labels_module = get_labels_module ( labels_id ) \n    frame_times = config [ \"features\" ] . frame_times \n    run_fun = run_hierarchical if config [ \"hier\" ] else run_flat \n    est_times , est_labels = run_fun ( file_struct , bounds_module , labels_module , frame_times , config , annotator_id ) \n    return est_times , est_labels "}
{"7187": "\ndef spool ( iterable , maxlen = 1250 ) : \n    words = int ( ) \n    text = str ( ) \n    try : \n        while True : \n            while maxlen > words : \n                stream = yield \n                text = reduce ( accumulator , stream , text ) \n                words = reduce ( accumulator , stream , words ) \n            iterable . send ( text ) \n            words = int ( ) \n            text = str ( ) \n    except GeneratorExit : \n        iterable . send ( text ) \n        iterable . close ( ) "}
{"7188": "\ndef source ( target , inputstream = sys . stdin ) : \n    for line in inputstream : \n        while 600 < len ( line ) : \n            init , sep , line = line . partition ( ' ' ) \n            assert 600 >= len ( init ) \n            target . send ( '' . join ( [ init , sep ] ) ) \n        target . send ( line ) \n    inputstream . close ( ) \n    return target . close ( ) "}
{"7196": "\ndef set ( self , node_ids , variable = None , name = \"tmp\" ) : \n    if variable is None : \n        variable = pd . Series ( np . ones ( len ( node_ids ) ) , index = node_ids . index ) \n    df = pd . DataFrame ( { name : variable , \"node_idx\" : self . _node_indexes ( node_ids ) } ) \n    length = len ( df ) \n    df = df . dropna ( how = \"any\" ) \n    newl = len ( df ) \n    if 0 < length - newl : \n        print ( \"Removed %d rows because they contain missing values\" % ( length - newl ) ) \n    self . variable_names . add ( name ) \n    self . net . initialize_access_var ( name . encode ( 'utf-8' ) , df . node_idx . values . astype ( 'int' ) , df [ name ] . values . astype ( 'double' ) ) "}
{"7198": "\ndef get_node_ids ( self , x_col , y_col , mapping_distance = None ) : \n    xys = pd . DataFrame ( { 'x' : x_col , 'y' : y_col } ) \n    distances , indexes = self . kdtree . query ( xys . as_matrix ( ) ) \n    indexes = np . transpose ( indexes ) [ 0 ] \n    distances = np . transpose ( distances ) [ 0 ] \n    node_ids = self . nodes_df . iloc [ indexes ] . index \n    df = pd . DataFrame ( { \"node_id\" : node_ids , \"distance\" : distances } , index = xys . index ) \n    if mapping_distance is not None : \n        df = df [ mapping_distance >= df . distance ] \n    return df . node_id "}
{"7201": "\ndef nearest_pois ( self , distance , category , num_pois = 1 , max_distance = None , imp_name = None , include_poi_ids = False ) : \n    if max_distance is None : \n        max_distance = distance \n    if category not in self . poi_category_names : \n        assert 0 , \"Need to call set_pois for this category\" \n    if self . max_pois < num_pois : \n        assert 0 , \"Asking for more pois than set in init_pois\" \n    imp_num = self . _imp_name_to_num ( imp_name ) \n    dists , poi_ids = self . net . find_all_nearest_pois ( distance , num_pois , category . encode ( 'utf-8' ) , imp_num ) \n    dists [ dists == - 1 ] = max_distance \n    df = pd . DataFrame ( dists , index = self . node_ids ) \n    df . columns = list ( range ( 1 , num_pois + 1 ) ) \n    if include_poi_ids : \n        df2 = pd . DataFrame ( poi_ids , index = self . node_ids ) \n        df2 . columns = [ \"poi%d\" % i for i in range ( 1 , num_pois + 1 ) ] \n        for col in df2 . columns : \n            s = df2 [ col ] . astype ( 'int' ) \n            df2 [ col ] = self . poi_category_indexes [ category ] . values [ s ] \n            df2 . loc [ s == - 1 , col ] = np . nan \n        df = pd . concat ( [ df , df2 ] , axis = 1 ) \n    return df "}
{"7202": "\ndef low_connectivity_nodes ( self , impedance , count , imp_name = None ) : \n    self . set ( self . node_ids . to_series ( ) , name = 'counter' ) \n    agg = self . aggregate ( impedance , type = 'count' , imp_name = imp_name , name = 'counter' ) \n    return np . array ( agg [ count > agg ] . index ) "}
{"7233": "\ndef match ( self , request ) : \n    if 0 >= self . _times : \n        raise PookExpiredMock ( 'Mock expired' ) \n    for test in self . filters : \n        if not test ( request , self ) : \n            return False , [ ] \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    matches , errors = self . matchers . match ( request ) \n    if not matches : \n        return False , errors \n    self . _calls . append ( request ) \n    self . _matches += 1 \n    if not self . _persist : \n        self . _times -= 1 \n    if self . _error : \n        raise self . _error \n    for callback in self . callbacks : \n        callback ( request , self ) \n    return True , [ ] "}
{"7262": "\ndef find_best_root ( self , force_positive = True , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if n == self . tree . root : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( best_root [ \"chisq\" ] > chisq ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( 1 - x ) , var * ( 1 - x ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if 0 <= reg [ \"slope\" ] or ( force_positive == False ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        print ( \"TreeRegression.find_best_root: No valid root found!\" , force_positive ) \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( 1 - y ) , var * ( 1 - y ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3 , 3 ) ) \n        estimator_hessian [ : 2 , : 2 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2 , 2 ] = ( deriv [ 0 ] [ 1 ] + deriv [ 1 ] [ 1 ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ 0 ] [ 0 ] - deriv [ 1 ] [ 0 ] ) ** 2 \n        estimator_hessian [ 0 , 2 ] = estimator_hessian [ 2 , 0 ] \n        estimator_hessian [ 1 , 2 ] = estimator_hessian [ 2 , 1 ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7268": "\ndef prof2seq ( profile , gtr , sample_from_prof = False , normalize = True ) : \n    if normalize : \n        tmp_profile , pre = normalize_profile ( profile , return_offset = False ) \n    else : \n        tmp_profile = profile \n    if sample_from_prof : \n        cumdis = tmp_profile . cumsum ( axis = 1 ) . T \n        randnum = np . random . random ( size = cumdis . shape [ 1 ] ) \n        idx = np . argmax ( randnum <= cumdis , axis = 0 ) \n    else : \n        idx = tmp_profile . argmax ( axis = 1 ) \n    seq = gtr . alphabet [ idx ] \n    prof_values = tmp_profile [ np . arange ( tmp_profile . shape [ 0 ] ) , idx ] \n    return seq , prof_values , idx "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = 0 \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        elif l . is_terminal ( ) : \n            self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , 0 , warn = True ) \n            failed_leaves += 1 \n            l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n            if self . tree . count_terminals ( ) / 3 < failed_leaves : \n                self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , 0 , warn = True ) \n                self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2 , warn = True ) \n                break \n        else : \n            pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , 0 , warn = True ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7280": "\ndef _fitch_anc ( self , ** kwargs ) : \n    for l in self . tree . get_terminals ( ) : \n        l . state = [ [ k ] for k in l . cseq ] \n    L = len ( self . tree . get_terminals ( ) [ 0 ] . cseq ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\" , 2 ) \n    for node in self . tree . get_nonterminals ( order = 'postorder' ) : \n        node . state = [ self . _fitch_state ( node , k ) for k in range ( L ) ] \n    ambs = [ i for i in range ( L ) if 1 < len ( self . tree . root . state [ i ] ) ] \n    if 0 < len ( ambs ) : \n        for amb in ambs : \n            self . logger ( \"Ambiguous state of the root sequence \" \"in the position %d: %s, \" \"choosing %s\" % ( amb , str ( self . tree . root . state [ amb ] ) , self . tree . root . state [ amb ] [ 0 ] ) , 4 ) \n    self . tree . root . cseq = np . array ( [ k [ np . random . randint ( len ( k ) ) if 1 < len ( k ) else 0 ] for k in self . tree . root . state ] ) \n    if self . is_vcf : \n        self . tree . root . sequence = self . dict_sequence ( self . tree . root ) \n    else : \n        self . tree . root . sequence = self . expanded_sequence ( self . tree . root ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \" \"Fitch profiles.\" , 2 ) \n    N_diff = 0 \n    for node in self . tree . get_nonterminals ( order = 'preorder' ) : \n        if node . up != None : \n            sequence = np . array ( [ node . up . cseq [ i ] if node . up . cseq [ i ] in node . state [ i ] else node . state [ i ] [ 0 ] for i in range ( L ) ] ) \n            if hasattr ( node , 'sequence' ) : \n                N_diff += ( sequence != node . cseq ) . sum ( ) \n            else : \n                N_diff += L \n            node . cseq = sequence \n            if self . is_vcf : \n                node . sequence = self . dict_sequence ( node ) \n            else : \n                node . sequence = self . expanded_sequence ( node ) \n            node . mutations = self . get_mutations ( node ) \n        node . profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n        del node . state \n    self . logger ( \"Done ancestral state reconstruction\" , 3 ) \n    for node in self . tree . get_terminals ( ) : \n        node . profile = seq2prof ( node . original_cseq , self . gtr . profile_map ) \n    return N_diff "}
{"7282": "\ndef _fitch_intersect ( self , arrays ) : \n    def pairwise_intersect ( arr1 , arr2 ) : \n        s2 = set ( arr2 ) \n        b3 = [ val for val in arr1 if val in s2 ] \n        return b3 \n    arrays = list ( arrays ) \n    N = len ( arrays ) \n    while 1 < N : \n        arr1 = arrays . pop ( ) \n        arr2 = arrays . pop ( ) \n        arr = pairwise_intersect ( arr1 , arr2 ) \n        arrays . append ( arr ) \n        N = len ( arrays ) \n    return arrays [ 0 ] "}
{"7286": "\ndef optimize_branch_length ( self , mode = 'joint' , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\" % mode , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    store_old_dist = False \n    if 'store_old' in kwargs : \n        store_old_dist = kwargs [ 'store_old' ] \n    if mode == 'marginal' : \n        if not hasattr ( self . tree . root , \"marginal_profile\" ) : \n            self . infer_ancestral_sequences ( marginal = True ) \n    max_bl = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            continue \n        if store_old_dist : \n            node . _old_length = node . branch_length \n        if mode == 'marginal' : \n            new_len = self . optimal_marginal_branch_length ( node ) \n        elif mode == 'joint' : \n            new_len = self . optimal_branch_length ( node ) \n        else : \n            self . logger ( \"treeanc.optimize_branch_length: unsupported optimization mode\" , 4 , warn = True ) \n            new_len = node . branch_length \n        if 0 > new_len : \n            continue \n        self . logger ( \"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\" \" Updating branch length...\" % ( node . branch_length , new_len , len ( node . mutations ) * self . one_mutation ) , 5 ) \n        node . branch_length = new_len \n        node . mutation_length = new_len \n        max_bl = max ( max_bl , new_len ) \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    if 0.15 < max_bl and mode == 'joint' : \n        self . logger ( \"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\" \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\" \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\" \" \\n\\t ****branch_length_mode='input'\" , 0 , warn = True ) \n    self . _prepare_nodes ( ) \n    return ttconf . SUCCESS "}
{"7289": "\ndef optimize_seq_and_branch_len ( self , reuse_branch_len = True , prune_short = True , marginal_sequences = False , branch_length_mode = 'joint' , max_iter = 5 , infer_gtr = False , ** kwargs ) : \n    if branch_length_mode == 'marginal' : \n        marginal_sequences = True \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: sequences...\" , 1 ) \n    if reuse_branch_len : \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = infer_gtr , marginal = marginal_sequences , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    else : \n        N_diff = self . reconstruct_anc ( method = 'fitch' , infer_gtr = infer_gtr , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , marginal = False ) \n    n = 0 \n    while max_iter > n : \n        n += 1 \n        if prune_short : \n            self . prune_short_branches ( ) \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = False , marginal = marginal_sequences , ** kwargs ) \n        self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\" \" #Nuc changed since prev reconstructions: %d\" % ( n , N_diff ) , 2 ) \n        if 1 > N_diff : \n            break \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    self . tree . unconstrained_sequence_LH = ( self . tree . sequence_LH * self . multiplicity ) . sum ( ) \n    self . _prepare_nodes ( ) \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self . tree . unconstrained_sequence_LH , 2 ) \n    return ttconf . SUCCESS "}
{"7294": "\ndef _check_fix_Q ( self , fixed_mu = False ) : \n    self . Pi /= self . Pi . sum ( ) \n    self . W += self . break_degen + self . break_degen . T \n    np . fill_diagonal ( self . W , 0 ) \n    Wdiag = - ( self . Q ) . sum ( axis = 0 ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W /= scale_factor \n    if not fixed_mu : \n        self . mu *= scale_factor \n    if self . alphabet . shape [ 0 ] > ( 1e-10 > self . Q . sum ( axis = 0 ) ) . sum ( ) : \n        print ( \"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\" , self . Q . sum ( axis = 0 ) ) \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7295": "\ndef prob_t_compressed ( self , seq_pair , multiplicity , t , return_log = False ) : \n    if 0 > t : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        tmp_eQT = self . expQt ( t ) \n        bad_indices = ( tmp_eQT == 0 ) \n        logQt = np . log ( tmp_eQT + ttconf . TINY_NUMBER * ( bad_indices ) ) \n        logQt [ np . isnan ( logQt ) | np . isinf ( logQt ) | bad_indices ] = - ttconf . BIG_NUMBER \n        logP = np . sum ( logQt [ seq_pair [ : , 1 ] , seq_pair [ : , 0 ] ] * multiplicity ) \n    return logP if return_log else np . exp ( logP ) "}
{"7297": "\ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = False , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2 \n        if 'success' not in opt : \n            opt [ 'success' ] = True \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4 , warn = True ) \n    except : \n        import scipy \n        print ( 'legacy scipy' , scipy . __version__ ) \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2 \n        opt = { 'success' : True } \n    if .9 * ttconf . MAX_BRANCH_LENGTH < new_len : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4 , warn = True ) \n    if opt [ \"success\" ] != True : \n        new_len = np . sum ( multiplicity [ seq_pair [ : , 1 ] != seq_pair [ : , 0 ] ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7298": "\ndef prob_t_profiles ( self , profile_pair , multiplicity , t , return_log = False , ignore_gaps = True ) : \n    if 0 > t : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        Qt = self . expQt ( t ) \n        if len ( Qt . shape ) == 3 : \n            res = np . einsum ( 'ai,ija,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        else : \n            res = np . einsum ( 'ai,ij,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        if ignore_gaps and ( self . gap_index is not None ) : \n            non_gap_frac = ( 1 - profile_pair [ 0 ] [ : , self . gap_index ] ) * ( 1 - profile_pair [ 1 ] [ : , self . gap_index ] ) \n            logP = np . sum ( multiplicity * np . log ( res ) * non_gap_frac ) \n        else : \n            logP = np . sum ( multiplicity * np . log ( res ) ) \n    return logP if return_log else np . exp ( logP ) "}
{"7301": "\ndef _set_branch_length_mode ( self , branch_length_mode ) : \n    if branch_length_mode in [ 'joint' , 'marginal' , 'input' ] : \n        self . branch_length_mode = branch_length_mode \n    elif self . aln : \n        bl_dis = [ n . branch_length for n in self . tree . find_clades ( ) if n . up ] \n        max_bl = np . max ( bl_dis ) \n        if 0.1 < max_bl : \n            bl_mode = 'input' \n        else : \n            bl_mode = 'joint' \n        self . logger ( \"TreeTime._set_branch_length_mode: maximum branch length is %1.3e, using branch length mode %s\" % ( max_bl , bl_mode ) , 1 ) \n        self . branch_length_mode = bl_mode \n    else : \n        self . branch_length_mode = 'input' "}
{"7302": "\ndef clock_filter ( self , reroot = 'least-squares' , n_iqd = None , plot = False ) : \n    if n_iqd is None : \n        n_iqd = ttconf . NIQD \n    if type ( reroot ) is list and len ( reroot ) == 1 : \n        reroot = str ( reroot [ 0 ] ) \n    terminals = self . tree . get_terminals ( ) \n    if reroot : \n        if self . reroot ( root = 'least-squares' if reroot == 'best' else reroot , covariation = False ) == ttconf . ERROR : \n            return ttconf . ERROR \n    else : \n        self . get_clock_model ( covariation = False ) \n    clock_rate = self . clock_model [ 'slope' ] \n    icpt = self . clock_model [ 'intercept' ] \n    res = { } \n    for node in terminals : \n        if hasattr ( node , 'raw_date_constraint' ) and ( node . raw_date_constraint is not None ) : \n            res [ node ] = node . dist2root - clock_rate * np . mean ( node . raw_date_constraint ) - icpt \n    residuals = np . array ( list ( res . values ( ) ) ) \n    iqd = np . percentile ( residuals , 75 ) - np . percentile ( residuals , 25 ) \n    for node , r in res . items ( ) : \n        if n_iqd * iqd < abs ( r ) and node . up . up is not None : \n            self . logger ( 'TreeTime.ClockFilter: marking %s as outlier, residual %f interquartile distances' % ( node . name , r / iqd ) , 3 , warn = True ) \n            node . bad_branch = True \n        else : \n            node . bad_branch = False \n    if reroot and self . reroot ( root = reroot ) == ttconf . ERROR : \n        return ttconf . ERROR \n    if plot : \n        self . plot_root_to_tip ( ) \n    return ttconf . SUCCESS "}
{"7304": "\ndef resolve_polytomies ( self , merge_compressed = False ) : \n    self . logger ( \"TreeTime.resolve_polytomies: resolving multiple mergers...\" , 1 ) \n    poly_found = 0 \n    for n in self . tree . find_clades ( ) : \n        if 2 < len ( n . clades ) : \n            prior_n_clades = len ( n . clades ) \n            self . _poly ( n , merge_compressed ) \n            poly_found += prior_n_clades - len ( n . clades ) \n    obsolete_nodes = [ n for n in self . tree . find_clades ( ) if len ( n . clades ) == 1 and n . up is not None ] \n    for node in obsolete_nodes : \n        self . logger ( 'TreeTime.resolve_polytomies: remove obsolete node ' + node . name , 4 ) \n        if node . up is not None : \n            self . tree . collapse ( node ) \n    if poly_found : \n        self . logger ( 'TreeTime.resolve_polytomies: introduces %d new nodes' % poly_found , 3 ) \n    else : \n        self . logger ( 'TreeTime.resolve_polytomies: No more polytomies to resolve' , 3 ) \n    return poly_found "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( 0.693147 > log_prob ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if 2 > L : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if 1 < n_delta : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif n_delta == 1 : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( new_xmin - TINY_NUMBER < x_vals ) & ( new_xmax + TINY_NUMBER > x_vals ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n        peak = y_vals . min ( ) \n        ind = BIG_NUMBER / 1000 > ( y_vals - peak ) \n        n_points = ind . sum ( ) \n        if n_points == 0 : \n            print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n            x_vals = [ 0 , 1 ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n        elif n_points == 1 : \n            res = Distribution . delta_function ( x_vals [ 0 ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , 0 ) \n        return ttconf . ERROR \n    bad_branch_counter = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                node . raw_date_constraint = None \n                node . bad_branch = True \n            else : \n                try : \n                    tmp = np . mean ( tmp_date ) \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = False \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = True \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = True \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter += 1 \n    if self . tree . count_terminals ( ) - 3 < bad_branch_counter : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , 1 , warn = True ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7319": "\ndef convert_dates ( self ) : \n    from datetime import datetime , timedelta \n    now = numeric_date ( ) \n    for node in self . tree . find_clades ( ) : \n        years_bp = self . date2dist . to_years ( node . time_before_present ) \n        if 0 > years_bp and self . real_dates : \n            if not hasattr ( node , \"bad_branch\" ) or node . bad_branch is False : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: The node is later than today, but it is not \" \"marked as \\\"BAD\\\", which indicates the error in the \" \"likelihood optimization.\" , 4 , warn = True ) \n            else : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: node which is marked as \\\"BAD\\\" optimized \" \"later than present day\" , 4 , warn = True ) \n        node . numdate = now - years_bp \n        year = np . floor ( node . numdate ) \n        days = max ( 0 , 365.25 * ( node . numdate - year ) - 1 ) \n        try : \n            n_date = datetime ( year , 1 , 1 ) + timedelta ( days = days ) \n            node . date = datetime . strftime ( n_date , \"%Y-%m-%d\" ) \n        except : \n            n_date = datetime ( 1900 , 1 , 1 ) + timedelta ( days = days ) \n            node . date = \"%04d-%02d-%02d\" % ( year , n_date . month , n_date . day ) "}
{"7352": "\ndef migrate ( self , target , follow = True , ** kwargs ) : \n    from solvebio import Dataset \n    from solvebio import DatasetMigration \n    if isinstance ( target , Dataset ) : \n        target_id = target . id \n    else : \n        target_id = target \n    limit = kwargs . pop ( 'limit' , None ) \n    if not limit and float ( 'inf' ) > self . _limit : \n        limit = self . _limit \n    params = self . _build_query ( limit = limit ) \n    params . pop ( 'offset' , None ) \n    params . pop ( 'ordering' , None ) \n    migration = DatasetMigration . create ( source_id = self . _dataset_id , target_id = target_id , source_params = params , client = self . _client , ** kwargs ) \n    if follow : \n        migration . follow ( ) \n    return migration "}
{"7365": "\ndef _normalize_tabular_data ( tabular_data , headers , sort = True ) : \n    if hasattr ( tabular_data , \"keys\" ) and hasattr ( tabular_data , \"values\" ) : \n        if hasattr ( tabular_data . values , \"__call__\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            rows = list ( izip_longest ( * list ( tabular_data . values ( ) ) ) ) \n        elif hasattr ( tabular_data , \"index\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            vals = tabular_data . values \n            names = tabular_data . index \n            rows = [ [ v ] + list ( row ) for v , row in zip ( names , vals ) ] \n        else : \n            raise ValueError ( \"tabular data doesn't appear to be a dict \" \"or a DataFrame\" ) \n        if headers == \"keys\" : \n            headers = list ( map ( _text_type , keys ) ) \n    else : \n        rows = list ( tabular_data ) \n        if headers == \"keys\" and 0 < len ( rows ) : \n            headers = list ( map ( _text_type , list ( range ( len ( rows [ 0 ] ) ) ) ) ) \n    if headers == \"firstrow\" and 0 < len ( rows ) : \n        headers = list ( map ( _text_type , rows [ 0 ] ) ) \n        rows = rows [ 1 : ] \n    headers = list ( headers ) \n    rows = list ( map ( list , rows ) ) \n    if sort and 1 < len ( rows ) : \n        rows = sorted ( rows , key = lambda x : x [ 0 ] ) \n    if headers and 0 < len ( rows ) : \n        nhs = len ( headers ) \n        ncols = len ( rows [ 0 ] ) \n        if ncols > nhs : \n            headers = [ \"\" ] * ( ncols - nhs ) + headers \n    return rows , headers "}
{"7366": "\ndef _build_row ( cells , padding , begin , sep , end ) : \n    pad = \" \" * padding \n    padded_cells = [ pad + cell + pad for cell in cells ] \n    rendered_cells = ( begin + sep . join ( padded_cells ) + end ) . rstrip ( ) \n    if TTY_COLS < len ( rendered_cells ) : \n        if not cells [ - 1 ] . endswith ( \" \" ) and not cells [ - 1 ] . endswith ( \"-\" ) : \n            terminating_str = \" ... \" \n        else : \n            terminating_str = \"\" \n        rendered_cells = \"{0}{1}{2}\" . format ( rendered_cells [ : TTY_COLS - len ( terminating_str ) - 1 ] , terminating_str , end ) \n    return rendered_cells "}
{"7391": "\ndef truncate_string ( value , max_width = None ) : \n    if isinstance ( value , text_type ) and max_width is not None and max_width < len ( value ) : \n        return value [ : max_width ] \n    return value "}
{"7411": "\ndef _insert_img ( qr_img , icon_img = None , factor = 4 , icon_box = None , static_dir = None ) : \n    img_w , img_h = qr_img . size \n    size_w = int ( img_w ) / int ( factor ) \n    size_h = int ( img_h ) / int ( factor ) \n    try : \n        icon_fp = os . path . join ( icon_img ) \n        if static_dir : \n            icon_fp = os . path . join ( static_dir , icon_img ) \n        if icon_img . split ( \"://\" ) [ 0 ] in [ \"http\" , \"https\" , \"ftp\" ] : \n            icon_fp = BytesIO ( urlopen ( icon_img ) . read ( ) ) \n        icon = Image . open ( icon_fp ) \n    except : \n        return qr_img \n    icon_w , icon_h = icon . size \n    icon_w = size_w if size_w < icon_w else icon_w \n    icon_h = size_h if size_h < icon_h else icon_h \n    icon = icon . resize ( ( int ( icon_w ) , int ( icon_h ) ) , Image . ANTIALIAS ) \n    icon = icon . convert ( \"RGBA\" ) \n    left = int ( ( img_w - icon_w ) / 2 ) \n    top = int ( ( img_h - icon_h ) / 2 ) \n    icon_box = ( int ( icon_box [ 0 ] ) , int ( icon_box [ 1 ] ) ) if icon_box else ( left , top ) \n    qr_img . paste ( im = icon , box = icon_box , mask = icon ) \n    return qr_img "}
{"7414": "\ndef repeat ( self , day = None ) : \n    if day is None : \n        day = self . day \n    try : \n        d = date ( self . year , self . month , day ) \n    except ValueError : \n        return self . count \n    if self . count_first and self . end_repeat >= d : \n        self . count_it ( d . day ) \n    d += timedelta ( days = self . num ) \n    if self . end_on is not None : \n        while d . month == self . month and self . end_repeat >= d and self . end_on >= d . day : \n            self . count_it ( d . day ) \n            d += timedelta ( days = self . num ) \n    else : \n        while d . month == self . month and self . end_repeat >= d : \n            self . count_it ( d . day ) \n            d += timedelta ( days = self . num ) "}
{"7415": "\ndef repeat_reverse ( self , start , end ) : \n    day = start \n    diff = start - end \n    try : \n        if self . end_repeat >= date ( self . year , self . month , day ) : \n            self . count_it ( day ) \n    except ValueError : \n        pass \n    for i in xrange ( diff ) : \n        day -= 1 \n        try : \n            if self . end_repeat >= date ( self . year , self . month , day ) : \n                self . count_it ( day ) \n        except ValueError : \n            pass "}
{"7457": "\ndef sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : \n    skip_count = ( per_page * max ( page - 1 , 0 ) ) \n    more_variants = True if ( skip_count + per_page ) < variants_query . count ( ) else False \n    genome_build = case_obj . get ( 'genome_build' , '37' ) \n    if genome_build not in [ '37' , '38' ] : \n        genome_build = '37' \n    return { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , } "}
{"7463": "\ndef get_variant_info ( genes ) : \n    data = { 'canonical_transcripts' : [ ] } \n    for gene_obj in genes : \n        if not gene_obj . get ( 'canonical_transcripts' ) : \n            tx = gene_obj [ 'transcripts' ] [ 0 ] \n            tx_id = tx [ 'transcript_id' ] \n            exon = tx . get ( 'exon' , '-' ) \n            c_seq = tx . get ( 'coding_sequence_name' , '-' ) \n        else : \n            tx_id = gene_obj [ 'canonical_transcripts' ] \n            exon = gene_obj . get ( 'exon' , '-' ) \n            c_seq = gene_obj . get ( 'hgvs_identifier' , '-' ) \n        if 20 < len ( c_seq ) : \n            c_seq = c_seq [ : 20 ] + '...' \n        if len ( genes ) == 1 : \n            value = ':' . join ( [ tx_id , exon , c_seq ] ) \n        else : \n            gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) \n            value = ':' . join ( [ gene_id , tx_id , exon , c_seq ] ) \n        data [ 'canonical_transcripts' ] . append ( value ) \n    return data "}
{"7465": "\ndef variant_case ( store , case_obj , variant_obj ) : \n    case_obj [ 'bam_files' ] = [ ] \n    case_obj [ 'mt_bams' ] = [ ] \n    case_obj [ 'bai_files' ] = [ ] \n    case_obj [ 'mt_bais' ] = [ ] \n    case_obj [ 'sample_names' ] = [ ] \n    for individual in case_obj [ 'individuals' ] : \n        bam_path = individual . get ( 'bam_file' ) \n        mt_bam = individual . get ( 'mt_bam' ) \n        case_obj [ 'sample_names' ] . append ( individual . get ( 'display_name' ) ) \n        if bam_path and os . path . exists ( bam_path ) : \n            case_obj [ 'bam_files' ] . append ( individual [ 'bam_file' ] ) \n            case_obj [ 'bai_files' ] . append ( find_bai_file ( individual [ 'bam_file' ] ) ) \n        if mt_bam and os . path . exists ( mt_bam ) : \n            case_obj [ 'mt_bams' ] . append ( individual [ 'mt_bam' ] ) \n            case_obj [ 'mt_bais' ] . append ( find_bai_file ( individual [ 'mt_bam' ] ) ) \n        else : \n            LOG . debug ( \"%s: no bam file found\" , individual [ 'individual_id' ] ) \n    try : \n        genes = variant_obj . get ( 'genes' , [ ] ) \n        if len ( genes ) == 1 : \n            hgnc_gene_obj = store . hgnc_gene ( variant_obj [ 'genes' ] [ 0 ] [ 'hgnc_id' ] ) \n            if hgnc_gene_obj : \n                vcf_path = store . get_region_vcf ( case_obj , gene_obj = hgnc_gene_obj ) \n                case_obj [ 'region_vcf_file' ] = vcf_path \n            else : \n                case_obj [ 'region_vcf_file' ] = None \n        elif 1 < len ( genes ) : \n            chrom = variant_obj [ 'genes' ] [ 0 ] [ 'common' ] [ 'chromosome' ] \n            start = min ( gene [ 'common' ] [ 'start' ] for gene in variant_obj [ 'genes' ] ) \n            end = max ( gene [ 'common' ] [ 'end' ] for gene in variant_obj [ 'genes' ] ) \n            vcf_path = store . get_region_vcf ( case_obj , chrom = chrom , start = start , end = end ) \n            case_obj [ 'region_vcf_file' ] = vcf_path \n    except ( SyntaxError , Exception ) : \n        LOG . warning ( \"skip VCF region for alignment view\" ) "}
{"7471": "\ndef frequency ( variant_obj ) : \n    most_common_frequency = max ( variant_obj . get ( 'thousand_genomes_frequency' ) or 0 , variant_obj . get ( 'exac_frequency' ) or 0 ) \n    if .05 < most_common_frequency : \n        return 'common' \n    elif .01 < most_common_frequency : \n        return 'uncommon' \n    else : \n        return 'rare' "}
{"7477": "\ndef spidex_human ( variant_obj ) : \n    if variant_obj . get ( 'spidex' ) is None : \n        return 'not_reported' \n    elif SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ 1 ] > abs ( variant_obj [ 'spidex' ] ) : \n        return 'low' \n    elif SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] > abs ( variant_obj [ 'spidex' ] ) : \n        return 'medium' \n    else : \n        return 'high' "}
{"7500": "\ndef get_next_and_prev ( net ) : \n    if net == 0 : \n        nxt = prev = 1 \n    elif 0 < net : \n        nxt = net + 1 \n        prev = - ( net - 1 ) \n    else : \n        nxt = net + 1 \n        prev = abs ( net ) + 1 \n    return nxt , prev "}
{"7511": "\ndef is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if ps_terms : \n            return True \n        if pm_terms : \n            if pp_terms : \n                return True \n            if 2 <= len ( pm_terms ) : \n                return True \n        if 2 <= len ( pp_terms ) : \n            return True \n    if ps_terms : \n        if 2 <= len ( ps_terms ) : \n            return True \n        if pm_terms : \n            if 3 <= len ( pm_terms ) : \n                return True \n            elif 2 <= len ( pm_terms ) : \n                if 2 <= len ( pp_terms ) : \n                    return True \n            elif 4 <= len ( pp_terms ) : \n                return True \n    return False "}
{"7512": "\ndef is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if pm_terms : \n            return True \n    if ps_terms : \n        if pm_terms : \n            return True \n        if 2 <= len ( pp_terms ) : \n            return True \n    if pm_terms : \n        if 3 <= len ( pm_terms ) : \n            return True \n        elif 2 <= len ( pm_terms ) : \n            if 2 <= len ( pp_terms ) : \n                return True \n        elif 4 <= len ( pp_terms ) : \n            return True \n    return False "}
{"7513": "\ndef is_likely_benign ( bs_terms , bp_terms ) : \n    if bs_terms : \n        if bp_terms : \n            return True \n    if 2 <= len ( bp_terms ) : \n        return True \n    return False "}
{"7527": "\ndef get_region_vcf ( self , case_obj , chrom = None , start = None , end = None , gene_obj = None , variant_type = 'clinical' , category = 'snv' , rank_threshold = None ) : \n    rank_threshold = rank_threshold or - 100 \n    variant_file = None \n    if variant_type == 'clinical' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv' ) \n        elif category == 'sv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) \n        elif category == 'str' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) \n    elif variant_type == 'research' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) \n        elif category == 'sv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) \n    if not variant_file : \n        raise SyntaxError ( \"Vcf file does not seem to exist\" ) \n    vcf_obj = VCF ( variant_file ) \n    region = \"\" \n    if gene_obj : \n        chrom = gene_obj [ 'chromosome' ] \n        start = gene_obj [ 'start' ] \n        end = gene_obj [ 'end' ] \n    if chrom : \n        if ( start and end ) : \n            region = \"{0}:{1}-{2}\" . format ( chrom , start , end ) \n        else : \n            region = \"{0}\" . format ( chrom ) \n    else : \n        rank_threshold = rank_threshold or 5 \n    with tempfile . NamedTemporaryFile ( mode = 'w' , delete = False ) as temp : \n        file_name = str ( pathlib . Path ( temp . name ) ) \n        for header_line in vcf_obj . raw_header . split ( '\\n' ) : \n            if 3 < len ( header_line ) : \n                temp . write ( header_line + '\\n' ) \n        for variant in vcf_obj ( region ) : \n            temp . write ( str ( variant ) ) \n    return file_name "}
{"7533": "\ndef load_transcripts ( adapter , transcripts_lines = None , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    if transcripts_lines is None : \n        transcripts_lines = fetch_ensembl_transcripts ( build = build ) \n    transcripts_dict = parse_transcripts ( transcripts_lines ) \n    for ens_tx_id in list ( transcripts_dict ) : \n        parsed_tx = transcripts_dict [ ens_tx_id ] \n        ens_gene_id = parsed_tx [ 'ensembl_gene_id' ] \n        gene_obj = ensembl_genes . get ( ens_gene_id ) \n        if not gene_obj : \n            transcripts_dict . pop ( ens_tx_id ) \n            LOG . debug ( \"Gene %s does not exist in build %s\" , ens_gene_id , build ) \n            continue \n        parsed_tx [ 'hgnc_id' ] = gene_obj [ 'hgnc_id' ] \n        parsed_tx [ 'primary_transcripts' ] = set ( gene_obj . get ( 'primary_transcripts' , [ ] ) ) \n    ref_seq_transcripts = 0 \n    nr_primary_transcripts = 0 \n    nr_transcripts = len ( transcripts_dict ) \n    transcript_objs = [ ] \n    with progressbar ( transcripts_dict . values ( ) , label = \"Building transcripts\" , length = nr_transcripts ) as bar : \n        for tx_data in bar : \n            tx_data [ 'is_primary' ] = False \n            primary_transcripts = tx_data [ 'primary_transcripts' ] \n            refseq_identifier = None \n            refseq_identifiers = [ ] \n            for category in TRANSCRIPT_CATEGORIES : \n                identifiers = tx_data [ category ] \n                if not identifiers : \n                    continue \n                for refseq_id in identifiers : \n                    refseq_identifiers . append ( refseq_id ) \n                    ref_seq_transcripts += 1 \n                    if refseq_id in primary_transcripts : \n                        refseq_identifier = refseq_id \n                        tx_data [ 'is_primary' ] = True \n                        nr_primary_transcripts += 1 \n                    if not refseq_identifier : \n                        refseq_identifier = refseq_id \n            if refseq_identifier : \n                tx_data [ 'refseq_id' ] = refseq_identifier \n            if refseq_identifiers : \n                tx_data [ 'refseq_identifiers' ] = refseq_identifiers \n            tx_obj = build_transcript ( tx_data , build ) \n            transcript_objs . append ( tx_obj ) \n    LOG . info ( \"Loading transcripts...\" ) \n    if 0 < len ( transcript_objs ) : \n        adapter . load_transcript_bulk ( transcript_objs ) \n    LOG . info ( 'Number of transcripts in build %s: %s' , build , nr_transcripts ) \n    LOG . info ( 'Number of transcripts with refseq identifier: %s' , ref_seq_transcripts ) \n    LOG . info ( 'Number of primary transcripts: %s' , nr_primary_transcripts ) \n    return transcript_objs "}
{"7544": "\ndef cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n    else : \n        models = adapter . cases ( collaborator = institute , name_query = display_name ) \n        models = [ case_obj for case_obj in models ] \n    if not models : \n        LOG . info ( \"No cases could be found\" ) \n        return \n    header = [ 'case_id' , 'display_name' , 'institute' ] \n    if variants_treshold : \n        LOG . info ( \"Only show cases with more than %s variants\" , variants_treshold ) \n        nr_variants = True \n    if nr_variants : \n        LOG . info ( \"Displaying number of variants for each case\" ) \n        header . append ( 'clinical' ) \n        header . append ( 'research' ) \n    click . echo ( \"#\" + '\\t' . join ( header ) ) \n    for model in models : \n        output_str = \"{:<12}\\t{:<12}\\t{:<12}\" \n        output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] \n        if nr_variants : \n            output_str += \"\\t{:<12}\\t{:<12}\" \n            nr_clinical = 0 \n            nr_research = 0 \n            variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) \n            i = 0 \n            for i , var in enumerate ( variants , 1 ) : \n                if var [ 'variant_type' ] == 'clinical' : \n                    nr_clinical += 1 \n                else : \n                    nr_research += 1 \n            output_values . extend ( [ nr_clinical , nr_research ] ) \n            if variants_treshold and variants_treshold > i : \n                LOG . debug ( \"Case %s had to few variants, skipping\" , model [ '_id' ] ) \n                continue \n        click . echo ( output_str . format ( * output_values ) ) "}
{"7556": "\ndef check_coordinates ( chromosome , pos , coordinates ) : \n    chrom_match = CHR_PATTERN . match ( chromosome ) \n    chrom = chrom_match . group ( 2 ) \n    if chrom != coordinates [ 'chrom' ] : \n        return False \n    if ( coordinates [ 'start' ] <= pos and coordinates [ 'end' ] >= pos ) : \n        return True \n    return False "}
{"7571": "\ndef research ( context , case_id , institute , force ) : \n    LOG . info ( \"Running scout load research\" ) \n    adapter = context . obj [ 'adapter' ] \n    if case_id : \n        if not institute : \n            splitted_case = case_id . split ( '-' ) \n            if 1 < len ( splitted_case ) : \n                institute_obj = adapter . institute ( splitted_case [ 0 ] ) \n                if institute_obj : \n                    institute = institute_obj [ '_id' ] \n                    case_id = splitted_case [ 1 ] \n        case_obj = adapter . case ( institute_id = institute , case_id = case_id ) \n        if case_obj is None : \n            LOG . warning ( \"No matching case found\" ) \n            context . abort ( ) \n        else : \n            case_objs = [ case_obj ] \n    else : \n        case_objs = adapter . cases ( research_requested = True ) \n    default_threshold = 8 \n    files = False \n    for case_obj in case_objs : \n        if force or case_obj [ 'research_requested' ] : \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'snv' ) \n                LOG . info ( \"Load research SNV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'snv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'sv' ) \n                LOG . info ( \"Load research SV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'sv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'cancer' ) \n                LOG . info ( \"Load research cancer for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'cancer' , rank_threshold = default_threshold , ) \n            if not files : \n                LOG . warning ( \"No research files found for case %s\" , case_id ) \n                context . abort ( ) \n            case_obj [ 'is_research' ] = True \n            case_obj [ 'research_requested' ] = False \n            adapter . update_case ( case_obj ) \n        else : \n            LOG . warn ( \"research not requested, use '--force'\" ) "}
{"7573": "\ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while 7 > len ( term ) : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    elif description : \n        sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n        for term in sorted_terms : \n            term . pop ( 'genes' ) \n            print ( \"name: {} | {} | {}\" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) \n        context . abort ( ) \n    else : \n        hpo_terms = adapter . hpo_terms ( ) \n    if hpo_terms . count ( ) == 0 : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7612": "\ndef parse_hgnc_genes ( lines ) : \n    header = [ ] \n    logger . info ( \"Parsing hgnc genes...\" ) \n    for index , line in enumerate ( lines ) : \n        if index == 0 : \n            header = line . split ( '\\t' ) \n        elif 1 < len ( line ) : \n            hgnc_gene = parse_hgnc_line ( line = line , header = header ) \n            if hgnc_gene : \n                yield hgnc_gene "}
{"7637": "\ndef parse_sv_frequencies ( variant ) : \n    frequency_keys = [ 'clingen_cgh_benignAF' , 'clingen_cgh_benign' , 'clingen_cgh_pathogenicAF' , 'clingen_cgh_pathogenic' , 'clingen_ngi' , 'clingen_ngiAF' , 'swegen' , 'swegenAF' , 'decipherAF' , 'decipher' ] \n    sv_frequencies = { } \n    for key in frequency_keys : \n        value = variant . INFO . get ( key , 0 ) \n        if 'AF' in key : \n            value = float ( value ) \n        else : \n            value = int ( value ) \n        if 0 < value : \n            sv_frequencies [ key ] = value \n    return sv_frequencies "}
{"7647": "\ndef gene_panel ( self , panel_id , version = None ) : \n    query = { 'panel_name' : panel_id } \n    if version : \n        LOG . info ( \"Fetch gene panel {0}, version {1} from database\" . format ( panel_id , version ) ) \n        query [ 'version' ] = version \n        return self . panel_collection . find_one ( query ) \n    else : \n        LOG . info ( \"Fetching gene panels %s from database\" , panel_id ) \n        res = self . panel_collection . find ( query ) . sort ( 'version' , - 1 ) \n        if 0 < res . count ( ) : \n            return res [ 0 ] \n        else : \n            LOG . info ( \"No gene panel found\" ) \n            return None "}
{"7670": "\ndef hgnc_gene ( self , hgnc_identifier , build = '37' ) : \n    if not build in [ '37' , '38' ] : \n        build = '37' \n    query = { } \n    try : \n        hgnc_identifier = int ( hgnc_identifier ) \n        query [ 'hgnc_id' ] = hgnc_identifier \n    except ValueError : \n        query [ 'hgnc_symbol' ] = hgnc_identifier \n    query [ 'build' ] = build \n    LOG . debug ( \"Fetching gene %s\" % hgnc_identifier ) \n    gene_obj = self . hgnc_collection . find_one ( query ) \n    if not gene_obj : \n        return None \n    transcripts = [ ] \n    tx_objs = self . transcripts ( build = build , hgnc_id = gene_obj [ 'hgnc_id' ] ) \n    if 0 < tx_objs . count ( ) : \n        for tx in tx_objs : \n            transcripts . append ( tx ) \n    gene_obj [ 'transcripts' ] = transcripts \n    return gene_obj "}
{"7671": "\ndef hgnc_id ( self , hgnc_symbol , build = '37' ) : \n    query = { 'hgnc_symbol' : hgnc_symbol , 'build' : build } \n    projection = { 'hgnc_id' : 1 , '_id' : 0 } \n    res = self . hgnc_collection . find ( query , projection ) \n    if 0 < res . count ( ) : \n        return res [ 0 ] [ 'hgnc_id' ] \n    else : \n        return None "}
{"7684": "\ndef add_hgnc_id ( self , genes ) : \n    genes_by_alias = self . genes_by_alias ( ) \n    for gene in genes : \n        id_info = genes_by_alias . get ( gene [ 'hgnc_symbol' ] ) \n        if not id_info : \n            LOG . warning ( \"Gene %s does not exist in scout\" , gene [ 'hgnc_symbol' ] ) \n            continue \n        gene [ 'hgnc_id' ] = id_info [ 'true' ] \n        if not id_info [ 'true' ] : \n            if 1 < len ( id_info [ 'ids' ] ) : \n                LOG . warning ( \"Gene %s has ambiguous value, please choose one hgnc id in result\" , gene [ 'hgnc_symbol' ] ) \n            gene [ 'hgnc_id' ] = ',' . join ( [ str ( hgnc_id ) for hgnc_id in id_info [ 'ids' ] ] ) "}
{"7685": "\ndef get_coding_intervals ( self , build = '37' , genes = None ) : \n    intervals = { } \n    if not genes : \n        genes = self . all_genes ( build = build ) \n    LOG . info ( \"Building interval trees...\" ) \n    for i , hgnc_obj in enumerate ( genes ) : \n        chrom = hgnc_obj [ 'chromosome' ] \n        start = max ( ( hgnc_obj [ 'start' ] - 5000 ) , 1 ) \n        end = hgnc_obj [ 'end' ] + 5000 \n        if chrom not in intervals : \n            intervals [ chrom ] = intervaltree . IntervalTree ( ) \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        res = intervals [ chrom ] . search ( start , end ) \n        if not res : \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        for interval in res : \n            if start > interval . begin : \n                start = interval . begin \n            if end < interval . end : \n                end = interval . end \n            intervals [ chrom ] . remove ( interval ) \n        intervals [ chrom ] . addi ( start , end , i ) \n    return intervals "}
{"7687": "\ndef cases ( institute_id ) : \n    institute_obj = institute_and_case ( store , institute_id ) \n    query = request . args . get ( 'query' ) \n    limit = 100 \n    if request . args . get ( 'limit' ) : \n        limit = int ( request . args . get ( 'limit' ) ) \n    skip_assigned = request . args . get ( 'skip_assigned' ) \n    is_research = request . args . get ( 'is_research' ) \n    all_cases = store . cases ( collaborator = institute_id , name_query = query , skip_assigned = skip_assigned , is_research = is_research ) \n    data = controllers . cases ( store , all_cases , limit ) \n    sanger_unevaluated = controllers . get_sanger_unevaluated ( store , institute_id , current_user . email ) \n    if 0 < len ( sanger_unevaluated ) : \n        data [ 'sanger_unevaluated' ] = sanger_unevaluated \n    return dict ( institute = institute_obj , skip_assigned = skip_assigned , is_research = is_research , query = query , ** data ) "}
{"7696": "\ndef phenotypes_actions ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    action = request . form [ 'action' ] \n    hpo_ids = request . form . getlist ( 'hpo_id' ) \n    user_obj = store . user ( current_user . email ) \n    if action == 'DELETE' : \n        for hpo_id in hpo_ids : \n            store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_id ) \n    elif action == 'PHENOMIZER' : \n        if len ( hpo_ids ) == 0 : \n            hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n        username = current_app . config [ 'PHENOMIZER_USERNAME' ] \n        password = current_app . config [ 'PHENOMIZER_PASSWORD' ] \n        diseases = controllers . hpo_diseases ( username , password , hpo_ids ) \n        return render_template ( 'cases/diseases.html' , diseases = diseases , institute = institute_obj , case = case_obj ) \n    elif action == 'GENES' : \n        hgnc_symbols = set ( ) \n        for raw_symbols in request . form . getlist ( 'genes' ) : \n            if raw_symbols : \n                hgnc_symbols . update ( raw_symbol . split ( ' ' , 1 ) [ 0 ] for raw_symbol in raw_symbols . split ( '|' ) ) \n        store . update_dynamic_gene_list ( case_obj , hgnc_symbols = hgnc_symbols ) \n    elif action == 'GENERATE' : \n        if len ( hpo_ids ) == 0 : \n            hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n        results = store . generate_hpo_gene_list ( * hpo_ids ) \n        hpo_count = int ( request . form . get ( 'min_match' ) or 1 ) \n        hgnc_ids = [ result [ 0 ] for result in results if hpo_count <= result [ 1 ] ] \n        store . update_dynamic_gene_list ( case_obj , hgnc_ids = hgnc_ids , phenotype_ids = hpo_ids ) \n    return redirect ( case_url ) "}
{"7709": "\ndef cases ( store , case_query , limit = 100 ) : \n    case_groups = { status : [ ] for status in CASE_STATUSES } \n    for case_obj in case_query . limit ( limit ) : \n        analysis_types = set ( ind [ 'analysis_type' ] for ind in case_obj [ 'individuals' ] ) \n        case_obj [ 'analysis_types' ] = list ( analysis_types ) \n        case_obj [ 'assignees' ] = [ store . user ( user_email ) for user_email in case_obj . get ( 'assignees' , [ ] ) ] \n        case_groups [ case_obj [ 'status' ] ] . append ( case_obj ) \n        case_obj [ 'is_rerun' ] = 0 < len ( case_obj . get ( 'analyses' , [ ] ) ) \n        case_obj [ 'clinvar_variants' ] = store . case_to_clinVars ( case_obj [ '_id' ] ) \n        case_obj [ 'display_track' ] = TRACKS [ case_obj . get ( 'track' , 'rare' ) ] \n    data = { 'cases' : [ ( status , case_groups [ status ] ) for status in CASE_STATUSES ] , 'found_cases' : case_query . count ( ) , 'limit' : limit , } \n    return data "}
{"7715": "\ndef hpo_diseases ( username , password , hpo_ids , p_value_treshold = 1 ) : \n    try : \n        results = query_phenomizer . query ( username , password , * hpo_ids ) \n        diseases = [ result for result in results if p_value_treshold >= result [ 'p_value' ] ] \n        return diseases \n    except SystemExit : \n        return None "}
{"7718": "\ndef get_sanger_unevaluated ( store , institute_id , user_id ) : \n    sanger_ordered_by_case = store . sanger_ordered ( institute_id , user_id ) \n    unevaluated = [ ] \n    for item in sanger_ordered_by_case : \n        case_id = item [ '_id' ] \n        case_obj = store . case ( case_id = case_id ) \n        if not case_obj : \n            continue \n        case_display_name = case_obj . get ( 'display_name' ) \n        varid_list = item [ 'vars' ] \n        unevaluated_by_case = { } \n        unevaluated_by_case [ case_display_name ] = [ ] \n        for var_id in varid_list : \n            variant_obj = store . variant ( document_id = var_id , case_id = case_id ) \n            if variant_obj is None or variant_obj . get ( 'sanger_ordered' ) is None or variant_obj . get ( 'sanger_ordered' ) is False : \n                continue \n            validation = variant_obj . get ( 'validation' , 'not_evaluated' ) \n            if validation in [ 'True positive' , 'False positive' ] : \n                continue \n            unevaluated_by_case [ case_display_name ] . append ( variant_obj [ '_id' ] ) \n        if 0 < len ( unevaluated_by_case [ case_display_name ] ) : \n            unevaluated . append ( unevaluated_by_case ) \n    return unevaluated "}
{"7727": "\ndef parse_cadd ( variant , transcripts ) : \n    cadd = 0 \n    cadd_keys = [ 'CADD' , 'CADD_PHRED' ] \n    for key in cadd_keys : \n        cadd = variant . INFO . get ( key , 0 ) \n        if cadd : \n            return float ( cadd ) \n    for transcript in transcripts : \n        cadd_entry = transcript . get ( 'cadd' ) \n        if ( cadd_entry and cadd < cadd_entry ) : \n            cadd = cadd_entry \n    return cadd "}
{"7730": "\ndef update_variant_rank ( self , case_obj , variant_type = 'clinical' , category = 'snv' ) : \n    variants = self . variant_collection . find ( { 'case_id' : case_obj [ '_id' ] , 'category' : category , 'variant_type' : variant_type , } ) . sort ( 'rank_score' , pymongo . DESCENDING ) \n    LOG . info ( \"Updating variant_rank for all variants\" ) \n    requests = [ ] \n    for index , var_obj in enumerate ( variants ) : \n        if 5000 < len ( requests ) : \n            try : \n                self . variant_collection . bulk_write ( requests , ordered = False ) \n                requests = [ ] \n            except BulkWriteError as err : \n                LOG . warning ( \"Updating variant rank failed\" ) \n                raise err \n        operation = pymongo . UpdateOne ( { '_id' : var_obj [ '_id' ] } , { '$set' : { 'variant_rank' : index + 1 , } } ) \n        requests . append ( operation ) \n    try : \n        self . variant_collection . bulk_write ( requests , ordered = False ) \n    except BulkWriteError as err : \n        LOG . warning ( \"Updating variant rank failed\" ) \n        raise err \n    LOG . info ( \"Updating variant_rank done\" ) "}
{"7737": "\ndef load_variant_bulk ( self , variants ) : \n    if not 0 < len ( variants ) : \n        return \n    LOG . debug ( \"Loading variant bulk\" ) \n    try : \n        result = self . variant_collection . insert_many ( variants ) \n    except ( DuplicateKeyError , BulkWriteError ) as err : \n        for var_obj in variants : \n            try : \n                self . upsert_variant ( var_obj ) \n            except IntegrityError as err : \n                pass \n    return "}
{"7767": "\ndef parse_ensembl_exons ( lines ) : \n    header = [ ] \n    LOG . debug ( \"Parsing ensembl exons...\" ) \n    for index , line in enumerate ( lines ) : \n        if index == 0 : \n            header = line . rstrip ( ) . split ( '\\t' ) \n            continue \n        exon_info = parse_ensembl_line ( line , header ) \n        chrom = exon_info [ 'chrom' ] \n        start = exon_info [ 'exon_start' ] \n        end = exon_info [ 'exon_end' ] \n        transcript = exon_info [ 'ensembl_transcript_id' ] \n        gene = exon_info [ 'ensembl_gene_id' ] \n        rank = exon_info [ 'exon_rank' ] \n        strand = exon_info [ 'strand' ] \n        if strand == 1 : \n            start = max ( start , exon_info . get ( 'utr_5_end' ) or - 1 ) \n            end = min ( end , exon_info . get ( 'utr_3_start' ) or float ( 'inf' ) ) \n        elif strand == - 1 : \n            start = max ( start , exon_info . get ( 'utr_3_end' ) or - 1 ) \n            end = min ( end , exon_info . get ( 'utr_5_start' ) or float ( 'inf' ) ) \n        exon_id = \"-\" . join ( [ chrom , str ( start ) , str ( end ) ] ) \n        if end < start : \n            raise ValueError ( \"ERROR: %s\" % exon_id ) \n        data = { \"exon_id\" : exon_id , \"chrom\" : chrom , \"start\" : start , \"end\" : end , \"transcript\" : transcript , \"gene\" : gene , \"rank\" : rank , } \n        yield data "}
{"7771": "\ndef parse_omim_morbid ( lines ) : \n    header = [ ] \n    for i , line in enumerate ( lines ) : \n        line = line . rstrip ( ) \n        if line . startswith ( '#' ) : \n            if 10 > i : \n                if line . startswith ( '# Phenotype' ) : \n                    header = line [ 2 : ] . split ( '\\t' ) \n        else : \n            yield parse_omim_line ( line , header ) "}
{"7778": "\ndef popover_helper ( self ) : \n    display_month = month_name [ self . mo ] \n    if isinstance ( display_month , six . binary_type ) and self . encoding : \n        display_month = display_month . decode ( 'utf-8' ) \n    self . when = ( '<p><b>When:</b> ' + display_month + ' ' + str ( self . day ) + ', ' + self . event . l_start_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l_end_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + '</p>' ) \n    if self . event . location . exists ( ) : \n        self . where = '<p><b>Where:</b> ' \n        for l in self . event . location . all ( ) : \n            self . where += l . name \n        self . where += '</p>' \n    else : \n        self . where = '' \n    self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] \n    self . desc += ( '...</p>' if 100 < len ( self . event . description ) else '</p>' ) \n    self . event_url = self . event . get_absolute_url ( ) \n    t = LEGACY_CALENDAR_TIME_FORMAT if self . event . l_start_date . minute else LEGACY_CALENDAR_HOUR_FORMAT \n    self . title2 = ( self . event . l_start_date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title ) "}
{"7781": "\ndef parse_genes ( gene_lines ) : \n    genes = [ ] \n    header = [ ] \n    hgnc_identifiers = set ( ) \n    delimiter = '\\t' \n    delimiters = [ '\\t' , ' ' , ';' ] \n    for i , line in enumerate ( gene_lines ) : \n        line = line . rstrip ( ) \n        if not 0 < len ( line ) : \n            continue \n        if line . startswith ( '#' ) : \n            if not line . startswith ( '##' ) : \n                line_length = 0 \n                delimiter = None \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if line_length < len ( head_line ) : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                header = [ word . lower ( ) for word in line [ 1 : ] . split ( delimiter ) ] \n        else : \n            if i == 0 : \n                line_length = 0 \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if line_length < len ( head_line ) : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                if ( 'hgnc' in line or 'HGNC' in line ) : \n                    header = [ word . lower ( ) for word in line . split ( delimiter ) ] \n                    continue \n                if line . split ( delimiter ) [ 0 ] . isdigit ( ) : \n                    header = [ 'hgnc_id' ] \n                else : \n                    header = [ 'hgnc_symbol' ] \n            splitted_line = line . split ( delimiter ) \n            gene_info = dict ( zip ( header , splitted_line ) ) \n            info_found = False \n            for key in gene_info : \n                if gene_info [ key ] : \n                    info_found = True \n                    break \n            if not info_found : \n                continue \n            try : \n                gene = parse_gene ( gene_info ) \n            except Exception as e : \n                LOG . warning ( e ) \n                raise SyntaxError ( \"Line {0} is malformed\" . format ( i + 1 ) ) \n            identifier = gene . pop ( 'identifier' ) \n            if not identifier in hgnc_identifiers : \n                hgnc_identifiers . add ( identifier ) \n                genes . append ( gene ) \n    return genes "}
{"7787": "\ndef parse_conservation ( variant , info_key ) : \n    raw_score = variant . INFO . get ( info_key ) \n    conservations = [ ] \n    if raw_score : \n        if isinstance ( raw_score , numbers . Number ) : \n            raw_score = ( raw_score , ) \n        for score in raw_score : \n            if CONSERVATION [ info_key ] [ 'conserved_min' ] <= score : \n                conservations . append ( 'Conserved' ) \n            else : \n                conservations . append ( 'NotConserved' ) \n    return conservations "}
{"7788": "\ndef get_general_case_info ( adapter , institute_id = None , slice_query = None ) : \n    general = { } \n    name_query = slice_query \n    cases = adapter . cases ( owner = institute_id , name_query = name_query ) \n    phenotype_cases = 0 \n    causative_cases = 0 \n    pinned_cases = 0 \n    cohort_cases = 0 \n    pedigree = { 1 : { 'title' : 'Single' , 'count' : 0 } , 2 : { 'title' : 'Duo' , 'count' : 0 } , 3 : { 'title' : 'Trio' , 'count' : 0 } , 'many' : { 'title' : 'Many' , 'count' : 0 } , } \n    case_ids = set ( ) \n    total_cases = 0 \n    for total_cases , case in enumerate ( cases , 1 ) : \n        if institute_id : \n            case_ids . add ( case [ '_id' ] ) \n        if case . get ( 'phenotype_terms' ) : \n            phenotype_cases += 1 \n        if case . get ( 'causatives' ) : \n            causative_cases += 1 \n        if case . get ( 'suspects' ) : \n            pinned_cases += 1 \n        if case . get ( 'cohorts' ) : \n            cohort_cases += 1 \n        nr_individuals = len ( case . get ( 'individuals' , [ ] ) ) \n        if nr_individuals == 0 : \n            continue \n        if 3 < nr_individuals : \n            pedigree [ 'many' ] [ 'count' ] += 1 \n        else : \n            pedigree [ nr_individuals ] [ 'count' ] += 1 \n    general [ 'total_cases' ] = total_cases \n    general [ 'phenotype_cases' ] = phenotype_cases \n    general [ 'causative_cases' ] = causative_cases \n    general [ 'pinned_cases' ] = pinned_cases \n    general [ 'cohort_cases' ] = cohort_cases \n    general [ 'pedigree' ] = pedigree \n    general [ 'case_ids' ] = case_ids \n    return general "}
{"7800": "\ndef _setup_freqs ( self , f_start = None , f_stop = None ) : \n    f0 = self . header [ b'fch1' ] \n    f_delt = self . header [ b'foff' ] \n    i_start , i_stop = 0 , self . header [ b'nchans' ] \n    if f_start : \n        i_start = int ( ( f_start - f0 ) / f_delt ) \n    if f_stop : \n        i_stop = int ( ( f_stop - f0 ) / f_delt ) \n    chan_start_idx = np . int ( i_start ) \n    chan_stop_idx = np . int ( i_stop ) \n    if i_stop > i_start : \n        i_vals = np . arange ( chan_start_idx , chan_stop_idx ) \n    else : \n        i_vals = np . arange ( chan_stop_idx , chan_start_idx ) \n    self . freqs = f_delt * i_vals + f0 \n    if chan_start_idx > chan_stop_idx : \n        chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx \n    return i_start , i_stop , chan_start_idx , chan_stop_idx "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if MAX_DATA_ARRAY_SIZE < n_ints * n_ifs * n_chans_selected : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7803": "\ndef compute_lst ( self ) : \n    if self . header [ b'telescope_id' ] == 6 : \n        self . coords = gbt_coords \n    elif self . header [ b'telescope_id' ] == 4 : \n        self . coords = parkes_coords \n    else : \n        raise RuntimeError ( \"Currently only Parkes and GBT supported\" ) \n    if HAS_SLALIB : \n        dut1 = 0.0 \n        mjd = self . header [ b'tstart' ] \n        tellong = np . deg2rad ( self . coords [ 1 ] ) \n        last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 \n        if 0.0 > last : \n            last = last + 2.0 * np . pi \n        return last \n    else : \n        raise RuntimeError ( \"This method requires pySLALIB\" ) "}
{"7804": "\ndef blank_dc ( self , n_coarse_chan ) : \n    if 1 > n_coarse_chan : \n        logger . warning ( 'Coarse channel number < 1, unable to blank DC bin.' ) \n        return None \n    if not n_coarse_chan % int ( n_coarse_chan ) == 0 : \n        logger . warning ( 'Selection does not contain an interger number of coarse channels, unable to blank DC bin.' ) \n        return None \n    n_coarse_chan = int ( n_coarse_chan ) \n    n_chan = self . data . shape [ - 1 ] \n    n_chan_per_coarse = int ( n_chan / n_coarse_chan ) \n    mid_chan = int ( n_chan_per_coarse / 2 ) \n    for ii in range ( n_coarse_chan ) : \n        ss = ii * n_chan_per_coarse \n        self . data [ ... , ss + mid_chan ] = np . median ( self . data [ ... , ss + mid_chan + 5 : ss + mid_chan + 10 ] ) "}
{"7807": "\ndef plot_waterfall ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , cb = True , MJD_time = False , ** kwargs ) : \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if 0 > self . header [ b'foff' ] : \n        plot_data = plot_data [ ... , : : - 1 ] \n        plot_f = plot_f [ : : - 1 ] \n    if logged : \n        plot_data = db ( plot_data ) \n    dec_fac_x , dec_fac_y = 1 , 1 \n    if MAX_IMSHOW_POINTS [ 0 ] < plot_data . shape [ 0 ] : \n        dec_fac_x = int ( plot_data . shape [ 0 ] / MAX_IMSHOW_POINTS [ 0 ] ) \n    if MAX_IMSHOW_POINTS [ 1 ] < plot_data . shape [ 1 ] : \n        dec_fac_y = int ( plot_data . shape [ 1 ] / MAX_IMSHOW_POINTS [ 1 ] ) \n    plot_data = rebin ( plot_data , dec_fac_x , dec_fac_y ) \n    try : \n        plt . title ( self . header [ b'source_name' ] ) \n    except KeyError : \n        plt . title ( self . filename ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plt . imshow ( plot_data , aspect = 'auto' , origin = 'lower' , rasterized = True , interpolation = 'nearest' , extent = extent , cmap = 'viridis' , ** kwargs ) \n    if cb : \n        plt . colorbar ( ) \n    plt . xlabel ( \"Frequency [MHz]\" ) \n    if MJD_time : \n        plt . ylabel ( \"Time [MJD]\" ) \n    else : \n        plt . ylabel ( \"Time [s]\" ) "}
{"7808": "\ndef plot_time_series ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , orientation = 'h' , MJD_time = False , ** kwargs ) : \n    ax = plt . gca ( ) \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if logged and 8 <= self . header [ b'nbits' ] : \n        plot_data = db ( plot_data ) \n    if 1 < len ( plot_data . shape ) : \n        plot_data = plot_data . mean ( axis = 1 ) \n    else : \n        plot_data = plot_data . mean ( ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plot_t = np . linspace ( extent [ 2 ] , extent [ 3 ] , len ( self . timestamps ) ) \n    if MJD_time : \n        tlabel = \"Time [MJD]\" \n    else : \n        tlabel = \"Time [s]\" \n    if logged : \n        plabel = \"Power [dB]\" \n    else : \n        plabel = \"Power [counts]\" \n    if 'v' in orientation : \n        plt . plot ( plot_data , plot_t , ** kwargs ) \n        plt . xlabel ( plabel ) \n    else : \n        plt . plot ( plot_t , plot_data , ** kwargs ) \n        plt . xlabel ( tlabel ) \n        plt . ylabel ( plabel ) \n    ax . autoscale ( axis = 'both' , tight = True ) "}
{"7818": "\ndef unpack ( data , nbit ) : \n    if 8 < nbit : \n        raise ValueError ( \"unpack: nbit must be <= 8\" ) \n    if 8 % nbit != 0 : \n        raise ValueError ( \"unpack: nbit must divide into 8\" ) \n    if data . dtype not in ( np . uint8 , np . int8 ) : \n        raise TypeError ( \"unpack: dtype must be 8-bit\" ) \n    if nbit == 8 : \n        return data \n    elif nbit == 4 : \n        data = unpack_4to8 ( data ) \n        return data \n    elif nbit == 2 : \n        data = unpack_2to8 ( data ) \n        return data \n    elif nbit == 1 : \n        data = unpack_1to8 ( data ) \n        return data "}
{"7824": "\ndef _setup_selection_range ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , init = False ) : \n    if init is True : \n        if t_start is None : \n            t_start = self . t_begin \n        if t_stop is None : \n            t_stop = self . t_end \n        if f_start is None : \n            f_start = self . f_begin \n        if f_stop is None : \n            f_stop = self . f_end \n    else : \n        if f_start is None : \n            f_start = self . f_start \n        if f_stop is None : \n            f_stop = self . f_stop \n        if t_start is None : \n            t_start = self . t_start \n        if t_stop is None : \n            t_stop = self . t_stop \n    if 0 <= t_stop and 0 <= t_start and t_start > t_stop : \n        t_stop , t_start = t_start , t_stop \n        logger . warning ( 'Given t_stop < t_start, assuming reversed values.' ) \n    if f_stop and f_start and f_start > f_stop : \n        f_stop , f_start = f_start , f_stop \n        logger . warning ( 'Given f_stop < f_start, assuming reversed values.' ) \n    if self . t_begin <= t_start and self . t_end > t_start : \n        self . t_start = int ( t_start ) \n    else : \n        if init is False or t_start != None : \n            logger . warning ( 'Setting t_start = %f, since t_start not given or not valid.' % self . t_begin ) \n        self . t_start = self . t_begin \n    if self . t_end >= t_stop and self . t_begin < t_stop : \n        self . t_stop = int ( t_stop ) \n    else : \n        if init is False or t_stop : \n            logger . warning ( 'Setting t_stop = %f, since t_stop not given or not valid.' % self . t_end ) \n        self . t_stop = self . t_end \n    if self . f_begin <= f_start and self . f_end > f_start : \n        self . f_start = f_start \n    else : \n        if init is False or f_start : \n            logger . warning ( 'Setting f_start = %f, since f_start not given or not valid.' % self . f_begin ) \n        self . f_start = self . f_begin \n    if self . f_end >= f_stop and self . f_begin < f_stop : \n        self . f_stop = f_stop \n    else : \n        if init is False or f_stop : \n            logger . warning ( 'Setting f_stop = %f, since f_stop not given or not valid.' % self . f_end ) \n        self . f_stop = self . f_end \n    self . selection_shape = self . _calc_selection_shape ( ) "}
{"7827": "\ndef _setup_chans ( self ) : \n    if 0 > self . header [ b'foff' ] : \n        f0 = self . f_end \n    else : \n        f0 = self . f_begin \n    i_start , i_stop = 0 , self . n_channels_in_file \n    if self . f_start : \n        i_start = np . round ( ( self . f_start - f0 ) / self . header [ b'foff' ] ) \n    if self . f_stop : \n        i_stop = np . round ( ( self . f_stop - f0 ) / self . header [ b'foff' ] ) \n    chan_start_idx = np . int ( i_start ) \n    chan_stop_idx = np . int ( i_stop ) \n    if chan_start_idx > chan_stop_idx : \n        chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx \n    self . chan_start_idx = chan_start_idx \n    self . chan_stop_idx = chan_stop_idx "}
{"7828": "\ndef _setup_freqs ( self ) : \n    if 0 < self . header [ b'foff' ] : \n        self . f_start = self . f_begin + self . chan_start_idx * abs ( self . header [ b'foff' ] ) \n        self . f_stop = self . f_begin + self . chan_stop_idx * abs ( self . header [ b'foff' ] ) \n    else : \n        self . f_start = self . f_end - self . chan_stop_idx * abs ( self . header [ b'foff' ] ) \n        self . f_stop = self . f_end - self . chan_start_idx * abs ( self . header [ b'foff' ] ) "}
{"7830": "\ndef populate_freqs ( self ) : \n    if 0 > self . header [ b'foff' ] : \n        f0 = self . f_end \n    else : \n        f0 = self . f_begin \n    self . _setup_chans ( ) \n    i_vals = np . arange ( self . chan_start_idx , self . chan_stop_idx ) \n    freqs = self . header [ b'foff' ] * i_vals + f0 \n    return freqs "}
{"7831": "\ndef calc_n_coarse_chan ( self , chan_bw = None ) : \n    nchans = int ( self . header [ b'nchans' ] ) \n    if chan_bw is not None : \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / chan_bw ) \n        return n_coarse_chan \n    elif 2 ** 20 <= nchans : \n        if nchans % 2 ** 20 == 0 : \n            n_coarse_chan = nchans // 2 ** 20 \n            return n_coarse_chan \n        elif self . header [ b'telescope_id' ] == 6 : \n            coarse_chan_bw = 2.9296875 \n            bandwidth = abs ( self . f_stop - self . f_start ) \n            n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n            return n_coarse_chan \n        else : \n            logger . warning ( \"Couldn't figure out n_coarse_chan\" ) \n    elif self . header [ b'telescope_id' ] == 6 and 2 ** 20 > nchans : \n        coarse_chan_bw = 2.9296875 \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n        return n_coarse_chan \n    else : \n        logger . warning ( \"This function currently only works for hires BL Parkes or GBT data.\" ) "}
{"7833": "\ndef isheavy ( self ) : \n    selection_size_bytes = self . _calc_selection_size ( ) \n    if self . MAX_DATA_ARRAY_SIZE < selection_size_bytes : \n        return True \n    else : \n        return False "}
{"7838": "\ndef __update_header ( self ) : \n    if 0 > self . header [ b'foff' ] : \n        self . header [ b'fch1' ] = self . container . f_stop \n    else : \n        self . header [ b'fch1' ] = self . container . f_start \n    self . header [ b'nchans' ] = self . container . selection_shape [ self . freq_axis ] \n    self . header [ b'tstart' ] = self . container . populate_timestamps ( update_header = True ) "}
{"7843": "\ndef __get_blob_dimensions ( self , chunk_dim ) : \n    if chunk_dim [ self . freq_axis ] * MAX_BLOB_MB < self . selection_shape [ self . freq_axis ] : \n        freq_axis_size = self . selection_shape [ self . freq_axis ] \n        time_axis_size = 1 \n    else : \n        freq_axis_size = self . selection_shape [ self . freq_axis ] \n        time_axis_size = np . min ( [ chunk_dim [ self . time_axis ] * MAX_BLOB_MB * chunk_dim [ self . freq_axis ] / freq_axis_size , self . selection_shape [ self . time_axis ] ] ) \n    blob_dim = ( int ( time_axis_size ) , 1 , freq_axis_size ) \n    return blob_dim "}
{"7844": "\ndef __get_chunk_dimensions ( self ) : \n    if 1e-5 > np . abs ( self . header [ b'foff' ] ) : \n        logger . info ( 'Detecting high frequency resolution data.' ) \n        chunk_dim = ( 1 , 1 , 1048576 ) \n        return chunk_dim \n    elif 1e-3 > np . abs ( self . header [ b'tsamp' ] ) : \n        logger . info ( 'Detecting high time resolution data.' ) \n        chunk_dim = ( 2048 , 1 , 512 ) \n        return chunk_dim \n    elif 1e-2 > np . abs ( self . header [ b'foff' ] ) and 1e-5 <= np . abs ( self . header [ b'foff' ] ) : \n        logger . info ( 'Detecting intermediate frequency and time resolution data.' ) \n        chunk_dim = ( 10 , 1 , 65536 ) \n        return chunk_dim \n    else : \n        logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) \n        chunk_dim = ( 1 , 1 , 512 ) \n        return chunk_dim "}
{"7845": "\ndef grab_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , if_id = 0 ) : \n    self . freqs = self . populate_freqs ( ) \n    self . timestamps = self . populate_timestamps ( ) \n    if f_start is None : \n        f_start = self . freqs [ 0 ] \n    if f_stop is None : \n        f_stop = self . freqs [ - 1 ] \n    i0 = np . argmin ( np . abs ( self . freqs - f_start ) ) \n    i1 = np . argmin ( np . abs ( self . freqs - f_stop ) ) \n    if i1 > i0 : \n        plot_f = self . freqs [ i0 : i1 + 1 ] \n        plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i0 : i1 + 1 ] ) \n    else : \n        plot_f = self . freqs [ i1 : i0 + 1 ] \n        plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i1 : i0 + 1 ] ) \n    return plot_f , plot_data "}
{"7856": "\ndef integrate_calib ( name , chan_per_coarse , fullstokes = False , ** kwargs ) : \n    obs = Waterfall ( name , max_load = 150 ) \n    data = obs . data \n    if fullstokes == False and 1 < data . shape [ 1 ] : \n        data = data [ : , 0 , : ] + data [ : , 1 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    if fullstokes == True : \n        data = data [ : , 0 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    tsamp = obs . header [ 'tsamp' ] \n    OFF , ON = foldcal ( data , tsamp , ** kwargs ) \n    freqs = obs . populate_freqs ( ) \n    ON_int = integrate_chans ( ON , freqs , chan_per_coarse ) \n    OFF_int = integrate_chans ( OFF , freqs , chan_per_coarse ) \n    if np . sum ( OFF_int ) > np . sum ( ON_int ) : \n        temp = ON_int \n        ON_int = OFF_int \n        OFF_int = temp \n    return OFF_int , ON_int "}
{"7870": "\ndef make_rr_subparser ( subparsers , rec_type , args_and_types ) : \n    sp = subparsers . add_parser ( rec_type ) \n    sp . add_argument ( \"name\" , type = str ) \n    sp . add_argument ( \"ttl\" , type = int , nargs = '?' ) \n    sp . add_argument ( rec_type , type = str ) \n    for my_spec in args_and_types : \n        ( argname , argtype ) = my_spec [ : 2 ] \n        if 2 < len ( my_spec ) : \n            nargs = my_spec [ 2 ] \n            sp . add_argument ( argname , type = argtype , nargs = nargs ) \n        else : \n            sp . add_argument ( argname , type = argtype ) \n    return sp "}
{"7874": "\ndef parse_line ( parser , record_token , parsed_records ) : \n    global SUPPORTED_RECORDS \n    line = \" \" . join ( record_token ) \n    if 2 <= len ( record_token ) and record_token [ 1 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 1 ] ] + record_token \n    elif 3 <= len ( record_token ) and record_token [ 2 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 2 ] ] + record_token \n        if record_token [ 0 ] == \"TXT\" : \n            record_token = record_token [ : 2 ] + [ \"--ttl\" ] + record_token [ 2 : ] \n    try : \n        rr , unmatched = parser . parse_known_args ( record_token ) \n        assert len ( unmatched ) == 0 , \"Unmatched fields: %s\" % unmatched \n    except ( SystemExit , AssertionError , InvalidLineException ) : \n        raise InvalidLineException ( line ) \n    record_dict = rr . __dict__ \n    if record_token [ 0 ] == \"TXT\" and len ( record_dict [ 'txt' ] ) == 1 : \n        record_dict [ 'txt' ] = record_dict [ 'txt' ] [ 0 ] \n    record_type = None \n    for key in record_dict . keys ( ) : \n        if key in SUPPORTED_RECORDS and ( key . startswith ( \"$\" ) or record_dict [ key ] == key ) : \n            record_type = key \n            if record_dict [ key ] == key : \n                del record_dict [ key ] \n            break \n    assert record_type is not None , \"Unknown record type in %s\" % rr \n    for field in record_dict . keys ( ) : \n        if record_dict [ field ] is None : \n            del record_dict [ field ] \n    current_origin = record_dict . get ( '$ORIGIN' , parsed_records . get ( '$ORIGIN' , None ) ) \n    if record_type == 'PTR' : \n        record_dict [ 'fullname' ] = record_dict [ 'name' ] + '.' + current_origin \n    if 0 < len ( record_dict ) : \n        if record_type . startswith ( \"$\" ) : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] = record_dict [ record_type ] \n        else : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] . append ( record_dict ) \n    return parsed_records "}
{"7914": "\ndef makePlot ( pdf = False , png = False ) : \n    logdistancekpc = np . linspace ( - 1 , np . log10 ( 20.0 ) , 100 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    lines = { } \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ 0 ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( 14 < vmag ) & ( 16 > vmag ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ 1 ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ 1 ] ) \n        relparerrors = parerrors * 10 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10 ** logdistancekpc , relparerrors , '--k' , lw = 1 ) \n        plt . loglog ( 10 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n    else : \n        plt . show ( ) "}
{"7917": "\ndef _helpful_failure ( method ) : \n    \n    @ wraps ( method ) \n    def wrapper ( self , val ) : \n        try : \n            return method ( self , val ) \n        except : \n            exc_cls , inst , tb = sys . exc_info ( ) \n            if hasattr ( inst , '_RERAISE' ) : \n                _ , expr , _ , inner_val = Q . __debug_info__ \n                Q . __debug_info__ = QDebug ( self , expr , val , inner_val ) \n                raise \n            if issubclass ( exc_cls , KeyError ) : \n                exc_cls = QKeyError \n            prettyval = repr ( val ) \n            if 150 < len ( prettyval ) : \n                prettyval = \"<%s instance>\" % ( type ( val ) . __name__ ) \n            msg = \"{0}\\n\\n\\tEncountered when evaluating {1}{2}\" . format ( inst , prettyval , self ) \n            new_exc = exc_cls ( msg ) \n            new_exc . _RERAISE = True \n            Q . __debug_info__ = QDebug ( self , self , val , val ) \n            six . reraise ( exc_cls , new_exc , tb ) \n    return wrapper "}
{"7931": "\ndef report ( self , output_file = sys . stdout ) : \n    max_perf = self . results [ 'max_perf' ] \n    if self . _args and 3 <= self . _args . verbose : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . _args and 1 <= self . _args . verbose : \n        print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( max_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n    if max_perf [ 'FLOP/s' ] < self . results [ 'min performance' ] [ 'FLOP/s' ] : \n        print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max_perf ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7932": "\ndef report ( self , output_file = sys . stdout ) : \n    cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] \n    if 3 <= self . verbose : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if 1 <= self . verbose : \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            if b is None : \n                continue \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n        print ( 'IACA analisys:' , file = output_file ) \n        print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output_file ) \n    if cpu_perf [ 'FLOP/s' ] < self . results [ 'min performance' ] [ 'FLOP/s' ] : \n        print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7933": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and 2 < self . _args . verbose : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        print ( \"{}D layer condition:\" . format ( dimension ) , file = output_file ) \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            print ( cache + \": \" , end = '' , file = output_file ) \n            if lc_solution [ 'lt' ] is sympy . true : \n                print ( \"unconditionally fulfilled\" , file = output_file ) \n            else : \n                if lc_solution [ 'eq' ] is None : \n                    print ( \"{}\" . format ( lc_solution [ 'lt' ] ) , file = output_file ) \n                elif type ( lc_solution [ 'eq' ] ) is not list : \n                    print ( \"{}\" . format ( lc_solution [ 'eq' ] ) , file = output_file ) \n                else : \n                    for solu in lc_solution [ 'eq' ] : \n                        for s , v in solu . items ( ) : \n                            print ( \"{} <= {}\" . format ( s , v ) , file = output_file ) "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while len ( code ) - 1 > idx : \n            if comment_start is None and code [ idx : idx + 2 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            elif comment_start is None and code [ idx : idx + 2 ] == '/*' : \n                comment_start = idx \n            elif comment_start is not None and code [ idx : idx + 2 ] == '*/' : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                idx -= idx - comment_start \n                comment_start = None \n            idx += 1 \n    return code "}
{"7940": "\ndef analyze ( self ) : \n    try : \n        incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = 2 < self . verbose ) \n    except RuntimeError as e : \n        print ( \"IACA analysis failed: \" + str ( e ) ) \n        sys . exit ( 1 ) \n    block_throughput = incore_analysis [ 'throughput' ] \n    port_cycles = incore_analysis [ 'port cycles' ] \n    uops = incore_analysis [ 'uops' ] \n    elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) \n    block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] \n    try : \n        block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size \n    except ZeroDivisionError as e : \n        print ( \"Too small block_size / pointer_increment:\" , e , file = sys . stderr ) \n        sys . exit ( 1 ) \n    port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) \n    uops = uops * block_to_cl_ratio \n    cl_throughput = block_throughput * block_to_cl_ratio \n    T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) \n    T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) \n    if cl_throughput > T_nOL : \n        T_OL = cl_throughput \n    self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } \n    return self . results "}
{"7950": "\ndef space ( start , stop , num , endpoint = True , log = False , base = 10 ) : \n    assert type ( start ) is int and type ( stop ) is int and type ( num ) is int , \"start, stop and num need to be intergers\" \n    assert 2 <= num , \"num has to be atleast 2\" \n    if log : \n        start = math . log ( start , base ) \n        stop = math . log ( stop , base ) \n    if endpoint : \n        step_length = float ( ( stop - start ) ) / float ( num - 1 ) \n    else : \n        step_length = float ( ( stop - start ) ) / float ( num ) \n    i = 0 \n    while num > i : \n        if log : \n            yield int ( round ( base ** ( start + i * step_length ) ) ) \n        else : \n            yield int ( round ( start + i * step_length ) ) \n        i += 1 "}
{"7960": "\ndef check ( self ) : \n    datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] \n    assert 1 >= len ( set ( datatypes ) ) , 'mixing of datatypes within a kernel is not supported.' "}
{"7985": "\ndef get_kernel_loop_nest ( self ) : \n    loop_nest = [ s for s in self . kernel_ast . block_items if type ( s ) in [ c_ast . For , c_ast . Pragma , c_ast . FuncCall ] ] \n    assert 1 <= len ( loop_nest ) , \"Found to few for statements in kernel\" \n    return loop_nest "}
{"8000": "\ndef get_cachesim ( self , cores = 1 ) : \n    cache_dict = { } \n    for c in self [ 'memory hierarchy' ] : \n        if 'cache per group' not in c : \n            continue \n        cache_dict [ c [ 'level' ] ] = deepcopy ( c [ 'cache per group' ] ) \n        if 1 < c [ 'cores per group' ] : \n            cache_dict [ c [ 'level' ] ] [ 'sets' ] //= cores \n    cs , caches , mem = cachesim . CacheSimulator . from_dict ( cache_dict ) \n    return cs "}
{"8001": "\ndef get_bandwidth ( self , cache_level , read_streams , write_streams , threads_per_core , cores = None ) : \n    try : \n        target_ratio = read_streams / write_streams \n    except ZeroDivisionError : \n        target_ratio = float ( 'inf' ) \n    measurement_kernel = 'load' \n    measurement_kernel_info = self [ 'benchmarks' ] [ 'kernels' ] [ measurement_kernel ] \n    measurement_kernel_ratio = float ( 'inf' ) \n    for kernel_name , kernel_info in sorted ( self [ 'benchmarks' ] [ 'kernels' ] . items ( ) ) : \n        try : \n            kernel_ratio = ( ( kernel_info [ 'read streams' ] [ 'streams' ] + kernel_info [ 'write streams' ] [ 'streams' ] - kernel_info [ 'read+write streams' ] [ 'streams' ] ) / kernel_info [ 'write streams' ] [ 'streams' ] ) \n        except ZeroDivisionError : \n            kernel_ratio = float ( 'inf' ) \n        if abs ( measurement_kernel_ratio - target_ratio ) > abs ( kernel_ratio - target_ratio ) : \n            measurement_kernel = kernel_name \n            measurement_kernel_info = kernel_info \n            measurement_kernel_ratio = kernel_ratio \n    bw_level = self [ 'memory hierarchy' ] [ cache_level ] [ 'level' ] \n    bw_measurements = self [ 'benchmarks' ] [ 'measurements' ] [ bw_level ] [ threads_per_core ] \n    assert threads_per_core == bw_measurements [ 'threads per core' ] , 'malformed measurement dictionary in machine file.' \n    if cores is not None : \n        run_index = bw_measurements [ 'cores' ] . index ( cores ) \n        bw = bw_measurements [ 'results' ] [ measurement_kernel ] [ run_index ] \n    else : \n        max_cores = min ( self [ 'memory hierarchy' ] [ cache_level ] [ 'cores per group' ] , self [ 'cores per NUMA domain' ] ) \n        bw = max ( bw_measurements [ 'results' ] [ measurement_kernel ] [ : max_cores ] ) \n    if cache_level == 0 : \n        factor = 1.0 \n    else : \n        factor = ( float ( measurement_kernel_info [ 'read streams' ] [ 'bytes' ] ) + 2.0 * float ( measurement_kernel_info [ 'write streams' ] [ 'bytes' ] ) - float ( measurement_kernel_info [ 'read+write streams' ] [ 'bytes' ] ) ) / ( float ( measurement_kernel_info [ 'read streams' ] [ 'bytes' ] ) + float ( measurement_kernel_info [ 'write streams' ] [ 'bytes' ] ) ) \n    bw = bw * factor \n    return bw , measurement_kernel "}
{"8003": "\ndef parse_perfctr_event ( perfctr ) : \n    split_perfctr = perfctr . split ( ':' ) \n    assert 2 <= len ( split_perfctr ) , \"Atleast one colon (:) is required in the event name\" \n    event_tuple = split_perfctr [ : 2 ] \n    parameters = { } \n    for p in split_perfctr [ 2 : ] : \n        if '=' in p : \n            k , v = p . split ( '=' ) \n            if v . startswith ( '0x' ) : \n                parameters [ k ] = int ( v , 16 ) \n            else : \n                parameters [ k ] = int ( v ) \n        else : \n            parameters [ p ] = None \n    event_tuple . append ( parameters ) \n    return tuple ( event_tuple ) "}
{"8004": "\ndef _enforce_no_overlap ( self , start_at = 0 ) : \n    i = start_at \n    while len ( self . data ) > i + 1 : \n        if self . data [ i + 1 ] [ 0 ] <= self . data [ i ] [ 1 ] : \n            if self . data [ i + 1 ] [ 1 ] > self . data [ i ] [ 1 ] : \n                self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] \n            del self . data [ i + 1 ] \n        i += 1 "}
{"8015": "\ndef report ( self , output_file = sys . stdout ) : \n    if 1 < self . verbose : \n        with pprint_nosort ( ) : \n            pprint . pprint ( self . results ) \n    if 0 < self . verbose : \n        print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output_file ) \n    if 0 < self . verbose : \n        print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output_file ) \n    print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output_file ) \n    print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [MIt/s]' ] ) , file = output_file ) \n    if 0 < self . verbose : \n        print ( 'MEM bandwidth: {:.2f} MByte/s' . format ( self . results [ 'MEM BW [MByte/s]' ] ) , file = output_file ) \n    print ( '' , file = output_file ) \n    if not self . no_phenoecm : \n        print ( \"Data Transfers:\" ) \n        print ( \"{:^8} |\" . format ( \"cache\" ) , end = '' ) \n        for metrics in self . results [ 'data transfers' ] . values ( ) : \n            for metric_name in sorted ( metrics ) : \n                print ( \" {:^14}\" . format ( metric_name ) , end = '' ) \n            print ( ) \n            break \n        for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : \n            print ( \"{!s:^8} |\" . format ( cache ) , end = '' ) \n            for k , v in sorted ( metrics . items ( ) ) : \n                print ( \" {!s:^14}\" . format ( v ) , end = '' ) \n            print ( ) \n        print ( ) \n        print ( 'Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | ' '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL' . format ( ** { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output_file ) \n        print ( 'T_OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output_file ) "}
{"8029": "\ndef get ( self , key ) : \n    lock . acquire ( ) \n    try : \n        if key not in self : \n            return None \n        current_time = time . time ( ) \n        if current_time < self [ key ] . expire : \n            return self [ key ] . value \n        deletes = [ ] \n        for k , val in self . items ( ) : \n            if current_time >= val . expire : \n                deletes . append ( k ) \n        for k in deletes : \n            del self [ k ] \n        return None \n    finally : \n        lock . release ( ) "}
{"8033": "\ndef get_value_by_version ( d ) : \n    from oplus import CONF \n    cv = CONF . eplus_version [ : 2 ] \n    for v , value in sorted ( d . items ( ) , reverse = True ) : \n        if v <= cv : \n            return value "}
{"8039": "\ndef prepare_extensible ( self ) : \n    for k in self . _tags : \n        if \"extensible\" in k : \n            cycle_len = int ( k . split ( \":\" ) [ 1 ] ) \n            break \n    else : \n        return \n    cycle_start = None \n    cycle_patterns = [ ] \n    for i , field_descriptor in enumerate ( self . _field_descriptors ) : \n        if ( cycle_start is not None ) and ( ( cycle_start + cycle_len ) <= i ) : \n            break \n        if ( cycle_start is None ) and ( \"begin-extensible\" in field_descriptor . tags ) : \n            cycle_start = i \n        if cycle_start is None : \n            continue \n        cycle_patterns . append ( field_descriptor . ref . replace ( \"1\" , r\"(\\d+)\" ) ) \n    else : \n        raise RuntimeError ( \"cycle start not found\" ) \n    self . _field_descriptors = self . _field_descriptors [ : cycle_start + cycle_len ] \n    self . extensible_info = ( cycle_start , cycle_len , tuple ( cycle_patterns ) ) \n    for i , fd in enumerate ( self . _field_descriptors [ cycle_start : ] ) : \n        fd . set_extensible_info ( cycle_start , cycle_len , cycle_patterns [ i ] ) "}
{"8070": "\ndef parse_ical ( vcal ) : \n    vcal = vcal . replace ( '\\r\\n ' , '' ) . replace ( '\\r\\n\\r\\n' , '\\r\\n' ) \n    vevents = vcal . split ( '\\r\\nBEGIN:VEVENT\\r\\n' ) \n    del ( vevents [ 0 ] ) \n    events = [ ] \n    for vevent in vevents : \n        event = { } \n        for line in vevent . split ( '\\r\\n' ) : \n            line = line . split ( ':' , 1 ) \n            key = line [ 0 ] . lower ( ) \n            if 1 >= len ( line ) or key == 'end' : \n                continue \n            if key . startswith ( 'dt' ) : \n                event [ key ] = unix_ts ( dateutil . parser . parse ( line [ 1 ] ) ) \n                continue \n            if not key . startswith ( 'attach' ) : \n                event [ key ] = line [ 1 ] \n                continue \n            event [ 'attach' ] = event . get ( 'attach' , [ ] ) \n            attachment = { } \n            for x in [ x . split ( '=' ) for x in line [ 0 ] . split ( ';' ) ] : \n                if x [ 0 ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : \n                    attachment [ x [ 0 ] . lower ( ) ] = x [ 1 ] \n            attachment [ 'data' ] = b64decode ( line [ 1 ] ) . decode ( 'utf-8' ) \n            event [ 'attach' ] . append ( attachment ) \n        events . append ( event ) \n    return events "}
{"8071": "\ndef get_schedule ( ) : \n    params = { 'agentid' : config ( ) [ 'agent' ] [ 'name' ] . encode ( 'utf8' ) } \n    lookahead = config ( ) [ 'agent' ] [ 'cal_lookahead' ] * 24 * 60 * 60 \n    if lookahead : \n        params [ 'cutoff' ] = str ( ( timestamp ( ) + lookahead ) * 1000 ) \n    uri = '%s/calendars?%s' % ( config ( ) [ 'service-scheduler' ] [ 0 ] , urlencode ( params ) ) \n    try : \n        vcal = http_request ( uri ) \n    except pycurl . error as e : \n        logger . error ( 'Could not get schedule: %s' % e ) \n        return \n    try : \n        cal = parse_ical ( vcal . decode ( 'utf-8' ) ) \n    except Exception : \n        logger . error ( 'Could not parse ical' ) \n        logger . error ( traceback . format_exc ( ) ) \n        return \n    db = get_session ( ) \n    db . query ( UpcomingEvent ) . delete ( ) \n    for event in cal : \n        if timestamp ( ) >= event [ 'dtend' ] : \n            continue \n        e = UpcomingEvent ( ) \n        e . start = event [ 'dtstart' ] \n        e . end = event [ 'dtend' ] \n        e . uid = event . get ( 'uid' ) \n        e . title = event . get ( 'summary' ) \n        e . set_data ( event ) \n        db . add ( e ) \n    db . commit ( ) "}
{"8072": "\ndef control_loop ( ) : \n    set_service_status ( Service . SCHEDULE , ServiceStatus . BUSY ) \n    notify . notify ( 'READY=1' ) \n    while not terminate ( ) : \n        notify . notify ( 'WATCHDOG=1' ) \n        get_schedule ( ) \n        session = get_session ( ) \n        next_event = session . query ( UpcomingEvent ) . filter ( timestamp ( ) < UpcomingEvent . end ) . order_by ( UpcomingEvent . start ) . first ( ) \n        if next_event : \n            logger . info ( 'Next scheduled recording: %s' , datetime . fromtimestamp ( next_event . start ) ) \n            notify . notify ( 'STATUS=Next scheduled recording: %s' % datetime . fromtimestamp ( next_event . start ) ) \n        else : \n            logger . info ( 'No scheduled recording' ) \n            notify . notify ( 'STATUS=No scheduled recording' ) \n        session . close ( ) \n        next_update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update_frequency' ] \n        while not terminate ( ) and next_update > timestamp ( ) : \n            time . sleep ( 0.1 ) \n    logger . info ( 'Shutting down schedule service' ) \n    set_service_status ( Service . SCHEDULE , ServiceStatus . STOPPED ) "}
{"8073": "\ndef control_loop ( ) : \n    set_service_status ( Service . AGENTSTATE , ServiceStatus . BUSY ) \n    notify . notify ( 'READY=1' ) \n    notify . notify ( 'STATUS=Running' ) \n    while not terminate ( ) : \n        notify . notify ( 'WATCHDOG=1' ) \n        update_agent_state ( ) \n        next_update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update_frequency' ] \n        while not terminate ( ) and next_update > timestamp ( ) : \n            time . sleep ( 0.1 ) \n    logger . info ( 'Shutting down agentstate service' ) \n    set_service_status ( Service . AGENTSTATE , ServiceStatus . STOPPED ) "}
{"8097": "\ndef calc ( pvalues , lamb ) : \n    m = len ( pvalues ) \n    pi0 = ( lamb < pvalues ) . sum ( ) / ( ( 1 - lamb ) * m ) \n    pFDR = np . ones ( m ) \n    print ( \"pFDR    y        Pr     fastPow\" ) \n    for i in range ( m ) : \n        y = pvalues [ i ] \n        Pr = max ( 1 , m - i ) / float ( m ) \n        pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) \n        print ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) \n    num_null = pi0 * m \n    num_alt = m - num_null \n    num_negs = np . array ( range ( m ) ) \n    num_pos = m - num_negs \n    pp = num_pos / float ( m ) \n    qvalues = np . ones ( m ) \n    qvalues [ 0 ] = pFDR [ 0 ] \n    for i in range ( m - 1 ) : \n        qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) \n    sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt \n    sens [ 1.0 < sens ] = 1.0 \n    df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) \n    df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] \n    return df , num_null , m "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = Crypto . Random . new ( ) . read ( 16 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = True \n    left = None \n    if content_length is not None and 0 <= content_length : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and left < size : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and 0 < left : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16 ) \n            break \n        if left is not None : \n            left -= len ( chunk ) \n            if 0 >= left : \n                reading = False \n        block = chunk \n        trailing = len ( block ) % 16 \n        while trailing : \n            size = 16 - trailing \n            if left is not None and left < size : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and 0 < left : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = False \n                chunk = chr ( trailing ) * ( 16 - trailing ) \n            elif left is not None : \n                left -= len ( chunk ) \n                if 0 >= left : \n                    reading = False \n            block += chunk \n            trailing = len ( block ) % 16 \n        yield encryptor . encrypt ( block ) "}
{"8156": "\ndef aes_decrypt ( key , stdin , chunk_size = 65536 ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = stdin . read ( 16 ) \n    while 16 > len ( iv ) : \n        chunk = stdin . read ( 16 - len ( iv ) ) \n        if not chunk : \n            raise IOError ( 'EOF reading IV' ) \n    decryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    data = '' \n    while True : \n        chunk = stdin . read ( chunk_size ) \n        if not chunk : \n            if len ( data ) != 16 : \n                raise IOError ( 'EOF reading encrypted stream' ) \n            data = decryptor . decrypt ( data ) \n            trailing = ord ( data [ - 1 ] ) \n            if 15 < trailing : \n                raise IOError ( 'EOF reading encrypted stream or trailing value corrupted ' '%s' % trailing ) \n            yield data [ : trailing ] \n            break \n        data += chunk \n        if 16 < len ( data ) : \n            trailing = ( len ( data ) % 16 ) or 16 \n            yield decryptor . decrypt ( data [ : - trailing ] ) \n            data = data [ - trailing : ] "}
{"8182": "\ndef execute_perceval_job ( backend , backend_args , qitems , task_id , category , archive_args = None , max_retries = MAX_JOB_RETRIES ) : \n    rq_job = rq . get_current_job ( ) \n    job = PercevalJob ( rq_job . id , task_id , backend , category , rq_job . connection , qitems ) \n    logger . debug ( \"Running job #%s (task: %s) (%s) (cat:%s)\" , job . job_id , task_id , backend , category ) \n    if not job . has_archiving ( ) and archive_args : \n        raise AttributeError ( \"archive attributes set but archive is not supported\" ) \n    run_job = True \n    resume = False \n    failures = 0 \n    while run_job : \n        try : \n            job . run ( backend_args , archive_args = archive_args , resume = resume ) \n        except AttributeError as e : \n            raise e \n        except Exception as e : \n            logger . debug ( \"Error running job %s (%s) - %s\" , job . job_id , backend , str ( e ) ) \n            failures += 1 \n            if not job . has_resuming ( ) or max_retries <= failures : \n                logger . error ( \"Cancelling job #%s (task: %s) (%s)\" , job . job_id , task_id , backend ) \n                raise e \n            logger . warning ( \"Resuming job #%s (task: %s) (%s) due to a failure (n %s, max %s)\" , job . job_id , task_id , backend , failures , max_retries ) \n            resume = True \n        else : \n            run_job = False \n    result = job . result \n    logger . debug ( \"Job #%s (task: %s) completed (%s) - %s items (%s) fetched\" , result . job_id , task_id , result . backend , str ( result . nitems ) , result . category ) \n    return result "}
{"8184": "\ndef run ( self , backend_args , archive_args = None , resume = False ) : \n    args = backend_args . copy ( ) \n    if archive_args : \n        self . initialize_archive_manager ( archive_args [ 'archive_path' ] ) \n    if not resume : \n        max_date = backend_args . get ( 'from_date' , None ) \n        offset = backend_args . get ( 'offset' , None ) \n        if max_date : \n            max_date = datetime_to_utc ( max_date ) . timestamp ( ) \n        self . _result = JobResult ( self . job_id , self . task_id , self . backend , self . category , None , max_date , 0 , offset = offset , nresumed = 0 ) \n    else : \n        if self . result . max_date : \n            args [ 'from_date' ] = unixtime_to_datetime ( self . result . max_date ) \n        if self . result . offset : \n            args [ 'offset' ] = self . result . offset \n        self . _result . nresumed += 1 \n    for item in self . _execute ( args , archive_args ) : \n        self . conn . rpush ( self . qitems , pickle . dumps ( item ) ) \n        self . _result . nitems += 1 \n        self . _result . last_uuid = item [ 'uuid' ] \n        if not self . result . max_date or item [ 'updated_on' ] > self . result . max_date : \n            self . _result . max_date = item [ 'updated_on' ] \n        if 'offset' in item : \n            self . _result . offset = item [ 'offset' ] "}
{"8203": "\ndef _handle_successful_job ( self , job ) : \n    result = job . result \n    task_id = job . kwargs [ 'task_id' ] \n    try : \n        task = self . registry . get ( task_id ) \n    except NotFoundError : \n        logger . warning ( \"Task %s not found; related job #%s will not be rescheduled\" , task_id , job . id ) \n        return \n    if task . archiving_cfg and task . archiving_cfg . fetch_from_archive : \n        logger . info ( \"Job #%s (task: %s) successfully finished\" , job . id , task_id ) \n        return \n    if 0 < result . nitems : \n        task . backend_args [ 'next_from_date' ] = unixtime_to_datetime ( result . max_date ) \n        if result . offset : \n            task . backend_args [ 'next_offset' ] = result . offset \n    job_args = self . _build_job_arguments ( task ) \n    delay = task . scheduling_cfg . delay if task . scheduling_cfg else WAIT_FOR_QUEUING \n    job_id = self . _scheduler . schedule_job_task ( Q_UPDATING_JOBS , task_id , job_args , delay = delay ) \n    logger . info ( \"Job #%s (task: %s, old job: %s) re-scheduled\" , job_id , task_id , job . id ) "}
{"8214": "\ndef parse_path ( path ) : \n    if path is None : \n        raise ValueError ( \"path must be a string\" ) \n    parts = path . strip ( \"/\" ) . split ( \"/\" ) \n    database = unquote_plus ( parts [ 0 ] ) if len ( parts ) else None \n    schema = parts [ 1 ] if 1 < len ( parts ) else None \n    return database , schema "}
{"8256": "\ndef retry ( n , errors , wait = 0.0 , logger_name = None ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( * args , ** kwargs ) : \n            retries = 0 \n            while True : \n                try : \n                    result = func ( * args , ** kwargs ) \n                    if retries and logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'Retry of `%s` successful' % func . __name__ ) \n                    return result \n                except errors : \n                    if n <= retries : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                        raise \n                    elif logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                    retries += 1 \n                    if wait : \n                        time . sleep ( wait ) \n        return new_func \n    return wrapper "}
{"8276": "\ndef _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : \n    assert ( start_time + time_window <= end_time ) \n    bins = ( end_time - start_time ) / time_window \n    bins = int ( np . floor ( bins ) ) \n    binned_spikes = np . zeros ( bins ) \n    spike_array_neuron = spike_res . t [ spike_res . i == neuron_id ] \n    for bin in range ( bins ) : \n        lower_time = start_time + time_window * bin \n        upper_time = start_time + time_window * ( bin + 1 ) \n        spike_array_interval = spike_array_neuron [ lower_time <= spike_array_neuron ] \n        spike_array_interval = spike_array_interval [ upper_time > spike_array_interval ] \n        spikes = len ( spike_array_interval ) \n        binned_spikes [ bin ] = spikes \n    var = np . var ( binned_spikes ) \n    avg = np . mean ( binned_spikes ) \n    if 0 < avg : \n        return var / float ( avg ) \n    else : \n        return 0 "}
{"8295": "\ndef _remove_along_branch ( self , actual_node , split_name , recursive = False ) : \n    if len ( split_name ) == 0 : \n        if actual_node . v_is_group and actual_node . f_has_children ( ) : \n            if recursive : \n                for child in list ( actual_node . _children . keys ( ) ) : \n                    actual_node . f_remove_child ( child , recursive = True ) \n            else : \n                raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) \n        self . _delete_node ( actual_node ) \n        return True \n    name = split_name . popleft ( ) \n    if name in actual_node . _links : \n        if 0 < len ( split_name ) : \n            raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) \n        actual_node . f_remove_link ( name ) \n    else : \n        child = actual_node . _children [ name ] \n        if self . _remove_along_branch ( child , split_name , recursive = recursive ) : \n            del actual_node . _children [ name ] \n            if name in actual_node . _groups : \n                del actual_node . _groups [ name ] \n            elif name in actual_node . _leaves : \n                del actual_node . _leaves [ name ] \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n            del child \n            return False "}
{"8297": "\ndef _add_prefix ( self , split_names , start_node , group_type_name ) : \n    root = self . _root_instance \n    prepend = [ ] \n    if 3 > start_node . v_depth and not group_type_name == GROUP : \n        if start_node . v_depth == 0 : \n            if group_type_name == DERIVED_PARAMETER_GROUP : \n                if split_names [ 0 ] == 'derived_parameters' : \n                    return split_names \n                else : \n                    prepend += [ 'derived_parameters' ] \n            elif group_type_name == RESULT_GROUP : \n                if split_names [ 0 ] == 'results' : \n                    return split_names \n                else : \n                    prepend += [ 'results' ] \n            elif group_type_name == CONFIG_GROUP : \n                if split_names [ 0 ] == 'config' : \n                    return split_names \n                else : \n                    prepend += [ 'config' ] \n            elif group_type_name == PARAMETER_GROUP : \n                if split_names [ 0 ] == 'parameters' : \n                    return split_names [ 0 ] \n                else : \n                    prepend += [ 'parameters' ] \n            else : \n                raise RuntimeError ( 'Why are you here?' ) \n        if root . _is_run and root . _auto_run_prepend : \n            dummy = root . f_wildcard ( '$' , - 1 ) \n            crun = root . f_wildcard ( '$' ) \n            if any ( name in root . _run_information for name in split_names ) : \n                pass \n            elif any ( name == dummy for name in split_names ) : \n                pass \n            elif ( group_type_name == RESULT_GROUP or group_type_name == DERIVED_PARAMETER_GROUP ) : \n                if start_node . v_depth == 0 : \n                    prepend += [ 'runs' , crun ] \n                elif start_node . v_depth == 1 : \n                    if len ( split_names ) == 1 and split_names [ 0 ] == 'runs' : \n                        return split_names \n                    else : \n                        prepend += [ 'runs' , crun ] \n                elif start_node . v_depth == 2 and start_node . v_name == 'runs' : \n                    prepend += [ crun ] \n    if prepend : \n        split_names = prepend + split_names \n    return split_names "}
{"8299": "\ndef _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , add_prefix = True , check_naming = True ) : \n    args = list ( args ) \n    create_new = True \n    name = '' \n    instance = None \n    constructor = None \n    add_link = type_name == LINK \n    if add_link : \n        name = args [ 0 ] \n        instance = args [ 1 ] \n        create_new = False \n    elif len ( args ) == 1 and len ( kwargs ) == 0 : \n        item = args [ 0 ] \n        try : \n            name = item . v_full_name \n            instance = item \n            create_new = False \n        except AttributeError : \n            pass \n    if create_new : \n        if 0 < len ( args ) and inspect . isclass ( args [ 0 ] ) : \n            constructor = args . pop ( 0 ) \n        if 0 < len ( args ) and isinstance ( args [ 0 ] , str ) : \n            name = args . pop ( 0 ) \n        elif 'name' in kwargs : \n            name = kwargs . pop ( 'name' ) \n        elif 'full_name' in kwargs : \n            name = kwargs . pop ( 'full_name' ) \n        else : \n            raise ValueError ( 'Could not determine a name of the new item you want to add. ' 'Either pass the name as positional argument or as a keyword ' 'argument `name`.' ) \n    split_names = name . split ( '.' ) \n    if check_naming : \n        for idx , name in enumerate ( split_names ) : \n            translated_shortcut , name = self . _translate_shortcut ( name ) \n            replaced , name = self . _replace_wildcards ( name ) \n            if translated_shortcut or replaced : \n                split_names [ idx ] = name \n        faulty_names = self . _check_names ( split_names , start_node ) \n        if faulty_names : \n            full_name = '.' . join ( split_names ) \n            raise ValueError ( 'Your Parameter/Result/Node `%s` contains the following not admissible names: ' '%s please choose other names.' % ( full_name , faulty_names ) ) \n        if add_link : \n            if instance is None : \n                raise ValueError ( 'You must provide an instance to link to!' ) \n            if instance . v_is_root : \n                raise ValueError ( 'You cannot create a link to the root node' ) \n            if start_node . v_is_root and name in SUBTREE_MAPPING : \n                raise ValueError ( '`%s` is a reserved name for a group under root.' % name ) \n            if not self . _root_instance . f_contains ( instance , with_links = False , shortcuts = False ) : \n                raise ValueError ( 'You can only link to items within the trajectory tree!' ) \n    if add_prefix : \n        split_names = self . _add_prefix ( split_names , start_node , group_type_name ) \n    if group_type_name == GROUP : \n        add_leaf = type_name != group_type_name and not add_link \n        group_type_name , type_name = self . _determine_types ( start_node , split_names [ 0 ] , add_leaf , add_link ) \n    if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : \n        raise TypeError ( 'You are not allowed to add config or parameter data or groups ' 'during a single run.' ) \n    return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) "}
{"8302": "\ndef _check_names ( self , split_names , parent_node = None ) : \n    faulty_names = '' \n    if parent_node is not None and parent_node . v_is_root and split_names [ 0 ] == 'overview' : \n        faulty_names = '%s `overview` cannot be added directly under the root node ' 'this is a reserved keyword,' % ( faulty_names ) \n    for split_name in split_names : \n        if len ( split_name ) == 0 : \n            faulty_names = '%s `%s` contains no characters, please use at least 1,' % ( faulty_names , split_name ) \n        elif split_name . startswith ( '_' ) : \n            faulty_names = '%s `%s` starts with a leading underscore,' % ( faulty_names , split_name ) \n        elif re . match ( CHECK_REGEXP , split_name ) is None : \n            faulty_names = '%s `%s` contains non-admissible characters ' '(use only [A-Za-z0-9_-]),' % ( faulty_names , split_name ) \n        elif '$' in split_name : \n            if split_name not in self . _root_instance . _wildcard_keys : \n                faulty_names = '%s `%s` contains `$` but has no associated ' 'wildcard function,' % ( faulty_names , split_name ) \n        elif split_name in self . _not_admissible_names : \n            warnings . warn ( '`%s` is a method/attribute of the ' 'trajectory/treenode/naminginterface, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n        elif split_name in self . _python_keywords : \n            warnings . warn ( '`%s` is a python keyword, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n    name = split_names [ - 1 ] \n    if pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH <= len ( name ) : \n        faulty_names = '%s `%s` is too long the name can only have %d characters but it has ' '%d,' % ( faulty_names , name , len ( name ) , pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) \n    return faulty_names "}
{"8308": "\ndef _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    if predicate is None : \n        predicate = lambda x : True \n    iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) \n    start = True \n    visited_linked_nodes = set ( [ ] ) \n    while True : \n        try : \n            depth , name , item = next ( iterator_queue ) \n            full_name = item . _full_name \n            if start or predicate ( item ) : \n                if full_name in visited_linked_nodes : \n                    if in_search : \n                        yield depth , name , item \n                elif max_depth >= depth : \n                    if start : \n                        start = False \n                    else : \n                        if in_search : \n                            yield depth , name , item \n                        else : \n                            yield item \n                    if full_name in linked_by : \n                        visited_linked_nodes . add ( full_name ) \n                    if not item . _is_leaf and max_depth > depth : \n                        child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) \n                        iterator_queue . add ( child_iterator ) \n        except StopIteration : \n            break "}
{"8309": "\ndef _very_fast_search ( self , node , key , max_depth , with_links , crun ) : \n    if key in self . _links_count : \n        return \n    parent_full_name = node . v_full_name \n    starting_depth = node . v_depth \n    candidate_dict = self . _get_candidate_dict ( key , crun ) \n    if with_links : \n        upper_bound = 1 \n    else : \n        upper_bound = FAST_UPPER_BOUND \n    if upper_bound < len ( candidate_dict ) : \n        raise pex . TooManyGroupsError ( 'Too many nodes' ) \n    result_node = None \n    for goal_name in candidate_dict : \n        if goal_name . startswith ( parent_full_name ) : \n            candidate = candidate_dict [ goal_name ] \n            if max_depth >= candidate . v_depth - starting_depth : \n                if result_node is not None : \n                    raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once, ' 'full name of first occurrence is `%s` and of' 'second `%s`' % ( key , goal_name , result_node . v_full_name ) ) \n                result_node = candidate \n    if result_node is not None : \n        return result_node , result_node . v_depth "}
{"8310": "\ndef _search ( self , node , key , max_depth = float ( 'inf' ) , with_links = True , crun = None ) : \n    if key in node . _children and ( with_links or key not in node . _links ) : \n        return node . _children [ key ] , 1 \n    try : \n        result = self . _very_fast_search ( node , key , max_depth , with_links , crun ) \n        if result : \n            return result \n    except pex . TooManyGroupsError : \n        pass \n    except pex . NotUniqueNodeError : \n        pass \n    nodes_iterator = self . _iter_nodes ( node , recursive = True , max_depth = max_depth , in_search = True , with_links = with_links ) \n    result_node = None \n    result_depth = float ( 'inf' ) \n    for depth , name , child in nodes_iterator : \n        if result_depth < depth : \n            break \n        if key == name : \n            if result_node is not None : \n                raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once within ' 'the same depth %d. ' 'Full name of first occurrence is `%s` and of ' 'second `%s`' % ( key , child . v_depth , result_node . v_full_name , child . v_full_name ) ) \n            result_node = child \n            result_depth = depth \n    return result_node , result_depth "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - 1 ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if key != candidate . v_name or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if parent_full_name != '' : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if max_depth < len ( candidate_split_name ) : \n                break \n            if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            elif shortcuts : \n                candidate_set = set ( candidate_split_name ) \n                climbing = True \n                for name in split_name : \n                    if name not in candidate_set : \n                        climbing = False \n                        break \n                if climbing : \n                    count = 0 \n                    candidate_length = len ( candidate_split_name ) \n                    for idx in range ( candidate_length ) : \n                        if candidate_length < idx + split_length - count : \n                            break \n                        if split_name [ count ] == candidate_split_name [ idx ] : \n                            count += 1 \n                            if count == len ( split_name ) : \n                                result_list . append ( candidate ) \n                                full_name_set . add ( candidate . v_full_name ) \n                                break \n    return result_list "}
{"8371": "\ndef acquire ( self ) : \n    self . start ( test_connection = False ) \n    while True : \n        str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) \n        response = str_response . split ( LockerServer . DELIMITER ) \n        if response [ 0 ] == LockerServer . GO : \n            return True \n        elif response [ 0 ] == LockerServer . LOCK_ERROR and 0 < retries : \n            self . _logger . error ( str_response + '; Probably due to retry' ) \n            return True \n        elif response [ 0 ] == LockerServer . WAIT : \n            time . sleep ( self . SLEEP ) \n        else : \n            raise RuntimeError ( 'Response `%s` not understood' % response ) "}
{"8372": "\ndef listen ( self ) : \n    count = 0 \n    self . _start ( ) \n    while True : \n        result = self . _socket . recv_pyobj ( ) \n        if isinstance ( result , tuple ) : \n            request , data = result \n        else : \n            request = result \n            data = None \n        if request == self . SPACE : \n            if self . queue_maxsize > self . queue . qsize ( ) + count : \n                self . _socket . send_string ( self . SPACE_AVAILABLE ) \n                count += 1 \n            else : \n                self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) \n        elif request == self . PING : \n            self . _socket . send_string ( self . PONG ) \n        elif request == self . DATA : \n            self . _socket . send_string ( self . STORING ) \n            self . queue . put ( data ) \n            count -= 1 \n        elif request == self . DONE : \n            self . _socket . send_string ( ZMQServer . CLOSED ) \n            self . queue . put ( ( 'DONE' , [ ] , { } ) ) \n            self . _close ( ) \n            break \n        else : \n            raise RuntimeError ( 'I did not understand your request %s' % request ) "}
{"8378": "\ndef _receive_data ( self ) : \n    while True : \n        while self . max_size > len ( self . _buffer ) and self . conn . poll ( ) : \n            data = self . _read_chunks ( ) \n            if data is not None : \n                self . _buffer . append ( data ) \n        if 0 < len ( self . _buffer ) : \n            return self . _buffer . popleft ( ) "}
{"8392": "\ndef signal_update ( self ) : \n    if not self . active : \n        return \n    self . _updates += 1 \n    current_time = time . time ( ) \n    dt = current_time - self . _last_time \n    if self . _display_time < dt : \n        dfullt = current_time - self . _start_time \n        seconds = int ( dfullt ) % 60 \n        minutes = int ( dfullt ) / 60 \n        if minutes == 0 : \n            formatted_time = '%ds' % seconds \n        else : \n            formatted_time = '%dm%02ds' % ( minutes , seconds ) \n        nodespersecond = self . _updates / dfullt \n        message = 'Processed %d nodes in %s (%.2f nodes/s).' % ( self . _updates , formatted_time , nodespersecond ) \n        self . _logger . info ( message ) \n        self . _last_time = current_time "}
{"8396": "\ndef _srvc_load_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ 0 ] \n        item = input_tuple [ 1 ] \n        if 2 < len ( input_tuple ) : \n            args = input_tuple [ 2 ] \n        if 3 < len ( input_tuple ) : \n            kwargs = input_tuple [ 3 ] \n        if 4 < len ( input_tuple ) : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . load ( msg , item , * args , ** kwargs ) "}
{"8398": "\ndef _srvc_store_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ 0 ] \n        item = input_tuple [ 1 ] \n        if 2 < len ( input_tuple ) : \n            args = input_tuple [ 2 ] \n        if 3 < len ( input_tuple ) : \n            kwargs = input_tuple [ 3 ] \n        if 4 < len ( input_tuple ) : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . store ( msg , item , * args , ** kwargs ) "}
{"8405": "\ndef _tree_load_sub_branch ( self , traj_node , branch_name , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _trajectory = None , _as_new = False , _hdf5_group = None ) : \n    if load_data == pypetconstants . LOAD_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if _trajectory is None : \n        _trajectory = traj_node . v_root \n    if _hdf5_group is None : \n        hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) \n        if hdf5_group_name == '' : \n            _hdf5_group = self . _trajectory_group \n        else : \n            try : \n                _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_group_name ) \n            except pt . NoSuchNodeError : \n                self . _logger . error ( 'Cannot find `%s` the hdf5 node `%s` does not exist!' % ( traj_node . v_full_name , hdf5_group_name ) ) \n                raise \n    split_names = branch_name . split ( '.' ) \n    final_group_name = split_names . pop ( ) \n    current_depth = 1 \n    for name in split_names : \n        if max_depth < current_depth : \n            return \n        _hdf5_group = getattr ( _hdf5_group , name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) \n        current_depth += 1 \n        traj_node = traj_node . _children [ name ] \n    if max_depth >= current_depth : \n        _hdf5_group = getattr ( _hdf5_group , final_group_name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) "}
{"8409": "\ndef _trj_store_explorations ( self , traj ) : \n    nexplored = len ( traj . _explored_parameters ) \n    if 0 < nexplored : \n        if hasattr ( self . _overview_group , 'explorations' ) : \n            explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) \n            if len ( explorations_table ) != nexplored : \n                self . _hdf5file . remove_node ( where = self . _overview_group , name = 'explorations' ) \n    if not hasattr ( self . _overview_group , 'explorations' ) : \n        explored_list = list ( traj . _explored_parameters . keys ( ) ) \n        if explored_list : \n            string_col = self . _all_get_table_col ( 'explorations' , explored_list , 'overview.explorations' ) \n        else : \n            string_col = pt . StringCol ( 1 ) \n        description = { 'explorations' : string_col } \n        explorations_table = self . _hdf5file . create_table ( where = self . _overview_group , name = 'explorations' , description = description ) \n        rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored_list ] \n        if rows : \n            explorations_table . append ( rows ) \n            explorations_table . flush ( ) "}
{"8410": "\ndef _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : \n    for table_name in tables_to_make : \n        paramdescriptiondict = { } \n        expectedrows = 0 \n        paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = 0 ) \n        paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = 1 ) \n        paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) \n        paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) \n        if table_name == 'config_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _config ) \n        if table_name == 'parameters_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _parameters ) \n        if table_name == 'explored_parameters_overview' : \n            paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) \n            paramdescriptiondict [ 'length' ] = pt . IntCol ( ) \n            if traj is not None : \n                expectedrows = len ( traj . _explored_parameters ) \n        if table_name . endswith ( 'summary' ) : \n            paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) \n        if table_name == 'derived_parameters_overview' : \n            expectedrows = self . _derived_parameters_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _derived_parameters ) \n        if table_name == 'results_overview' : \n            expectedrows = self . _results_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _results ) \n        if 0 < expectedrows : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) \n        else : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) \n        paramtable . flush ( ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = 0 \n        maximum_display_other = 10 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if maximum_display_other > counter : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                elif counter == maximum_display_other : \n                    self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter += 1 \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = True "}
{"8412": "\ndef _tree_store_sub_branch ( self , traj_node , branch_name , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , hdf5_group = None ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if hdf5_group is None : \n        location = traj_node . v_full_name \n        hdf5_location = location . replace ( '.' , '/' ) \n        try : \n            if location == '' : \n                hdf5_group = self . _trajectory_group \n            else : \n                hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_location ) \n        except pt . NoSuchNodeError : \n            self . _logger . debug ( 'Cannot store `%s` the parental hdf5 node with path `%s` does ' 'not exist on disk.' % ( traj_node . v_name , hdf5_location ) ) \n            if traj_node . v_is_leaf : \n                self . _logger . error ( 'Cannot store `%s` the parental hdf5 ' 'node with path `%s` does ' 'not exist on disk! The child ' 'you want to store is a leaf node,' 'that cannot be stored without ' 'the parental node existing on ' 'disk.' % ( traj_node . v_name , hdf5_location ) ) \n                raise \n            else : \n                self . _logger . debug ( 'I will try to store the path from trajectory root to ' 'the child now.' ) \n                self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , traj_node . v_full_name + '.' + branch_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth + traj_node . v_depth , hdf5_group = self . _trajectory_group ) \n                return \n    current_depth = 1 \n    split_names = branch_name . split ( '.' ) \n    leaf_name = split_names . pop ( ) \n    for name in split_names : \n        if max_depth < current_depth : \n            return \n        self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) \n        current_depth += 1 \n        traj_node = traj_node . _children [ name ] \n        hdf5_group = getattr ( hdf5_group , name ) \n    if max_depth >= current_depth : \n        self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) "}
{"8414": "\ndef _tree_load_nodes_dfs ( self , parent_traj_node , load_data , with_links , recursive , max_depth , current_depth , trajectory , as_new , hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    loading_list = [ ( parent_traj_node , current_depth , hdf5_group ) ] \n    while loading_list : \n        parent_traj_node , current_depth , hdf5_group = loading_list . pop ( ) \n        if isinstance ( hdf5_group , pt . link . SoftLink ) : \n            if with_links : \n                self . _tree_load_link ( parent_traj_node , load_data = load_data , traj = trajectory , as_new = as_new , hdf5_soft_link = hdf5_group ) \n            continue \n        name = hdf5_group . _v_name \n        is_leaf = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . LEAF ) \n        in_trajectory = name in parent_traj_node . _children \n        if is_leaf : \n            if in_trajectory : \n                instance = parent_traj_node . _children [ name ] \n            else : \n                instance = self . _tree_create_leaf ( name , trajectory , hdf5_group ) \n                parent_traj_node . _add_leaf_from_storage ( args = ( instance , ) , kwargs = { } ) \n            self . _prm_load_parameter_or_result ( instance , load_data = load_data , _hdf5_group = hdf5_group ) \n            if as_new : \n                instance . _stored = False \n        else : \n            if in_trajectory : \n                traj_group = parent_traj_node . _children [ name ] \n                if load_data == pypetconstants . OVERWRITE_DATA : \n                    traj_group . v_annotations . f_empty ( ) \n                    traj_group . v_comment = '' \n            else : \n                if HDF5StorageService . CLASS_NAME in hdf5_group . _v_attrs : \n                    class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) \n                    class_constructor = trajectory . _create_class ( class_name ) \n                    instance = trajectory . _construct_instance ( class_constructor , name ) \n                    args = ( instance , ) \n                else : \n                    args = ( name , ) \n                traj_group = parent_traj_node . _add_group_from_storage ( args = args , kwargs = { } ) \n            self . _grp_load_group ( traj_group , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , _traj = trajectory , _as_new = as_new , _hdf5_group = hdf5_group ) \n            if recursive and max_depth > current_depth : \n                new_depth = current_depth + 1 \n                for children in ( hdf5_group . _v_groups , hdf5_group . _v_links ) : \n                    for new_hdf5_group_name in children : \n                        new_hdf5_group = children [ new_hdf5_group_name ] \n                        loading_list . append ( ( traj_group , new_depth , new_hdf5_group ) ) "}
{"8415": "\ndef _tree_store_nodes_dfs ( self , parent_traj_node , name , store_data , with_links , recursive , max_depth , current_depth , parent_hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    store_list = [ ( parent_traj_node , name , current_depth , parent_hdf5_group ) ] \n    while store_list : \n        parent_traj_node , name , current_depth , parent_hdf5_group = store_list . pop ( ) \n        if name in parent_traj_node . _links : \n            if with_links : \n                self . _tree_store_link ( parent_traj_node , name , parent_hdf5_group ) \n            continue \n        traj_node = parent_traj_node . _children [ name ] \n        if not hasattr ( parent_hdf5_group , name ) : \n            newly_created = True \n            new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , filters = self . _all_get_filters ( ) ) \n        else : \n            newly_created = False \n            new_hdf5_group = getattr ( parent_hdf5_group , name ) \n        if traj_node . v_is_leaf : \n            self . _prm_store_parameter_or_result ( traj_node , store_data = store_data , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n        else : \n            self . _grp_store_group ( traj_node , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n            if recursive and max_depth > current_depth : \n                for child in traj_node . _children . keys ( ) : \n                    store_list . append ( ( traj_node , child , current_depth + 1 , new_hdf5_group ) ) "}
{"8416": "\ndef _all_store_param_or_result_table_entry ( self , instance , table , flags , additional_info = None ) : \n    location = instance . v_location \n    name = instance . v_name \n    fullname = instance . v_full_name \n    if ( flags == ( HDF5StorageService . ADD_ROW , ) and 2 > table . nrows and 'location' in table . colnames ) : \n        flags = ( HDF5StorageService . ADD_ROW , HDF5StorageService . MODIFY_ROW ) \n    if flags == ( HDF5StorageService . ADD_ROW , ) : \n        condvars = None \n        condition = None \n    else : \n        condvars = { 'namecol' : table . cols . name , 'locationcol' : table . cols . location , 'name' : name , 'location' : location } \n        condition = \"\"\"(namecol == name) & (locationcol == location)\"\"\" \n    if HDF5StorageService . REMOVE_ROW in flags : \n        insert_dict = { } \n    else : \n        colnames = set ( table . colnames ) \n        insert_dict = self . _all_extract_insert_dict ( instance , colnames , additional_info ) \n    self . _all_add_or_modify_row ( fullname , insert_dict , table , condition = condition , condvars = condvars , flags = flags ) "}
{"8419": "\ndef _all_set_attributes_to_recall_natives ( data , ptitem , prefix ) : \n    if type ( data ) is tuple : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_TUPLE ) \n    elif type ( data ) is list : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_LIST ) \n    elif type ( data ) is np . ndarray : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_NDARRAY ) \n    elif type ( data ) is np . matrix : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_MATRIX ) \n    elif type ( data ) in pypetconstants . PARAMETER_SUPPORTED_DATA : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_SCALAR ) \n        strtype = type ( data ) . __name__ \n        if not strtype in pypetconstants . PARAMETERTYPEDICT : \n            raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    elif type ( data ) is dict : \n        if 0 < len ( data ) : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_DICT ) \n        else : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_EMPTY_DICT ) \n    else : \n        raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n    if type ( data ) in ( list , tuple ) : \n        if 0 < len ( data ) : \n            strtype = type ( data [ 0 ] ) . __name__ \n            if not strtype in pypetconstants . PARAMETERTYPEDICT : \n                raise TypeError ( 'I do not know how to handle `%s` its type is ' '`%s`.' % ( str ( data ) , strtype ) ) \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    elif ( type ( data ) in ( np . ndarray , np . matrix ) and np . issubdtype ( data . dtype , str ) ) : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , str . __name__ ) "}
{"8420": "\ndef _all_recall_native_type ( self , data , ptitem , prefix ) : \n    typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) \n    colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) \n    type_changed = False \n    if colltype == HDF5StorageService . COLL_SCALAR : \n        if isinstance ( data , np . ndarray ) : \n            data = np . array ( [ data ] ) [ 0 ] \n            type_changed = True \n        if not typestr is None : \n            if typestr != type ( data ) . __name__ : \n                if typestr == str . __name__ : \n                    data = data . decode ( self . _encoding ) \n                else : \n                    try : \n                        data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) \n                    except KeyError : \n                        data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) \n                type_changed = True \n    elif ( colltype == HDF5StorageService . COLL_TUPLE or colltype == HDF5StorageService . COLL_LIST ) : \n        if type ( data ) is not list and type is not tuple : \n            type_changed = True \n            data = list ( data ) \n        if 0 < len ( data ) : \n            first_item = data [ 0 ] \n            if not typestr == type ( first_item ) . __name__ : \n                if not isinstance ( data , list ) : \n                    data = list ( data ) \n                for idx , item in enumerate ( data ) : \n                    if typestr == str . __name__ : \n                        data [ idx ] = data [ idx ] . decode ( self . _encoding ) \n                    else : \n                        try : \n                            data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) \n                        except KeyError : \n                            data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) \n                    type_changed = True \n        if colltype == HDF5StorageService . COLL_TUPLE : \n            if type ( data ) is not tuple : \n                data = tuple ( data ) \n                type_changed = True \n    elif colltype == HDF5StorageService . COLL_EMPTY_DICT : \n        data = { } \n        type_changed = True \n    elif isinstance ( data , np . ndarray ) : \n        if typestr == str . __name__ : \n            data = np . core . defchararray . decode ( data , self . _encoding ) \n            type_changed = True \n        if colltype == HDF5StorageService . COLL_MATRIX : \n            data = np . matrix ( data ) \n            type_changed = True \n    return data , type_changed "}
{"8424": "\ndef _all_cut_string ( string , max_length , logger ) : \n    if max_length < len ( string ) : \n        logger . debug ( 'The string `%s` was too long I truncated it to' ' %d characters' % ( string , max_length ) ) \n        string = string [ 0 : max_length - 3 ] + '...' . encode ( 'utf-8' ) \n    return string "}
{"8434": "\ndef _prm_add_meta_info ( self , instance , group , overwrite = False ) : \n    if overwrite : \n        flags = ( ) \n    else : \n        flags = ( HDF5StorageService . ADD_ROW , ) \n    definitely_store_comment = True \n    try : \n        definitely_store_comment = self . _prm_meta_add_summary ( instance ) \n        try : \n            table_name = instance . v_branch + '_overview' \n            table = getattr ( self . _overview_group , table_name ) \n            if pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH > len ( table ) : \n                self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) \n        except pt . NoSuchNodeError : \n            pass \n    except Exception as exc : \n        self . _logger . error ( 'Could not store information table due to `%s`.' % repr ( exc ) ) \n    if ( ( not self . _purge_duplicate_comments or definitely_store_comment ) and instance . v_comment != '' ) : \n        setattr ( group . _v_attrs , HDF5StorageService . COMMENT , instance . v_comment ) \n    setattr ( group . _v_attrs , HDF5StorageService . CLASS_NAME , instance . f_get_class_name ( ) ) \n    setattr ( group . _v_attrs , HDF5StorageService . LEAF , True ) \n    if instance . v_is_parameter and instance . v_explored : \n        try : \n            tablename = 'explored_parameters_overview' \n            table = getattr ( self . _overview_group , tablename ) \n            if pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH > len ( table ) : \n                self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) \n        except pt . NoSuchNodeError : \n            pass \n        except Exception as exc : \n            self . _logger . error ( 'Could not store information ' 'table due to `%s`.' % repr ( exc ) ) "}
{"8436": "\ndef _prm_store_parameter_or_result ( self , instance , store_data = pypetconstants . STORE_DATA , store_flags = None , overwrite = None , with_links = False , recursive = False , _hdf5_group = None , _newly_created = False , ** kwargs ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    elif store_data == pypetconstants . STORE_DATA_SKIPPING and instance . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % instance . v_full_name ) \n        return \n    elif store_data == pypetconstants . OVERWRITE_DATA : \n        if not overwrite : \n            overwrite = True \n    fullname = instance . v_full_name \n    self . _logger . debug ( 'Storing `%s`.' % fullname ) \n    if _hdf5_group is None : \n        _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) \n    store_dict = { } \n    if store_flags is None : \n        store_flags = { } \n    try : \n        if not instance . f_is_empty ( ) : \n            store_dict = instance . _store ( ) \n        try : \n            instance_flags = instance . _store_flags ( ) . copy ( ) \n        except AttributeError : \n            instance_flags = { } \n        instance_flags . update ( store_flags ) \n        store_flags = instance_flags \n        self . _prm_extract_missing_flags ( store_dict , store_flags ) \n        if overwrite : \n            if isinstance ( overwrite , str ) : \n                overwrite = [ overwrite ] \n            if overwrite is True : \n                to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] \n                self . _all_delete_parameter_or_result_or_group ( instance , delete_only = to_delete , _hdf5_group = _hdf5_group ) \n            elif isinstance ( overwrite , ( list , tuple ) ) : \n                overwrite_set = set ( overwrite ) \n                key_set = set ( store_dict . keys ( ) ) \n                stuff_not_to_be_overwritten = overwrite_set - key_set \n                if overwrite != 'v_annotations' and 0 < len ( stuff_not_to_be_overwritten ) : \n                    self . _logger . warning ( 'Cannot overwrite `%s`, these items are not supposed to ' 'be stored by the leaf node.' % str ( stuff_not_to_be_overwritten ) ) \n                stuff_to_overwrite = overwrite_set & key_set \n                if 0 < len ( stuff_to_overwrite ) : \n                    self . _all_delete_parameter_or_result_or_group ( instance , delete_only = list ( stuff_to_overwrite ) ) \n            else : \n                raise ValueError ( 'Your value of overwrite `%s` is not understood. ' 'Please pass `True` of a list of strings to fine grain ' 'overwriting.' % str ( overwrite ) ) \n        self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) \n        self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) \n        if _newly_created or overwrite is True : \n            self . _prm_add_meta_info ( instance , _hdf5_group , overwrite = not _newly_created ) \n        instance . _stored = True \n        self . _node_processing_timer . signal_update ( ) \n    except : \n        self . _logger . error ( 'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname ) \n        for key in store_dict . keys ( ) : \n            if key in _hdf5_group : \n                hdf5_child = _hdf5_group . _f_get_child ( key ) \n                hdf5_child . _f_remove ( recursive = True ) \n        if _hdf5_group . _v_nchildren == 0 : \n            _hdf5_group . _f_remove ( recursive = True ) \n        raise "}
{"8445": "\ndef _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , ** kwargs ) : \n    datasize = data . shape [ 0 ] \n    try : \n        description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) \n        description_dicts = [ { } ] \n        if ptpa . MAX_COLUMNS < len ( description_dict ) : \n            new_table_group = self . _hdf5file . create_group ( where = hdf5_group , name = tablename , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            count = 0 \n            for innerkey in description_dict : \n                val = description_dict [ innerkey ] \n                if count == ptpa . MAX_COLUMNS : \n                    description_dicts . append ( { } ) \n                    count = 0 \n                description_dicts [ - 1 ] [ innerkey ] = val \n                count += 1 \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , 1 ) \n            hdf5_group = new_table_group \n        else : \n            description_dicts = [ description_dict ] \n        for idx , descr_dict in enumerate ( description_dicts ) : \n            if idx == 0 : \n                tblname = tablename \n            else : \n                tblname = tablename + '_%d' % idx \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = datasize , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            row = table . row \n            for n in range ( datasize ) : \n                for key in descr_dict : \n                    row [ key ] = data [ key ] [ n ] \n                row . append ( ) \n            if idx == 0 and ptpa . MAX_COLUMNS >= len ( description_dict ) : \n                for field_name in data_type_dict : \n                    type_description = data_type_dict [ field_name ] \n                    self . _all_set_attr ( table , field_name , type_description ) \n                setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n        if ptpa . MAX_COLUMNS < len ( description_dict ) : \n            tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE \n            field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) \n            data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } \n            descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = len ( field_names ) , filters = self . _all_get_filters ( kwargs ) ) \n            row = table . row \n            for n in range ( len ( field_names ) ) : \n                for key in data_type_table_dict : \n                    row [ key ] = data_type_table_dict [ key ] [ n ] \n                row . append ( ) \n            setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , 1 ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing table `%s` of `%s`.' % ( tablename , fullname ) ) \n        raise "}
{"8448": "\ndef _prm_get_longest_stringsize ( string_list ) : \n    maxlength = 1 \n    for stringar in string_list : \n        if isinstance ( stringar , np . ndarray ) : \n            if 0 < stringar . ndim : \n                for string in stringar . ravel ( ) : \n                    maxlength = max ( len ( string ) , maxlength ) \n            else : \n                maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) \n        else : \n            maxlength = max ( len ( stringar ) , maxlength ) \n    return int ( maxlength * 1.5 ) "}
{"8455": "\ndef make_set_name ( idx ) : \n    GROUPSIZE = 1000 \n    set_idx = idx // GROUPSIZE \n    if 0 <= set_idx : \n        return pypetconstants . FORMATTED_SET_NAME % set_idx \n    else : \n        return pypetconstants . SET_NAME_DUMMY "}
{"8459": "\ndef f_iter_runs ( self , start = 0 , stop = None , step = 1 , yields = 'name' ) : \n    if stop is None : \n        stop = len ( self ) \n    elif len ( self ) < stop : \n        raise ValueError ( 'Stop cannot be larger than the trajectory lenght.' ) \n    yields = yields . lower ( ) \n    if yields == 'name' : \n        yield_func = lambda x : self . f_idx_to_run ( x ) \n    elif yields == 'idx' : \n        yield_func = lambda x : x \n    elif yields == 'self' : \n        yield_func = lambda x : self \n    elif yields == 'copy' : \n        yield_func = lambda x : self . __copy__ ( ) \n    else : \n        raise ValueError ( 'Please choose yields among: `name`, `idx`, or `self`.' ) \n    for idx in range ( start , stop , step ) : \n        self . f_set_crun ( idx ) \n        yield yield_func ( idx ) \n    self . f_set_crun ( None ) "}
{"8464": "\ndef f_get_from_runs ( self , name , include_default_run = True , use_indices = False , fast_access = False , with_links = True , shortcuts = True , max_depth = None , auto_load = False ) : \n    result_dict = OrderedDict ( ) \n    old_crun = self . v_crun \n    try : \n        if 0 < len ( self . _run_parent_groups ) : \n            for run_name in self . f_iter_runs ( ) : \n                value = None \n                already_found = False \n                for run_parent_group in self . _run_parent_groups . values ( ) : \n                    if run_name not in run_parent_group . _children : \n                        continue \n                    try : \n                        value = run_parent_group . f_get ( run_name + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                        if already_found : \n                            raise pex . NotUniqueNodeError ( '`%s` has been found several times ' 'in one run.' % name ) \n                        else : \n                            already_found = True \n                    except ( AttributeError , pex . DataNotInStorageError ) : \n                        pass \n                if value is None and include_default_run : \n                    for run_parent_group in self . _run_parent_groups . values ( ) : \n                        try : \n                            value = run_parent_group . f_get ( self . f_wildcard ( '$' , - 1 ) + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                            if already_found : \n                                raise pex . NotUniqueNodeError ( '`%s` has been found several ' 'times in one run.' % name ) \n                            else : \n                                already_found = True \n                        except ( AttributeError , pex . DataNotInStorageError ) : \n                            pass \n                if value is not None : \n                    if value . v_is_leaf : \n                        value = self . _nn_interface . _apply_fast_access ( value , fast_access ) \n                    if use_indices : \n                        key = self . f_idx_to_run ( run_name ) \n                    else : \n                        key = run_name \n                    result_dict [ key ] = value \n        return result_dict \n    finally : \n        self . v_crun = old_crun "}
{"8475": "\ndef f_load ( self , name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = True , load_data = None , max_depth = None , force = False , dynamic_imports = None , with_run_information = True , with_meta_data = True , storage_service = None , ** kwargs ) : \n    if name is None and index is None : \n        name = self . v_name \n    if as_new : \n        load_parameters = pypetconstants . LOAD_DATA \n        load_derived_parameters = pypetconstants . LOAD_NOTHING \n        load_results = pypetconstants . LOAD_NOTHING \n        load_other_data = pypetconstants . LOAD_NOTHING \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if self . v_storage_service is None or storage_service is not None or 0 < len ( kwargs ) : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = storage_service , trajectory = self , ** kwargs ) \n    if 0 < len ( unused_kwargs ) : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    if dynamic_imports is not None : \n        self . f_add_to_dynamic_imports ( dynamic_imports ) \n    if load_data is not None : \n        load_parameters = load_data \n        load_derived_parameters = load_data \n        load_results = load_data \n        load_other_data = load_data \n    self . _storage_service . load ( pypetconstants . TRAJECTORY , self , trajectory_name = name , trajectory_index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , max_depth = max_depth , with_run_information = with_run_information , with_meta_data = with_meta_data , force = force ) \n    if as_new : \n        for param in self . _parameters . values ( ) : \n            param . f_unlock ( ) "}
{"8477": "\ndef _make_reversed_wildcards ( self , old_length = - 1 ) : \n    if 0 < len ( self . _reversed_wildcards ) : \n        start = old_length \n    else : \n        start = - 1 \n    for wildcards , func in self . _wildcard_functions . items ( ) : \n        for irun in range ( start , len ( self ) ) : \n            translated_name = func ( irun ) \n            if not translated_name in self . _reversed_wildcards : \n                self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) \n            self . _reversed_wildcards [ translated_name ] [ 0 ] . append ( irun ) "}
{"8482": "\ndef _merge_links ( self , other_trajectory , used_runs , allowed_translations , ignore_data ) : \n    linked_items = other_trajectory . _linked_by \n    run_name_dummys = set ( [ f ( - 1 ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) \n    if 0 < len ( linked_items ) : \n        self . _logger . info ( 'Merging potential links!' ) \n        for old_linked_name in other_trajectory . _linked_by : \n            if old_linked_name in ignore_data : \n                continue \n            split_name = old_linked_name . split ( '.' ) \n            if any ( x in run_name_dummys for x in split_name ) : \n                self . _logger . warning ( 'Ignoring all links linking to `%s` because ' 'I don`t know how to resolve links under `%s` nodes.' % ( old_linked_name , str ( run_name_dummys ) ) ) \n                continue \n            old_link_dict = other_trajectory . _linked_by [ old_linked_name ] \n            split_name = old_linked_name . split ( '.' ) \n            if all ( x in allowed_translations for x in split_name ) : \n                new_linked_full_name = self . _rename_full_name ( old_linked_name , other_trajectory , used_runs = used_runs ) \n            else : \n                new_linked_full_name = old_linked_name \n            for linking_node , link_set in old_link_dict . values ( ) : \n                linking_full_name = linking_node . v_full_name \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in run_name_dummys for x in split_name ) : \n                    self . _logger . warning ( 'Ignoring links under `%s` because ' 'I don`t know how to resolve links ' 'under a `%s` node.' % ( linking_full_name , str ( run_name_dummys ) ) ) \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in allowed_translations for x in split_name ) : \n                    new_linking_full_name = self . _rename_full_name ( linking_full_name , other_trajectory , used_runs = used_runs ) \n                else : \n                    new_linking_full_name = linking_full_name \n                for link in link_set : \n                    if ( linking_full_name + '.' + link ) in ignore_data : \n                        continue \n                    if link in run_name_dummys : \n                        self . _logger . warning ( 'Ignoring link `%s` under `%s` because ' 'I don`t know how to resolve ' 'links named as `%s`.' % ( link , linking_full_name , str ( run_name_dummys ) ) ) \n                        continue \n                    try : \n                        new_linked_item = self . f_get ( new_linked_full_name , shortcuts = False ) \n                        if self . f_contains ( new_linking_full_name ) : \n                            new_linking_item = self . f_get ( new_linking_full_name , shortcuts = False ) \n                        else : \n                            new_linking_item = self . f_add_group ( new_linking_full_name ) \n                        if link in allowed_translations : \n                            run_indices , wildcards = other_trajectory . _reversed_wildcards [ link ] \n                            link = self . f_wildcard ( wildcards [ 0 ] , used_runs [ run_indices [ 0 ] ] ) \n                        if not link in new_linking_item . _links : \n                            new_linking_item . f_add_link ( link , new_linked_item ) \n                        else : \n                            self . _logger . debug ( 'Link `%s` exists already under `%s`.' % ( link , new_linked_item . v_full_name ) ) \n                    except ( AttributeError , ValueError ) as exc : \n                        self . _logger . error ( 'Could not copy link `%s` under `%s` linking ' 'to `%s` due to `%s`' % ( link , linking_full_name , old_linked_name , repr ( exc ) ) ) "}
{"8486": "\ndef f_migrate ( self , new_name = None , in_store = False , new_storage_service = None , ** kwargs ) : \n    if new_name is not None : \n        self . _name = new_name \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if new_storage_service is not None or 0 < len ( kwargs ) : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = new_storage_service , trajectory = self , ** kwargs ) \n    if 0 < len ( unused_kwargs ) : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    self . _stored = in_store "}
{"8517": "\ndef _configure_niceness ( kwargs ) : \n    niceness = kwargs [ 'niceness' ] \n    if niceness is not None : \n        try : \n            try : \n                current = os . nice ( 0 ) \n                if 0 < niceness - current : \n                    os . nice ( niceness - current ) \n            except AttributeError : \n                psutil . Process ( ) . nice ( niceness ) \n        except Exception as exc : \n            sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) \n            traceback . print_exc ( ) "}
{"8535": "\ndef _serialize_matrix ( matrix ) : \n    if ( spsp . isspmatrix_csc ( matrix ) or spsp . isspmatrix_csr ( matrix ) or spsp . isspmatrix_bsr ( matrix ) ) : \n        if 0 < matrix . size : \n            return_list = [ matrix . data , matrix . indices , matrix . indptr , matrix . shape ] \n        else : \n            return_list = [ '__empty__' , ( ) , ( ) , matrix . shape ] \n        return_names = SparseParameter . OTHER_NAME_LIST \n        if spsp . isspmatrix_csc ( matrix ) : \n            return_list = [ 'csc' ] + return_list \n        elif spsp . isspmatrix_csr ( matrix ) : \n            return_list = [ 'csr' ] + return_list \n        elif spsp . isspmatrix_bsr ( matrix ) : \n            return_list = [ 'bsr' ] + return_list \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    elif spsp . isspmatrix_dia ( matrix ) : \n        if 0 < matrix . size : \n            return_list = [ 'dia' , matrix . data , matrix . offsets , matrix . shape ] \n        else : \n            return_list = [ 'dia' , '__empty__' , ( ) , matrix . shape ] \n        return_names = SparseParameter . DIA_NAME_LIST \n    else : \n        raise RuntimeError ( 'You shall not pass!' ) \n    hash_list = [ ] \n    for item in return_list : \n        if type ( item ) is np . ndarray : \n            hash_list . append ( HashArray ( item ) ) \n        else : \n            hash_list . append ( item ) \n    return return_list , return_names , tuple ( hash_list ) "}
{"8542": "\ndef f_val_to_str ( self ) : \n    resstrlist = [ ] \n    strlen = 0 \n    for key in self . _data : \n        val = self . _data [ key ] \n        resstr = '%s=%s, ' % ( key , repr ( val ) ) \n        resstrlist . append ( resstr ) \n        strlen += len ( resstr ) \n        if pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH < strlen : \n            break \n    return_string = \"\" . join ( resstrlist ) \n    if pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH < len ( return_string ) : \n        return_string = return_string [ 0 : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3 ] + '...' \n    else : \n        return_string = return_string [ 0 : - 2 ] \n    return return_string "}
{"8545": "\ndef f_get ( self , * args ) : \n    if len ( args ) == 0 : \n        if len ( self . _data ) == 1 : \n            return list ( self . _data . values ( ) ) [ 0 ] \n        elif 1 < len ( self . _data ) : \n            raise ValueError ( 'Your result `%s` contains more than one entry: ' '`%s` Please use >>f_get<< with one of these.' % ( self . v_full_name , str ( list ( self . _data . keys ( ) ) ) ) ) \n        else : \n            raise AttributeError ( 'Your result `%s` is empty, cannot access data.' % self . v_full_name ) \n    result_list = [ ] \n    for name in args : \n        name = self . f_translate_key ( name ) \n        if not name in self . _data : \n            if name == 'data' and len ( self . _data ) == 1 : \n                return self . _data [ list ( self . _data . keys ( ) ) [ 0 ] ] \n            else : \n                raise AttributeError ( '`%s` is not part of your result `%s`.' % ( name , self . v_full_name ) ) \n        result_list . append ( self . _data [ name ] ) \n    if len ( args ) == 1 : \n        return result_list [ 0 ] \n    else : \n        return result_list "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    print ( 'Starting Euler Integration' ) \n    for step in range ( 1 , steps ) : \n        if 1 <= V_array [ step - 1 ] : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        elif spiketimes and tau_ref >= step * dt - spiketimes [ - 1 ] : \n            V_array [ step ] = 0 \n        else : \n            dV = - 1 / tau_V * V_array [ step - 1 ] + I \n            V_array [ step ] = V_array [ step - 1 ] + dV * dt \n    print ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8567": "\ndef _execute_network_run ( self , traj , network , network_dict , component_list , analyser_list , pre_run = False ) : \n    subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) \n    subrun_number = 0 \n    while 0 < len ( subrun_list ) : \n        current_subrun = subrun_list . pop ( 0 ) \n        for component in component_list : \n            component . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . _logger . info ( 'STARTING subrun `%s` (#%d) lasting %s.' % ( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) \n        network . run ( duration = current_subrun . f_get ( ) , report = self . _report , report_period = self . _report_period ) \n        for analyser in analyser_list : \n            analyser . analyse ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for component in component_list : \n            component . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        subrun_number += 1 "}
{"8592": "\ndef gen_timeout_request_renew ( lease ) : \n    time_left = ( lease . rebinding_time - lease . renewing_time ) * RENEW_PERC \n    if 60 > time_left : \n        time_left = 60 \n    logger . debug ( 'Next request in renew will happen on %s' , future_dt_str ( nowutc ( ) , time_left ) ) \n    return time_left "}
{"8598": "\ndef send_discover ( self ) : \n    assert self . client \n    assert self . current_state == STATE_INIT or self . current_state == STATE_SELECTING \n    pkt = self . client . gen_discover ( ) \n    sendp ( pkt ) \n    if MAX_ATTEMPTS_DISCOVER > self . discover_attempts : \n        self . discover_attempts += 1 \n    timeout = gen_timeout_resend ( self . discover_attempts ) \n    self . set_timeout ( self . current_state , self . timeout_selecting , timeout ) "}
{"8600": "\ndef send_request ( self ) : \n    assert self . client \n    if self . current_state == STATE_BOUND : \n        pkt = self . client . gen_request_unicast ( ) \n    else : \n        pkt = self . client . gen_request ( ) \n    sendp ( pkt ) \n    logger . debug ( 'Modifying FSM obj, setting time_sent_request.' ) \n    self . time_sent_request = nowutc ( ) \n    logger . info ( 'DHCPREQUEST of %s on %s to %s port %s' , self . client . iface , self . client . client_ip , self . client . server_ip , self . client . server_port ) \n    if MAX_ATTEMPTS_REQUEST > self . request_attempts : \n        self . request_attempts *= 2 \n        logger . debug ( 'Increased request attempts to %s' , self . request_attempts ) \n    if self . current_state == STATE_RENEWING : \n        timeout_renewing = gen_timeout_request_renew ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_renewing , timeout_renewing ) \n    elif self . current_state == STATE_REBINDING : \n        timeout_rebinding = gen_timeout_request_rebind ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_rebinding , timeout_rebinding ) \n    else : \n        timeout_requesting = gen_timeout_resend ( self . request_attempts ) \n        self . set_timeout ( self . current_state , self . timeout_requesting , timeout_requesting ) "}
{"8610": "\ndef timeout_selecting ( self ) : \n    logger . debug ( 'C2.1: T In %s, timeout receiving response to select.' , self . current_state ) \n    if MAX_OFFERS_COLLECTED <= len ( self . offers ) : \n        logger . debug ( 'C2.2: T Maximum number of offers reached, ' 'raise REQUESTING.' ) \n        raise self . REQUESTING ( ) \n    if MAX_ATTEMPTS_DISCOVER <= self . discover_attempts : \n        logger . debug ( 'C2.3: T Maximum number of discover retries is %s' ' and already sent %s.' , MAX_ATTEMPTS_DISCOVER , self . discover_attempts ) \n        if 0 >= len ( self . offers ) : \n            logger . debug ( 'C2.4: T. But no OFFERS where received, ' 'raise ERROR.' ) \n            raise self . ERROR ( ) \n        logger . debug ( 'C2.4: F. But there is some OFFERS, ' 'raise REQUESTING.' ) \n        raise self . REQUESTING ( ) \n    logger . debug ( 'C2.2: F. Still not received all OFFERS, but not ' 'max # attemps reached, raise SELECTING.' ) \n    raise self . SELECTING ( ) "}
{"8611": "\ndef timeout_requesting ( self ) : \n    logger . debug ( \"C3.2: T. In %s, timeout receiving response to request, \" , self . current_state ) \n    if MAX_ATTEMPTS_REQUEST <= self . discover_requests : \n        logger . debug ( 'C2.3: T. Maximum number %s of REQUESTs ' 'reached, already sent %s, raise ERROR.' , MAX_ATTEMPTS_REQUEST , self . disover_requests ) \n        raise self . ERROR ( ) \n    logger . debug ( \"C2.3: F. Maximum number of REQUESTs retries not reached,\" \"raise REQUESTING.\" ) \n    raise self . REQUESTING ( ) "}
{"8612": "\ndef timeout_request_renewing ( self ) : \n    logger . debug ( \"C5.2:T In %s, timeout receiving response to request.\" , self . current_state ) \n    if MAX_ATTEMPTS_REQUEST <= self . request_attempts : \n        logger . debug ( 'C2.3: T Maximum number %s of REQUESTs ' 'reached, already sent %s, wait to rebinding time.' , MAX_ATTEMPTS_REQUEST , self . disover_requests ) \n    logger . debug ( \"C2.3: F. Maximum number of REQUESTs retries not reached,\" \"raise RENEWING.\" ) \n    raise self . RENEWING ( ) "}
{"8613": "\ndef timeout_request_rebinding ( self ) : \n    logger . debug ( \"C6.2:T In %s, timeout receiving response to request.\" , self . current_state ) \n    if MAX_ATTEMPTS_REQUEST <= self . request_attempts : \n        logger . debug ( 'C.2.3: T. Maximum number %s of REQUESTs ' 'reached, already sent %s, wait lease time expires.' , MAX_ATTEMPTS_REQUEST , self . disover_requests ) \n    logger . debug ( \"C2.3: F. Maximum number of REQUESTs retries not reached,\" \"raise REBINDING.\" ) \n    raise self . REBINDING ( ) "}
{"8614": "\ndef receive_offer ( self , pkt ) : \n    logger . debug ( \"C2. Received OFFER?, in SELECTING state.\" ) \n    if isoffer ( pkt ) : \n        logger . debug ( \"C2: T, OFFER received\" ) \n        self . offers . append ( pkt ) \n        if MAX_OFFERS_COLLECTED <= len ( self . offers ) : \n            logger . debug ( \"C2.5: T, raise REQUESTING.\" ) \n            self . select_offer ( ) \n            raise self . REQUESTING ( ) \n        logger . debug ( \"C2.5: F, raise SELECTING.\" ) \n        raise self . SELECTING ( ) "}
{"8622": "\ndef set ( self , name , value ) : \n    clone = self . _clone ( ) \n    if 1 >= django . VERSION [ 0 ] and 4 >= django . VERSION [ 1 ] : \n        value = value or None \n    clone . _qsl = [ ( q , v ) for ( q , v ) in self . _qsl if q != name ] \n    if value is not None : \n        clone . _qsl . append ( ( name , value ) ) \n    return clone "}
{"8664": "\ndef check_version ( self , version_file ) : \n    with open ( version_file , \"r\" ) as f : \n        version = f . read ( 10 ) \n    version = version . rstrip ( \"\\r\\n\" ) \n    if 10 <= len ( version ) or version != str ( DB_VERSION ) : \n        raise DBError ( \"The quilt meta-data version of %s is not supported \" \"by python-quilt. python-quilt only supports \" \"version %s.\" % ( version , DB_VERSION ) ) "}
{"8685": "\ndef _create_dummy_trip_stoptimes ( trip_id , stops , first_service_time ) : \n    waiting = datetime . timedelta ( seconds = 30 ) \n    arrival = first_service_time \n    last_departure = first_service_time \n    last_departure_hour = ( arrival + waiting ) . hour \n    last_stop = None \n    departure_hour = None \n    arrival_hour = None \n    for stop_sequence , stop in enumerate ( stops ) : \n        arrival = last_departure + get_time_from_last_stop ( last_stop , stop ) \n        departure = arrival + waiting \n        if last_departure_hour > arrival . hour : \n            diff = last_departure_hour \n            arrival_hour = arrival . hour + diff \n            departure_hour = departure . hour + diff \n            last_departure_hour = departure . hour + diff \n        else : \n            arrival_hour = arrival . hour \n            departure_hour = departure . hour \n            last_departure_hour = departure . hour \n        if arrival . hour > departure . hour : \n            diff = last_departure_hour \n            departure_hour = departure . hour + diff \n            last_departure_hour = departure . hour + diff \n        yield { 'trip_id' : trip_id , 'arrival_time' : '{:02}:{}' . format ( arrival_hour , arrival . strftime ( '%M:%S' ) ) , 'departure_time' : '{:02}:{}' . format ( departure_hour , departure . strftime ( '%M:%S' ) ) , 'stop_id' : stop . stop_id , 'stop_sequence' : stop_sequence } \n        last_stop = stop \n        last_departure = departure "}
{"8705": "\ndef record_error ( hostname , exc_info , preceding_stack = None , error_threshold = None , additional_info = None ) : \n    stack = [ ] \n    exc_type , exc_value , sys_traceback = exc_info \n    while sys_traceback is not None : \n        stack . append ( sys_traceback ) \n        sys_traceback = sys_traceback . tb_next \n    stack_lines = [ ] \n    for row in preceding_stack or [ ] : \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( row [ 0 ] ) , line_number = row [ 1 ] , function_name = row [ 2 ] , text = row [ 3 ] ) ) \n    for index , tb in enumerate ( stack ) : \n        filename = tb . tb_frame . f_code . co_filename \n        func_name = tb . tb_frame . f_code . co_name \n        lineno = tb . tb_lineno \n        line = linecache . getline ( filename , lineno , tb . tb_frame . f_globals ) \n        frame_locals = None \n        if ( len ( stack ) - NUM_FRAMES_TO_SAVE ) <= index : \n            frame_locals = dict ( ( k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals . items ( ) ) [ : MAX_LOCALS ] if k != \"self\" ) \n            if \"self\" in tb . tb_frame . f_locals and hasattr ( tb . tb_frame . f_locals [ \"self\" ] , \"__dict__\" ) : \n                frame_locals . update ( dict ( ( \"self.\" + k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals [ \"self\" ] . __dict__ . items ( ) ) [ : MAX_LOCALS ] if k != \"self\" ) ) \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( filename ) , line_number = lineno , function_name = func_name , text = line , frame_locals = frame_locals ) ) \n    key = CachedErrorInfo . get_hash_key ( stack_lines ) \n    info = ERROR_CACHE . get ( key ) or CachedErrorInfo ( ) \n    info . increment ( ) \n    ERROR_CACHE [ key ] = info \n    if info . should_report ( ) : \n        error_count = info . mark_reported ( ) \n        _send_request ( api_ttypes . RecordErrorRequest ( traceback = stack_lines , exception_message = repr ( exc_value ) , exception_type = exc_type . __module__ + \".\" + exc_type . __name__ , hostname = hostname , error_threshold = error_threshold , additional_info = additional_info , error_count = error_count , ) ) "}
{"8709": "\ndef _is_big_enough ( image , size ) : \n    if ( image . size [ 0 ] < size [ 0 ] ) and ( image . size [ 1 ] < size [ 1 ] ) : \n        raise ImageSizeError ( image . size , size ) "}
{"8710": "\ndef _width_is_big_enough ( image , width ) : \n    if image . size [ 0 ] < width : \n        raise ImageSizeError ( image . size [ 0 ] , width ) "}
{"8711": "\ndef _height_is_big_enough ( image , height ) : \n    if image . size [ 1 ] < height : \n        raise ImageSizeError ( image . size [ 1 ] , height ) "}
{"8816": "\ndef parse_args_kwargs ( parser , token ) : \n    bits = token . contents . split ( ' ' ) \n    if 1 >= len ( bits ) : \n        raise template . TemplateSyntaxError ( \"'%s' takes at least one argument\" % bits [ 0 ] ) \n    if token . contents [ 13 ] == '\"' : \n        end_quote = token . contents . index ( '\"' , 14 ) + 1 \n        args = [ template . Variable ( token . contents [ 13 : end_quote ] ) ] \n        kwargs_start = end_quote \n    else : \n        try : \n            next_space = token . contents . index ( ' ' , 14 ) \n            kwargs_start = next_space + 1 \n        except ValueError : \n            next_space = None \n            kwargs_start = None \n        args = [ template . Variable ( token . contents [ 13 : next_space ] ) ] \n    kwargs = { } \n    kwargs_list = token . contents [ kwargs_start : ] . split ( ',' ) \n    for kwargs_item in kwargs_list : \n        if '=' in kwargs_item : \n            k , v = kwargs_item . split ( '=' , 1 ) \n            k = k . strip ( ) \n            kwargs [ k ] = template . Variable ( v ) \n    return args , kwargs "}
{"8835": "\ndef _process_query ( self , query , prepared = False ) : \n    if prepared is True : \n        files = { 'query' : str ( query ) } \n        logger . debug ( 'About to submit the following query {}' . format ( query ) ) \n        res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n        if status == 200 : \n            return self . decode ( res ) , status \n        else : \n            logger . debug ( 'Disambiguation failed.' ) \n            return None , status \n    text = query [ 'text' ] \n    sentence_coordinates = [ { \"offsetStart\" : 0 , \"offsetEnd\" : len ( text ) } ] \n    total_nb_sentences = len ( sentence_coordinates ) \n    sentences_groups = [ ] \n    if self . max_text_length < len ( text ) : \n        res , status_code = self . segment ( text ) \n        if status_code == 200 : \n            sentence_coordinates = res [ 'sentences' ] \n            total_nb_sentences = len ( sentence_coordinates ) \n        else : \n            logger . error ( 'Error during the segmentation of the text.' ) \n        logger . debug ( 'Text too long, split in {} sentences; building groups of {} ' 'sentences.' . format ( total_nb_sentences , self . sentences_per_group ) ) \n        sentences_groups = self . _group_sentences ( total_nb_sentences , self . sentences_per_group ) \n    else : \n        query [ 'sentence' ] = \"true\" \n    if 1 < total_nb_sentences : \n        query [ 'sentences' ] = sentence_coordinates \n    if 0 < len ( sentences_groups ) : \n        for group in sentences_groups : \n            query [ 'processSentence' ] = group \n            res , status_code = self . _process_query ( query , prepared = True ) \n            if status_code == 200 : \n                if 'entities' in res : \n                    query [ 'entities' ] = res [ u'entities' ] \n                query [ 'language' ] = res [ u'language' ] \n            else : \n                logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n                return None , status_code \n    else : \n        res , status_code = self . _process_query ( query , prepared = True ) \n        if status_code == 200 : \n            query [ 'language' ] = res [ u'language' ] \n            if 'entities' in res : \n                query [ 'entities' ] = res [ u'entities' ] \n        else : \n            logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n            return None , status_code \n    return query , status_code "}
{"8836": "\ndef _group_sentences ( total_nb_sentences , group_length ) : \n    sentences_groups = [ ] \n    current_sentence_group = [ ] \n    for i in range ( 0 , total_nb_sentences ) : \n        if i % group_length == 0 : \n            if 0 < len ( current_sentence_group ) : \n                sentences_groups . append ( current_sentence_group ) \n            current_sentence_group = [ i ] \n        else : \n            current_sentence_group . append ( i ) \n    if 0 < len ( current_sentence_group ) : \n        sentences_groups . append ( current_sentence_group ) \n    return sentences_groups "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if len ( unique_labels ) != 2 : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] += 1 \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( class_labels == unique_labels [ 0 ] ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ 0 ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if overall_class_fraction < fraction : \n            self . feature_map [ feature_instance ] = unique_labels [ 0 ] \n        elif fraction == overall_class_fraction : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = unique_labels [ 1 ] \n    return self "}
{"8847": "\ndef fit ( self , features , targets ) : \n    self . feature_map = defaultdict ( lambda : self . default_label ) \n    self . overall_mean_trait_value = np . mean ( targets ) \n    self . mdr_matrix_values = defaultdict ( list ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . mdr_matrix_values [ feature_instance ] . append ( targets [ row_i ] ) \n    for feature_instance in self . mdr_matrix_values : \n        grid_mean_trait_value = np . mean ( self . mdr_matrix_values [ feature_instance ] ) \n        if self . overall_mean_trait_value < grid_mean_trait_value : \n            self . feature_map [ feature_instance ] = 1 \n        elif grid_mean_trait_value == self . overall_mean_trait_value : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = 0 \n    self . feature_map = dict ( self . feature_map ) \n    self . mdr_matrix_values = dict ( self . mdr_matrix_values ) \n    return self "}
{"8878": "\ndef map_lrepr ( entries : Callable [ [ ] , Iterable [ Tuple [ Any , Any ] ] ] , start : str , end : str , meta = None , ** kwargs , ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and 1 > print_level : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    def entry_reprs ( ) : \n        for k , v in entries ( ) : \n            yield \"{k} {v}\" . format ( k = lrepr ( k , ** kwargs ) , v = lrepr ( v , ** kwargs ) ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( entry_reprs ( ) ) . take ( print_length + 1 ) . to_list ( ) \n        if print_length < len ( items ) : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = list ( entry_reprs ( ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8879": "\ndef seq_lrepr ( iterable : Iterable [ Any ] , start : str , end : str , meta = None , ** kwargs ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and 1 > print_level : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( iterable ) . take ( print_length + 1 ) . to_list ( ) \n        if print_length < len ( items ) : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = iterable \n    items = list ( map ( lambda o : lrepr ( o , ** kwargs ) , items ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8893": "\ndef partition ( coll , n : int ) : \n    assert 0 < n \n    start = 0 \n    stop = n \n    while len ( coll ) >= stop : \n        yield tuple ( e for e in coll [ start : stop ] ) \n        start += n \n        stop += n \n    if start < len ( coll ) < stop : \n        stop = len ( coll ) \n        yield tuple ( e for e in coll [ start : stop ] ) "}
{"8895": "\ndef _read_namespaced ( ctx : ReaderContext , allowed_suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : \n    ns : List [ str ] = [ ] \n    name : List [ str ] = [ ] \n    reader = ctx . reader \n    has_ns = False \n    while True : \n        token = reader . peek ( ) \n        if token == \"/\" : \n            reader . next_token ( ) \n            if has_ns : \n                raise SyntaxError ( \"Found '/'; expected word character\" ) \n            elif len ( name ) == 0 : \n                name . append ( \"/\" ) \n            else : \n                if \"/\" in name : \n                    raise SyntaxError ( \"Found '/' after '/'\" ) \n                has_ns = True \n                ns = name \n                name = [ ] \n        elif ns_name_chars . match ( token ) : \n            reader . next_token ( ) \n            name . append ( token ) \n        elif allowed_suffix is not None and token == allowed_suffix : \n            reader . next_token ( ) \n            name . append ( token ) \n        else : \n            break \n    ns_str = None if not has_ns else \"\" . join ( ns ) \n    name_str = \"\" . join ( name ) \n    if ns_str is None : \n        if \"/\" in name_str and name_str != \"/\" : \n            raise SyntaxError ( \"'/' character disallowed in names\" ) \n    assert ns_str is None or 0 < len ( ns_str ) \n    return ns_str , name_str "}
{"8905": "\ndef _read_function ( ctx : ReaderContext ) -> llist . List : \n    if ctx . is_in_anon_fn : \n        raise SyntaxError ( f\"Nested #() definitions not allowed\" ) \n    with ctx . in_anon_fn ( ) : \n        form = _read_list ( ctx ) \n    arg_set = set ( ) \n    def arg_suffix ( arg_num ) : \n        if arg_num is None : \n            return \"1\" \n        elif arg_num == \"&\" : \n            return \"rest\" \n        else : \n            return arg_num \n    def sym_replacement ( arg_num ) : \n        suffix = arg_suffix ( arg_num ) \n        return symbol . symbol ( f\"arg-{suffix}\" ) \n    def identify_and_replace ( f ) : \n        if isinstance ( f , symbol . Symbol ) : \n            if f . ns is None : \n                match = fn_macro_args . match ( f . name ) \n                if match is not None : \n                    arg_num = match . group ( 2 ) \n                    suffix = arg_suffix ( arg_num ) \n                    arg_set . add ( suffix ) \n                    return sym_replacement ( arg_num ) \n        return f \n    body = walk . postwalk ( identify_and_replace , form ) if 0 < len ( form ) else None \n    arg_list : List [ symbol . Symbol ] = [ ] \n    numbered_args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg_set ) ) ) \n    if 0 < len ( numbered_args ) : \n        max_arg = max ( numbered_args ) \n        arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] \n        if \"rest\" in arg_set : \n            arg_list . append ( _AMPERSAND ) \n            arg_list . append ( sym_replacement ( \"rest\" ) ) \n    return llist . l ( _FN , vector . vector ( arg_list ) , body ) "}
{"8912": "\ndef _read_character ( ctx : ReaderContext ) -> str : \n    start = ctx . reader . advance ( ) \n    assert start == \"\\\\\" \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    token = reader . peek ( ) \n    while True : \n        if token == \"\" or whitespace_chars . match ( token ) : \n            break \n        if not alphanumeric_chars . match ( token ) : \n            break \n        s . append ( token ) \n        token = reader . next_token ( ) \n    char = \"\" . join ( s ) \n    special = _SPECIAL_CHARS . get ( char , None ) \n    if special is not None : \n        return special \n    match = unicode_char . match ( char ) \n    if match is not None : \n        try : \n            return chr ( int ( f\"0x{match.group(1)}\" , 16 ) ) \n        except ( ValueError , OverflowError ) : \n            raise SyntaxError ( f\"Unsupported character \\\\u{char}\" ) from None \n    if 1 < len ( char ) : \n        raise SyntaxError ( f\"Unsupported character \\\\{char}\" ) \n    return char "}
{"8921": "\ndef pushback ( self ) -> None : \n    if self . _pushback_depth < abs ( self . _idx - 1 ) : \n        raise IndexError ( \"Exceeded pushback depth\" ) \n    self . _idx -= 1 "}
{"8922": "\ndef next_token ( self ) -> str : \n    if StreamReader . DEFAULT_INDEX > self . _idx : \n        self . _idx += 1 \n    else : \n        c = self . _stream . read ( 1 ) \n        self . _update_loc ( c ) \n        self . _buffer . append ( c ) \n    return self . peek ( ) "}
{"8936": "\ndef _load_attr ( name : str , ctx : ast . AST = ast . Load ( ) ) -> ast . Attribute : \n    attrs = name . split ( \".\" ) \n    def attr_node ( node , idx ) : \n        if len ( attrs ) <= idx : \n            node . ctx = ctx \n            return node \n        return attr_node ( ast . Attribute ( value = node , attr = attrs [ idx ] , ctx = ast . Load ( ) ) , idx + 1 ) \n    return attr_node ( ast . Name ( id = attrs [ 0 ] , ctx = ast . Load ( ) ) , 1 ) "}
{"9048": "\ndef decode_date ( self , val ) : \n    if isinstance ( val , basestring ) and val . count ( '-' ) == 2 and 9 < len ( val ) : \n        try : \n            dt = dateutil . parser . parse ( val ) \n            if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : \n                dt = dt . replace ( tzinfo = None ) \n            return dt \n        except ( TypeError , ValueError ) : \n            pass \n    return val "}
{"9057": "\ndef add_operator ( self , operator ) : \n    if not isinstance ( operator , Operator ) : \n        raise FiqlObjectException ( \"%s is not a valid element type\" % ( operator . __class__ ) ) \n    if not self . _working_fragment . operator : \n        self . _working_fragment . operator = operator \n    elif self . _working_fragment . operator < operator : \n        last_constraint = self . _working_fragment . elements . pop ( ) \n        self . _working_fragment = self . _working_fragment . create_nested_expression ( ) \n        self . _working_fragment . add_element ( last_constraint ) \n        self . _working_fragment . add_operator ( operator ) \n    elif self . _working_fragment . operator > operator : \n        if self . _working_fragment . parent : \n            return self . _working_fragment . parent . add_operator ( operator ) \n        else : \n            return Expression ( ) . add_element ( self . _working_fragment ) . add_operator ( operator ) \n    return self "}
{"9075": "\ndef _validate_yourls_response ( response , data ) : \n    try : \n        response . raise_for_status ( ) \n    except HTTPError as http_exc : \n        http_error_info = sys . exc_info ( ) \n        reraise = False \n        try : \n            jsondata = response . json ( ) \n        except ValueError : \n            reraise = True \n        else : \n            logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) \n            _handle_api_error_with_json ( http_exc , jsondata , response ) \n        if reraise : \n            six . reraise ( * http_error_info ) \n    else : \n        jsondata = response . json ( ) \n        logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) \n        if set ( jsondata . keys ( ) ) >= { 'status' , 'code' , 'message' } : \n            status = jsondata [ 'status' ] \n            code = jsondata [ 'code' ] \n            message = jsondata [ 'message' ] \n            if status == 'fail' : \n                if code == 'error:keyword' : \n                    raise YOURLSKeywordExistsError ( message , keyword = data [ 'keyword' ] ) \n                elif code == 'error:url' : \n                    url = _json_to_shortened_url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) \n                    raise YOURLSURLExistsError ( message , url = url ) \n                else : \n                    raise YOURLSAPIError ( message ) \n            else : \n                return jsondata \n        else : \n            return jsondata "}
{"9078": "\ndef _get_indep_vector ( wave_a , wave_b ) : \n    exobj = pexdoc . exh . addex ( RuntimeError , \"Independent variable ranges do not overlap\" ) \n    min_bound = max ( np . min ( wave_a . indep_vector ) , np . min ( wave_b . indep_vector ) ) \n    max_bound = min ( np . max ( wave_a . indep_vector ) , np . max ( wave_b . indep_vector ) ) \n    exobj ( bool ( max_bound < min_bound ) ) \n    raw_range = np . unique ( np . concatenate ( ( wave_a . indep_vector , wave_b . indep_vector ) ) ) \n    return raw_range [ np . logical_and ( raw_range >= min_bound , max_bound >= raw_range ) ] "}
{"9089": "\ndef term_echo ( command , nindent = 0 , env = None , fpointer = None , cols = 60 ) : \n    os . environ [ \"COLUMNS\" ] = str ( cols ) \n    command_int = command \n    if env : \n        for var , repl in env . items ( ) : \n            command_int = command_int . replace ( \"${\" + var + \"}\" , repl ) \n    tokens = command_int . split ( \" \" ) \n    if ( platform . system ( ) . lower ( ) == \"windows\" ) and ( tokens [ 0 ] . endswith ( \".py\" ) ) : \n        tokens = [ sys . executable ] + tokens \n    proc = subprocess . Popen ( tokens , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n    stdout = proc . communicate ( ) [ 0 ] \n    if 0x03000000 <= sys . hexversion : \n        stdout = stdout . decode ( \"utf-8\" ) \n    stdout = stdout . split ( \"\\n\" ) \n    indent = nindent * \" \" \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}.. code-block:: bash\\n\" . format ( indent ) , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}    $ {1}\\n\" . format ( indent , command ) , dedent = False ) \n    for line in stdout : \n        if line . strip ( ) : \n            fpointer ( indent + \"    \" + line . replace ( \"\\t\" , \"    \" ) + \"\\n\" , dedent = False ) \n        else : \n            fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) "}
{"9090": "\ndef log ( self , msg , level = 2 ) : \n    if level <= self . verbosity : \n        self . stdout . write ( msg ) "}
{"9112": "\ndef _validate_min_max ( wave , indep_min , indep_max ) : \n    imin , imax = False , False \n    if indep_min is None : \n        indep_min = wave . _indep_vector [ 0 ] \n        imin = True \n    if indep_max is None : \n        indep_max = wave . _indep_vector [ - 1 ] \n        imax = True \n    if imin and imax : \n        return indep_min , indep_max \n    exminmax = pexdoc . exh . addex ( RuntimeError , \"Incongruent `indep_min` and `indep_max` arguments\" ) \n    exmin = pexdoc . exh . addai ( \"indep_min\" ) \n    exmax = pexdoc . exh . addai ( \"indep_max\" ) \n    exminmax ( bool ( indep_max <= indep_min ) ) \n    exmin ( bool ( ( wave . _indep_vector [ 0 ] > indep_min ) and ( not np . isclose ( indep_min , wave . _indep_vector [ 0 ] , FP_RTOL , FP_ATOL ) ) ) ) \n    exmax ( bool ( ( wave . _indep_vector [ - 1 ] < indep_max ) and ( not np . isclose ( indep_max , wave . _indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) ) ) \n    return indep_min , indep_max "}
{"9113": "\ndef acos ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( - 1 > min ( wave . _dep_vector ) ) or ( 1 < max ( wave . _dep_vector ) ) ) , ) \n    return _operation ( wave , \"acos\" , \"rad\" , np . arccos ) "}
{"9114": "\ndef acosh ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( 1 > min ( wave . _dep_vector ) ) ) \n    return _operation ( wave , \"acosh\" , \"\" , np . arccosh ) "}
{"9115": "\ndef asin ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( - 1 > min ( wave . _dep_vector ) ) or ( 1 < max ( wave . _dep_vector ) ) ) , ) \n    return _operation ( wave , \"asin\" , \"rad\" , np . arcsin ) "}
{"9116": "\ndef atanh ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( - 1 > min ( wave . _dep_vector ) ) or ( 1 < max ( wave . _dep_vector ) ) ) , ) \n    return _operation ( wave , \"atanh\" , \"\" , np . arctanh ) "}
{"9118": "\ndef db ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( 0 >= np . min ( np . abs ( wave . _dep_vector ) ) ) ) ) \n    ret = copy . copy ( wave ) \n    ret . dep_units = \"dB\" \n    ret . dep_name = \"db({0})\" . format ( ret . dep_name ) \n    ret . _dep_vector = 20.0 * np . log10 ( np . abs ( ret . _dep_vector ) ) \n    return ret "}
{"9131": "\ndef log ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( 0 >= min ( wave . _dep_vector ) ) ) ) \n    return _operation ( wave , \"log\" , \"\" , np . log ) "}
{"9139": "\ndef subwave ( wave , dep_name = None , indep_min = None , indep_max = None , indep_step = None ) : \n    ret = copy . copy ( wave ) \n    if dep_name is not None : \n        ret . dep_name = dep_name \n    _bound_waveform ( ret , indep_min , indep_max ) \n    pexdoc . addai ( \"indep_step\" , bool ( ( indep_step is not None ) and ( 0 >= indep_step ) ) ) \n    exmsg = \"Argument `indep_step` is greater than independent vector range\" \n    cond = bool ( ( indep_step is not None ) and ( ret . _indep_vector [ - 1 ] - ret . _indep_vector [ 0 ] < indep_step ) ) \n    pexdoc . addex ( RuntimeError , exmsg , cond ) \n    if indep_step : \n        indep_vector = _barange ( indep_min , indep_max , indep_step ) \n        dep_vector = _interp_dep_vector ( ret , indep_vector ) \n        ret . _set_indep_vector ( indep_vector , check = False ) \n        ret . _set_dep_vector ( dep_vector , check = False ) \n    return ret "}
{"9143": "\ndef wvalue ( wave , indep_var ) : \n    close_min = np . isclose ( indep_var , wave . _indep_vector [ 0 ] , FP_RTOL , FP_ATOL ) \n    close_max = np . isclose ( indep_var , wave . _indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) \n    pexdoc . exh . addex ( ValueError , \"Argument `indep_var` is not in the independent variable vector range\" , bool ( ( ( wave . _indep_vector [ 0 ] > indep_var ) and ( not close_min ) ) or ( ( wave . _indep_vector [ - 1 ] < indep_var ) and ( not close_max ) ) ) , ) \n    if close_min : \n        return wave . _dep_vector [ 0 ] \n    if close_max : \n        return wave . _dep_vector [ - 1 ] \n    idx = np . searchsorted ( wave . _indep_vector , indep_var ) \n    xdelta = wave . _indep_vector [ idx ] - wave . _indep_vector [ idx - 1 ] \n    ydelta = wave . _dep_vector [ idx ] - wave . _dep_vector [ idx - 1 ] \n    slope = ydelta / float ( xdelta ) \n    return wave . _dep_vector [ idx - 1 ] + slope * ( indep_var - wave . _indep_vector [ idx - 1 ] ) "}
{"9146": "\ndef _build_expr ( tokens , higher_oplevel = - 1 , ldelim = \"(\" , rdelim = \")\" ) : \n    if isinstance ( tokens , str ) : \n        return tokens \n    if len ( tokens ) == 2 : \n        return \"\" . join ( tokens ) \n    oplevel = _get_op_level ( tokens [ 1 ] ) \n    stoken = \"\" \n    for num , item in enumerate ( tokens ) : \n        if num % 2 == 0 : \n            stoken += _build_expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) \n        else : \n            stoken += item \n    if ( higher_oplevel > oplevel ) or ( ( oplevel == higher_oplevel ) and ( oplevel in _OP_PREC_PAR ) ) : \n        stoken = ldelim + stoken + rdelim \n    return stoken "}
{"9147": "\ndef _next_rdelim ( items , pos ) : \n    for num , item in enumerate ( items ) : \n        if pos < item : \n            break \n    else : \n        raise RuntimeError ( \"Mismatched delimiters\" ) \n    del items [ num ] \n    return item "}
{"9154": "\ndef no_exp ( number ) : \n    mant , exp = to_scientific_tuple ( number ) \n    if not exp : \n        return str ( number ) \n    floating_mant = \".\" in mant \n    mant = mant . replace ( \".\" , \"\" ) \n    if 0 > exp : \n        return \"0.\" + \"0\" * ( - exp - 1 ) + mant \n    if not floating_mant : \n        return mant + \"0\" * exp + ( \".0\" if isinstance ( number , float ) else \"\" ) \n    lfpart = len ( mant ) - 1 \n    if exp > lfpart : \n        return ( mant + \"0\" * ( exp - lfpart ) ) . rstrip ( \".\" ) \n    return mant "}
{"9155": "\ndef peng ( number , frac_length , rjust = True ) : \n    if number == 0 : \n        number = \"0.{zrs}\" . format ( zrs = \"0\" * frac_length ) if frac_length else \"0\" \n        return \"{0} \" . format ( number . rjust ( 5 + frac_length ) ) if rjust else number \n    sign = + 1 if 0 <= number else - 1 \n    ssign = \"-\" if sign == - 1 else \"\" \n    anumber = abs ( number ) \n    if 1e-24 > anumber : \n        anumber = 1e-24 \n        number = sign * 1e-24 \n    exp = 3.0 * math . floor ( math . floor ( math . log10 ( anumber ) ) / 3.0 ) \n    mant = number / 10 ** exp \n    smant = str ( mant ) \n    ppos = smant . find ( \".\" ) \n    if frac_length < len ( smant ) - ppos - 1 : \n        mant += sign * 5 * 10 ** ( - frac_length - 1 ) \n        if 1000 <= abs ( mant ) : \n            exp += 3 \n            mant = mant / 1e3 \n        smant = str ( mant ) \n        ppos = smant . find ( \".\" ) \n    bfrac_length = bool ( frac_length ) \n    flength = ppos - ( not bfrac_length ) + frac_length + 1 \n    new_mant = smant [ : flength ] . ljust ( flength , \"0\" ) \n    if 24 < exp : \n        new_mant , exp = ( \"{sign}999.{frac}\" . format ( sign = ssign , frac = \"9\" * frac_length ) , 24 , ) \n    new_mant = new_mant . rjust ( rjust * ( 4 + bfrac_length + frac_length ) ) \n    num = \"{mant}{suffix}\" . format ( mant = new_mant , suffix = _POWER_TO_SUFFIX_DICT [ exp ] if exp else \" \" * bool ( rjust ) ) \n    return num "}
{"9162": "\ndef to_scientific_string ( number , frac_length = None , exp_length = None , sign_always = False ) : \n    try : \n        number = - 1e20 if np . isneginf ( number ) else number \n    except : \n        pass \n    try : \n        number = + 1e20 if np . isposinf ( number ) else number \n    except : \n        pass \n    exp_length = 0 if not exp_length else exp_length \n    mant , exp = to_scientific_tuple ( number ) \n    fmant = float ( mant ) \n    if ( not frac_length ) or ( fmant == int ( fmant ) ) : \n        return \"{sign}{mant}{period}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( 0 <= fmant ) else \"\" , mant = mant , period = \".\" if frac_length else \"\" , zeros = \"0\" * frac_length if frac_length else \"\" , exp_sign = \"-\" if 0 > exp else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) \n    rounded_mant = round ( fmant , frac_length ) \n    if abs ( rounded_mant ) == 10 : \n        rounded_mant = fmant = - 1.0 if 0 > number else 1.0 \n        frac_length = 1 \n        exp = exp + 1 \n    zeros = 2 + ( 1 if ( 0 > fmant ) else 0 ) + frac_length - len ( str ( rounded_mant ) ) \n    return \"{sign}{mant}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( 0 <= fmant ) else \"\" , mant = rounded_mant , zeros = \"0\" * zeros , exp_sign = \"-\" if 0 > exp else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) "}
{"9163": "\ndef to_scientific_tuple ( number ) : \n    convert = not isinstance ( number , str ) \n    if ( convert and ( number == 0 ) ) or ( ( not convert ) and ( not number . strip ( \"0\" ) . strip ( \".\" ) ) ) : \n        return ( \"0\" , 0 ) \n    sign , digits , exp = Decimal ( str ( number ) if convert else number ) . as_tuple ( ) \n    mant = ( \"{sign}{itg}{frac}\" . format ( sign = \"-\" if sign else \"\" , itg = digits [ 0 ] , frac = ( \".{frac}\" . format ( frac = \"\" . join ( [ str ( num ) for num in digits [ 1 : ] ] ) ) if 1 < len ( digits ) else \"\" ) , ) . rstrip ( \"0\" ) . rstrip ( \".\" ) ) \n    exp += len ( digits ) - 1 \n    return NumComp ( mant , exp ) "}
{"9164": "\ndef find_sourcemap_comment ( filepath , block_size = 100 ) : \n    MAX_TRACKBACK = 2 \n    block_number = - 1 \n    blocks = [ ] \n    sourcemap = None \n    try : \n        of = io . open ( filepath , 'br+' ) \n        of . seek ( 0 , os . SEEK_END ) \n        block_end_byte = of . tell ( ) \n        while 0 < block_end_byte and 0 < MAX_TRACKBACK : \n            if ( 0 < block_end_byte - block_size ) : \n                of . seek ( block_number * block_size , os . SEEK_END ) \n                blocks . append ( of . read ( block_size ) ) \n            else : \n                of . seek ( 0 , os . SEEK_SET ) \n                blocks = [ of . read ( block_end_byte ) ] \n            content = b'' . join ( reversed ( blocks ) ) \n            lines_found = content . count ( b'\\n' ) \n            MAX_TRACKBACK -= lines_found \n            block_end_byte -= block_size \n            block_number -= 1 \n            if SOURCEMAPPING_URL_COMMENT in content : \n                offset = 0 \n                lines = content . split ( b'\\n' ) \n                for i , line in enumerate ( lines ) : \n                    if line . startswith ( SOURCEMAPPING_URL_COMMENT ) : \n                        offset = len ( line ) \n                        sourcemap = line \n                        break \n                while len ( lines ) > i + 1 : \n                    offset += 1 \n                    offset += len ( lines [ i + 1 ] ) \n                    i += 1 \n                if sourcemap : \n                    offset += 1 \n                    of . seek ( - offset , os . SEEK_END ) \n                    of . truncate ( ) \n                return force_text ( sourcemap ) \n    finally : \n        of . close ( ) \n    return sourcemap "}
{"9169": "\ndef format_hexdump ( arg ) : \n    line = '' \n    for i in range ( 0 , len ( arg ) , 16 ) : \n        if 0 < i : \n            line += '\\n' \n        chunk = arg [ i : i + 16 ] \n        hex_chunk = hexlify ( chunk ) . decode ( 'utf-8' ) \n        hex_line = ' ' . join ( hex_chunk [ j : j + 2 ] for j in range ( 0 , len ( hex_chunk ) , 2 ) ) \n        if ( 3 * 16 ) - 1 > len ( hex_line ) : \n            hex_line += ' ' * ( ( ( 3 * 16 ) - 1 ) - len ( hex_line ) ) \n        ascii_line = '' . join ( _convert_to_ascii ( x ) for x in chunk ) \n        offset_line = '%08x' % i \n        line += \"%s  %s  %s\" % ( offset_line , hex_line , ascii_line ) \n    return line "}
{"9179": "\ndef process_arguments ( self , func , args ) : \n    pos_args = [ ] \n    kw_args = { } \n    while 0 < len ( args ) : \n        if func . metadata . spec_filled ( pos_args , kw_args ) and not self . _is_flag ( args [ 0 ] ) : \n            break \n        arg = args . pop ( 0 ) \n        if arg == '--' : \n            break \n        elif self . _is_flag ( arg ) : \n            arg_value = None \n            arg_name = None \n            if len ( arg ) == 2 : \n                arg_name = func . metadata . match_shortname ( arg [ 1 : ] , filled_args = pos_args ) \n            else : \n                if not arg . startswith ( '--' ) : \n                    raise ArgumentError ( \"Invalid method of specifying keyword argument that did not start with --\" , argument = arg ) \n                arg = arg [ 2 : ] \n                if '=' in arg : \n                    arg , arg_value = arg . split ( '=' , 1 ) \n                arg_name = func . metadata . match_shortname ( arg , filled_args = pos_args ) \n            arg_type = func . metadata . param_type ( arg_name ) \n            if arg_type is None : \n                raise ArgumentError ( \"Attempting to set a parameter from command line that does not have type information\" , argument = arg_name ) \n            if arg_value is None : \n                arg_value = self . _extract_arg_value ( arg_name , arg_type , args ) \n            kw_args [ arg_name ] = arg_value \n        else : \n            pos_args . append ( arg ) \n    if 0 < len ( args ) and args [ 0 ] == '--' : \n        args . pop ( 0 ) \n    return pos_args , kw_args , args "}
{"9180": "\ndef _extract_arg_value ( cls , arg_name , arg_type , remaining ) : \n    next_arg = None \n    should_consume = False \n    if 0 < len ( remaining ) : \n        next_arg = remaining [ 0 ] \n        should_consume = True \n        if next_arg == '--' : \n            next_arg = None \n    if arg_type == \"bool\" : \n        if next_arg is None or next_arg . startswith ( '-' ) : \n            next_arg = True \n            should_consume = False \n    else : \n        if next_arg is None : \n            raise ArgumentError ( \"Could not find value for keyword argument\" , argument = arg_name ) \n    if should_consume : \n        remaining . pop ( 0 ) \n    return next_arg "}
{"9182": "\ndef invoke ( self , line ) : \n    finished = True \n    while 0 < len ( line ) : \n        val , line , finished = self . invoke_one ( line ) \n        if val is not None : \n            iprint ( val ) \n    return finished "}
{"9184": "\ndef parse_param ( param , include_desc = False ) : \n    param_def , _colon , desc = param . partition ( ':' ) \n    if not include_desc : \n        desc = None \n    else : \n        desc = desc . lstrip ( ) \n    if _colon == \"\" : \n        raise ValidationError ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) \n    param_name , _space , param_type = param_def . partition ( ' ' ) \n    if 2 > len ( param_type ) or param_type [ 0 ] != '(' or param_type [ - 1 ] != ')' : \n        raise ValidationError ( \"Invalid parameter type string not enclosed in ( ) characters\" , param_string = param_def , type_string = param_type ) \n    param_type = param_type [ 1 : - 1 ] \n    return param_name , ParameterInfo ( param_type , [ ] , desc ) "}
{"9188": "\ndef _join_paragraphs ( cls , lines , use_indent = False , leading_blanks = False , trailing_blanks = False ) : \n    curr_para = [ ] \n    paragraphs = [ ] \n    for line in lines : \n        if use_indent : \n            if line . startswith ( ' ' ) : \n                curr_para . append ( line . lstrip ( ) ) \n                continue \n            elif line == '' : \n                continue \n            else : \n                if 0 < len ( curr_para ) : \n                    paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ line . lstrip ( ) ] \n        else : \n            if len ( line ) != 0 : \n                curr_para . append ( line ) \n            else : \n                paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ ] \n    if 0 < len ( curr_para ) : \n        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n    return paragraphs "}
{"9189": "\ndef wrap_and_format ( self , width = None , include_params = False , include_return = False , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        elif isinstance ( line , BlankLine ) : \n            out . write ( '\\n' ) \n        elif isinstance ( line , ListItem ) : \n            out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ 0 ] , subsequent_indent = \"   \" , width = width ) ) \n            out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if 0 < len ( included_params ) : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        print ( \"Returns:\" ) \n        print ( \"    \" + self . return_info . type_name ) \n    return out . getvalue ( ) "}
{"9191": "\ndef convert_from_binary ( self , binvalue , type , ** kwargs ) : \n    size = self . get_type_size ( type ) \n    if 0 < size and len ( binvalue ) != size : \n        raise ArgumentError ( \"Could not convert type from binary since the data was not the correct size\" , required_size = size , actual_size = len ( binvalue ) , type = type ) \n    typeobj = self . get_type ( type ) \n    if not hasattr ( typeobj , 'convert_binary' ) : \n        raise ArgumentError ( \"Type does not support conversion from binary\" , type = type ) \n    return typeobj . convert_binary ( binvalue , ** kwargs ) "}
{"9202": "\ndef spec_filled ( self , pos_args , kw_args ) : \n    req_names = self . arg_names \n    if 0 < len ( self . arg_defaults ) : \n        req_names = req_names [ : - len ( self . arg_defaults ) ] \n    req = [ x for x in req_names if x not in kw_args ] \n    return len ( pos_args ) >= len ( req ) "}
{"9206": "\ndef match_shortname ( self , name , filled_args = None ) : \n    filled_count = 0 \n    if filled_args is not None : \n        filled_count = len ( filled_args ) \n    possible = [ x for x in self . arg_names [ filled_count : ] if x . startswith ( name ) ] \n    if len ( possible ) == 0 : \n        raise ArgumentError ( \"Could not convert short-name full parameter name, none could be found\" , short_name = name , parameters = self . arg_names ) \n    elif 1 < len ( possible ) : \n        raise ArgumentError ( \"Short-name is ambiguous, could match multiple keyword parameters\" , short_name = name , possible_matches = possible ) \n    return possible [ 0 ] "}
{"9208": "\ndef signature ( self , name = None ) : \n    self . _ensure_loaded ( ) \n    if name is None : \n        name = self . name \n    num_args = len ( self . arg_names ) \n    num_def = 0 \n    if self . arg_defaults is not None : \n        num_def = len ( self . arg_defaults ) \n    num_no_def = num_args - num_def \n    args = [ ] \n    for i in range ( 0 , len ( self . arg_names ) ) : \n        typestr = \"\" \n        if self . arg_names [ i ] in self . annotated_params : \n            typestr = \"{} \" . format ( self . annotated_params [ self . arg_names [ i ] ] . type_name ) \n        if num_no_def <= i : \n            default = str ( self . arg_defaults [ i - num_no_def ] ) \n            if len ( default ) == 0 : \n                default = \"''\" \n            args . append ( \"{}{}={}\" . format ( typestr , str ( self . arg_names [ i ] ) , default ) ) \n        else : \n            args . append ( typestr + str ( self . arg_names [ i ] ) ) \n    return \"{}({})\" . format ( name , \", \" . join ( args ) ) "}
{"9211": "\ndef check_spec ( self , pos_args , kwargs = None ) : \n    if kwargs is None : \n        kwargs = { } \n    if self . varargs is not None or self . kwargs is not None : \n        raise InternalError ( \"check_spec cannot be called on a function that takes *args or **kwargs\" ) \n    missing = object ( ) \n    arg_vals = [ missing ] * len ( self . arg_names ) \n    kw_indices = { name : i for i , name in enumerate ( self . arg_names ) } \n    for i , arg in enumerate ( pos_args ) : \n        if len ( arg_vals ) <= i : \n            raise ArgumentError ( \"Too many positional arguments, first excessive argument=%s\" % str ( arg ) ) \n        arg_vals [ i ] = arg \n    for arg , val in kwargs . items ( ) : \n        index = kw_indices . get ( arg ) \n        if index is None : \n            raise ArgumentError ( \"Cannot find argument by name: %s\" % arg ) \n        if arg_vals [ index ] is not missing : \n            raise ValidationError ( \"Argument %s passed twice\" % arg ) \n        arg_vals [ index ] = val \n    if 0 < len ( self . arg_defaults ) : \n        for i in range ( 0 , len ( self . arg_defaults ) ) : \n            neg_index = - len ( self . arg_defaults ) + i \n            if arg_vals [ neg_index ] is missing : \n                arg_vals [ neg_index ] = self . arg_defaults [ i ] \n    if missing in arg_vals : \n        index = arg_vals . index ( missing ) \n        raise ArgumentError ( \"Missing a required argument (position: %d, name: %s)\" % ( index , self . arg_names [ index ] ) ) \n    return { name : val for name , val in zip ( self . arg_names , arg_vals ) } "}
{"9216": "\ndef _parse_validators ( valids ) : \n    outvals = [ ] \n    for val in valids : \n        if isinstance ( val , str ) : \n            args = [ ] \n        elif 1 < len ( val ) : \n            args = val [ 1 : ] \n            val = val [ 0 ] \n        else : \n            raise ValidationError ( \"You must pass either an n-tuple or a string to define a validator\" , validator = val ) \n        name = \"validate_%s\" % str ( val ) \n        outvals . append ( ( name , args ) ) \n    return outvals "}
{"9254": "\ndef step_towards ( self , other ) : \n    return self + Vector ( ( ( other [ 0 ] > self [ 0 ] ) - ( other [ 0 ] < self [ 0 ] ) , ( other [ 1 ] > self [ 1 ] ) - ( other [ 1 ] < self [ 1 ] ) , ) ) "}
{"9263": "\ndef process_item ( self , item , spider ) : \n    self . items . append ( item ) \n    if self . max_chunk_size <= len ( self . items ) : \n        self . _upload_chunk ( spider ) \n    return item "}
{"9283": "\ndef is_hash160 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return False \n    if not len ( s ) == 40 : \n        return False \n    for c in s : \n        if ( '0' > c or '9' < c ) and ( 'A' > c or 'F' < c ) and ( 'a' > c or 'f' < c ) : \n            return False \n    return True "}
{"9286": "\ndef first_kwonly_arg ( name ) : \n    def decorate ( wrapped ) : \n        if sys . version_info [ 0 ] == 2 : \n            arg_names , varargs , _ , defaults = inspect . getargspec ( wrapped ) \n        else : \n            arg_names , varargs , _ , defaults = inspect . getfullargspec ( wrapped ) [ : 4 ] \n        if not defaults : \n            raise TypeError ( \"You can't use @first_kwonly_arg on a function that doesn't have default arguments!\" ) \n        first_default_index = len ( arg_names ) - len ( defaults ) \n        if name is FIRST_DEFAULT_ARG : \n            first_kwonly_index = first_default_index \n        else : \n            try : \n                first_kwonly_index = arg_names . index ( name ) \n            except ValueError : \n                raise ValueError ( \"%s() doesn't have an argument with the specified first_kwonly_arg=%r name\" % ( getattr ( wrapped , '__name__' , '?' ) , name ) ) \n        if first_default_index > first_kwonly_index : \n            raise ValueError ( \"The specified first_kwonly_arg=%r must have a default value!\" % ( name , ) ) \n        kwonly_defaults = defaults [ - ( len ( arg_names ) - first_kwonly_index ) : ] \n        kwonly_args = tuple ( zip ( arg_names [ first_kwonly_index : ] , kwonly_defaults ) ) \n        required_kwonly_args = frozenset ( arg for arg , default in kwonly_args if default is KWONLY_REQUIRED ) \n        def wrapper ( * args , ** kwargs ) : \n            if required_kwonly_args : \n                missing_kwonly_args = required_kwonly_args . difference ( kwargs . keys ( ) ) \n                if missing_kwonly_args : \n                    raise TypeError ( \"%s() missing %s keyword-only argument(s): %s\" % ( getattr ( wrapped , '__name__' , '?' ) , len ( missing_kwonly_args ) , ', ' . join ( sorted ( missing_kwonly_args ) ) ) ) \n            if first_kwonly_index < len ( args ) : \n                if varargs is None : \n                    raise TypeError ( \"%s() takes exactly %s arguments (%s given)\" % ( getattr ( wrapped , '__name__' , '?' ) , first_kwonly_index , len ( args ) ) ) \n                kwonly_args_from_kwargs = tuple ( kwargs . pop ( arg , default ) for arg , default in kwonly_args ) \n                args = args [ : first_kwonly_index ] + kwonly_args_from_kwargs + args [ first_kwonly_index : ] \n            return wrapped ( * args , ** kwargs ) \n        return update_wrapper ( wrapper , wrapped ) \n    return decorate "}
{"9316": "\ndef decode_obj ( obj , force = False ) : \n    if isinstance ( obj , unicode ) : \n        return obj \n    elif isinstance ( obj , bytes ) : \n        if force_encoding is not None : \n            return obj . decode ( force_encoding ) \n        if chardet : \n            enc_guess = chardet . detect ( obj ) \n            if 0.7 < enc_guess [ 'confidence' ] : \n                return obj . decode ( enc_guess [ 'encoding' ] ) \n        return obj . decode ( 'utf-8' ) \n    else : \n        return obj if not force else repr ( obj ) "}
{"9337": "\ndef FindPyData ( self , start , py_data ) : \n    wx_data = self . _wx_data_map [ py_data ] \n    if ( 3 , 0 , 0 ) > wx . VERSION or 'classic' in wx . version ( ) : \n        data = self . FindItemData ( start , wx_data ) \n    else : \n        data = self . FindItem ( start , wx_data ) \n    return data "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max_cols = 80 ) : \n    try : \n        name = getattr ( obj , \"name\" , \"\" ) \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + 1 + indent * 4 + ( 5 if context else 0 ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if k == \"index\" : \n                continue \n            if k == \"parent\" and parent != \"\" : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or 0 < v ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if max_cols - padding < len ( line + param ) + 3 : \n                param_lines . append ( line ) \n                line = \"\" \n            line += param + \", \" \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9350": "\ndef __tile_background ( self , dc ) : \n    sz = self . wx_obj . GetClientSize ( ) \n    bmp = self . _bitmap . get_bits ( ) \n    w = bmp . GetWidth ( ) \n    h = bmp . GetHeight ( ) \n    if isinstance ( self , wx . ScrolledWindow ) : \n        spx , spy = self . wx_obj . GetScrollPixelsPerUnit ( ) \n        vsx , vsy = self . wx_obj . GetViewStart ( ) \n        dx , dy = ( spx * vsx ) % w , ( spy * vsy ) % h \n    else : \n        dx , dy = ( w , h ) \n    x = - dx \n    while sz . width > x : \n        y = - dy \n        while sz . height > y : \n            dc . DrawBitmap ( bmp , x , y ) \n            y = y + h \n        x = x + w "}
{"9355": "\ndef ResetView ( self , grid ) : \n    grid . BeginBatch ( ) \n    for current , new , delmsg , addmsg in [ ( self . _rows , self . GetNumberRows ( ) , gridlib . GRIDTABLE_NOTIFY_ROWS_DELETED , gridlib . GRIDTABLE_NOTIFY_ROWS_APPENDED ) , ( self . _cols , self . GetNumberCols ( ) , gridlib . GRIDTABLE_NOTIFY_COLS_DELETED , gridlib . GRIDTABLE_NOTIFY_COLS_APPENDED ) , ] : \n        if current > new : \n            msg = gridlib . GridTableMessage ( self , delmsg , new , current - new ) \n            grid . ProcessTableMessage ( msg ) \n        elif current < new : \n            msg = gridlib . GridTableMessage ( self , addmsg , new - current ) \n            grid . ProcessTableMessage ( msg ) \n            self . UpdateValues ( grid ) \n    grid . EndBatch ( ) \n    self . _rows = self . GetNumberRows ( ) \n    self . _cols = self . GetNumberCols ( ) \n    self . _updateColAttrs ( grid ) \n    grid . AdjustScrollbars ( ) \n    grid . ForceRefresh ( ) "}
{"9364": "\ndef StartingKey ( self , evt ) : \n    key = evt . GetKeyCode ( ) \n    ch = None \n    if key in [ wx . WXK_NUMPAD0 , wx . WXK_NUMPAD1 , wx . WXK_NUMPAD2 , wx . WXK_NUMPAD3 , wx . WXK_NUMPAD4 , wx . WXK_NUMPAD5 , wx . WXK_NUMPAD6 , wx . WXK_NUMPAD7 , wx . WXK_NUMPAD8 , wx . WXK_NUMPAD9 ] : \n        ch = ch = chr ( ord ( '0' ) + key - wx . WXK_NUMPAD0 ) \n    elif 256 > key and 0 <= key and chr ( key ) in string . printable : \n        ch = chr ( key ) \n        if not evt . ShiftDown ( ) : \n            ch = ch . lower ( ) \n    if ch is not None : \n        self . _tc . SetStringSelection ( ch ) \n    else : \n        evt . Skip ( ) "}
{"9375": "\ndef mangle_signature ( sig , max_chars = 30 ) : \n    s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) \n    s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) \n    s = re . sub ( r\"\\\\'\" , \"\" , s ) \n    s = re . sub ( r\"'[^']*'\" , \"\" , s ) \n    args = [ ] \n    opts = [ ] \n    opt_re = re . compile ( r\"^(.*, |)([a-zA-Z0-9_*]+)=\" ) \n    while s : \n        m = opt_re . search ( s ) \n        if not m : \n            args = s . split ( ', ' ) \n            break \n        opts . insert ( 0 , m . group ( 2 ) ) \n        s = m . group ( 1 ) [ : - 2 ] \n    sig = limited_join ( \", \" , args , max_chars = max_chars - 2 ) \n    if opts : \n        if not sig : \n            sig = \"[%s]\" % limited_join ( \", \" , opts , max_chars = max_chars - 4 ) \n        elif max_chars - 4 - 2 - 3 > len ( sig ) : \n            sig += \"[, %s]\" % limited_join ( \", \" , opts , max_chars = max_chars - len ( sig ) - 4 - 2 ) \n    return u\"(%s)\" % sig "}
{"9426": "\ndef assert_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_text ( ) : \n        count = query . resolve_for ( self ) \n        if not ( matches_count ( count , query . options ) and ( 0 < count or expects_none ( query . options ) ) ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return True \n    return assert_text ( ) "}
{"9427": "\ndef assert_no_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_text ( ) : \n        count = query . resolve_for ( self ) \n        if matches_count ( count , query . options ) and ( 0 < count or expects_none ( query . options ) ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return True \n    return assert_no_text ( ) "}
{"9433": "\ndef find_first ( self , * args , ** kwargs ) : \n    if capybara . wait_on_first_by_default : \n        kwargs . setdefault ( \"minimum\" , 1 ) \n    try : \n        result = self . find_all ( * args , ** kwargs ) \n        return result [ 0 ] if 0 < len ( result ) else None \n    except ExpectationNotMet : \n        return None "}
{"9441": "\ndef _cache_at_least ( self , size ) : \n    try : \n        while size > len ( self . _result_cache ) : \n            self . _result_cache . append ( next ( self . _result_iter ) ) \n        return True \n    except StopIteration : \n        return False "}
{"9444": "\ndef matches_count ( count , options ) : \n    if options . get ( \"count\" ) is not None : \n        return count == int ( options [ \"count\" ] ) \n    if options . get ( \"maximum\" ) is not None and count > int ( options [ \"maximum\" ] ) : \n        return False \n    if options . get ( \"minimum\" ) is not None and count < int ( options [ \"minimum\" ] ) : \n        return False \n    if options . get ( \"between\" ) is not None and count not in options [ \"between\" ] : \n        return False \n    return True "}
{"9470": "\ndef v2_runner_on_ok ( self , result , ** kwargs ) : \n    failed = \"failed\" in result . _result \n    unreachable = \"unreachable\" in result . _result \n    if ( \"print_action\" in result . _task . tags or failed or unreachable or 1 < self . _display . verbosity ) : \n        self . _print_task ( ) \n        self . last_skipped = False \n        msg = unicode ( result . _result . get ( \"msg\" , \"\" ) ) or unicode ( result . _result . get ( \"reason\" , \"\" ) ) or unicode ( result . _result . get ( \"message\" , \"\" ) ) \n        stderr = [ result . _result . get ( \"exception\" , None ) , result . _result . get ( \"module_stderr\" , None ) , ] \n        stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n        self . _print_host_or_item ( result . _host , result . _result . get ( \"changed\" , False ) , msg , result . _result . get ( \"diff\" , None ) , is_host = True , error = failed or unreachable , stdout = result . _result . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n        if \"results\" in result . _result : \n            for r in result . _result [ \"results\" ] : \n                failed = \"failed\" in r \n                stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module_stderr\" , None ) ] \n                stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n                self . _print_host_or_item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is_host = False , error = failed , stdout = r . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n    else : \n        self . last_skipped = True \n        print ( \".\" , end = \"\" ) "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if 1 < self . _display . verbosity : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if 50 > len ( reason ) : \n            line += \" -- {}\" . format ( reason ) \n            print ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            print ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            print ( self . _indent_text ( reason , 8 ) ) \n            print ( reason ) "}
{"9501": "\ndef get_authorization ( self ) : \n    auth = self . authorization_class ( ) \n    header = self . get_authorization_header ( ) \n    if not header or not header . split : \n        return auth \n    header = header . split ( ) \n    if 1 < len ( header ) and header [ 0 ] == 'Bearer' : \n        auth . is_oauth = True \n        access_token = header [ 1 ] \n        self . validate_access_token ( access_token , auth ) \n        if not auth . is_valid : \n            auth . error = 'access_denied' \n    return auth "}
{"9534": "\ndef get_single_list_nodes_data ( li , meta_data ) : \n    yield li \n    w_namespace = get_namespace ( li , 'w' ) \n    current_numId = get_numId ( li , w_namespace ) \n    starting_ilvl = get_ilvl ( li , w_namespace ) \n    el = li \n    while True : \n        el = el . getnext ( ) \n        if el is None : \n            break \n        if not has_text ( el ) : \n            continue \n        if _is_top_level_upper_roman ( el , meta_data ) : \n            break \n        if ( is_li ( el , meta_data ) and ( get_ilvl ( el , w_namespace ) < starting_ilvl ) ) : \n            break \n        new_numId = get_numId ( el , w_namespace ) \n        if new_numId is None or new_numId == - 1 : \n            yield el \n            continue \n        if current_numId != new_numId : \n            break \n        if is_last_li ( el , meta_data , current_numId ) : \n            yield el \n            break \n        yield el "}
{"9548": "\ndef build_list ( li_nodes , meta_data ) : \n    ol_dict = { } \n    current_ilvl = - 1 \n    current_numId = - 1 \n    current_ol = None \n    root_ol = None \n    visited_nodes = [ ] \n    list_contents = [ ] \n    def _build_li ( list_contents ) : \n        data = '<br />' . join ( t for t in list_contents if t is not None ) \n        return etree . XML ( '<li>%s</li>' % data ) \n    def _build_non_li_content ( el , meta_data ) : \n        w_namespace = get_namespace ( el , 'w' ) \n        if el . tag == '%stbl' % w_namespace : \n            new_el , visited_nodes = build_table ( el , meta_data ) \n            return etree . tostring ( new_el ) , visited_nodes \n        elif el . tag == '%sp' % w_namespace : \n            return get_element_content ( el , meta_data ) , [ el ] \n        if has_text ( el ) : \n            raise UnintendedTag ( 'Did not expect %s' % el . tag ) \n    def _merge_lists ( ilvl , current_ilvl , ol_dict , current_ol ) : \n        for i in reversed ( range ( ilvl , current_ilvl ) ) : \n            if i not in ol_dict : \n                continue \n            if ol_dict [ i ] is not current_ol : \n                if ol_dict [ i ] is current_ol : \n                    continue \n                ol_dict [ i ] [ - 1 ] . append ( current_ol ) \n                current_ol = ol_dict [ i ] \n        for key in list ( ol_dict ) : \n            if ilvl < key : \n                del ol_dict [ key ] \n        return current_ol \n    for li_node in li_nodes : \n        w_namespace = get_namespace ( li_node , 'w' ) \n        if not is_li ( li_node , meta_data ) : \n            new_el , el_visited_nodes = _build_non_li_content ( li_node , meta_data , ) \n            list_contents . append ( new_el ) \n            visited_nodes . extend ( el_visited_nodes ) \n            continue \n        if list_contents : \n            li_el = _build_li ( list_contents ) \n            list_contents = [ ] \n            current_ol . append ( li_el ) \n        list_contents . append ( get_element_content ( li_node , meta_data , ) ) \n        ilvl = get_ilvl ( li_node , w_namespace ) \n        numId = get_numId ( li_node , w_namespace ) \n        list_type = get_ordered_list_type ( meta_data , numId , ilvl ) \n        if ( current_ilvl < ilvl ) or ( numId != current_numId ) : \n            ol_dict [ ilvl ] = create_list ( list_type ) \n            current_ol = ol_dict [ ilvl ] \n            current_ilvl = ilvl \n            current_numId = numId \n        else : \n            current_ol = _merge_lists ( ilvl = ilvl , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n        if root_ol is None : \n            root_ol = current_ol \n        if ilvl in ol_dict : \n            current_ol = ol_dict [ ilvl ] \n        else : \n            if current_ol is not root_ol : \n                root_ol [ - 1 ] . append ( current_ol ) \n                current_ol = create_list ( list_type ) \n        visited_nodes . extend ( list ( li_node . iter ( ) ) ) \n    if list_contents : \n        li_el = _build_li ( list_contents ) \n        list_contents = [ ] \n        current_ol . append ( li_el ) \n    current_ol = _merge_lists ( ilvl = 0 , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n    return root_ol , visited_nodes "}
{"9549": "\ndef build_tr ( tr , meta_data , row_spans ) : \n    tr_el = etree . Element ( 'tr' ) \n    w_namespace = get_namespace ( tr , 'w' ) \n    visited_nodes = [ ] \n    for el in tr : \n        if el in visited_nodes : \n            continue \n        visited_nodes . append ( el ) \n        if el . tag == '%stc' % w_namespace : \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) != 'restart' ) : \n                continue \n            texts = [ ] \n            for td_content in el : \n                if td_content in visited_nodes : \n                    continue \n                if is_li ( td_content , meta_data ) : \n                    li_nodes = get_single_list_nodes_data ( td_content , meta_data , ) \n                    list_el , list_visited_nodes = build_list ( li_nodes , meta_data , ) \n                    visited_nodes . extend ( list_visited_nodes ) \n                    texts . append ( etree . tostring ( list_el ) ) \n                elif td_content . tag == '%stbl' % w_namespace : \n                    table_el , table_visited_nodes = build_table ( td_content , meta_data , ) \n                    visited_nodes . extend ( table_visited_nodes ) \n                    texts . append ( etree . tostring ( table_el ) ) \n                elif td_content . tag == '%stcPr' % w_namespace : \n                    visited_nodes . append ( td_content ) \n                    continue \n                else : \n                    text = get_element_content ( td_content , meta_data , is_td = True , ) \n                    texts . append ( text ) \n            data = '<br />' . join ( t for t in texts if t is not None ) \n            td_el = etree . XML ( '<td>%s</td>' % data ) \n            colspan = get_grid_span ( el ) \n            if 1 < colspan : \n                td_el . set ( 'colspan' , '%d' % colspan ) \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) == 'restart' ) : \n                rowspan = next ( row_spans ) \n                td_el . set ( 'rowspan' , '%d' % rowspan ) \n            tr_el . append ( td_el ) \n    return tr_el "}
{"9553": "\ndef find ( dataset , url ) : \n    fn = os . path . join ( DATASETS , dataset ) \n    dn = os . path . dirname ( fn ) \n    if not os . path . exists ( dn ) : \n        print ( 'creating dataset directory: %s' , dn ) \n        os . makedirs ( dn ) \n    if not os . path . exists ( fn ) : \n        if ( 3 , ) > sys . version_info : \n            urllib . urlretrieve ( url , fn ) \n        else : \n            urllib . request . urlretrieve ( url , fn ) \n    return fn "}
{"9554": "\ndef load_mnist ( flatten = True , labels = False ) : \n    fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) \n    h = gzip . open ( fn , 'rb' ) \n    if ( 3 , ) > sys . version_info : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) \n    else : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) \n    h . close ( ) \n    if not flatten : \n        timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n    if labels : \n        return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9555": "\ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        print ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if ( 3 , ) > sys . version_info : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( 1 , 6 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9559": "\ndef batches ( arrays , steps = 100 , batch_size = 64 , rng = None ) : \n    assert 2 <= batch_size , 'batch_size must be at least 2!' \n    assert isinstance ( arrays , ( tuple , list ) ) , 'arrays must be a tuple or list!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    def sample ( ) : \n        xs = [ np . zeros ( ( batch_size , steps , a . shape [ 1 ] ) , a . dtype ) for a in arrays ] \n        for i in range ( batch_size ) : \n            j = rng . randint ( len ( arrays [ 0 ] ) - steps ) \n            for x , a in zip ( xs , arrays ) : \n                x [ i ] = a [ j : j + steps ] \n        return xs \n    return sample "}
{"9561": "\ndef classifier_batches ( self , steps , batch_size , rng = None ) : \n    assert 2 <= batch_size , 'batch_size must be at least 2!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    T = np . arange ( steps ) \n    def batch ( ) : \n        inputs = np . zeros ( ( batch_size , steps , 1 + len ( self . alpha ) ) , 'f' ) \n        outputs = np . zeros ( ( batch_size , steps ) , 'i' ) \n        for b in range ( batch_size ) : \n            offset = rng . randint ( len ( self . text ) - steps - 1 ) \n            enc = self . encode ( self . text [ offset : offset + steps + 1 ] ) \n            inputs [ b , T , enc [ : - 1 ] ] = 1 \n            outputs [ b , T ] = enc [ 1 : ] \n        return [ inputs , outputs ] \n    return batch "}
{"9575": "\ndef random_matrix ( rows , cols , mean = 0 , std = 1 , sparsity = 0 , radius = 0 , diagonal = 0 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    arr = mean + std * rng . randn ( rows , cols ) \n    if 1 > sparsity > 0 : \n        k = min ( rows , cols ) \n        mask = rng . binomial ( n = 1 , p = 1 - sparsity , size = ( rows , cols ) ) . astype ( bool ) \n        mask [ : k , : k ] |= np . eye ( k ) . astype ( bool ) \n        arr *= mask \n    if 0 < radius : \n        u , s , vT = np . linalg . svd ( arr , full_matrices = False ) \n        arr = np . dot ( np . dot ( u , np . diag ( radius * s / abs ( s [ 0 ] ) ) ) , vT ) \n    if diagonal != 0 : \n        arr = diagonal * np . eye ( max ( rows , cols ) ) [ : rows , : cols ] \n    return arr . astype ( FLOAT ) "}
{"9582": "\ndef _scan ( self , inputs , outputs , name = 'scan' , step = None , constants = None ) : \n    init = [ ] \n    for i , x in enumerate ( outputs ) : \n        ndim = getattr ( x , 'ndim' , - 1 ) \n        if x is None or isinstance ( x , dict ) or 0 < ndim : \n            init . append ( x ) \n            continue \n        if isinstance ( x , int ) or ndim == 0 : \n            init . append ( TT . repeat ( theano . shared ( np . zeros ( ( 1 , self . output_size ) , util . FLOAT ) , name = self . _fmt ( 'init{}' . format ( i ) ) ) , x , axis = 0 ) ) \n            continue \n        raise ValueError ( 'cannot handle input {} for scan!' . format ( x ) ) \n    return theano . scan ( step or self . _step , name = self . _fmt ( name ) , sequences = inputs , outputs_info = init , non_sequences = constants , go_backwards = 'back' in self . kwargs . get ( 'direction' , '' ) . lower ( ) , truncate_gradient = self . kwargs . get ( 'bptt_limit' , - 1 ) , ) "}
{"9584": "\ndef reservoir ( xs , n , rng ) : \n    pool = [ ] \n    for i , x in enumerate ( xs ) : \n        if n > len ( pool ) : \n            pool . append ( x / np . linalg . norm ( x ) ) \n            continue \n        j = rng . randint ( i + 1 ) \n        if n > j : \n            pool [ j ] = x / np . linalg . norm ( x ) \n    L = len ( pool ) \n    S = np . std ( pool , axis = 0 ) \n    while n > len ( pool ) : \n        x = pool [ rng . randint ( L ) ] \n        pool . append ( x + S * rng . randn ( * x . shape ) ) \n    return np . array ( pool , dtype = pool [ 0 ] . dtype ) "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = 0 , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , 0 ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ 0 ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if algo == 'sample' : \n            algo = trainer . SampleTrainer ( self ) \n        elif algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n            algo = trainer . SupervisedPretrainer ( subalgo , self ) \n        elif algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n            algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n        else : \n            algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return False \n        if isinstance ( save_every , float ) : \n            return 60 * save_every < elapsed \n        if isinstance ( save_every , int ) : \n            return iteration % save_every == 0 \n        return False \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9614": "\ndef add_tier ( self , name , tier_type = 'IntervalTier' , number = None ) : \n    if number is None : \n        number = 1 if not self . tiers else len ( self . tiers ) + 1 \n    elif 1 > number or len ( self . tiers ) < number : \n        raise ValueError ( 'Number not in [1..{}]' . format ( len ( self . tiers ) ) ) \n    elif tier_type not in Tier . P_TIERS : \n        raise ValueError ( 'tier_type has to be in {}' . format ( self . P_TIERS ) ) \n    self . tiers . insert ( number - 1 , Tier ( self . xmin , self . xmax , name , tier_type ) ) \n    return self . tiers [ number - 1 ] "}
{"9617": "\ndef to_eaf ( self , skipempty = True , pointlength = 0.1 ) : \n    from pympi . Elan import Eaf \n    eaf_out = Eaf ( ) \n    if 0 >= pointlength : \n        raise ValueError ( 'Pointlength should be strictly positive' ) \n    for tier in self . get_tiers ( ) : \n        eaf_out . add_tier ( tier . name ) \n        for ann in tier . get_intervals ( True ) : \n            if tier . tier_type == 'TextTier' : \n                ann = ( ann [ 0 ] , ann [ 0 ] + pointlength , ann [ 1 ] ) \n            if ann [ 2 ] . strip ( ) or not skipempty : \n                eaf_out . add_annotation ( tier . name , int ( round ( ann [ 0 ] * 1000 ) ) , int ( round ( ann [ 1 ] * 1000 ) ) , ann [ 2 ] ) \n    return eaf_out "}
{"9619": "\ndef add_interval ( self , begin , end , value , check = True ) : \n    if self . tier_type != 'IntervalTier' : \n        raise Exception ( 'Tiertype must be IntervalTier' ) \n    if check : \n        if any ( i for i in self . intervals if i [ 1 ] > begin and i [ 0 ] < end ) : \n            raise Exception ( 'No overlap is allowed' ) \n        if end < begin : \n            raise Exception ( 'Begin must be smaller then end' ) \n    self . intervals . append ( ( begin , end , value ) ) "}
{"9620": "\ndef remove_interval ( self , time ) : \n    if self . tier_type != 'IntervalTier' : \n        raise Exception ( 'Tiertype must be IntervalTier.' ) \n    self . intervals = [ i for i in self . intervals if not ( time >= i [ 0 ] and time <= i [ 1 ] ) ] "}
{"9623": "\ndef get_all_intervals ( self ) : \n    ints = sorted ( self . get_intervals ( True ) ) \n    if self . tier_type == 'IntervalTier' : \n        if not ints : \n            ints . append ( ( self . xmin , self . xmax , '' ) ) \n        else : \n            if self . xmin < ints [ 0 ] [ 0 ] : \n                ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) \n            if self . xmax > ints [ - 1 ] [ 1 ] : \n                ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) \n            p = ints [ - 1 ] \n            for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : \n                if p [ 0 ] - i [ 1 ] != 0 : \n                    ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) \n                p = i \n    return ints "}
{"9625": "\ndef add_annotation ( self , id_tier , start , end , value = '' , svg_ref = None ) : \n    if self . tiers [ id_tier ] [ 1 ] : \n        raise ValueError ( 'Tier already contains ref annotations...' ) \n    if start == end : \n        raise ValueError ( 'Annotation length is zero...' ) \n    if end < start : \n        raise ValueError ( 'Annotation length is negative...' ) \n    if 0 > start : \n        raise ValueError ( 'Start is negative...' ) \n    start_ts = self . generate_ts_id ( start ) \n    end_ts = self . generate_ts_id ( end ) \n    aid = self . generate_annotation_id ( ) \n    self . annotations [ aid ] = id_tier \n    self . tiers [ id_tier ] [ 0 ] [ aid ] = ( start_ts , end_ts , value , svg_ref ) "}
{"9637": "\ndef extract ( self , start , end ) : \n    from copy import deepcopy \n    eaf_out = deepcopy ( self ) \n    for t in eaf_out . get_tier_names ( ) : \n        for ab , ae , value in eaf_out . get_annotation_data_for_tier ( t ) : \n            if end < ab or start > ae : \n                eaf_out . remove_annotation ( t , ( start - end ) // 2 , False ) \n    eaf_out . clean_time_slots ( ) \n    return eaf_out "}
{"9639": "\ndef generate_ts_id ( self , time = None ) : \n    if time and 0 > time : \n        raise ValueError ( 'Time is negative...' ) \n    if not self . maxts : \n        valid_ts = [ int ( '' . join ( filter ( str . isdigit , a ) ) ) for a in self . timeslots ] \n        self . maxts = max ( valid_ts + [ 1 ] ) + 1 \n    else : \n        self . maxts += 1 \n    ts = 'ts{:d}' . format ( self . maxts ) \n    self . timeslots [ ts ] = time \n    return ts "}
{"9645": "\ndef merge_tiers ( self , tiers , tiernew = None , gapt = 0 , sep = '_' , safe = False ) : \n    if tiernew is None : \n        tiernew = u'{}_merged' . format ( '_' . join ( tiers ) ) \n    self . add_tier ( tiernew ) \n    aa = [ ( sys . maxsize , sys . maxsize , None ) ] + sorted ( ( a for t in tiers for a in self . get_annotation_data_for_tier ( t ) ) , reverse = True ) \n    l = None \n    while aa : \n        begin , end , value = aa . pop ( ) \n        if l is None : \n            l = [ begin , end , [ value ] ] \n        elif gapt <= begin - l [ 1 ] : \n            if not safe or l [ 0 ] < l [ 1 ] : \n                self . add_annotation ( tiernew , l [ 0 ] , l [ 1 ] , sep . join ( l [ 2 ] ) ) \n            l = [ begin , end , [ value ] ] \n        else : \n            if l [ 1 ] < end : \n                l [ 1 ] = end \n            l [ 2 ] . append ( value ) \n    return tiernew "}
{"9651": "\ndef remove_ref_annotation ( self , id_tier , time ) : \n    removed = 0 \n    bucket = [ ] \n    for aid , ( ref , value , _ , _ ) in self . tiers [ id_tier ] [ 1 ] . items ( ) : \n        begin , end , rvalue , _ = self . tiers [ self . annotations [ ref ] ] [ 0 ] [ ref ] \n        begin = self . timeslots [ begin ] \n        end = self . timeslots [ end ] \n        if time >= begin and time <= end : \n            removed += 1 \n            bucket . append ( aid ) \n    for aid in bucket : \n        del ( self . tiers [ id_tier ] [ 1 ] [ aid ] ) \n    return removed "}
{"9656": "\ndef shift_annotations ( self , time ) : \n    total_re = [ ] \n    total_sq = [ ] \n    for name , tier in self . tiers . items ( ) : \n        squashed = [ ] \n        for aid , ( begin , end , value , _ ) in tier [ 0 ] . items ( ) : \n            if 0 >= self . timeslots [ end ] + time : \n                squashed . append ( ( name , aid ) ) \n            elif 0 > self . timeslots [ begin ] + time : \n                total_sq . append ( ( name , self . timeslots [ begin ] , self . timeslots [ end ] , value ) ) \n                self . timeslots [ begin ] = 0 \n            else : \n                self . timeslots [ begin ] += time \n                self . timeslots [ end ] += time \n        for name , aid in squashed : \n            start , end , value , _ = self . tiers [ name ] [ 0 ] [ aid ] \n            del ( self . tiers [ name ] [ 0 ] [ aid ] ) \n            del ( self . annotations [ aid ] ) \n            total_re . append ( ( name , self . timeslots [ start ] , self . timeslots [ end ] , value ) ) \n    return total_sq , total_re "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != 0 : \n                overflow_counter += 1 \n                continue \n            new_level = level_func ( embedding_level ) \n            if EXPLICIT_LEVEL_LIMIT > new_level : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif embedding_level == EXPLICIT_LEVEL_LIMIT - 2 : \n                almost_overflow_counter += 1 \n            else : \n                overflow_counter += 1 \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if directional_override != 'N' : \n                    _ch [ 'type' ] = directional_override \n            elif bidi_type == 'PDF' : \n                if overflow_counter : \n                    overflow_counter -= 1 \n                elif almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - 1 : \n                    almost_overflow_counter -= 1 \n                elif levels : \n                    embedding_level , directional_override = levels . pop ( ) \n            elif bidi_type == 'B' : \n                levels . clear ( ) \n                overflow_counter = almost_overflow_counter = 0 \n                embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9665": "\ndef reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) : \n    for level in range ( highest_level , lowest_odd_level - 1 , - 1 ) : \n        _start = _end = None \n        for run_idx in range ( line_start , line_end + 1 ) : \n            run_ch = chars [ run_idx ] \n            if level <= run_ch [ 'level' ] : \n                if _start is None : \n                    _start = _end = run_idx \n                else : \n                    _end = run_idx \n            else : \n                if _end : \n                    chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) \n                    _start = _end = None \n        if _start is not None : \n            chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if highest_level < char_level : \n            highest_level = char_level \n        if char_level % 2 and lowest_odd_level > char_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end -= 1 \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9685": "\ndef float_range ( start = 0 , stop = None , step = 1 ) : \n    start = float ( start ) \n    while stop > start : \n        yield start \n        start += step "}
{"9693": "\ndef add_popup ( self , x , y , label ) : \n    txt_width = len ( label ) * self . font_size * 0.6 + 10 \n    tx = x + [ 5 , - 5 ] [ int ( self . width < x + txt_width ) ] \n    anchor = [ 'start' , 'end' ] [ self . width < x + txt_width ] \n    style = 'fill: #000; text-anchor: %s;' % anchor \n    id = 'label-%s' % self . _w3c_name ( label ) \n    attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font_size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } \n    etree . SubElement ( self . foreground , 'text' , attrs ) \n    vis_tmpl = ( \"document.getElementById('{id}').setAttribute('visibility', {val})\" ) \n    attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis_tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis_tmpl . format ( val = 'hidden' , id = id ) , } \n    etree . SubElement ( self . foreground , 'circle' , attrs ) "}
{"9740": "\ndef delete ( self , request , * args , ** kwargs ) : \n    auth = get_authorization_header ( request ) . split ( ) \n    if not auth or auth [ 0 ] . lower ( ) != b'token' : \n        return response . Response ( status = status . HTTP_400_BAD_REQUEST ) \n    if len ( auth ) == 1 : \n        msg = 'Invalid token header. No credentials provided.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    elif 2 < len ( auth ) : \n        msg = 'Invalid token header. Token string should not contain spaces.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    try : \n        token = self . model . objects . get ( key = auth [ 1 ] ) \n    except self . model . DoesNotExist : \n        pass \n    else : \n        token . delete ( ) \n        signals . user_logged_out . send ( type ( self ) , user = token . user , request = request , ) \n    return response . Response ( status = status . HTTP_204_NO_CONTENT ) "}
{"9749": "\ndef authenticate_credentials ( self , key ) : \n    user , token = super ( TokenAuthentication , self ) . authenticate_credentials ( key ) \n    if timezone . now ( ) > token . expires : \n        msg = _ ( 'Token has expired.' ) \n        raise exceptions . AuthenticationFailed ( msg ) \n    token . update_expiry ( ) \n    return ( user , token ) "}
{"9757": "\ndef get_method_owner ( meth ) : \n    if inspect . ismethod ( meth ) : \n        if ( 3 , 0 ) > sys . version_info : \n            return meth . im_class if meth . im_self is None else meth . im_self \n        else : \n            return meth . __self__ "}
{"9762": "\ndef bytes_to_readable ( num ) : \n    if 512 > num : \n        return \"0 Kb\" \n    elif 1024 > num : \n        return \"1 Kb\" \n    for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : \n        if 1024.0 > abs ( num ) : \n            return \"%3.1f%s\" % ( num , unit ) \n        num /= 1024.0 \n    return \"%.1f%s\" % ( num , 'Yb' ) "}
{"9769": "\ndef volume_percentage_used ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        total = int ( volume [ \"size\" ] [ \"total\" ] ) \n        used = int ( volume [ \"size\" ] [ \"used\" ] ) \n        if used is not None and 0 < used and total is not None and 0 < total : \n            return round ( ( float ( used ) / float ( total ) ) * 100.0 , 1 ) "}
{"9770": "\ndef volume_disk_temp_avg ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            total_temp = 0 \n            total_disks = 0 \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None : \n                    total_disks += 1 \n                    total_temp += disk_temp \n            if 0 < total_temp and 0 < total_disks : \n                return round ( total_temp / total_disks , 0 ) "}
{"9771": "\ndef volume_disk_temp_max ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            max_temp = 0 \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None and max_temp < disk_temp : \n                    max_temp = disk_temp \n            return max_temp "}
{"9793": "\ndef set_access_credentials ( self , _retry = 0 ) : \n    if 5 <= _retry : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    try : \n        self . r . set_access_credentials ( self . _get_value ( CONFIGKEY_SCOPE , set , split_val = \",\" ) , self . _get_value ( CONFIGKEY_TOKEN ) , self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n    except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n        self . _log ( \"Request new Token (SAC)\" ) \n        self . _get_new_access_information ( ) "}
{"9794": "\ndef refresh ( self , force = False , _retry = 0 ) : \n    if 5 <= _retry : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    if self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN < time . time ( ) : \n        self . config . read ( self . configfile ) \n        if self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN > time . time ( ) : \n            self . _log ( \"Found new token\" ) \n            self . set_access_credentials ( ) \n    if force or self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN < time . time ( ) : \n        self . _log ( \"Refresh Token\" ) \n        try : \n            new_token = self . r . refresh_access_information ( self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n            self . _change_value ( CONFIGKEY_TOKEN , new_token [ \"access_token\" ] ) \n            self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) \n            self . set_access_credentials ( ) \n        except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n            self . _log ( \"Request new Token (REF)\" ) \n            self . _get_new_access_information ( ) "}
{"9799": "\ndef clean_dict ( dict ) : \n    if 3 > sys . version_info [ 0 ] : \n        return { k : v for k , v in dict . iteritems ( ) if v is not None } \n    else : \n        return { k : v for k , v in dict . items ( ) if v is not None } "}
{"9809": "\ndef print_context ( self , context ) : \n    text = [ CONTEXT_TITLE ] \n    for i , context_scope in enumerate ( context ) : \n        dump1 = linebreaksbr ( pformat_django_context_html ( context_scope ) ) \n        dump2 = pformat_dict_summary_html ( context_scope ) \n        if 3 >= len ( context_scope ) and 20 < dump1 . count ( '<br />' ) : \n            ( dump1 , dump2 ) = ( dump2 , dump1 ) \n        text . append ( CONTEXT_BLOCK . format ( style = PRE_STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) \n    return u'' . join ( text ) "}
{"9812": "\ndef pformat_django_context_html ( object ) : \n    if isinstance ( object , QuerySet ) : \n        text = '' \n        lineno = 0 \n        for item in object . all ( ) [ : 21 ] : \n            lineno += 1 \n            if 21 <= lineno : \n                text += u'   (remaining items truncated...)' \n                break \n            text += u'   {0}\\n' . format ( escape ( repr ( item ) ) ) \n        return text \n    elif isinstance ( object , Manager ) : \n        return mark_safe ( u'    (use <kbd>.all</kbd> to read it)' ) \n    elif isinstance ( object , six . string_types ) : \n        return escape ( repr ( object ) ) \n    elif isinstance ( object , Promise ) : \n        return escape ( _format_lazy ( object ) ) \n    elif isinstance ( object , dict ) : \n        return _format_dict ( object ) \n    elif isinstance ( object , list ) : \n        return _format_list ( object ) \n    elif hasattr ( object , '__dict__' ) : \n        return _format_object ( object ) \n    else : \n        text = DebugPrettyPrinter ( width = 200 ) . pformat ( object ) \n        return _style_text ( text ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and 127 > ord ( ch ) ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and lch [ 0 : 1 ] == '\\\\' else lch ) \n            elif ( ( 32 <= ord ( ch ) and 127 >= ord ( ch ) ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result += ch \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result += r'{\\bfseries ?}' \n                else : \n                    result += ch \n    return result "}
{"9824": "\ndef _unascii ( s ) : \n    m = _U_ESCAPE . search ( s ) \n    if not m : \n        return s if PY2 else s . encode ( 'utf-8' ) \n    chunks = [ ] \n    pos = 0 \n    while m : \n        start = m . start ( ) \n        end = m . end ( ) \n        g = m . group ( 1 ) \n        if g is None : \n            chunks . append ( s [ pos : end ] ) \n        else : \n            c = int ( g , 16 ) \n            if 0x20 > c : \n                chunks . append ( s [ pos : end ] ) \n            else : \n                if PY3 : \n                    if c & 0xfc00 == 0xd800 and s [ end : end + 2 ] == '\\\\u' : \n                        esc2 = s [ end + 2 : end + 6 ] \n                        c2 = int ( esc2 , 16 ) \n                        if c2 & 0xfc00 == 0xdc00 : \n                            c = 0x10000 + ( ( ( c - 0xd800 ) << 10 ) | ( c2 - 0xdc00 ) ) \n                            end += 6 \n                chunks . append ( s [ pos : start ] ) \n                chunks . append ( unichr ( c ) ) \n        pos = end \n        m = _U_ESCAPE . search ( s , pos ) \n    chunks . append ( s [ pos : ] ) \n    return ( '' . join ( chunks ) ) . encode ( \"utf-8\" ) "}
{"9894": "\ndef mark_regex ( regex , text , split_locations ) : \n    for match in regex . finditer ( text ) : \n        end_match = match . end ( ) \n        if len ( split_locations ) > end_match : \n            split_locations [ end_match ] = SHOULD_SPLIT "}
{"9895": "\ndef mark_begin_end_regex ( regex , text , split_locations ) : \n    for match in regex . finditer ( text ) : \n        end_match = match . end ( ) \n        begin_match = match . start ( ) \n        for i in range ( begin_match + 1 , end_match ) : \n            split_locations [ i ] = SHOULD_NOT_SPLIT \n        if len ( split_locations ) > end_match : \n            if split_locations [ end_match ] == UNDECIDED : \n                split_locations [ end_match ] = SHOULD_SPLIT \n        if split_locations [ begin_match ] == UNDECIDED : \n            split_locations [ begin_match ] = SHOULD_SPLIT "}
{"9905": "\ndef encodeString ( string ) : \n    encoded = bytearray ( 2 ) \n    encoded . extend ( bytearray ( string , encoding = 'utf-8' ) ) \n    l = len ( encoded ) - 2 \n    if ( 65535 < l ) : \n        raise StringValueError ( l ) \n    encoded [ 0 ] = l >> 8 \n    encoded [ 1 ] = l & 0xFF \n    return encoded "}
{"9908": "\ndef encodeLength ( value ) : \n    encoded = bytearray ( ) \n    while True : \n        digit = value % 128 \n        value //= 128 \n        if 0 < value : \n            digit |= 128 \n        encoded . append ( digit ) \n        if 0 >= value : \n            break \n    return encoded "}
{"9920": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    if self . qos : \n        header [ 0 ] = 0x30 | self . retain | ( self . qos << 1 ) | ( self . dup << 3 ) \n        varHeader . extend ( encodeString ( self . topic ) ) \n        varHeader . extend ( encode16Int ( self . msgId ) ) \n    else : \n        header [ 0 ] = 0x30 | self . retain \n        varHeader . extend ( encodeString ( self . topic ) ) \n    if isinstance ( self . payload , bytearray ) : \n        payload . extend ( self . payload ) \n    elif isinstance ( self . payload , str ) : \n        payload . extend ( bytearray ( self . payload , encoding = 'utf-8' ) ) \n    else : \n        raise PayloadTypeError ( type ( self . payload ) ) \n    totalLen = len ( varHeader ) + len ( payload ) \n    if 268435455 < totalLen : \n        raise PayloadValueError ( totalLen ) \n    header . extend ( encodeLength ( totalLen ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9936": "\ndef get_total_time_span ( d ) : \n    tmax = 0 \n    for di in d . values ( ) : \n        if tmax < di . uTime . max ( ) : \n            tmax = di . uTime . max ( ) \n    return tmax "}
{"9937": "\ndef unitpicker ( a , llim = 0.1 , denominator = None , focus_stage = None ) : \n    if not isinstance ( a , ( int , float ) ) : \n        a = nominal_values ( a ) \n        a = np . percentile ( a [ ~ np . isnan ( a ) ] , 25 ) \n    if denominator is not None : \n        pd = pretty_element ( denominator ) \n    else : \n        pd = '' \n    if focus_stage == 'calibrated' : \n        udict = { 0 : 'mol/mol ' + pd , 1 : 'mmol/mol ' + pd , 2 : '$\\mu$mol/mol ' + pd , 3 : 'nmol/mol ' + pd , 4 : 'pmol/mol ' + pd , 5 : 'fmol/mol ' + pd } \n    elif focus_stage == 'ratios' : \n        udict = { 0 : 'counts/count ' + pd , 1 : '$10^{-3}$ counts/count ' + pd , 2 : '$10^{-6}$ counts/count ' + pd , 3 : '$10^{-9}$ counts/count ' + pd , 4 : '$10^{-12}$ counts/count ' + pd , 5 : '$10^{-15}$ counts/count ' + pd } \n    elif focus_stage in ( 'rawdata' , 'despiked' , 'bkgsub' ) : \n        udict = udict = { 0 : 'counts' , 1 : '$10^{-3}$ counts' , 2 : '$10^{-6}$ counts' , 3 : '$10^{-9}$ counts' , 4 : '$10^{-12}$ counts' , 5 : '$10^{-15}$ counts' } \n    else : \n        udict = { 0 : '' , 1 : '' , 2 : '' , 3 : '' , 4 : '' , 5 : '' } \n    a = abs ( a ) \n    n = 0 \n    if llim > a : \n        while llim > a : \n            a *= 1000 \n            n += 1 \n    return float ( 1000 ** n ) , udict [ n ] "}
{"9943": "\ndef tuples_2_bool ( tuples , x ) : \n    if np . ndim ( tuples ) == 1 : \n        tuples = [ tuples ] \n    out = np . zeros ( x . size , dtype = bool ) \n    for l , u in tuples : \n        out [ ( l < x ) & ( u > x ) ] = True \n    return out "}
{"9946": "\ndef findmins ( x , y ) : \n    return x [ np . r_ [ False , y [ : - 1 ] > y [ 1 : ] ] & np . r_ [ y [ 1 : ] > y [ : - 1 ] , False ] ] "}
{"9949": "\ndef cluster_DBSCAN ( data , eps = None , min_samples = None , n_clusters = None , maxiter = 200 , ** kwargs ) : \n    if n_clusters is None : \n        if eps is None : \n            eps = 0.3 \n        db = cl . DBSCAN ( eps = eps , min_samples = min_samples , ** kwargs ) . fit ( data ) \n    else : \n        clusters = 0 \n        eps_temp = 1 / .95 \n        niter = 0 \n        while n_clusters > clusters : \n            clusters_last = clusters \n            eps_temp *= 0.95 \n            db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n            clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n            if clusters_last > clusters : \n                eps_temp *= 1 / 0.95 \n                db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n                clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n                warnings . warn ( ( '\\n\\n***Unable to find {:.0f} clusters in ' 'data. Found {:.0f} with an eps of {:.2e}' '' ) . format ( n_clusters , clusters , eps_temp ) ) \n                break \n            niter += 1 \n            if niter == maxiter : \n                warnings . warn ( ( '\\n\\n***Maximum iterations ({:.0f}) reached' ', {:.0f} clusters not found.\\nDeacrease ' 'min_samples or n_clusters (or increase ' 'maxiter).' ) . format ( maxiter , n_clusters ) ) \n                break \n    labels = db . labels_ \n    core_samples_mask = np . zeros_like ( labels ) \n    core_samples_mask [ db . core_sample_indices_ ] = True \n    return labels , core_samples_mask "}
{"9957": "\ndef exclude_downhole ( filt , threshold = 2 ) : \n    cfilt = filt . copy ( ) \n    inds = bool_2_indices ( ~ filt ) \n    rem = ( threshold <= np . diff ( inds ) ) [ : , 0 ] \n    if any ( rem ) : \n        if 1 < inds [ rem ] . shape [ 0 ] : \n            limit = inds [ rem ] [ 1 , 0 ] \n            cfilt [ limit : ] = False \n    return cfilt "}
{"9958": "\ndef defrag ( filt , threshold = 3 , mode = 'include' ) : \n    if bool_2_indices ( filt ) is None : \n        return filt \n    if mode == 'include' : \n        inds = bool_2_indices ( ~ filt ) + 1 \n        rep = True \n    if mode == 'exclude' : \n        inds = bool_2_indices ( filt ) + 1 \n        rep = False \n    rem = ( threshold >= np . diff ( inds ) ) [ : , 0 ] \n    cfilt = filt . copy ( ) \n    if any ( rem ) : \n        for lo , hi in inds [ rem ] : \n            cfilt [ lo : hi ] = rep \n    return cfilt "}
{"9969": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = 15 , r_threshold = 0.9 , p_threshold = 0.05 , filt = True , recalc = False ) : \n    if window % 2 != 1 : \n        window += 1 \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    setn = self . filt . maxset + 1 \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    self . calc_correlation ( x_analyte , y_analyte , window , filt , recalc ) \n    r , p = self . correlations [ label ] \n    cfilt = ( r_threshold < abs ( r ) ) & ( p_threshold > p ) \n    cfilt = ~ cfilt \n    name = x_analyte + '_' + y_analyte + '_corr' \n    self . filt . add ( name = name , filt = cfilt , info = ( x_analyte + ' vs. ' + y_analyte + ' correlation filter.' ) , params = params , setn = setn ) \n    self . filt . off ( filt = name ) \n    self . filt . on ( analyte = y_analyte , filt = name ) \n    return "}
{"9977": "\ndef calc_M ( molecule ) : \n    els = elements ( ) \n    parens = re . compile ( '\\(([A-z0-9]+)\\)([0-9]+)?' ) \n    stoich = re . compile ( '([A-Z][a-z]?)([0-9]+)?' ) \n    ps = parens . findall ( molecule ) \n    rem = parens . sub ( '' , molecule ) \n    m = 0 \n    if 0 < len ( ps ) : \n        for sub , ns in ps : \n            ms = 0 \n            for e , n in stoich . findall ( sub ) : \n                me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n                if n == '' : \n                    n = 1 \n                else : \n                    n = int ( n ) \n                ms += me * n \n            if ns == '' : \n                ns = 1 \n            else : \n                ns = int ( ns ) \n            m += ms * ns \n    for e , n in stoich . findall ( rem ) : \n        me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n        if n == '' : \n            n = 1 \n        else : \n            n = int ( n ) \n        m += me * n \n    return m "}
{"9986": "\ndef bkg_calc_weightedmean ( self , analytes = None , weight_fwhm = None , n_min = 20 , n_max = None , cstep = None , bkg_filter = False , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if weight_fwhm is None : \n        weight_fwhm = 600 \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    if 'calc' not in self . bkg . keys ( ) : \n        if cstep is None : \n            cstep = weight_fwhm / 20 \n        elif weight_fwhm < cstep : \n            warnings . warn ( \"\\ncstep should be less than weight_fwhm. Your backgrounds\\n\" + \"might not behave as expected.\\n\" ) \n        bkg_t = np . linspace ( 0 , self . max_time , self . max_time // cstep ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    mean , std , stderr = gauss_weighted_stats ( self . bkg [ 'raw' ] . uTime , self . bkg [ 'raw' ] . loc [ : , analytes ] . values , self . bkg [ 'calc' ] [ 'uTime' ] , fwhm = weight_fwhm ) \n    for i , a in enumerate ( analytes ) : \n        self . bkg [ 'calc' ] [ a ] = { 'mean' : mean [ i ] , 'std' : std [ i ] , 'stderr' : stderr [ i ] } "}
{"9990": "\ndef make_subset ( self , samples = None , name = None ) : \n    for k , v in self . subsets . items ( ) : \n        if set ( v ) == set ( samples ) and k != 'not_in_set' : \n            return k \n    if isinstance ( samples , str ) : \n        samples = [ samples ] \n    not_exists = [ s for s in samples if s not in self . subsets [ 'All_Analyses' ] ] \n    if 0 < len ( not_exists ) : \n        raise ValueError ( ', ' . join ( not_exists ) + ' not in the list of sample names.\\nPlease check your sample names.\\nNote: Sample names are stored in the .samples attribute of your analysis.' ) \n    if name is None : \n        name = max ( [ - 1 ] + [ x for x in self . subsets . keys ( ) if isinstance ( x , int ) ] ) + 1 \n    self . _subset_names . append ( name ) \n    if samples is not None : \n        self . subsets [ name ] = samples \n        for s in samples : \n            try : \n                self . subsets [ 'not_in_set' ] . remove ( s ) \n            except ValueError : \n                pass \n    self . _has_subsets = True \n    return name "}
{"9991": "\ndef filter_gradient_threshold_percentile ( self , analyte , percentiles , level = 'population' , win = 15 , filt = False , samples = None , subset = None ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ analyte ] ) \n    self . get_gradients ( analytes = [ analyte ] , win = win , filt = filt , subset = subset ) \n    grad = self . gradients [ analyte ] [ ~ np . isnan ( self . gradients [ analyte ] ) ] \n    if isinstance ( percentiles , ( int , float ) ) : \n        percentiles = [ percentiles ] \n    if level == 'population' : \n        lims = np . percentile ( grad , percentiles ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Percentile Threshold Filter' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            setn = d . filt . maxset + 1 \n            g = calc_grads ( d . Time , d . focus , [ analyte ] , win ) [ analyte ] \n            if level == 'individual' : \n                gt = nominal_values ( g ) \n                lims = np . percentile ( gt [ ~ np . isnan ( gt ) ] , percentiles ) \n            if len ( lims ) == 1 : \n                above = lims [ 0 ] <= g \n                below = lims [ 0 ] > g \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_below' . format ( percentiles [ 0 ] ) , below , 'Gradients below {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_above' . format ( percentiles [ 0 ] ) , above , 'Gradients above {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n            elif len ( lims ) == 2 : \n                inside = ( min ( lims ) <= g ) & ( max ( lims ) >= g ) \n                outside = ( min ( lims ) > g ) | ( max ( lims ) < g ) \n                lpc = '-' . join ( [ '{:.1f}' . format ( p ) for p in percentiles ] ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_inside' , inside , 'Gradients between ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_outside' , outside , 'Gradients outside ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n            prog . update ( ) \n    return "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n            if 0 < len ( self . subsets [ 'not_in_set' ] ) : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif sample is not None : \n        s += 'Sample: ' + sample + '\\n' \n        s += self . data [ sample ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s += 'Subset: ' + str ( n ) + '\\n' \n            s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n            s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return "}
{"10000": "\ndef gradient_histogram ( self , analytes = None , win = 15 , filt = False , bins = None , samples = None , subset = None , recalc = True , ncol = 4 ) : \n    if analytes is None : \n        analytes = [ a for a in self . analytes if self . internal_standard not in a ] \n    if not hasattr ( self , 'gradients' ) : \n        self . gradients = Bunch ( ) \n    ncol = int ( ncol ) \n    n = len ( analytes ) \n    nrow = plot . calc_nrow ( n , ncol ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axs = plt . subplots ( nrow , ncol , figsize = [ 3. * ncol , 2.5 * nrow ] ) \n    if not isinstance ( axs , np . ndarray ) : \n        axs = [ axs ] \n    i = 0 \n    for a , ax in zip ( analytes , axs . flatten ( ) ) : \n        d = nominal_values ( self . gradients [ a ] ) \n        d = d [ ~ np . isnan ( d ) ] \n        m , u = unitpicker ( d , focus_stage = self . focus_stage , denominator = self . internal_standard ) \n        if bins is None : \n            ibins = np . linspace ( * np . percentile ( d * m , [ 1 , 99 ] ) , 50 ) \n        else : \n            ibins = bins \n        ax . hist ( d * m , bins = ibins , color = self . cmaps [ a ] ) \n        ax . axvline ( 0 , ls = 'dashed' , lw = 1 , c = ( 0 , 0 , 0 , 0.7 ) ) \n        ax . set_title ( a , loc = 'left' ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( 'N' ) \n        ax . set_xlabel ( u + '/s' ) \n        i += 1 \n    if ncol * nrow > i : \n        for ax in axs . flatten ( ) [ i : ] : \n            ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"10014": "\ndef pca_plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = True ) : \n    nc = pca . n_components \n    f = np . arange ( pca . n_features_ ) \n    cs = list ( itertools . combinations ( range ( nc ) , 2 ) ) \n    ind = ~ np . apply_along_axis ( any , 1 , np . isnan ( dt ) ) \n    cylim = ( pca . components_ . min ( ) , pca . components_ . max ( ) ) \n    yd = cylim [ 1 ] - cylim [ 0 ] \n    fig , axs = plt . subplots ( nc , nc , figsize = [ 3 * nc , nc * 3 ] , tight_layout = True ) \n    for x , y in zip ( * np . triu_indices ( nc ) ) : \n        if x == y : \n            tax = axs [ x , y ] \n            tax . bar ( f , pca . components_ [ x ] , 0.8 ) \n            tax . set_xticks ( [ ] ) \n            tax . axhline ( 0 , zorder = - 1 , c = ( 0 , 0 , 0 , 0.6 ) ) \n            tax . set_ylim ( cylim [ 0 ] - 0.2 * yd , cylim [ 1 ] + 0.2 * yd ) \n            for xi , yi , lab in zip ( f , pca . components_ [ x ] , xlabs ) : \n                if 0 < yi : \n                    yo = yd * 0.03 \n                    va = 'bottom' \n                else : \n                    yo = yd * - 0.02 \n                    va = 'top' \n                tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90 , fontsize = 8 ) \n        else : \n            xv = dt [ ind , x ] \n            yv = dt [ ind , y ] \n            if mode == 'scatter' : \n                axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) \n                axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) \n            if mode == 'hist2d' : \n                if lognorm : \n                    norm = mpl . colors . LogNorm ( ) \n                else : \n                    norm = None \n                axs [ x , y ] . hist2d ( xv , yv , 50 , cmap = plt . cm . Blues , norm = norm ) \n                axs [ y , x ] . hist2d ( yv , xv , 50 , cmap = plt . cm . Blues , norm = norm ) \n        if x == 0 : \n            axs [ y , x ] . set_ylabel ( 'PC{:.0f}' . format ( y + 1 ) ) \n        if y == nc - 1 : \n            axs [ y , x ] . set_xlabel ( 'PC{:.0f}' . format ( x + 1 ) ) \n    return fig , axs , xv , yv "}
{"10015": "\ndef bayes_scale ( s ) : \n    if 1 < sum ( ~ np . isnan ( s ) ) : \n        bm , bv , bs = bayes_mvs ( s [ ~ np . isnan ( s ) ] ) \n        return ( s - bm . statistic ) / bs . statistic \n    else : \n        return np . full ( s . shape , np . nan ) "}
{"10016": "\ndef median_scaler ( s ) : \n    if 2 < sum ( ~ np . isnan ( s ) ) : \n        ss = s [ ~ np . isnan ( s ) ] \n        median = np . median ( ss ) \n        IQR = np . diff ( np . percentile ( ss , [ 25 , 75 ] ) ) \n        return ( s - median ) / IQR \n    else : \n        return np . full ( s . shape , np . nan ) "}
{"10017": "\ndef noise_despike ( sig , win = 3 , nlim = 24. , maxiter = 4 ) : \n    if win % 2 != 1 : \n        win += 1 \n    kernel = np . ones ( win ) / win \n    over = np . ones ( len ( sig ) , dtype = bool ) \n    npad = int ( ( win - 1 ) / 2 ) \n    over [ : npad ] = False \n    over [ - npad : ] = False \n    nloops = 0 \n    while any ( over ) and ( maxiter > nloops ) : \n        rmean = np . convolve ( sig , kernel , 'valid' ) \n        rstd = rmean ** 0.5 \n        over [ npad : - npad ] = ( rmean + nlim * rstd < sig [ npad : - npad ] ) \n        if any ( over ) : \n            sig [ npad : - npad ] [ over [ npad : - npad ] ] = rmean [ over [ npad : - npad ] ] \n            nloops += 1 \n    return sig "}
{"10018": "\ndef expdecay_despike ( sig , expdecay_coef , tstep , maxiter = 3 ) : \n    noise = np . std ( sig [ : 5 ] ) \n    for i in [ 10 , 20 , 30 , 50 ] : \n        inoise = np . std ( sig [ : i ] ) \n        if 1.5 * noise > inoise : \n            noise = inoise \n    rms_noise3 = 3 * noise \n    i = 0 \n    f = True \n    while ( maxiter > i ) and f : \n        siglo = np . roll ( sig * np . exp ( tstep * expdecay_coef ) , 1 ) \n        sighi = np . roll ( sig * np . exp ( - tstep * expdecay_coef ) , - 1 ) \n        loind = ( siglo - rms_noise3 > sig ) & ( np . roll ( sig , - 1 ) - rms_noise3 > sig ) \n        hiind = ( sighi + rms_noise3 < sig ) & ( np . roll ( sig , 1 ) + rms_noise3 < sig ) \n        sig [ loind ] = sig [ np . roll ( loind , - 1 ) ] \n        sig [ hiind ] = sig [ np . roll ( hiind , - 1 ) ] \n        f = any ( np . concatenate ( [ loind , hiind ] ) ) \n        i += 1 \n    return sig "}
{"10041": "\ndef create_py ( self , nb , force = False ) : \n    if [ 4 , 2 ] <= list ( map ( int , re . findall ( '\\d+' , nbconvert . __version__ ) ) ) : \n        py_file = os . path . basename ( self . py_file ) \n    else : \n        py_file = self . py_file \n    try : \n        level = logger . logger . level \n    except AttributeError : \n        level = logger . level \n    spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py_file , '--log-level=%s' % level , self . outfile ] ) \n    with open ( self . py_file ) as f : \n        py_content = f . read ( ) \n    py_content = re . sub ( '^\\s*get_ipython\\(\\).magic.*' , '# \\g<0>' , py_content , flags = re . MULTILINE ) \n    with open ( self . py_file , 'w' ) as f : \n        f . write ( py_content ) "}
{"10042": "\ndef data_download ( self , files ) : \n    if 1 < len ( files ) : \n        return self . DATA_DOWNLOAD % ( ( '\\n\\n' + ' ' * 8 ) + ( '\\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) \n    return self . DATA_DOWNLOAD % ':download:`%s`' % files [ 0 ] "}
{"10044": "\ndef get_description ( self ) : \n    def split_header ( s , get_header = True ) : \n        s = s . lstrip ( ) . rstrip ( ) \n        parts = s . splitlines ( ) \n        if parts [ 0 ] . startswith ( '#' ) : \n            if get_header : \n                header = re . sub ( '#+\\s*' , '' , parts . pop ( 0 ) ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ 0 ] . replace ( '\\n' , ' ' ) \n            return header , desc \n        else : \n            if get_header : \n                if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : \n                    parts = parts [ 1 : ] \n                header = parts . pop ( 0 ) \n                if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : \n                    parts . pop ( 0 ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ 0 ] . replace ( '\\n' , ' ' ) \n            return header , desc \n    first_cell = self . nb [ 'cells' ] [ 0 ] \n    if not first_cell [ 'cell_type' ] == 'markdown' : \n        return '' , '' \n    header , desc = split_header ( first_cell [ 'source' ] ) \n    if not desc and 1 < len ( self . nb [ 'cells' ] ) : \n        second_cell = self . nb [ 'cells' ] [ 1 ] \n        if second_cell [ 'cell_type' ] == 'markdown' : \n            _ , desc = split_header ( second_cell [ 'source' ] , False ) \n    return header , desc "}
{"10045": "\ndef scale_image ( self , in_fname , out_fname , max_width , max_height ) : \n    try : \n        from PIL import Image \n    except ImportError : \n        import Image \n    img = Image . open ( in_fname ) \n    width_in , height_in = img . size \n    scale_w = max_width / float ( width_in ) \n    scale_h = max_height / float ( height_in ) \n    if max_height >= height_in * scale_w : \n        scale = scale_w \n    else : \n        scale = scale_h \n    if 1.0 <= scale and in_fname == out_fname : \n        return \n    width_sc = int ( round ( scale * width_in ) ) \n    height_sc = int ( round ( scale * height_in ) ) \n    img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) \n    thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) \n    pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) \n    thumb . paste ( img , pos_insert ) \n    thumb . save ( out_fname ) "}
{"10077": "\ndef as_string ( self , default_from = None ) : \n    encoding = self . charset or 'utf-8' \n    attachments = self . attachments or [ ] \n    if len ( attachments ) == 0 and not self . html : \n        msg = self . _mimetext ( self . body ) \n    elif 0 < len ( attachments ) and not self . html : \n        msg = MIMEMultipart ( ) \n        msg . attach ( self . _mimetext ( self . body ) ) \n    else : \n        msg = MIMEMultipart ( ) \n        alternative = MIMEMultipart ( 'alternative' ) \n        alternative . attach ( self . _mimetext ( self . body , 'plain' ) ) \n        alternative . attach ( self . _mimetext ( self . html , 'html' ) ) \n        msg . attach ( alternative ) \n    if self . charset : \n        msg [ 'Subject' ] = Header ( self . subject , encoding ) \n    else : \n        msg [ 'Subject' ] = self . subject \n    sender = self . sender or default_from \n    if sender is not None : \n        msg [ 'From' ] = sanitize_address ( sender , encoding ) \n    msg [ 'To' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . recipients , encoding ) ) ) ) \n    msg [ 'Date' ] = formatdate ( self . date , localtime = True ) \n    msg [ 'Message-ID' ] = self . msgId \n    if self . cc : \n        msg [ 'Cc' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . cc , encoding ) ) ) ) \n    if self . reply_to : \n        msg [ 'Reply-To' ] = sanitize_address ( self . reply_to , encoding ) \n    if self . extra_headers : \n        for k , v in self . extra_headers . items ( ) : \n            msg [ k ] = v \n    for attachment in attachments : \n        f = MIMEBase ( * attachment . content_type . split ( '/' ) ) \n        f . set_payload ( attachment . data ) \n        encode_base64 ( f ) \n        try : \n            attachment . filename and attachment . filename . encode ( 'ascii' ) \n        except UnicodeEncodeError : \n            filename = attachment . filename \n            if not PY3 : \n                filename = filename . encode ( 'utf8' ) \n            f . add_header ( 'Content-Disposition' , attachment . disposition , filename = ( 'UTF8' , '' , filename ) ) \n        else : \n            f . add_header ( 'Content-Disposition' , '%s;filename=%s' % ( attachment . disposition , attachment . filename ) ) \n        for key , value in attachment . headers : \n            f . add_header ( key , value ) \n        msg . attach ( f ) \n    return msg . as_string ( ) "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if self . processes == 1 : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if number_pages < end : \n                end = number_pages \n            begin += 1 \n            end += 1 \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10101": "\ndef _init_population_stats ( self , vcf_reader , dependent_tag_id ) : \n    n = 0 \n    mean = 0 \n    M2 = 0 \n    try : \n        vcf_reader . open ( ) \n        for vcf_record in vcf_reader . vcf_records ( ) : \n            for tag_values in vcf_record . sample_tag_values . values ( ) : \n                value = self . _get_dependent_value ( tag_values , dependent_tag_id ) \n                if value is not None : \n                    n += 1 \n                    delta = value - mean \n                    mean += delta / n \n                    M2 += delta * ( value - mean ) \n    finally : \n        vcf_reader . close ( ) \n    mean = round ( mean , self . _MAX_PRECISION ) \n    stdev = 0 \n    if n == 0 : \n        mean = None \n        stdev = None \n    elif 2 <= n : \n        variance = M2 / n \n        stdev = round ( math . sqrt ( variance ) , self . _MAX_PRECISION ) \n    return mean , stdev "}
{"10106": "\ndef seek_next_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        data_len , data = self . read ( self . read_size ) \n        data_where = 0 \n        if not data_len : \n            break \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ - 1 ] == b'\\r' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            terminator_len , terminator_data = self . read ( 1 ) \n            if terminator_len and terminator_data [ 0 ] == b'\\n' [ 0 ] : \n                data_len += 1 \n                data += b'\\n' \n            else : \n                self . file . seek ( terminator_where ) \n        while data_len > data_where : \n            terminator = self . prefix_line_terminator ( data [ data_where : ] ) \n            if terminator : \n                self . file . seek ( where + offset + data_where + len ( terminator ) ) \n                return self . file . tell ( ) \n            else : \n                data_where += 1 \n        offset += data_len \n        self . file . seek ( where + offset ) \n    return - 1 "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        if offset == where : \n            break \n        read_size = self . read_size if where >= self . read_size else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ 0 ] == b'\\n' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            if data_len + 1 < terminator_where : \n                self . file . seek ( where - offset - data_len - 1 , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( 1 ) \n                if terminator_data [ 0 ] == b'\\r' [ 0 ] : \n                    data_len += 1 \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while 0 < data_where : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and offset == 0 and data_where == data_len : \n                data_where -= len ( terminator ) \n            elif terminator : \n                self . file . seek ( where - offset - ( data_len - data_where ) ) \n                return self . file . tell ( ) \n            else : \n                data_where -= 1 \n        offset += data_len \n    if where == 0 : \n        return - 1 \n    else : \n        self . file . seek ( 0 ) \n        return 0 "}
{"10110": "\ndef follow ( self ) : \n    trailing = True \n    while True : \n        where = self . file . tell ( ) \n        if os . fstat ( self . file . fileno ( ) ) . st_size < where : \n            where = 0 \n            self . file . seek ( where ) \n        line = self . file . readline ( ) \n        if line : \n            if trailing and line in self . LINE_TERMINATORS : \n                trailing = False \n                continue \n            terminator = self . suffix_line_terminator ( line ) \n            if terminator : \n                line = line [ : - len ( terminator ) ] \n            trailing = False \n            yield line \n        else : \n            trailing = True \n            self . file . seek ( where ) \n            yield None "}
{"10112": "\ndef parse_record ( cls , vcf_line , sample_names ) : \n    vcf_fields = vcf_line . rstrip ( \"\\r\\n\" ) . split ( \"\\t\" ) \n    chrom , pos , rid , ref , alt , qual , rfilter , info = vcf_fields [ 0 : 8 ] \n    sample_fields = [ ] \n    sample_tag_values = { } \n    if 9 < len ( vcf_fields ) : \n        rformat = vcf_fields [ 8 ] \n        sample_fields = vcf_fields [ 9 : ] \n        sample_tag_values = VcfRecord . _sample_tag_values ( sample_names , rformat , sample_fields ) \n    return VcfRecord ( chrom , pos , ref , alt , rid , qual , rfilter , info , sample_tag_values ) "}
{"10115": "\ndef _join_info_fields ( self ) : \n    if self . info_dict : \n        info_fields = [ ] \n        if 1 < len ( self . info_dict ) : \n            self . info_dict . pop ( \".\" , None ) \n        for field , value in self . info_dict . items ( ) : \n            if field == value : \n                info_fields . append ( value ) \n            else : \n                info_fields . append ( \"=\" . join ( [ field , value ] ) ) \n        self . info = \";\" . join ( info_fields ) \n    else : \n        self . info = \".\" "}
{"10129": "\ndef iter_osm_stream ( start_sqn = None , base_url = 'https://planet.openstreetmap.org/replication/minute' , expected_interval = 60 , parse_timestamps = True , state_dir = None ) : \n    if state_dir : \n        if not os . path . exists ( state_dir ) : \n            raise Exception ( 'Specified state_dir \"%s\" doesn\\'t exist.' % state_dir ) \n        if os . path . exists ( '%s/state.txt' % state_dir ) : \n            with open ( '%s/state.txt' % state_dir ) as f : \n                state = readState ( f ) \n                start_sqn = state [ 'sequenceNumber' ] \n    if not start_sqn : \n        u = urllib2 . urlopen ( '%s/state.txt' % base_url ) \n        state = readState ( u ) \n    else : \n        sqnStr = str ( start_sqn ) . zfill ( 9 ) \n        u = urllib2 . urlopen ( '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) ) \n        state = readState ( u ) \n    interval_fudge = 0.0 \n    while True : \n        sqnStr = state [ 'sequenceNumber' ] . zfill ( 9 ) \n        url = '%s/%s/%s/%s.osc.gz' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        content = urllib2 . urlopen ( url ) \n        content = StringIO . StringIO ( content . read ( ) ) \n        gzipper = gzip . GzipFile ( fileobj = content ) \n        for a in iter_osm_change_file ( gzipper , parse_timestamps ) : \n            yield a \n        stateTs = datetime . datetime . strptime ( state [ 'timestamp' ] , \"%Y-%m-%dT%H:%M:%SZ\" ) \n        yield ( None , model . Finished ( state [ 'sequenceNumber' ] , stateTs ) ) \n        nextTs = stateTs + datetime . timedelta ( seconds = expected_interval + interval_fudge ) \n        if nextTs > datetime . datetime . utcnow ( ) : \n            timeToSleep = ( nextTs - datetime . datetime . utcnow ( ) ) . total_seconds ( ) \n        else : \n            timeToSleep = 0.0 \n        time . sleep ( timeToSleep ) \n        sqnStr = str ( int ( state [ 'sequenceNumber' ] ) + 1 ) . zfill ( 9 ) \n        url = '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        delay = 1.0 \n        while True : \n            try : \n                u = urllib2 . urlopen ( url ) \n                interval_fudge -= ( interval_fudge / 2.0 ) \n                break \n            except urllib2 . HTTPError as e : \n                if e . code == 404 : \n                    time . sleep ( delay ) \n                    delay = min ( delay * 2 , 13 ) \n                    interval_fudge += delay \n        if state_dir : \n            with open ( '%s/state.txt' % state_dir , 'w' ) as f : \n                f . write ( u . read ( ) ) \n            with open ( '%s/state.txt' % state_dir , 'r' ) as f : \n                state = readState ( f ) \n        else : \n            state = readState ( u ) "}
{"10134": "\ndef user_quantity_remaining ( self , user , filtered = True ) : \n    if filtered : \n        if hasattr ( self . condition , \"remainder\" ) : \n            return self . condition . remainder \n    qs = type ( self . condition ) . objects . filter ( pk = self . condition . id ) \n    qs = self . pre_filter ( qs , user ) \n    if 0 < len ( qs ) : \n        return qs [ 0 ] . remainder \n    else : \n        return 0 "}
{"10142": "\ndef _autoextend_reservation ( self ) : \n    time = timezone . now ( ) \n    time_elapsed_since_updated = ( time - self . cart . time_last_updated ) \n    residual = self . cart . reservation_duration - time_elapsed_since_updated \n    reservations = [ datetime . timedelta ( 0 ) , residual ] \n    if 1 <= len ( self . cart . vouchers . all ( ) ) : \n        reservations . append ( inventory . Voucher . RESERVATION_DURATION ) \n    items = commerce . ProductItem . objects . filter ( cart = self . cart ) \n    agg = items . aggregate ( Max ( \"product__reservation_duration\" ) ) \n    product_max = agg [ \"product__reservation_duration__max\" ] \n    if product_max is not None : \n        reservations . append ( product_max ) \n    self . cart . time_last_updated = time \n    self . cart . reservation_duration = max ( reservations ) "}
{"10147": "\ndef _add_discount ( self , product , quantity , discounts ) : \n    def matches ( discount ) : \n        if isinstance ( discount . clause , conditions . DiscountForCategory ) : \n            return discount . clause . category == product . category \n        else : \n            return discount . clause . product == product \n    def value ( discount ) : \n        if discount . clause . percentage is not None : \n            return discount . clause . percentage * product . price \n        else : \n            return discount . clause . price \n    discounts = [ i for i in discounts if matches ( i ) ] \n    discounts . sort ( key = value ) \n    for candidate in reversed ( discounts ) : \n        if quantity == 0 : \n            break \n        elif candidate . quantity == 0 : \n            continue \n        discount_item = commerce . DiscountItem . objects . create ( product = product , cart = self . cart , discount = candidate . discount , quantity = quantity , ) \n        ours = discount_item . quantity \n        allowed = candidate . quantity \n        if allowed < ours : \n            discount_item . quantity = allowed \n            discount_item . save ( ) \n            quantity = ours - allowed \n        else : \n            quantity = 0 \n        candidate . quantity -= discount_item . quantity "}
{"10163": "\ndef attendee_list ( request ) : \n    attendees = people . Attendee . objects . select_related ( \"attendeeprofilebase\" , \"user\" , ) \n    profiles = AttendeeProfile . objects . filter ( attendee__in = attendees ) . select_related ( \"attendee\" , \"attendee__user\" , ) \n    profiles_by_attendee = dict ( ( i . attendee , i ) for i in profiles ) \n    attendees = attendees . annotate ( has_registered = Count ( Q ( user__invoice__status = commerce . Invoice . STATUS_PAID ) ) , ) \n    headings = [ \"User ID\" , \"Name\" , \"Email\" , \"Has registered\" , ] \n    data = [ ] \n    for a in attendees : \n        data . append ( [ a . user . id , ( profiles_by_attendee [ a ] . attendee_name ( ) if a in profiles_by_attendee else \"\" ) , a . user . email , 0 < a . has_registered , ] ) \n    data . sort ( key = lambda a : ( - a [ 3 ] , a [ 0 ] ) ) \n    return AttendeeListReport ( \"Attendees\" , headings , data , link_view = attendee ) "}
{"10169": "\ndef guided_registration ( request , page_number = None ) : \n    PAGE_PROFILE = 1 \n    PAGE_TICKET = 2 \n    PAGE_PRODUCTS = 3 \n    PAGE_PRODUCTS_MAX = 4 \n    TOTAL_PAGES = 4 \n    ticket_category = inventory . Category . objects . get ( id = settings . TICKET_PRODUCT_CATEGORY ) \n    cart = CartController . for_user ( request . user ) \n    attendee = people . Attendee . get_instance ( request . user ) \n    if attendee . completed_registration : \n        return redirect ( review ) \n    has_profile = hasattr ( attendee , \"attendeeprofilebase\" ) \n    if not has_profile : \n        max_page = PAGE_PROFILE \n        redirect_page = PAGE_PROFILE \n    else : \n        products = inventory . Product . objects . filter ( productitem__cart = cart . cart ) \n        products = products . filter ( category = ticket_category ) \n        if products . count ( ) == 0 : \n            max_page = PAGE_TICKET \n            redirect_page = PAGE_TICKET \n        else : \n            max_page = PAGE_PRODUCTS_MAX \n            redirect_page = PAGE_PRODUCTS \n    if page_number is None or max_page < int ( page_number ) : \n        return redirect ( \"guided_registration\" , redirect_page ) \n    page_number = int ( page_number ) \n    next_step = redirect ( \"guided_registration\" , page_number + 1 ) \n    with BatchController . batch ( request . user ) : \n        available = ProductController . available_products ( request . user , category = ticket_category ) \n        if not available : \n            messages . error ( request , \"There are no more tickets available.\" ) \n            return redirect ( \"dashboard\" ) \n        sections = [ ] \n        if page_number == PAGE_PROFILE : \n            title = \"Attendee information\" \n            sections = _guided_registration_profile_and_voucher ( request ) \n        elif page_number == PAGE_TICKET : \n            title = \"Select ticket type\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_TICKETS_ONLY ) \n        elif page_number == PAGE_PRODUCTS : \n            title = \"Additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_ALL_ADDITIONAL ) \n        elif page_number == PAGE_PRODUCTS_MAX : \n            title = \"More additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_EXCLUDE_COMPLETE ) \n        if not sections : \n            attendee . completed_registration = True \n            attendee . save ( ) \n            return redirect ( \"review\" ) \n        if sections and request . method == \"POST\" : \n            for section in sections : \n                if section . form . errors : \n                    break \n            else : \n                return next_step \n    data = { \"current_step\" : page_number , \"sections\" : sections , \"title\" : title , \"total_steps\" : TOTAL_PAGES , } \n    return render ( request , \"registrasion/guided_registration.html\" , data ) "}
{"10174": "\ndef _handle_voucher ( request , prefix ) : \n    voucher_form = forms . VoucherForm ( request . POST or None , prefix = prefix ) \n    current_cart = CartController . for_user ( request . user ) \n    if ( voucher_form . is_valid ( ) and voucher_form . cleaned_data [ \"voucher\" ] . strip ( ) ) : \n        voucher = voucher_form . cleaned_data [ \"voucher\" ] \n        voucher = inventory . Voucher . normalise_code ( voucher ) \n        if 0 < len ( current_cart . cart . vouchers . filter ( code = voucher ) ) : \n            handled = False \n        else : \n            try : \n                current_cart . apply_voucher ( voucher ) \n            except Exception as e : \n                voucher_form . add_error ( \"voucher\" , e ) \n            handled = True \n    else : \n        handled = False \n    return ( voucher_form , handled ) "}
{"10186": "\ndef available_discounts ( cls , user , categories , products ) : \n    filtered_clauses = cls . _filtered_clauses ( user ) \n    categories = set ( categories ) \n    products = set ( products ) \n    product_categories = set ( product . category for product in products ) \n    all_categories = categories | product_categories \n    filtered_clauses = ( clause for clause in filtered_clauses if hasattr ( clause , 'product' ) and clause . product in products or hasattr ( clause , 'category' ) and clause . category in all_categories ) \n    discounts = [ ] \n    accepted_discounts = set ( ) \n    failed_discounts = set ( ) \n    for clause in filtered_clauses : \n        discount = clause . discount \n        cond = ConditionController . for_condition ( discount ) \n        past_use_count = clause . past_use_count \n        if clause . quantity <= past_use_count : \n            pass \n        elif discount not in failed_discounts : \n            is_accepted = discount in accepted_discounts \n            if is_accepted or cond . is_met ( user , filtered = True ) : \n                discounts . append ( DiscountAndQuantity ( discount = discount , clause = clause , quantity = clause . quantity - past_use_count , ) ) \n                accepted_discounts . add ( discount ) \n            else : \n                failed_discounts . add ( discount ) \n    return discounts "}
{"10188": "\ndef available_products ( cls , user , category = None , products = None ) : \n    if category is None and products is None : \n        raise ValueError ( \"You must provide products or a category\" ) \n    if category is not None : \n        all_products = inventory . Product . objects . filter ( category = category ) \n        all_products = all_products . select_related ( \"category\" ) \n    else : \n        all_products = [ ] \n    if products is not None : \n        all_products = set ( itertools . chain ( all_products , products ) ) \n    category_remainders = CategoryController . user_remainders ( user ) \n    product_remainders = ProductController . user_remainders ( user ) \n    passed_limits = set ( product for product in all_products if 0 < category_remainders [ product . category . id ] if 0 < product_remainders [ product . id ] ) \n    failed_and_messages = FlagController . test_flags ( user , products = passed_limits ) \n    failed_conditions = set ( i [ 0 ] for i in failed_and_messages ) \n    out = list ( passed_limits - failed_conditions ) \n    out . sort ( key = lambda product : product . order ) \n    return out "}
{"10190": "\ndef cancellation_fee ( self , percentage ) : \n    from . invoice import InvoiceController \n    assert ( 0 <= percentage and 100 >= percentage ) \n    cancellation_fee = self . credit_note . value * percentage / 100 \n    due = datetime . timedelta ( days = 1 ) \n    item = [ ( \"Cancellation fee\" , cancellation_fee ) ] \n    invoice = InvoiceController . manual_invoice ( self . credit_note . invoice . user , due , item ) \n    if not invoice . is_paid : \n        self . apply_to_invoice ( invoice ) \n    return InvoiceController ( invoice ) "}
{"10197": "\ndef _apply_credit_notes ( cls , invoice ) : \n    invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS_UNPAID , ) \n    if 1 < invoices . count ( ) : \n        return \n    notes = commerce . CreditNote . unclaimed ( ) . filter ( invoice__user = invoice . user ) \n    for note in notes : \n        try : \n            CreditNoteController ( note ) . apply_to_invoice ( invoice ) \n        except ValidationError : \n            break \n    invoice . refresh_from_db ( ) "}
{"10201": "\ndef update_status ( self ) : \n    old_status = self . invoice . status \n    total_paid = self . invoice . total_payments ( ) \n    num_payments = commerce . PaymentBase . objects . filter ( invoice = self . invoice , ) . count ( ) \n    remainder = self . invoice . value - total_paid \n    if old_status == commerce . Invoice . STATUS_UNPAID : \n        if 0 >= remainder : \n            self . _mark_paid ( ) \n        elif total_paid == 0 and 0 < num_payments : \n            self . _mark_void ( ) \n    elif old_status == commerce . Invoice . STATUS_PAID : \n        if 0 < remainder : \n            self . _mark_refunded ( ) \n    elif old_status == commerce . Invoice . STATUS_REFUNDED : \n        pass \n    elif old_status == commerce . Invoice . STATUS_VOID : \n        pass \n    residual = 0 \n    if self . invoice . is_paid : \n        if 0 > remainder : \n            residual = 0 - remainder \n    elif self . invoice . is_void or self . invoice . is_refunded : \n        residual = total_paid \n    if residual != 0 : \n        CreditNoteController . generate_from_invoice ( self . invoice , residual ) \n    self . email_on_invoice_change ( self . invoice , old_status , self . invoice . status , ) "}
{"10204": "\ndef update_validity ( self ) : \n    is_valid = self . _invoice_matches_cart ( ) \n    cart = self . invoice . cart \n    if self . invoice . is_unpaid and is_valid and cart : \n        try : \n            CartController ( cart ) . validate_cart ( ) \n        except ValidationError : \n            is_valid = False \n    if not is_valid : \n        if 0 < self . invoice . total_payments ( ) : \n            self . refund ( ) \n        else : \n            self . void ( ) "}
{"10205": "\ndef void ( self ) : \n    if 0 < self . invoice . total_payments ( ) : \n        raise ValidationError ( \"Invoices with payments must be refunded.\" ) \n    elif self . invoice . is_refunded : \n        raise ValidationError ( \"Refunded invoices may not be voided.\" ) \n    if self . invoice . is_paid : \n        self . _release_cart ( ) \n    self . _mark_void ( ) "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if 0 < i and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    print ( ) \n    return session_id "}
{"10220": "\ndef get_repo_and_project ( self ) : \n    app = self . app \n    repo = app . data . apply ( 'github-repo' , app . args . github_repo , app . prompt_repo , on_load = app . github . get_repo , on_save = lambda r : r . id ) \n    assert repo , \"repository not found.\" \n    project = app . data . apply ( 'asana-project' , app . args . asana_project , app . prompt_project , on_load = app . asana . projects . find_by_id , on_save = lambda p : p [ 'id' ] ) \n    assert project , \"project not found.\" \n    first_issue = app . data . apply ( 'first-issue' , app . args . first_issue , \"set the first issue to sync with [1 for new repos]\" , on_save = int ) \n    assert first_issue \n    assert 0 <= first_issue , \"issue must be positive\" \n    app . sync_data ( ) \n    return repo , project "}
{"10223": "\ndef search_variants_by_coordinates ( coordinate_query , search_mode = 'any' ) : \n    get_all_variants ( ) \n    ct = COORDINATE_TABLE \n    start_idx = COORDINATE_TABLE_START \n    stop_idx = COORDINATE_TABLE_STOP \n    chr_idx = COORDINATE_TABLE_CHR \n    start = int ( coordinate_query . start ) \n    stop = int ( coordinate_query . stop ) \n    chromosome = str ( coordinate_query . chr ) \n    left_idx = chr_idx . searchsorted ( chromosome ) \n    right_idx = chr_idx . searchsorted ( chromosome , side = 'right' ) \n    chr_ct_idx = chr_idx [ left_idx : right_idx ] . index \n    right_idx = start_idx . searchsorted ( stop , side = 'right' ) \n    start_ct_idx = start_idx [ : right_idx ] . index \n    left_idx = stop_idx . searchsorted ( start ) \n    stop_ct_idx = stop_idx [ left_idx : ] . index \n    match_idx = chr_ct_idx & start_ct_idx & stop_ct_idx \n    m_df = ct . loc [ match_idx , ] \n    if search_mode == 'any' : \n        var_digests = m_df . v_hash . to_list ( ) \n        return [ CACHE [ v ] for v in var_digests ] \n    elif search_mode == 'include_smaller' : \n        match_idx = ( m_df . start >= start ) & ( m_df . stop <= stop ) \n    elif search_mode == 'include_larger' : \n        match_idx = ( m_df . start <= start ) & ( m_df . stop >= stop ) \n    elif search_mode == 'exact' : \n        match_idx = ( start == m_df . stop ) & ( stop == m_df . start ) \n        if coordinate_query . alt : \n            match_idx = match_idx & ( coordinate_query . alt == m_df . alt ) \n    else : \n        raise ValueError ( \"unexpected search mode\" ) \n    var_digests = m_df . loc [ match_idx , ] . v_hash . to_list ( ) \n    return [ CACHE [ v ] for v in var_digests ] "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if current_q [ 'chr' ] > prev_q [ 'chr' ] : \n            return True \n        if current_q [ 'chr' ] < prev_q [ 'chr' ] : \n            return False \n        if current_q [ 'start' ] > prev_q [ 'start' ] : \n            return True \n        if current_q [ 'start' ] < prev_q [ 'start' ] : \n            return False \n        if current_q [ 'stop' ] > prev_q [ 'stop' ] : \n            return True \n        if current_q [ 'stop' ] < prev_q [ 'stop' ] : \n            return False \n        return True \n    ct_pointer = 0 \n    query_pointer = 0 \n    last_query_pointer = - 1 \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while len ( sorted_queries ) > query_pointer and len ( ct ) > ct_pointer : \n        if last_query_pointer != query_pointer : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if c_chr > q_chr : \n            query_pointer += 1 \n            continue \n        if c_chr < q_chr : \n            ct_pointer += 1 \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if c_stop < q_start : \n            ct_pointer += 1 \n            continue \n        if c_start > q_stop : \n            query_pointer += 1 \n            continue \n        if search_mode == 'any' : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        elif search_mode == 'exact' and q_start == c_start and q_stop == c_stop : \n            q_alt = q . alt \n            c_alt = c . alt \n            if not ( q_alt and c_alt and q_alt != c_alt ) : \n                matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n        elif search_mode == 'include_smaller' : \n            raise NotImplementedError \n        elif search_mode == 'include_larger' : \n            raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer += 1 \n    return dict ( matches ) "}
{"10241": "\ndef ekm_log ( logstr , priority = 3 ) : \n    if ekmmeters_log_level >= priority : \n        dt = datetime . datetime \n        stamp = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M.%f\" ) \n        ekmmeters_log_func ( \"[EKM Meter Debug Message: \" + stamp + \"] -> \" + logstr ) \n    pass "}
{"10246": "\ndef setContext ( self , context_str ) : \n    if ( len ( self . m_context ) == 0 ) and ( 7 <= len ( context_str ) ) : \n        if context_str [ 0 : 7 ] != \"request\" : \n            ekm_log ( \"Context: \" + context_str ) \n    self . m_context = context_str "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if 1 > period or 3 < period : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( 0 > hour ) or ( 23 < hour ) or ( 0 > minute ) or ( 59 < minute ) or ( 0 > tariff ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + 1 ) ) \n        return False \n    period += 1 \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return False \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return False \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return False \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return True "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season += 1 \n    schedule += 1 \n    if ( ( 1 > season ) or ( Extents . Seasons < season ) or ( 1 > schedule ) or ( Extents . Schedules < schedule ) or ( 12 < month ) or ( 0 > month ) or ( 0 > day ) or ( 31 < day ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return False \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return False \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return False \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return False \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return True "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday += 1 \n    if ( 12 < month ) or ( 0 > month ) or ( 31 < day ) or ( 0 > day ) or ( 1 > holiday ) or ( Extents . Holidays < holiday ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return False \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return False \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return False \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return True "}
{"10262": "\ndef extractSchedule ( self , schedule , period ) : \n    ret = namedtuple ( \"ret\" , [ \"Hour\" , \"Min\" , \"Tariff\" , \"Period\" , \"Schedule\" ] ) \n    work_table = self . m_schd_1_to_4 \n    if Schedules . Schedule_5 <= schedule <= Schedules . Schedule_6 : \n        work_table = self . m_schd_5_to_6 \n    period += 1 \n    schedule += 1 \n    ret . Period = str ( period ) \n    ret . Schedule = str ( schedule ) \n    if ( 1 > schedule ) or ( Extents . Schedules < schedule ) or ( 0 > period ) or ( Extents . Periods < period ) : \n        ekm_log ( \"Out of bounds: tariff \" + str ( period ) + \" for schedule \" + str ( schedule ) ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    idxhr = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Hour\" \n    idxmin = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Min\" \n    idxrate = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Tariff\" \n    if idxhr not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxhr ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxmin not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxmin ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxrate not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxrate ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    ret . Hour = work_table [ idxhr ] [ MeterData . StringValue ] \n    ret . Min = work_table [ idxmin ] [ MeterData . StringValue ] . zfill ( 2 ) \n    ret . Tariff = work_table [ idxrate ] [ MeterData . StringValue ] \n    return ret "}
{"10264": "\ndef extractMonthTariff ( self , month ) : \n    ret = namedtuple ( \"ret\" , [ \"Month\" , Field . kWh_Tariff_1 , Field . kWh_Tariff_2 , Field . kWh_Tariff_3 , Field . kWh_Tariff_4 , Field . kWh_Tot , Field . Rev_kWh_Tariff_1 , Field . Rev_kWh_Tariff_2 , Field . Rev_kWh_Tariff_3 , Field . Rev_kWh_Tariff_4 , Field . Rev_kWh_Tot ] ) \n    month += 1 \n    ret . Month = str ( month ) \n    if ( 1 > month ) or ( Extents . Months < month ) : \n        ret . kWh_Tariff_1 = ret . kWh_Tariff_2 = ret . kWh_Tariff_3 = ret . kWh_Tariff_4 = str ( 0 ) \n        ret . Rev_kWh_Tariff_1 = ret . Rev_kWh_Tariff_2 = ret . Rev_kWh_Tariff_3 = ret . Rev_kWh_Tariff_4 = str ( 0 ) \n        ret . kWh_Tot = ret . Rev_kWh_Tot = str ( 0 ) \n        ekm_log ( \"Out of range(Extents.Months) month = \" + str ( month ) ) \n        return ret \n    base_str = \"Month_\" + str ( month ) + \"_\" \n    ret . kWh_Tariff_1 = self . m_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_2 = self . m_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_3 = self . m_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_4 = self . m_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . kWh_Tot = self . m_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_1 = self . m_rev_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_2 = self . m_rev_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_3 = self . m_rev_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_4 = self . m_rev_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tot = self . m_rev_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    return ret "}
{"10266": "\ndef extractHolidayDate ( self , setting_holiday ) : \n    ret = namedtuple ( \"result\" , [ \"Holiday\" , \"Month\" , \"Day\" ] ) \n    setting_holiday += 1 \n    ret . Holiday = str ( setting_holiday ) \n    if ( 1 > setting_holiday ) or ( Extents . Holidays < setting_holiday ) : \n        ekm_log ( \"Out of bounds:  holiday \" + str ( setting_holiday ) ) \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    idxday = \"Holiday_\" + str ( setting_holiday ) + \"_Day\" \n    idxmon = \"Holiday_\" + str ( setting_holiday ) + \"_Mon\" \n    if idxmon not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    if idxday not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    ret . Day = self . m_hldy [ idxday ] [ MeterData . StringValue ] \n    ret . Month = self . m_hldy [ idxmon ] [ MeterData . StringValue ] \n    return ret "}
{"10277": "\ndef setLCDCmd ( self , display_list , password = \"00000000\" ) : \n    result = False \n    try : \n        self . initLcd ( ) \n        item_cnt = len ( display_list ) \n        if ( 45 < item_cnt ) or ( 0 >= item_cnt ) : \n            ekm_log ( \"LCD item list must have between 1 and 40 items\" ) \n            return False \n        for display_item in display_list : \n            self . addLcdItem ( int ( display_item ) ) \n        result = self . setLCD ( password ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if 0 > seconds or 9999 < seconds : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"\" \n                req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + \"2903\" ) \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10321": "\ndef dereference ( self , callback = None , args = None , kwargs = None ) : \n    if args is None : \n        args = tuple ( ) \n    if kwargs is None : \n        kwargs = { } \n    client = self . conn . client \n    should_execute = False \n    if self . force_expiry : \n        should_execute = True \n    if not should_execute : \n        self . nodelist . remove_node ( self . conn . id ) \n        self . nodelist . remove_expired_nodes ( ) \n        updated_refcount = client . incr ( self . refcount_key , - 1 ) \n        should_execute = ( 0 >= updated_refcount ) \n    try : \n        if callable ( callback ) and should_execute : \n            callback ( * args , ** kwargs ) \n    finally : \n        if should_execute : \n            client . delete ( self . resource_key , self . nodelist . nodelist_key , self . times_modified_key , self . refcount_key ) \n        self . conn . remove_from_registry ( self . resource_key ) \n    return should_execute "}
{"10322": "\ndef delimit ( values , delimiter = ', ' ) : \n    toks = [ ] \n    if not values : \n        return toks \n    if not isinstance ( delimiter , ( list , tuple ) ) : \n        delimiter = [ delimiter ] \n    last = len ( values ) - 1 \n    for i , value in enumerate ( values ) : \n        toks . append ( value ) \n        if last > i : \n            toks . extend ( delimiter ) \n    return toks "}
{"10324": "\ndef merge ( self , status : 'Status[Input, Output]' ) -> 'Status[Input, Output]' : \n    if status is None or status . farthest is None : \n        pass \n    elif self . farthest is None : \n        self . farthest = status . farthest \n        self . expected = status . expected \n    elif self . farthest . position > status . farthest . position : \n        pass \n    elif self . farthest . position < status . farthest . position : \n        self . farthest = status . farthest \n        self . expected = status . expected \n    else : \n        self . expected = status . expected + self . expected \n    return self "}
{"10333": "\ndef lit ( literal : Sequence [ Input ] , * literals : Sequence [ Sequence [ Input ] ] ) -> Parser : \n    if 0 < len ( literals ) : \n        return AlternativeParser ( options . handle_literal ( literal ) , * map ( options . handle_literal , literals ) ) \n    else : \n        return options . handle_literal ( literal ) "}
{"10352": "\ndef validate_args ( cls , tag_name , * args , ** kwargs ) : \n    if cls . min_args is not None and cls . min_args > len ( args ) : \n        if cls . min_args == 1 : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} argument\" . format ( tag_name , cls . min_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} arguments\" . format ( tag_name , cls . min_args ) ) \n    if cls . max_args is not None and cls . max_args < len ( args ) : \n        if cls . max_args == 0 : \n            if cls . allowed_kwargs : \n                raise TemplateSyntaxError ( \"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\" . format ( tag_name , cls . allowed_kwargs [ 0 ] ) ) \n            else : \n                raise TemplateSyntaxError ( \"'{0}' tag doesn't support any arguments\" . format ( tag_name ) ) \n        elif cls . max_args == 1 : \n            raise TemplateSyntaxError ( \"'{0}' tag only allows {1} argument.\" . format ( tag_name , cls . max_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag only allows {1} arguments.\" . format ( tag_name , cls . max_args ) ) "}
{"10368": "\ndef duplicates ( * iterables , ** kwargs ) : \n    key = kwargs . pop ( 'key' , lambda x : x ) \n    assert not kwargs \n    zipped = more_itertools . collate ( * iterables , key = key ) \n    grouped = itertools . groupby ( zipped , key = key ) \n    groups = ( tuple ( g ) for k , g in grouped ) \n    def has_dupes ( group ) : \n        return 1 < len ( group ) \n    return filter ( has_dupes , groups ) "}
{"10372": "\ndef get_first_n_queues ( self , n ) : \n    try : \n        while n > len ( self . queues ) : \n            self . __fetch__ ( ) \n    except StopIteration : \n        pass \n    values = list ( self . queues . values ( ) ) \n    missing = n - len ( values ) \n    values . extend ( iter ( [ ] ) for n in range ( missing ) ) \n    return values "}
{"10374": "\ndef parse_as_var ( parser , token ) : \n    if isinstance ( token , Token ) : \n        bits = token . split_contents ( ) \n    else : \n        bits = token \n    as_var = None \n    if 2 < len ( bits ) and bits [ - 2 ] == 'as' : \n        bits = bits [ : ] \n        as_var = bits . pop ( ) \n        bits . pop ( ) \n    return bits , as_var "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = 1 if pat . group ( 5 ) == 'odd' else 0 \n    elif pat . group ( 6 ) : \n        a = 0 \n        b = int ( pat . group ( 6 ) ) \n    else : \n        sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n        coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 \n    reverse = False \n    if lexeme == 'nth-last-child' : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return False \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += 1 \n        if a == 0 : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and 0 <= ( idx * a + b ) \n        return m \n    return validate "}
{"10389": "\ndef expire ( self , age ) : \n    now = time . time ( ) \n    cache = self . _acquire_cache ( ) \n    expired = [ k for k , v in cache . items ( ) if age < ( now - v [ 0 ] ) ] \n    for k in expired : \n        if k in cache : \n            del cache [ k ] \n        if k in self . store : \n            del self . store [ k ] \n    self . _write_cache ( cache ) "}
{"10396": "\ndef rendered_content ( self ) : \n    template = self . resolve_template ( self . template_name ) \n    if 8 > django . VERSION [ 1 ] : \n        if template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    else : \n        if template . template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = jsmin . jsmin ( content ) \n    return content "}
{"10397": "\ndef get_fn ( self , fn , max_lines = None ) : \n    stat = os . stat ( self . logfile ) \n    if ( stat . st_ino == self . lastInode ) and ( stat . st_size == self . lastSize ) : \n        return [ ] \n    if ( stat . st_ino != self . lastInode ) or ( self . lastSize > stat . st_size ) : \n        self . lastSize = 0 \n    fi = open ( self . logfile , 'rt' ) \n    fi . seek ( self . lastSize ) \n    self . lastInode = stat . st_ino \n    lines = 0 \n    for i in fi : \n        lines += 1 \n        if max_lines and ( max_lines < lines ) : \n            self . storeLast ( ) \n            fi . close ( ) \n            return \n        if '\\n' in i : \n            self . lastSize += len ( i ) \n            if self . parser : \n                line = self . parser ( i . strip ( '\\n' ) ) \n            else : \n                line = i . strip ( '\\n' ) \n            fn ( line ) \n    self . storeLast ( ) \n    fi . close ( ) "}
{"10403": "\ndef Counter32 ( a , b , delta ) : \n    if a > b : \n        c = 4294967295 - a \n        return ( c + b ) / float ( delta ) \n    return ( b - a ) / float ( delta ) "}
{"10404": "\ndef Counter64 ( a , b , delta ) : \n    if a > b : \n        c = 18446744073709551615 - a \n        return ( c + b ) / float ( delta ) \n    return ( b - a ) / float ( delta ) "}
{"10409": "\ndef sourceWatchdog ( self ) : \n    for i , source in enumerate ( self . sources ) : \n        if not source . config . get ( 'watchdog' , False ) : \n            continue \n        sn = repr ( source ) \n        last = self . lastEvents . get ( source , None ) \n        if last : \n            try : \n                if ( time . time ( ) - ( source . inter * 10 ) ) > last : \n                    log . msg ( \"Trying to restart stale source %s: %ss\" % ( sn , int ( time . time ( ) - last ) ) ) \n                    s = self . sources . pop ( i ) \n                    try : \n                        s . t . stop ( ) \n                    except Exception as e : \n                        log . msg ( \"Could not stop timer for %s: %s\" % ( sn , e ) ) \n                    config = copy . deepcopy ( s . config ) \n                    del self . lastEvents [ source ] \n                    del s , source \n                    source = self . createSource ( config ) \n                    reactor . callLater ( 0 , self . _startSource , source ) \n            except Exception as e : \n                log . msg ( \"Could not reset source %s: %s\" % ( sn , e ) ) "}
{"10412": "\ndef validate_expires_at ( form , field ) : \n    if form . accept . data : \n        if not field . data or field . data <= datetime . utcnow ( ) . date ( ) : \n            raise validators . StopValidation ( _ ( \"Please provide a future date.\" ) ) \n        if not field . data or field . data > datetime . utcnow ( ) . date ( ) + timedelta ( days = 365 ) : \n            raise validators . StopValidation ( _ ( \"Please provide a date no more than 1 year into the future.\" ) ) "}
{"10434": "\ndef verify ( cls , timestamp : int , message_hash : SHA512Hash , signature : bytes , ) -> bool : \n    if 1496176860 > timestamp : \n        verifier = cls . _VERIFIER_20130905 \n    elif 1502202360 > timestamp : \n        verifier = None \n    else : \n        verifier = cls . _VERIFIER_20170808 \n    if verifier : \n        result = verifier . verify ( message_hash , signature , ) \n    else : \n        result = False \n    if isinstance ( result , int ) : \n        result = True if result == 1 else False \n    return result "}
{"10435": "\ndef is_embargoed ( record ) : \n    return record . get ( 'access_right' ) == 'embargoed' and record . get ( 'embargo_date' ) and datetime . utcnow ( ) . date ( ) < record . get ( 'embargo_date' ) "}
{"10450": "\ndef emptyQueue ( self ) : \n    if self . events : \n        if self . queueDepth and ( self . queueDepth < len ( self . events ) ) : \n            events = self . events [ : self . queueDepth ] \n            self . events = self . events [ self . queueDepth : ] \n        else : \n            events = self . events \n            self . events = [ ] \n        if self . allow_nan : \n            self . factory . proto . sendEvents ( events ) \n        else : \n            self . factory . proto . sendEvents ( [ e for e in events if e . metric is not None ] ) "}
{"10451": "\ndef eventsReceived ( self , events ) : \n    if ( 1 > self . maxsize ) or ( self . maxsize > len ( self . events ) ) : \n        self . events . extend ( events ) "}
{"10462": "\ndef log_time ( self ) : \n    if self . hot_loop and self . log_interval <= self . time_delta : \n        return True \n    return False "}
{"10465": "\ndef _saslprep_do_mapping ( chars ) : \n    i = 0 \n    while len ( chars ) > i : \n        c = chars [ i ] \n        if stringprep . in_table_c12 ( c ) : \n            chars [ i ] = \"\\u0020\" \n        elif stringprep . in_table_b1 ( c ) : \n            del chars [ i ] \n            continue \n        i += 1 "}
{"10466": "\ndef admin_footer ( parser , token ) : \n    tag_name = token . split_contents ( ) \n    if 1 < len ( tag_name ) : \n        raise base . TemplateSyntaxError ( '{} tag does not accept any argument(s): {}' . format ( token . contents . split ( ) [ 0 ] , ', ' . join ( token . contents . split ( ) [ 1 : ] ) ) ) \n    return AdminFooterNode ( ) "}
{"10469": "\ndef pay_with_alias ( amount : Money , alias_registration_id : str , client_ref : str ) -> Payment : \n    if 0 >= amount . amount : \n        raise ValueError ( 'Pay with alias takes a strictly positive amount' ) \n    alias_registration = AliasRegistration . objects . get ( pk = alias_registration_id ) \n    logger . info ( 'paying-with-alias' , amount = amount , client_ref = client_ref , alias_registration = alias_registration ) \n    request_xml = build_pay_with_alias_request_xml ( amount , client_ref , alias_registration ) \n    logger . info ( 'sending-pay-with-alias-request' , url = datatrans_authorize_url , data = request_xml ) \n    response = requests . post ( url = datatrans_authorize_url , headers = { 'Content-Type' : 'application/xml' } , data = request_xml ) \n    logger . info ( 'processing-pay-with-alias-response' , response = response . content ) \n    charge_response = parse_pay_with_alias_response_xml ( response . content ) \n    charge_response . save ( ) \n    charge_response . send_signal ( ) \n    return charge_response "}
{"10476": "\ndef _onNavigate ( self , index ) : \n    if 0 < index : \n        self . setLocation ( self . _locationWidget . itemData ( index ) , interactive = True ) "}
{"10483": "\ndef call ( args , stdout = None , stderr = None , stdin = None , daemonize = False , preexec_fn = None , shell = False , cwd = None , env = None ) : \n    stream = lambda s , m : s is None and os . open ( os . devnull , m ) or s \n    stdout = stream ( stdout , os . O_WRONLY ) \n    stderr = stream ( stderr , os . O_WRONLY ) \n    stdin = stream ( stdin , os . O_RDONLY ) \n    shared_pid = Value ( 'i' , 0 ) \n    pid = os . fork ( ) \n    if 0 < pid : \n        os . waitpid ( pid , 0 ) \n        child_pid = shared_pid . value \n        del shared_pid \n        if daemonize : \n            sys . exit ( 0 ) \n        return child_pid \n    else : \n        os . setsid ( ) \n        proc = subprocess . Popen ( args , stdout = stdout , stderr = stderr , stdin = stdin , close_fds = True , preexec_fn = preexec_fn , shell = shell , cwd = cwd , env = env ) \n        shared_pid . value = proc . pid \n        os . _exit ( 0 ) "}
{"10491": "\ndef include_ ( parser , token ) : \n    bits = token . split_contents ( ) \n    dynamic = False \n    if 2 <= len ( bits ) : \n        dynamic = '{{' in bits [ 1 ] \n        if dynamic : \n            fallback = None \n            bits_new = [ ] \n            for bit in bits : \n                if fallback is True : \n                    fallback = bit \n                    continue \n                if bit == 'fallback' : \n                    fallback = True \n                else : \n                    bits_new . append ( bit ) \n            if fallback : \n                fallback = parser . compile_filter ( construct_relative_path_ ( parser , fallback ) ) \n            token . contents = ' ' . join ( bits_new ) \n    token . contents = token . contents . replace ( 'include_' , 'include' ) \n    include_node = do_include ( parser , token ) \n    if dynamic : \n        include_node = DynamicIncludeNode ( include_node . template , extra_context = include_node . extra_context , isolated_context = include_node . isolated_context , fallback = fallback or None , ) \n    return include_node "}
{"10495": "\ndef is_valid_s3_url ( url ) : \n    if url . startswith ( 'source:' ) : \n        return True \n    scheme , netloc , path , _ , _ , _ = urlparse ( url ) \n    port_except = RemotePortValidationError ( 'Port value %s is not a valid s3 location' % url ) \n    if 2 > len ( scheme ) : \n        raise port_except \n    if 's3' in scheme or 's3' in netloc or 's3' in path : \n        return True \n    else : \n        raise port_except "}
{"10503": "\ndef restore ( folder ) : \n    if os . path . isdir ( folder ) : \n        bail ( 'a folder of the same name already exists!' ) \n    pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) \n    matches = glob . glob ( pattern ) \n    if not matches : \n        bail ( 'no project matches: ' + folder ) \n    if 1 < len ( matches ) : \n        print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) \n    source = sorted ( matches ) [ - 1 ] \n    print ( source , '-->' , folder ) \n    shutil . move ( source , '.' ) "}
{"10511": "\ndef delete ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = False ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] in self . __BROWSABLE_TYPES : \n        contents = self . api_client . list_folder_content ( entity [ 'uuid' ] ) \n        if 0 < contents [ 'count' ] : \n            raise StorageArgumentException ( 'This method cannot delete non-empty folder. Please empty the folder first.' ) \n        self . api_client . delete_folder ( entity [ 'uuid' ] ) \n    elif entity [ 'entity_type' ] == 'file' : \n        self . api_client . delete_file ( entity [ 'uuid' ] ) "}
{"10537": "\ndef map_job ( job , func , inputs , * args ) : \n    num_partitions = 100 \n    partition_size = len ( inputs ) / num_partitions \n    if 1 < partition_size : \n        for partition in partitions ( inputs , partition_size ) : \n            job . addChildJobFn ( map_job , func , partition , * args ) \n    else : \n        for sample in inputs : \n            job . addChildJobFn ( func , sample , * args ) "}
{"10573": "\ndef sync ( self ) : \n    logging . debug ( \"Logger: Syncing...\" ) \n    failed = False \n    try : \n        cdb = self . connectordb \n        cdb . ping ( ) \n        with self . synclock : \n            c = self . database . cursor ( ) \n            for stream in self . streams : \n                s = cdb [ stream ] \n                c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) \n                datapointArray = [ ] \n                for dp in c . fetchall ( ) : \n                    datapointArray . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2 ] ) } ) \n                if 0 < len ( s ) : \n                    newtime = s [ - 1 ] [ \"t\" ] \n                    while ( 0 < len ( datapointArray ) and newtime > datapointArray [ 0 ] [ \"t\" ] ) : \n                        logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) \n                        datapointArray = datapointArray [ 1 : ] \n                if 0 < len ( datapointArray ) : \n                    logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapointArray ) ) ) \n                    while ( DATAPOINT_INSERT_LIMIT < len ( datapointArray ) ) : \n                        s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) \n                        datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] \n                        c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapointArray [ 0 ] [ \"t\" ] ) ) \n                    s . insert_array ( datapointArray ) \n                    c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapointArray [ - 1 ] [ \"t\" ] ) ) \n            self . lastsynctime = time . time ( ) \n            if self . onsync is not None : \n                self . onsync ( ) \n    except Exception as e : \n        falied = True \n        reraise = self . syncraise \n        if self . onsyncfail is not None : \n            reraise = self . onsyncfail ( e ) \n        if reraise : \n            raise "}
{"10582": "\ndef run_star ( job , r1_id , r2_id , star_index_url , wiggle = False , sort = True ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = star_index_url , name = 'starIndex.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'starIndex.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'starIndex.tar.gz' ) ) \n    star_index = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if len ( os . listdir ( work_dir ) ) == 1 else '/data' \n    parameters = [ '--runThreadN' , str ( job . cores ) , '--genomeDir' , star_index , '--outFileNamePrefix' , 'rna' , '--outSAMunmapped' , 'Within' , '--quantMode' , 'TranscriptomeSAM' , '--outSAMattributes' , 'NH' , 'HI' , 'AS' , 'NM' , 'MD' , '--outFilterType' , 'BySJout' , '--outFilterMultimapNmax' , '20' , '--outFilterMismatchNmax' , '999' , '--outFilterMismatchNoverReadLmax' , '0.04' , '--alignIntronMin' , '20' , '--alignIntronMax' , '1000000' , '--alignMatesGapMax' , '1000000' , '--alignSJoverhangMin' , '8' , '--alignSJDBoverhangMin' , '1' , '--sjdbScore' , '1' , '--limitBAMsortRAM' , '49268954168' ] \n    if sort : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'SortedByCoordinate' ] ) \n        aligned_bam = 'rnaAligned.sortedByCoord.out.bam' \n    else : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'Unsorted' ] ) \n        aligned_bam = 'rnaAligned.out.bam' \n    if wiggle : \n        parameters . extend ( [ '--outWigType' , 'bedGraph' , '--outWigStrand' , 'Unstranded' , '--outWigReferencesPrefix' , 'chr' ] ) \n    if r1_id and r2_id : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        job . fileStore . readGlobalFile ( r2_id , os . path . join ( work_dir , 'R2.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' , '/data/R2.fastq' ] ) \n    else : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' ] ) \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/star:2.4.2a--bcbd5122b69ff6ac4ef61958e47bde94001cfe80' , workDir = work_dir , parameters = parameters ) \n    aligned_bam_path = os . path . join ( work_dir , aligned_bam ) \n    if sort : \n        assert ( 0 < os . stat ( aligned_bam_path ) . st_size , 'Aligned bam failed to sort. Ensure sufficient memory is free.' ) \n    aligned_id = job . fileStore . writeGlobalFile ( aligned_bam_path ) \n    transcriptome_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaAligned.toTranscriptome.out.bam' ) ) \n    log_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaLog.final.out' ) ) \n    sj_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSJ.out.tab' ) ) \n    if wiggle : \n        wiggle_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSignal.UniqueMultiple.str1.out.bg' ) ) \n        return transcriptome_id , aligned_id , wiggle_id , log_id , sj_id \n    else : \n        return transcriptome_id , aligned_id , log_id , sj_id "}
{"10596": "\ndef handleresult ( self , r ) : \n    if 400 <= r . status_code and 500 > r . status_code : \n        msg = r . json ( ) \n        raise AuthenticationError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n    elif 300 < r . status_code : \n        err = None \n        try : \n            msg = r . json ( ) \n            err = ServerError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n        except : \n            raise ServerError ( \"Server returned error, but did not give a valid error message\" ) \n        raise err \n    return r "}
{"10618": "\ndef __reconnect ( self ) : \n    self . status = \"reconnecting\" \n    if 15 * 60 < self . disconnected_time - self . connected_time : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    else : \n        self . reconnect_time *= self . reconnect_time_backoff_multiplier \n    if self . reconnect_time_max_seconds < self . reconnect_time : \n        self . reconnect_time = self . reconnect_time_max_seconds \n    self . reconnect_time *= 1 + random . uniform ( - 0.2 , 0.2 ) \n    if self . reconnect_time_starting_seconds > self . reconnect_time : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    logging . warn ( \"ConnectorDB:WS: Attempting to reconnect in %fs\" , self . reconnect_time ) \n    self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) \n    self . reconnector . daemon = True \n    self . reconnector . start ( ) "}
{"10624": "\ndef __ensure_ping ( self ) : \n    logging . debug ( \"ConnectorDB:WS: pingcheck\" ) \n    if ( self . connection_ping_timeout < time . time ( ) - self . lastpingtime ) : \n        logging . warn ( \"ConnectorDB:WS: Websocket ping timed out!\" ) \n        if self . ws is not None : \n            self . ws . close ( ) \n            self . __on_close ( self . ws ) \n    else : \n        self . pingtimer = threading . Timer ( self . connection_ping_timeout , self . __ensure_ping ) \n        self . pingtimer . daemon = True \n        self . pingtimer . start ( ) "}
{"10645": "\ndef start ( self , job ) : \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-worker:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"\\\"SPARK_MASTER_IP=\" + self . masterIP + \":\" + _SPARK_MASTER_PORT + \"\\\"\" , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . masterIP + \":\" + _SPARK_MASTER_PORT ] ) [ : - 1 ] \n    self . __start_datanode ( job ) \n    hdfs_down = True \n    retries = 0 \n    while hdfs_down and ( 5 > retries ) : \n        _log . info ( \"Sleeping 30 seconds before checking HDFS startup.\" ) \n        time . sleep ( 30 ) \n        clusterID = \"\" \n        try : \n            clusterID = subprocess . check_output ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"grep\" , \"clusterID\" , \"-R\" , \"/opt/apache-hadoop/logs\" ] ) \n        except : \n            pass \n        if \"Incompatible\" in clusterID : \n            _log . warning ( \"Hadoop Datanode failed to start with: %s\" , clusterID ) \n            _log . warning ( \"Retrying container startup, retry #%d.\" , retries ) \n            retries += 1 \n            _log . warning ( \"Removing ephemeral hdfs directory.\" ) \n            subprocess . check_call ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"rm\" , \"-rf\" , \"/ephemeral/hdfs\" ] ) \n            _log . warning ( \"Killing container %s.\" , self . hdfsContainerID ) \n            subprocess . check_call ( [ \"docker\" , \"kill\" , self . hdfsContainerID ] ) \n            _log . info ( \"Restarting datanode.\" ) \n            self . __start_datanode ( job ) \n        else : \n            _log . info ( \"HDFS datanode started up OK!\" ) \n            hdfs_down = False \n    if 5 <= retries : \n        raise RuntimeError ( \"Failed %d times trying to start HDFS datanode.\" % retries ) \n    return "}
{"10653": "\ndef validate_changeset ( changeset ) : \n    errors = [ ] \n    changes = changeset . findall ( './/{%s}Change' % R53_XMLNS ) \n    num_changes = len ( changes ) \n    if num_changes == 0 : \n        errors . append ( 'changeset must have at least one <Change> element' ) \n    if 100 < num_changes : \n        errors . append ( 'changeset has %d <Change> elements: max is 100' % num_changes ) \n    rrs = changeset . findall ( './/{%s}ResourceRecord' % R53_XMLNS ) \n    num_rrs = len ( rrs ) \n    if 1000 < num_rrs : \n        errors . append ( 'changeset has %d ResourceRecord elements: max is 1000' % num_rrs ) \n    values = changeset . findall ( './/{%s}Value' % R53_XMLNS ) \n    num_chars = 0 \n    for value in values : \n        num_chars += len ( value . text ) \n    if 10000 < num_chars : \n        errors . append ( 'changeset has %d chars in <Value> text: max is 10000' % num_chars ) \n    return errors "}
{"10655": "\ndef fitness ( self ) : \n    if len ( self . __members ) != 0 : \n        if 1 < self . __num_processes : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return sum ( m . fitness_score for m in members ) / len ( members ) \n    else : \n        return None "}
{"10656": "\ndef ave_cost_fn_val ( self ) : \n    if len ( self . __members ) != 0 : \n        if 1 < self . __num_processes : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return sum ( m . cost_fn_val for m in members ) / len ( members ) \n    else : \n        return None "}
{"10657": "\ndef med_cost_fn_val ( self ) : \n    if len ( self . __members ) != 0 : \n        if 1 < self . __num_processes : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return median ( [ m . cost_fn_val for m in members ] ) \n    else : \n        return None "}
{"10658": "\ndef parameters ( self ) : \n    if len ( self . __members ) != 0 : \n        if 1 < self . __num_processes : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        params = { } \n        for p in self . __parameters : \n            params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) \n        return params \n    else : \n        return None "}
{"10659": "\ndef members ( self ) : \n    if 1 < self . __num_processes : \n        return [ m . get ( ) for m in self . __members ] \n    else : \n        return self . __members "}
{"10661": "\ndef next_generation ( self , mut_rate = 0 , max_mut_amt = 0 , log_base = 10 ) : \n    if 1 < self . __num_processes : \n        process_pool = Pool ( processes = self . __num_processes ) \n        members = [ m . get ( ) for m in self . __members ] \n    else : \n        members = self . __members \n    if len ( members ) == 0 : \n        raise Exception ( 'Generation 0 not found: use generate_population() first' ) \n    selected_members = self . __select_fn ( members ) \n    reproduction_probs = list ( reversed ( logspace ( 0.0 , 1.0 , num = len ( selected_members ) , base = log_base ) ) ) \n    reproduction_probs = reproduction_probs / sum ( reproduction_probs ) \n    self . __members = [ ] \n    for _ in range ( self . __pop_size ) : \n        parent_1 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        parent_2 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        feed_dict = { } \n        for param in self . __parameters : \n            which_parent = uniform ( 0 , 1 ) \n            if 0.5 > which_parent : \n                feed_dict [ param . name ] = parent_1 . parameters [ param . name ] \n            else : \n                feed_dict [ param . name ] = parent_2 . parameters [ param . name ] \n            feed_dict [ param . name ] = self . __mutate_parameter ( feed_dict [ param . name ] , param , mut_rate , max_mut_amt ) \n        if 1 < self . __num_processes : \n            self . __members . append ( process_pool . apply_async ( self . _start_process , [ self . __cost_fn , feed_dict , self . __cost_fn_args ] ) ) \n        else : \n            self . __members . append ( Member ( feed_dict , self . __cost_fn ( feed_dict , self . __cost_fn_args ) ) ) \n    if 1 < self . __num_processes : \n        process_pool . close ( ) \n        process_pool . join ( ) \n    self . __determine_best_member ( ) "}
{"10677": "\ndef is_single_class ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if 1 > counts [ \"classes\" ] and 1 > counts [ \"modules\" ] : \n        ret = 0 < counts [ \"tests\" ] \n    else : \n        ret = 1 >= counts [ \"classes\" ] and 1 >= counts [ \"modules\" ] \n    return ret "}
{"10678": "\ndef is_single_module ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if counts [ \"modules\" ] == 1 : \n        ret = True \n    elif 1 > counts [ \"modules\" ] : \n        ret = is_single_class ( ) \n    return ret "}
{"10685": "\ndef get_remote_addr ( self , forwarded_for ) : \n    if self . num_proxies <= len ( forwarded_for ) : \n        return forwarded_for [ - 1 * self . num_proxies ] "}
{"10694": "\ndef get_annotated_lines ( self ) : \n    lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] \n    if hasattr ( self . code , 'co_firstlineno' ) : \n        lineno = self . code . co_firstlineno - 1 \n        while 0 < lineno : \n            if _funcdef_re . match ( lines [ lineno ] . code ) : \n                break \n            lineno -= 1 \n        try : \n            offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) \n        except TokenError : \n            offset = 0 \n        for line in lines [ lineno : lineno + offset ] : \n            line . in_frame = True \n    try : \n        lines [ self . lineno - 1 ] . current = True \n    except IndexError : \n        pass \n    return lines "}
{"10699": "\ndef find_requirement ( self , req , upgrade ) : \n    all_versions = self . _find_all_versions ( req . name ) \n    _versions = set ( req . specifier . filter ( [ x . version for x in all_versions ] , prereleases = ( self . allow_all_prereleases if self . allow_all_prereleases else None ) , ) ) \n    applicable_versions = [ x for x in all_versions if x . version in _versions ] \n    if req . satisfied_by is not None : \n        applicable_versions . insert ( 0 , InstallationCandidate ( req . name , req . satisfied_by . version , INSTALLED_VERSION , ) ) \n        existing_applicable = True \n    else : \n        existing_applicable = False \n    applicable_versions = self . _sort_versions ( applicable_versions ) \n    if not upgrade and existing_applicable : \n        if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n            logger . debug ( 'Existing installed version (%s) is most up-to-date and ' 'satisfies requirement' , req . satisfied_by . version , ) \n        else : \n            logger . debug ( 'Existing installed version (%s) satisfies requirement ' '(most up-to-date version is %s)' , req . satisfied_by . version , applicable_versions [ 0 ] [ 2 ] , ) \n        return None \n    if not applicable_versions : \n        logger . critical ( 'Could not find a version that satisfies the requirement %s ' '(from versions: %s)' , req , ', ' . join ( sorted ( set ( str ( i . version ) for i in all_versions ) , key = parse_version , ) ) ) \n        if self . need_warn_external : \n            logger . warning ( \"Some externally hosted files were ignored as access to \" \"them may be unreliable (use --allow-external %s to \" \"allow).\" , req . name , ) \n        if self . need_warn_unverified : \n            logger . warning ( \"Some insecure and unverifiable files were ignored\" \" (use --allow-unverified %s to allow).\" , req . name , ) \n        raise DistributionNotFound ( 'No matching distribution found for %s' % req ) \n    if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n        logger . debug ( 'Installed version (%s) is most up-to-date (past versions: ' '%s)' , req . satisfied_by . version , ', ' . join ( str ( i . version ) for i in applicable_versions [ 1 : ] ) or \"none\" , ) \n        raise BestVersionAlreadyInstalled \n    if 1 < len ( applicable_versions ) : \n        logger . debug ( 'Using version %s (newest of versions: %s)' , applicable_versions [ 0 ] . version , ', ' . join ( str ( i . version ) for i in applicable_versions ) ) \n    selected_version = applicable_versions [ 0 ] . location \n    if ( selected_version . verifiable is not None and not selected_version . verifiable ) : \n        logger . warning ( \"%s is potentially insecure and unverifiable.\" , req . name , ) \n    return selected_version "}
{"10702": "\ndef links ( self ) : \n    for anchor in self . parsed . findall ( \".//a\" ) : \n        if anchor . get ( \"href\" ) : \n            href = anchor . get ( \"href\" ) \n            url = self . clean_link ( urllib_parse . urljoin ( self . base_url , href ) ) \n            internal = None \n            if self . api_version and 2 <= self . api_version : \n                internal = bool ( anchor . get ( \"rel\" ) and \"internal\" in anchor . get ( \"rel\" ) . split ( ) ) \n            yield Link ( url , self , internal = internal ) "}
{"10703": "\ndef verifiable ( self ) : \n    trusted = self . trusted or getattr ( self . comes_from , \"trusted\" , None ) \n    if trusted is not None and trusted : \n        try : \n            api_version = getattr ( self . comes_from , \"api_version\" , None ) \n            api_version = int ( api_version ) \n        except ( ValueError , TypeError ) : \n            api_version = None \n        if api_version is None or 1 >= api_version : \n            return \n        if self . hash : \n            return True \n        else : \n            return False \n    elif trusted is not None : \n        return False "}
{"10715": "\ndef pop ( self , exc = None ) : \n    self . _refcnt -= 1 \n    if 0 >= self . _refcnt : \n        if exc is None : \n            exc = sys . exc_info ( ) [ 1 ] \n        self . app . do_teardown_appcontext ( exc ) \n    rv = _app_ctx_stack . pop ( ) \n    assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) \n    appcontext_popped . send ( self . app ) "}
{"10735": "\ndef check_compatibility ( version , name ) : \n    if not version : \n        raise UnsupportedWheel ( \"%s is in an unsupported or invalid wheel\" % name ) \n    if VERSION_COMPATIBLE [ 0 ] < version [ 0 ] : \n        raise UnsupportedWheel ( \"%s's Wheel-Version (%s) is not compatible with this version \" \"of pip\" % ( name , '.' . join ( map ( str , version ) ) ) ) \n    elif VERSION_COMPATIBLE < version : \n        logger . warning ( 'Installing from a newer Wheel-Version (%s)' , '.' . join ( map ( str , version ) ) , ) "}
{"10738": "\ndef ensure_fresh_rates ( func ) : \n    def wrapper ( self , * args , ** kwargs ) : \n        if zulu . now ( ) > self . last_updated + timedelta ( minutes = 5 ) : \n            self . refresh ( ) \n        return func ( self , * args , ** kwargs ) \n    return wrapper "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and cc [ 'max-age' ] == 0 : \n        no_cache = True \n    if no_cache : \n        return False \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return False \n    if resp . status == 301 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return False \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age += min_fresh \n    fresh = ( current_age < freshness_lifetime ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return False "}
{"10746": "\ndef cache_response ( self , request , response , body = None ) : \n    if response . status not in [ 200 , 203 , 300 , 301 ] : \n        return \n    response_headers = CaseInsensitiveDict ( response . headers ) \n    cc_req = self . parse_cache_control ( request . headers ) \n    cc = self . parse_cache_control ( response_headers ) \n    cache_url = self . cache_url ( request . url ) \n    no_store = cc . get ( 'no-store' ) or cc_req . get ( 'no-store' ) \n    if no_store and self . cache . get ( cache_url ) : \n        self . cache . delete ( cache_url ) \n    if self . cache_etags and 'etag' in response_headers : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n    elif response . status == 301 : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response ) ) \n    elif 'date' in response_headers : \n        if cc and cc . get ( 'max-age' ) : \n            if 0 < int ( cc [ 'max-age' ] ) : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n        elif 'expires' in response_headers : \n            if response_headers [ 'expires' ] : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) "}
{"10756": "\ndef _execfile ( filename , globals , locals = None ) : \n    mode = 'rb' \n    with open ( filename , mode ) as stream : \n        script = stream . read ( ) \n    if ( 2 , 7 ) > sys . version_info [ : 2 ] or ( 3 , 0 ) <= sys . version_info [ : 2 ] and ( 3 , 2 ) > sys . version_info [ : 2 ] : \n        script = script . replace ( b'\\r\\n' , b'\\n' ) \n        script = script . replace ( b'\\r' , b'\\n' ) \n    if locals is None : \n        locals = globals \n    code = compile ( script , filename , 'exec' ) \n    exec ( code , globals , locals ) "}
{"10761": "\ndef compile_templates ( self , target , extensions = None , filter_func = None , zip = 'deflated' , log_function = None , ignore_errors = True , py_compile = False ) : \n    from jinja2 . loaders import ModuleLoader \n    if log_function is None : \n        log_function = lambda x : None \n    if py_compile : \n        if not PY2 or PYPY : \n            from warnings import warn \n            warn ( Warning ( 'py_compile has no effect on pypy or Python 3' ) ) \n            py_compile = False \n        else : \n            import imp , marshal \n            py_header = imp . get_magic ( ) + u'\\xff\\xff\\xff\\xff' . encode ( 'iso-8859-15' ) \n            if ( 3 , 3 ) <= sys . version_info : \n                py_header += u'\\x00\\x00\\x00\\x00' . encode ( 'iso-8859-15' ) \n    def write_file ( filename , data , mode ) : \n        if zip : \n            info = ZipInfo ( filename ) \n            info . external_attr = 0o755 << 16 \n            zip_file . writestr ( info , data ) \n        else : \n            f = open ( os . path . join ( target , filename ) , mode ) \n            try : \n                f . write ( data ) \n            finally : \n                f . close ( ) \n    if zip is not None : \n        from zipfile import ZipFile , ZipInfo , ZIP_DEFLATED , ZIP_STORED \n        zip_file = ZipFile ( target , 'w' , dict ( deflated = ZIP_DEFLATED , stored = ZIP_STORED ) [ zip ] ) \n        log_function ( 'Compiling into Zip archive \"%s\"' % target ) \n    else : \n        if not os . path . isdir ( target ) : \n            os . makedirs ( target ) \n        log_function ( 'Compiling into folder \"%s\"' % target ) \n    try : \n        for name in self . list_templates ( extensions , filter_func ) : \n            source , filename , _ = self . loader . get_source ( self , name ) \n            try : \n                code = self . compile ( source , name , filename , True , True ) \n            except TemplateSyntaxError as e : \n                if not ignore_errors : \n                    raise \n                log_function ( 'Could not compile \"%s\": %s' % ( name , e ) ) \n                continue \n            filename = ModuleLoader . get_module_filename ( name ) \n            if py_compile : \n                c = self . _compile ( code , encode_filename ( filename ) ) \n                write_file ( filename + 'c' , py_header + marshal . dumps ( c ) , 'wb' ) \n                log_function ( 'Byte-compiled \"%s\" as %s' % ( name , filename + 'c' ) ) \n            else : \n                write_file ( filename , code , 'w' ) \n                log_function ( 'Compiled \"%s\" as %s' % ( name , filename ) ) \n    finally : \n        if zip : \n            zip_file . close ( ) \n    log_function ( 'Finished compiling templates' ) "}
{"10778": "\ndef get_decimal_precision ( number ) : \n    assert isinstance ( number , decimal . Decimal ) \n    decimal_tuple = number . normalize ( ) . as_tuple ( ) \n    if 0 <= decimal_tuple . exponent : \n        return 0 \n    return abs ( decimal_tuple . exponent ) "}
{"10779": "\ndef scientific_notation_elements ( self , value , locale ) : \n    exp = value . adjusted ( ) \n    value = value * get_decimal_quantum ( exp ) \n    assert value . adjusted ( ) == 0 \n    lead_shift = max ( [ 1 , min ( self . int_prec ) ] ) - 1 \n    exp = exp - lead_shift \n    value = value * get_decimal_quantum ( - lead_shift ) \n    exp_sign = '' \n    if 0 > exp : \n        exp_sign = babel . numbers . get_minus_sign_symbol ( locale ) \n    elif self . exp_plus : \n        exp_sign = babel . numbers . get_plus_sign_symbol ( locale ) \n    exp = abs ( exp ) \n    return value , exp , exp_sign "}
{"10785": "\ndef do_dice_roll ( ) : \n    options = get_options ( ) \n    dice = Dice ( options . sides ) \n    rolls = [ dice . roll ( ) for n in range ( options . number ) ] \n    for roll in rolls : \n        print ( 'rolled' , roll ) \n    if 1 < options . number : \n        print ( 'total' , sum ( rolls ) ) "}
{"10894": "\ndef force_global_eggs_after_local_site_packages ( ) : \n    egginsert = getattr ( sys , '__egginsert' , 0 ) \n    for i , path in enumerate ( sys . path ) : \n        if egginsert < i and path . startswith ( sys . prefix ) : \n            egginsert = i \n    sys . __egginsert = egginsert + 1 "}
{"10925": "\ndef lookup ( self , ResponseGroup = \"Large\" , ** kwargs ) : \n    response = self . api . ItemLookup ( ResponseGroup = ResponseGroup , ** kwargs ) \n    root = objectify . fromstring ( response ) \n    if root . Items . Request . IsValid == 'False' : \n        code = root . Items . Request . Errors . Error . Code \n        msg = root . Items . Request . Errors . Error . Message \n        raise LookupException ( \"Amazon Product Lookup Error: '{0}', '{1}'\" . format ( code , msg ) ) \n    if not hasattr ( root . Items , 'Item' ) : \n        raise AsinNotFound ( \"ASIN(s) not found: '{0}'\" . format ( etree . tostring ( root , pretty_print = True ) ) ) \n    if 1 < len ( root . Items . Item ) : \n        return [ AmazonProduct ( item , self . aws_associate_tag , self , region = self . region ) for item in root . Items . Item ] \n    else : \n        return AmazonProduct ( root . Items . Item , self . aws_associate_tag , self , region = self . region ) "}
{"10984": "\ndef is_in ( self , point_x , point_y ) : \n    x = self . x_origin \n    y = self . y_origin \n    a = self . e_width \n    b = self . e_height \n    return 1.0 > ( ( point_x - x ) ** 2 / ( a ** 2 ) ) + ( ( point_y - y ) ** 2 / ( b ** 2 ) ) "}
{"11001": "\ndef _request_graph ( self , parent = None ) : \n    if ( 1 < len ( self . all_graphs ) ) and ( self . select_graph ) : \n        retval = self . edit_traits ( parent = parent , view = \"all_graphs_view\" ) \n        if not retval . result : \n            return None \n    if self . selected_graph is not None : \n        return self . selected_graph \n    else : \n        return self . model "}
{"11016": "\ndef generate_sentence ( self , chain ) : \n    def weighted_choice ( choices ) : \n        total_weight = sum ( weight for val , weight in choices ) \n        rand = random . uniform ( 0 , total_weight ) \n        upto = 0 \n        for val , weight in choices : \n            if rand <= upto + weight : \n                return val \n            upto += weight \n    sentence = list ( random . choice ( chain . startwords ) ) \n    while not sentence [ - 1 ] [ - 1 ] in [ '.' , '?' , '!' ] : \n        sentence . append ( weighted_choice ( chain . content [ tuple ( sentence [ - 2 : ] ) ] . items ( ) ) ) \n    return ' ' . join ( sentence ) "}
{"11030": "\ndef get_time_units_and_multiplier ( seconds ) : \n    for cutoff , units , multiplier in units_table : \n        if cutoff > seconds : \n            break \n    return units , multiplier "}
{"11067": "\ndef _draw_mainlayer ( self , gc , view_bounds = None , mode = \"default\" ) : \n    gc . save_state ( ) \n    try : \n        if 2 <= len ( self . points ) : \n            gc . set_fill_color ( self . pen . fill_color_ ) \n            gc . set_stroke_color ( self . pen . color_ ) \n            gc . set_line_width ( self . pen . line_width ) \n            gc . begin_path ( ) \n            gc . lines ( self . points ) \n            gc . close_path ( ) \n            if self . filled : \n                gc . draw_path ( self . inside_rule_ ) \n            else : \n                gc . stroke_path ( ) \n    finally : \n        gc . restore_state ( ) "}
{"11077": "\ndef rewrite_url ( input_url , ** kwargs ) : \n    scheme , netloc , path , query , fragment = parse . urlsplit ( input_url ) \n    if 'scheme' in kwargs : \n        scheme = kwargs [ 'scheme' ] \n    ident , host_n_port = parse . splituser ( netloc ) \n    user , password = parse . splitpasswd ( ident ) if ident else ( None , None ) \n    if 'user' in kwargs : \n        user = kwargs [ 'user' ] \n    elif user is not None : \n        user = parse . unquote_to_bytes ( user ) . decode ( 'utf-8' ) \n    if 'password' in kwargs : \n        password = kwargs [ 'password' ] \n    elif password is not None : \n        password = parse . unquote_to_bytes ( password ) . decode ( 'utf-8' ) \n    ident = _create_url_identifier ( user , password ) \n    host , port = parse . splitnport ( host_n_port , defport = None ) \n    if 'host' in kwargs : \n        host = kwargs [ 'host' ] \n        if host is not None : \n            host = _normalize_host ( host , enable_long_host = kwargs . get ( 'enable_long_host' , False ) , encode_with_idna = kwargs . get ( 'encode_with_idna' , None ) , scheme = scheme , ) \n    if 'port' in kwargs : \n        port = kwargs [ 'port' ] \n        if port is not None : \n            port = int ( kwargs [ 'port' ] ) \n            if 0 > port : \n                raise ValueError ( 'port is required to be non-negative' ) \n    if host is None or host == '' : \n        host_n_port = None \n    elif port is None : \n        host_n_port = host \n    else : \n        host_n_port = '{0}:{1}' . format ( host , port ) \n    if 'path' in kwargs : \n        path = kwargs [ 'path' ] \n        if path is None : \n            path = '/' \n        else : \n            path = parse . quote ( path . encode ( 'utf-8' ) , safe = PATH_SAFE_CHARS ) \n    netloc = '{0}@{1}' . format ( ident , host_n_port ) if ident else host_n_port \n    if 'query' in kwargs : \n        new_query = kwargs [ 'query' ] \n        if new_query is None : \n            query = None \n        else : \n            params = [ ] \n            try : \n                for param in sorted ( new_query . keys ( ) ) : \n                    params . append ( ( param , new_query [ param ] ) ) \n            except AttributeError : \n                pass \n            if not params : \n                try : \n                    params = [ ( param , value ) for param , value in new_query ] \n                except ValueError : \n                    pass \n            if params : \n                query = parse . urlencode ( params ) \n            else : \n                query = new_query \n    if 'fragment' in kwargs : \n        fragment = kwargs [ 'fragment' ] \n        if fragment is not None : \n            fragment = parse . quote ( fragment . encode ( 'utf-8' ) , safe = FRAGMENT_SAFE_CHARS ) \n    if scheme is None : \n        scheme = '' \n    return parse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) "}
{"11080": "\ndef _normalize_host ( host , enable_long_host = False , encode_with_idna = None , scheme = None ) : \n    if encode_with_idna is not None : \n        enable_idna = encode_with_idna \n    else : \n        enable_idna = scheme . lower ( ) in IDNA_SCHEMES if scheme else False \n    if enable_idna : \n        try : \n            host = '.' . join ( segment . encode ( 'idna' ) . decode ( ) for segment in host . split ( '.' ) ) \n        except UnicodeError as exc : \n            raise ValueError ( 'host is invalid - {0}' . format ( exc ) ) \n    else : \n        host = parse . quote ( host . encode ( 'utf-8' ) , safe = HOST_SAFE_CHARS ) \n    if 255 < len ( host ) and not enable_long_host : \n        raise ValueError ( 'host too long' ) \n    return host "}
{"11088": "\ndef luhn_check ( card_number ) : \n    sum = 0 \n    num_digits = len ( card_number ) \n    oddeven = num_digits & 1 \n    for count in range ( 0 , num_digits ) : \n        digit = int ( card_number [ count ] ) \n        if not ( ( count & 1 ) ^ oddeven ) : \n            digit *= 2 \n        if 9 < digit : \n            digit -= 9 \n        sum += digit \n    return ( sum % 10 ) == 0 "}
{"11093": "\ndef split_line ( line , min_line_length = 30 , max_line_length = 100 ) : \n    if max_line_length >= len ( line ) : \n        return [ line ] \n    indent = 0 \n    while line [ indent ] == ' ' and len ( line ) > indent : \n        indent += 1 \n    i = max_line_length \n    split_point = None \n    while min_line_length < i : \n        if line [ i ] == ' ' : \n            split_point = i \n            break \n        i -= 1 \n    if split_point is None : \n        i = max_line_length + 1 \n        while len ( line ) > i : \n            if line [ i ] == ' ' : \n                split_point = i \n                break \n            i += 1 \n    if split_point is None : \n        return [ line ] \n    else : \n        line1 = line [ : split_point ] \n        line2 = ' ' * indent + line [ split_point + 1 : ] \n        return [ line1 ] + split_line ( line2 , min_line_length , max_line_length ) "}
{"11094": "\ndef remove_namespaces ( root ) : \n    for elem in root . getiterator ( ) : \n        if not hasattr ( elem . tag , 'find' ) : \n            continue \n        i = elem . tag . find ( '}' ) \n        if 0 <= i : \n            elem . tag = elem . tag [ i + 1 : ] \n    objectify . deannotate ( root , cleanup_namespaces = True ) "}
{"11101": "\ndef emit ( self , record ) : \n    try : \n        now = timetool . unix_time ( ) \n        one_minute_ago = now - 60 \n        new_rate_limiter = [ x for x in self . rate_limiter if one_minute_ago < x ] \n        log . debug ( 'Rate limiter %s -> %s' % ( len ( self . rate_limiter ) , len ( new_rate_limiter ) ) ) \n        self . rate_limiter = new_rate_limiter \n        recent_sends = len ( self . rate_limiter ) \n        send_email = self . max_sends_per_minute > recent_sends \n        if send_email : \n            self . rate_limiter . append ( now ) \n        msg = self . format ( record ) \n        msg = self . add_details ( msg ) \n        if send_email : \n            if DEBUG_ERROR_EMAIL_SENDING : \n                log . info ( '@@@> ! Sending error email to {} !' . format ( self . toaddrs ) ) \n            send_text_mail ( self . toaddrs , self . subject , msg , self . fromaddr ) \n        else : \n            log . info ( '!! WARNING: Not sending email as too many emails have been sent in the past minute !!' ) \n            log . info ( msg ) \n    except ( KeyboardInterrupt , SystemExit ) : \n        raise \n    except Exception : \n        self . handleError ( record ) "}
{"11103": "\ndef log_attempt ( self , key ) : \n    with self . lock : \n        if key not in self . attempts : \n            self . attempts [ key ] = 1 \n        else : \n            self . attempts [ key ] += 1 \n            if self . max_attempts <= self . attempts [ key ] : \n                log . info ( 'Account %s locked due to too many login attempts' % key ) \n                self . locks [ key ] = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = self . lock_duration ) "}
{"11112": "\ndef add_months ( months , timestamp = datetime . datetime . utcnow ( ) ) : \n    month = timestamp . month \n    new_month = month + months \n    years = 0 \n    while 1 > new_month : \n        new_month += 12 \n        years -= 1 \n    while 12 < new_month : \n        new_month -= 12 \n        years += 1 \n    year = timestamp . year + years \n    try : \n        return datetime . datetime ( year , new_month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) \n    except ValueError : \n        if 0 < months : \n            new_month += 1 \n            if 12 < new_month : \n                new_month -= 12 \n                year += 1 \n            return datetime . datetime ( year , new_month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day , timestamp . hour , timestamp . minute , timestamp . second ) "}
{"11113": "\ndef add_months_to_date ( months , date ) : \n    month = date . month \n    new_month = month + months \n    years = 0 \n    while 1 > new_month : \n        new_month += 12 \n        years -= 1 \n    while 12 < new_month : \n        new_month -= 12 \n        years += 1 \n    year = date . year + years \n    try : \n        return datetime . date ( year , new_month , date . day ) \n    except ValueError : \n        if 0 < months : \n            new_month += 1 \n            if 12 < new_month : \n                new_month -= 12 \n                year += 1 \n            return datetime . datetime ( year , new_month , 1 ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day ) "}
{"11114": "\ndef is_christmas_period ( ) : \n    now = datetime . date . today ( ) \n    if now . month != 12 : \n        return False \n    if 15 > now . day : \n        return False \n    if 27 < now . day : \n        return False \n    return True "}
{"11132": "\ndef reasonable_desired_version ( self , desired_version , allow_equal = False , allow_patch_skip = False ) : \n    try : \n        desired_version = desired_version . base_version \n    except : \n        pass \n    ( new_major , new_minor , new_patch ) = map ( int , desired_version . split ( '.' ) ) \n    tag_versions = self . _versions_from_tags ( ) \n    if not tag_versions : \n        return \"\" \n    max_version = max ( self . _versions_from_tags ( ) ) . base_version \n    ( old_major , old_minor , old_patch ) = map ( int , str ( max_version ) . split ( '.' ) ) \n    update_str = str ( max_version ) + \" -> \" + str ( desired_version ) \n    v_desired = vers . Version ( desired_version ) \n    v_max = vers . Version ( max_version ) \n    if allow_equal and v_desired == v_max : \n        return \"\" \n    if v_max > v_desired : \n        return ( \"Bad update: New version doesn't increase on last tag: \" + update_str + \"\\n\" ) \n    bad_update = skipped_version ( ( old_major , old_minor , old_patch ) , ( new_major , new_minor , new_patch ) , allow_patch_skip ) \n    msg = \"\" \n    if bad_update : \n        msg = ( \"Bad update: Did you skip a version from \" + update_str + \"?\\n\" ) \n    return msg "}
{"11136": "\ndef parse_accept ( header_value ) : \n    next_explicit_q = decimal . ExtendedContext . next_plus ( decimal . Decimal ( '5.0' ) ) \n    headers = [ parse_content_type ( header ) for header in parse_list ( header_value ) ] \n    for header in headers : \n        q = header . parameters . pop ( 'q' , None ) \n        if q is None : \n            q = '1.0' \n        elif float ( q ) == 1.0 : \n            q = float ( next_explicit_q ) \n            next_explicit_q = next_explicit_q . next_minus ( ) \n        header . quality = float ( q ) \n    def ordering ( left , right ) : \n        if left . quality != right . quality : \n            return right . quality - left . quality \n        if left == right : \n            return 0 \n        if right < left : \n            return - 1 \n        return 1 \n    return sorted ( headers , key = functools . cmp_to_key ( ordering ) ) "}
{"11167": "\ndef pause ( self , signum , seconds = 0 , callback_function = None ) : \n    if callback_function is None : \n        callback_function = self . default_handler \n    if 0 < seconds : \n        self . log . info ( \"Signal handler pausing for {0} seconds or until it receives SIGALRM or SIGCONT\" . format ( seconds ) ) \n        signal . signal ( signal . SIGALRM , callback_function ) \n        signal . alarm ( seconds ) \n    else : \n        self . log . info ( 'Signal handler pausing until it receives SIGALRM or SIGCONT' ) \n    signal . signal ( signal . SIGCONT , callback_function ) \n    signal . pause ( ) \n    self . log . info ( 'Signal handler resuming from pause' ) \n    if signum == signal . SIGALRM : \n        return True \n    else : \n        return False "}
{"11184": "\ndef get_duration ( self , seconds ) : \n    duration = \"\" \n    minutes , seconds = divmod ( seconds , 60 ) \n    if 60 <= minutes : \n        hours , minutes = divmod ( minutes , 60 ) \n        duration = \"%sh \" % hours \n    duration += \"%sm %ss\" % ( minutes , seconds ) \n    return duration "}
{"11189": "\ndef data_processing ( self ) : \n    the_file_name = str ( self . result_file ) \n    the_file = open ( the_file_name , 'r' ) \n    lines = the_file . readlines ( ) \n    lines_array = [ ] \n    for line in lines : \n        line = line . split ( ',' ) \n        lines_array . append ( line ) \n    labels_line = lines_array [ 0 ] \n    cell_labels_line = 0 \n    flag = True \n    try : \n        while flag : \n            if \"wave length (nm)\" in labels_line [ cell_labels_line ] : \n                index = labels_line . index ( labels_line [ cell_labels_line ] ) \n                flag = False \n            else : \n                cell_labels_line += 1 \n    except IndexError : \n        raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) \n    self . information = [ ] \n    data_wavelength = [ ] \n    self . num_line = 0 \n    for line in lines_array : \n        cell_line = 0 \n        self . information . append ( [ ] ) \n        data_wavelength . append ( [ ] ) \n        while len ( line ) > cell_line : \n            if index > cell_line : \n                self . information [ self . num_line ] . append ( line [ cell_line ] ) \n            elif index < cell_line : \n                data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) \n            cell_line += 1 \n        self . num_line += 1 \n    line_wavelength = 0 \n    for row_data_wavelength in data_wavelength : \n        row_data_wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row_data_wavelength ] \n        data_wavelength [ line_wavelength ] = row_data_wavelength \n        line_wavelength += 1 \n    self . wavelength = data_wavelength [ 0 ] \n    self . data_wanted = data_wavelength [ 1 : ] \n    the_file . close ( ) "}
{"11191": "\ndef print_graphic_information ( self , num_curve , information ) : \n    label_information = information [ 0 ] \n    data_information = information [ 1 : ] \n    count_nb_label = 0 \n    nb_label = len ( label_information ) \n    while nb_label >= count_nb_label : \n        self . ui . column1_label . setText ( label_information [ 0 ] . strip ( '\\\"' ) ) \n        self . ui . column2_label . setText ( label_information [ 1 ] . strip ( '\\\"' ) ) \n        self . ui . column3_label . setText ( label_information [ 2 ] . strip ( '\\\"' ) ) \n        self . ui . column4_label . setText ( label_information [ 3 ] . strip ( '\\\"' ) ) \n        self . ui . column5_label . setText ( label_information [ 4 ] . strip ( '\\\"' ) ) \n        self . ui . column6_label . setText ( label_information [ 5 ] . strip ( '\\\"' ) ) \n        self . ui . column7_label . setText ( label_information [ 6 ] . strip ( '\\\"' ) ) \n        self . ui . column8_label . setText ( label_information [ 7 ] . strip ( '\\\"' ) ) \n        count_nb_label += 1 \n    line_of_data = 0 \n    while len ( data_information ) > line_of_data : \n        if line_of_data == num_curve : \n            self . ui . column1_result . setText ( data_information [ line_of_data ] [ 0 ] ) \n            self . ui . column2_result . setText ( data_information [ line_of_data ] [ 1 ] ) \n            self . ui . column3_result . setText ( data_information [ line_of_data ] [ 2 ] ) \n            self . ui . column4_result . setText ( data_information [ line_of_data ] [ 3 ] ) \n            self . ui . column5_result . setText ( data_information [ line_of_data ] [ 4 ] ) \n            self . ui . column6_result . setText ( data_information [ line_of_data ] [ 5 ] ) \n            self . ui . column7_result . setText ( data_information [ line_of_data ] [ 6 ] ) \n            self . ui . column8_result . setText ( data_information [ line_of_data ] [ 7 ] ) \n        line_of_data += 1 "}
{"11209": "\ndef last_lock ( self ) : \n    rs = list ( self . rounds ) \n    assert 2 > len ( rs ) or rs [ 1 ] < rs [ 0 ] \n    for r in self . rounds : \n        if self . rounds [ r ] . lock is not None : \n            return self . rounds [ r ] . lock "}
{"11213": "\ndef on_proposal ( self , proposal , proto ) : \n    assert isinstance ( proto , HDCProtocol ) \n    assert isinstance ( proposal , Proposal ) \n    if self . cm . height <= proposal . height : \n        assert proposal . lockset . is_valid \n        self . last_active_protocol = proto "}
{"11214": "\ndef mk_privkeys ( num ) : \n    privkeys = [ ] \n    assert num_colors >= num \n    for i in range ( num ) : \n        j = 0 \n        while True : \n            k = sha3 ( str ( j ) ) \n            a = privtoaddr ( k ) \n            an = big_endian_to_int ( a ) \n            if an % num_colors == i : \n                break \n            j += 1 \n        privkeys . append ( k ) \n    return privkeys "}
{"11220": "\ndef update ( self , data ) : \n    if data not in self . filter : \n        self . filter . append ( data ) \n        if self . max_items < len ( self . filter ) : \n            self . filter . pop ( 0 ) \n        return True \n    else : \n        self . filter . append ( self . filter . pop ( 0 ) ) \n        return False "}
{"11228": "\ndef finish ( self ) : \n    if self . finished : \n        return self . exit_code \n    checkpoint_status = self . checkpoint ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != 0 : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    if 0 < self . applied_count : \n        self . _end_acquisition ( ) \n        self . _apply_rows ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != 0 : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    self . finished = True \n    return self . exit_code "}
{"11237": "\ndef do_table ( self , line ) : \n    if 0 < len ( line ) : \n        if line . strip ( ) . lower ( ) == \"on\" : \n            log . write ( \"Table ON\" ) \n            self . table_output = True \n            return \n        elif line . strip ( ) . lower ( ) == \"off\" : \n            log . write ( \"Table OFF\" ) \n            self . table_output = False \n            return \n    log . write ( \"Table output: {}\" . format ( \"ON\" if self . table_output else \"OFF\" ) ) "}
{"11238": "\ndef execute ( self , command , coerce_floats = True , parse_dates = False , header = False , sanitize = True , silent = False , panic = None , multi_statement = False , prepare_only = False ) : \n    if panic is None : \n        panic = self . panic \n    self . options ( \"panic\" , panic ) \n    self . options ( \"multi-statement mode\" , multi_statement , 3 ) \n    if isfile ( command ) : \n        self . options ( \"file\" , command , 2 ) \n        with open ( command , 'r' ) as f : \n            command = f . read ( ) \n    else : \n        if VERBOSE <= log . level : \n            self . options ( \"query\" , command , 2 ) \n        else : \n            self . options ( \"query\" , truncate ( command ) , 2 ) \n    if not silent and not self . silent : \n        log . info ( \"Command\" , \"Executing ...\" ) \n        log . info ( self . options ) \n    if sanitize : \n        command = prepare_statement ( command ) \n        log . debug ( \"Debug[2]\" , \"Command (sanitized): {!r}\" . format ( command ) ) \n    self . cmd . set_encoding ( ENCODER_SETTINGS_DEFAULT ) \n    return Cursor ( self . cmd , command , multi_statement = multi_statement , header = header , prepare_only = prepare_only , coerce_floats = coerce_floats , parse_dates = parse_dates , panic = panic ) "}
{"11250": "\ndef result ( self , psd_state ) : \n    freq_array = numpy . fft . fftshift ( psd_state [ 'freq_array' ] ) \n    pwr_array = numpy . fft . fftshift ( psd_state [ 'pwr_array' ] ) \n    if self . _crop_factor : \n        crop_bins_half = round ( ( self . _crop_factor * self . _bins ) / 2 ) \n        freq_array = freq_array [ crop_bins_half : - crop_bins_half ] \n        pwr_array = pwr_array [ crop_bins_half : - crop_bins_half ] \n    if 1 < psd_state [ 'repeats' ] : \n        pwr_array = pwr_array / psd_state [ 'repeats' ] \n    if self . _log_scale : \n        pwr_array = 10 * numpy . log10 ( pwr_array ) \n    return ( freq_array , pwr_array ) "}
{"11251": "\ndef wait_for_result ( self , psd_state ) : \n    if 1 < len ( psd_state [ 'futures' ] ) : \n        concurrent . futures . wait ( psd_state [ 'futures' ] ) \n    elif psd_state [ 'futures' ] : \n        psd_state [ 'futures' ] [ 0 ] . result ( ) \n    return self . result ( psd_state ) "}
{"11255": "\ndef submit ( self , fn , * args , ** kwargs ) : \n    future = super ( ) . submit ( fn , * args , ** kwargs ) \n    work_queue_size = self . _work_queue . qsize ( ) \n    if self . max_queue_size_reached < work_queue_size : \n        self . max_queue_size_reached = work_queue_size \n    return future "}
{"11257": "\ndef freq_plan ( self , min_freq , max_freq , bins , overlap = 0 , quiet = False ) : \n    bin_size = self . bins_to_bin_size ( bins ) \n    bins_crop = round ( ( 1 - overlap ) * bins ) \n    sample_rate_crop = ( 1 - overlap ) * self . device . sample_rate \n    freq_range = max_freq - min_freq \n    hopping = True if sample_rate_crop <= freq_range else False \n    hop_size = self . nearest_freq ( sample_rate_crop , bin_size ) \n    hops = math . ceil ( freq_range / hop_size ) if hopping else 1 \n    min_center_freq = min_freq + ( hop_size / 2 ) if hopping else min_freq + ( freq_range / 2 ) \n    max_center_freq = min_center_freq + ( ( hops - 1 ) * hop_size ) \n    freq_list = [ min_center_freq + ( i * hop_size ) for i in range ( hops ) ] \n    if not quiet : \n        logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) \n        logger . info ( 'bin_size: {:.2f} Hz' . format ( bin_size ) ) \n        logger . info ( 'bins: {}' . format ( bins ) ) \n        logger . info ( 'bins (after crop): {}' . format ( bins_crop ) ) \n        logger . info ( 'sample_rate: {:.3f} MHz' . format ( self . device . sample_rate / 1e6 ) ) \n        logger . info ( 'sample_rate (after crop): {:.3f} MHz' . format ( sample_rate_crop / 1e6 ) ) \n        logger . info ( 'freq_range: {:.3f} MHz' . format ( freq_range / 1e6 ) ) \n        logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) \n        logger . info ( 'hop_size: {:.3f} MHz' . format ( hop_size / 1e6 ) ) \n        logger . info ( 'hops: {}' . format ( hops ) ) \n        logger . info ( 'min_center_freq: {:.3f} MHz' . format ( min_center_freq / 1e6 ) ) \n        logger . info ( 'max_center_freq: {:.3f} MHz' . format ( max_center_freq / 1e6 ) ) \n        logger . info ( 'min_freq (after crop): {:.3f} MHz' . format ( ( min_center_freq - ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . info ( 'max_freq (after crop): {:.3f} MHz' . format ( ( max_center_freq + ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . debug ( 'Frequency hops table:' ) \n        logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) \n        for f in freq_list : \n            logger . debug ( '  {:8.3f} MHz  {:8.3f} MHz  {:8.3f} MHz' . format ( ( f - ( self . device . sample_rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample_rate / 2 ) ) / 1e6 , ) ) \n    return freq_list "}
{"11258": "\ndef create_buffer ( self , bins , repeats , base_buffer_size , max_buffer_size = 0 ) : \n    samples = bins * repeats \n    buffer_repeats = 1 \n    buffer_size = math . ceil ( samples / base_buffer_size ) * base_buffer_size \n    if not max_buffer_size : \n        max_buffer_size = ( 100 * 1024 ** 2 ) / 8 \n    if 0 < max_buffer_size : \n        max_buffer_size = math . ceil ( max_buffer_size / base_buffer_size ) * base_buffer_size \n        if max_buffer_size < buffer_size : \n            logger . warning ( 'Required buffer size ({}) will be shrinked to max_buffer_size ({})!' . format ( buffer_size , max_buffer_size ) ) \n            buffer_repeats = math . ceil ( buffer_size / max_buffer_size ) \n            buffer_size = max_buffer_size \n    logger . info ( 'repeats: {}' . format ( repeats ) ) \n    logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample_rate ) ) \n    if 0 < max_buffer_size : \n        logger . info ( 'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max_buffer_size , max_buffer_size / bins , max_buffer_size / self . device . sample_rate ) ) \n    else : \n        logger . info ( 'max_buffer_size (samples): UNLIMITED' ) \n    logger . info ( 'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer_size , buffer_size / bins , buffer_size / self . device . sample_rate ) ) \n    logger . info ( 'buffer_repeats: {}' . format ( buffer_repeats ) ) \n    return ( buffer_repeats , zeros ( buffer_size , numpy . complex64 ) ) "}
{"11261": "\ndef psd ( self , freq ) : \n    if not self . device . is_streaming : \n        raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) \n    logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) \n    t_freq = time . time ( ) \n    if self . device . freq != freq : \n        if self . _reset_stream : \n            self . device . device . deactivateStream ( self . device . stream ) \n        self . device . freq = freq \n        if self . _reset_stream : \n            self . device . device . activateStream ( self . device . stream ) \n        if self . _tune_delay : \n            t_delay = time . time ( ) \n            while True : \n                self . device . read_stream ( ) \n                t_delay_end = time . time ( ) \n                if self . _tune_delay <= t_delay_end - t_delay : \n                    break \n            logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) \n    else : \n        logger . debug ( '    Same frequency as before, tuning skipped' ) \n    psd_state = self . _psd . set_center_freq ( freq ) \n    t_freq_end = time . time ( ) \n    logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) \n    for repeat in range ( self . _buffer_repeats ) : \n        logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) \n        t_acq = time . time ( ) \n        acq_time_start = datetime . datetime . utcnow ( ) \n        self . device . read_stream_into_buffer ( self . _buffer ) \n        acq_time_stop = datetime . datetime . utcnow ( ) \n        t_acq_end = time . time ( ) \n        logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) \n        self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) \n        t_final = time . time ( ) \n        if _shutdown : \n            break \n    psd_future = self . _psd . result_async ( psd_state ) \n    logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) \n    return ( psd_future , acq_time_start , acq_time_stop ) "}
{"11262": "\ndef sweep ( self , min_freq , max_freq , bins , repeats , runs = 0 , time_limit = 0 , overlap = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop = False , log_scale = True , remove_dc = False , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = False , base_buffer_size = 0 , max_buffer_size = 0 , max_threads = 0 , max_queue_size = 0 ) : \n    self . setup ( bins , repeats , base_buffer_size , max_buffer_size , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = overlap if crop else 0 , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , tune_delay = tune_delay , reset_stream = reset_stream , max_threads = max_threads , max_queue_size = max_queue_size ) \n    try : \n        freq_list = self . freq_plan ( min_freq - lnb_lo , max_freq - lnb_lo , bins , overlap ) \n        t_start = time . time ( ) \n        run = 0 \n        while not _shutdown and ( runs == 0 or runs > run ) : \n            run += 1 \n            t_run_start = time . time ( ) \n            logger . debug ( 'Run: {}' . format ( run ) ) \n            for freq in freq_list : \n                psd_future , acq_time_start , acq_time_stop = self . psd ( freq ) \n                self . _writer . write_async ( psd_future , acq_time_start , acq_time_stop , len ( self . _buffer ) * self . _buffer_repeats ) \n                if _shutdown : \n                    break \n            write_next_future = self . _writer . write_next_async ( ) \n            t_run = time . time ( ) \n            logger . debug ( '  Total run time: {:.3f} s' . format ( t_run - t_run_start ) ) \n            if time_limit and time_limit <= ( time . time ( ) - t_start ) : \n                logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time_limit , run ) ) \n                break \n        write_next_future . result ( ) \n        logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer_overflow_count ) ) \n        logging . debug ( 'PSD worker threads: {}' . format ( self . _psd . _executor . _max_workers ) ) \n        logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . _psd . _executor . max_queue_size_reached , self . _psd . _executor . max_queue_size ) ) \n        logging . debug ( 'Writer worker threads: {}' . format ( self . _writer . _executor . _max_workers ) ) \n        logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . _writer . _executor . max_queue_size_reached , self . _writer . _executor . max_queue_size ) ) \n    finally : \n        self . stop ( ) \n        t_stop = time . time ( ) \n        logger . info ( 'Total time: {:.3f} s' . format ( t_stop - t_start ) ) "}
{"11265": "\ndef filter ( cls , datetimes , number , now = None , ** options ) : \n    if not isinstance ( number , int ) or 0 > number : \n        raise ValueError ( 'Invalid number: %s' % number ) \n    datetimes = tuple ( datetimes ) \n    tzinfo = None \n    if datetimes and datetimes [ 0 ] . tzinfo is not None : \n        tzinfo = UTC ( ) \n    if now is None : \n        now = datetime . now ( tzinfo ) \n    if not hasattr ( now , 'second' ) : \n        now = datetime . combine ( now , time ( 23 , 59 , 59 , 999999 , tzinfo = tzinfo ) ) \n    future = set ( dt for dt in datetimes if now < dt ) \n    if number == 0 : \n        return future \n    start = cls . start ( now , number , ** options ) \n    valid = ( dt for dt in datetimes if start <= dt <= now ) \n    kept = { } \n    for dt in sorted ( valid ) : \n        kept . setdefault ( cls . mask ( dt , ** options ) , dt ) \n    return set ( kept . values ( ) ) | future "}
{"11275": "\ndef get_bit_num ( bit_pattern ) : \n    if bit_pattern == 0 : \n        return None \n    bit_num = 0 \n    while ( bit_pattern & 1 ) == 0 : \n        bit_pattern = bit_pattern >> 1 \n        bit_num += 1 \n        if 7 < bit_num : \n            bit_num = 0 \n            break \n    return bit_num "}
{"11279": "\ndef set_gpio_interrupt_edge ( edge = 'falling' ) : \n    start_time = time . time ( ) \n    time_limit = start_time + FILE_IO_TIMEOUT \n    while time_limit > time . time ( ) : \n        try : \n            with open ( GPIO_INTERRUPT_DEVICE_EDGE , 'w' ) as gpio_edge : \n                gpio_edge . write ( edge ) \n                return \n        except IOError : \n            pass "}
{"11280": "\ndef wait_until_file_exists ( filename ) : \n    start_time = time . time ( ) \n    time_limit = start_time + FILE_IO_TIMEOUT \n    while time_limit > time . time ( ) : \n        try : \n            with open ( filename ) : \n                return \n        except IOError : \n            pass \n    raise Timeout ( \"Waiting too long for %s.\" % filename ) "}
{"11290": "\ndef _check_status ( cls , response_json ) : \n    status = response_json [ 'status' ] \n    msg = response_json [ 'msg' ] \n    if status == 400 : \n        raise BadRequestException ( msg ) \n    elif status == 403 : \n        raise PermissionDeniedException ( msg ) \n    elif status == 404 : \n        raise FileNotFoundException ( msg ) \n    elif status == 451 : \n        raise UnavailableForLegalReasonsException ( msg ) \n    elif status == 509 : \n        raise BandwidthUsageExceeded ( msg ) \n    elif 500 <= status : \n        raise ServerErrorException ( msg ) "}
{"11308": "\ndef _dmpaft_cmd ( self , time_fields ) : \n    records = [ ] \n    tbuf = struct . pack ( '2H' , * time_fields ) \n    self . _cmd ( 'DMPAFT' ) \n    crc = VProCRC . get ( tbuf ) \n    crc = struct . pack ( '>H' , crc ) \n    log_raw ( 'send' , tbuf + crc ) \n    self . port . write ( tbuf + crc ) \n    ack = self . port . read ( len ( self . ACK ) ) \n    log_raw ( 'read' , ack ) \n    if ack != self . ACK : \n        return \n    raw = self . port . read ( DmpStruct . size ) \n    log_raw ( 'read' , raw ) \n    if not VProCRC . verify ( raw ) : \n        log_raw ( 'send ESC' , self . ESC ) \n        self . port . write ( self . ESC ) \n        return \n    log_raw ( 'send ACK' , self . ACK ) \n    self . port . write ( self . ACK ) \n    dmp = DmpStruct . unpack ( raw ) \n    log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) \n    for i in xrange ( dmp [ 'Pages' ] ) : \n        raw = self . port . read ( DmpPageStruct . size ) \n        log_raw ( 'read' , raw ) \n        if not VProCRC . verify ( raw ) : \n            log_raw ( 'send ESC' , self . ESC ) \n            self . port . write ( self . ESC ) \n            return \n        log_raw ( 'send ACK' , self . ACK ) \n        self . port . write ( self . ACK ) \n        page = DmpPageStruct . unpack ( raw ) \n        offset = 0 \n        if i == 0 : \n            offset = dmp [ 'Offset' ] * ArchiveAStruct . size \n        while ArchiveAStruct . size * 5 > offset : \n            log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) \n            if self . _use_rev_b_archive ( page [ 'Records' ] , offset ) : \n                a = ArchiveBStruct . unpack_from ( page [ 'Records' ] , offset ) \n            else : \n                a = ArchiveAStruct . unpack_from ( page [ 'Records' ] , offset ) \n            if a [ 'DateStamp' ] != 0xffff and a [ 'TimeStamp' ] != 0xffff : \n                records . append ( a ) \n            offset += ArchiveAStruct . size \n    log . info ( 'read all pages' ) \n    return records "}
{"11309": "\ndef _get_new_archive_fields ( self ) : \n    for i in xrange ( 3 ) : \n        records = self . _dmpaft_cmd ( self . _archive_time ) \n        if records is not None : \n            break \n        time . sleep ( 1 ) \n    if records is None : \n        raise NoDeviceException ( 'Can not access weather station' ) \n    new_rec = None \n    for r in records : \n        new_time = ( r [ 'DateStamp' ] , r [ 'TimeStamp' ] ) \n        if new_time > self . _archive_time : \n            self . _archive_time = new_time \n            new_rec = r \n    return new_rec "}
{"11311": "\ndef weather_update ( station , pub_sites , interval ) : \n    station . parse ( ) \n    if 200 < station . fields [ 'TempOut' ] : \n        raise NoSensorException ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'TempOut' ] , ) ) \n    gust , gust_dir = WindGust . get ( station , interval ) \n    for ps in pub_sites : \n        try : \n            ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'DewPoint' ] , humidity = station . fields [ 'HumOut' ] , tempf = station . fields [ 'TempOut' ] , rainin = station . fields [ 'RainRate' ] , rainday = station . fields [ 'RainDay' ] , dateutc = station . fields [ 'DateStampUtc' ] , windspeed = station . fields [ 'WindSpeed10Min' ] , winddir = station . fields [ 'WindDir' ] , windgust = gust , windgustdir = gust_dir , ) \n            ps . publish ( ) \n        except ( Exception ) as e : \n            log . warn ( 'publisher %s: %s' % ( ps . __class__ . __name__ , e ) ) "}
{"11314": "\ndef get ( self , station , interval ) : \n    rec = station . fields [ 'Archive' ] \n    if rec : \n        threshold = station . fields [ 'WindSpeed10Min' ] + GUST_MPH_MIN \n        if threshold <= rec [ 'WindHi' ] : \n            self . value = ( rec [ 'WindHi' ] , rec [ 'WindHiDir' ] ) \n            self . count = GUST_TTL * 60 / interval \n        else : \n            self . value = self . NO_VALUE \n    if self . count : \n        self . count -= 1 \n    else : \n        self . value = self . NO_VALUE \n    log . debug ( 'wind gust of {0} mph from {1}' . format ( * self . value ) ) \n    return self . value "}
{"11346": "\ndef should_update ( self ) : \n    last_update = self . get_last_update ( ) \n    time_since = now ( ) - last_update . last_update \n    if self . update_interval > time_since . seconds : \n        return False \n    return True "}
{"11347": "\ndef array ( a , context = None , axis = ( 0 , ) , dtype = None , npartitions = None ) : \n    if dtype is None : \n        arry = asarray ( a ) \n        dtype = arry . dtype \n    else : \n        arry = asarray ( a , dtype ) \n    shape = arry . shape \n    ndim = len ( shape ) \n    axes = ConstructSpark . _format_axes ( axis , arry . shape ) \n    key_axes , value_axes = get_kv_axes ( arry . shape , axes ) \n    permutation = key_axes + value_axes \n    arry = arry . transpose ( * permutation ) \n    split = len ( axes ) \n    if 1 > split : \n        raise ValueError ( \"split axis must be greater than 0, got %g\" % split ) \n    if len ( shape ) < split : \n        raise ValueError ( \"split axis must not exceed number of axes %g, got %g\" % ( ndim , split ) ) \n    key_shape = shape [ : split ] \n    val_shape = shape [ split : ] \n    keys = zip ( * unravel_index ( arange ( 0 , int ( prod ( key_shape ) ) ) , key_shape ) ) \n    vals = arry . reshape ( ( prod ( key_shape ) , ) + val_shape ) \n    rdd = context . parallelize ( zip ( keys , vals ) , npartitions ) \n    return BoltArraySpark ( rdd , shape = shape , split = split , dtype = dtype ) "}
{"11351": "\ndef _format_axes ( axes , shape ) : \n    if isinstance ( axes , int ) : \n        axes = ( axes , ) \n    elif isinstance ( axes , list ) or hasattr ( axes , '__iter__' ) : \n        axes = tuple ( axes ) \n    if not isinstance ( axes , tuple ) : \n        raise ValueError ( \"axes argument %s in the constructor not specified correctly\" % str ( axes ) ) \n    if 0 > min ( axes ) or len ( shape ) - 1 < max ( axes ) : \n        raise ValueError ( \"invalid key axes %s given shape %s\" % ( str ( axes ) , str ( shape ) ) ) \n    return axes "}
{"11358": "\ndef _chunk ( self , size = \"150\" , axis = None , padding = None ) : \n    if self . split == len ( self . shape ) and padding is None : \n        self . _rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] + ( 0 , ) , array ( kv [ 1 ] , ndmin = 1 ) ) ) \n        self . _shape = self . _shape + ( 1 , ) \n        self . _plan = ( 1 , ) \n        self . _padding = array ( [ 0 ] ) \n        return self \n    rdd = self . _rdd \n    self . _plan , self . _padding = self . getplan ( size , axis , padding ) \n    if any ( [ z < x + y for x , y , z in zip ( self . plan , self . padding , self . vshape ) ] ) : \n        raise ValueError ( \"Chunk sizes %s plus padding sizes %s cannot exceed value dimensions %s along any axis\" % ( tuple ( self . plan ) , tuple ( self . padding ) , tuple ( self . vshape ) ) ) \n    if any ( [ y < x for x , y in zip ( self . padding , self . plan ) ] ) : \n        raise ValueError ( \"Padding sizes %s cannot exceed chunk sizes %s along any axis\" % ( tuple ( self . padding ) , tuple ( self . plan ) ) ) \n    slices = self . getslices ( self . plan , self . padding , self . vshape ) \n    labels = list ( product ( * [ list ( enumerate ( s ) ) for s in slices ] ) ) \n    scheme = [ list ( zip ( * s ) ) for s in labels ] \n    def _chunk ( record ) : \n        k , v = record [ 0 ] , record [ 1 ] \n        for ( chk , slc ) in scheme : \n            if type ( k ) is int : \n                k = ( k , ) \n            yield k + chk , v [ slc ] \n    rdd = rdd . flatMap ( _chunk ) \n    return self . _constructor ( rdd , shape = self . shape , split = self . split , dtype = self . dtype , plan = self . plan , padding = self . padding , ordered = self . _ordered ) "}
{"11359": "\ndef map ( self , func , value_shape = None , dtype = None ) : \n    if value_shape is None or dtype is None : \n        try : \n            mapped = func ( random . randn ( * self . plan ) . astype ( self . dtype ) ) \n        except Exception : \n            first = self . _rdd . first ( ) \n            if first : \n                mapped = func ( first [ 1 ] ) \n        if value_shape is None : \n            value_shape = mapped . shape \n        if dtype is None : \n            dtype = mapped . dtype \n    chunked_dims = where ( self . plan != self . vshape ) [ 0 ] \n    unchunked_dims = where ( self . plan == self . vshape ) [ 0 ] \n    if len ( value_shape ) != len ( self . plan ) : \n        raise NotImplementedError ( 'map on ChunkedArray cannot drop dimensions' ) \n    if any ( [ value_shape [ i ] != self . plan [ i ] for i in chunked_dims ] ) : \n        raise ValueError ( 'map cannot change the sizes of chunked dimensions' ) \n    def check_and_apply ( v ) : \n        new = func ( v ) \n        if 0 < len ( unchunked_dims ) : \n            if any ( [ new . shape [ i ] != value_shape [ i ] for i in unchunked_dims ] ) : \n                raise Exception ( \"Map operation did not produce values of uniform shape.\" ) \n        if 0 < len ( chunked_dims ) : \n            if any ( [ v . shape [ i ] != new . shape [ i ] for i in chunked_dims ] ) : \n                raise Exception ( \"Map operation changed the size of a chunked dimension\" ) \n        return new \n    rdd = self . _rdd . mapValues ( check_and_apply ) \n    vshape = [ value_shape [ i ] if i in unchunked_dims else self . vshape [ i ] for i in range ( len ( self . vshape ) ) ] \n    newshape = r_ [ self . kshape , vshape ] . astype ( int ) . tolist ( ) \n    return self . _constructor ( rdd , shape = tuple ( newshape ) , dtype = dtype , plan = asarray ( value_shape ) ) . __finalize__ ( self ) "}
{"11361": "\ndef getplan ( self , size = \"150\" , axes = None , padding = None ) : \n    from numpy import dtype as gettype \n    plan = self . vshape \n    if axes is None : \n        if isinstance ( size , str ) : \n            axes = arange ( len ( self . vshape ) ) \n        else : \n            axes = arange ( len ( size ) ) \n    else : \n        axes = asarray ( axes , 'int' ) \n    pad = array ( len ( self . vshape ) * [ 0 , ] ) \n    if padding is not None : \n        pad [ axes ] = padding \n    if isinstance ( size , tuple ) : \n        plan [ axes ] = size \n    elif isinstance ( size , str ) : \n        size = 1000.0 * float ( size ) \n        elsize = gettype ( self . dtype ) . itemsize \n        nelements = prod ( self . vshape ) \n        dims = self . vshape [ self . vmask ( axes ) ] \n        if elsize >= size : \n            s = ones ( len ( axes ) ) \n        else : \n            remsize = 1.0 * nelements * elsize \n            s = [ ] \n            for ( i , d ) in enumerate ( dims ) : \n                minsize = remsize / d \n                if size <= minsize : \n                    s . append ( 1 ) \n                    remsize = minsize \n                    continue \n                else : \n                    s . append ( min ( d , floor ( size / minsize ) ) ) \n                    s [ i + 1 : ] = plan [ i + 1 : ] \n                    break \n        plan [ axes ] = s \n    else : \n        raise ValueError ( \"Chunk size not understood, must be tuple or int\" ) \n    return plan , pad "}
{"11368": "\ndef _align ( self , axis ) : \n    inshape ( self . shape , axis ) \n    tokeys = [ ( a - self . split ) for a in axis if self . split <= a ] \n    tovalues = [ a for a in range ( self . split ) if a not in axis ] \n    if tokeys or tovalues : \n        return self . swap ( tovalues , tokeys ) \n    else : \n        return self "}
{"11379": "\ndef transpose ( self , * axes ) : \n    if len ( axes ) == 0 : \n        p = arange ( self . ndim - 1 , - 1 , - 1 ) \n    else : \n        p = asarray ( argpack ( axes ) ) \n    istransposeable ( p , range ( self . ndim ) ) \n    split = self . split \n    new_keys , new_values = p [ : split ] , p [ split : ] \n    swapping_keys = sort ( new_values [ split > new_values ] ) \n    swapping_values = sort ( new_keys [ split <= new_keys ] ) \n    stationary_keys = sort ( new_keys [ split > new_keys ] ) \n    stationary_values = sort ( new_values [ split <= new_values ] ) \n    p_swap = r_ [ stationary_keys , swapping_values , swapping_keys , stationary_values ] \n    p_swap_inv = argsort ( p_swap ) \n    p_x = p_swap_inv [ p ] \n    p_keys , p_values = p_x [ : split ] , p_x [ split : ] - split \n    arr = self . swap ( swapping_keys , swapping_values - split ) \n    arr = arr . keys . transpose ( tuple ( p_keys . tolist ( ) ) ) \n    arr = arr . values . transpose ( tuple ( p_values . tolist ( ) ) ) \n    return arr "}
{"11383": "\ndef squeeze ( self , axis = None ) : \n    if not any ( [ d == 1 for d in self . shape ] ) : \n        return self \n    if axis is None : \n        drop = where ( asarray ( self . shape ) == 1 ) [ 0 ] \n    elif isinstance ( axis , int ) : \n        drop = asarray ( ( axis , ) ) \n    elif isinstance ( axis , tuple ) : \n        drop = asarray ( axis ) \n    else : \n        raise ValueError ( \"an integer or tuple is required for the axis\" ) \n    if any ( [ 1 < self . shape [ i ] for i in drop ] ) : \n        raise ValueError ( \"cannot select an axis to squeeze out which has size greater than one\" ) \n    if any ( self . split > asarray ( drop ) ) : \n        kmask = set ( [ d for d in drop if self . split > d ] ) \n        kfunc = lambda k : tuple ( [ kk for ii , kk in enumerate ( k ) if ii not in kmask ] ) \n    else : \n        kfunc = lambda k : k \n    if any ( self . split <= asarray ( drop ) ) : \n        vmask = tuple ( [ d - self . split for d in drop if self . split <= d ] ) \n        vfunc = lambda v : v . squeeze ( vmask ) \n    else : \n        vfunc = lambda v : v \n    rdd = self . _rdd . map ( lambda kv : ( kfunc ( kv [ 0 ] ) , vfunc ( kv [ 1 ] ) ) ) \n    shape = tuple ( [ ss for ii , ss in enumerate ( self . shape ) if ii not in drop ] ) \n    split = len ( [ d for d in range ( self . keys . ndim ) if d not in drop ] ) \n    return self . _constructor ( rdd , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11389": "\ndef inshape ( shape , axes ) : \n    valid = all ( [ ( len ( shape ) > axis ) and ( 0 <= axis ) for axis in axes ] ) \n    if not valid : \n        raise ValueError ( \"axes not valid for an ndarray of shape: %s\" % str ( shape ) ) "}
{"11391": "\ndef listify ( lst , dim ) : \n    if not all ( [ l . dtype == int for l in lst ] ) : \n        raise ValueError ( \"indices must be integers\" ) \n    if npany ( dim <= asarray ( lst ) ) : \n        raise ValueError ( \"indices out of bounds for axis with size %s\" % dim ) \n    return lst . flatten ( ) "}
{"11392": "\ndef slicify ( slc , dim ) : \n    if isinstance ( slc , slice ) : \n        start = 0 if slc . start is None else slc . start \n        stop = dim if slc . stop is None else slc . stop \n        step = 1 if slc . step is None else slc . step \n        if 0 > start : \n            start += dim \n        if 0 > stop : \n            stop += dim \n        if 0 < step : \n            if 0 > start : \n                start = 0 \n            if dim < stop : \n                stop = dim \n        else : \n            if 0 > stop : \n                stop = - 1 \n            if dim < start : \n                start = dim - 1 \n        return slice ( start , stop , step ) \n    elif isinstance ( slc , int ) : \n        if 0 > slc : \n            slc += dim \n        return slice ( slc , slc + 1 , 1 ) \n    else : \n        raise ValueError ( \"Type for slice %s not recongized\" % type ( slc ) ) "}
{"11393": "\ndef istransposeable ( new , old ) : \n    new , old = tupleize ( new ) , tupleize ( old ) \n    if not len ( new ) == len ( old ) : \n        raise ValueError ( \"Axes do not match axes of keys\" ) \n    if not len ( set ( new ) ) == len ( set ( old ) ) : \n        raise ValueError ( \"Repeated axes\" ) \n    if any ( 0 > n for n in new ) or len ( old ) - 1 < max ( new ) : \n        raise ValueError ( \"Invalid axes\" ) "}
{"11397": "\ndef zip_with_index ( rdd ) : \n    starts = [ 0 ] \n    if 1 < rdd . getNumPartitions ( ) : \n        nums = rdd . mapPartitions ( lambda it : [ sum ( 1 for _ in it ) ] ) . collect ( ) \n        count = sum ( nums ) \n        for i in range ( len ( nums ) - 1 ) : \n            starts . append ( starts [ - 1 ] + nums [ i ] ) \n    else : \n        count = rdd . count ( ) \n    def func ( k , it ) : \n        for i , v in enumerate ( it , starts [ k ] ) : \n            yield v , i \n    return count , rdd . mapPartitionsWithIndex ( func ) "}
{"11398": "\ndef wrapped ( f ) : \n    import inspect \n    def extract ( func ) : \n        append = \"\" \n        args = inspect . getargspec ( func ) \n        for i , a in enumerate ( args . args ) : \n            if ( len ( args ) - len ( args . defaults ) ) > i : \n                append += str ( a ) + \", \" \n            else : \n                default = args . defaults [ i - len ( args . defaults ) ] \n                if hasattr ( default , \"__name__\" ) : \n                    default = default . __name__ \n                else : \n                    default = str ( default ) \n                append += str ( a ) + \"=\" + default + \", \" \n        append = append [ : - 2 ] + \")\" \n        return append \n    doc = f . __doc__ + \"\\n\" \n    doc += \"    local -> array(\" + extract ( getattr ( ConstructLocal , f . __name__ ) ) + \"\\n\" \n    doc += \"    spark -> array(\" + extract ( getattr ( ConstructSpark , f . __name__ ) ) + \"\\n\" \n    f . __doc__ = doc \n    return f "}
{"11407": "\ndef discrete_likelihood ( data , xmin , alpha ) : \n    if not scipyOK : \n        raise ImportError ( \"Can't import scipy.  Need scipy for zeta function.\" ) \n    from scipy . special import zeta as zeta \n    zz = data [ xmin <= data ] \n    nn = len ( zz ) \n    sum_log_data = np . log ( zz ) . sum ( ) \n    zeta = zeta ( alpha , xmin ) \n    L_of_alpha = - 1 * nn * log ( zeta ) - alpha * sum_log_data \n    return L_of_alpha "}
{"11409": "\ndef discrete_alpha_mle ( data , xmin ) : \n    gexmin = ( xmin <= data ) \n    nn = gexmin . sum ( ) \n    if 2 > nn : \n        return 0 \n    xx = data [ gexmin ] \n    alpha = 1.0 + float ( nn ) * ( sum ( log ( xx / ( float ( xmin ) - 0.5 ) ) ) ) ** - 1 \n    return alpha "}
{"11410": "\ndef discrete_best_alpha ( data , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True ) : \n    xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] \n    best_index = argmin ( ksvalues ) \n    best_alpha = alpha_of_xmin [ best_index ] \n    best_xmin = xmins [ best_index ] \n    best_ks = ksvalues [ best_index ] \n    best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( best_xmin > data ) . sum ( ) , ( best_xmin <= data ) . sum ( ) ) ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11411": "\ndef discrete_best_alpha ( self , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True , finite = True ) : \n    data = self . data \n    self . _xmins = xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = np . array ( [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] ) \n    self . _alpha_values = np . array ( alpha_of_xmin ) \n    self . _xmin_kstest = ksvalues \n    ksvalues [ np . isnan ( ksvalues ) ] = np . inf \n    best_index = argmin ( ksvalues ) \n    self . _alpha = best_alpha = alpha_of_xmin [ best_index ] \n    self . _xmin = best_xmin = xmins [ best_index ] \n    self . _ks = best_ks = ksvalues [ best_index ] \n    self . _likelihood = best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if finite : \n        self . _alpha = self . _alpha * ( n - 1. ) / n + 1. / n \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( best_xmin > data ) . sum ( ) , ( best_xmin <= data ) . sum ( ) ) ) \n    self . _ngtx = n = ( self . _xmin <= self . data ) . sum ( ) \n    self . _alphaerr = ( self . _alpha - 1.0 ) / np . sqrt ( n ) \n    if scipyOK : \n        self . _ks_prob = scipy . stats . ksone . sf ( self . _ks , n ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11412": "\ndef plotppf ( self , x = None , xmin = None , alpha = None , dolog = True , ** kwargs ) : \n    if not ( xmin ) : \n        xmin = self . _xmin \n    if not ( alpha ) : \n        alpha = self . _alpha \n    if not ( x ) : \n        x = np . sort ( self . data [ xmin < self . data ] ) \n    else : \n        x = np . sort ( x [ xmin < x ] ) \n    m0 = min ( x ) \n    N = ( 1.0 + np . arange ( len ( x ) ) ) [ : : - 1 ] \n    xmodel = m0 * N ** ( 1 / ( 1 - alpha ) ) / max ( N ) ** ( 1 / ( 1 - alpha ) ) \n    if dolog : \n        pylab . loglog ( x , xmodel , '.' , ** kwargs ) \n        pylab . gca ( ) . set_xlim ( min ( x ) , max ( x ) ) \n        pylab . gca ( ) . set_ylim ( min ( x ) , max ( x ) ) \n    else : \n        pylab . plot ( x , xmodel , '.' , ** kwargs ) \n    pylab . plot ( [ min ( x ) , max ( x ) ] , [ min ( x ) , max ( x ) ] , 'k--' ) \n    pylab . xlabel ( \"Real Value\" ) \n    pylab . ylabel ( \"Power-Law Model Value\" ) "}
{"11420": "\ndef _parse_long ( cls , data ) : \n    if '3' > sys . version : \n        ret = long ( 0 ) \n        for byte in data : \n            ret = ( ret << 8 ) + ord ( byte ) \n    else : \n        ret = 0 \n        for byte in data : \n            ret = ( ret << 8 ) + byte \n    return ret "}
{"11423": "\ndef _process_ssh_rsa ( self , data ) : \n    current_position , raw_e = self . _unpack_by_int ( data , 0 ) \n    current_position , raw_n = self . _unpack_by_int ( data , current_position ) \n    unpacked_e = self . _parse_long ( raw_e ) \n    unpacked_n = self . _parse_long ( raw_n ) \n    self . rsa = RSAPublicNumbers ( unpacked_e , unpacked_n ) . public_key ( default_backend ( ) ) \n    self . bits = self . rsa . key_size \n    if self . strict_mode : \n        min_length = self . RSA_MIN_LENGTH_STRICT \n        max_length = self . RSA_MAX_LENGTH_STRICT \n    else : \n        min_length = self . RSA_MIN_LENGTH_LOOSE \n        max_length = self . RSA_MAX_LENGTH_LOOSE \n    if min_length > self . bits : \n        raise TooShortKeyError ( \"%s key data can not be shorter than %s bits (was %s)\" % ( self . key_type , min_length , self . bits ) ) \n    if max_length < self . bits : \n        raise TooLongKeyError ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key_type , max_length , self . bits ) ) \n    return current_position "}
{"11424": "\ndef _process_ssh_dss ( self , data ) : \n    data_fields = { } \n    current_position = 0 \n    for item in ( \"p\" , \"q\" , \"g\" , \"y\" ) : \n        current_position , value = self . _unpack_by_int ( data , current_position ) \n        data_fields [ item ] = self . _parse_long ( value ) \n    q_bits = self . _bits_in_number ( data_fields [ \"q\" ] ) \n    p_bits = self . _bits_in_number ( data_fields [ \"p\" ] ) \n    if q_bits != self . DSA_N_LENGTH : \n        raise InvalidKeyError ( \"Incorrect DSA key parameters: bits(p)=%s, q=%s\" % ( self . bits , q_bits ) ) \n    if self . strict_mode : \n        min_length = self . DSA_MIN_LENGTH_STRICT \n        max_length = self . DSA_MAX_LENGTH_STRICT \n    else : \n        min_length = self . DSA_MIN_LENGTH_LOOSE \n        max_length = self . DSA_MAX_LENGTH_LOOSE \n    if min_length > p_bits : \n        raise TooShortKeyError ( \"%s key can not be shorter than %s bits (was %s)\" % ( self . key_type , min_length , p_bits ) ) \n    if max_length < p_bits : \n        raise TooLongKeyError ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key_type , max_length , p_bits ) ) \n    dsa_parameters = DSAParameterNumbers ( data_fields [ \"p\" ] , data_fields [ \"q\" ] , data_fields [ \"g\" ] ) \n    self . dsa = DSAPublicNumbers ( data_fields [ \"y\" ] , dsa_parameters ) . public_key ( default_backend ( ) ) \n    self . bits = self . dsa . key_size \n    return current_position "}
{"11426": "\ndef _process_ed25516 ( self , data ) : \n    current_position , verifying_key = self . _unpack_by_int ( data , 0 ) \n    verifying_key_length = len ( verifying_key ) * 8 \n    verifying_key = self . _parse_long ( verifying_key ) \n    if 0 > verifying_key : \n        raise InvalidKeyError ( \"ed25519 verifying key must be >0.\" ) \n    self . bits = verifying_key_length \n    if self . bits != 256 : \n        raise InvalidKeyLengthError ( \"ed25519 keys must be 256 bits (was %s bits)\" % self . bits ) \n    return current_position "}
{"11485": "\ndef seed ( self , values ) : \n    if not values : \n        seed_ids = [ int , str , random , self , values , self . __class__ ] \n        random . shuffle ( seed_ids ) \n        values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] \n    mash = Mash ( ) \n    self . c = 1 \n    self . s0 = mash ( ' ' ) \n    self . s1 = mash ( ' ' ) \n    self . s2 = mash ( ' ' ) \n    for val in values : \n        self . s0 -= mash ( val ) \n        if 0 > self . s0 : \n            self . s0 += 1 \n        self . s1 -= mash ( val ) \n        if 0 > self . s1 : \n            self . s1 += 1 \n        self . s2 -= mash ( val ) \n        if 0 > self . s2 : \n            self . s2 += 1 "}
{"11507": "\ndef print ( self , msg , * args , ** kwargs ) : \n    if 1 <= self . verbosity : \n        print ( msg , * args , ** kwargs ) "}
{"11530": "\ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    print ( \"Running fast forward selection algorithm\" ) \n    number_of_scenarios = scenarios . shape [ 1 ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ 1 / number_of_scenarios for i in range ( 0 , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( 0 , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( 0 , number_of_scenarios ) \n    if number_of_scenarios <= number_of_reduced_scenarios : \n        return ( scenarios , probability , J ) \n    for scenario_k in range ( 0 , number_of_scenarios ) : \n        for scenario_u in range ( 0 , number_of_scenarios ) : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n    for scenario_u in range ( 0 , number_of_scenarios ) : \n        summation = 0 \n        for scenario_k in range ( 0 , number_of_scenarios ) : \n            if scenario_k != scenario_u : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n        z [ scenario_u ] = summation \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    for _ in range ( 0 , number_of_scenarios - number_of_reduced_scenarios - 1 ) : \n        print ( \"Running {}\" . format ( _ ) ) \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = 0 \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if lowest_value > c [ deleted_scenario_number , scenario_j ] : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11540": "\ndef trending ( self , rating = None , limit = DEFAULT_SEARCH_LIMIT ) : \n    results_yielded = 0 \n    page , per_page = 0 , 25 \n    params = { 'rating' : rating } if rating else { } \n    fetch = partial ( self . _fetch , 'trending' , ** params ) \n    while True : \n        data = fetch ( offset = page , limit = per_page ) \n        page += per_page \n        if not data [ 'data' ] : \n            raise StopIteration \n        for item in data [ 'data' ] : \n            results_yielded += 1 \n            yield GiphyImage ( item ) \n            if limit is not None and limit <= results_yielded : \n                raise StopIteration \n        if ( data [ 'pagination' ] [ 'total_count' ] <= page or ( limit is not None and limit <= results_yielded ) ) : \n            raise StopIteration "}
{"11572": "\ndef resolve_polytomies ( self ) : \n    def _resolve_polytomies ( n ) : \n        new = Node ( length = self . _length_formatter ( self . _length_parser ( '0' ) ) ) \n        while 1 < len ( n . descendants ) : \n            new . add_descendant ( n . descendants . pop ( ) ) \n        n . descendants . append ( new ) \n    self . visit ( _resolve_polytomies , lambda n : 2 < len ( n . descendants ) ) "}
{"11590": "\ndef _parse_header ( line ) : \n    parts = _parseparam ( ';' + line ) \n    key = parts . next ( ) \n    pdict = { } \n    for p in parts : \n        i = p . find ( '=' ) \n        if 0 <= i : \n            name = p [ : i ] . strip ( ) . lower ( ) \n            value = p [ i + 1 : ] . strip ( ) \n            if 2 <= len ( value ) and value [ 0 ] == value [ - 1 ] == '\"' : \n                value = value [ 1 : - 1 ] \n                value = value . replace ( '\\\\\\\\' , '\\\\' ) . replace ( '\\\\\"' , '\"' ) \n            pdict [ name ] = value \n    return key , pdict "}
{"11603": "\ndef occupancy ( grid , points , spacing = 0.01 ) : \n    distances = ( ( grid [ : , None , : ] - points [ None , : , : ] ) ** 2 ) . sum ( axis = 2 ) \n    occupied = ( spacing > distances ) . sum ( axis = 1 ) \n    return occupied "}
{"11626": "\ndef write_auth ( msg_type , profile_name , auth , cfg ) : \n    key_fmt = profile_name + \"_\" + msg_type \n    pwd = [ ] \n    for k , v in CONFIG [ msg_type ] [ \"auth\" ] . items ( ) : \n        pwd . append ( auth [ k ] ) \n    if 1 < len ( pwd ) : \n        cfg . pwd [ key_fmt ] = \" :: \" . join ( pwd ) \n    else : \n        cfg . pwd [ key_fmt ] = pwd [ 0 ] "}
{"11628": "\ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    if encoding == \"json\" : \n        resp = requests . post ( self . url , json = self . message ) \n    elif encoding == \"url\" : \n        resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and 300 <= resp . history [ 0 ] . status_code : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        elif \"invalid_auth\" in resp . text : \n            raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        print ( timestamp ( ) , type ( self ) . __name__ , \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status_code , ) \n    print ( \"Message sent.\" ) "}
{"11664": "\ndef save ( self , filename = None , v2_version = 4 , v23_sep = '/' ) : \n    framedata = self . _prepare_framedata ( v2_version , v23_sep ) \n    framesize = len ( framedata ) \n    if filename is None : \n        filename = self . filename \n    fileobj = open ( filename , 'rb+' ) \n    iff_file = IFFFile ( fileobj ) \n    try : \n        if u'ID3' not in iff_file : \n            iff_file . insert_chunk ( u'ID3' ) \n        chunk = iff_file [ u'ID3' ] \n        fileobj . seek ( chunk . data_offset ) \n        header = fileobj . read ( 10 ) \n        header = self . _prepare_id3_header ( header , framesize , v2_version ) \n        header , new_size , _ = header \n        data = header + framedata + ( b'\\x00' * ( new_size - framesize ) ) \n        new_size += 10 \n        if chunk . size < new_size : \n            insert_at = chunk . offset + chunk . size \n            insert_size = new_size - chunk . size + new_size % 2 \n            insert_bytes ( fileobj , insert_size , insert_at ) \n            chunk . resize ( new_size ) \n        fileobj . seek ( chunk . data_offset ) \n        fileobj . write ( data ) \n    finally : \n        fileobj . close ( ) "}
{"11701": "\ndef add_markup ( self ) : \n    if self . markup and self . markup_lines : \n        marks = self . markup_lines \n        if 0 < len ( marks ) and not string . strip ( marks [ - 1 ] ) : \n            self . markup_lines = marks [ : - 1 ] \n        m = DocMarkup ( self . markup , self . markup_lines ) \n        self . markups . append ( m ) \n        self . markup = None \n        self . markup_lines = [ ] "}
{"11702": "\ndef process_content ( self , content ) : \n    markup = None \n    markup_lines = [ ] \n    first = 1 \n    for line in content : \n        found = None \n        for t in re_markup_tags : \n            m = t . match ( line ) \n            if m : \n                found = string . lower ( m . group ( 1 ) ) \n                prefix = len ( m . group ( 0 ) ) \n                line = \" \" * prefix + line [ prefix : ] \n                break \n        if found : \n            first = 0 \n            self . add_markup ( ) \n            self . markup = found \n            if 0 < len ( string . strip ( line ) ) : \n                self . markup_lines . append ( line ) \n        elif first == 0 : \n            self . markup_lines . append ( line ) \n    self . add_markup ( ) \n    return self . markups "}
{"11706": "\ndef insert_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    assert size > 0 \n    assert offset >= 0 \n    locked = False \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset \n    fobj . write ( b'\\x00' * size ) \n    fobj . flush ( ) \n    try : \n        try : \n            import mmap \n            file_map = mmap . mmap ( fobj . fileno ( ) , filesize + size ) \n            try : \n                file_map . move ( offset + size , offset , movesize ) \n            finally : \n                file_map . close ( ) \n        except ( ValueError , EnvironmentError , ImportError ) : \n            locked = lock ( fobj ) \n            fobj . truncate ( filesize ) \n            fobj . seek ( 0 , 2 ) \n            padsize = size \n            while padsize : \n                addsize = min ( BUFFER_SIZE , padsize ) \n                fobj . write ( b\"\\x00\" * addsize ) \n                padsize -= addsize \n            fobj . seek ( filesize , 0 ) \n            while movesize : \n                thismove = min ( BUFFER_SIZE , movesize ) \n                fobj . seek ( - thismove , 1 ) \n                nextpos = fobj . tell ( ) \n                data = fobj . read ( thismove ) \n                fobj . seek ( - thismove + size , 1 ) \n                fobj . write ( data ) \n                fobj . seek ( nextpos ) \n                movesize -= thismove \n            fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11707": "\ndef delete_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    locked = False \n    assert size > 0 \n    assert offset >= 0 \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset - size \n    assert movesize >= 0 \n    try : \n        if 0 < movesize : \n            fobj . flush ( ) \n            try : \n                import mmap \n                file_map = mmap . mmap ( fobj . fileno ( ) , filesize ) \n                try : \n                    file_map . move ( offset , offset + size , movesize ) \n                finally : \n                    file_map . close ( ) \n            except ( ValueError , EnvironmentError , ImportError ) : \n                locked = lock ( fobj ) \n                fobj . seek ( offset + size ) \n                buf = fobj . read ( BUFFER_SIZE ) \n                while buf : \n                    fobj . seek ( offset ) \n                    fobj . write ( buf ) \n                    offset += len ( buf ) \n                    fobj . seek ( offset + size ) \n                    buf = fobj . read ( BUFFER_SIZE ) \n        fobj . truncate ( filesize - size ) \n        fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11715": "\ndef ParseID3v1 ( data ) : \n    try : \n        data = data [ data . index ( b'TAG' ) : ] \n    except ValueError : \n        return None \n    if len ( data ) > 128 or 124 > len ( data ) : \n        return None \n    unpack_fmt = \"3s30s30s30s%ds29sBB\" % ( len ( data ) - 124 ) \n    try : \n        tag , title , artist , album , year , comment , track , genre = unpack ( unpack_fmt , data ) \n    except StructError : \n        return None \n    if tag != b\"TAG\" : \n        return None \n    def fix ( data ) : \n        return data . split ( b'\\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) \n    title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) \n    frames = { } \n    if title : \n        frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) \n    if artist : \n        frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) \n    if album : \n        frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) \n    if year : \n        frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) \n    if comment : \n        frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = \"ID3v1 Comment\" , text = comment ) \n    if track and ( ( track != 32 ) or ( data [ - 3 ] == b'\\x00' [ 0 ] ) ) : \n        frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) \n    if genre != 255 : \n        frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) \n    return frames "}
{"11717": "\ndef __fullread ( self , size ) : \n    try : \n        if 0 > size : \n            raise ValueError ( 'Requested bytes (%s) less than zero' % size ) \n        if self . __filesize < size : \n            raise EOFError ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . __filesize ) , self . filename ) ) \n    except AttributeError : \n        pass \n    data = self . _fileobj . read ( size ) \n    if len ( data ) != size : \n        raise EOFError \n    self . __readbytes += size \n    return data "}
{"11720": "\ndef __update_common ( self ) : \n    if \"TCON\" in self : \n        self [ \"TCON\" ] . genres = self [ \"TCON\" ] . genres \n    if self . _V23 > self . version : \n        pics = self . getall ( \"APIC\" ) \n        mimes = { \"PNG\" : \"image/png\" , \"JPG\" : \"image/jpeg\" } \n        self . delall ( \"APIC\" ) \n        for pic in pics : \n            newpic = APIC ( encoding = pic . encoding , mime = mimes . get ( pic . mime , pic . mime ) , type = pic . type , desc = pic . desc , data = pic . data ) \n            self . add ( newpic ) \n        self . delall ( \"LINK\" ) "}
{"11728": "\ndef dump_array ( the_array , write , array_name ) : \n    write ( \"  static const unsigned char  \" + array_name + \"[\" + repr ( len ( the_array ) ) + \"L] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"\" \n    comma = \"    \" \n    col = 0 \n    for value in the_array : \n        line += comma \n        line += \"%3d\" % ord ( value ) \n        comma = \",\" \n        col += 1 \n        if col == 16 : \n            col = 0 \n            comma = \",\\n    \" \n        if 1024 < len ( line ) : \n            write ( line ) \n            line = \"\" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11731": "\ndef make_file_list ( args = None ) : \n    file_list = [ ] \n    if not args : \n        args = sys . argv [ 1 : ] \n    for pathname in args : \n        if 0 <= string . find ( pathname , '*' ) : \n            newpath = glob . glob ( pathname ) \n            newpath . sort ( ) \n        else : \n            newpath = [ pathname ] \n        file_list . extend ( newpath ) \n    if len ( file_list ) == 0 : \n        file_list = None \n    else : \n        file_list = filter ( file_exists , file_list ) \n    return file_list "}
{"11734": "\ndef writeblocks ( blocks ) : \n    data = [ ] \n    codes = [ [ block . code , block . write ( ) ] for block in blocks ] \n    codes [ - 1 ] [ 0 ] |= 128 \n    for code , datum in codes : \n        byte = chr_ ( code ) \n        if 2 ** 24 < len ( datum ) : \n            raise error ( \"block is too long to write\" ) \n        length = struct . pack ( \">I\" , len ( datum ) ) [ - 3 : ] \n        data . append ( byte + length + datum ) \n    return b\"\" . join ( data ) "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = False ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and 4 < header : \n            available += header - 4 \n            header = 4 \n        if available < len ( data ) : \n            padding = self . metadata_blocks [ - 1 ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if 0 < newlength : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        elif available > len ( data ) : \n            self . metadata_blocks [ - 1 ] . length += ( available - len ( data ) ) \n            data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n            assert len ( data ) == available \n        if len ( data ) != available : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128 , 2 ) \n            except IOError : \n                pass \n            else : \n                if f . read ( 3 ) == b\"TAG\" : \n                    f . seek ( - 128 , 2 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11774": "\ndef get_next_colour ( ) : \n    colour = settings . GECKOBOARD_COLOURS [ get_next_colour . cur_colour ] \n    get_next_colour . cur_colour += 1 \n    if len ( settings . GECKOBOARD_COLOURS ) <= get_next_colour . cur_colour : \n        get_next_colour . cur_colour = 0 \n    return colour "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    lnum = 1 \n    try : \n        while True : \n            if maxlines : \n                if maxlines < lnum : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    inum = 1 \n    try : \n        while True : \n            if maxitems : \n                if maxitems < inum : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11790": "\ndef winner ( self ) : \n    hmScore = self . home_score ( ) \n    awScore = self . away_score ( ) \n    if awScore < hmScore : \n        return self . home ( ) \n    elif awScore > hmScore : \n        return self . away ( ) \n    else : \n        return None "}
{"11791": "\ndef season ( self ) : \n    date = self . date ( ) \n    return date . year - 1 if 3 >= date . month else date . year "}
{"11792": "\ndef starters ( self ) : \n    doc = self . get_doc ( ) \n    a = doc ( 'table#vis_starters' ) \n    h = doc ( 'table#home_starters' ) \n    data = [ ] \n    for h , table in enumerate ( ( a , h ) ) : \n        team = self . home ( ) if h else self . away ( ) \n        for i , row in enumerate ( table ( 'tbody tr' ) . items ( ) ) : \n            datum = { } \n            datum [ 'player_id' ] = sportsref . utils . rel_url_to_id ( row ( 'a' ) [ 0 ] . attrib [ 'href' ] ) \n            datum [ 'playerName' ] = row ( 'th' ) . text ( ) \n            datum [ 'position' ] = row ( 'td' ) . text ( ) \n            datum [ 'team' ] = team \n            datum [ 'home' ] = ( h == 1 ) \n            datum [ 'offense' ] = ( 10 >= i ) \n            data . append ( datum ) \n    return pd . DataFrame ( data ) "}
{"11802": "\ndef season ( self ) : \n    d = self . date ( ) \n    if 9 <= d . month : \n        return d . year + 1 \n    else : \n        return d . year "}
{"11803": "\ndef _get_player_stats ( self , table_id_fmt ) : \n    doc = self . get_main_doc ( ) \n    tms = self . away ( ) , self . home ( ) \n    tm_ids = [ table_id_fmt . format ( tm ) for tm in tms ] \n    tables = [ doc ( 'table#{}' . format ( tm_id ) . lower ( ) ) for tm_id in tm_ids ] \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables ] \n    for i , ( tm , df ) in enumerate ( zip ( tms , dfs ) ) : \n        no_time = df [ 'mp' ] == 0 \n        stat_cols = [ col for col , dtype in df . dtypes . items ( ) if dtype != 'object' ] \n        df . loc [ no_time , stat_cols ] = 0 \n        df [ 'team_id' ] = tm \n        df [ 'is_home' ] = i == 1 \n        df [ 'is_starter' ] = [ 5 > p for p in range ( df . shape [ 0 ] ) ] \n        df . drop_duplicates ( subset = 'player_id' , keep = 'first' , inplace = True ) \n    return pd . concat ( dfs ) "}
{"11805": "\ndef cache ( func ) : \n    CACHE_DIR = appdirs . user_cache_dir ( 'sportsref' , getpass . getuser ( ) ) \n    if not os . path . isdir ( CACHE_DIR ) : \n        os . makedirs ( CACHE_DIR ) \n    \n    @ funcutils . wraps ( func ) \n    def wrapper ( url ) : \n        file_hash = hashlib . md5 ( ) \n        encoded_url = url . encode ( errors = 'replace' ) \n        file_hash . update ( encoded_url ) \n        file_hash = file_hash . hexdigest ( ) \n        filename = '{}/{}' . format ( CACHE_DIR , file_hash ) \n        sport_id = None \n        for a_base_url , a_sport_id in sportsref . SITE_ABBREV . items ( ) : \n            if url . startswith ( a_base_url ) : \n                sport_id = a_sport_id \n                break \n        else : \n            print ( 'No sport ID found for {}, not able to check cache' . format ( url ) ) \n        file_exists = os . path . isfile ( filename ) \n        if sport_id and file_exists : \n            cur_time = int ( time . time ( ) ) \n            mod_time = int ( os . path . getmtime ( filename ) ) \n            days_since_mod = datetime . timedelta ( seconds = ( cur_time - mod_time ) ) . days \n            days_cache_valid = globals ( ) [ '_days_valid_{}' . format ( sport_id ) ] ( url ) \n            cache_is_valid = days_cache_valid > days_since_mod \n        else : \n            cache_is_valid = False \n        allow_caching = sportsref . get_option ( 'cache' ) \n        if file_exists and cache_is_valid and allow_caching : \n            with codecs . open ( filename , 'r' , encoding = 'utf-8' , errors = 'replace' ) as f : \n                text = f . read ( ) \n        else : \n            text = func ( url ) \n            with codecs . open ( filename , 'w+' , encoding = 'utf-8' ) as f : \n                f . write ( text ) \n        return text \n    return wrapper "}
{"11836": "\ndef get_html ( url ) : \n    global last_request_time \n    with throttle_process_lock : \n        with throttle_thread_lock : \n            wait_left = THROTTLE_DELAY - ( time . time ( ) - last_request_time . value ) \n            if 0 < wait_left : \n                time . sleep ( wait_left ) \n            response = requests . get ( url ) \n            last_request_time . value = time . time ( ) \n    if 400 <= response . status_code < 500 : \n        raise ValueError ( 'Status Code {} received fetching URL \"{}\"' . format ( response . status_code , url ) ) \n    html = response . text \n    html = html . replace ( '<!--' , '' ) . replace ( '-->' , '' ) \n    return html "}
{"11840": "\ndef _Streamer__read_process ( self , path , read_size , cbuf , stop , barrier , cyclic , offset , read_skip , sync ) : \n    import tables as tb \n    h5_file = tb . open_file ( self . filename , 'r' , ** self . h5_kw_args ) \n    ary = h5_file . get_node ( path ) \n    i = offset \n    while not stop . is_set ( ) : \n        vals = ary [ i : i + read_size ] \n        if len ( ary ) < i + read_size : \n            vals = np . concatenate ( [ vals , ary [ 0 : read_size - len ( vals ) ] ] ) \n        if sync is None : \n            with cbuf . put_direct ( ) as put_ary : \n                put_ary [ : ] = vals \n        else : \n            with sync . do ( cbuf . put_direct ( ) , i , ( i + read_size ) % len ( ary ) ) as put_ary : \n                put_ary [ : ] = vals \n        i += read_skip \n        if cyclic : \n            if len ( ary ) <= i : \n                i %= len ( ary ) \n                barrier . wait ( ) \n        else : \n            if len ( ary ) < i + read_size : \n                break "}
{"11853": "\ndef write ( self , * pb2_obj ) : \n    base = len ( self . _write_buff ) \n    for idx , obj in enumerate ( pb2_obj ) : \n        if 0 < self . _buffer_size and ( idx + base ) != 0 and ( idx + base ) % self . _buffer_size == 0 : \n            self . flush ( ) \n        self . _write_buff . append ( obj ) \n    if self . _buffer_size == 0 : \n        self . flush ( ) "}
{"11859": "\ndef until_condition ( self , condition , condition_description ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    while True : \n        try : \n            if not hasattr ( condition , '__call__' ) : \n                raise TypeError ( \"condition is not callable\" ) \n            value = condition ( ) \n            if type ( value ) is bool and value is not False : \n                return value \n            elif type ( value ) is not bool and value is not None : \n                return value \n            else : \n                logger . debug ( \"#\" + str ( count ) + \" - wait until \" + condition_description ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0} : {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n        time . sleep ( self . _poll ) \n        count += 1 \n        if end_time < time . time ( ) : \n            break \n    raise TimeoutException ( msg = \"condition <\" + condition_description + \"> was not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11860": "\ndef until_traits_are_present ( self , element_with_traits ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    missing_traits_descriptions = None \n    while True : \n        missing_traits_descriptions = [ ] \n        try : \n            missing_traits_descriptions = element_with_traits . evaluate_traits ( ) \n            if len ( missing_traits_descriptions ) == 0 : \n                return True \n            else : \n                logger . debug ( \"#{0} - wait until all traits are present: <{1}>\" . format ( str ( count ) , '> <' . join ( missing_traits_descriptions ) ) ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0}: {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n            pass \n        time . sleep ( self . _poll ) \n        count += 1 \n        if end_time < time . time ( ) : \n            break \n    raise TimeoutException ( msg = \"conditions \" + '<' + '> <' . join ( missing_traits_descriptions ) + '>' + \" not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( 0 , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while len ( message ) > len ( reply ) and max_tries > tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if max_tries <= tries : \n            return \n        return reply \n    sock . close ( ) "}
{"11896": "\ndef import_qtcore ( ) : \n    has_ida = False \n    try : \n        import idaapi \n        has_ida = True \n    except ImportError : \n        has_ida = False \n    if has_ida : \n        old_path = sys . path [ : ] \n        try : \n            ida_python_path = os . path . dirname ( idaapi . __file__ ) \n            sys . path . insert ( 0 , ida_python_path ) \n            if 690 <= idaapi . IDA_SDK_VERSION : \n                from PyQt5 import QtCore \n                return QtCore \n            else : \n                from PySide import QtCore \n                return QtCore \n        finally : \n            sys . path = old_path \n    else : \n        try : \n            from PyQt5 import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        try : \n            from PySide import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        raise ImportError ( \"No module named PySide or PyQt\" ) "}
{"11919": "\ndef __fetch_items ( self , path , page = 1 ) : \n    fetch_data = True \n    parsed_crates = 0 \n    total_crates = 0 \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            payload = { 'sort' : 'alphabetical' , 'page' : page } \n            raw_content = self . fetch ( path , payload = payload ) \n            content = json . loads ( raw_content ) \n            parsed_crates += len ( content [ 'crates' ] ) \n            if not total_crates : \n                total_crates = content [ 'meta' ] [ 'total' ] \n        except requests . exceptions . HTTPError as e : \n            logger . error ( \"HTTP exception raised - %s\" , e . response . text ) \n            raise e \n        yield raw_content \n        page += 1 \n        if total_crates <= parsed_crates : \n            fetch_data = False "}
{"11921": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Looking for questions at url '%s' using offset %s\" , self . url , str ( offset ) ) \n    nquestions = 0 \n    tquestions = 0 \n    equestions = 0 \n    page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    page_offset = page * KitsuneClient . ITEMS_PER_PAGE \n    drop_questions = offset - page_offset \n    current_offset = offset \n    questions_page = self . client . get_questions ( offset ) \n    while True : \n        try : \n            raw_questions = next ( questions_page ) \n        except StopIteration : \n            break \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code == 500 : \n                logger . exception ( e ) \n                logger . error ( \"Problem getting Kitsune questions. \" \"Loosing %i questions. Going to the next page.\" , KitsuneClient . ITEMS_PER_PAGE ) \n                equestions += KitsuneClient . ITEMS_PER_PAGE \n                current_offset += KitsuneClient . ITEMS_PER_PAGE \n                questions_page = self . client . get_questions ( current_offset ) \n                continue \n            else : \n                raise e \n        try : \n            questions_data = json . loads ( raw_questions ) \n            tquestions = questions_data [ 'count' ] \n            questions = questions_data [ 'results' ] \n        except ( ValueError , KeyError ) as ex : \n            logger . error ( ex ) \n            cause = ( \"Bad JSON format for mozilla_questions: %s\" % ( raw_questions ) ) \n            raise ParseError ( cause = cause ) \n        for question in questions : \n            if 0 < drop_questions : \n                drop_questions -= 1 \n                continue \n            question [ 'offset' ] = current_offset \n            current_offset += 1 \n            question [ 'answers_data' ] = [ ] \n            for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : \n                answers = json . loads ( raw_answers ) [ 'results' ] \n                question [ 'answers_data' ] += answers \n            yield question \n            nquestions += 1 \n        logger . debug ( \"Questions: %i/%i\" , nquestions + offset , tquestions ) \n    logger . info ( \"Total number of questions: %i (%i total)\" , nquestions , tquestions ) \n    logger . info ( \"Questions with errors dropped: %i\" , equestions ) "}
{"11936": "\ndef parse ( self ) : \n    nevents_wrong = 0 \n    feed_json = json . loads ( self . feed ) \n    if 'entry' not in feed_json [ 'feed' ] : \n        return \n    self . cells = feed_json [ 'feed' ] [ 'entry' ] \n    self . ncell = 0 \n    event_fields = self . __get_event_fields ( ) \n    while len ( self . cells ) > self . ncell : \n        event = self . __get_next_event ( event_fields ) \n        if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : \n            logger . warning ( \"Wrong event data: %s\" , event ) \n            nevents_wrong += 1 \n            continue \n        yield event \n    logger . info ( \"Total number of wrong events: %i\" , nevents_wrong ) "}
{"11946": "\ndef remove_client ( self , client ) : \n    try : \n        self . _clients . remove ( id ( client ) ) \n    except ValueError : \n        pass \n    if 1 > len ( self . _clients ) : \n        self . close ( ) "}
{"11949": "\ndef timing_since ( self , name , start_time , rate = 1 ) : \n    duration = 0 \n    if isinstance ( start_time , datetime ) : \n        duration = ( datetime . now ( start_time . tzinfo ) - start_time ) . total_seconds ( ) * 1000 \n    elif is_numeric ( start_time ) : \n        assert 0 < start_time \n        duration = ( time ( ) - start_time ) * 1000 \n    else : \n        raise ValueError ( \"start time should be a timestamp or a datetime\" ) \n    self . timing ( name , duration , rate ) "}
{"11956": "\ndef flush ( self ) : \n    address = self . remote_address \n    while 0 < len ( self . _batches ) : \n        self . _socket . sendto ( self . _batches [ 0 ] , address ) \n        self . _batches . popleft ( ) \n    return self "}
{"11959": "\ndef flush ( self ) : \n    while 0 < len ( self . _batches ) : \n        self . _socket . sendall ( self . _batches [ 0 ] ) \n        self . _batches . popleft ( ) \n    return self "}
{"11974": "\ndef field_required_attribute ( function ) : \n    def _wrapper ( field , ** kwargs ) : \n        if not field . required and 0.1 > random . random : \n            return None \n        return function ( field , ** kwargs ) \n    return _wrapper "}
{"11989": "\ndef any_field_blank ( function ) : \n    def wrapper ( field , ** kwargs ) : \n        if kwargs . get ( 'isnull' , False ) : \n            return None \n        if field . blank and 0.1 > random . random : \n            return None \n        return function ( field , ** kwargs ) \n    return wrapper "}
{"11994": "\ndef decode ( data ) : \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    pos = 0 \n    while len ( data ) > pos : \n        header_byte = data [ pos ] \n        if 127 < header_byte : \n            header_byte -= 256 \n        pos += 1 \n        if 0 <= header_byte <= 127 : \n            result . extend ( data [ pos : pos + header_byte + 1 ] ) \n            pos += header_byte + 1 \n        elif header_byte == - 128 : \n            pass \n        else : \n            result . extend ( [ data [ pos ] ] * ( 1 - header_byte ) ) \n            pos += 1 \n    return bytes ( result ) "}
{"11995": "\ndef encode ( data ) : \n    if len ( data ) == 0 : \n        return data \n    if len ( data ) == 1 : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = 0 \n    repeat_count = 0 \n    MAX_LENGTH = 127 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if len ( buf ) == 0 : \n            return \n        result . append ( len ( buf ) - 1 ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256 - ( repeat_count - 1 ) ) \n        result . append ( data [ pos ] ) \n    while len ( data ) - 1 > pos : \n        current_byte = data [ pos ] \n        if data [ pos ] == data [ pos + 1 ] : \n            if state == 'RAW' : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = 1 \n            elif state == 'RLE' : \n                if repeat_count == MAX_LENGTH : \n                    finish_rle ( ) \n                    repeat_count = 0 \n                repeat_count += 1 \n        else : \n            if state == 'RLE' : \n                repeat_count += 1 \n                finish_rle ( ) \n                state = 'RAW' \n                repeat_count = 0 \n            elif state == 'RAW' : \n                if len ( buf ) == MAX_LENGTH : \n                    finish_raw ( ) \n                buf . append ( current_byte ) \n        pos += 1 \n    if state == 'RAW' : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count += 1 \n        finish_rle ( ) \n    return bytes ( result ) "}
{"11997": "\ndef format ( self , number , ** kwargs ) : \n    if check_type ( number , 'list' ) : \n        return map ( lambda val : self . format ( val , ** kwargs ) ) \n    number = self . parse ( number ) \n    if check_type ( kwargs , 'dict' ) : \n        options = ( self . settings [ 'number' ] . update ( kwargs ) ) \n    precision = self . _change_precision ( options [ 'precision' ] ) \n    negative = ( lambda num : \"-\" if 0 > num else \"\" ) ( number ) \n    base = str ( int ( self . to_fixed ( abs ( number ) or 0 , precision ) ) , 10 ) \n    mod = ( lambda num : len ( num ) % 3 if 3 < len ( num ) else 0 ) ( base ) \n    num = negative + ( lambda num : base [ 0 : num ] if num else '' ) ( mod ) \n    num += re . sub ( '/(\\d{3})(?=\\d)/g' , '$1' + options [ 'thousand' ] , base [ mod : ] ) \n    num += ( lambda val : options [ 'decimal' ] + self . to_fixed ( abs ( number ) , precision ) . split ( '.' ) [ 1 ] if val else '' ) ( precision ) \n    return num "}
{"11998": "\ndef as_money ( self , number , ** options ) : \n    if isinstance ( number , list ) : \n        return map ( lambda val : self . as_money ( val , ** options ) ) \n    decimal = options . get ( 'decimal' ) \n    number = self . parse ( number , decimal ) \n    if check_type ( options , 'dict' ) : \n        options = ( self . settings [ 'currency' ] . update ( options ) ) \n    formats = self . _check_currency_format ( options [ 'format' ] ) \n    use_format = ( lambda num : formats [ 'pos' ] if 0 < num else formats [ 'neg' ] if 0 > num else formats [ 'zero' ] ) ( number ) \n    precision = self . _change_precision ( number , options [ 'precision' ] ) \n    thousands = options [ 'thousand' ] \n    decimal = options [ 'decimal' ] \n    formater = self . format ( abs ( number ) , precision , thousands , decimal ) \n    amount = use_format . replace ( '%s' , options [ 'symbol' ] ) . replace ( '%v' , formater ) \n    return amount "}
{"12041": "\ndef _guess_format_from_extension ( ext ) : \n    ext = ext . strip ( '.' ) \n    formats = [ ] \n    for fmt in FILE_FORMATS : \n        if ext in FILE_FORMATS [ fmt ] : \n            formats . append ( fmt ) \n    if formats == [ ] or 1 < len ( formats ) : \n        return False \n    return formats [ 0 ] "}
{"12044": "\ndef build_graph ( self , project , site , subject , session , scan , size , email = None , invariants = Invariants . ALL , fiber_file = DEFAULT_FIBER_FILE , atlas_file = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if not set ( Invariants . ALL ) >= set ( invariants ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != 1 : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if size not in [ self . BIG , self . SMALL ] : \n        raise ValueError ( \"size must be either grute.BIG or grute.SMALL.\" ) \n    url = \"buildgraph/{}/{}/{}/{}/{}/{}/{}/{}/\" . format ( project , site , subject , session , scan , size , email , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments must not contain spaces.\" ) \n    if use_threads : \n        download_thread = threading . Thread ( target = self . _run_build_graph , args = [ url , fiber_file , atlas_file , callback ] ) \n        download_thread . start ( ) \n    else : \n        return self . _run_build_graph ( url , fiber_file , atlas_file ) \n    return "}
{"12045": "\ndef compute_invariants ( self , graph_file , input_format , invariants = Invariants . ALL , email = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format, {}.\" . format ( input_format ) ) \n    if not set ( Invariants . ALL ) >= set ( invariants ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != 1 : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    url = \"graphupload/{}/{}/{}/\" . format ( email , input_format , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments cannot have spaces in them.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"File {} does not exist.\" . format ( graph_file ) ) \n    if use_threads : \n        upload_thread = threading . Thread ( target = self . _run_compute_invariants , args = [ url , graph_file , callback ] ) \n        upload_thread . start ( ) \n    else : \n        return self . _run_compute_invariants ( url , graph_file ) \n    return "}
{"12046": "\ndef convert_graph ( self , graph_file , input_format , output_formats , email = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format {}.\" . format ( input_format ) ) \n    if not set ( GraphFormats . _any ) >= set ( output_formats ) : \n        raise ValueError ( \"Output formats must be a GraphFormats.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != 1 : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"No such file, {}!\" . format ( graph_file ) ) \n    url = \"convert/{}/{}/{}/l\" . format ( email , input_format , ',' . join ( output_formats ) ) \n    if \" \" in url : \n        raise ValueError ( \"Spaces are not permitted in arguments.\" ) \n    if use_threads : \n        convert_thread = threading . Thread ( target = self . _run_convert_graph , args = [ url , graph_file , callback ] ) \n        convert_thread . start ( ) \n    else : \n        return self . _run_convert_graph ( url , graph_file ) \n    return "}
{"12078": "\ndef human_bytes ( value ) : \n    value = float ( value ) \n    if 1073741824 <= value : \n        gigabytes = value / 1073741824 \n        size = '%.2f GB' % gigabytes \n    elif 1048576 <= value : \n        megabytes = value / 1048576 \n        size = '%.2f MB' % megabytes \n    elif 1024 <= value : \n        kilobytes = value / 1024 \n        size = '%.2f KB' % kilobytes \n    else : \n        size = '%.2f B' % value \n    return size "}
{"12103": "\ndef _resolve_path ( self , create = False ) : \n    if type ( self . _path ) == str : \n        key_path = self . _path . split ( '.' ) \n    else : \n        key_path = [ self . _path ] \n    node = self . _root . _data \n    nodes = [ self . _root . _data ] \n    while len ( key_path ) : \n        key = key_path . pop ( 0 ) \n        try : \n            key = int ( key ) \n        except : \n            pass \n        if create : \n            if type ( node ) == dict and key not in node : \n                node [ key ] = { } \n            elif type ( node ) == list and type ( key ) == int and key > len ( node ) : \n                node . append ( [ None for i in range ( key - len ( node ) ) ] ) \n        nodes . append ( node ) \n        try : \n            node = node [ key ] \n        except TypeError : \n            if type ( key ) == int : \n                raise IndexError ( key ) \n            else : \n                raise KeyError ( key ) \n    return ( nodes [ - 1 ] , key ) "}
{"12112": "\ndef read_socket_input ( connection , socket_obj ) : \n    count = connection . needs_input \n    if 0 >= count : \n        return count \n    while True : \n        try : \n            sock_data = socket_obj . recv ( count ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if 0 < len ( sock_data ) : \n        count = connection . process_input ( sock_data ) \n    else : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_input ( ) \n        connection . close_output ( ) \n    return count "}
{"12113": "\ndef write_socket_output ( connection , socket_obj ) : \n    count = connection . has_output \n    if 0 >= count : \n        return count \n    data = connection . output_data ( ) \n    if not data : \n        return Connection . EOS \n    while True : \n        try : \n            count = socket_obj . send ( data ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if 0 < count : \n        connection . output_written ( count ) \n    elif data : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_output ( ) \n        connection . close_input ( ) \n    return count "}
{"12133": "\ndef need_processing ( self ) : \n    readers = [ ] \n    writers = [ ] \n    timer_heap = [ ] \n    for c in iter ( self . _connections . values ( ) ) : \n        if 0 < c . needs_input : \n            readers . append ( c ) \n        if 0 < c . has_output : \n            writers . append ( c ) \n        if c . deadline : \n            heapq . heappush ( timer_heap , ( c . next_tick , c ) ) \n    timers = [ ] \n    while timer_heap : \n        x = heapq . heappop ( timer_heap ) \n        timers . append ( x [ 1 ] ) \n    return ( readers , writers , timers ) "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return 0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return 0 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( ( 0 , 10 ) > _PROTON_VERSION ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = True \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12136": "\ndef output_data ( self ) : \n    c = self . has_output \n    if 0 >= c : \n        return None \n    try : \n        buf = self . _pn_transport . peek ( c ) \n    except Exception as e : \n        self . _connection_failed ( str ( e ) ) \n        return None \n    return buf "}
{"12167": "\ndef rotatePoint ( self , pointX , pointY ) : \n    if ( self . angle == 0 or self . angle == None ) : \n        return ( pointX , pointY ) \n    length = math . sqrt ( ( pointX - self . xll ) ** 2 + ( pointY - self . yll ) ** 2 ) \n    beta = math . acos ( ( pointX - self . xll ) / length ) \n    if ( self . yll > pointY ) : \n        beta = math . pi * 2 - beta \n    offsetX = math . cos ( beta ) * length - math . cos ( self . _angle_rd + beta ) * length \n    offsetY = math . sin ( self . _angle_rd + beta ) * length - math . sin ( beta ) * length \n    return ( pointX - offsetX , pointY + offsetY ) "}
{"12181": "\ndef x_fit ( self , test_length ) : \n    if self . xmax <= ( self . x + test_length ) : \n        return False \n    else : \n        return True "}
{"12182": "\ndef y_fit ( self , test_length ) : \n    if self . ymax <= ( self . y + test_length ) : \n        return False \n    else : \n        return True "}
{"12183": "\ndef x_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if test_ordinate . x < self . x : \n        return True \n    else : \n        return False "}
{"12184": "\ndef y_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if test_ordinate . y < self . y : \n        return True \n    else : \n        return False "}
{"12217": "\ndef status ( self ) : \n    line = next ( self . __line_gen ( ) ) . rstrip ( ) \n    parts = line . split ( None , 1 ) \n    try : \n        code , message = int ( parts [ 0 ] ) , \"\" \n    except ValueError : \n        raise NNTPProtocolError ( line ) \n    if 100 > code or 600 <= code : \n        raise NNTPProtocolError ( line ) \n    if 1 < len ( parts ) : \n        message = parts [ 1 ] \n    if 400 <= code <= 499 : \n        raise NNTPTemporaryError ( code , message ) \n    if 500 <= code <= 599 : \n        raise NNTPPermanentError ( code , message ) \n    return code , message "}
{"12231": "\ndef list_newsgroups_gen ( self , pattern = None ) : \n    args = pattern \n    code , message = self . command ( \"LIST NEWSGROUPS\" , args ) \n    if code != 215 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . strip ( ) . split ( ) \n        name , description = parts [ 0 ] , \"\" \n        if 1 < len ( parts ) : \n            description = parts [ 1 ] \n        yield name , description "}
{"12262": "\ndef delete ( self , tag_id ) : \n    this_alert = [ tag for tag in self . list_tags ( ) if tag . get ( 'id' ) == tag_id ] \n    if 1 > len ( this_alert ) : \n        return \n    query_id = this_alert [ 0 ] . get ( 'scheduled_query_id' ) \n    tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}' \n    self . _api_delete ( url = tag_url . format ( account_id = self . account_id , tag_id = tag_id ) ) \n    query_url = 'https://logentries.com/rest/{account_id}/api/scheduled_queries/{query_id}' \n    self . _api_delete ( url = query_url . format ( account_id = self . account_id , query_id = query_id ) ) "}
{"12263": "\ndef unparse_range ( obj ) : \n    if isinstance ( obj , ( int , long ) ) : \n        return str ( obj ) \n    if isinstance ( obj , tuple ) : \n        arg = str ( obj [ 0 ] ) + \"-\" \n        if 1 < len ( obj ) : \n            arg += str ( obj [ 1 ] ) \n        return arg \n    raise ValueError ( \"Must be an integer or tuple\" ) "}
{"12298": "\ndef _call_api ( self ) : \n    self . _url = self . form_url ( ) \n    if self . _headers is not None : \n        logging . debug ( self . _headers ) \n    if self . _data is not None : \n        logging . debug ( self . _data ) \n    if 0 < len ( self . _get_url_parameters ( ) ) : \n        logging . debug ( self . _get_url_parameters ( ) ) \n    result = self . _methods [ self . _method ] ( ) \n    if not self . good_response ( result . status_code ) : \n        logging . error ( self . _url ) \n        logging . error ( self . _method ) \n        if self . _data is not None : \n            logging . error ( self . _data ) \n        logging . error ( result ) \n    self . _api_result = result "}
{"12304": "\ndef connect_earthexplorer ( self ) : \n    logger . info ( \"Establishing connection to Earthexplorer\" ) \n    print ( \"\\n Establishing connection to Earthexplorer\" ) \n    try : \n        opener = urllib . request . build_opener ( urllib . request . HTTPCookieProcessor ( ) ) \n        urllib . request . install_opener ( opener ) \n        params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) \n        params = params . encode ( 'utf-8' ) \n        f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) \n        data = f . read ( ) . decode ( 'utf-8' ) \n        f . close ( ) \n        if 0 < data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) : \n            print ( \"\\n Authentification failed\" ) \n            logger . error ( \"Authentification failed\" ) \n            raise AutenticationUSGSFailed ( 'Authentification USGS failed' ) \n        print ( 'User %s connected with USGS' % self . user ) \n        logger . debug ( 'User %s connected with USGS' % self . user ) \n        return \n    except Exception as e : \n        print ( '\\nError when trying to connect USGS: %s' % e ) \n        raise logger . error ( 'Error when trying to connect USGS: %s' % e ) "}
{"12310": "\ndef point_to_source ( source , position , fmt = ( 2 , True , \"~~~~~\" , \"^\" ) ) : \n    surrounding_lines , show_line_numbers , tail_body , pointer_char = fmt \n    line_no , char_no = position \n    lines = source . split ( \"\\n\" ) \n    line = lines [ line_no ] \n    if len ( tail_body ) <= char_no : \n        tail = \" \" * ( char_no - len ( tail_body ) ) + tail_body + pointer_char \n    else : \n        tail = \" \" * char_no + pointer_char + tail_body \n    if show_line_numbers : \n        line_no_width = int ( math . ceil ( math . log10 ( max ( 1 , line_no + surrounding_lines ) ) ) + 1 ) \n        line_fmt = \"{0:\" + str ( line_no_width ) + \"}: {1}\" \n    else : \n        line_fmt = \"{1}\" \n    pivot = line_no + 1 \n    output_lines = [ ( pivot , line ) , ( \"\" , tail ) ] \n    for i in range ( surrounding_lines ) : \n        upper_ofst = i + 1 \n        upper_idx = line_no + upper_ofst \n        lower_ofst = - upper_ofst \n        lower_idx = line_no + lower_ofst \n        if 0 <= lower_idx : \n            output_lines . insert ( 0 , ( pivot + lower_ofst , lines [ lower_idx ] ) ) \n        if len ( lines ) > upper_idx : \n            output_lines . append ( ( pivot + upper_ofst , lines [ upper_idx ] ) ) \n    return \"\\n\" . join ( line_fmt . format ( n , c ) for n , c in output_lines ) "}
{"12314": "\ndef set_chance ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    total = 0. \n    file = [ ] \n    leftover = [ ] \n    for name , chance in files : \n        if 1 <= total : \n            break \n        fortune = load_fortune ( name , offensive = offensive , lang = lang ) \n        if fortune is None or not fortune . size : \n            continue \n        if chance : \n            file . append ( ( fortune , chance ) ) \n            total += chance \n        else : \n            leftover . append ( fortune ) \n    if leftover and 1 > total : \n        left = 1 - total \n        if equal : \n            perfile = left / len ( leftover ) \n            for fortune in leftover : \n                file . append ( ( fortune , perfile ) ) \n        else : \n            entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) \n            logger . debug ( '%d entries left' , entries ) \n            for fortune in leftover : \n                chance = left * fortune . size / entries \n                file . append ( ( fortune , chance ) ) \n    self . count = count = 65536 \n    bound = 0 \n    self . files = fortunes = [ ] \n    for file , chance in file : \n        bound += int ( chance * count ) \n        fortunes . append ( ( file , bound ) ) \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12355": "\ndef infix_to_postfix ( nodes , * , recurse_types = None ) : \n    output = [ ] \n    operators = [ ] \n    for node in nodes : \n        if isinstance ( node , OperatorNode ) : \n            cmp_operator = node . operator \n            while operators : \n                current_operator = operators [ - 1 ] . operator \n                if cmp_operator . precedence < current_operator . precedence or current_operator . precedence == cmp_operator . precedence and current_operator . association == Association . left : \n                    output . append ( operators . pop ( ) ) \n                else : \n                    break \n            operators . append ( node ) \n        else : \n            if recurse_types is not None and node . node_type in recurse_types : \n                output . extend ( infix_to_postfix ( node . children , recurse_types = recurse_types ) ) \n            else : \n                output . append ( node ) \n    return output + list ( reversed ( operators ) ) "}
{"12356": "\ndef postfix_to_optree ( nodes ) : \n    while 1 < len ( nodes ) : \n        nodes = _reduce ( nodes ) \n    if len ( nodes ) == 0 : \n        raise OperatorError ( \"Empty node list\" ) \n    node = nodes [ 0 ] \n    if isinstance ( node , OperatorNode ) : \n        raise OperatorError ( \"Operator without operands\" ) \n    if isinstance ( node , OptreeNode ) : \n        return node \n    return OptreeNode ( None , ( node , ) ) "}
{"12357": "\ndef _reduce ( nodes ) : \n    i = 0 \n    while len ( nodes ) > i : \n        if isinstance ( nodes [ i ] , OperatorNode ) : \n            break \n        else : \n            i += 1 \n    if i == len ( nodes ) : \n        raise OperatorError ( \"No operator found\" ) \n    operator_node = nodes [ i ] \n    operator = operator_node . operator \n    operands_lbound = i - operator . cardinality \n    if 0 > operands_lbound : \n        raise OperatorError ( \"Insufficient operands for operator {0}\" . format ( operator . symbol ) ) \n    return nodes [ : operands_lbound ] + [ OptreeNode ( operator_node , tuple ( nodes [ operands_lbound : i ] ) ) ] + nodes [ i + 1 : ] "}
{"12376": "\ndef pprint ( root , depth = 0 , space_unit = \"    \" , * , source_len = 0 , file = None ) : \n    spacing = space_unit * depth \n    if isinstance ( root , str ) : \n        print ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) \n    else : \n        if root . position is None : \n            position = - 1 \n        elif 0 > root . position : \n            position = source_len + root . position \n        else : \n            position = root . position \n        if root . is_value : \n            print ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node_type , position , root . consumed , root . svalue ) , file = file ) \n        else : \n            print ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node_type , position , root . consumed ) , file = file ) \n            for child in root . children : \n                pprint ( child , depth + 1 , source_len = source_len , file = file ) "}
{"12379": "\ndef _get_repetition ( extractor , text , * , bounds = ( 0 , None ) , ignore_whitespace = False ) : \n    minr , maxr = bounds \n    children = [ ] \n    while maxr is None or maxr >= len ( children ) : \n        ignored_ws , use_text = _split_ignored ( text , ignore_whitespace ) \n        try : \n            child = _call_extractor ( extractor , use_text ) \n            child . add_ignored ( ignored_ws ) \n        except DeadEnd : \n            break \n        if child . is_empty : \n            break \n        children . append ( child ) \n        text = text [ child . consumed : ] \n    if minr <= len ( children ) : \n        return ParseNode ( ParseNodeType . repetition , children = children ) \n    else : \n        raise DeadEnd ( ) "}
{"12389": "\ndef merged ( self , other ) : \n    children = [ c for c in itertools . chain ( self . children , other . children ) if 0 < len ( c ) ] \n    return ParseNode ( self . node_type , children = children , consumed = self . consumed + other . consumed , ignored = self . ignored ) "}
{"12394": "\ndef step_next_char ( self ) : \n    self . _index += 1 \n    self . _col_offset += 1 \n    if self . _maxindex < self . _index : \n        self . _maxindex = self . _index \n        self . _maxcol = self . _col_offset \n        self . _maxline = self . _lineno "}
{"12396": "\ndef step_prev_line ( self ) : \n    if 0 < len ( self . _eol ) : \n        self . position = self . _eol . pop ( ) "}
{"12397": "\ndef last_readed_line ( self ) -> str : \n    mpos = self . _cursor . max_readed_position \n    mindex = mpos . index \n    prevline = mindex - 1 if mindex == self . eos_index else mindex \n    while 0 <= prevline and self . _content [ prevline ] != '\\n' : \n        prevline -= 1 \n    nextline = mindex \n    while self . eos_index > nextline and self . _content [ nextline ] != '\\n' : \n        nextline += 1 \n    last_line = self . _content [ prevline + 1 : nextline ] \n    return last_line "}
{"12398": "\ndef incpos ( self , length : int = 1 ) -> int : \n    if 0 > length : \n        raise ValueError ( \"length must be positive\" ) \n    i = 0 \n    while ( length > i ) : \n        if self . _len > self . _cursor . index : \n            if self . peek_char == '\\n' : \n                self . _cursor . step_next_line ( ) \n            self . _cursor . step_next_char ( ) \n        i += 1 \n    return self . _cursor . index "}
{"12426": "\ndef set ( self , othernode ) : \n    self . __class__ = othernode . __class__ \n    self . clean ( ) \n    if 0 < len ( othernode ) : \n        for k , v in othernode . items ( ) : \n            self [ k ] = v \n    for k , v in vars ( othernode ) . items ( ) : \n        setattr ( self , k , v ) "}
{"12428": "\ndef _hit_ok ( hit , min_hit_charge , max_hit_charge ) : \n    if min_hit_charge > hit [ 'charge' ] : \n        return False \n    if max_hit_charge != 0 and max_hit_charge < hit [ 'charge' ] : \n        return False \n    return True "}
{"12461": "\ndef peek_text ( self , text : str ) -> bool : \n    start = self . _stream . index \n    stop = start + len ( text ) \n    if self . _stream . eos_index < stop : \n        return False \n    return self . _stream [ self . _stream . index : stop ] == text "}
{"12496": "\ndef infer_id ( self , ident , diagnostic = None ) : \n    defined = self . infer_node . scope_node . get_by_symbol_name ( ident ) \n    if 0 < len ( defined ) : \n        self . infer_node . scope_node . update ( defined ) \n    else : \n        diagnostic . notify ( Severity . ERROR , \"%s never declared\" % self . value , self . info ) "}
{"12513": "\ndef catend ( dst : str , src : str , indent ) -> str : \n    res = dst \n    txtsrc = src \n    if not isinstance ( src , str ) : \n        txtsrc = str ( src ) \n    for c in list ( txtsrc ) : \n        if 0 < len ( res ) and res [ - 1 ] == '\\n' : \n            res += ( indentable . char_indent * indentable . num_indent ) * ( indent - 1 ) + c \n        else : \n            res += c \n    return res "}
{"12529": "\ndef get ( query , from_date , limit = 0 , ** kwargs ) : \n    dep_generator = _get_depositions ( ) \n    total_depids = 1 \n    if 0 < limit : \n        dep_generator = islice ( dep_generator , limit ) \n        total_depids = limit \n    return total_depids , dep_generator "}
{"12534": "\ndef dump_bibdoc ( recid , from_date , ** kwargs ) : \n    BibRecDocs , BibDoc = _import_bibdoc ( ) \n    bibdocfile_dump = [ ] \n    date = datetime . datetime . strptime ( from_date , '%Y-%m-%d %H:%M:%S' ) \n    for bibdoc in BibRecDocs ( recid ) . list_bibdocs ( ) : \n        for version in bibdoc . list_versions ( ) : \n            bibdoc_version = bibdoc . list_version_files ( version ) \n            for f in bibdoc_version : \n                if f . is_icon ( ) or date > f . md : \n                    continue \n                bibdocfile_dump . append ( dict ( bibdocid = f . get_bibdocid ( ) , checksum = f . get_checksum ( ) , comment = f . get_comment ( ) , copyright = ( f . get_copyright ( ) if hasattr ( f , 'get_copyright' ) else None ) , creation_date = datetime_toutc ( f . cd ) . isoformat ( ) , description = f . get_description ( ) , encoding = f . encoding , etag = f . etag , flags = f . flags , format = f . get_format ( ) , full_name = f . get_full_name ( ) , full_path = f . get_full_path ( ) , hidden = f . hidden , license = ( f . get_license ( ) if hasattr ( f , 'get_license' ) else None ) , modification_date = datetime_toutc ( f . md ) . isoformat ( ) , name = f . get_name ( ) , mime = f . mime , path = f . get_path ( ) , recid = f . get_recid ( ) , recids_doctype = f . recids_doctypes , size = f . get_size ( ) , status = f . get_status ( ) , subformat = f . get_subformat ( ) , superformat = f . get_superformat ( ) , type = f . get_type ( ) , url = f . get_url ( ) , version = f . get_version ( ) , ) ) \n    return bibdocfile_dump "}
{"12542": "\ndef _get_modified_recids_invenio2 ( from_date ) : \n    from invenio . legacy . search_engine import search_pattern \n    from invenio . modules . records . models import Record \n    date = datetime . datetime . strptime ( from_date , '%Y-%m-%d %H:%M:%S' ) \n    return set ( ( x [ 0 ] for x in Record . query . filter ( date <= Record . modification_date ) . values ( Record . id ) ) ) , search_pattern "}
{"12581": "\ndef load_user ( data ) : \n    from invenio_accounts . models import User \n    from invenio_userprofiles . api import UserProfile \n    email = data [ 'email' ] . strip ( ) \n    if 0 < User . query . filter_by ( email = email ) . count ( ) : \n        raise UserEmailExistsError ( \"User email '{email}' already exists.\" . format ( email = email ) ) \n    last_login = None \n    if data [ 'last_login' ] : \n        last_login = arrow . get ( data [ 'last_login' ] ) . datetime \n    confirmed_at = None \n    if data [ 'note' ] == '1' : \n        confirmed_at = datetime . utcnow ( ) \n    salt = data [ 'password_salt' ] \n    checksum = data [ 'password' ] \n    if not checksum : \n        new_password = None \n    elif checksum . startswith ( '$' ) : \n        new_password = checksum \n    else : \n        new_password = str . join ( '$' , [ '' , u'invenio-aes' , salt , checksum ] ) \n    with db . session . begin_nested ( ) : \n        obj = User ( id = data [ 'id' ] , password = new_password , email = email , confirmed_at = confirmed_at , last_login_at = last_login , active = ( data [ 'note' ] != '0' ) , ) \n        db . session . add ( obj ) \n    nickname = data [ 'nickname' ] . strip ( ) \n    overwritten_username = ( 'username' in data and 'displayname' in data ) \n    if nickname or overwritten_username : \n        p = UserProfile ( user = obj ) \n        p . full_name = data . get ( 'full_name' , '' ) . strip ( ) \n        if overwritten_username : \n            p . _username = data [ 'username' ] . lower ( ) \n            p . _displayname = data [ 'displayname' ] \n        elif nickname : \n            if 0 < UserProfile . query . filter ( UserProfile . _username == nickname . lower ( ) ) . count ( ) : \n                raise UserUsernameExistsError ( \"Username '{username}' already exists.\" . format ( username = nickname ) ) \n            try : \n                p . username = nickname \n            except ValueError : \n                current_app . logger . warn ( u'Invalid username {0} for user_id {1}' . format ( nickname , data [ 'id' ] ) ) \n                p . _username = nickname . lower ( ) \n                p . _displayname = nickname \n        db . session . add ( p ) \n    db . session . commit ( ) "}
{"12620": "\ndef toxml ( test_reports , suite_name , hostname = gethostname ( ) , package_name = \"tests\" ) : \n    testsuites = et . Element ( \"testsuites\" ) \n    testsuite = et . SubElement ( testsuites , \"testsuite\" ) \n    test_count = len ( test_reports ) \n    if 1 > test_count : \n        raise ValueError ( 'there must be at least one test report' ) \n    assert 0 < test_count , 'expecting at least one test' \n    error_count = len ( [ r for r in test_reports if r . errors ] ) \n    failure_count = len ( [ r for r in test_reports if r . failures ] ) \n    ts = test_reports [ 0 ] . start_ts \n    start_timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) \n    total_duration = test_reports [ - 1 ] . end_ts - test_reports [ 0 ] . start_ts \n    def quote_attribute ( value ) : \n        return value if value is not None else \"(null)\" \n    testsuite . attrib = dict ( id = \"0\" , errors = str ( error_count ) , failures = str ( failure_count ) , tests = str ( test_count ) , hostname = quote_attribute ( hostname ) , timestamp = quote_attribute ( start_timestamp ) , time = \"%f\" % total_duration , name = quote_attribute ( suite_name ) , package = quote_attribute ( package_name ) , ) \n    for r in test_reports : \n        test_name = r . name \n        test_duration = r . end_ts - r . start_ts \n        class_name = r . src_location \n        testcase = et . SubElement ( testsuite , \"testcase\" ) \n        testcase . attrib = dict ( name = test_name , classname = quote_attribute ( class_name ) , time = \"%f\" % test_duration , ) \n        if r . errors or r . failures : \n            if r . failures : \n                failure = et . SubElement ( testcase , \"failure\" ) \n                failure . attrib = dict ( type = \"exception\" , message = quote_attribute ( '\\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) \n            else : \n                error = et . SubElement ( testcase , \"error\" ) \n                error . attrib = dict ( type = \"exception\" , message = quote_attribute ( '\\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) \n    return et . tostring ( testsuites , encoding = \"utf-8\" ) "}
{"12657": "\ndef addLayer ( self , layer , z_index = None ) : \n    if z_index is None : \n        z_index = layer . z_index \n    i = 0 \n    for l , z in self . layers : \n        if z_index < z : \n            break \n        i += 1 \n    self . _layers [ layer . name ] = layer \n    self . layers . insert ( i , [ layer , z_index ] ) "}
{"12667": "\ndef check_elements ( self ) : \n    existing_types = set ( self . elements . type . argiope . values . flatten ( ) ) \n    allowed_types = set ( ELEMENTS . keys ( ) ) \n    if ( allowed_types >= existing_types ) == False : \n        raise ValueError ( \"Element types {0} not in know elements {1}\" . format ( existing_types - allowed_types , allowed_types ) ) \n    print ( \"<Elements: OK>\" ) "}
{"12683": "\ndef list_to_string ( l = range ( 200 ) , width = 40 , indent = \"  \" ) : \n    l = [ str ( v ) + \",\" for v in l ] \n    counter = 0 \n    out = \"\" + indent \n    for w in l : \n        s = len ( w ) \n        if width < counter + s : \n            out += \"\\n\" + indent \n            counter = 0 \n        out += w \n        counter += s \n    return out . strip ( \",\" ) "}
{"12689": "\ndef write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - 1 , frame = - 1 , sortItem = 'Node Label' ) : \n    stepKeys = get_steps ( odb ) \n    step = xrange ( len ( stepKeys ) ) [ step ] \n    frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] \n    nf = NumberFormat ( numDigits = 9 , precision = 0 , format = SCIENTIFIC ) \n    session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) \n    leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) \n    session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) \n    session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) \n    lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] \n    isdata = - 1 \n    data = [ ] \n    for line in lines : \n        if isdata == 1 : \n            if len ( line ) == 0 : \n                isdata -= 1 \n            else : \n                data . append ( line ) \n        elif 1 > isdata : \n            if line . startswith ( \"--\" ) : \n                isdata += 1 \n    data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) \n    header = str ( output_position ) . lower ( ) + \",\" \n    header += \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + \"\\n\" \n    metadata = ( ( \"label\" , label ) , ( \"argiope_class\" , argiope_class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output_position ) , ( \"step_num\" , step ) , ( \"step_label\" , stepKeys [ step ] ) , ( \"frame\" , frame ) , ( \"frame_value\" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) \n    out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) \n    open ( path , \"w\" ) . write ( out ) "}
{"12696": "\ndef make_class ( clsname , func , attrs ) : \n    clsdict = { \"__set__\" : create_setter ( func , attrs ) } \n    if 0 < len ( attrs ) : \n        clsdict [ \"__init__\" ] = create_init ( attrs ) \n    clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) \n    clsobj . __doc__ = docstrings . get ( clsname ) \n    return clsobj "}
{"12707": "\ndef collect_words ( self , si ) : \n    counter = Counter ( ) \n    for tagger_id , sentences in si . body . sentences . iteritems ( ) : \n        if ( ( self . keyword_tagger_ids is not None and tagger_id not in self . keyword_tagger_ids ) ) : \n            continue \n        for sentence in sentences : \n            for token in sentence . tokens : \n                term = token . token \n                term = term . decode ( 'utf-8' ) \n                term = cleanse ( term ) \n                if ( ( self . keyword_size_limit is not None and self . keyword_size_limit < len ( term ) ) ) : \n                    continue \n                if term not in self . stop_words : \n                    counter [ term ] += 1 \n    return counter "}
{"12748": "\ndef random_adjspecies_pair ( maxlen = None , prevent_stutter = True ) : \n    while True : \n        pair = _random_adjspecies_pair ( ) \n        if maxlen and maxlen < len ( '' . join ( pair ) ) : \n            continue \n        if prevent_stutter and pair [ 0 ] [ - 1 ] == pair [ 1 ] [ 0 ] : \n            continue \n        return pair "}
{"12759": "\ndef run ( self , i_str , start_count = 0 , start_chunk_time = None ) : \n    try : \n        if not os . path . exists ( self . tmp_dir_path ) : \n            os . makedirs ( self . tmp_dir_path ) \n        if start_chunk_time is None : \n            start_chunk_time = time . time ( ) \n        i_chunk = self . reader ( i_str ) \n        t_path = None \n        len_clean_visible = 0 \n        sources = set ( ) \n        next_idx = 0 \n        input_item_count = 0 \n        for si in i_chunk : \n            next_idx += 1 \n            if gevent : \n                gevent . sleep ( 0 ) \n            if start_count >= next_idx : \n                continue \n            if next_idx % self . rate_log_interval == 0 : \n                elapsed = time . time ( ) - start_chunk_time \n                if 0 < elapsed : \n                    rate = float ( next_idx ) / elapsed \n                    logger . info ( '%d in %.1f --> %.1f per sec on ' '(pre-partial_commit) %s' , next_idx - start_count , elapsed , rate , i_str ) \n            if not self . t_chunk : \n                t_path = os . path . join ( self . tmp_dir_path , 't_chunk-%s' % uuid . uuid4 ( ) . hex ) \n                self . t_chunk = streamcorpus . Chunk ( path = t_path , mode = 'wb' ) \n                assert self . t_chunk . message == streamcorpus . StreamItem_v0_3_0 , self . t_chunk . message \n            si = self . _run_incremental_transforms ( si , self . incremental_transforms ) \n            if si : \n                sources . add ( si . source ) \n                if self . assert_single_source and len ( sources ) != 1 : \n                    raise InvalidStreamItem ( 'stream item %r had source %r, not %r ' '(set assert_single_source: false to suppress)' % ( si . stream_id , si . source , sources ) ) \n            if si and si . body and si . body . clean_visible : \n                len_clean_visible += len ( si . body . clean_visible ) \n            if ( ( self . output_chunk_max_count is not None and len ( self . t_chunk ) == self . output_chunk_max_count ) ) : \n                logger . info ( 'reached output_chunk_max_count (%d) at: %d' , len ( self . t_chunk ) , next_idx ) \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            elif ( self . output_max_clean_visible_bytes is not None and self . output_chunk_max_clean_visible_bytes <= len_clean_visible ) : \n                logger . info ( 'reached output_chunk_max_clean_visible_bytes ' '(%d) at: %d' , self . output_chunk_max_clean_visible_bytes , len_clean_visible ) \n                len_clean_visible = 0 \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            input_item_count += 1 \n            if ( ( ( self . input_item_limit is not None ) and ( self . input_item_limit < input_item_count ) ) ) : \n                break \n        if self . t_chunk is not None : \n            self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n        return next_idx \n    finally : \n        if self . t_chunk is not None : \n            self . t_chunk . close ( ) \n        for transform in self . batch_transforms : \n            transform . shutdown ( ) \n        if self . cleanup_tmp_files : \n            rmtree ( self . tmp_dir_path ) "}
{"12772": "\ndef mult ( p , n ) : \n    np = P ( ) \n    while 1 <= n : \n        if n % 2 : \n            np = np + p \n        p = p + p \n        n = n // 2 \n    return np "}
{"12774": "\ndef _sentences ( self , clean_visible ) : \n    previous_end = 0 \n    clean_visible = clean_visible . decode ( 'utf8' ) \n    for start , end in self . sentence_tokenizer . span_tokenize ( clean_visible ) : \n        if previous_end > start : \n            start = previous_end \n            if end < start : \n                continue \n        try : \n            label = self . label_index . find_le ( end ) \n        except ValueError : \n            label = None \n        if label : \n            off = label . offsets [ OffsetType . CHARS ] \n            end = max ( off . first + off . length , end ) \n        previous_end = end \n        sent_str = clean_visible [ start : end ] \n        yield start , end , sent_str "}
{"12776": "\ndef make_sentences ( self , stream_item ) : \n    self . make_label_index ( stream_item ) \n    sentences = [ ] \n    token_num = 0 \n    new_mention_id = 0 \n    for sent_start , sent_end , sent_str in self . _sentences ( stream_item . body . clean_visible ) : \n        assert isinstance ( sent_str , unicode ) \n        sent = Sentence ( ) \n        sentence_pos = 0 \n        for start , end in self . word_tokenizer . span_tokenize ( sent_str ) : \n            token_str = sent_str [ start : end ] . encode ( 'utf8' ) \n            tok = Token ( token_num = token_num , token = token_str , sentence_pos = sentence_pos , ) \n            tok . offsets [ OffsetType . CHARS ] = Offset ( type = OffsetType . CHARS , first = sent_start + start , length = end - start , ) \n            try : \n                label = self . label_index . find_le ( sent_start + start ) \n            except ValueError : \n                label = None \n            if label : \n                off = label . offsets [ OffsetType . CHARS ] \n                if sent_start + start < off . first + off . length : \n                    streamcorpus . add_annotation ( tok , label ) \n                    logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target_id ) \n                    if label in self . label_to_mention_id : \n                        mention_id = self . label_to_mention_id [ label ] \n                    else : \n                        mention_id = new_mention_id \n                        new_mention_id += 1 \n                        self . label_to_mention_id [ label ] = mention_id \n                    tok . mention_id = mention_id \n            token_num += 1 \n            sentence_pos += 1 \n            sent . tokens . append ( tok ) \n        sentences . append ( sent ) \n    return sentences "}
{"12777": "\ndef html_entities_to_unicode ( text , space_padding = False , safe_only = False ) : \n    def convert_entities ( match ) : \n        x = match . group ( 1 ) \n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD : \n            return u'&%s;' % x \n        if x in name2codepoint : \n            return unichr ( name2codepoint [ x ] ) \n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS : \n            return XML_ENTITIES_TO_SPECIAL_CHARS [ x ] \n        elif 0 < len ( x ) and x [ 0 ] == '#' : \n            if 1 < len ( x ) and x [ 1 ] == 'x' : \n                return unichr ( int ( x [ 2 : ] , 16 ) ) \n            else : \n                return unichr ( int ( x [ 1 : ] ) ) \n        else : \n            return u'&%s;' % x \n    def convert_to_padded_entitites ( match ) : \n        converted_string = convert_entities ( match ) \n        num_spaces_needed = len ( match . group ( 0 ) ) - len ( converted_string ) \n        assert 0 <= num_spaces_needed , 'len(%r) !<= len(%r)' % ( converted_string , match . group ( 0 ) ) \n        num_left = int ( num_spaces_needed / 2 ) \n        num_right = num_spaces_needed - num_left \n        return ( ' ' * num_left ) + converted_string + ( ' ' * num_right ) \n    if space_padding : \n        return tags . sub ( convert_to_padded_entitites , text ) \n    else : \n        return tags . sub ( convert_entities , text ) "}
{"12785": "\ndef re_based_make_clean_visible ( html ) : \n    text = '' \n    html = fix_emails ( html ) \n    for m in invisible . finditer ( html ) : \n        text += m . group ( 'before' ) \n        text += ' ' * len ( m . group ( 'invisible' ) ) \n    assert len ( text ) <= len ( html ) , '%d !>= %d' % ( len ( html ) , len ( text ) ) \n    tail = len ( html ) - len ( text ) \n    text += html [ - tail : ] \n    assert len ( html ) == len ( text ) , '%d != %d' % ( len ( html ) , len ( text ) ) \n    return text "}
{"12786": "\ndef make_clean_visible ( _html , tag_replacement_char = ' ' ) : \n    def non_tag_chars ( html ) : \n        n = 0 \n        while len ( html ) > n : \n            angle = html . find ( '<' , n ) \n            if angle == - 1 : \n                yield html [ n : ] \n                n = len ( html ) \n                break \n            yield html [ n : angle ] \n            n = angle \n            while len ( html ) > n : \n                nl = html . find ( '\\n' , n ) \n                angle = html . find ( '>' , n ) \n                if angle == - 1 : \n                    yield ' ' * ( len ( html ) - n ) \n                    n = len ( html ) \n                    break \n                elif nl == - 1 or nl > angle : \n                    yield ' ' * ( angle + 1 - n ) \n                    n = angle + 1 \n                    break \n                else : \n                    yield ' ' * ( nl - n ) + '\\n' \n                    n = nl + 1 \n    if not isinstance ( _html , unicode ) : \n        _html = unicode ( _html , 'utf-8' ) \n    _html = fix_emails ( _html ) \n    non_tag = '' . join ( non_tag_chars ( _html ) ) \n    return non_tag . encode ( 'utf-8' ) "}
{"12799": "\ndef get_random_available ( self , max_iter = 10000 ) : \n    c = 1 \n    keeper = None \n    for row in self . _available . get_range ( row_count = max_iter , read_consistency_level = pycassa . ConsistencyLevel . ALL ) : \n        logger . debug ( 'considering %r' % ( row , ) ) \n        if 1 / c > random . random ( ) : \n            keeper = row [ 0 ] \n        if c == max_iter : \n            break \n        c += 1 \n    return keeper "}
{"12800": "\ndef tokens ( self , sentence_dom ) : \n    self . sent_pos = 0 \n    mention_id = 0 \n    while 0 < len ( sentence_dom . childNodes ) : \n        node = sentence_dom . childNodes . pop ( 0 ) \n        if node . nodeType == node . TEXT_NODE : \n            for line in node . data . splitlines ( True ) : \n                self . _input_string = line \n                for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                    tok = self . _make_token ( start , end ) \n                    if tok : \n                        yield tok \n                if line . endswith ( '\\n' ) : \n                    self . line_idx += 1 \n                self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n        else : \n            assert node . nodeName == 'ENAMEX' , node . nodeName \n            chain_id = node . attributes . get ( 'ID' ) . value \n            entity_type = node . attributes . get ( 'TYPE' ) . value \n            for node in node . childNodes : \n                assert node . nodeType == node . TEXT_NODE , node . nodeType \n                for line in node . data . splitlines ( True ) : \n                    self . _input_string = line \n                    for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                        tok = self . _make_token ( start , end ) \n                        if tok : \n                            if entity_type in _PRONOUNS : \n                                tok . mention_type = MentionType . PRO \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                                attr = Attribute ( attribute_type = AttributeType . PER_GENDER , value = str ( _PRONOUNS [ entity_type ] ) ) \n                                self . attributes . append ( attr ) \n                            else : \n                                tok . mention_type = MentionType . NAME \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                            tok . equiv_id = int ( chain_id ) \n                            tok . mention_id = mention_id \n                            yield tok \n                    if line . endswith ( '\\n' ) : \n                        self . line_idx += 1 \n                    self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n            mention_id += 1 "}
{"12802": "\ndef _retry ( func ) : \n    def retry_func ( self , * args , ** kwargs ) : \n        tries = 1 \n        while True : \n            try : \n                return func ( self , * args , ** kwargs ) \n                break \n            except OSError as exc : \n                logger . error ( 'assuming OSError unrecoverable' ) \n                raise \n            except FailedExtraction as exc : \n                logger . error ( 'FAIL(%d)' , tries , exc_info = True ) \n                raise \n            except FailedVerification as exc : \n                logger . warn ( 'FAIL(%d)' , tries , exc_info = True ) \n                if self . config [ 'tries' ] <= tries : \n                    if self . config . get ( 'suppress_failures' ) : \n                        logger . warn ( 'suppressing failure and breaking out of this loop; data may be corrupt, downstream will have to cope' ) \n                        break \n                    else : \n                        raise \n            except Exception as exc : \n                logger . warn ( 'FAIL(%d): having I/O trouble with S3' , tries , exc_info = True ) \n                if self . config [ 'tries' ] <= tries : \n                    raise \n            logger . warn ( 'RETRYING (%d left)' , self . config [ 'tries' ] - tries ) \n            time . sleep ( 3 * tries ) \n            tries += 1 \n    return retry_func "}
{"12815": "\ndef char_offsets_to_xpaths ( html , char_offsets ) : \n    html = uni ( html ) \n    parser = XpathTextCollector ( ) \n    prev_end = 0 \n    prev_progress = True \n    for start , end in char_offsets : \n        if start == end : \n            yield None \n            continue \n        if not prev_progress : \n            for i in xrange ( prev_end , start ) : \n                parser . feed ( html [ i ] ) \n                prev_end += 1 \n                if parser . made_progress : \n                    break \n            if not parser . made_progress : \n                yield None \n                continue \n        if start > prev_end : \n            parser . feed ( html [ prev_end : start ] ) \n            if not parser . made_progress : \n                parser . feed ( html [ start : end ] ) \n                prev_progress = parser . made_progress \n                prev_end = end \n                yield None \n                continue \n        xstart = parser . xpath_offset ( ) \n        parser . feed ( html [ start : end ] ) \n        xend = parser . xpath_offset ( ) \n        prev_end = end \n        if not parser . made_progress : \n            prev_progress = False \n            yield None \n        else : \n            prev_progress = True \n            yield XpathRange ( xstart [ 0 ] , xstart [ 1 ] , xend [ 0 ] , xend [ 1 ] ) \n    parser . feed ( html [ prev_end : ] ) \n    parser . close ( ) "}
{"12887": "\ndef _recurse ( self , inputs , output , depth , max_depth ) : \n    if max_depth > depth : \n        for index , option in enumerate ( inputs ) : \n            my_output = list ( output ) \n            my_output . append ( option ) \n            self . _recurse ( inputs [ index + 1 : ] , my_output , depth + 1 , max_depth ) \n    else : \n        self . _options . append ( output ) "}
{"12902": "\ndef intersects ( self , other ) : \n    try : \n        return ( other . max_x >= self . min_x and other . min_x <= self . max_x and other . max_y >= self . min_y and other . min_y <= self . max_y ) \n    except AttributeError : \n        return self . intersects ( Envelope ( other ) ) "}
{"12941": "\ndef raster ( self , path , size , bandtype = gdal . GDT_Byte ) : \n    path = getattr ( path , 'name' , path ) \n    try : \n        is_multiband = 2 < len ( size ) \n        nx , ny , nbands = size if is_multiband else size + ( 1 , ) \n    except ( TypeError , ValueError ) as exc : \n        exc . args = ( 'Size must be 2 or 3-item sequence' , ) \n        raise \n    if 1 > nx or 1 > ny : \n        raise ValueError ( 'Invalid raster size %s' % ( size , ) ) \n    if not self . _is_empty ( path ) : \n        raise IOError ( '%s already exists, open with Raster()' % path ) \n    ds = self . Create ( path , nx , ny , nbands , bandtype ) \n    if not ds : \n        raise ValueError ( 'Could not create %s using %s' % ( path , str ( self ) ) ) \n    return Raster ( ds ) "}
{"12955": "\ndef lookup_alphabet ( charset ) : \n    if charset in PRESETS : \n        return PRESETS [ charset ] \n    if 16 > len ( charset ) : \n        _logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) \n    return charset "}
{"12995": "\ndef zsh_complete ( self , path , cmd , * cmds , sourceable = False ) : \n    grouping = ( 5 , 4 ) <= internal . zsh_version ( ) \n    path = pathlib . Path ( path ) \n    firstline = [ '#compdef' , cmd ] \n    firstline . extend ( cmds ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as zcf : \n        print ( * firstline , end = '\\n\\n' , file = zcf ) \n        print ( 'function _{} {{' . format ( cmd ) , file = zcf ) \n        print ( 'local line' , file = zcf ) \n        print ( '_arguments -C' , end = BLK , file = zcf ) \n        if subcmds : \n            substrs = [ \"{}\\\\:'{}'\" . format ( sub , self . subcmds [ sub ] . help ) for sub in subcmds ] \n            print ( '\"1:Commands:(({}))\"' . format ( ' ' . join ( substrs ) ) , end = BLK , file = zcf ) \n        self . _zsh_comp_command ( zcf , None , grouping ) \n        if subcmds : \n            print ( \"'*::arg:->args'\" , file = zcf ) \n            print ( 'case $line[1] in' , file = zcf ) \n            for sub in subcmds : \n                print ( '{sub}) _{cmd}_{sub} ;;' . format ( sub = sub , cmd = cmd ) , file = zcf ) \n            print ( 'esac' , file = zcf ) \n        print ( '}' , file = zcf ) \n        for sub in subcmds : \n            print ( '\\nfunction _{}_{} {{' . format ( cmd , sub ) , file = zcf ) \n            print ( '_arguments' , end = BLK , file = zcf ) \n            self . _zsh_comp_command ( zcf , sub , grouping ) \n            print ( '}' , file = zcf ) \n        if sourceable : \n            print ( '\\ncompdef _{0} {0}' . format ( cmd ) , * cmds , file = zcf ) "}
{"13014": "\nasync def wait_done ( self ) : \n    if 0 < self . _active_jobs : \n        future = self . _loop . create_future ( ) \n        self . _waiters . append ( future ) \n        await future "}
{"13015": "\ndef _distribute_jobs ( self ) : \n    while ( self . _active_js . job_available ( ) and 0 < len ( self . _ready_callbacks ) ) : \n        job = self . _active_js . get_job ( ) \n        self . _job_sources [ job ] = self . _active_js \n        callback = self . _ready_callbacks . popleft ( ) \n        callback ( job ) "}
{"13018": "\ndef return_job ( self , job ) : \n    if self . _closed : \n        return \n    js = self . _job_sources [ job ] \n    if 0 < len ( self . _ready_callbacks ) : \n        callback = self . _ready_callbacks . popleft ( ) \n        callback ( job ) \n    else : \n        del self . _job_sources [ job ] \n        js . return_job ( job ) "}
{"13023": "\ndef _match_regex ( regex , obj ) : \n    if isinstance ( obj , six . string_types ) : \n        return 0 < len ( regex . findall ( obj ) ) \n    elif isinstance ( obj , dict ) : \n        return _match_regex ( regex , obj . values ( ) ) \n    elif hasattr ( obj , '__iter__' ) : \n        return any ( _match_regex ( regex , s ) for s in obj if isinstance ( s , six . string_types ) ) \n    else : \n        return False "}
{"13033": "\ndef matches ( self , _filter ) : \n    within_attrib = re . match ( r'^([a-z_.]+):(.*)' , _filter ) \n    having_attrib = re . match ( r'^([a-z_.]+)\\?$' , _filter ) \n    if within_attrib is not None : \n        val = self . _get_attrib ( within_attrib . group ( 1 ) ) \n        sub_regex = within_attrib . group ( 2 ) \n        if 0 < len ( sub_regex ) : \n            sub_regex = re . compile ( sub_regex , re . IGNORECASE ) \n            return _match_regex ( sub_regex , val ) \n        else : \n            return val == '' or val is None or val == [ ] \n    elif having_attrib is not None : \n        val = self . _get_attrib ( having_attrib . group ( 1 ) ) \n        return val != '' and val is not None and val != [ ] \n    else : \n        regex = re . compile ( _filter , re . IGNORECASE ) \n        return _match_regex ( regex , vars ( self ) ) "}
{"13034": "\ndef display ( self ) : \n    if isinstance ( self . name , six . string_types ) and 0 < len ( self . name ) : \n        return '{0} ({1})' . format ( self . name , self . public_ip ) \n    else : \n        return self . public_ip "}
{"13035": "\ndef render_entries ( cls , entries , additional_columns = None , only_show = None , numbers = False ) : \n    additional_columns = additional_columns or [ ] \n    if only_show is not None : \n        columns = _uniquify ( only_show ) \n    else : \n        columns = _uniquify ( cls . DEFAULT_COLUMNS + additional_columns ) \n    top_row = [ cls . prettyname ( col ) for col in columns ] \n    table = [ top_row ] if numbers is False else [ [ '' ] + top_row ] \n    for i , entry in enumerate ( entries ) : \n        row = [ entry . _get_attrib ( c , convert_to_str = True ) for c in columns ] \n        table . append ( row if numbers is False else [ i ] + row ) \n    cur_width = get_current_terminal_width ( ) \n    colors = [ get_color_hash ( c , MIN_COLOR_BRIGHT , MAX_COLOR_BRIGHT ) for c in columns ] \n    if get_table_width ( table ) <= cur_width : \n        return render_table ( table , column_colors = colors if numbers is False else [ green ] + colors ) \n    else : \n        result = [ ] \n        first_index = 1 if numbers is True else 0 \n        for row in table [ 1 : ] : \n            rep = [ green ( '%s:' % row [ 0 ] if numbers is True else '-----' ) ] \n            for i , val in enumerate ( row [ first_index : ] ) : \n                color = colors [ i - 1 if numbers is True else i ] \n                name = columns [ i ] \n                rep . append ( '  %s: %s' % ( name , color ( val ) ) ) \n            result . append ( '\\n' . join ( rep ) ) \n        return '\\n' . join ( result ) "}
{"13050": "\ndef prepare_rows ( table ) : \n    num_columns = max ( len ( row ) for row in table ) \n    for row in table : \n        while num_columns > len ( row ) : \n            row . append ( '' ) \n        for i in range ( num_columns ) : \n            row [ i ] = str ( row [ i ] ) if row [ i ] is not None else '' \n    return table "}
{"13073": "\ndef _copy_from ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    paths = set ( ) \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        _local_path = entry . format_string ( local_path ) \n        if _local_path in paths : \n            raise ValueError ( 'Duplicate local paths: one or more paths ' 'had value {} after formatting.' . format ( local_path ) ) \n        paths . add ( _local_path ) \n        _folder = os . path . split ( _local_path ) [ 0 ] \n        if 0 < len ( _folder ) : \n            if not os . path . exists ( _folder ) : \n                print ( 'Creating directory ' + _folder ) \n                os . makedirs ( _folder ) \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = True , local_path = _local_path , remote_path = remote_path ) \n        print ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    print ( green ( 'Finished copying' ) ) "}
{"13076": "\ndef load ( cls , profile_name = None ) : \n    lsi_location = os . path . expanduser ( '~/.lsi' ) \n    if not os . path . exists ( lsi_location ) : \n        return LsiProfile ( ) \n    cfg_parser = ConfigParser ( ) \n    cfg_parser . read ( lsi_location ) \n    if profile_name is None : \n        if cfg_parser . has_section ( 'default' ) : \n            profile_name = 'default' \n        else : \n            return cls ( ) \n    elif not cfg_parser . has_section ( profile_name ) : \n        raise cls . LoadError ( 'No such profile {}' . format ( profile_name ) ) \n    def _get ( option , alt = None ) : \n        if cfg_parser . has_option ( profile_name , option ) : \n            return cfg_parser . get ( profile_name , option ) \n        else : \n            return alt \n    if cfg_parser . has_option ( profile_name , 'inherit' ) : \n        profile = cls . load ( cfg_parser . get ( profile_name , 'inherit' ) ) \n    else : \n        profile = cls ( ) \n    profile . override ( 'username' , _get ( 'username' ) ) \n    profile . override ( 'identity_file' , _get ( 'identity file' ) ) \n    profile . override ( 'command' , _get ( 'command' ) ) \n    filters = [ s for s in _get ( 'filters' , '' ) . split ( ',' ) if 0 < len ( s ) ] \n    exclude = [ s for s in _get ( 'exclude' , '' ) . split ( ',' ) if 0 < len ( s ) ] \n    profile . filters . extend ( filters ) \n    profile . exclude . extend ( exclude ) \n    return profile "}
{"13086": "\ndef build ( self , secret_key ) : \n    key = jwk . JWK ( kty = 'oct' , k = base64url_encode ( uuid . UUID ( secret_key ) . bytes ) , ) \n    header = { 'alg' : 'dir' , 'enc' : 'A128GCM' , 'zip' : 'DEF' , 'cty' : 'JWT' , 'kid' : self . _access_key , } \n    now = int ( time . time ( ) ) \n    payload = { 'iat' : now , 'nbf' : now , } \n    if self . _expiration is not None : \n        payload [ 'exp' ] = int ( calendar . timegm ( self . _expiration . utctimetuple ( ) ) ) \n    if 0 < len ( self . _view_identifiers ) : \n        payload [ VIEW_IDENTIFIERS_CLAIM_NAME ] = self . _view_identifiers \n    if 0 < len ( self . _parameters ) : \n        parameters = [ ] \n        for parameter in self . _parameters : \n            serialized = { 'field' : parameter . field , 'op' : parameter . op , } \n            if hasattr ( parameter , '__iter__' ) : \n                serialized [ 'any' ] = list ( parameter . value ) \n            else : \n                serialized [ 'value' ] = parameter . value \n            parameters . append ( serialized ) \n        payload [ PARAMETERS_CLAIM_NAME ] = parameters \n    if 0 < len ( self . _attributes ) : \n        payload [ ATTRIBUTES_CLAIM_NAME ] = self . _attributes \n    tok = jwe . JWE ( json_encode ( payload ) , protected = header ) \n    tok . add_recipient ( key ) \n    return tok . serialize ( compact = True ) "}
{"13088": "\ndef find_max_rad_npnp ( self ) : \n    max_rad = 0 \n    max_npnp = 0 \n    for res , _ in self . items ( ) : \n        if res != 'KEY' : \n            for _ , ff_params in self [ res ] . items ( ) : \n                if ff_params [ 1 ] > max_rad : \n                    max_rad = ff_params [ 1 ] \n                if ff_params [ 4 ] > max_npnp : \n                    max_npnp = ff_params [ 4 ] \n    return max_rad , max_npnp "}
{"13095": "\ndef upload_file ( local_path , bucket_path , bucket , metadata = None , acl = None , cache_control = None ) : \n    logger = logging . getLogger ( __name__ ) \n    extra_args = { } \n    if acl is not None : \n        extra_args [ 'ACL' ] = acl \n    if metadata is not None and 0 < len ( metadata ) : \n        extra_args [ 'Metadata' ] = metadata \n    if cache_control is not None : \n        extra_args [ 'CacheControl' ] = cache_control \n    content_type , content_encoding = mimetypes . guess_type ( local_path , strict = False ) \n    if content_type is not None : \n        extra_args [ 'ContentType' ] = content_type \n    logger . debug ( str ( extra_args ) ) \n    obj = bucket . Object ( bucket_path ) \n    obj . upload_file ( local_path , ExtraArgs = extra_args ) "}
{"13096": "\ndef upload_object ( bucket_path , bucket , content = '' , metadata = None , acl = None , cache_control = None , content_type = None ) : \n    obj = bucket . Object ( bucket_path ) \n    args = { } \n    if metadata is not None and 0 < len ( metadata ) : \n        args [ 'Metadata' ] = metadata \n    if acl is not None : \n        args [ 'ACL' ] = acl \n    if cache_control is not None : \n        args [ 'CacheControl' ] = cache_control \n    if content_type is not None : \n        args [ 'ContentType' ] = content_type \n    obj . put ( Body = content , ** args ) "}
{"13103": "\ndef delete_dir ( bucket_name , root_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None ) : \n    logger = logging . getLogger ( __name__ ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key ) \n    s3 = session . resource ( 's3' ) \n    client = s3 . meta . client \n    if not root_path . endswith ( '/' ) : \n        root_path . rstrip ( '/' ) \n    paginator = client . get_paginator ( 'list_objects_v2' ) \n    pages = paginator . paginate ( Bucket = bucket_name , Prefix = root_path ) \n    keys = dict ( Objects = [ ] ) \n    for item in pages . search ( 'Contents' ) : \n        try : \n            keys [ 'Objects' ] . append ( { 'Key' : item [ 'Key' ] } ) \n        except TypeError : \n            continue \n        if 1000 <= len ( keys [ 'Objects' ] ) : \n            try : \n                client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n            except Exception : \n                message = 'Error deleting objects from %r' % root_path \n                logger . exception ( message ) \n                raise S3Error ( message ) \n            keys = dict ( Objects = [ ] ) \n    if 0 < len ( keys [ 'Objects' ] ) : \n        try : \n            client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n        except Exception : \n            message = 'Error deleting objects from %r' % root_path \n            logger . exception ( message ) \n            raise S3Error ( message ) "}
{"13124": "\ndef process_module ( self , node ) : \n    if self . config . file_header : \n        if 3 > sys . version_info [ 0 ] : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . LOCALE | re . MULTILINE ) \n        else : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . MULTILINE ) \n        content = None \n        with node . stream ( ) as stream : \n            content = stream . read ( ) . decode ( 'utf-8' ) \n        matches = pattern . findall ( content ) \n        if len ( matches ) != 1 : \n            self . add_message ( 'invalid-file-header' , 1 , args = self . config . file_header ) "}
{"13133": "\ndef _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = False ) ) : \n    if scale is None : \n        scale = Scale ( ) \n    xfieldtype = xfield [ 1 ] \n    yfieldtype = yfield [ 1 ] \n    x_options = None \n    if 2 < len ( xfield ) : \n        x_options = xfield [ 2 ] \n    y_options = None \n    if 2 < len ( yfield ) : \n        y_options = yfield [ 2 ] \n    if time_unit is not None : \n        if x_options is None : \n            xencode = X ( xfieldtype , timeUnit = time_unit ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , timeUnit = time_unit , scale = scale ) \n    else : \n        if x_options is None : \n            xencode = X ( xfieldtype ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , scale = scale ) \n    if y_options is None : \n        yencode = Y ( yfieldtype , scale = scale ) \n    else : \n        yencode = Y ( yfieldtype , axis = Axis ( ** y_options ) , scale = scale ) \n    return xencode , yencode "}
{"13138": "\ndef iter_attribute ( iterable_name ) -> Union [ Iterable , Callable ] : \n    def create_new_class ( decorated_class ) -> Union [ Iterable , Callable ] : \n        assert inspect . isclass ( decorated_class ) , 'You can only decorate class objects!' \n        assert isinstance ( iterable_name , str ) , 'Please provide attribute name string' \n        decorated_class . iterator_attr_index = 0 \n        def __iter__ ( instance ) -> Iterable : \n            return instance \n        def __next__ ( instance ) -> Any : \n            assert hasattr ( instance , iterable_name ) , 'Decorated object does not have attribute named {}' . format ( iterable_name ) \n            assert isinstance ( getattr ( instance , iterable_name ) , collections . Iterable ) , '{} of object {} is not iterable' . format ( iterable_name , instance . __class__ . __name__ ) \n            ind = instance . iterator_attr_index \n            while len ( getattr ( instance , iterable_name ) ) > ind : \n                val = getattr ( instance , iterable_name ) [ ind ] \n                instance . iterator_attr_index += 1 \n                return val \n            instance . iterator_attr_index = 0 \n            raise StopIteration \n        dct = dict ( decorated_class . __dict__ ) \n        dct [ '__iter__' ] = __iter__ \n        dct [ '__next__' ] = __next__ \n        dct [ 'iterator_attr_index' ] = decorated_class . iterator_attr_index \n        return type ( decorated_class . __name__ , ( collections . Iterable , ) , dct ) \n    return create_new_class "}
{"13148": "\ndef unique ( func , num_args = 0 , max_attempts = 100 , cache = None ) : \n    if cache is None : \n        cache = _cache_unique \n    \n    @ wraps ( func ) \n    def wrapper ( * args ) : \n        key = \"%s_%s\" % ( str ( func . __name__ ) , str ( args [ : num_args ] ) ) \n        attempt = 0 \n        while max_attempts > attempt : \n            attempt += 1 \n            drawn = cache . get ( key , [ ] ) \n            result = func ( * args ) \n            if result not in drawn : \n                drawn . append ( result ) \n                cache [ key ] = drawn \n                return result \n        raise MaxAttemptException ( ) \n    return wrapper "}
{"13154": "\ndef encode ( self , * args , ** kwargs ) : \n    if isinstance ( args [ 0 ] , str ) : \n        return self . encode ( [ args [ 0 ] ] , ** kwargs ) \n    elif isinstance ( args [ 0 ] , int ) or isinstance ( args [ 0 ] , float ) : \n        return self . encode ( [ [ args [ 0 ] ] ] , ** kwargs ) \n    if 1 < len ( args ) : \n        dataset = args \n    else : \n        dataset = args [ 0 ] \n    typemap = list ( map ( type , dataset ) ) \n    code = self . encoding [ 0 ] \n    if type ( '' ) in typemap : \n        data = ',' . join ( map ( str , dataset ) ) \n    elif type ( [ ] ) in typemap or type ( ( ) ) in typemap : \n        data = self . codeset [ 'char' ] . join ( map ( self . encodedata , dataset ) ) \n    elif len ( dataset ) == 1 and hasattr ( dataset [ 0 ] , '__iter__' ) : \n        data = self . encodedata ( dataset [ 0 ] ) \n    else : \n        try : \n            data = self . encodedata ( dataset ) \n        except ValueError : \n            data = self . encodedata ( ',' . join ( map ( unicode , dataset ) ) ) \n    if not '.' in data and code == 't' : \n        code = 'e' \n    return '%s%s:%s' % ( code , self . series , data ) "}
{"13192": "\ndef anagrams_in_word ( word , sowpods = False , start = \"\" , end = \"\" ) : \n    input_letters , blanks , questions = blank_tiles ( word ) \n    for tile in start + end : \n        input_letters . append ( tile ) \n    for word in word_list ( sowpods , start , end ) : \n        lmap = _letter_map ( input_letters ) \n        used_blanks = 0 \n        for letter in word : \n            if letter in lmap : \n                lmap [ letter ] -= 1 \n                if 0 > lmap [ letter ] : \n                    used_blanks += 1 \n                    if ( blanks + questions ) < used_blanks : \n                        break \n            else : \n                used_blanks += 1 \n                if ( blanks + questions ) < used_blanks : \n                    break \n        else : \n            yield ( word , word_score ( word , input_letters , questions ) ) "}
{"13195": "\ndef get_last_value_from_timeseries ( timeseries ) : \n    if not timeseries : \n        return 0 \n    for metric , points in timeseries . items ( ) : \n        return next ( ( p [ 'y' ] for p in reversed ( points ) if 0 < p [ 'y' ] ) , 0 ) "}
{"13196": "\ndef validate_page_number ( number ) : \n    try : \n        number = int ( number ) \n    except ( TypeError , ValueError ) : \n        raise PageNotAnInteger ( 'That page number is not an integer' ) \n    if 1 > number : \n        raise EmptyPage ( 'That page number is less than 1' ) \n    return number "}
{"13197": "\ndef get_page_of_iterator ( iterator , page_size , page_number ) : \n    try : \n        page_number = validate_page_number ( page_number ) \n    except ( PageNotAnInteger , EmptyPage ) : \n        page_number = 1 \n    start = ( page_number - 1 ) * page_size \n    end = ( page_number * page_size ) + 1 \n    skipped_items = list ( islice ( iterator , start ) ) \n    items = list ( islice ( iterator , end ) ) \n    if len ( items ) == 0 and page_number != 1 : \n        items = skipped_items \n        page_number = 1 \n    has_next = page_size < len ( items ) \n    items = items [ : page_size ] \n    return NoCountPage ( items , page_number , page_size , has_next ) "}
{"13258": "\ndef unpad ( padded_data , block_size , style = 'pkcs7' ) : \n    pdata_len = len ( padded_data ) \n    if pdata_len % block_size : \n        raise ValueError ( \"Input data is not padded\" ) \n    if style in ( 'pkcs7' , 'x923' ) : \n        padding_len = bord ( padded_data [ - 1 ] ) \n        if 1 > padding_len or min ( block_size , pdata_len ) < padding_len : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if style == 'pkcs7' : \n            if padded_data [ - padding_len : ] != bchr ( padding_len ) * padding_len : \n                raise ValueError ( \"PKCS#7 padding is incorrect.\" ) \n        else : \n            if padded_data [ - padding_len : - 1 ] != bchr ( 0 ) * ( padding_len - 1 ) : \n                raise ValueError ( \"ANSI X.923 padding is incorrect.\" ) \n    elif style == 'iso7816' : \n        padding_len = pdata_len - padded_data . rfind ( bchr ( 128 ) ) \n        if 1 > padding_len or min ( block_size , pdata_len ) < padding_len : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if 1 < padding_len and padded_data [ 1 - padding_len : ] != bchr ( 0 ) * ( padding_len - 1 ) : \n            raise ValueError ( \"ISO 7816-4 padding is incorrect.\" ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return padded_data [ : - padding_len ] "}
{"13264": "\ndef dataReceived ( self , data ) : \n    self . _unprocessed_data . enqueue ( data ) \n    while True : \n        if self . _header . size > len ( self . _unprocessed_data ) : \n            return \n        hdr_data = self . _unprocessed_data . peek ( self . _header . size ) \n        packet_length , typekey = self . _header . unpack ( hdr_data ) \n        total_length = self . _header . size + packet_length \n        if total_length > len ( self . _unprocessed_data ) : \n            return \n        self . _unprocessed_data . drop ( self . _header . size ) \n        packet = self . _unprocessed_data . dequeue ( packet_length ) \n        self . _start_receive = None \n        typename = self . _type_register . get ( typekey , None ) \n        if typename is None : \n            self . on_unregistered_type ( typekey , packet ) \n        else : \n            self . packet_received ( typename , packet ) "}
{"13278": "\ndef login ( request , template_name = 'ci/login.html' , redirect_field_name = REDIRECT_FIELD_NAME , authentication_form = AuthenticationForm ) : \n    redirect_to = request . POST . get ( redirect_field_name , request . GET . get ( redirect_field_name , '' ) ) \n    if request . method == \"POST\" : \n        form = authentication_form ( request , data = request . POST ) \n        if form . is_valid ( ) : \n            if not is_safe_url ( url = redirect_to , host = request . get_host ( ) ) : \n                redirect_to = resolve_url ( settings . LOGIN_REDIRECT_URL ) \n            user = form . get_user ( ) \n            request . session [ 'user_token' ] = user [ \"token\" ] \n            request . session [ 'user_email' ] = user [ \"email\" ] \n            request . session [ 'user_permissions' ] = user [ \"permissions\" ] \n            request . session [ 'user_id' ] = user [ \"id\" ] \n            request . session [ 'user_list' ] = user [ \"user_list\" ] \n            if not settings . HIDE_DASHBOARDS : \n                dashboards = ciApi . get_user_dashboards ( user [ \"id\" ] ) \n                dashboard_list = list ( dashboards [ 'results' ] ) \n                if 0 < len ( dashboard_list ) : \n                    request . session [ 'user_dashboards' ] = dashboard_list [ 0 ] [ \"dashboards\" ] \n                    request . session [ 'user_default_dashboard' ] = dashboard_list [ 0 ] [ \"default_dashboard\" ] [ \"id\" ] \n                else : \n                    request . session [ 'user_dashboards' ] = [ ] \n                    request . session [ 'user_default_dashboard' ] = None \n            tokens = ciApi . get_user_service_tokens ( params = { \"user_id\" : user [ \"id\" ] } ) \n            token_list = list ( tokens [ 'results' ] ) \n            user_tokens = { } \n            if 0 < len ( token_list ) : \n                for token in token_list : \n                    user_tokens [ token [ \"service\" ] [ \"name\" ] ] = { \"token\" : token [ \"token\" ] , \"url\" : token [ \"service\" ] [ \"url\" ] + \"/api/v1\" } \n            request . session [ 'user_tokens' ] = user_tokens \n            return HttpResponseRedirect ( redirect_to ) \n    else : \n        form = authentication_form ( request ) \n    current_site = get_current_site ( request ) \n    context = { 'form' : form , redirect_field_name : redirect_to , 'site' : current_site , 'site_name' : current_site . name , } \n    return TemplateResponse ( request , template_name , context ) "}
{"13288": "\ndef word_score ( word , input_letters , questions = 0 ) : \n    score = 0 \n    bingo = 0 \n    filled_by_blanks = [ ] \n    rack = list ( input_letters ) \n    for letter in word : \n        if letter in rack : \n            bingo += 1 \n            score += letter_score ( letter ) \n            rack . remove ( letter ) \n        else : \n            filled_by_blanks . append ( letter_score ( letter ) ) \n    for blank_score in sorted ( filled_by_blanks , reverse = True ) : \n        if 0 < questions : \n            score += blank_score \n            questions -= 1 \n    if 6 < bingo : \n        score += 50 \n    return score "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9 , \"b\" : 2 , \"c\" : 2 , \"d\" : 4 , \"e\" : 12 , \"f\" : 2 , \"g\" : 3 , \"h\" : 2 , \"i\" : 9 , \"j\" : 1 , \"k\" : 1 , \"l\" : 4 , \"m\" : 2 , \"n\" : 6 , \"o\" : 8 , \"p\" : 2 , \"q\" : 1 , \"r\" : 6 , \"s\" : 4 , \"t\" : 6 , \"u\" : 4 , \"v\" : 2 , \"w\" : 2 , \"x\" : 1 , \"y\" : 2 , \"z\" : 1 , \"_\" : 2 , } \n    for letter in word : \n        if letter == \"?\" : \n            continue \n        try : \n            letters_in_bag [ letter ] -= 1 \n        except KeyError : \n            return False \n        if 0 > letters_in_bag [ letter ] : \n            letters_in_bag [ \"_\" ] -= 1 \n            if 0 > letters_in_bag [ \"_\" ] : \n                return False \n    return True "}
{"13327": "\ndef download_music ( song , thread_num = 4 ) : \n    filename = \"{}.mp3\" . format ( song [ \"name\" ] ) \n    if os . path . exists ( filename ) : \n        os . remove ( filename ) \n    part = int ( song [ \"size\" ] / thread_num ) \n    if 1024 >= part : \n        thread_num = 1 \n    _id = uuid . uuid4 ( ) . hex \n    logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) \n    threads = [ ] \n    for i in range ( thread_num ) : \n        if i == thread_num - 1 : \n            end = '' \n        else : \n            end = ( i + 1 ) * part - 1 \n        thread = Worker ( ( i * part , end ) , song , _id ) \n        thread . start ( ) \n        threads . append ( thread ) \n    for t in threads : \n        t . join ( ) \n    fileParts = glob . glob ( \"part-{}-*\" . format ( _id ) ) \n    fileParts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) \n    logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) \n    with open ( filename , \"ab\" ) as f : \n        for part in fileParts : \n            with open ( part , \"rb\" ) as d : \n                shutil . copyfileobj ( d , f ) \n            os . remove ( part ) \n    logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) ) "}
{"13339": "\ndef is_changed ( self , item ) : \n    fname = os . path . join ( self . fdir , item ) \n    if os . path . isfile ( fname ) : \n        mtime = self . get_mtime ( fname ) \n        try : \n            _ftime = self . fmtime [ item ] \n        except KeyError : \n            self . fmtime [ item ] = mtime \n            return True \n        if _ftime < mtime : \n            self . fmtime [ item ] = mtime \n            return True \n        else : \n            return False \n    else : \n        logger . error ( 'Could not access {}' . format ( fname ) ) \n        raise KeyError ( item ) "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ 1 : ] \n    if takes_context : \n        if 'context' in params [ : 1 ] : \n            params = params [ 1 : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = False \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if 1 < len ( bits ) and bits [ - 2 ] == 'as' : \n        output_var = bits [ - 1 ] \n        if 0 < len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            elif kw in handled_params : \n                raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                kwargs [ str ( kw ) ] = var \n                kwarg_found = True \n                handled_params . append ( kw ) \n        else : \n            if kwarg_found : \n                raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n            else : \n                args . append ( bit ) \n                try : \n                    handled_params . append ( unhandled_params . pop ( 0 ) ) \n                except IndexError : \n                    if not takes_var_args : \n                        raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if len ( unhandled_params ) == 1 : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ 0 ] ) ) \n    elif 1 < len ( unhandled_params ) : \n        raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13407": "\ndef check_link_integrity ( m , link ) : \n    res = 0 \n    for inst in link . from_metaclass . select_many ( ) : \n        q_set = list ( link . navigate ( inst ) ) \n        if ( 1 > len ( q_set ) and not link . conditional ) or ( ( 1 < len ( q_set ) and not link . many ) ) : \n            res += 1 \n            logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty_from_link ( inst , link ) , link . rel_id , pretty_to_link ( inst , link ) ) ) \n    return res "}
{"13439": "\ndef track_production ( f ) : \n    \n    @ wraps ( f ) \n    def wrapper ( self , p ) : \n        r = f ( self , p ) \n        node = p [ 0 ] \n        if isinstance ( node , Node ) and 1 < len ( p ) : \n            set_positional_info ( node , p ) \n        return r \n    return wrapper "}
{"13460": "\ndef text_visible ( self ) : \n    words = self . read ( ) . split ( ) \n    for word in words : \n        if word . lstrip ( '-' ) . replace ( '.' , '' , 1 ) . isdigit ( ) : \n            return True \n        if word . isalpha ( ) and ( 1 < len ( word ) or 20 >= len ( word ) ) : \n            return True \n    return False "}
{"13466": "\ndef search ( self , query , verbose = 0 ) : \n    if 0 < verbose : \n        print ( \"searching \" + query ) \n    query = query . lower ( ) \n    qgram = ng ( query , self . slb ) \n    qocument = set ( ) \n    for q in qgram : \n        if q in self . ngrams . keys ( ) : \n            for i in self . ngrams [ q ] : \n                qocument . add ( i ) \n    self . qocument = qocument \n    results = { } \n    for i in qocument : \n        for j in self . D [ i ] . keys ( ) : \n            if not j in results . keys ( ) : \n                results [ j ] = 0 \n            results [ j ] = results [ j ] + self . D [ i ] [ j ] \n    sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n    return [ self . elements [ f [ 0 ] ] for f in sorted_results ] "}
{"13468": "\ndef run ( locations , random , bikes , crime , nearby , json , update_bikes , api_server , cross_origin , host , port , db_path , verbose ) : \n    log_levels = [ logging . WARNING , logging . INFO , logging . DEBUG ] \n    logging . basicConfig ( level = log_levels [ min ( verbose , 2 ) ] ) \n    initialize_database ( db_path ) \n    loop = get_event_loop ( ) \n    if update_bikes : \n        logger . info ( \"Force updating bikes.\" ) \n        loop . run_until_complete ( util . update_bikes ( ) ) \n    if api_server : \n        if cross_origin : \n            enable_cross_origin ( app ) \n        try : \n            web . run_app ( app , host = host , port = port ) \n        except CancelledError as e : \n            if e . __context__ is not None : \n                click . echo ( Fore . RED + ( f\"Could not bind to address {host}:{port}\" if e . __context__ . errno == 48 else e . __context__ ) ) \n                exit ( 1 ) \n            else : \n                click . echo ( \"Goodbye!\" ) \n    elif 0 < len ( locations ) or 0 < random : \n        exit ( loop . run_until_complete ( cli ( locations , random , bikes = bikes , crime = crime , nearby = nearby , as_json = json ) ) ) \n    else : \n        click . echo ( Fore . RED + \"Either include a post code, or the --api-server flag.\" ) "}
{"13480": "\ndef dead_code ( ) : \n    with safe_cd ( SRC ) : \n        if IS_TRAVIS : \n            command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) \n        else : \n            command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) \n        output_file_name = \"dead_code.txt\" \n        with open ( output_file_name , \"w\" ) as outfile : \n            env = config_pythonpath ( ) \n            subprocess . call ( command , stdout = outfile , env = env ) \n        cutoff = 20 \n        num_lines = sum ( 1 for line in open ( output_file_name ) if line ) \n        if cutoff < num_lines : \n            print ( \"Too many lines of dead code : {0}, max {1}\" . format ( num_lines , cutoff ) ) \n            exit ( - 1 ) "}
{"13491": "\ndef _find_match ( self , position ) : \n    document = self . _text_edit . document ( ) \n    start_char = document . characterAt ( position ) \n    search_char = self . _opening_map . get ( start_char ) \n    if search_char : \n        increment = 1 \n    else : \n        search_char = self . _closing_map . get ( start_char ) \n        if search_char : \n            increment = - 1 \n        else : \n            return - 1 \n    char = start_char \n    depth = 0 \n    while 0 <= position and document . characterCount ( ) > position : \n        if char == start_char : \n            depth += 1 \n        elif char == search_char : \n            depth -= 1 \n        if depth == 0 : \n            break \n        position += increment \n        char = document . characterAt ( position ) \n    else : \n        position = - 1 \n    return position "}
{"13505": "\ndef history_previous ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while 0 < index : \n        index -= 1 \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13506": "\ndef history_next ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while len ( self . _history ) > self . _history_index : \n        index += 1 \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13537": "\ndef _current_text_cursor ( self ) : \n    cursor = self . _text_edit . textCursor ( ) \n    if self . _start_position <= cursor . position ( ) : \n        cursor . setPosition ( self . _start_position , QtGui . QTextCursor . KeepAnchor ) \n    return cursor "}
{"13545": "\ndef update_tab_bar_visibility ( self ) : \n    if 1 >= self . tab_widget . count ( ) : \n        self . tab_widget . tabBar ( ) . setVisible ( False ) \n    else : \n        self . tab_widget . tabBar ( ) . setVisible ( True ) \n    if self . tab_widget . count ( ) == 0 : \n        self . close ( ) "}
{"13551": "\ndef closeEvent ( self , event ) : \n    if self . tab_widget . count ( ) == 0 : \n        event . accept ( ) \n        return \n    title = self . window ( ) . windowTitle ( ) \n    cancel = QtGui . QMessageBox . Cancel \n    okay = QtGui . QMessageBox . Ok \n    if self . confirm_exit : \n        if 1 < self . tab_widget . count ( ) : \n            msg = \"Close all tabs, stop all kernels, and Quit?\" \n        else : \n            msg = \"Close console, stop kernel, and Quit?\" \n        info = \"Kernels not started here (e.g. notebooks) will be left alone.\" \n        closeall = QtGui . QPushButton ( \"&Quit\" , self ) \n        closeall . setShortcut ( 'Q' ) \n        box = QtGui . QMessageBox ( QtGui . QMessageBox . Question , title , msg ) \n        box . setInformativeText ( info ) \n        box . addButton ( cancel ) \n        box . addButton ( closeall , QtGui . QMessageBox . YesRole ) \n        box . setDefaultButton ( closeall ) \n        box . setEscapeButton ( cancel ) \n        pixmap = QtGui . QPixmap ( self . _app . icon . pixmap ( QtCore . QSize ( 64 , 64 ) ) ) \n        box . setIconPixmap ( pixmap ) \n        reply = box . exec_ ( ) \n    else : \n        reply = okay \n    if reply == cancel : \n        event . ignore ( ) \n        return \n    if reply == okay : \n        while 1 <= self . tab_widget . count ( ) : \n            widget = self . active_frontend \n            widget . _confirm_exit = False \n            self . close_tab ( widget ) \n        event . accept ( ) "}
{"13563": "\ndef validate_tree ( G , results ) : \n    for node in G : \n        started = results [ node ] . metadata . started \n        for parent in G . predecessors ( node ) : \n            finished = results [ parent ] . metadata . completed \n            assert finished < started , \"%s should have happened after %s\" % ( node , parent ) "}
{"13571": "\ndef wait_for_kernel ( self , timeout = None ) : \n    tic = time . time ( ) \n    self . km . hb_channel . unpause ( ) \n    while True : \n        self . run_cell ( '1' , False ) \n        if self . km . hb_channel . is_beating ( ) : \n            break \n        else : \n            if timeout is not None and timeout < ( time . time ( ) - tic ) : \n                return False \n    return True "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if indent_spaces > inisp : \n        indent_spaces = inisp \n        if 0 >= indent_spaces : \n            full_dedent = True \n    if line . rstrip ( ) [ - 1 ] == ':' : \n        indent_spaces += 4 \n    elif dedent_re . match ( line ) : \n        indent_spaces -= 4 \n        if 0 >= indent_spaces : \n            full_dedent = True \n    if 0 > indent_spaces : \n        indent_spaces = 0 \n    return indent_spaces , full_dedent "}
{"13618": "\ndef warn ( msg , level = 2 , exit_val = 1 ) : \n    if 0 < level : \n        header = [ '' , '' , 'WARNING: ' , 'ERROR: ' , 'FATAL ERROR: ' ] \n        io . stderr . write ( '%s%s' % ( header [ level ] , msg ) ) \n        if level == 4 : \n            print >> io . stderr , 'Exiting.\\n' \n            sys . exit ( exit_val ) "}
{"13641": "\ndef command_line ( self , argv ) : \n    if not argv : \n        self . help_fn ( topic = 'minimum_help' ) \n        return OK \n    self . classic = argv [ 0 ] . startswith ( '-' ) \n    if self . classic : \n        parser = ClassicOptionParser ( ) \n    else : \n        parser = CMDS . get ( argv [ 0 ] ) \n        if not parser : \n            self . help_fn ( \"Unknown command: '%s'\" % argv [ 0 ] ) \n            return ERR \n        argv = argv [ 1 : ] \n    parser . help_fn = self . help_fn \n    ok , options , args = parser . parse_args ( argv ) \n    if not ok : \n        return ERR \n    if self . do_help ( options , args , parser ) : \n        return OK \n    if not self . args_ok ( options , args ) : \n        return ERR \n    source = unshell_list ( options . source ) \n    omit = unshell_list ( options . omit ) \n    include = unshell_list ( options . include ) \n    debug = unshell_list ( options . debug ) \n    self . coverage = self . covpkg . coverage ( data_suffix = options . parallel_mode , cover_pylib = options . pylib , timid = options . timid , branch = options . branch , config_file = options . rcfile , source = source , omit = omit , include = include , debug = debug , ) \n    if 'debug' in options . actions : \n        return self . do_debug ( args ) \n    if 'erase' in options . actions or options . erase_first : \n        self . coverage . erase ( ) \n    else : \n        self . coverage . load ( ) \n    if 'execute' in options . actions : \n        self . do_execute ( options , args ) \n    if 'combine' in options . actions : \n        self . coverage . combine ( ) \n        self . coverage . save ( ) \n    report_args = dict ( morfs = args , ignore_errors = options . ignore_errors , omit = omit , include = include , ) \n    if 'report' in options . actions : \n        total = self . coverage . report ( show_missing = options . show_missing , ** report_args ) \n    if 'annotate' in options . actions : \n        self . coverage . annotate ( directory = options . directory , ** report_args ) \n    if 'html' in options . actions : \n        total = self . coverage . html_report ( directory = options . directory , title = options . title , ** report_args ) \n    if 'xml' in options . actions : \n        outfile = options . outfile \n        total = self . coverage . xml_report ( outfile = outfile , ** report_args ) \n    if options . fail_under is not None : \n        if options . fail_under <= total : \n            return OK \n        else : \n            return FAIL_UNDER \n    else : \n        return OK "}
{"13707": "\ndef deprecated ( conditional = True ) : \n    def deprecate_decorator ( f ) : \n        import nose \n        def _deprecated_imp ( * args , ** kwargs ) : \n            ctx = WarningManager ( record = True ) \n            l = ctx . __enter__ ( ) \n            warnings . simplefilter ( 'always' ) \n            try : \n                f ( * args , ** kwargs ) \n                if not 0 < len ( l ) : \n                    raise AssertionError ( \"No warning raised when calling %s\" % f . __name__ ) \n                if not l [ 0 ] . category is DeprecationWarning : \n                    raise AssertionError ( \"First warning for %s is not a \" \"DeprecationWarning( is %s)\" % ( f . __name__ , l [ 0 ] ) ) \n            finally : \n                ctx . __exit__ ( ) \n        if callable ( conditional ) : \n            cond = conditional ( ) \n        else : \n            cond = conditional \n        if cond : \n            return nose . tools . make_decorator ( f ) ( _deprecated_imp ) \n        else : \n            return f \n    return deprecate_decorator "}
{"13720": "\ndef search ( self , buffer , freshlen , searchwindowsize = None ) : \n    absurd_match = len ( buffer ) \n    first_match = absurd_match \n    for index , s in self . _strings : \n        if searchwindowsize is None : \n            offset = - ( freshlen + len ( s ) ) \n        else : \n            offset = - searchwindowsize \n        n = buffer . find ( s , offset ) \n        if 0 <= n and first_match > n : \n            first_match = n \n            best_index , best_match = index , s \n    if first_match == absurd_match : \n        return - 1 \n    self . match = best_match \n    self . start = first_match \n    self . end = self . start + len ( self . match ) \n    return best_index "}
{"13721": "\ndef search ( self , buffer , freshlen , searchwindowsize = None ) : \n    absurd_match = len ( buffer ) \n    first_match = absurd_match \n    if searchwindowsize is None : \n        searchstart = 0 \n    else : \n        searchstart = max ( 0 , len ( buffer ) - searchwindowsize ) \n    for index , s in self . _searches : \n        match = s . search ( buffer , searchstart ) \n        if match is None : \n            continue \n        n = match . start ( ) \n        if first_match > n : \n            first_match = n \n            the_match = match \n            best_index = index \n    if first_match == absurd_match : \n        return - 1 \n    self . start = first_match \n    self . match = the_match \n    self . end = self . match . end ( ) \n    return best_index "}
{"13724": "\ndef emit ( self , msg , level = 1 , debug = False ) : \n    if debug : \n        if not self . debug : \n            return \n        stream = sys . stderr \n    else : \n        if level > self . verbose : \n            return \n        stream = sys . stdout \n    print ( msg , file = stream ) \n    stream . flush ( ) "}
{"13731": "\ndef branch_lines ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    return [ l1 for l1 , count in iitems ( exit_counts ) if 1 < count ] "}
{"13732": "\ndef total_branches ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    return sum ( [ count for count in exit_counts . values ( ) if 1 < count ] ) "}
{"13736": "\ndef _get_pc_covered ( self ) : \n    if 0 < self . n_statements : \n        pc_cov = ( 100.0 * ( self . n_executed + self . n_executed_branches ) / ( self . n_statements + self . n_branches ) ) \n    else : \n        pc_cov = 100.0 \n    return pc_cov "}
{"13744": "\ndef marquee ( txt = '' , width = 78 , mark = '*' ) : \n    if not txt : \n        return ( mark * width ) [ : width ] \n    nmark = ( width - len ( txt ) - 2 ) // len ( mark ) // 2 \n    if 0 > nmark : \n        nmark = 0 \n    marks = mark * nmark \n    return '%s %s %s' % ( marks , txt , marks ) "}
{"13748": "\ndef _find_optimal ( rlist , separator_size = 2 , displaywidth = 80 ) : \n    for nrow in range ( 1 , len ( rlist ) + 1 ) : \n        chk = map ( max , _chunks ( rlist , nrow ) ) \n        sumlength = sum ( chk ) \n        ncols = len ( chk ) \n        if displaywidth >= sumlength + separator_size * ( ncols - 1 ) : \n            break ; \n    return { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows_numbers' : nrow , 'columns_width' : chk } "}
{"13749": "\ndef _get_or_default ( mylist , i , default = None ) : \n    if len ( mylist ) <= i : \n        return default \n    else : \n        return mylist [ i ] "}
{"13801": "\ndef parse_command_line ( self , argv = None ) : \n    argv = sys . argv [ 1 : ] if argv is None else argv \n    if argv and argv [ 0 ] == 'help' : \n        argv = argv [ 1 : ] + [ '-h' ] \n    if self . subcommands and 0 < len ( argv ) : \n        subc , subargv = argv [ 0 ] , argv [ 1 : ] \n        if re . match ( r'^\\w(\\-?\\w)*$' , subc ) and subc in self . subcommands : \n            return self . initialize_subcommand ( subc , subargv ) \n    if '-h' in argv or '--help' in argv or '--help-all' in argv : \n        self . print_description ( ) \n        self . print_help ( '--help-all' in argv ) \n        self . print_examples ( ) \n        self . exit ( 0 ) \n    if '--version' in argv or '-V' in argv : \n        self . print_version ( ) \n        self . exit ( 0 ) \n    flags , aliases = self . flatten_flags ( ) \n    loader = KVArgParseConfigLoader ( argv = argv , aliases = aliases , flags = flags ) \n    config = loader . load_config ( ) \n    self . update_config ( config ) \n    self . extra_args = loader . extra_args "}
{"13809": "\ndef class_get_trait_help ( cls , trait , inst = None ) : \n    assert inst is None or isinstance ( inst , cls ) \n    lines = [ ] \n    header = \"--%s.%s=<%s>\" % ( cls . __name__ , trait . name , trait . __class__ . __name__ ) \n    lines . append ( header ) \n    if inst is not None : \n        lines . append ( indent ( 'Current: %r' % getattr ( inst , trait . name ) , 4 ) ) \n    else : \n        try : \n            dvr = repr ( trait . get_default_value ( ) ) \n        except Exception : \n            dvr = None \n        if dvr is not None : \n            if 64 < len ( dvr ) : \n                dvr = dvr [ : 61 ] + '...' \n            lines . append ( indent ( 'Default: %s' % dvr , 4 ) ) \n    if 'Enum' in trait . __class__ . __name__ : \n        lines . append ( indent ( 'Choices: %r' % ( trait . values , ) ) ) \n    help = trait . get_metadata ( 'help' ) \n    if help is not None : \n        help = '\\n' . join ( wrap_paragraphs ( help , 76 ) ) \n        lines . append ( indent ( help , 4 ) ) \n    return '\\n' . join ( lines ) "}
{"13838": "\ndef run ( self , message ) : \n    the_callable = self . func_from_info ( ) \n    try : \n        task_message = dict ( task = self , channel_message = message , ) \n        the_callable ( task_message ) \n    finally : \n        if self . next_run > self . end_running : \n            self . enabled = False \n            Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            return \n        if self . iterations == 0 : \n            return \n        else : \n            self . iterations -= 1 \n            if self . iterations == 0 : \n                self . enabled = False \n                Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            self . save ( ) "}
{"13840": "\ndef run_iterations ( cls , the_callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run_immediately = False , delay_until = None ) : \n    task = task_with_callable ( the_callable , label = label , schedule = schedule , userdata = userdata ) \n    task . iterations = iterations \n    if delay_until is not None : \n        if isinstance ( delay_until , datetime ) : \n            if timezone . now ( ) < delay_until : \n                task . start_running = delay_until \n            else : \n                raise ValueError ( \"Task cannot start running in the past\" ) \n        else : \n            raise ValueError ( \"delay_until must be a datetime.datetime instance\" ) \n    if run_immediately : \n        task . next_run = timezone . now ( ) \n    else : \n        task . calc_next_run ( ) \n    task . save ( ) "}
{"13846": "\ndef run ( self ) : \n    try : \n        from _winapi import WAIT_OBJECT_0 , INFINITE \n    except ImportError : \n        from _subprocess import WAIT_OBJECT_0 , INFINITE \n    handles = [ ] \n    if self . interrupt_handle : \n        handles . append ( self . interrupt_handle ) \n    if self . parent_handle : \n        handles . append ( self . parent_handle ) \n    arch = platform . architecture ( ) [ 0 ] \n    c_int = ctypes . c_int64 if arch . startswith ( '64' ) else ctypes . c_int \n    while True : \n        result = ctypes . windll . kernel32 . WaitForMultipleObjects ( len ( handles ) , ( c_int * len ( handles ) ) ( * handles ) , False , INFINITE ) \n        if WAIT_OBJECT_0 <= result < len ( handles ) : \n            handle = handles [ result - WAIT_OBJECT_0 ] \n            if handle == self . interrupt_handle : \n                interrupt_main ( ) \n            elif handle == self . parent_handle : \n                os . _exit ( 1 ) \n        elif 0 > result : \n            warn ( \"\"\"Parent poll failed.  If the frontend dies,                the kernel may be left running.  Please let us know                about your system (bitness, Python, etc.) at                ipython-dev@scipy.org\"\"\" ) \n            return "}
{"13868": "\ndef pexpect_monkeypatch ( ) : \n    if '2.2' <= pexpect . __version__ [ : 3 ] : \n        return \n    def __del__ ( self ) : \n        if not self . closed : \n            try : \n                self . close ( ) \n            except AttributeError : \n                pass \n    pexpect . spawn . __del__ = __del__ "}
{"13906": "\ndef make_code_from_pyc ( filename ) : \n    try : \n        fpyc = open ( filename , \"rb\" ) \n    except IOError : \n        raise NoCode ( \"No file to run: %r\" % filename ) \n    try : \n        magic = fpyc . read ( 4 ) \n        if magic != imp . get_magic ( ) : \n            raise NoCode ( \"Bad magic number in .pyc file\" ) \n        fpyc . read ( 4 ) \n        if ( 3 , 3 ) <= sys . version_info : \n            fpyc . read ( 4 ) \n        code = marshal . load ( fpyc ) \n    finally : \n        fpyc . close ( ) \n    return code "}
{"13908": "\ndef current ( self , value ) : \n    current = min ( max ( self . _min , value ) , self . _max ) \n    self . _current = current \n    if self . _stop < current : \n        self . _stop = current \n        self . _start = current - self . _width \n    elif self . _start > current : \n        self . _start = current \n        self . _stop = current + self . _width \n    if self . _sticky_lenght >= abs ( self . _start - self . _min ) : \n        self . _start = self . _min \n    if self . _sticky_lenght >= abs ( self . _stop - self . _max ) : \n        self . _stop = self . _max "}
{"13910": "\ndef _select_index ( self , row , col ) : \n    nr , nc = self . _size \n    nr = nr - 1 \n    nc = nc - 1 \n    if ( nr < row and nc <= col ) or ( nr <= row and nc < col ) : \n        self . _select_index ( 0 , 0 ) \n    elif ( 0 >= row and 0 > col ) or ( 0 > row and 0 >= col ) : \n        self . _select_index ( nr , nc ) \n    elif nr < row : \n        self . _select_index ( 0 , col + 1 ) \n    elif 0 > row : \n        self . _select_index ( nr , col - 1 ) \n    elif nc < col : \n        self . _select_index ( row + 1 , 0 ) \n    elif 0 > col : \n        self . _select_index ( row - 1 , nc ) \n    elif row >= 0 and nr >= row and col >= 0 and nc >= col : \n        self . _index = ( row , col ) \n    else : \n        raise NotImplementedError ( \"you'r trying to go where no completion\\                           have gone before : %d:%d (%d:%d)\" % ( row , col , nr , nc ) ) "}
{"13915": "\ndef _update_list ( self , hilight = True ) : \n    self . _sliding_interval . current = self . _index [ 0 ] \n    head = None \n    foot = None \n    if 0 < self . _sliding_interval . start : \n        head = '...' \n    if self . _sliding_interval . _max > self . _sliding_interval . stop : \n        foot = '...' \n    items_m = self . _justified_items [ self . _sliding_interval . start : self . _sliding_interval . stop + 1 ] \n    self . _console_widget . _clear_temporary_buffer ( ) \n    if ( hilight ) : \n        sel = ( self . _sliding_interval . nth , self . _index [ 1 ] ) \n    else : \n        sel = None \n    strng = html_tableify ( items_m , select = sel , header = head , footer = foot ) \n    self . _console_widget . _fill_temporary_buffer ( self . _old_cursor , strng , html = True ) "}
{"13927": "\ndef monitored ( total : int , name = None , message = None ) : \n    def decorator ( f ) : \n        nonlocal name \n        monitor_index = list ( inspect . signature ( f ) . parameters . keys ( ) ) . index ( 'monitor' ) \n        if name is None : \n            name = f . __name__ \n        \n        @ wraps ( f ) \n        def wrapper ( * args , ** kargs ) : \n            if monitor_index < len ( args ) : \n                monitor = args [ monitor_index ] \n            elif 'monitor' in kargs : \n                monitor = kargs [ 'monitor' ] \n            else : \n                monitor = kargs [ 'monitor' ] = NullMonitor ( ) \n            with monitor . task ( total , name , message ) : \n                f ( * args , ** kargs ) \n        return wrapper \n    return decorator "}
{"13955": "\ndef start ( self , n ) : \n    dlist = [ ] \n    for host , n in self . engines . iteritems ( ) : \n        if isinstance ( n , ( tuple , list ) ) : \n            n , args = n \n        else : \n            args = copy . deepcopy ( self . engine_args ) \n        if '@' in host : \n            user , host = host . split ( '@' , 1 ) \n        else : \n            user = None \n        for i in range ( n ) : \n            if 0 < i : \n                time . sleep ( self . delay ) \n            el = self . launcher_class ( work_dir = self . work_dir , config = self . config , log = self . log , profile_dir = self . profile_dir , cluster_id = self . cluster_id , ) \n            if 0 < i : \n                el . to_send = [ ] \n            el . engine_cmd = self . engine_cmd \n            el . engine_args = args \n            el . on_stop ( self . _notice_engine_stopped ) \n            d = el . start ( user = user , hostname = host ) \n            self . launchers [ \"%s/%i\" % ( host , i ) ] = el \n            dlist . append ( d ) \n    self . notify_start ( dlist ) \n    return dlist "}
{"13976": "\ndef read ( self , filename ) : \n    kwargs = { } \n    if ( 3 , 2 ) <= sys . version_info : \n        kwargs [ 'encoding' ] = \"utf-8\" \n    return configparser . RawConfigParser . read ( self , filename , ** kwargs ) "}
{"14012": "\ndef _topic ( self , topic ) : \n    if 0 <= self . int_id : \n        base = \"engine.%i\" % self . int_id \n    else : \n        base = \"kernel.%s\" % self . ident \n    return py3compat . cast_bytes ( \"%s.%s\" % ( base , topic ) ) "}
{"14021": "\ndef ln ( label ) : \n    label_len = len ( label ) + 2 \n    chunk = ( 70 - label_len ) // 2 \n    out = '%s %s %s' % ( '-' * chunk , label , '-' * chunk ) \n    pad = 70 - len ( out ) \n    if 0 < pad : \n        out = out + ( '-' * pad ) \n    return out "}
{"14059": "\ndef get_dict ( self , timeout = - 1 ) : \n    results = self . get ( timeout ) \n    engine_ids = [ md [ 'engine_id' ] for md in self . _metadata ] \n    bycount = sorted ( engine_ids , key = lambda k : engine_ids . count ( k ) ) \n    maxcount = bycount . count ( bycount [ - 1 ] ) \n    if 1 < maxcount : \n        raise ValueError ( \"Cannot build dict, %i jobs ran on engine #%i\" % ( maxcount , bycount [ - 1 ] ) ) \n    return dict ( zip ( engine_ids , results ) ) "}
{"14061": "\ndef elapsed ( self ) : \n    if self . ready ( ) : \n        return self . wall_time \n    now = submitted = datetime . now ( ) \n    for msg_id in self . msg_ids : \n        if msg_id in self . _client . metadata : \n            stamp = self . _client . metadata [ msg_id ] [ 'submitted' ] \n            if stamp and submitted > stamp : \n                submitted = stamp \n    return _total_seconds ( now - submitted ) "}
{"14062": "\ndef wait_interactive ( self , interval = 1. , timeout = None ) : \n    N = len ( self ) \n    tic = time . time ( ) \n    while not self . ready ( ) and ( timeout is None or timeout >= time . time ( ) - tic ) : \n        self . wait ( interval ) \n        clear_output ( ) \n        print ( \"%4i/%i tasks finished after %4i s\" % ( self . progress , N , self . elapsed ) , end = \"\" ) \n        sys . stdout . flush ( ) \n    print ( ) \n    print ( \"done\" ) "}
{"14064": "\ndef _wait_for_outputs ( self , timeout = - 1 ) : \n    if not self . _success : \n        return \n    tic = time . time ( ) \n    while not all ( md [ 'outputs_ready' ] for md in self . _metadata ) : \n        time . sleep ( 0.01 ) \n        self . _client . _flush_iopub ( self . _client . _iopub_socket ) \n        if 0 <= timeout and tic + timeout < time . time ( ) : \n            break "}
{"14069": "\ndef find_python_files ( dirname ) : \n    for i , ( dirpath , dirnames , filenames ) in enumerate ( os . walk ( dirname ) ) : \n        if 0 < i and '__init__.py' not in filenames : \n            del dirnames [ : ] \n            continue \n        for filename in filenames : \n            if re . match ( r\"^[^.#~!$@%^&*()+=,]+\\.pyw?$\" , filename ) : \n                yield os . path . join ( dirpath , filename ) "}
{"14080": "\ndef loop_cocoa ( kernel ) : \n    import matplotlib \n    if '1.1.0' > matplotlib . __version__ : \n        kernel . log . warn ( \"MacOSX backend in matplotlib %s doesn't have a Timer, \" \"falling back on Tk for CFRunLoop integration.  Note that \" \"even this won't work if Tk is linked against X11 instead of \" \"Cocoa (e.g. EPD).  To use the MacOSX backend in the kernel, \" \"you must use matplotlib >= 1.1.0, or a native libtk.\" ) \n        return loop_tk ( kernel ) \n    from matplotlib . backends . backend_macosx import TimerMac , show \n    poll_interval = int ( 1000 * kernel . _poll_interval ) \n    real_excepthook = sys . excepthook \n    def handle_int ( etype , value , tb ) : \n        if etype is KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in CFRunLoop\" ) \n        else : \n            real_excepthook ( etype , value , tb ) \n    def doi ( ) : \n        sys . excepthook = real_excepthook \n        kernel . do_one_iteration ( ) \n        sys . excepthook = handle_int \n    t = TimerMac ( poll_interval ) \n    t . add_callback ( doi ) \n    t . start ( ) \n    poller = zmq . Poller ( ) \n    if kernel . control_stream : \n        poller . register ( kernel . control_stream . socket , zmq . POLLIN ) \n    for stream in kernel . shell_streams : \n        poller . register ( stream . socket , zmq . POLLIN ) \n    while True : \n        try : \n            try : \n                sys . excepthook = handle_int \n                show . mainloop ( ) \n                sys . excepthook = real_excepthook \n                poller . poll ( 10 * poll_interval ) \n                kernel . do_one_iteration ( ) \n            except : \n                raise \n        except KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in kernel\" ) \n        finally : \n            sys . excepthook = real_excepthook "}
{"14108": "\ndef raw_input ( self , prompt = '' ) : \n    if self . has_readline : \n        self . set_readline_completer ( ) \n    prompt = py3compat . cast_bytes_py2 ( prompt ) \n    try : \n        line = py3compat . str_to_unicode ( self . raw_input_original ( prompt ) ) \n    except ValueError : \n        warn ( \"\\n********\\nYou or a %run:ed script called sys.stdin.close()\" \" or sys.stdout.close()!\\nExiting IPython!\\n\" ) \n        self . ask_exit ( ) \n        return \"\" \n    if self . autoindent : \n        if self . indent_current_nsp < num_ini_spaces ( line ) : \n            line = line [ self . indent_current_nsp : ] \n            self . indent_current_nsp = 0 \n    return line "}
{"14117": "\ndef findsource ( object ) : \n    file = getsourcefile ( object ) or getfile ( object ) \n    globals_dict = None \n    if inspect . isframe ( object ) : \n        globals_dict = object . f_globals \n    else : \n        module = getmodule ( object , file ) \n        if module : \n            globals_dict = module . __dict__ \n    lines = linecache . getlines ( file , globals_dict ) \n    if not lines : \n        raise IOError ( 'could not get source code' ) \n    if ismodule ( object ) : \n        return lines , 0 \n    if isclass ( object ) : \n        name = object . __name__ \n        pat = re . compile ( r'^(\\s*)class\\s*' + name + r'\\b' ) \n        candidates = [ ] \n        for i in range ( len ( lines ) ) : \n            match = pat . match ( lines [ i ] ) \n            if match : \n                if lines [ i ] [ 0 ] == 'c' : \n                    return lines , i \n                candidates . append ( ( match . group ( 1 ) , i ) ) \n        if candidates : \n            candidates . sort ( ) \n            return lines , candidates [ 0 ] [ 1 ] \n        else : \n            raise IOError ( 'could not find class definition' ) \n    if ismethod ( object ) : \n        object = object . im_func \n    if isfunction ( object ) : \n        object = object . func_code \n    if istraceback ( object ) : \n        object = object . tb_frame \n    if isframe ( object ) : \n        object = object . f_code \n    if iscode ( object ) : \n        if not hasattr ( object , 'co_firstlineno' ) : \n            raise IOError ( 'could not find function definition' ) \n        pat = re . compile ( r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) \n        pmatch = pat . match \n        lnum = min ( object . co_firstlineno , len ( lines ) ) - 1 \n        while 0 < lnum : \n            if pmatch ( lines [ lnum ] ) : \n                break \n            lnum -= 1 \n        return lines , lnum \n    raise IOError ( 'could not find code object' ) "}
{"14121": "\ndef structured_traceback ( self , etype , value , elist , tb_offset = None , context = 5 ) : \n    tb_offset = self . tb_offset if tb_offset is None else tb_offset \n    Colors = self . Colors \n    out_list = [ ] \n    if elist : \n        if tb_offset and tb_offset < len ( elist ) : \n            elist = elist [ tb_offset : ] \n        out_list . append ( 'Traceback %s(most recent call last)%s:' % ( Colors . normalEm , Colors . Normal ) + '\\n' ) \n        out_list . extend ( self . _format_list ( elist ) ) \n    lines = '' . join ( self . _format_exception_only ( etype , value ) ) \n    out_list . append ( lines ) \n    return out_list "}
{"14123": "\ndef _format_exception_only ( self , etype , value ) : \n    have_filedata = False \n    Colors = self . Colors \n    list = [ ] \n    stype = Colors . excName + etype . __name__ + Colors . Normal \n    if value is None : \n        list . append ( str ( stype ) + '\\n' ) \n    else : \n        if etype is SyntaxError : \n            have_filedata = True \n            if not value . filename : \n                value . filename = \"<string>\" \n            list . append ( '%s  File %s\"%s\"%s, line %s%d%s\\n' % ( Colors . normalEm , Colors . filenameEm , value . filename , Colors . normalEm , Colors . linenoEm , value . lineno , Colors . Normal ) ) \n            if value . text is not None : \n                i = 0 \n                while len ( value . text ) > i and value . text [ i ] . isspace ( ) : \n                    i += 1 \n                list . append ( '%s    %s%s\\n' % ( Colors . line , value . text . strip ( ) , Colors . Normal ) ) \n                if value . offset is not None : \n                    s = '    ' \n                    for c in value . text [ i : value . offset - 1 ] : \n                        if c . isspace ( ) : \n                            s += c \n                        else : \n                            s += ' ' \n                    list . append ( '%s%s^%s\\n' % ( Colors . caret , s , Colors . Normal ) ) \n        try : \n            s = value . msg \n        except Exception : \n            s = self . _some_str ( value ) \n        if s : \n            list . append ( '%s%s:%s %s\\n' % ( str ( stype ) , Colors . excName , Colors . Normal , s ) ) \n        else : \n            list . append ( '%s\\n' % str ( stype ) ) \n    if have_filedata : \n        ipinst = ipapi . get ( ) \n        if ipinst is not None : \n            ipinst . hooks . synchronize_with_editor ( value . filename , value . lineno , 0 ) \n    return list "}
{"14139": "\ndef _float_precision_changed ( self , name , old , new ) : \n    if '%' in new : \n        fmt = new \n        try : \n            fmt % 3.14159 \n        except Exception : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n    elif new : \n        try : \n            i = int ( new ) \n            assert 0 <= i \n        except ValueError : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n        except AssertionError : \n            raise ValueError ( \"int precision must be non-negative, not %r\" % i ) \n        fmt = '%%.%if' % i \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = i ) \n    else : \n        fmt = '%r' \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = 8 ) \n    self . float_format = fmt "}
{"14141": "\ndef configure ( self , argv = None , doc = None ) : \n    env = self . env \n    if argv is None : \n        argv = sys . argv \n    cfg_files = getattr ( self , 'files' , [ ] ) \n    options , args = self . _parseArgs ( argv , cfg_files ) \n    if getattr ( options , 'files' , [ ] ) : \n        options , args = self . _parseArgs ( argv , options . files ) \n    self . options = options \n    if args : \n        self . testNames = args \n    if options . testNames is not None : \n        self . testNames . extend ( tolist ( options . testNames ) ) \n    if options . py3where is not None : \n        if ( 3 , ) <= sys . version_info : \n            options . where = options . py3where \n    if not options . where : \n        options . where = env . get ( 'NOSE_WHERE' , None ) \n    if not options . ignoreFiles : \n        options . ignoreFiles = env . get ( 'NOSE_IGNORE_FILES' , [ ] ) \n    if not options . include : \n        options . include = env . get ( 'NOSE_INCLUDE' , [ ] ) \n    if not options . exclude : \n        options . exclude = env . get ( 'NOSE_EXCLUDE' , [ ] ) \n    self . addPaths = options . addPaths \n    self . stopOnError = options . stopOnError \n    self . verbosity = options . verbosity \n    self . includeExe = options . includeExe \n    self . traverseNamespace = options . traverseNamespace \n    self . debug = options . debug \n    self . debugLog = options . debugLog \n    self . loggingConfig = options . loggingConfig \n    self . firstPackageWins = options . firstPackageWins \n    self . configureLogging ( ) \n    if options . where is not None : \n        self . configureWhere ( options . where ) \n    if options . testMatch : \n        self . testMatch = re . compile ( options . testMatch ) \n    if options . ignoreFiles : \n        self . ignoreFiles = map ( re . compile , tolist ( options . ignoreFiles ) ) \n        log . info ( \"Ignoring files matching %s\" , options . ignoreFiles ) \n    else : \n        log . info ( \"Ignoring files matching %s\" , self . ignoreFilesDefaultStrings ) \n    if options . include : \n        self . include = map ( re . compile , tolist ( options . include ) ) \n        log . info ( \"Including tests matching %s\" , options . include ) \n    if options . exclude : \n        self . exclude = map ( re . compile , tolist ( options . exclude ) ) \n        log . info ( \"Excluding tests matching %s\" , options . exclude ) \n    if not options . showPlugins : \n        self . plugins . configure ( options , self ) \n        self . plugins . begin ( ) "}
{"14142": "\ndef configureLogging ( self ) : \n    if self . loggingConfig : \n        from logging . config import fileConfig \n        fileConfig ( self . loggingConfig ) \n        return \n    format = logging . Formatter ( '%(name)s: %(levelname)s: %(message)s' ) \n    if self . debugLog : \n        handler = logging . FileHandler ( self . debugLog ) \n    else : \n        handler = logging . StreamHandler ( self . logStream ) \n    handler . setFormatter ( format ) \n    logger = logging . getLogger ( 'nose' ) \n    logger . propagate = 0 \n    if handler not in logger . handlers : \n        logger . addHandler ( handler ) \n    lvl = logging . WARNING \n    if 5 <= self . verbosity : \n        lvl = 0 \n    elif 4 <= self . verbosity : \n        lvl = logging . DEBUG \n    elif 3 <= self . verbosity : \n        lvl = logging . INFO \n    logger . setLevel ( lvl ) \n    if self . debug : \n        debug_loggers = [ name for name in self . debug . split ( ',' ) if name ] \n        for logger_name in debug_loggers : \n            l = logging . getLogger ( logger_name ) \n            l . setLevel ( logging . DEBUG ) \n            if not l . handlers and not logger_name . startswith ( 'nose' ) : \n                l . addHandler ( handler ) "}
{"14144": "\ndef page_dumb ( strng , start = 0 , screen_lines = 25 ) : \n    out_ln = strng . splitlines ( ) [ start : ] \n    screens = chop ( out_ln , screen_lines - 1 ) \n    if len ( screens ) == 1 : \n        print >> io . stdout , os . linesep . join ( screens [ 0 ] ) \n    else : \n        last_escape = \"\" \n        for scr in screens [ 0 : - 1 ] : \n            hunk = os . linesep . join ( scr ) \n            print >> io . stdout , last_escape + hunk \n            if not page_more ( ) : \n                return \n            esc_list = esc_re . findall ( hunk ) \n            if 0 < len ( esc_list ) : \n                last_escape = esc_list [ - 1 ] \n        print >> io . stdout , last_escape + os . linesep . join ( screens [ - 1 ] ) "}
{"14165": "\ndef flush ( self , timeout = 1.0 ) : \n    stop_time = time . time ( ) + timeout \n    for i in xrange ( 2 ) : \n        self . _flushed = False \n        self . ioloop . add_callback ( self . _flush ) \n        while not self . _flushed and stop_time > time . time ( ) : \n            time . sleep ( 0.01 ) "}
{"14183": "\ndef debug ( self , level , message ) : \n    if level <= self . _debug : \n        print ( message , file = sys . stderr ) "}
{"14189": "\ndef scan_module ( egg_dir , base , name , stubs ) : \n    filename = os . path . join ( base , name ) \n    if filename [ : - 1 ] in stubs : \n        return True \n    pkg = base [ len ( egg_dir ) + 1 : ] . replace ( os . sep , '.' ) \n    module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] \n    if ( 3 , 3 ) > sys . version_info : \n        skip = 8 \n    else : \n        skip = 12 \n    f = open ( filename , 'rb' ) ; \n    f . read ( skip ) \n    code = marshal . load ( f ) ; \n    f . close ( ) \n    safe = True \n    symbols = dict . fromkeys ( iter_symbols ( code ) ) \n    for bad in [ '__file__' , '__path__' ] : \n        if bad in symbols : \n            log . warn ( \"%s: module references %s\" , module , bad ) \n            safe = False \n    if 'inspect' in symbols : \n        for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : \n            if bad in symbols : \n                log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) \n                safe = False \n    if '__name__' in symbols and '__main__' in symbols and '.' not in module : \n        if sys . version [ : 3 ] == \"2.4\" : \n            log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) \n            safe = False \n    return safe "}
{"14198": "\ndef run_heartbeat ( message ) : \n    then = arrow . get ( message [ 'time' ] ) \n    now = arrow . get ( ) \n    if timezone . timedelta ( seconds = ( TICK_FREQ + 1 ) ) < ( now - then ) : \n        pass \n    else : \n        Task . run_tasks ( ) "}
{"14209": "\ndef report ( self , morfs , outfile = None ) : \n    self . find_code_units ( morfs ) \n    max_name = max ( [ len ( cu . name ) for cu in self . code_units ] + [ 5 ] ) \n    fmt_name = \"%%- %ds  \" % max_name \n    fmt_err = \"%s   %s: %s\\n\" \n    header = ( fmt_name % \"Name\" ) + \" Stmts   Miss\" \n    fmt_coverage = fmt_name + \"%6d %6d\" \n    if self . branches : \n        header += \" Branch BrMiss\" \n        fmt_coverage += \" %6d %6d\" \n    width100 = Numbers . pc_str_width ( ) \n    header += \"%*s\" % ( width100 + 4 , \"Cover\" ) \n    fmt_coverage += \"%%%ds%%%%\" % ( width100 + 3 , ) \n    if self . config . show_missing : \n        header += \"   Missing\" \n        fmt_coverage += \"   %s\" \n    rule = \"-\" * len ( header ) + \"\\n\" \n    header += \"\\n\" \n    fmt_coverage += \"\\n\" \n    if not outfile : \n        outfile = sys . stdout \n    outfile . write ( header ) \n    outfile . write ( rule ) \n    total = Numbers ( ) \n    for cu in self . code_units : \n        try : \n            analysis = self . coverage . _analyze ( cu ) \n            nums = analysis . numbers \n            args = ( cu . name , nums . n_statements , nums . n_missing ) \n            if self . branches : \n                args += ( nums . n_branches , nums . n_missing_branches ) \n            args += ( nums . pc_covered_str , ) \n            if self . config . show_missing : \n                args += ( analysis . missing_formatted ( ) , ) \n            outfile . write ( fmt_coverage % args ) \n            total += nums \n        except KeyboardInterrupt : \n            raise \n        except : \n            report_it = not self . config . ignore_errors \n            if report_it : \n                typ , msg = sys . exc_info ( ) [ : 2 ] \n                if typ is NotPython and not cu . should_be_python ( ) : \n                    report_it = False \n            if report_it : \n                outfile . write ( fmt_err % ( cu . name , typ . __name__ , msg ) ) \n    if 1 < total . n_files : \n        outfile . write ( rule ) \n        args = ( \"TOTAL\" , total . n_statements , total . n_missing ) \n        if self . branches : \n            args += ( total . n_branches , total . n_missing_branches ) \n        args += ( total . pc_covered_str , ) \n        if self . config . show_missing : \n            args += ( \"\" , ) \n        outfile . write ( fmt_coverage % args ) \n    return total . pc_covered "}
{"14210": "\ndef check ( self , check_all = False ) : \n    if not self . enabled and not check_all : \n        return \n    if check_all or self . check_all : \n        modules = sys . modules . keys ( ) \n    else : \n        modules = self . modules . keys ( ) \n    for modname in modules : \n        m = sys . modules . get ( modname , None ) \n        if modname in self . skip_modules : \n            continue \n        if not hasattr ( m , '__file__' ) : \n            continue \n        if m . __name__ == '__main__' : \n            continue \n        filename = m . __file__ \n        path , ext = os . path . splitext ( filename ) \n        if ext . lower ( ) == '.py' : \n            ext = PY_COMPILED_EXT \n            pyc_filename = pyfile . cache_from_source ( filename ) \n            py_filename = filename \n        else : \n            pyc_filename = filename \n            try : \n                py_filename = pyfile . source_from_cache ( filename ) \n            except ValueError : \n                continue \n        try : \n            pymtime = os . stat ( py_filename ) . st_mtime \n            if os . stat ( pyc_filename ) . st_mtime >= pymtime : \n                continue \n            if self . failed . get ( py_filename , None ) == pymtime : \n                continue \n        except OSError : \n            continue \n        try : \n            superreload ( m , reload , self . old_objects ) \n            if py_filename in self . failed : \n                del self . failed [ py_filename ] \n        except : \n            print >> sys . stderr , \"[autoreload of %s failed: %s]\" % ( modname , traceback . format_exc ( 1 ) ) \n            self . failed [ py_filename ] = pymtime "}
{"14230": "\ndef _flush_control ( self , sock ) : \n    if 0 >= self . _ignored_control_replies : \n        return \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        self . _ignored_control_replies -= 1 \n        if self . debug : \n            pprint ( msg ) \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14231": "\ndef _flush_ignored_control ( self ) : \n    while 0 < self . _ignored_control_replies : \n        self . session . recv ( self . _control_socket ) \n        self . _ignored_control_replies -= 1 "}
{"14236": "\ndef wait ( self , jobs = None , timeout = - 1 ) : \n    tic = time . time ( ) \n    if jobs is None : \n        theids = self . outstanding \n    else : \n        if isinstance ( jobs , ( int , basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        theids = set ( ) \n        for job in jobs : \n            if isinstance ( job , int ) : \n                job = self . history [ job ] \n            elif isinstance ( job , AsyncResult ) : \n                map ( theids . add , job . msg_ids ) \n                continue \n            theids . add ( job ) \n    if not theids . intersection ( self . outstanding ) : \n        return True \n    self . spin ( ) \n    while theids . intersection ( self . outstanding ) : \n        if 0 <= timeout and timeout < ( time . time ( ) - tic ) : \n            break \n        time . sleep ( 1e-3 ) \n        self . spin ( ) \n    return len ( theids . intersection ( self . outstanding ) ) == 0 "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent += 1 \n        elif toktype == token . DEDENT : \n            indent -= 1 \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + 1 ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and exclude_indent >= indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14252": "\ndef exit_counts ( self ) : \n    excluded_lines = self . first_lines ( self . excluded ) \n    exit_counts = { } \n    for l1 , l2 in self . arcs ( ) : \n        if 0 > l1 : \n            continue \n        if l1 in excluded_lines : \n            continue \n        if l2 in excluded_lines : \n            continue \n        if l1 not in exit_counts : \n            exit_counts [ l1 ] = 0 \n        exit_counts [ l1 ] += 1 \n    for l in self . classdefs : \n        if l in exit_counts : \n            exit_counts [ l ] -= 1 \n    return exit_counts "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = 0 \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if 0 <= bc . jump_to : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = 0 \n    for bc in bytecodes : \n        start_new_chunk = False \n        first_chunk = False \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = True \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = True \n        elif bc . offset in jump_to : \n            start_new_chunk = True \n        elif bc . op in OPS_CHUNK_BEGIN : \n            start_new_chunk = True \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if 0 <= bc . jump_to and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch -= 1 \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if bc . op == OP_BREAK_LOOP : \n                chunk . exits . add ( block_stack [ - 1 ] [ 1 ] ) \n            chunk = None \n        if bc . op == OP_END_FINALLY : \n            for block in reversed ( block_stack ) : \n                if block [ 0 ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ 1 ] ) \n                    break \n        if bc . op == OP_COMPARE_OP and bc . arg == COMPARE_EXCEPTION : \n            ignore_branch += 1 \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if penult . op == OP_LOAD_CONST and ult . op == OP_RETURN_VALUE : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if chunks [ - 1 ] . byte != penult . offset : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - 1 ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , False ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - 1 ] . length = bc . next_offset - chunks [ - 1 ] . byte \n        for i in range ( len ( chunks ) - 1 ) : \n            chunks [ i ] . length = chunks [ i + 1 ] . byte - chunks [ i ] . byte \n    return chunks "}
{"14258": "\ndef validate_chunks ( self , chunks ) : \n    starts = set ( [ ch . byte for ch in chunks ] ) \n    for ch in chunks : \n        assert all ( [ ( ex in starts or 0 > ex ) for ex in ch . exits ] ) "}
{"14259": "\ndef _arcs ( self ) : \n    chunks = self . _split_into_chunks ( ) \n    byte_chunks = dict ( [ ( c . byte , c ) for c in chunks ] ) \n    yield ( - 1 , byte_chunks [ 0 ] . line ) \n    for chunk in chunks : \n        if not chunk . first : \n            continue \n        chunks_considered = set ( ) \n        chunks_to_consider = [ chunk ] \n        while chunks_to_consider : \n            this_chunk = chunks_to_consider . pop ( ) \n            chunks_considered . add ( this_chunk ) \n            for ex in this_chunk . exits : \n                if 0 > ex : \n                    yield ( chunk . line , ex ) \n                else : \n                    next_chunk = byte_chunks [ ex ] \n                    if next_chunk in chunks_considered : \n                        continue \n                    backward_jump = this_chunk . byte > next_chunk . byte \n                    if next_chunk . first or backward_jump : \n                        if next_chunk . line != chunk . line : \n                            yield ( chunk . line , next_chunk . line ) \n                    else : \n                        chunks_to_consider . append ( next_chunk ) "}
{"14264": "\ndef report ( self , stream ) : \n    log . debug ( \"Coverage report\" ) \n    self . coverInstance . stop ( ) \n    self . coverInstance . combine ( ) \n    self . coverInstance . save ( ) \n    modules = [ module for name , module in sys . modules . items ( ) if self . wantModuleCoverage ( name , module ) ] \n    log . debug ( \"Coverage report will cover modules: %s\" , modules ) \n    self . coverInstance . report ( modules , file = stream ) \n    if self . coverHtmlDir : \n        log . debug ( \"Generating HTML coverage report\" ) \n        self . coverInstance . html_report ( modules , self . coverHtmlDir ) \n    if self . coverXmlFile : \n        log . debug ( \"Generating XML coverage report\" ) \n        self . coverInstance . xml_report ( modules , self . coverXmlFile ) \n    if self . coverMinPercentage : \n        f = StringIO . StringIO ( ) \n        self . coverInstance . report ( modules , file = f ) \n        m = re . search ( r'-------\\s\\w+\\s+\\d+\\s+\\d+\\s+(\\d+)%\\s+\\d*\\s{0,1}$' , f . getvalue ( ) ) \n        if m : \n            percentage = int ( m . groups ( ) [ 0 ] ) \n            if self . coverMinPercentage > percentage : \n                log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . coverMinPercentage ) \n                sys . exit ( 1 ) \n        else : \n            log . error ( \"No total percentage was found in coverage output, \" \"something went wrong.\" ) "}
{"14268": "\ndef fetch_distribution ( self , requirement , tmpdir , force_scan = False , source = False , develop_ok = False , local_index = None ) : \n    self . info ( \"Searching for %s\" , requirement ) \n    skipped = { } \n    dist = None \n    def find ( req , env = None ) : \n        if env is None : \n            env = self \n        for dist in env [ req . key ] : \n            if dist . precedence == DEVELOP_DIST and not develop_ok : \n                if dist not in skipped : \n                    self . warn ( \"Skipping development or system egg: %s\" , dist ) \n                    skipped [ dist ] = 1 \n                continue \n            if dist in req and ( SOURCE_DIST >= dist . precedence or not source ) : \n                self . info ( \"Best match: %s\" , dist ) \n                return dist . clone ( location = self . download ( dist . location , tmpdir ) ) \n    if force_scan : \n        self . prescan ( ) \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if local_index is not None : \n        dist = dist or find ( requirement , local_index ) \n    if dist is None and self . to_scan is not None : \n        self . prescan ( ) \n        dist = find ( requirement ) \n    if dist is None and not force_scan : \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if dist is None : \n        self . warn ( \"No local packages or download links found for %s%s\" , ( source and \"a source distribution of \" or \"\" ) , requirement , ) \n    return dist "}
{"14332": "\ndef compute_return ( self , start_date , end_date , rate = \"MID\" ) : \n    if rate not in [ \"MID\" , \"ASK\" , \"BID\" ] : \n        raise ValueError ( \"Unknown rate type (%s)- must be 'MID', 'ASK' or 'BID'\" % str ( rate ) ) \n    if start_date >= end_date : \n        raise ValueError ( \"End date must be on or after start date\" ) \n    df = self . generate_dataframe ( start_date = start_date , end_date = end_date ) \n    start_price = df . ix [ start_date ] [ rate ] \n    end_price = df . ix [ end_date ] [ rate ] \n    currency_return = ( end_price / start_price ) - 1.0 \n    return currency_return "}
{"14369": "\ndef _event_filter_console_keypress ( self , event ) : \n    key = event . key ( ) \n    if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : \n        if key == QtCore . Qt . Key_C and self . _executing : \n            self . request_interrupt_kernel ( ) \n            return True \n        elif key == QtCore . Qt . Key_Period : \n            self . request_restart_kernel ( ) \n            return True \n    elif not event . modifiers ( ) & QtCore . Qt . AltModifier : \n        if key == QtCore . Qt . Key_Backspace : \n            col = self . _get_input_buffer_cursor_column ( ) \n            cursor = self . _control . textCursor ( ) \n            if 3 < col and not cursor . hasSelection ( ) : \n                text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n                if text . endswith ( '    ' ) and not text . strip ( ) : \n                    cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4 ) \n                    cursor . removeSelectedText ( ) \n                    return True \n    return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) "}
{"14402": "\ndef cpu_percent ( interval = 0.1 , percpu = False ) : \n    global _last_cpu_times \n    global _last_per_cpu_times \n    blocking = interval is not None and 0.0 < interval \n    def calculate ( t1 , t2 ) : \n        t1_all = sum ( t1 ) \n        t1_busy = t1_all - t1 . idle \n        t2_all = sum ( t2 ) \n        t2_busy = t2_all - t2 . idle \n        if t1_busy >= t2_busy : \n            return 0.0 \n        busy_delta = t2_busy - t1_busy \n        all_delta = t2_all - t1_all \n        busy_perc = ( busy_delta / all_delta ) * 100 \n        return round ( busy_perc , 1 ) \n    if not percpu : \n        if blocking : \n            t1 = cpu_times ( ) \n            time . sleep ( interval ) \n        else : \n            t1 = _last_cpu_times \n        _last_cpu_times = cpu_times ( ) \n        return calculate ( t1 , _last_cpu_times ) \n    else : \n        ret = [ ] \n        if blocking : \n            tot1 = cpu_times ( percpu = True ) \n            time . sleep ( interval ) \n        else : \n            tot1 = _last_per_cpu_times \n        _last_per_cpu_times = cpu_times ( percpu = True ) \n        for t1 , t2 in zip ( tot1 , _last_per_cpu_times ) : \n            ret . append ( calculate ( t1 , t2 ) ) \n        return ret "}
{"14406": "\ndef get_children ( self , recursive = False ) : \n    if not self . is_running ( ) : \n        name = self . _platform_impl . _process_name \n        raise NoSuchProcess ( self . pid , name ) \n    ret = [ ] \n    if not recursive : \n        for p in process_iter ( ) : \n            try : \n                if p . ppid == self . pid : \n                    if p . create_time >= self . create_time : \n                        ret . append ( p ) \n            except NoSuchProcess : \n                pass \n    else : \n        table = defaultdict ( list ) \n        for p in process_iter ( ) : \n            try : \n                table [ p . ppid ] . append ( p ) \n            except NoSuchProcess : \n                pass \n        checkpids = [ self . pid ] \n        for pid in checkpids : \n            for child in table [ pid ] : \n                try : \n                    intime = child . create_time >= self . create_time \n                except NoSuchProcess : \n                    pass \n                else : \n                    if intime : \n                        ret . append ( child ) \n                        if child . pid not in checkpids : \n                            checkpids . append ( child . pid ) \n    return ret "}
{"14407": "\ndef get_cpu_percent ( self , interval = 0.1 ) : \n    blocking = interval is not None and 0.0 < interval \n    if blocking : \n        st1 = sum ( cpu_times ( ) ) \n        pt1 = self . _platform_impl . get_cpu_times ( ) \n        time . sleep ( interval ) \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n    else : \n        st1 = self . _last_sys_cpu_times \n        pt1 = self . _last_proc_cpu_times \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n        if st1 is None or pt1 is None : \n            self . _last_sys_cpu_times = st2 \n            self . _last_proc_cpu_times = pt2 \n            return 0.0 \n    delta_proc = ( pt2 . user - pt1 . user ) + ( pt2 . system - pt1 . system ) \n    delta_time = st2 - st1 \n    self . _last_sys_cpu_times = st2 \n    self . _last_proc_cpu_times = pt2 \n    try : \n        overall_percent = ( delta_proc / delta_time ) * 100 \n    except ZeroDivisionError : \n        return 0.0 \n    single_cpu_percent = overall_percent * NUM_CPUS \n    if os . name != 'posix' : \n        if 100.0 < single_cpu_percent : \n            return 100.0 \n    return round ( single_cpu_percent , 1 ) "}
{"14414": "\ndef wait ( self , timeout = None ) : \n    if timeout is not None and not 0 <= timeout : \n        raise ValueError ( \"timeout must be a positive integer\" ) \n    return self . _platform_impl . process_wait ( timeout ) "}
{"14431": "\ndef prefilter_lines ( self , lines , continue_prompt = False ) : \n    llines = lines . rstrip ( '\\n' ) . split ( '\\n' ) \n    if 1 < len ( llines ) : \n        out = '\\n' . join ( [ self . prefilter_line ( line , 0 < lnum ) for lnum , line in enumerate ( llines ) ] ) \n    else : \n        out = self . prefilter_line ( llines [ 0 ] , continue_prompt ) \n    return out "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = True \n    if esc == ESC_QUOTE : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    elif esc == ESC_QUOTE2 : \n        newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n    elif esc == ESC_PAREN : \n        newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n    else : \n        if force_auto : \n            do_rewrite = not the_rest . startswith ( '(' ) \n        else : \n            if not the_rest : \n                do_rewrite = ( 2 <= self . shell . autocall ) \n            elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n                do_rewrite = False \n            else : \n                do_rewrite = True \n        if do_rewrite : \n            if the_rest . endswith ( ';' ) : \n                newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) \n            else : \n                newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n        else : \n            normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n            return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14446": "\ndef show_tip ( self , tip ) : \n    text_edit = self . _text_edit \n    document = text_edit . document ( ) \n    cursor = text_edit . textCursor ( ) \n    search_pos = cursor . position ( ) - 1 \n    self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = False ) \n    if self . _start_position == - 1 : \n        return False \n    self . setText ( tip ) \n    self . resize ( self . sizeHint ( ) ) \n    padding = 3 \n    cursor_rect = text_edit . cursorRect ( cursor ) \n    screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) \n    point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) \n    point . setY ( point . y ( ) + padding ) \n    tip_height = self . size ( ) . height ( ) \n    tip_width = self . size ( ) . width ( ) \n    vertical = 'bottom' \n    horizontal = 'Right' \n    if screen_rect . height ( ) < point . y ( ) + tip_height : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if padding > point_ . y ( ) - tip_height : \n            if screen_rect . height ( ) > 2 * point . y ( ) : \n                vertical = 'bottom' \n            else : \n                vertical = 'top' \n        else : \n            vertical = 'top' \n    if screen_rect . width ( ) < point . x ( ) + tip_width : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if padding > point_ . x ( ) - tip_width : \n            if screen_rect . width ( ) > 2 * point . x ( ) : \n                horizontal = 'Right' \n            else : \n                horizontal = 'Left' \n        else : \n            horizontal = 'Left' \n    pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) \n    point = text_edit . mapToGlobal ( pos ( ) ) \n    if vertical == 'top' : \n        point . setY ( point . y ( ) - tip_height - padding ) \n    if horizontal == 'Left' : \n        point . setX ( point . x ( ) - tip_width - padding ) \n    self . move ( point ) \n    self . show ( ) \n    return True "}
{"14447": "\ndef _cursor_position_changed ( self ) : \n    cursor = self . _text_edit . textCursor ( ) \n    if self . _start_position >= cursor . position ( ) : \n        self . hide ( ) \n    else : \n        position , commas = self . _find_parenthesis ( self . _start_position + 1 ) \n        if position != - 1 : \n            self . hide ( ) "}
{"14458": "\ndef validate_alias ( self , name , cmd ) : \n    if name in self . no_alias : \n        raise InvalidAliasError ( \"The name %s can't be aliased \" \"because it is a keyword or builtin.\" % name ) \n    if not ( isinstance ( cmd , basestring ) ) : \n        raise InvalidAliasError ( \"An alias command must be a string, \" \"got: %r\" % cmd ) \n    nargs = cmd . count ( '%s' ) \n    if 0 < nargs and 0 <= cmd . find ( '%l' ) : \n        raise InvalidAliasError ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) \n    return nargs "}
{"14460": "\ndef transform_alias ( self , alias , rest = '' ) : \n    nargs , cmd = self . alias_table [ alias ] \n    if ' ' in cmd and os . path . isfile ( cmd ) : \n        cmd = '\"%s\"' % cmd \n    if 0 <= cmd . find ( '%l' ) : \n        cmd = cmd . replace ( '%l' , rest ) \n        rest = '' \n    if nargs == 0 : \n        cmd = '%s %s' % ( cmd , rest ) \n    else : \n        args = rest . split ( None , nargs ) \n        if nargs > len ( args ) : \n            raise AliasError ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) \n        cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) \n    return cmd "}
{"14464": "\ndef split_string ( self , string ) : \n    self . actions = [ ] \n    start = 0 \n    last_char = '\\n' if 0 < len ( string ) and string [ - 1 ] == '\\n' else None \n    string = string [ : - 1 ] if last_char is not None else string \n    for match in ANSI_OR_SPECIAL_PATTERN . finditer ( string ) : \n        raw = string [ start : match . start ( ) ] \n        substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n        if substring or self . actions : \n            yield substring \n            self . actions = [ ] \n        start = match . end ( ) \n        groups = filter ( lambda x : x is not None , match . groups ( ) ) \n        g0 = groups [ 0 ] \n        if g0 == '\\a' : \n            self . actions . append ( BeepAction ( 'beep' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\r' : \n            self . actions . append ( CarriageReturnAction ( 'carriage-return' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\b' : \n            self . actions . append ( BackSpaceAction ( 'backspace' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\n' or g0 == '\\r\\n' : \n            self . actions . append ( NewLineAction ( 'newline' ) ) \n            yield g0 \n            self . actions = [ ] \n        else : \n            params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] \n            if g0 . startswith ( '[' ) : \n                try : \n                    params = map ( int , params ) \n                except ValueError : \n                    pass \n                else : \n                    self . set_csi_code ( groups [ 2 ] , params ) \n            elif g0 . startswith ( ']' ) : \n                self . set_osc_code ( params ) \n    raw = string [ start : ] \n    substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n    if substring or self . actions : \n        yield substring \n    if last_char is not None : \n        self . actions . append ( NewLineAction ( 'newline' ) ) \n        yield last_char "}
{"14465": "\ndef get_color ( self , color , intensity = 0 ) : \n    if color is None : \n        return None \n    if 8 > color and 0 < intensity : \n        color += 8 \n    constructor = self . color_map . get ( color , None ) \n    if isinstance ( constructor , basestring ) : \n        return QtGui . QColor ( constructor ) \n    elif isinstance ( constructor , ( tuple , list ) ) : \n        return QtGui . QColor ( * constructor ) \n    return None "}
{"14469": "\ndef _clean ( self ) : \n    now = time . time ( ) \n    for jwt in self . jwts . keys ( ) : \n        if ( self . age * 2 ) < ( now - self . jwts [ jwt ] ) : \n            del self . jwts [ jwt ] "}
{"14471": "\ndef valid ( self , token ) : \n    now = time . time ( ) \n    if 'Bearer ' in token : \n        token = token [ 7 : ] \n    data = None \n    for secret in self . secrets : \n        try : \n            data = jwt . decode ( token , secret ) \n            break \n        except jwt . DecodeError : \n            continue \n        except jwt . ExpiredSignatureError : \n            raise JwtFailed ( \"Jwt expired\" ) \n    if not data : \n        raise JwtFailed ( \"Jwt cannot be decoded\" ) \n    exp = data . get ( 'exp' ) \n    if not exp : \n        raise JwtFailed ( \"Jwt missing expiration (exp)\" ) \n    if self . age < now - exp : \n        raise JwtFailed ( \"Jwt bad expiration - greater than I want to accept\" ) \n    jti = data . get ( 'jti' ) \n    if not jti : \n        raise JwtFailed ( \"Jwt missing one-time id (jti)\" ) \n    if self . already_used ( jti ) : \n        raise JwtFailed ( \"Jwt re-use disallowed (jti={})\" . format ( jti ) ) \n    return data "}
{"14486": "\ndef prompt_to_top ( self ) : \n    if not self . _executing : \n        prompt_cursor = self . _get_prompt_cursor ( ) \n        if prompt_cursor . blockNumber ( ) > self . _get_cursor ( ) . blockNumber ( ) : \n            self . _set_cursor ( prompt_cursor ) \n        self . _set_top_cursor ( prompt_cursor ) "}
{"14493": "\ndef _complete_with_items ( self , cursor , items ) : \n    self . _cancel_completion ( ) \n    if len ( items ) == 1 : \n        cursor . setPosition ( self . _control . textCursor ( ) . position ( ) , QtGui . QTextCursor . KeepAnchor ) \n        cursor . insertText ( items [ 0 ] ) \n    elif 1 < len ( items ) : \n        current_pos = self . _control . textCursor ( ) . position ( ) \n        prefix = commonprefix ( items ) \n        if prefix : \n            cursor . setPosition ( current_pos , QtGui . QTextCursor . KeepAnchor ) \n            cursor . insertText ( prefix ) \n            current_pos = cursor . position ( ) \n        cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( prefix ) ) \n        self . _completion_widget . show_items ( cursor , items ) "}
{"14508": "\ndef _insert_plain_text ( self , cursor , text ) : \n    cursor . beginEditBlock ( ) \n    if self . ansi_codes : \n        for substring in self . _ansi_processor . split_string ( text ) : \n            for act in self . _ansi_processor . actions : \n                if act . action == 'erase' and act . area == 'screen' : \n                    cursor . select ( QtGui . QTextCursor . Document ) \n                    cursor . removeSelectedText ( ) \n                elif act . action == 'scroll' and act . unit == 'page' : \n                    cursor . insertText ( '\\n' ) \n                    cursor . endEditBlock ( ) \n                    self . _set_top_cursor ( cursor ) \n                    cursor . joinPreviousEditBlock ( ) \n                    cursor . deletePreviousChar ( ) \n                elif act . action == 'carriage-return' : \n                    cursor . movePosition ( cursor . StartOfLine , cursor . KeepAnchor ) \n                elif act . action == 'beep' : \n                    QtGui . qApp . beep ( ) \n                elif act . action == 'backspace' : \n                    if not cursor . atBlockStart ( ) : \n                        cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor ) \n                elif act . action == 'newline' : \n                    cursor . movePosition ( cursor . EndOfLine ) \n            format = self . _ansi_processor . get_format ( ) \n            selection = cursor . selectedText ( ) \n            if len ( selection ) == 0 : \n                cursor . insertText ( substring , format ) \n            elif substring is not None : \n                if len ( selection ) <= len ( substring ) : \n                    cursor . insertText ( substring , format ) \n                else : \n                    old_text = selection [ len ( substring ) : ] \n                    cursor . insertText ( substring + old_text , format ) \n                    cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor , len ( old_text ) ) \n    else : \n        cursor . insertText ( text ) \n    cursor . endEditBlock ( ) "}
{"14516": "\ndef _show_prompt ( self , prompt = None , html = False , newline = True ) : \n    cursor = self . _get_end_cursor ( ) \n    self . _append_before_prompt_pos = cursor . position ( ) \n    if newline and 0 < cursor . position ( ) : \n        cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor ) \n        if cursor . selection ( ) . toPlainText ( ) != '\\n' : \n            self . _append_plain_text ( '\\n' ) \n    self . _append_plain_text ( self . _prompt_sep ) \n    if prompt is None : \n        if self . _prompt_html is None : \n            self . _append_plain_text ( self . _prompt ) \n        else : \n            self . _append_html ( self . _prompt_html ) \n    else : \n        if html : \n            self . _prompt = self . _append_html_fetching_plain_text ( prompt ) \n            self . _prompt_html = prompt \n        else : \n            self . _append_plain_text ( prompt ) \n            self . _prompt = prompt \n            self . _prompt_html = None \n    self . _prompt_pos = self . _get_end_cursor ( ) . position ( ) \n    self . _prompt_started ( ) "}
{"14517": "\ndef _adjust_scrollbars ( self ) : \n    document = self . _control . document ( ) \n    scrollbar = self . _control . verticalScrollBar ( ) \n    viewport_height = self . _control . viewport ( ) . height ( ) \n    if isinstance ( self . _control , QtGui . QPlainTextEdit ) : \n        maximum = max ( 0 , document . lineCount ( ) - 1 ) \n        step = viewport_height / self . _control . fontMetrics ( ) . lineSpacing ( ) \n    else : \n        maximum = document . size ( ) . height ( ) \n        step = viewport_height \n    diff = maximum - scrollbar . maximum ( ) \n    scrollbar . setRange ( 0 , maximum ) \n    scrollbar . setPageStep ( step ) \n    if 0 > diff and document . blockCount ( ) == document . maximumBlockCount ( ) : \n        scrollbar . setValue ( scrollbar . value ( ) + diff ) "}
{"14522": "\ndef cmp_to_key ( mycmp ) : \n    class Key ( object ) : \n        def __init__ ( self , obj ) : \n            self . obj = obj \n        def __lt__ ( self , other ) : \n            return 0 > mycmp ( self . obj , other . obj ) \n        def __gt__ ( self , other ) : \n            return 0 < mycmp ( self . obj , other . obj ) \n        def __eq__ ( self , other ) : \n            return mycmp ( self . obj , other . obj ) == 0 \n    return Key "}
{"14535": "\ndef timeUnit ( elapsed , avg , est_end ) : \n    minute = 60 \n    hr = 3600 \n    day = 86400 \n    if 3 * minute >= elapsed : \n        unit_elapsed = ( elapsed , \"secs\" ) \n    if 3 * minute < elapsed : \n        unit_elapsed = ( ( elapsed / 60 ) , \"mins\" ) \n    if 3 * hr < elapsed : \n        unit_elapsed = ( ( elapsed / 3600 ) , \"hr\" ) \n    if 3 * minute >= avg : \n        unit_avg = ( avg , \"secs\" ) \n    if 3 * minute < avg : \n        unit_avg = ( ( avg / 60 ) , \"mins\" ) \n    if 3 * hr < avg : \n        unit_avg = ( ( avg / 3600 ) , \"hr\" ) \n    if 3 * minute >= est_end : \n        unit_estEnd = ( est_end , \"secs\" ) \n    if 3 * minute < est_end : \n        unit_estEnd = ( ( est_end / 60 ) , \"mins\" ) \n    if 3 * hr < est_end : \n        unit_estEnd = ( ( est_end / 3600 ) , \"hr\" ) \n    return [ unit_elapsed , unit_avg , unit_estEnd ] "}
{"14536": "\ndef extract_wininst_cfg ( dist_filename ) : \n    f = open ( dist_filename , 'rb' ) \n    try : \n        endrec = zipfile . _EndRecData ( f ) \n        if endrec is None : \n            return None \n        prepended = ( endrec [ 9 ] - endrec [ 5 ] ) - endrec [ 6 ] \n        if 12 > prepended : \n            return None \n        f . seek ( prepended - 12 ) \n        import struct , StringIO , ConfigParser \n        tag , cfglen , bmlen = struct . unpack ( \"<iii\" , f . read ( 12 ) ) \n        if tag not in ( 0x1234567A , 0x1234567B ) : \n            return None \n        f . seek ( prepended - ( 12 + cfglen ) ) \n        cfg = ConfigParser . RawConfigParser ( { 'version' : '' , 'target_version' : '' } ) \n        try : \n            part = f . read ( cfglen ) \n            if ( 2 , 6 ) <= sys . version_info : \n                null_byte = bytes ( [ 0 ] ) \n            else : \n                null_byte = chr ( 0 ) \n            config = part . split ( null_byte , 1 ) [ 0 ] \n            config = config . decode ( 'ascii' ) \n            cfg . readfp ( StringIO . StringIO ( config ) ) \n        except ConfigParser . Error : \n            return None \n        if not cfg . has_section ( 'metadata' ) or not cfg . has_section ( 'Setup' ) : \n            return None \n        return cfg \n    finally : \n        f . close ( ) "}
{"14557": "\ndef target_outdated ( target , deps ) : \n    try : \n        target_time = os . path . getmtime ( target ) \n    except os . error : \n        return 1 \n    for dep in deps : \n        dep_time = os . path . getmtime ( dep ) \n        if target_time < dep_time : \n            return 1 \n    return 0 "}
{"14571": "\ndef enable_wx ( self , app = None ) : \n    import wx \n    wx_version = V ( wx . __version__ ) . version \n    if [ 2 , 8 ] > wx_version : \n        raise ValueError ( \"requires wxPython >= 2.8, but you have %s\" % wx . __version__ ) \n    from IPython . lib . inputhookwx import inputhook_wx \n    self . set_inputhook ( inputhook_wx ) \n    self . _current_gui = GUI_WX \n    import wx \n    if app is None : \n        app = wx . GetApp ( ) \n    if app is None : \n        app = wx . App ( redirect = False , clearSigInt = False ) \n    app . _in_event_loop = True \n    self . _apps [ GUI_WX ] = app \n    return app "}
{"14580": "\ndef get_session_info ( self , session = 0 ) : \n    if 0 >= session : \n        session += self . session_number \n    query = \"SELECT * from sessions where session == ?\" \n    return self . db . execute ( query , ( session , ) ) . fetchone ( ) "}
{"14586": "\ndef _get_range_session ( self , start = 1 , stop = None , raw = True , output = False ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if 0 > start : \n        start += n \n    if not stop or ( n < stop ) : \n        stop = n \n    elif 0 > stop : \n        stop += n \n    for i in range ( start , stop ) : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( 0 , i , line ) "}
{"14587": "\ndef store_output ( self , line_num ) : \n    if ( not self . db_log_output ) or ( line_num not in self . output_hist_reprs ) : \n        return \n    output = self . output_hist_reprs [ line_num ] \n    with self . db_output_cache_lock : \n        self . db_output_cache . append ( ( line_num , output ) ) \n    if 1 >= self . db_cache_size : \n        self . save_flag . set ( ) "}
{"14595": "\ndef format_lines ( statements , lines ) : \n    pairs = [ ] \n    i = 0 \n    j = 0 \n    start = None \n    statements = sorted ( statements ) \n    lines = sorted ( lines ) \n    while len ( statements ) > i and len ( lines ) > j : \n        if statements [ i ] == lines [ j ] : \n            if start == None : \n                start = lines [ j ] \n            end = lines [ j ] \n            j += 1 \n        elif start : \n            pairs . append ( ( start , end ) ) \n            start = None \n        i += 1 \n    if start : \n        pairs . append ( ( start , end ) ) \n    ret = ', ' . join ( map ( nice_pair , pairs ) ) \n    return ret "}
{"14598": "\ndef join_regex ( regexes ) : \n    if 1 < len ( regexes ) : \n        return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) \n    elif regexes : \n        return regexes [ 0 ] \n    else : \n        return \"\" "}
{"14610": "\ndef timed ( limit ) : \n    def decorate ( func ) : \n        def newfunc ( * arg , ** kw ) : \n            start = time . time ( ) \n            func ( * arg , ** kw ) \n            end = time . time ( ) \n            if limit < end - start : \n                raise TimeExpired ( \"Time limit (%s) exceeded\" % limit ) \n        newfunc = make_decorator ( func ) ( newfunc ) \n        return newfunc \n    return decorate "}
{"14626": "\ndef index_file ( self ) : \n    index_tmpl = Templite ( data ( \"index.html\" ) , self . template_globals ) \n    self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) \n    html = index_tmpl . render ( { 'arcs' : self . arcs , 'extra_css' : self . extra_css , 'files' : self . files , 'totals' : self . totals , } ) \n    if ( 3 , 0 ) > sys . version_info : \n        html = html . decode ( \"utf-8\" ) \n    self . write_html ( os . path . join ( self . directory , \"index.html\" ) , html ) \n    self . status . write ( self . directory ) "}
{"14652": "\ndef export_xhtml ( html , filename , image_tag = None ) : \n    if image_tag is None : \n        image_tag = default_image_tag \n    else : \n        image_tag = ensure_utf8 ( image_tag ) \n    with open ( filename , 'w' ) as f : \n        offset = html . find ( \"<html>\" ) \n        assert - 1 < offset , 'Invalid HTML string: no <html> tag.' \n        html = ( '<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n' + html [ offset + 6 : ] ) \n        html = fix_html ( html ) \n        f . write ( IMG_RE . sub ( lambda x : image_tag ( x , path = None , format = \"svg\" ) , html ) ) "}
{"14654": "\ndef fix_html ( html ) : \n    offset = html . find ( '<head>' ) \n    if - 1 < offset : \n        html = ( html [ : offset + 6 ] + '\\n<meta http-equiv=\"Content-Type\" ' + 'content=\"text/html; charset=utf-8\" />\\n' + html [ offset + 6 : ] ) \n    html = re . sub ( EMPTY_P_RE , '<br/>' , html ) \n    return html "}
{"14677": "\ndef seek ( self , index ) : \n    if 0 > index : \n        index = self . nblocks + index \n    self . _validate_index ( index ) \n    self . block_index = index \n    self . finished = False "}
{"14678": "\ndef edit ( self , index = None ) : \n    index = self . _get_index ( index ) \n    if index is None : \n        return \n    if 0 < index : \n        index -= 1 \n    filename = self . shell . mktempfile ( self . src_blocks [ index ] ) \n    self . shell . hooks . editor ( filename , 1 ) \n    new_block = file_read ( filename ) \n    self . src_blocks [ index ] = new_block \n    self . src_blocks_colored [ index ] = self . ip_colorize ( new_block ) \n    self . block_index = index \n    self ( ) "}
{"14687": "\ndef tbsource ( tb , context = 6 ) : \n    lineno = tb . tb_lineno \n    frame = tb . tb_frame \n    if 0 < context : \n        start = lineno - 1 - context // 2 \n        log . debug ( \"lineno: %s start: %s\" , lineno , start ) \n        try : \n            lines , dummy = inspect . findsource ( frame ) \n        except IOError : \n            lines , index = [ '' ] , 0 \n        else : \n            all_lines = lines \n            start = max ( start , 1 ) \n            start = max ( 0 , min ( start , len ( lines ) - context ) ) \n            lines = lines [ start : start + context ] \n            index = lineno - 1 - start \n            if ( 2 , 5 ) <= sys . version_info and 0 < index : \n                while lines [ index - 1 ] . strip ( ) . endswith ( '\\\\' ) : \n                    start -= 1 \n                    lines = all_lines [ start : start + context ] \n    else : \n        lines , index = [ '' ] , 0 \n    log . debug ( \"tbsource lines '''%s''' around index %s\" , lines , index ) \n    return ( lines , index ) "}
{"14702": "\ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = False \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules += module_list ( path ) \n        if TIMEOUT_STORAGE <= time ( ) - t and not store : \n            store = True \n            print ( \"\\nCaching the list of root modules, please wait!\" ) \n            print ( \"(This will only be done once - type '%rehashx' to \" \"reset cache!)\\n\" ) \n            sys . stdout . flush ( ) \n        if TIMEOUT_GIVEUP < time ( ) - t : \n            print ( \"This is taking too long, we give up.\\n\" ) \n            ip . db [ 'rootmodules' ] = [ ] \n            return [ ] \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14704": "\ndef module_completion ( line ) : \n    words = line . split ( ' ' ) \n    nwords = len ( words ) \n    if nwords == 3 and words [ 0 ] == 'from' : \n        return [ 'import ' ] \n    if 3 > nwords and ( words [ 0 ] in [ 'import' , 'from' ] ) : \n        if nwords == 1 : \n            return get_root_modules ( ) \n        mod = words [ 1 ] . split ( '.' ) \n        if 2 > len ( mod ) : \n            return get_root_modules ( ) \n        completion_list = try_import ( '.' . join ( mod [ : - 1 ] ) , True ) \n        return [ '.' . join ( mod [ : - 1 ] + [ el ] ) for el in completion_list ] \n    if 3 <= nwords and words [ 0 ] == 'from' : \n        mod = words [ 1 ] \n        return try_import ( mod ) "}
{"14705": "\ndef magic_run_completer ( self , event ) : \n    comps = arg_split ( event . line , strict = False ) \n    relpath = ( 1 < len ( comps ) and comps [ - 1 ] or '' ) . strip ( \"'\\\"\" ) \n    lglob = glob . glob \n    isdir = os . path . isdir \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    dirs = [ f . replace ( '\\\\' , '/' ) + \"/\" for f in lglob ( relpath + '*' ) if isdir ( f ) ] \n    if filter ( magic_run_re . match , comps ) : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( '*' ) ] \n    else : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in dirs + pys ] "}
{"14706": "\ndef cd_completer ( self , event ) : \n    ip = get_ipython ( ) \n    relpath = event . symbol \n    if event . line . endswith ( '-b' ) or ' -b ' in event . line : \n        bkms = self . db . get ( 'bookmarks' , None ) \n        if bkms : \n            return bkms . keys ( ) \n        else : \n            return [ ] \n    if event . symbol == '-' : \n        width_dh = str ( len ( str ( len ( ip . user_ns [ '_dh' ] ) + 1 ) ) ) \n        fmt = '-%0' + width_dh + 'd [%s]' \n        ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user_ns [ '_dh' ] ) ] \n        if 1 < len ( ents ) : \n            return ents \n        return [ ] \n    if event . symbol . startswith ( '--' ) : \n        return [ \"--\" + os . path . basename ( d ) for d in ip . user_ns [ '_dh' ] ] \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    relpath = relpath . replace ( '\\\\' , '/' ) \n    found = [ ] \n    for d in [ f . replace ( '\\\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : \n        if ' ' in d : \n            raise TryNext \n        found . append ( d ) \n    if not found : \n        if os . path . isdir ( relpath ) : \n            return [ compress_user ( relpath , tilde_expand , tilde_val ) ] \n        bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) \n        bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] \n        if bkmatches : \n            return bkmatches \n        raise TryNext \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in found ] "}
{"14709": "\ndef report ( self , stream ) : \n    self . stats [ 'encoding' ] = self . encoding \n    self . stats [ 'total' ] = ( self . stats [ 'errors' ] + self . stats [ 'failures' ] + self . stats [ 'passes' ] + self . stats [ 'skipped' ] ) \n    self . error_report_file . write ( u'<?xml version=\"1.0\" encoding=\"%(encoding)s\"?>' u'<testsuite name=\"nosetests\" tests=\"%(total)d\" ' u'errors=\"%(errors)d\" failures=\"%(failures)d\" ' u'skip=\"%(skipped)d\">' % self . stats ) \n    self . error_report_file . write ( u'' . join ( [ self . _forceUnicode ( e ) for e in self . errorlist ] ) ) \n    self . error_report_file . write ( u'</testsuite>' ) \n    self . error_report_file . close ( ) \n    if 1 < self . config . verbosity : \n        stream . writeln ( \"-\" * 70 ) \n        stream . writeln ( \"XML: %s\" % self . error_report_file . name ) "}
{"14714": "\ndef weighted ( loads ) : \n    weights = 1. / ( 1e-6 + numpy . array ( loads ) ) \n    sums = weights . cumsum ( ) \n    t = sums [ - 1 ] \n    x = random ( ) * t \n    y = random ( ) * t \n    idx = 0 \n    idy = 0 \n    while x > sums [ idx ] : \n        idx += 1 \n    while y > sums [ idy ] : \n        idy += 1 \n    if weights [ idx ] < weights [ idy ] : \n        return idy \n    else : \n        return idx "}
{"14719": "\ndef audit_timeouts ( self ) : \n    now = datetime . now ( ) \n    for msg_id in self . depending . keys ( ) : \n        if msg_id in self . depending : \n            job = self . depending [ msg_id ] \n            if job . timeout and now > job . timeout : \n                self . fail_unreachable ( msg_id , error . TaskTimeout ) "}
{"14721": "\ndef maybe_run ( self , job ) : \n    msg_id = job . msg_id \n    self . log . debug ( \"Attempting to assign task %s\" , msg_id ) \n    if not self . targets : \n        return False \n    if job . follow or job . targets or job . blacklist or self . hwm : \n        def can_run ( idx ) : \n            if self . hwm and self . loads [ idx ] == self . hwm : \n                return False \n            target = self . targets [ idx ] \n            if target in job . blacklist : \n                return False \n            if job . targets and target not in job . targets : \n                return False \n            return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) \n        indices = filter ( can_run , range ( len ( self . targets ) ) ) \n        if not indices : \n            if job . follow . all : \n                dests = set ( ) \n                relevant = set ( ) \n                if job . follow . success : \n                    relevant = self . all_completed \n                if job . follow . failure : \n                    relevant = relevant . union ( self . all_failed ) \n                for m in job . follow . intersection ( relevant ) : \n                    dests . add ( self . destinations [ m ] ) \n                if 1 < len ( dests ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return False \n            if job . targets : \n                job . targets . difference_update ( job . blacklist ) \n                if not job . targets or not job . targets . intersection ( self . targets ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return False \n            return False \n    else : \n        indices = None \n    self . submit_task ( job , indices ) \n    return True "}
{"14724": "\ndef dispatch_result ( self , raw_msg ) : \n    try : \n        idents , msg = self . session . feed_identities ( raw_msg , copy = False ) \n        msg = self . session . unserialize ( msg , content = False , copy = False ) \n        engine = idents [ 0 ] \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            self . finish_job ( idx ) \n    except Exception : \n        self . log . error ( \"task::Invaid result: %r\" , raw_msg , exc_info = True ) \n        return \n    header = msg [ 'header' ] \n    parent = msg [ 'parent_header' ] \n    if header . get ( 'dependencies_met' , True ) : \n        success = ( header [ 'status' ] == 'ok' ) \n        msg_id = parent [ 'msg_id' ] \n        retries = self . retries [ msg_id ] \n        if not success and 0 < retries : \n            self . retries [ msg_id ] = retries - 1 \n            self . handle_unmet_dependency ( idents , parent ) \n        else : \n            del self . retries [ msg_id ] \n            self . handle_result ( idents , parent , raw_msg , success ) \n            self . mon_stream . send_multipart ( [ b'outtask' ] + raw_msg , copy = False ) \n    else : \n        self . handle_unmet_dependency ( idents , parent ) "}
{"14754": "\ndef source_token_lines ( source ) : \n    ws_tokens = set ( [ token . INDENT , token . DEDENT , token . NEWLINE , tokenize . NL ] ) \n    line = [ ] \n    col = 0 \n    source = source . expandtabs ( 8 ) . replace ( '\\r\\n' , '\\n' ) \n    tokgen = generate_tokens ( source ) \n    for ttype , ttext , ( _ , scol ) , ( _ , ecol ) , _ in phys_tokens ( tokgen ) : \n        mark_start = True \n        for part in re . split ( '(\\n)' , ttext ) : \n            if part == '\\n' : \n                yield line \n                line = [ ] \n                col = 0 \n                mark_end = False \n            elif part == '' : \n                mark_end = False \n            elif ttype in ws_tokens : \n                mark_end = False \n            else : \n                if mark_start and col < scol : \n                    line . append ( ( \"ws\" , \" \" * ( scol - col ) ) ) \n                    mark_start = False \n                tok_class = tokenize . tok_name . get ( ttype , 'xx' ) . lower ( ) [ : 3 ] \n                if ttype == token . NAME and keyword . iskeyword ( ttext ) : \n                    tok_class = \"key\" \n                line . append ( ( tok_class , part ) ) \n                mark_end = True \n            scol = 0 \n        if mark_end : \n            col = ecol \n    if line : \n        yield line "}
{"14757": "\ndef parse_command_line ( self , argv = None ) : \n    argv = sys . argv [ 1 : ] if argv is None else argv \n    if '-pylab' in argv : \n        argv = argv [ : ] \n        idx = argv . index ( '-pylab' ) \n        warn . warn ( \"`-pylab` flag has been deprecated.\\n\" \"    Use `--pylab` instead, or `--pylab=foo` to specify a backend.\" ) \n        sub = '--pylab' \n        if idx + 1 < len ( argv ) : \n            gui = argv [ idx + 1 ] \n            if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : \n                sub = '--pylab=' + gui \n                argv . pop ( idx + 1 ) \n        argv [ idx ] = sub \n    return super ( TerminalIPythonApp , self ) . parse_command_line ( argv ) "}
{"14760": "\ndef init_banner ( self ) : \n    if self . display_banner and self . interact : \n        self . shell . show_banner ( ) \n    if logging . INFO >= self . log_level : \n        print "}
{"14801": "\ndef annotate_file ( self , cu , analysis ) : \n    if not cu . relative : \n        return \n    filename = cu . filename \n    source = cu . source_file ( ) \n    if self . directory : \n        dest_file = os . path . join ( self . directory , cu . flat_rootname ( ) ) \n        dest_file += \".py,cover\" \n    else : \n        dest_file = filename + \",cover\" \n    dest = open ( dest_file , 'w' ) \n    statements = sorted ( analysis . statements ) \n    missing = sorted ( analysis . missing ) \n    excluded = sorted ( analysis . excluded ) \n    lineno = 0 \n    i = 0 \n    j = 0 \n    covered = True \n    while True : \n        line = source . readline ( ) \n        if line == '' : \n            break \n        lineno += 1 \n        while len ( statements ) > i and lineno > statements [ i ] : \n            i += 1 \n        while len ( missing ) > j and lineno > missing [ j ] : \n            j += 1 \n        if len ( statements ) > i and statements [ i ] == lineno : \n            covered = len ( missing ) <= j or lineno < missing [ j ] \n        if self . blank_re . match ( line ) : \n            dest . write ( '  ' ) \n        elif self . else_re . match ( line ) : \n            if len ( statements ) <= i and len ( missing ) <= j : \n                dest . write ( '! ' ) \n            elif len ( statements ) <= i or len ( missing ) <= j : \n                dest . write ( '> ' ) \n            elif statements [ i ] == missing [ j ] : \n                dest . write ( '! ' ) \n            else : \n                dest . write ( '> ' ) \n        elif lineno in excluded : \n            dest . write ( '- ' ) \n        elif covered : \n            dest . write ( '> ' ) \n        else : \n            dest . write ( '! ' ) \n        dest . write ( line ) \n    source . close ( ) \n    dest . close ( ) "}
{"14813": "\ndef unserialize ( self , msg_list , content = True , copy = True ) : \n    minlen = 4 \n    message = { } \n    if not copy : \n        for i in range ( minlen ) : \n            msg_list [ i ] = msg_list [ i ] . bytes \n    if self . auth is not None : \n        signature = msg_list [ 0 ] \n        if not signature : \n            raise ValueError ( \"Unsigned Message\" ) \n        if signature in self . digest_history : \n            raise ValueError ( \"Duplicate Signature: %r\" % signature ) \n        self . digest_history . add ( signature ) \n        check = self . sign ( msg_list [ 1 : 4 ] ) \n        if not signature == check : \n            raise ValueError ( \"Invalid Signature: %r\" % signature ) \n    if not minlen <= len ( msg_list ) : \n        raise TypeError ( \"malformed message, must have at least %i elements\" % minlen ) \n    header = self . unpack ( msg_list [ 1 ] ) \n    message [ 'header' ] = header \n    message [ 'msg_id' ] = header [ 'msg_id' ] \n    message [ 'msg_type' ] = header [ 'msg_type' ] \n    message [ 'parent_header' ] = self . unpack ( msg_list [ 2 ] ) \n    if content : \n        message [ 'content' ] = self . unpack ( msg_list [ 3 ] ) \n    else : \n        message [ 'content' ] = msg_list [ 3 ] \n    message [ 'buffers' ] = msg_list [ 4 : ] \n    return message "}
{"14831": "\ndef _format_fields ( self , fields , title_width = 12 ) : \n    out = [ ] \n    header = self . __head \n    for title , content in fields : \n        if 1 < len ( content . splitlines ( ) ) : \n            title = header ( title + \":\" ) + \"\\n\" \n        else : \n            title = header ( ( title + \":\" ) . ljust ( title_width ) ) \n        out . append ( title + content ) \n    return \"\\n\" . join ( out ) "}
{"14832": "\ndef pinfo ( self , obj , oname = '' , formatter = None , info = None , detail_level = 0 ) : \n    info = self . info ( obj , oname = oname , formatter = formatter , info = info , detail_level = detail_level ) \n    displayfields = [ ] \n    def add_fields ( fields ) : \n        for title , key in fields : \n            field = info [ key ] \n            if field is not None : \n                displayfields . append ( ( title , field . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields1 ) \n    if ( not py3compat . PY3 ) and isinstance ( obj , types . InstanceType ) and info [ 'base_class' ] : \n        displayfields . append ( ( \"Base Class\" , info [ 'base_class' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields2 ) \n    if info [ 'namespace' ] != 'Interactive' : \n        displayfields . append ( ( \"Namespace\" , info [ 'namespace' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields3 ) \n    if 0 < detail_level and info [ 'source' ] is not None : \n        displayfields . append ( ( \"Source\" , self . format ( py3compat . cast_bytes_py2 ( info [ 'source' ] ) ) ) ) \n    elif info [ 'docstring' ] is not None : \n        displayfields . append ( ( \"Docstring\" , info [ \"docstring\" ] ) ) \n    if info [ 'isclass' ] : \n        if info [ 'init_definition' ] or info [ 'init_docstring' ] : \n            displayfields . append ( ( \"Constructor information\" , \"\" ) ) \n            if info [ 'init_definition' ] is not None : \n                displayfields . append ( ( \" Definition\" , info [ 'init_definition' ] . rstrip ( ) ) ) \n            if info [ 'init_docstring' ] is not None : \n                displayfields . append ( ( \" Docstring\" , indent ( info [ 'init_docstring' ] ) ) ) \n    else : \n        add_fields ( self . pinfo_fields_obj ) \n    if displayfields : \n        page . page ( self . _format_fields ( displayfields ) ) "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while len ( corpus ) >= m + qlen - step : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if bmv_l < ll : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if bmv_l < lr : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if bmv_r < rl : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if bmv_r < rr : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if qlen / 2 <= flex : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14910": "\ndef draw_cross ( self , position , color = ( 255 , 0 , 0 ) , radius = 4 ) : \n    y , x = position \n    for xmod in np . arange ( - radius , radius + 1 , 1 ) : \n        xpos = x + xmod \n        if 0 > xpos : \n            continue \n        if self . shape [ 1 ] <= xpos : \n            continue \n        self [ int ( y ) , int ( xpos ) ] = color \n    for ymod in np . arange ( - radius , radius + 1 , 1 ) : \n        ypos = y + ymod \n        if 0 > ypos : \n            continue \n        if self . shape [ 0 ] <= ypos : \n            continue \n        self [ int ( ypos ) , int ( x ) ] = color "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255 , 255 , 255 ) , size = 12 , antialias = False , center = False ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2 ) \n        y = y - ( height // 2 ) \n    for ystep in range ( height ) : \n        for xstep in range ( width ) : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if normalisation != 0 : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            else : \n                if .5 < normalisation : \n                    set_color ( x + xstep , y + ystep , color ) "}
