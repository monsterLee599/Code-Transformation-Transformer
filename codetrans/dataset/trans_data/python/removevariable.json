{"14": "\ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    elif 'info_only' in kwargs and kwargs [ 'info_only' ] : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n        else : \n            if 'index' not in kwargs : \n                self . p ( [ ] ) \n            else : \n                stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n                self . p_i ( stream_id ) \n    else : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n        else : \n            stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n        if 'index' not in kwargs : \n            self . p ( stream_id ) \n        else : \n            self . p_i ( stream_id ) \n        if stream_id in self . streams : \n            urls = self . streams [ stream_id ] [ 'src' ] \n            pass \n            pass \n        else : \n            urls = self . dash_streams [ stream_id ] [ 'src' ] \n            pass \n            pass \n        if not urls : \n            log . wtf ( '[Failed] Cannot extract video source.' ) \n        download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping captions.' ) \n            return \n        for lang in self . caption_tracks : \n            filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n            print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) \n            srt = self . caption_tracks [ lang ] \n            with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( srt ) \n            print ( 'Done.' ) "}
{"606": "\ndef preferences ( ) : \n    if request . method == 'POST' : \n        resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) \n        try : \n            request . preferences . parse_form ( request . form ) \n        except ValidationException : \n            request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) \n            return resp \n        return request . preferences . save ( resp ) \n    image_proxy = request . preferences . get_value ( 'image_proxy' ) \n    pass \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    allowed_plugins = request . preferences . plugins . get_enabled ( ) \n    stats = { } \n    for c in categories : \n        for e in categories [ c ] : \n            stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } \n            if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : \n                stats [ e . name ] [ 'warn_timeout' ] = True \n            stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) \n    for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : \n        stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) \n        if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : \n            stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True \n    return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True ) "}
{"964": "\ndef is_namedtuple_like ( x ) : \n    try : \n        for fn in x . _fields : \n            pass \n        return True \n    except AttributeError : \n        return False "}
{"1205": "\ndef _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : \n    if isinstance ( utterance , str ) : \n        pass \n    elif isinstance ( utterance , RichMessage ) : \n        utterance = utterance . json ( ) \n    elif isinstance ( utterance , ( list , dict ) ) : \n        utterance = jsonify_data ( utterance ) \n    else : \n        utterance = str ( utterance ) \n    dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id \n    if self . log_file . tell ( ) >= self . log_max_size * 1024 : \n        self . log_file . close ( ) \n        self . log_file = self . _get_log_file ( ) \n    else : \n        try : \n            log_msg = { } \n            log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) \n            log_msg [ 'dialog_id' ] = dialog_id \n            log_msg [ 'direction' ] = direction \n            log_msg [ 'message' ] = utterance \n            pass \n            self . log_file . write ( f'{log_str}\\n' ) \n        except IOError : \n            log . error ( 'Failed to write dialog log.' ) "}
{"1249": "\ndef main ( ) : \n    args = parser . parse_args ( ) \n    pass \n    if args . default : \n        if populate_settings_dir ( force = True ) : \n            print ( f'Populated {path} with default settings files' ) \n        else : \n            print ( f'{path} is already a default settings directory' ) \n    else : \n        print ( f'Current DeepPavlov settings path: {path}' ) "}
{"1266": "\ndef handle_request ( self , request : dict ) -> dict : \n    request_type = request [ 'request' ] [ 'type' ] \n    pass \n    log . debug ( f'Received request. Type: {request_type}, id: {request_id}' ) \n    if request_type in self . handled_requests . keys ( ) : \n        response : dict = self . handled_requests [ request_type ] ( request ) \n    else : \n        response : dict = self . handled_requests [ '_unsupported' ] ( request ) \n        log . warning ( f'Unsupported request type: {request_type}, request id: {request_id}' ) \n    self . _rearm_self_destruct ( ) \n    return response "}
{"1269": "\ndef _handle_intent ( self , request : dict ) -> dict : \n    intent_name = self . config [ 'intent_name' ] \n    slot_name = self . config [ 'slot_name' ] \n    pass \n    request_intent : dict = request [ 'request' ] [ 'intent' ] \n    if intent_name != request_intent [ 'name' ] : \n        log . error ( f\"Wrong intent name received: {request_intent['name']} in request {request_id}\" ) \n        return { 'error' : 'wrong intent name' } \n    if slot_name not in request_intent [ 'slots' ] . keys ( ) : \n        log . error ( f'No slot named {slot_name} found in request {request_id}' ) \n        return { 'error' : 'no slot found' } \n    utterance = request_intent [ 'slots' ] [ slot_name ] [ 'value' ] \n    agent_response = self . _act ( utterance ) \n    if not agent_response : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return { 'error' : 'error during response generation' } \n    prediction : RichMessage = agent_response [ 0 ] \n    prediction : list = prediction . alexa ( ) \n    if not prediction : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return { 'error' : 'error during response generation' } \n    response = self . _generate_response ( prediction [ 0 ] , request ) \n    return response "}
{"1330": "\ndef validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60 , ** kwargs ) : \n    assert algo is not None , '\"algo\" parameter is null' \n    assert parameters is not None , '\"parameters\" parameter is null' \n    model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) \n    assert model_builders is not None , \"/ModelBuilders REST call failed\" \n    assert algo in model_builders [ 'model_builders' ] \n    pass \n    if training_frame is not None : \n        frames = self . frames ( key = training_frame ) \n        assert frames is not None , \"/Frames/{0} REST call failed\" . format ( training_frame ) \n        key_name = frames [ 'frames' ] [ 0 ] [ 'key' ] [ 'name' ] \n        assert key_name == training_frame , \"/Frames/{0} returned Frame {1} rather than Frame {2}\" . format ( training_frame , key_name , training_frame ) \n        parameters [ 'training_frame' ] = training_frame \n    result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + \"/parameters\" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = True , noExtraErrorCheck = True ) \n    verboseprint ( \"model parameters validation: \" + repr ( result ) ) \n    return result "}
{"1988": "\ndef compileActions ( self ) : \n    import re \n    self . actionList = actions = [ None ] * 121 \n    actions [ 73 ] = \"b' the '+w+b' of the '\" \n    actionLines = self . actionTable . splitlines ( ) \n    colonPositions = [ m . start ( ) for m in re . finditer ( ':' , actionLines [ 1 ] ) ] + [ 100 ] \n    columns = [ ( colonPositions [ i ] - 3 , colonPositions [ i + 1 ] - 3 ) for i in range ( len ( colonPositions ) - 1 ) ] \n    for line in self . actionTable . splitlines ( keepends = False ) : \n        for start , end in columns : \n            action = line [ start : end ] \n            if not action or action . isspace ( ) : \n                continue \n            index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] \n            assert colon == ':' \n            action = action . rstrip ( ) \n            action = action . replace ( '_' , ' ' ) \n            pass \n            action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) \n            action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) \n            action = action . replace ( \".U\" , \".upper()\" ) \n            actions [ int ( index ) ] = action "}
{"1989": "\ndef doAction ( self , w , action ) : \n    pass \n    return eval ( self . actionList [ action ] , locals ( ) ) "}
{"2024": "\ndef getinfo ( filename , seek = None ) : \n    pass \n    HEAD = '=I6I6dddii6iiiddddii6ii60xI' \n    keys = ( 'Npart' , 'Massarr' , 'Time' , 'Redshift' , 'FlagSfr' , 'FlagFeedback' , 'Nall' , 'FlagCooling' , 'NumFiles' , 'BoxSize' , 'Omega0' , 'OmegaLambda' , 'HubbleParam' , 'FlagAge' , 'FlagMetals' , 'NallHW' , 'flag_entr_ics' , 'filename' ) \n    f = open ( filename , 'rb' ) \n    firstbytes = struct . unpack ( 'I' , f . read ( 4 ) ) \n    if firstbytes [ 0 ] == 8 : \n        gtype = 2 \n    else : \n        gtype = 1 \n    if gtype == 2 : \n        f . seek ( 16 ) \n    else : \n        f . seek ( 0 ) \n    if seek is not None : \n        f . seek ( seek ) \n    raw = struct . unpack ( HEAD , f . read ( 264 ) ) [ 1 : - 1 ] \n    values = ( raw [ : 6 ] , raw [ 6 : 12 ] ) + raw [ 12 : 16 ] + ( raw [ 16 : 22 ] , ) + raw [ 22 : 30 ] + ( raw [ 30 : 36 ] , raw [ 36 ] , filename ) \n    header = dict ( list ( zip ( keys , values ) ) ) \n    f . close ( ) \n    if gtype == 2 : \n        posoffset = ( 2 * 16 + ( 8 + 256 ) ) \n    else : \n        posoffset = ( 8 + 256 ) \n    Npart = sum ( header [ 'Npart' ] ) \n    if gtype == 2 : \n        veloffset = 3 * 16 + ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) \n    else : \n        veloffset = ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) \n    return Npart , posoffset + 4 , veloffset + 4 , header "}
{"2061": "\ndef add_virtual_columns_cartesian_to_polar ( self , x = \"x\" , y = \"y\" , radius_out = \"r_polar\" , azimuth_out = \"phi_polar\" , propagate_uncertainties = False , radians = False ) : \n    x = self [ x ] \n    y = self [ y ] \n    if radians : \n        pass \n    else : \n        pass \n    r = np . sqrt ( x ** 2 + y ** 2 ) \n    self [ radius_out ] = r \n    phi = np . arctan2 ( y , x ) \n    if not radians : \n        phi = phi * 180 / np . pi \n    self [ azimuth_out ] = phi \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ radius_out ] , self [ azimuth_out ] ] ) "}
{"2065": "\ndef add_virtual_columns_rotation ( self , x , y , xnew , ynew , angle_degrees , propagate_uncertainties = False ) : \n    x = _ensure_string_from_expression ( x ) \n    y = _ensure_string_from_expression ( y ) \n    theta = np . radians ( angle_degrees ) \n    matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) ] , [ np . sin ( theta ) , np . cos ( theta ) ] ] ) \n    pass \n    for i in range ( 2 ) : \n        for j in range ( 2 ) : \n            self . set_variable ( matrix_name + \"_%d%d\" % ( i , j ) , matrix [ i , j ] . item ( ) ) \n    self [ xnew ] = self . _expr ( \"{m}_00 * {x} + {m}_01 * {y}\" . format ( ** locals ( ) ) ) \n    self [ ynew ] = self . _expr ( \"{m}_10 * {x} + {m}_11 * {y}\" . format ( ** locals ( ) ) ) \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xnew ] , self [ ynew ] ] ) "}
{"2067": "\ndef add_virtual_columns_cartesian_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , alpha = \"l\" , delta = \"b\" , distance = \"distance\" , radians = False , center = None , center_name = \"solar_position\" ) : \n    pass \n    if center is not None : \n        self . add_variable ( center_name , center ) \n    if center is not None and center [ 0 ] != 0 : \n        x = \"({x} - {center_name}[0])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 1 ] != 0 : \n        y = \"({y} - {center_name}[1])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 2 ] != 0 : \n        z = \"({z} - {center_name}[2])\" . format ( ** locals ( ) ) \n    self . add_virtual_column ( distance , \"sqrt({x}**2 + {y}**2 + {z}**2)\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( alpha , \"arctan2({y}, {x}){transform}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( delta , \"(-arccos({z}/{distance})+pi/2){transform}\" . format ( ** locals ( ) ) ) "}
{"2068": "\ndef add_virtual_column ( self , name , expression , unique = False ) : \n    pass \n    expression = _ensure_string_from_expression ( expression ) \n    if name in self . get_column_names ( virtual = False ) : \n        renamed = '__' + vaex . utils . find_valid_name ( name , used = self . get_column_names ( ) ) \n        expression = self . _rename ( name , renamed , expression ) [ 0 ] . expression \n    name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else self . get_column_names ( ) ) \n    self . virtual_columns [ name ] = expression \n    self . column_names . append ( name ) \n    self . _save_assign_expression ( name ) \n    self . signal_column_changed . emit ( self , name , \"add\" ) "}
{"2086": "\ndef selection_undo ( self , name = \"default\" , executor = None ) : \n    logger . debug ( \"undo\" ) \n    executor = executor or self . executor \n    assert self . selection_can_undo ( name = name ) \n    selection_history = self . selection_histories [ name ] \n    pass \n    self . selection_history_indices [ name ] -= 1 \n    self . signal_selection_changed . emit ( self ) \n    logger . debug ( \"undo: selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) "}
{"2087": "\ndef selection_redo ( self , name = \"default\" , executor = None ) : \n    logger . debug ( \"redo\" ) \n    executor = executor or self . executor \n    assert self . selection_can_redo ( name = name ) \n    selection_history = self . selection_histories [ name ] \n    index = self . selection_history_indices [ name ] \n    pass \n    self . selection_history_indices [ name ] += 1 \n    self . signal_selection_changed . emit ( self ) \n    logger . debug ( \"redo: selection history is %r, index is %r\" , selection_history , index ) "}
{"2104": "\ndef ordinal_encode ( self , column , values = None , inplace = False ) : \n    column = _ensure_string_from_expression ( column ) \n    df = self if inplace else self . copy ( ) \n    df_unfiltered = df . copy ( ) \n    df_unfiltered . select_nothing ( name = FILTER_SELECTION_NAME ) \n    df_unfiltered . _length_unfiltered = df . _length_original \n    df_unfiltered . set_active_range ( 0 , df . _length_original ) \n    found_values , codes = df_unfiltered . unique ( column , return_inverse = True ) \n    if values is None : \n        values = found_values \n    else : \n        translation = np . zeros ( len ( found_values ) , dtype = np . uint64 ) \n        missing_value = len ( found_values ) \n        for i , found_value in enumerate ( found_values ) : \n            try : \n                found_value = found_value . decode ( 'ascii' ) \n            except : \n                pass \n            if found_value not in values : \n                translation [ i ] = missing_value \n            else : \n                translation [ i ] = values . index ( found_value ) \n        codes = translation [ codes ] \n        if missing_value in translation : \n            codes = np . ma . masked_array ( codes , codes == missing_value ) \n    pass \n    labels = [ str ( k ) for k in values ] \n    df . add_column ( column , codes ) \n    df . _categories [ column ] = dict ( labels = labels , N = len ( values ) , values = values ) \n    return df "}
{"2375": "\nasync def read ( self , keys : List [ str ] ) -> dict : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        if len ( keys ) > 0 : \n            parameters = [ { 'name' : f'@id{i}' , 'value' : f'{self.__sanitize_key(key)}' } for i , key in enumerate ( keys ) ] \n            pass \n            query = { \"query\" : f\"SELECT c.id, c.realId, c.document, c._etag \\FROM c WHERE c.id in ({parameter_sequence})\" , \"parameters\" : parameters } \n            options = { 'enableCrossPartitionQuery' : True } \n            results = list ( self . client . QueryItems ( self . __container_link , query , options ) ) \n            return { r . get ( 'realId' ) : self . __create_si ( r ) for r in results } \n        else : \n            raise Exception ( 'cosmosdb_storage.read(): \\provide at least one key' ) \n    except TypeError as e : \n        raise e "}
{"2442": "\ndef _validate ( instance ) : \n    try : \n        pass \n    except ValidationError as ex : \n        raise ModelValidationError ( ex . messages , ex . field_names , ex . fields , ex . data , ** ex . kwargs ) "}
{"2443": "\ndef _validate_after_init ( init_method ) : \n    \n    @ wraps ( init_method ) \n    def _decorated ( self , ** kwargs ) : \n        try : \n            pass \n        except ValidationError as ex : \n            raise ModelValidationError ( ex . messages , ex . field_names , ex . fields , ex . data , ** ex . kwargs ) from None \n        init_method ( self , ** kwargs ) \n    return _decorated "}
{"2476": "\ndef check_type ( self , value , attr , data ) : \n    root_value = super ( InstructionParameter , self ) . check_type ( value , attr , data ) \n    if is_collection ( value ) : \n        pass \n    return root_value "}
{"2723": "\ndef serial_layers ( self ) : \n    for next_node in self . topological_op_nodes ( ) : \n        new_layer = DAGCircuit ( ) \n        for qreg in self . qregs . values ( ) : \n            new_layer . add_qreg ( qreg ) \n        for creg in self . cregs . values ( ) : \n            new_layer . add_creg ( creg ) \n        support_list = [ ] \n        op = copy . copy ( next_node . op ) \n        qa = copy . copy ( next_node . qargs ) \n        ca = copy . copy ( next_node . cargs ) \n        co = copy . copy ( next_node . condition ) \n        pass \n        new_layer . apply_operation_back ( op , qa , ca , co ) \n        if next_node . name not in [ \"barrier\" , \"snapshot\" , \"save\" , \"load\" , \"noise\" ] : \n            support_list . append ( list ( qa ) ) \n        l_dict = { \"graph\" : new_layer , \"partition\" : support_list } \n        yield l_dict "}
{"3130": "\ndef create_command ( text , commands ) : \n    class CustomCommand ( BaseCommand ) : \n        pass \n        def run ( self ) : \n            for cmd in commands : \n                subprocess . check_call ( cmd ) \n    return CustomCommand "}
{"4154": "\ndef heatmap ( df , z_score = None , title = '' , figsize = ( 5 , 5 ) , cmap = 'RdBu_r' , xticklabels = True , yticklabels = True , ofname = None , ** kwargs ) : \n    df = zscore ( df , axis = z_score ) \n    df = df . iloc [ : : - 1 ] \n    ny , nx = df . shape \n    xticks = np . arange ( 0 , nx , 1 ) + .5 \n    yticks = np . arange ( 0 , ny , 1 ) + .5 \n    if hasattr ( sys , 'ps1' ) and ( ofname is None ) : \n        fig = plt . figure ( figsize = figsize ) \n    else : \n        fig = Figure ( figsize = figsize ) \n        pass \n    ax = fig . add_subplot ( 111 ) \n    vmin = np . percentile ( df . min ( ) , 2 ) \n    vmax = np . percentile ( df . max ( ) , 98 ) \n    matrix = ax . pcolormesh ( df . values , cmap = cmap , vmin = vmin , vmax = vmax ) \n    ax . set_ylim ( [ 0 , len ( df ) ] ) \n    ax . set ( xticks = xticks , yticks = yticks ) \n    ax . set_xticklabels ( df . columns . values if xticklabels else '' , fontsize = 14 , rotation = 90 ) \n    ax . set_yticklabels ( df . index . values if yticklabels else '' , fontsize = 14 ) \n    ax . set_title ( \"%s\\nHeatmap of the Analyzed Geneset\" % title , fontsize = 20 ) \n    ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , right = False , left = False ) \n    cbar = colorbar ( matrix ) \n    cbar . ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , right = False , left = False ) \n    for side in [ \"top\" , \"right\" , \"left\" , \"bottom\" ] : \n        ax . spines [ side ] . set_visible ( False ) \n        cbar . ax . spines [ side ] . set_visible ( False ) \n    if ofname is not None : \n        fig . savefig ( ofname , bbox_inches = 'tight' , dpi = 300 ) \n    return "}
{"4197": "\ndef plane_hires_edges ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , center = False , color = None ) : \n    size = util . make_list ( size , 2 ) \n    grid ( script , size = [ x_segments + y_segments - 1 , 1 ] , x_segments = ( x_segments + y_segments - 1 ) , y_segments = 1 ) \n    if ml_script1 . ml_version == '1.3.4BETA' : \n        pass \n    else : \n        pass \n    if script . ml_version == '1.3.4BETA' : \n        transform . vert_function ( script , x_func = 'if((y>0) and (x<%s),0,x)' % ( y_segments ) , y_func = 'if((y>0) and (x<%s),(x+1)*%s,y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = 'if((y>0) and (x>=%s),%s,y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x>%s),%s,x)' % ( x_segments , size [ 0 ] ) , y_func = 'if((y<.00001) and (x>%s),(x-%s)*%s,y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = 'if((y<.00001) and (x<=%s) and (x>0),0,y)' % ( x_segments ) ) \n    else : \n        transform . vert_function ( script , x_func = '((y>0) && (x<{yseg}) ? 0 : x)' . format ( yseg = y_segments ) , y_func = '((y>0) && (x<%s) ? (x+1)*%s : y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = '((y>0) && (x>=%s) ? %s : y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x>%s) ? %s : x)' % ( x_segments , size [ 0 ] ) , y_func = '((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = '((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % ( x_segments ) ) \n    if center : \n        transform . translate ( script , [ - size [ 0 ] / 2 , - size [ 1 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4259": "\ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) \n    if fext != 'obj' : \n        print ( 'Input file must be obj. Exiting ...' ) \n        sys . exit ( 1 ) \n    fread = open ( fbasename , 'r' ) \n    pass \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if element == 'v' : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        elif element == 'l' : \n            p1 = x_co \n            p2 = y_co \n            line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( prop_mask_active > activation_threshold ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( dists > 5.5 ) & ( dists < 6.5 ) ) [ 0 ] \n            pass \n            found |= set ( self . activations [ dists <= r ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    elif return_type == 'data' : \n        return self . get_image_data ( ids ) "}
{"4591": "\ndef file ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n        list_to_test = domain2idna ( list_to_test ) \n        if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . hierarchical ) \n        else : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . standard ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        pass \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . domain ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] \n    except IndexError : \n        print ( PyFunceble . Fore . CYAN + PyFunceble . Style . BRIGHT + \"Nothing to test.\" ) "}
{"4592": "\ndef file_url ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        pass \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . url ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] \n    except IndexError : \n        print ( PyFunceble . Fore . CYAN + PyFunceble . Style . BRIGHT + \"Nothing to test.\" ) "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            pass \n            time_autorisation = False \n            try : \n                time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                print ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"5206": "\ndef _build_list_of_Intervals ( cls , data_dict , stop = None , points = False , include = None , exclude = None , ignore = None , lexicon = None ) : \n    include = include or { } \n    exclude = exclude or { } \n    ignore = ignore or [ ] \n    all_data = [ ] \n    for data in zip ( * data_dict . values ( ) ) : \n        all_data . append ( { k : v for k , v in zip ( data_dict . keys ( ) , data ) } ) \n    all_data = sorted ( all_data , key = lambda x : x [ 'top' ] ) \n    wanted_data = [ ] \n    for dictionary in all_data : \n        keep = True \n        delete = [ ] \n        for k , v in dictionary . items ( ) : \n            incl = include . get ( k , utils . null_default ( True ) ) \n            excl = exclude . get ( k , utils . null_default ( False ) ) \n            if k in ignore : \n                delete . append ( k ) \n            if not incl ( v ) : \n                keep = False \n            if excl ( v ) : \n                keep = False \n        if delete : \n            for key in delete : \n                pass \n        if keep : \n            wanted_data . append ( dictionary ) \n    if not points : \n        for i , iv in enumerate ( wanted_data ) : \n            if iv . get ( 'base' , None ) is None : \n                try : \n                    iv [ 'base' ] = wanted_data [ i + 1 ] [ 'top' ] \n                except ( IndexError , KeyError ) : \n                    if stop is not None : \n                        thick = stop - iv [ 'top' ] \n                    else : \n                        thick = 1 \n                    iv [ 'base' ] = iv [ 'top' ] + thick \n    list_of_Intervals = [ ] \n    for iv in wanted_data : \n        top = iv . pop ( 'top' ) \n        base = iv . pop ( 'base' , None ) \n        descr = iv . pop ( 'description' , '' ) \n        if iv : \n            c , d = { } , { } \n            for k , v in iv . items ( ) : \n                if ( k [ : 5 ] . lower ( ) == 'comp ' ) or ( k [ : 9 ] . lower ( ) == 'component' ) : \n                    k = re . sub ( r'comp(?:onent)? ' , '' , k , flags = re . I ) \n                    c [ k ] = v \n                else : \n                    if v is not None : \n                        d [ k ] = v \n            comp = [ Component ( c ) ] if c else None \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'data' : d , 'components' : comp } ) \n        else : \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'lexicon' : lexicon } ) \n        list_of_Intervals . append ( this ) \n    return list_of_Intervals "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == False : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    pass \n    total_gend = 0 \n    while total_gend < num : \n        if len ( self . _last_pref_keys ) > 0 and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            print ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += 1 \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5560": "\ndef is_valid_uuid ( id ) : \n    if not isinstance ( id , basestring ) : \n        return False \n    try : \n        pass \n    except ValueError : \n        return False \n    return True "}
{"5890": "\ndef generate_doc_length_stats ( self ) : \n    pass \n    histdf = ( pd . DataFrame ( [ ( a , b ) for a , b in self . document_length_histogram . items ( ) ] , columns = [ 'bin' , 'doc_count' ] ) . sort_values ( by = 'bin' ) ) \n    histdf [ 'cumsum_pct' ] = histdf . doc_count . cumsum ( ) / histdf . doc_count . sum ( ) \n    self . document_length_stats = histdf \n    self . doc_length_huerestic = histdf . query ( f'cumsum_pct >= {heuristic}' ) . bin . head ( 1 ) . values [ 0 ] \n    logging . warning ( ' ' . join ( [ \"Setting maximum document length to\" , f'{self.doc_length_huerestic} based upon' , f'heuristic of {heuristic} percentile.\\n' , 'See full histogram by insepecting the' , \"`document_length_stats` attribute.\" ] ) ) \n    self . padding_maxlen = self . doc_length_huerestic "}
{"6138": "\ndef _notify_modified ( self , change ) : \n    root = self . root_object ( ) \n    if isinstance ( root , Html ) : \n        pass \n        change = { 'ref' : self . ref , 'type' : change [ 'type' ] , 'name' : change [ 'name' ] , 'value' : change [ 'value' ] } \n        root . modified ( change ) "}
{"6292": "\ndef cross_origin ( app , * args , ** kwargs ) : \n    _options = kwargs \n    _real_decorator = cors . decorate ( app , * args , run_middleware = False , with_context = False , ** kwargs ) \n    def wrapped_decorator ( f ) : \n        spf = SanicPluginsFramework ( app ) \n        try : \n            pass \n        except ValueError as e : \n            assert e . args and len ( e . args ) > 1 \n            pass \n        context = cors . get_context_from_spf ( spf ) \n        log = context . log \n        log ( logging . DEBUG , \"Enabled {:s} for cross_origin using options: {}\" . format ( str ( f ) , str ( _options ) ) ) \n        return _real_decorator ( f ) \n    return wrapped_decorator "}
{"6615": "\ndef prune ( symbol : str , all : str ) : \n    app = PriceDbApplication ( ) \n    app . logger = logger \n    pass \n    if symbol is not None : \n        sec_symbol = SecuritySymbol ( \"\" , \"\" ) \n        sec_symbol . parse ( symbol ) \n        deleted = app . prune ( sec_symbol ) \n        if deleted : \n            pass \n    else : \n        pass \n    print ( f\"Removed {count} old price entries.\" ) "}
{"6633": "\ndef add_price_entity ( self , price : dal . Price ) : \n    from decimal import Decimal \n    repo = self . get_price_repository ( ) \n    existing = ( repo . query . filter ( dal . Price . namespace == price . namespace ) . filter ( dal . Price . symbol == price . symbol ) . filter ( dal . Price . date == price . date ) . filter ( dal . Price . time == price . time ) . first ( ) ) \n    if existing : \n        pass \n        self . logger . info ( f\"Exists: {price}\" ) \n        if price . currency != existing . currency : \n            raise ValueError ( f\"The currency is different for price {price}!\" ) \n        if existing . value != price . value : \n            existing . value = price . value \n            self . logger . info ( f\"Updating to {new_value}.\" ) \n        if existing . denom != price . denom : \n            existing . denom = price . denom \n    else : \n        self . session . add ( price ) \n        self . logger . info ( f\"Added {price}\" ) "}
{"6693": "\ndef download ( url , file_name , headers = None , show_progress = True ) : \n    fd , tmp_file = tempfile . mkstemp ( prefix = ( \"%s.tmp.\" % file_name ) ) \n    os . close ( fd ) \n    if DISABLE_SSL_CHECK is True : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    pass \n    pass \n    shutil . move ( tmp_file , file_name ) \n    return file_name "}
{"6777": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    pass \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20 \n    metadata = { 'sizemb' : \"%s\" % image_size , 'client' : 'sregistry' } \n    self . bucket . upload_file ( path , names [ 'storage_uri' ] , { \"Metadata\" : metadata } ) "}
{"6784": "\ndef mv ( self , image_name , path ) : \n    container = self . get ( image_name , quiet = True ) \n    if container is not None : \n        pass \n        image = container . image or '' \n        if os . path . exists ( image ) : \n            filename = os . path . basename ( image ) \n            filedir = os . path . abspath ( path ) \n            if not os . path . isdir ( path ) : \n                filename = os . path . basename ( path ) \n                filedir = os . path . dirname ( path ) \n            if filedir == '' : \n                filedir = os . getcwd ( ) \n            fullpath = os . path . abspath ( os . path . join ( filedir , filename ) ) \n            return self . cp ( move_to = fullpath , container = container , command = \"move\" ) \n    bot . warning ( '%s not found' % ( image_name ) ) "}
{"6787": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    pass \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    self . require_secrets ( ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    pass \n    if names [ 'registry' ] == None : \n        names [ 'registry' ] = self . base \n    names = self . _add_https ( names ) \n    url = '%s/push/' % names [ 'registry' ] \n    auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] \n    SREGISTRY_EVENT = self . authorize ( request_type = \"push\" , names = names ) \n    fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    r = requests . post ( auth_url , json = fields , headers = headers ) \n    message = self . _read_response ( r ) \n    print ( '\\n[1. Collection return status {0} {1}]' . format ( r . status_code , message ) ) \n    if r . status_code != 200 : \n        sys . exit ( 1 ) \n    url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) \n    bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) \n    cid = r . json ( ) [ 'cid' ] \n    upload_to = os . path . basename ( names [ 'storage' ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"upload\" , names = names ) \n    encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) \n    progress_callback = create_callback ( encoder , self . quiet ) \n    monitor = MultipartEncoderMonitor ( encoder , progress_callback ) \n    headers = { 'Content-Type' : monitor . content_type , 'Authorization' : SREGISTRY_EVENT } \n    try : \n        r = requests . post ( url , data = monitor , headers = headers ) \n        r . raise_for_status ( ) \n        message = r . json ( ) [ 'message' ] \n        print ( '\\n[Return status {0} {1}]' . format ( r . status_code , message ) ) \n    except requests . HTTPError as e : \n        print ( '\\nUpload failed: {0}.' . format ( e ) ) \n    except KeyboardInterrupt : \n        print ( '\\nUpload cancelled.' ) \n    except Exception as e : \n        print ( e ) "}
{"6802": "\ndef _update_secrets ( self ) : \n    pass \n    pass \n    self . secrets = read_client_secrets ( ) \n    if self . _credential_cache is not None : \n        bot . info ( \"credential cache set to %s\" % self . _credential_cache ) "}
{"6864": "\ndef check_data_redundancy ( self , file_path = '' , dict_to_check = { } ) : \n    count = 0 \n    exists = os . path . isfile ( file_path ) \n    pass \n    if exists : \n        with open ( file_path , 'r' ) as input : \n            input . readline ( ) \n            for row in csv . reader ( input ) : \n                timestamp = calendar . timegm ( time . strptime ( row [ 0 ] , '%Y-%m-%d' ) ) \n                if timestamp in dict_to_check : \n                    del dict_to_check [ timestamp ] \n                count += 1 \n        input . close ( ) \n    return count "}
{"6894": "\ndef calc_total_commits ( self , starting_commits = 0 ) : \n    for week_of_commits in self . commits_dict_list : \n        try : \n            self . commits [ week_of_commits [ 'week' ] ] -= week_of_commits [ 'total' ] \n        except KeyError : \n            pass \n    self . sorted_weeks = sorted ( self . commits ) \n    for week in reversed ( self . sorted_weeks ) : \n        self . commits [ week ] = self . commits [ week ] + starting_commits \n        starting_commits = self . commits [ week ] "}
{"7047": "\ndef parse_log ( self , bowtie_log ) : \n    print ( \"is here!\" ) \n    regexes = { 'unpaired' : { 'unpaired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" , 'unpaired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'unpaired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" } , 'paired' : { 'paired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly 0 times\" , 'paired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly exactly 1 time\" , 'paired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly >1 times\" , 'paired_aligned_discord_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly 1 time\" , 'paired_aligned_discord_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly >1 times\" , 'paired_aligned_mate_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'paired_aligned_mate_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" , 'paired_aligned_mate_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" } } \n    with open ( bowtie_log , \"r\" ) as f : \n        for l in f : \n            print ( l ) \n            total = re . search ( r\"(\\\\d+) reads; of these:\" , l ) \n            print ( total ) \n            if total : \n                print ( total ) \n                self . set_n_reads ( total . group ( 1 ) ) \n            paired = re . search ( r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) were paired; of these:\" , l ) \n            if paired : \n                pass \n                paired_numbers = { } \n                l = f . readline ( ) \n                while l . startswith ( '    ' ) : \n                    for k , r in regexes [ 'paired' ] . items ( ) : \n                        match = re . search ( r , l ) \n                        if match : \n                            paired_numbers [ k ] = int ( match . group ( 1 ) ) \n                    l = f . readline ( ) \n                align_zero_times = paired_numbers [ 'paired_aligned_none' ] + paired_numbers [ 'paired_aligned_mate_none' ] \n                if align_zero_times : \n                    self . set_align_0x ( align_zero_times ) \n                align_one_time = paired_numbers [ 'paired_aligned_one' ] + paired_numbers [ 'paired_aligned_mate_one' ] \n                if align_one_time : \n                    self . set_align_1x ( align_one_time ) \n                align_more_than_one_time = paired_numbers [ 'paired_aligned_multi' ] + paired_numbers [ 'paired_aligned_mate_multi' ] \n                if align_more_than_one_time : \n                    self . set_align_mt1x ( align_more_than_one_time ) \n            overall = re . search ( r\"([\\\\d\\\\.]+)% overall alignment rate\" , l ) \n            if overall : \n                self . overall_rate = float ( overall . group ( 1 ) ) "}
{"7149": "\ndef AddConfigVar ( name , doc , configparam , root = config ) : \n    if root is config : \n        configparam . fullname = name \n    sections = name . split ( '.' ) \n    if len ( sections ) > 1 : \n        if not hasattr ( root , sections [ 0 ] ) : \n            class SubObj ( object ) : \n                pass \n            setattr ( root . __class__ , sections [ 0 ] , SubObj ( ) ) \n        newroot = getattr ( root , sections [ 0 ] ) \n        if ( not getattr ( newroot , '_i_am_a_config_class' , False ) or isinstance ( newroot , type ) ) : \n            raise TypeError ( 'Internal config nodes must be config class instances' , newroot ) \n        return AddConfigVar ( '.' . join ( sections [ 1 : ] ) , doc , configparam , root = newroot ) \n    else : \n        if hasattr ( root , name ) : \n            raise AttributeError ( 'This name is already taken' , configparam . fullname ) \n        configparam . doc = doc \n        if not callable ( configparam . default ) : \n            configparam . __get__ ( root , type ( root ) , delete_key = True ) \n        else : \n            try : \n                fetch_val_for_key ( configparam . fullname ) \n                configparam . __get__ ( root , type ( root ) , delete_key = True ) \n            except KeyError : \n                pass \n        setattr ( root . __class__ , sections [ 0 ] , configparam ) \n        _config_var_list . append ( configparam ) "}
{"7308": "\ndef assure_tree ( params , tmp_dir = 'treetime_tmp' ) : \n    if params . tree is None : \n        params . tree = os . path . basename ( params . aln ) + '.nwk' \n        print ( \"No tree given: inferring tree\" ) \n        utils . tree_inference ( params . aln , params . tree , tmp_dir = tmp_dir ) \n    if os . path . isdir ( tmp_dir ) : \n        shutil . rmtree ( tmp_dir ) \n    try : \n        pass \n    except : \n        print ( \"Tree loading/building failed.\" ) \n        return 1 \n    return 0 "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2 : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    elif keyval [ 0 ] not in [ 'alphabet' ] : \n                        keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            pass \n        except : \n            print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            pass \n    return gtr "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , 0 ) \n        return ttconf . ERROR \n    bad_branch_counter = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                node . raw_date_constraint = None \n                node . bad_branch = True \n            else : \n                try : \n                    pass \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = False \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = True \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = True \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter += 1 \n    if bad_branch_counter > self . tree . count_terminals ( ) - 3 : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , 1 , warn = True ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7322": "\ndef min_interp ( interp_object ) : \n    try : \n        return interp_object . x [ interp_object ( interp_object . x ) . argmin ( ) ] \n    except Exception as e : \n        pass \n        raise e "}
{"7412": "\ndef panel ( context , panel , build , bed , version ) : \n    LOG . info ( \"Running scout export panel\" ) \n    adapter = context . obj [ 'adapter' ] \n    pass \n    if not panel : \n        LOG . warning ( \"Please provide at least one gene panel\" ) \n        context . abort ( ) \n    LOG . info ( \"Exporting panels: {}\" . format ( ', ' . join ( panel ) ) ) \n    if bed : \n        if version : \n            version = [ version ] \n        lines = export_panels ( adapter = adapter , panels = panel , versions = version , build = build , ) \n    else : \n        lines = export_gene_panels ( adapter = adapter , panels = panel , version = version , ) \n    for line in lines : \n        click . echo ( line ) "}
{"7419": "\ndef export_verified_variants ( aggregate_variants , unique_callers ) : \n    document_lines = [ ] \n    for variant in aggregate_variants : \n        pass \n        for sample in variant [ 'samples' ] : \n            line = [ ] \n            line . append ( variant [ 'institute' ] ) \n            line . append ( variant [ '_id' ] ) \n            line . append ( variant [ 'category' ] ) \n            line . append ( variant [ 'variant_type' ] ) \n            line . append ( variant [ 'display_name' ] [ : 30 ] ) \n            case_name = variant [ 'case_obj' ] [ 'display_name' ] \n            local_link = '/' . join ( [ '' , variant [ 'institute' ] , case_name , variant [ '_id' ] ] ) \n            line . append ( local_link ) \n            line . append ( variant . get ( 'validation' ) ) \n            line . append ( case_name ) \n            case_individual = next ( ind for ind in variant [ 'case_obj' ] [ 'individuals' ] if ind [ 'individual_id' ] == sample [ 'sample_id' ] ) \n            if case_individual [ 'phenotype' ] == 2 : \n                line . append ( ' ' . join ( [ sample . get ( 'display_name' ) , '(A)' ] ) ) \n            else : \n                line . append ( sample . get ( 'display_name' ) ) \n            line . append ( '' . join ( [ 'chr' , variant [ 'chromosome' ] , ':' , str ( variant [ 'position' ] ) ] ) ) \n            line . append ( '>' . join ( [ variant . get ( 'reference' ) [ : 10 ] , variant . get ( 'alternative' ) [ : 10 ] ] ) ) \n            genes = [ ] \n            prot_effect = [ ] \n            funct_anno = [ ] \n            for gene in variant . get ( 'genes' ) : \n                genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) \n                funct_anno . append ( gene . get ( 'functional_annotation' ) ) \n                for transcript in gene . get ( 'transcripts' ) : \n                    if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : \n                        prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) \n            line . append ( ',' . join ( prot_effect ) ) \n            line . append ( ',' . join ( funct_anno ) ) \n            line . append ( ',' . join ( genes ) ) \n            line . append ( variant . get ( 'rank_score' ) ) \n            line . append ( variant . get ( 'cadd_score' ) ) \n            line . append ( sample . get ( 'genotype_call' ) ) \n            line . append ( sample [ 'allele_depths' ] [ 0 ] ) \n            line . append ( sample [ 'allele_depths' ] [ 1 ] ) \n            line . append ( sample [ 'genotype_quality' ] ) \n            for caller in unique_callers : \n                if variant . get ( caller ) : \n                    line . append ( variant . get ( caller ) ) \n                else : \n                    line . append ( '-' ) \n            document_lines . append ( line ) \n    return document_lines "}
{"7421": "\ndef user ( context , user_id , update_role , add_institute , remove_admin , remove_institute ) : \n    adapter = context . obj [ 'adapter' ] \n    user_obj = adapter . user ( user_id ) \n    if not user_obj : \n        LOG . warning ( \"User %s could not be found\" , user_id ) \n        context . abort ( ) \n    existing_roles = set ( user_obj . get ( 'roles' , [ ] ) ) \n    if update_role : \n        if not update_role in user_obj [ 'roles' ] : \n            existing_roles = set ( user_obj [ 'roles' ] ) \n            existing_roles . add ( update_role ) \n            LOG . info ( \"Adding role %s to user\" , update_role ) \n        else : \n            LOG . warning ( \"User already have role %s\" , update_role ) \n    if remove_admin : \n        try : \n            existing_roles . remove ( 'admin' ) \n            LOG . info ( \"Removing admin rights from user %s\" , user_id ) \n        except KeyError as err : \n            LOG . info ( \"User %s does not have admin rights\" , user_id ) \n    user_obj [ 'roles' ] = list ( existing_roles ) \n    existing_institutes = set ( user_obj . get ( 'institutes' , [ ] ) ) \n    for institute_id in add_institute : \n        institute_obj = adapter . institute ( institute_id ) \n        if not institute_obj : \n            LOG . warning ( \"Institute %s could not be found\" , institute_id ) \n        else : \n            existing_institutes . add ( institute_id ) \n            LOG . info ( \"Adding institute %s to user\" , institute_id ) \n    for institute_id in remove_institute : \n        try : \n            existing_institutes . remove ( institute_id ) \n            LOG . info ( \"Removing institute %s from user\" , institute_id ) \n        except KeyError as err : \n            LOG . info ( \"User does not have access to institute %s\" , institute_id ) \n    user_obj [ 'institutes' ] = list ( existing_institutes ) \n    pass "}
{"7426": "\ndef clinvar ( institute_id , case_name , variant_id ) : \n    data = controllers . clinvar_export ( store , institute_id , case_name , variant_id ) \n    if request . method == 'GET' : \n        return data \n    else : \n        form_dict = request . form . to_dict ( ) \n        submission_objects = set_submission_objects ( form_dict ) \n        open_submission = store . get_open_clinvar_submission ( current_user . email , institute_id ) \n        pass \n        return redirect ( url_for ( 'cases.clinvar_submissions' , institute_id = institute_id ) ) "}
{"7487": "\ndef verified_excel_file ( store , institute_list , temp_excel_dir ) : \n    pass \n    written_files = 0 \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    LOG . info ( 'Creating verified variant document..' ) \n    for cust in institute_list : \n        verif_vars = store . verified ( institute_id = cust ) \n        LOG . info ( 'Found {} verified variants for customer {}' . format ( len ( verif_vars ) , cust ) ) \n        if not verif_vars : \n            continue \n        unique_callers = set ( ) \n        for var_type , var_callers in CALLERS . items ( ) : \n            for caller in var_callers : \n                unique_callers . add ( caller . get ( 'id' ) ) \n        cust_verified = export_verified_variants ( verif_vars , unique_callers ) \n        document_name = '.' . join ( [ cust , '_verified_variants' , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( temp_excel_dir , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        row = 0 \n        for col , field in enumerate ( VERIFIED_VARIANTS_HEADER + list ( unique_callers ) ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( cust_verified , 1 ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( temp_excel_dir , document_name ) ) : \n            written_files += 1 \n    return written_files "}
{"7504": "\ndef add_peddy_information ( config_data ) : \n    ped_info = { } \n    ped_check = { } \n    sex_check = { } \n    pass \n    if config_data . get ( 'peddy_ped' ) : \n        file_handle = open ( config_data [ 'peddy_ped' ] , 'r' ) \n        for ind_info in parse_peddy_ped ( file_handle ) : \n            ped_info [ ind_info [ 'sample_id' ] ] = ind_info \n    if config_data . get ( 'peddy_ped_check' ) : \n        file_handle = open ( config_data [ 'peddy_ped_check' ] , 'r' ) \n        for pair_info in parse_peddy_ped_check ( file_handle ) : \n            ped_check [ ( pair_info [ 'sample_a' ] , pair_info [ 'sample_b' ] ) ] = pair_info \n    if config_data . get ( 'peddy_sex_check' ) : \n        file_handle = open ( config_data [ 'peddy_sex_check' ] , 'r' ) \n        for ind_info in parse_peddy_sex_check ( file_handle ) : \n            sex_check [ ind_info [ 'sample_id' ] ] = ind_info \n    if not ped_info : \n        return \n    analysis_inds = { } \n    for ind in config_data [ 'samples' ] : \n        ind_id = ind [ 'sample_id' ] \n        analysis_inds [ ind_id ] = ind \n    for ind_id in analysis_inds : \n        ind = analysis_inds [ ind_id ] \n        if ind_id in ped_info : \n            ind [ 'predicted_ancestry' ] = ped_info [ ind_id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) \n        if ind_id in sex_check : \n            if sex_check [ ind_id ] [ 'error' ] : \n                ind [ 'confirmed_sex' ] = False \n            else : \n                ind [ 'confirmed_sex' ] = True \n        for parent in [ 'mother' , 'father' ] : \n            if ind [ parent ] != '0' : \n                for pair in ped_check : \n                    if ( ind_id in pair and ind [ parent ] in pair ) : \n                        if ped_check [ pair ] [ 'parent_error' ] : \n                            analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = False \n                        else : \n                            if 'confirmed_parent' not in analysis_inds [ ind [ parent ] ] : \n                                analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = True "}
{"7594": "\ndef add_institute ( self , institute_obj ) : \n    internal_id = institute_obj [ 'internal_id' ] \n    display_name = institute_obj [ 'internal_id' ] \n    if self . institute ( institute_id = internal_id ) : \n        raise IntegrityError ( \"Institute {0} already exists in database\" . format ( display_name ) ) \n    LOG . info ( \"Adding institute with internal_id: {0} and \" \"display_name: {1}\" . format ( internal_id , display_name ) ) \n    pass \n    LOG . info ( \"Institute saved\" ) "}
{"7595": "\ndef update_institute ( self , internal_id , sanger_recipient = None , coverage_cutoff = None , frequency_cutoff = None , display_name = None , remove_sanger = None , phenotype_groups = None , group_abbreviations = None , add_groups = None ) : \n    add_groups = add_groups or False \n    institute_obj = self . institute ( internal_id ) \n    if not institute_obj : \n        raise IntegrityError ( \"Institute {} does not exist in database\" . format ( internal_id ) ) \n    updates = { } \n    updated_institute = institute_obj \n    if sanger_recipient : \n        user_obj = self . user ( sanger_recipient ) \n        if not user_obj : \n            raise IntegrityError ( \"user {} does not exist in database\" . format ( sanger_recipient ) ) \n        LOG . info ( \"Updating sanger recipients for institute: {0} with {1}\" . format ( internal_id , sanger_recipient ) ) \n        updates [ '$push' ] = { 'sanger_recipients' : remove_sanger } \n    if remove_sanger : \n        LOG . info ( \"Removing sanger recipient {0} from institute: {1}\" . format ( remove_sanger , internal_id ) ) \n        updates [ '$pull' ] = { 'sanger_recipients' : remove_sanger } \n    if coverage_cutoff : \n        LOG . info ( \"Updating coverage cutoff for institute: {0} to {1}\" . format ( internal_id , coverage_cutoff ) ) \n        updates [ '$set' ] = { 'coverage_cutoff' : coverage_cutoff } \n    if frequency_cutoff : \n        LOG . info ( \"Updating frequency cutoff for institute: {0} to {1}\" . format ( internal_id , frequency_cutoff ) ) \n        if not '$set' in updates : \n            updates [ '$set' ] = { } \n        updates [ '$set' ] = { 'frequency_cutoff' : frequency_cutoff } \n    if display_name : \n        LOG . info ( \"Updating display name for institute: {0} to {1}\" . format ( internal_id , display_name ) ) \n        if not '$set' in updates : \n            updates [ '$set' ] = { } \n        updates [ '$set' ] = { 'display_name' : display_name } \n    if phenotype_groups : \n        if group_abbreviations : \n            group_abbreviations = list ( group_abbreviations ) \n        existing_groups = { } \n        if add_groups : \n            existing_groups = institute_obj . get ( 'phenotype_groups' , PHENOTYPE_GROUPS ) \n        for i , hpo_term in enumerate ( phenotype_groups ) : \n            hpo_obj = self . hpo_term ( hpo_term ) \n            if not hpo_obj : \n                raise IntegrityError ( \"Term {} does not exist\" . format ( hpo_term ) ) \n            pass \n            description = hpo_obj [ 'description' ] \n            abbreviation = None \n            if group_abbreviations : \n                abbreviation = group_abbreviations [ i ] \n            existing_groups [ hpo_term ] = { 'name' : description , 'abbr' : abbreviation } \n        updates [ '$set' ] = { 'phenotype_groups' : existing_groups } \n    if updates : \n        if not '$set' in updates : \n            updates [ '$set' ] = { } \n        updates [ '$set' ] [ 'updated_at' ] = datetime . now ( ) \n        updated_institute = self . institute_collection . find_one_and_update ( { '_id' : internal_id } , updates , return_document = pymongo . ReturnDocument . AFTER ) \n        LOG . info ( \"Institute updated\" ) \n    return updated_institute "}
{"7619": "\ndef delete_clinvar_object ( self , object_id , object_type , submission_id ) : \n    LOG . info ( \"Deleting clinvar object %s (%s)\" , object_id , object_type ) \n    pass \n    if object_type == 'variant_data' : \n        self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$pull' : { 'variant_data' : object_id } } ) \n        variant_object = self . clinvar_collection . find_one ( { '_id' : object_id } ) \n        linking_id = variant_object . get ( \"linking_id\" ) \n        pass \n    else : \n        pass \n    self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$pull' : { 'case_data' : object_id } } ) \n    updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : submission_id } , { '$set' : { 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) \n    return updated_submission "}
{"7682": "\ndef ensembl_genes ( self , build = '37' ) : \n    genes = { } \n    LOG . info ( \"Fetching all genes\" ) \n    for gene_obj in self . hgnc_collection . find ( { 'build' : build } ) : \n        ensg_id = gene_obj [ 'ensembl_id' ] \n        pass \n        genes [ ensg_id ] = gene_obj \n    LOG . info ( \"Ensembl genes fetched\" ) \n    return genes "}
{"7711": "\ndef coverage_report_contents ( store , institute_obj , case_obj , base_url ) : \n    request_data = { } \n    request_data [ 'sample_id' ] = [ ind [ 'individual_id' ] for ind in case_obj [ 'individuals' ] ] \n    pass \n    panel_names = [ ] \n    for panel_info in case_obj . get ( 'panels' , [ ] ) : \n        if not panel_info . get ( 'is_default' ) : \n            continue \n        panel_obj = store . gene_panel ( panel_info [ 'panel_name' ] , version = panel_info . get ( 'version' ) ) \n        full_name = \"{} ({})\" . format ( panel_obj [ 'display_name' ] , panel_obj [ 'version' ] ) \n        panel_names . append ( full_name ) \n    panel_names = ' ,' . join ( panel_names ) \n    request_data [ 'panel_name' ] = panel_names \n    request_data [ 'level' ] = institute_obj . get ( 'coverage_cutoff' , 15 ) \n    resp = requests . get ( base_url + 'reports/report' , params = request_data ) \n    soup = BeautifulSoup ( resp . text ) \n    for tag in soup . find_all ( 'a' ) : \n        tag . replaceWith ( '' ) \n    coverage_data = '' . join ( [ '%s' % x for x in soup . body . contents ] ) \n    return coverage_data "}
{"7723": "\ndef genes ( context , build , api_key ) : \n    LOG . info ( \"Running scout update genes\" ) \n    adapter = context . obj [ 'adapter' ] \n    api_key = api_key or context . obj . get ( 'omim_api_key' ) \n    if not api_key : \n        LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) \n        context . abort ( ) \n    try : \n        mim_files = fetch_mim_files ( api_key , mim2genes = True , morbidmap = True , genemap2 = True ) \n    except Exception as err : \n        LOG . warning ( err ) \n        context . abort ( ) \n    LOG . warning ( \"Dropping all gene information\" ) \n    adapter . drop_genes ( build ) \n    LOG . info ( \"Genes dropped\" ) \n    LOG . warning ( \"Dropping all transcript information\" ) \n    adapter . drop_transcripts ( build ) \n    LOG . info ( \"transcripts dropped\" ) \n    hpo_genes = fetch_hpo_genes ( ) \n    if build : \n        builds = [ build ] \n    else : \n        builds = [ '37' , '38' ] \n    hgnc_lines = fetch_hgnc ( ) \n    exac_lines = fetch_exac_constraint ( ) \n    for build in builds : \n        ensembl_genes = fetch_ensembl_genes ( build = build ) \n        hgnc_genes = load_hgnc_genes ( adapter = adapter , ensembl_lines = ensembl_genes , hgnc_lines = hgnc_lines , exac_lines = exac_lines , mim2gene_lines = mim_files [ 'mim2genes' ] , genemap_lines = mim_files [ 'genemap2' ] , hpo_lines = hpo_genes , build = build , ) \n        ensembl_genes = { } \n        for gene_obj in hgnc_genes : \n            ensembl_id = gene_obj [ 'ensembl_id' ] \n            ensembl_genes [ ensembl_id ] = gene_obj \n        ensembl_transcripts = fetch_ensembl_transcripts ( build = build ) \n        pass \n    adapter . update_indexes ( ) \n    LOG . info ( \"Genes, transcripts and Exons loaded\" ) "}
{"7728": "\ndef case ( context , vcf , vcf_sv , vcf_cancer , vcf_str , owner , ped , update , config , no_variants , peddy_ped , peddy_sex , peddy_check ) : \n    adapter = context . obj [ 'adapter' ] \n    if config is None and ped is None : \n        LOG . warning ( \"Please provide either scout config or ped file\" ) \n        context . abort ( ) \n    config_raw = yaml . load ( config ) if config else { } \n    try : \n        config_data = parse_case_data ( config = config_raw , ped = ped , owner = owner , vcf_snv = vcf , vcf_sv = vcf_sv , vcf_str = vcf_str , vcf_cancer = vcf_cancer , peddy_ped = peddy_ped , peddy_sex = peddy_sex , peddy_check = peddy_check ) \n    except SyntaxError as err : \n        LOG . warning ( err ) \n        context . abort ( ) \n    LOG . info ( \"Use family %s\" % config_data [ 'family' ] ) \n    try : \n        pass \n    except Exception as err : \n        LOG . error ( \"Something went wrong during loading\" ) \n        LOG . warning ( err ) \n        context . abort ( ) "}
{"7734": "\ndef update_case_compounds ( self , case_obj , build = '37' ) : \n    case_id = case_obj [ '_id' ] \n    categories = set ( ) \n    variant_types = set ( ) \n    for file_type in FILE_TYPE_MAP : \n        if case_obj . get ( 'vcf_files' , { } ) . get ( file_type ) : \n            categories . add ( FILE_TYPE_MAP [ file_type ] [ 'category' ] ) \n            variant_types . add ( FILE_TYPE_MAP [ file_type ] [ 'variant_type' ] ) \n    coding_intervals = self . get_coding_intervals ( build = build ) \n    for chrom in CHROMOSOMES : \n        pass \n        for var_type in variant_types : \n            for category in categories : \n                LOG . info ( \"Updating compounds on chromosome:{0}, type:{1}, category:{2} for case:{3}\" . format ( chrom , var_type , category , case_id ) ) \n                query = { 'variant_type' : var_type , 'chrom' : chrom , } \n                variant_objs = self . variants ( case_id = case_id , query = query , category = category , nr_of_variants = - 1 , sort_key = 'position' ) \n                bulk = { } \n                current_region = None \n                pass \n                for var_obj in variant_objs : \n                    var_id = var_obj [ '_id' ] \n                    var_chrom = var_obj [ 'chromosome' ] \n                    var_start = var_obj [ 'position' ] \n                    var_end = var_obj [ 'end' ] + 1 \n                    update_bulk = True \n                    new_region = None \n                    genomic_regions = coding_intervals . get ( var_chrom , IntervalTree ( ) ) . search ( var_start , var_end ) \n                    if genomic_regions : \n                        new_region = genomic_regions . pop ( ) . data \n                    if new_region and ( new_region == current_region ) : \n                        update_bulk = False \n                    current_region = new_region \n                    if update_bulk and bulk : \n                        self . update_compounds ( bulk ) \n                        self . update_mongo_compound_variants ( bulk ) \n                        bulk = { } \n                    if new_region : \n                        bulk [ var_id ] = var_obj \n                if not bulk : \n                    continue \n                self . update_compounds ( bulk ) \n                self . update_mongo_compound_variants ( bulk ) \n    LOG . info ( \"All compounds updated\" ) \n    return "}
{"7736": "\ndef upsert_variant ( self , variant_obj ) : \n    LOG . debug ( \"Upserting variant %s\" , variant_obj [ '_id' ] ) \n    try : \n        result = self . variant_collection . insert_one ( variant_obj ) \n    except DuplicateKeyError as err : \n        LOG . debug ( \"Variant %s already exists in database\" , variant_obj [ '_id' ] ) \n        result = self . variant_collection . find_one_and_update ( { '_id' : variant_obj [ '_id' ] } , { '$set' : { 'compounds' : variant_obj . get ( 'compounds' , [ ] ) } } ) \n        pass \n    return result "}
{"7737": "\ndef load_variant_bulk ( self , variants ) : \n    if not len ( variants ) > 0 : \n        return \n    LOG . debug ( \"Loading variant bulk\" ) \n    try : \n        pass \n    except ( DuplicateKeyError , BulkWriteError ) as err : \n        for var_obj in variants : \n            try : \n                self . upsert_variant ( var_obj ) \n            except IntegrityError as err : \n                pass \n    return "}
{"7772": "\ndef get_mim_phenotypes ( genemap_lines ) : \n    pass \n    phenotypes_found = { } \n    for entry in parse_genemap2 ( genemap_lines ) : \n        hgnc_symbol = entry [ 'hgnc_symbol' ] \n        for phenotype in entry [ 'phenotypes' ] : \n            mim_nr = phenotype [ 'mim_number' ] \n            if mim_nr in phenotypes_found : \n                phenotype_entry = phenotypes_found [ mim_nr ] \n                phenotype_entry [ 'inheritance' ] = phenotype_entry [ 'inheritance' ] . union ( phenotype [ 'inheritance' ] ) \n                phenotype_entry [ 'hgnc_symbols' ] . add ( hgnc_symbol ) \n            else : \n                phenotype [ 'hgnc_symbols' ] = set ( [ hgnc_symbol ] ) \n                phenotypes_found [ mim_nr ] = phenotype \n    return phenotypes_found "}
{"7773": "\ndef cli ( context , morbid , genemap , mim2gene , mim_titles , phenotypes ) : \n    from scout . utils . handle import get_file_handle \n    from pprint import pprint as pp \n    print ( \"Morbid file: %s\" % morbid ) \n    print ( \"Genemap file: %s\" % genemap ) \n    print ( \"mim2gene file: %s\" % mim2gene ) \n    print ( \"MimTitles file: %s\" % mim_titles ) \n    if morbid : \n        pass \n    if genemap : \n        genemap_handle = get_file_handle ( genemap ) \n    if mim2gene : \n        mim2gene_handle = get_file_handle ( mim2gene ) \n    if mim_titles : \n        pass \n    mim_genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for entry in mim_genes : \n        if entry == 'C10orf11' : \n            pp ( mim_genes [ entry ] ) \n    context . abort ( ) \n    if phenotypes : \n        if not genemap : \n            click . echo ( \"Please provide the genemap file\" ) \n            context . abort ( ) \n        phenotypes = get_mim_phenotypes ( genemap_handle ) \n        for i , mim_term in enumerate ( phenotypes ) : \n            pass \n    print ( \"Number of phenotypes found: %s\" % i ) \n    context . abort ( ) \n    genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for hgnc_symbol in genes : \n        if hgnc_symbol == 'OPA1' : \n            print ( genes [ hgnc_symbol ] ) "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    pass \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7822": "\ndef plot_gain_offsets ( dio_cross , dio_chan_per_coarse = 8 , feedtype = 'l' , ax1 = None , ax2 = None , legend = True , ** kwargs ) : \n    Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    obs = Waterfall ( dio_cross , max_load = 150 ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    coarse_G = gain_offsets ( I , Q , U , V , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    coarse_freqs = convert_to_coarse ( freqs , dio_chan_per_coarse ) \n    XX_OFF , XX_ON = foldcal ( np . expand_dims ( data [ : , 0 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    YY_OFF , YY_ON = foldcal ( np . expand_dims ( data [ : , 1 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    if ax1 == None : \n        plt . subplot ( 211 ) \n    else : \n        axG = plt . axes ( ax1 ) \n        plt . setp ( axG . get_xticklabels ( ) , visible = False ) \n    plt . plot ( coarse_freqs , coarse_G , 'ko' , markersize = 2 ) \n    plt . ylabel ( r'$\\frac{\\Delta G}{2}$' , rotation = 90 ) \n    if feedtype == 'l' : \n        plt . title ( 'XY Gain Difference' ) \n    if feedtype == 'c' : \n        plt . title ( 'LR Gain Difference' ) \n    plt . grid ( True ) \n    if ax2 == None : \n        plt . subplot ( 212 ) \n    else : \n        pass \n    if feedtype == 'l' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'XX' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'YY' ) \n    if feedtype == 'c' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'LL' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'RR' ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . ylabel ( 'Power (Counts)' ) \n    if legend == True : \n        plt . legend ( ) "}
{"7848": "\ndef find_n_data_blocks ( self ) : \n    self . file_obj . seek ( 0 ) \n    header0 , data_idx0 = self . read_header ( ) \n    self . file_obj . seek ( data_idx0 ) \n    pass \n    pass \n    self . file_obj . seek ( int ( header0 [ 'BLOCSIZE' ] ) , 1 ) \n    n_blocks = 1 \n    end_found = False \n    while not end_found : \n        try : \n            header , data_idx = self . read_header ( ) \n            self . file_obj . seek ( data_idx ) \n            self . file_obj . seek ( header [ 'BLOCSIZE' ] , 1 ) \n            n_blocks += 1 \n        except EndOfFileError : \n            end_found = True \n            break \n    self . file_obj . seek ( 0 ) \n    return n_blocks "}
{"7854": "\ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    if not HAS_BITSHUFFLE : \n        print ( \"Error: the bitshuffle library is required to run this script.\" ) \n        exit ( ) \n    parser = ArgumentParser ( description = \"Command line utility for creating HDF5 Raw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) \n    args = parser . parse_args ( ) \n    fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] \n    filelist = glob . glob ( fileroot + '*.raw' ) \n    filelist = sorted ( filelist ) \n    r = GuppiRaw ( filelist [ 0 ] ) \n    header , data = r . read_next_data_block ( ) \n    dshape = data . shape \n    print ( dshape ) \n    n_blocks_total = 0 \n    for filename in filelist : \n        print ( filename ) \n        r = GuppiRaw ( filename ) \n        n_blocks_total += r . n_blocks \n    print ( n_blocks_total ) \n    full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) \n    h5 = h5py . File ( fileroot + '.h5' , 'w' ) \n    h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' \n    pass \n    dset = h5 . create_dataset ( 'data' , shape = full_dshape , dtype = data . dtype ) \n    h5_idx = 0 \n    for filename in filelist : \n        print ( \"\\nReading %s header...\" % filename ) \n        r = GuppiRaw ( filename ) \n        h5 = h5py . File ( filename + '.h5' , 'w' ) \n        header , data = r . read_next_data_block ( ) \n        for ii in range ( 0 , r . n_blocks ) : \n            t0 = time . time ( ) \n            print ( \"Reading block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            header , data = r . read_next_data_block ( ) \n            t1 = time . time ( ) \n            t2 = time . time ( ) \n            print ( \"Writing block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            dset [ h5_idx , : ] = data \n            t3 = time . time ( ) \n            print ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) \n            h5_idx += 1 \n            for key , value in header . items ( ) : \n                dset . attrs [ key ] = value \n        h5 . close ( ) \n        t1 = time . time ( ) \n        print ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) ) "}
{"7907": "\ndef makePlot ( args ) : \n    gmag = np . linspace ( 3.0 , 20.0 , 171 ) \n    vmini = args [ 'vmini' ] \n    vmag = gmag - gminvFromVmini ( vmini ) \n    if args [ 'eom' ] : \n        sigmaG = gMagnitudeErrorEoM ( gmag ) \n        sigmaGBp = bpMagnitudeErrorEoM ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeErrorEoM ( gmag , vmini ) \n        pass \n    else : \n        sigmaG = gMagnitudeError ( gmag ) \n        sigmaGBp = bpMagnitudeError ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeError ( gmag , vmini ) \n        pass \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    if ( args [ 'vmagAbscissa' ] ) : \n        plt . semilogy ( vmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( vmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( vmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6 , 20 ) ) \n        plt . legend ( loc = 0 ) \n        plt . xlabel ( '$V$ [mag]' ) \n    else : \n        pass \n        plt . semilogy ( gmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( gmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( gmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6 , 20 ) ) \n        plt . legend ( loc = 0 ) \n        plt . xlabel ( '$G$ [mag]' ) \n    plt . xticks ( np . arange ( 6 , 20 , 2 ) ) \n    pass \n    plt . grid ( which = 'both' ) \n    plt . ylabel ( 'Photometric error [mag]' ) \n    if args [ 'eom' ] : \n        plt . title ( 'End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14 ) \n    else : \n        plt . title ( 'Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14 ) \n    basename = 'PhotometricErrors' \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( basename + '.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( basename + '.png' ) \n    else : \n        plt . show ( ) "}
{"7914": "\ndef makePlot ( pdf = False , png = False ) : \n    logdistancekpc = np . linspace ( - 1 , np . log10 ( 20.0 ) , 100 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    pass \n    pass \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ 0 ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( vmag > 14 ) & ( vmag < 16 ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ 1 ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ 1 ] ) \n        relparerrors = parerrors * 10 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10 ** logdistancekpc , relparerrors , '--k' , lw = 1 ) \n        plt . loglog ( 10 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n    else : \n        plt . show ( ) "}
{"7915": "\ndef makePlot ( args ) : \n    gRvs = np . linspace ( 5.7 , 16.1 , 101 ) \n    spts = [ 'B0V' , 'B5V' , 'A0V' , 'A5V' , 'F0V' , 'G0V' , 'G5V' , 'K0V' , 'K1IIIMP' , 'K4V' , 'K1III' ] \n    pass \n    deltaHue = 240.0 / ( len ( spts ) - 1 ) \n    hsv = np . zeros ( ( 1 , 1 , 3 ) ) \n    hsv [ 0 , 0 , 1 ] = 1.0 \n    hsv [ 0 , 0 , 2 ] = 0.9 \n    count = 0 \n    for spt in spts : \n        hsv [ 0 , 0 , 0 ] = ( 240 - count * deltaHue ) / 360.0 \n        vmag = vminGrvsFromVmini ( vminiFromSpt ( spt ) ) + gRvs \n        vradErrors = vradErrorSkyAvg ( vmag , spt ) \n        plt . plot ( vmag , vradErrors , '-' , label = spt , color = hsv_to_rgb ( hsv ) [ 0 , 0 , : ] ) \n        count += 1 \n    plt . grid ( which = 'both' ) \n    plt . xlim ( 9 , 17.5 ) \n    plt . ylim ( 0 , 20 ) \n    plt . xticks ( np . arange ( 9 , 18 , 1 ) ) \n    plt . yticks ( np . arange ( 0 , 20.5 , 5 ) ) \n    plt . xlabel ( '$V$ [mag]' ) \n    plt . ylabel ( 'End-of-mission radial velocity error [km s$^{-1}$]' ) \n    leg = plt . legend ( loc = 0 , handlelength = 2.0 , labelspacing = 0.10 ) \n    for t in leg . get_texts ( ) : \n        t . set_fontsize ( 12 ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.png' ) \n    else : \n        plt . show ( ) "}
{"8050": "\ndef register_record_hook ( self , hook ) : \n    for key in hook . keys : \n        if key in self . _record_hooks : \n            pass \n            raise FieldValidationError ( f\"Reference key already exists, can't create: {key}. \" f\"{field_descriptor.get_error_location_message(hook.target_value, hook.target_index)}\" ) \n        self . _record_hooks [ key ] = hook "}
{"8051": "\ndef register_link ( self , link ) : \n    keys = tuple ( ( ref , link . initial_hook_value ) for ref in link . hook_references ) \n    for k in keys : \n        if k in self . _record_hooks : \n            link . set_target ( target_record = self . _record_hooks [ k ] . target_record ) \n            break \n    else : \n        for k in keys : \n            if k in self . _table_hooks : \n                link . set_target ( target_table = self . _table_hooks [ k ] ) \n                break \n        else : \n            pass \n            raise FieldValidationError ( f\"No object found with any of given references : {keys}. \" f\"{field_descriptor.get_error_location_message(link.initial_hook_value)}\" ) \n    if link . source_record not in self . _links_by_source : \n        self . _links_by_source [ link . source_record ] = set ( ) \n    self . _links_by_source [ link . source_record ] . add ( link ) \n    if link . target not in self . _links_by_target : \n        self . _links_by_target [ link . target ] = set ( ) \n    self . _links_by_target [ link . target ] . add ( link ) "}
{"8119": "\ndef is_effective_member ( self , group_id , netid ) : \n    self . _valid_group_id ( group_id ) \n    netid = re . sub ( '@washington.edu' , '' , netid ) \n    url = \"{}/group/{}/effective_member/{}\" . format ( self . API , group_id , netid ) \n    try : \n        pass \n        return True \n    except DataFailureException as ex : \n        if ex . status == 404 : \n            return False \n        else : \n            raise "}
{"8479": "\ndef _merge_single_runs ( self , other_trajectory , used_runs ) : \n    pass \n    run_indices = range ( len ( other_trajectory ) ) \n    run_name_dict = OrderedDict ( ) \n    pass \n    for idx in run_indices : \n        if idx in used_runs : \n            other_info_dict = other_trajectory . f_get_run_information ( idx ) \n            time_ = other_info_dict [ 'time' ] \n            timestamp = other_info_dict [ 'timestamp' ] \n            completed = other_info_dict [ 'completed' ] \n            short_environment_hexsha = other_info_dict [ 'short_environment_hexsha' ] \n            finish_timestamp = other_info_dict [ 'finish_timestamp' ] \n            runtime = other_info_dict [ 'runtime' ] \n            new_idx = used_runs [ idx ] \n            new_runname = self . f_wildcard ( '$' , new_idx ) \n            run_name_dict [ idx ] = new_runname \n            info_dict = dict ( idx = new_idx , time = time_ , timestamp = timestamp , completed = completed , short_environment_hexsha = short_environment_hexsha , finish_timestamp = finish_timestamp , runtime = runtime ) \n            self . _add_run_info ( ** info_dict ) "}
{"8759": "\ndef _delete_external_tool ( self , context , context_id , external_tool_id ) : \n    url = context . format ( context_id ) + \"/external_tools/{}\" . format ( external_tool_id ) \n    pass \n    return True "}
{"8777": "\ndef delete_admin ( self , account_id , user_id , role ) : \n    url = ADMINS_API . format ( account_id ) + \"/{}?role={}\" . format ( user_id , quote ( role ) ) \n    pass \n    return True "}
{"8810": "\ndef delete_report ( self , report ) : \n    url = ACCOUNTS_API . format ( report . account_id ) + \"/reports/{}/{}\" . format ( report . type , report . report_id ) \n    pass \n    return True "}
{"8822": "\nasync def _handle_home ( self , request : Request ) -> Response : \n    if self . description : \n        pass \n    else : \n        pass \n    text = dedent ( f'''<!DOCTYPE html>            <html>              <head>                <title>{title}</title>              </head>              <body>                <h1>{title}</h1>                <p>                  Metric are exported at the                  <a href=\"/metrics\">/metrics</a> endpoint.                </p>              </body>            </html>            ''' ) \n    return Response ( content_type = 'text/html' , text = text ) "}
{"8878": "\ndef map_lrepr ( entries : Callable [ [ ] , Iterable [ Tuple [ Any , Any ] ] ] , start : str , end : str , meta = None , ** kwargs , ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and print_level < 1 : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    def entry_reprs ( ) : \n        for k , v in entries ( ) : \n            yield \"{k} {v}\" . format ( k = lrepr ( k , ** kwargs ) , v = lrepr ( v , ** kwargs ) ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( entry_reprs ( ) ) . take ( print_length + 1 ) . to_list ( ) \n        if len ( items ) > print_length : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = list ( entry_reprs ( ) ) \n    pass \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8927": "\ndef find_spec ( self , fullname : str , path , target : types . ModuleType = None , ) -> Optional [ importlib . machinery . ModuleSpec ] : \n    package_components = fullname . split ( \".\" ) \n    if path is None : \n        path = sys . path \n        pass \n    else : \n        pass \n    for entry in path : \n        filenames = [ f\"{os.path.join(entry, *module_name, '__init__')}.lpy\" , f\"{os.path.join(entry, *module_name)}.lpy\" , ] \n        for filename in filenames : \n            if os . path . exists ( filename ) : \n                state = { \"fullname\" : fullname , \"filename\" : filename , \"path\" : entry , \"target\" : target , \"cache_filename\" : _cache_from_source ( filename ) , } \n                logger . debug ( f\"Found potential Basilisp module '{fullname}' in file '{filename}'\" ) \n                return importlib . machinery . ModuleSpec ( fullname , self , origin = filename , loader_state = state ) \n    return None "}
{"8964": "\ndef _var_sym_to_py_ast ( ctx : GeneratorContext , node : VarRef , is_assigning : bool = False ) -> GeneratedPyAST : \n    assert node . op == NodeOp . VAR \n    var = node . var \n    ns = var . ns \n    ns_name = ns . name \n    ns_module = ns . module \n    pass \n    var_name = var . name . name \n    py_var_ctx = ast . Store ( ) if is_assigning else ast . Load ( ) \n    if node . return_var : \n        return GeneratedPyAST ( node = ast . Call ( func = _FIND_VAR_FN_NAME , args = [ ast . Call ( func = _NEW_SYM_FN_NAME , args = [ ast . Str ( var_name ) ] , keywords = [ ast . keyword ( arg = \"ns\" , value = ast . Str ( ns_name ) ) ] , ) ] , keywords = [ ] , ) ) \n    if ctx . use_var_indirection or _is_dynamic ( var ) or _is_redefable ( var ) : \n        return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) \n    safe_name = munge ( var_name ) \n    if safe_name not in ns_module . __dict__ : \n        safe_name = munge ( var_name , allow_builtins = True ) \n    if safe_name in ns_module . __dict__ : \n        if ns is ctx . current_ns : \n            return GeneratedPyAST ( node = ast . Name ( id = safe_name , ctx = py_var_ctx ) ) \n        return GeneratedPyAST ( node = _load_attr ( f\"{safe_ns}.{safe_name}\" , ctx = py_var_ctx ) ) \n    if ctx . warn_on_var_indirection : \n        logger . warning ( f\"could not resolve a direct link to Var '{var_name}'\" ) \n    return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) "}
{"9334": "\ndef CalculateBestPosition ( self , widget ) : \n    if isinstance ( widget , wx . Frame ) : \n        pass \n        left , top = widget . ClientToScreenXY ( 0 , 0 ) \n        right , bottom = widget . ClientToScreenXY ( * widget . GetClientRect ( ) [ 2 : ] ) \n        size = self . GetSize ( ) \n        xpos = right \n        ypos = bottom - size [ 1 ] \n        self . SetPosition ( ( xpos , ypos ) ) \n    else : \n        STT . ToolTipWindow . CalculateBestPosition ( self , widget ) "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max_cols = 80 ) : \n    try : \n        pass \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + 1 + indent * 4 + ( 5 if context else 0 ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if k == \"index\" : \n                continue \n            if k == \"parent\" and parent != \"\" : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if len ( line + param ) + 3 > max_cols - padding : \n                param_lines . append ( line ) \n                line = \"\" \n            line += param + \", \" \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9378": "\ndef alert ( message , title = \"\" , parent = None , scrolled = False , icon = \"exclamation\" ) : \n    if not scrolled : \n        icons = { 'exclamation' : wx . ICON_EXCLAMATION , 'error' : wx . ICON_ERROR , 'question' : wx . ICON_QUESTION , 'info' : wx . ICON_INFORMATION } \n        style = wx . OK | icons [ icon ] \n        pass \n    else : \n        pass "}
{"9380": "\ndef select_font ( message = \"\" , title = \"\" , font = None , parent = None ) : \n    if font is not None : \n        wx_font = font . _get_wx_font ( ) \n    else : \n        wx_font = None \n        font = Font ( ) \n    result = dialogs . fontDialog ( parent , font = wx_font ) \n    if result . accepted : \n        pass \n        result . color = result . fontData . GetColour ( ) . Get ( ) \n        wx_font = result . fontData . GetChosenFont ( ) \n        font . set_wx_font ( wx_font ) \n        wx_font = None \n        return font "}
{"9682": "\ndef __draw_constant_line ( self , value_label_style ) : \n    value , label , style = value_label_style \n    start = self . transform_output_coordinates ( ( 0 , value ) ) [ 1 ] \n    pass \n    path = etree . SubElement ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constantLine' } ) \n    if style : \n        path . set ( 'style' , style ) \n    text = etree . SubElement ( self . graph , 'text' , { 'x' : str ( 2 ) , 'y' : str ( start - 2 ) , 'class' : 'constantLine' } ) \n    text . text = label "}
{"9683": "\ndef load_transform_parameters ( self ) : \n    x_min , x_max , x_div = self . x_range ( ) \n    y_min , y_max , y_div = self . y_range ( ) \n    pass \n    pass \n    self . __transform_parameters = dict ( locals ( ) ) \n    del self . __transform_parameters [ 'self' ] "}
{"9705": "\ndef run_bot ( bot_class , host , port , nick , channels = None , ssl = None ) : \n    conn = IRCConnection ( host , port , nick , ssl ) \n    pass \n    while 1 : \n        if not conn . connect ( ) : \n            break \n        channels = channels or [ ] \n        for channel in channels : \n            conn . join ( channel ) \n        conn . enter_event_loop ( ) "}
{"9904": "\ndef handleCONNACK ( self , response ) : \n    pass \n    log . error ( \"Unexpected {packet:7} packet received in {log_source}\" , packet = \"CONNACK\" ) "}
{"10005": "\ndef filter_reports ( self , analytes , filt_str = 'all' , nbin = 5 , samples = None , outdir = None , subset = 'All_Samples' ) : \n    if outdir is None : \n        outdir = self . report_dir + '/filters/' + filt_str \n        if not os . path . isdir ( self . report_dir + '/filters' ) : \n            os . mkdir ( self . report_dir + '/filters' ) \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            pass \n            prog . update ( ) \n    return "}
{"10014": "\ndef pca_plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = True ) : \n    nc = pca . n_components \n    f = np . arange ( pca . n_features_ ) \n    pass \n    ind = ~ np . apply_along_axis ( any , 1 , np . isnan ( dt ) ) \n    cylim = ( pca . components_ . min ( ) , pca . components_ . max ( ) ) \n    yd = cylim [ 1 ] - cylim [ 0 ] \n    fig , axs = plt . subplots ( nc , nc , figsize = [ 3 * nc , nc * 3 ] , tight_layout = True ) \n    for x , y in zip ( * np . triu_indices ( nc ) ) : \n        if x == y : \n            tax = axs [ x , y ] \n            tax . bar ( f , pca . components_ [ x ] , 0.8 ) \n            tax . set_xticks ( [ ] ) \n            tax . axhline ( 0 , zorder = - 1 , c = ( 0 , 0 , 0 , 0.6 ) ) \n            tax . set_ylim ( cylim [ 0 ] - 0.2 * yd , cylim [ 1 ] + 0.2 * yd ) \n            for xi , yi , lab in zip ( f , pca . components_ [ x ] , xlabs ) : \n                if yi > 0 : \n                    yo = yd * 0.03 \n                    va = 'bottom' \n                else : \n                    yo = yd * - 0.02 \n                    va = 'top' \n                tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90 , fontsize = 8 ) \n        else : \n            xv = dt [ ind , x ] \n            yv = dt [ ind , y ] \n            if mode == 'scatter' : \n                axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) \n                axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) \n            if mode == 'hist2d' : \n                if lognorm : \n                    norm = mpl . colors . LogNorm ( ) \n                else : \n                    norm = None \n                axs [ x , y ] . hist2d ( xv , yv , 50 , cmap = plt . cm . Blues , norm = norm ) \n                axs [ y , x ] . hist2d ( yv , xv , 50 , cmap = plt . cm . Blues , norm = norm ) \n        if x == 0 : \n            axs [ y , x ] . set_ylabel ( 'PC{:.0f}' . format ( y + 1 ) ) \n        if y == nc - 1 : \n            axs [ y , x ] . set_xlabel ( 'PC{:.0f}' . format ( x + 1 ) ) \n    return fig , axs , xv , yv "}
{"10123": "\ndef staff_products_form_factory ( user ) : \n    products = inventory . Product . objects . all ( ) \n    products = ProductController . available_products ( user , products = products ) \n    product_ids = [ product . id for product in products ] \n    product_set = inventory . Product . objects . filter ( id__in = product_ids ) \n    class StaffProductsForm ( forms . Form ) : \n        product = forms . ModelChoiceField ( widget = forms . Select , queryset = product_set , ) \n        pass \n    return StaffProductsForm "}
{"10241": "\ndef ekm_log ( logstr , priority = 3 ) : \n    if priority <= ekmmeters_log_level : \n        pass \n        stamp = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M.%f\" ) \n        ekmmeters_log_func ( \"[EKM Meter Debug Message: \" + stamp + \"] -> \" + logstr ) \n    pass "}
{"10320": "\ndef increment_times_modified ( self ) : \n    pass \n    self . conn . client . pexpire ( self . times_modified_key , phonon . s_to_ms ( TTL ) ) "}
{"10453": "\ndef createClient ( self ) : \n    pass \n    pass \n    self . client = elasticsearch . ElasticSearch ( self . url , self . user , self . password , self . index ) \n    self . t . start ( self . inter ) "}
{"10573": "\ndef sync ( self ) : \n    logging . debug ( \"Logger: Syncing...\" ) \n    pass \n    try : \n        cdb = self . connectordb \n        cdb . ping ( ) \n        with self . synclock : \n            c = self . database . cursor ( ) \n            for stream in self . streams : \n                s = cdb [ stream ] \n                c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) \n                datapointArray = [ ] \n                for dp in c . fetchall ( ) : \n                    datapointArray . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2 ] ) } ) \n                if len ( s ) > 0 : \n                    newtime = s [ - 1 ] [ \"t\" ] \n                    while ( len ( datapointArray ) > 0 and datapointArray [ 0 ] [ \"t\" ] < newtime ) : \n                        logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) \n                        datapointArray = datapointArray [ 1 : ] \n                if len ( datapointArray ) > 0 : \n                    logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapointArray ) ) ) \n                    while ( len ( datapointArray ) > DATAPOINT_INSERT_LIMIT ) : \n                        s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) \n                        datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] \n                        c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapointArray [ 0 ] [ \"t\" ] ) ) \n                    s . insert_array ( datapointArray ) \n                    c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapointArray [ - 1 ] [ \"t\" ] ) ) \n            self . lastsynctime = time . time ( ) \n            if self . onsync is not None : \n                self . onsync ( ) \n    except Exception as e : \n        pass \n        reraise = self . syncraise \n        if self . onsyncfail is not None : \n            reraise = self . onsyncfail ( e ) \n        if reraise : \n            raise "}
{"10818": "\ndef kill_conditional_comments ( self , doc ) : \n    pass \n    self . _kill_elements ( doc , lambda el : _conditional_comment_re . search ( el . text ) , etree . Comment ) "}
{"11003": "\ndef configure_dot_code ( self , info ) : \n    if not info . initialized : \n        return \n    self . dot_code = str ( self . model ) \n    pass "}
{"11060": "\ndef parse_xdot_drawing_directive ( self , new ) : \n    components = XdotAttrParser ( ) . parse_xdot_data ( new ) \n    pass \n    pass \n    pos_x = min ( [ c . x for c in components ] ) \n    pos_y = min ( [ c . y for c in components ] ) \n    move_to_origin ( components ) \n    container = Container ( auto_size = True , position = [ pos_x - self . pos [ 0 ] , pos_y - self . pos [ 1 ] ] , bgcolor = \"blue\" ) \n    container . add ( * components ) \n    self . drawing = container "}
{"11131": "\ndef convert_to_mp3 ( file_name , delete_queue ) : \n    file = os . path . splitext ( file_name ) \n    if file [ 1 ] == '.mp3' : \n        log . info ( f\"{file_name} is already a MP3 file, no conversion needed.\" ) \n        return file_name \n    new_file_name = file [ 0 ] + '.mp3' \n    ff = FFmpeg ( inputs = { file_name : None } , outputs = { new_file_name : None } ) \n    log . info ( f\"Conversion for {file_name} has started\" ) \n    pass \n    try : \n        ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) \n    except FFRuntimeError : \n        os . remove ( new_file_name ) \n        ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) \n    pass \n    log . info ( f\"Conversion for {file_name} has finished in {end_time - start_time} seconds\" ) \n    delete_queue . put ( file_name ) \n    return new_file_name "}
{"11134": "\ndef init_celery ( app , celery ) : \n    celery . conf . update ( app . config ) \n    TaskBase = celery . Task \n    class ContextTask ( TaskBase ) : \n        pass \n        def __call__ ( self , * args , ** kwargs ) : \n            with app . app_context ( ) : \n                return TaskBase . __call__ ( self , * args , ** kwargs ) \n    celery . Task = ContextTask \n    return celery "}
{"11144": "\ndef download ( self , url ) : \n    try : \n        yt = YouTube ( url ) \n    except RegexMatchError : \n        log . error ( f\"Cannot download file at {url}\" ) \n    else : \n        stream = yt . streams . first ( ) \n        log . info ( f\"Download for {stream.default_filename} has started\" ) \n        pass \n        stream . download ( ) \n        pass \n        log . info ( f\"Download for {stream.default_filename} has finished in {end_time - start_time} seconds\" ) \n        return stream . default_filename "}
{"11145": "\ndef connect ( self ) : \n    SCOPES = 'https://www.googleapis.com/auth/drive' \n    store = file . Storage ( 'drive_credentials.json' ) \n    creds = store . get ( ) \n    if not creds or creds . invalid : \n        try : \n            flow = client . flow_from_clientsecrets ( 'client_secret.json' , SCOPES ) \n        except InvalidClientSecretsError : \n            log . error ( 'ERROR: Could not find client_secret.json in current directory, please obtain it from the API console.' ) \n            return \n        creds = tools . run_flow ( flow , store ) \n    self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) \n    response = self . connection . files ( ) . list ( q = \"name='Music' and mimeType='application/vnd.google-apps.folder' and trashed=false\" ) . execute ( ) \n    try : \n        pass \n    except IndexError : \n        log . warning ( 'Music folder is missing. Creating it.' ) \n        folder_metadata = { 'name' : 'Music' , 'mimeType' : 'application/vnd.google-apps.folder' } \n        pass "}
{"11146": "\ndef upload ( self , file_name ) : \n    response = self . connection . files ( ) . list ( q = \"name='Music' and mimeType='application/vnd.google-apps.folder' and trashed=false\" ) . execute ( ) \n    folder_id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] \n    file_metadata = { 'name' : file_name , 'parents' : [ folder_id ] } \n    media = MediaFileUpload ( file_name , mimetype = 'audio/mpeg' ) \n    log . info ( f\"Upload for {file_name} has started\" ) \n    pass \n    self . connection . files ( ) . create ( body = file_metadata , media_body = media , fields = 'id' ) . execute ( ) \n    pass \n    log . info ( f\"Upload for {file_name} has finished in {end_time - start_time} seconds\" ) \n    return file_name "}
{"11163": "\ndef read_pr_report ( self , filename ) : \n    pass \n    f = open ( filename ) \n    while f : \n        line = f . readline ( ) \n        if not line : \n            pass \n            break \n        if \"# Quad solid angle mean point theta table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_theta' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"# Quad solid angle mean point phi table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_phi' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"L_w band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_w_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n        if \"L_it band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_it_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n    return self . data_dictionary "}
{"11196": "\ndef quit ( self ) : \n    if self . is_running == True : \n        pass \n    else : \n        quit = QtGui . QMessageBox . question ( self . ui . quit , 'Quit PlanarRad' , \"Are you sure to quit ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if quit == QtGui . QMessageBox . Yes : \n            QtGui . qApp . quit ( ) "}
{"11206": "\ndef hash ( self ) : \n    if self . sender is None : \n        raise MissingSignatureError ( ) \n    class HashSerializable ( rlp . Serializable ) : \n        fields = [ ( field , sedes ) for field , sedes in self . fields if field not in ( 'v' , 'r' , 's' ) ] + [ ( '_sender' , binary ) ] \n        pass \n    return sha3 ( rlp . encode ( self , HashSerializable ) ) "}
{"11228": "\ndef finish ( self ) : \n    if self . finished : \n        return self . exit_code \n    pass \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != 0 : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    if self . applied_count > 0 : \n        self . _end_acquisition ( ) \n        self . _apply_rows ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != 0 : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    self . finished = True \n    return self . exit_code "}
{"11229": "\ndef from_file ( self , filename , table = None , delimiter = '|' , null = 'NULL' , panic = True , quotechar = '\"' , parse_dates = False ) : \n    if not self . table : \n        if not table : \n            raise GiraffeError ( \"Table must be set or specified to load a file.\" ) \n        self . table = table \n    if not isinstance ( null , basestring ) : \n        raise GiraffeError ( \"Expected 'null' to be str, received {}\" . format ( type ( null ) ) ) \n    with Reader ( filename , delimiter = delimiter , quotechar = quotechar ) as f : \n        if not isinstance ( f . delimiter , basestring ) : \n            raise GiraffeError ( \"Expected 'delimiter' to be str, received {}\" . format ( type ( delimiter ) ) ) \n        self . columns = f . header \n        if isinstance ( f , ArchiveFileReader ) : \n            self . mload . set_encoding ( ROW_ENCODING_RAW ) \n            self . preprocessor = lambda s : s \n        if parse_dates : \n            self . preprocessor = DateHandler ( self . columns ) \n        self . _initiate ( ) \n        self . mload . set_null ( null ) \n        self . mload . set_delimiter ( delimiter ) \n        i = 0 \n        for i , line in enumerate ( f , 1 ) : \n            self . put ( line , panic = panic ) \n            if i % self . checkpoint_interval == 1 : \n                log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) , console = True ) \n                pass \n                self . exit_code = self . _exit_code ( ) \n                if self . exit_code != 0 : \n                    return self . exit_code \n        log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) ) \n        return self . finish ( ) "}
{"11230": "\ndef put ( self , items , panic = True ) : \n    if not self . initiated : \n        self . _initiate ( ) \n    try : \n        pass \n        self . applied_count += 1 \n    except ( TeradataPTError , EncoderError ) as error : \n        self . error_count += 1 \n        if panic : \n            raise error \n        log . info ( \"BulkLoad\" , error ) "}
{"11242": "\ndef to_archive ( self , writer ) : \n    if 'b' not in writer . mode : \n        raise GiraffeError ( \"Archive writer must be in binary mode\" ) \n    writer . write ( GIRAFFE_MAGIC ) \n    writer . write ( self . columns . serialize ( ) ) \n    pass \n    for n , chunk in enumerate ( self . _fetchall ( ROW_ENCODING_RAW ) , 1 ) : \n        writer . write ( chunk ) \n        yield TeradataEncoder . count ( chunk ) "}
{"11276": "\ndef watch_port_events ( port , chip , pin_function_maps , event_queue , return_after_kbdint = False ) : \n    gpio25 = open ( GPIO_INTERRUPT_DEVICE_VALUE , 'r' ) \n    epoll = select . epoll ( ) \n    epoll . register ( gpio25 , select . EPOLLIN | select . EPOLLET ) \n    while True : \n        try : \n            pass \n        except KeyboardInterrupt as e : \n            if return_after_kbdint : \n                return \n            else : \n                raise e \n        except IOError as e : \n            if e . errno != errno . EINTR : \n                raise \n        if port == pifacecommon . mcp23s17 . GPIOA : \n            interrupt_flag = chip . intfa . value \n        else : \n            interrupt_flag = chip . intfb . value \n        if interrupt_flag == 0 : \n            continue \n        else : \n            if port == pifacecommon . mcp23s17 . GPIOA : \n                interrupt_capture = chip . intcapa . value \n            else : \n                interrupt_capture = chip . intcapb . value \n            event_queue . add_event ( InterruptEvent ( interrupt_flag , interrupt_capture , chip , time . time ( ) ) ) \n    epoll . close ( ) "}
{"11607": "\ndef resize_pbc_for_lipids ( pbc , relL , relU , absL , absU , uparea , area , hole , proteins ) : \n    if any ( relL ) and any ( relU ) : \n        if 0 in ( pbc . x , pbc . y , pbc . z ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n    elif any ( absL ) or any ( absU ) : \n        if pbc . z == 0 : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n        if 0 in ( pbc . x , pbc . y ) : \n            pbc . x = pbc . y = 1 \n        upsize = sum ( absU ) * uparea \n        losize = sum ( absL ) * area \n        holesize = np . pi * hole ** 2 \n        xysize = pbc . x * pbc . y \n        psize_up = sum ( [ p . areaxy ( 0 , 2.4 ) for p in proteins ] ) \n        psize_lo = sum ( [ p . areaxy ( - 2.4 , 0 ) for p in proteins ] ) \n        unavail_up = holesize + psize_up \n        unavail_lo = holesize + psize_lo \n        upscale = ( upsize + unavail_up ) / xysize \n        loscale = ( losize + unavail_lo ) / xysize \n        area_scale = max ( upscale , loscale ) \n        aspect_ratio = pbc . x / pbc . y \n        pass \n        pass \n        pbc . box [ : 2 , : ] *= math . sqrt ( area_scale ) "}
{"11633": "\ndef validate_input ( msg_type , attr , value ) : \n    try : \n        pass \n    except KeyError : \n        return 1 \n    else : \n        return 0 "}
{"11726": "\ndef filter_glyph_names ( alist , filter ) : \n    pass \n    extras = [ ] \n    for name in alist : \n        try : \n            pass \n        except : \n            extras . append ( name ) \n    return extras "}
{"11818": "\ndef get ( self ) : \n    self . write ( \"Memory Session Object Demo:\" ) \n    if \"sv\" in self . session : \n        pass \n        self . write ( \"current sv value is %s, and system will delete this value.<br/>\" % self . session [ \"sv\" ] ) \n        self . session . delete ( \"sv\" ) \n        if \"sv\" not in self . session : \n            self . write ( \"current sv value is empty\" ) \n    else : \n        self . write ( \"Session data not found\" ) "}
{"12023": "\ndef save ( tiff_filename , numpy_data ) : \n    tiff_filename = os . path . expanduser ( tiff_filename ) \n    if type ( numpy_data ) is str : \n        fp = open ( png_filename , \"wb\" ) \n        fp . write ( numpy_data ) \n        fp . close ( ) \n        return png_filename \n    try : \n        pass \n    except Exception as e : \n        raise ValueError ( \"Could not save TIFF file {0}.\" . format ( tiff_filename ) ) \n    return tiff_filename "}
{"12100": "\ndef load_plugins ( self , directory ) : \n    for filename in os . listdir ( directory ) : \n        filepath = os . path . join ( directory , filename ) \n        modname , ext = os . path . splitext ( filename ) \n        if os . path . isfile ( filepath ) and ext == '.py' : \n            file , path , descr = imp . find_module ( modname , [ directory ] ) \n            if file : \n                pass \n        if os . path . isdir ( filepath ) : \n            self . load_plugins ( filepath ) "}
{"12158": "\ndef add_pie_chart ( self , data , cursor , width , height , title = None , data_type = \"raw\" , fill_colors = None , labels = False , background = None , legend = None ) : \n    save_draw_color = self . draw_color \n    save_fill_color = self . fill_color \n    pass \n    self . set_draw_color ( save_draw_color ) \n    self . set_fill_color ( save_fill_color ) "}
{"12269": "\ndef defaults_docstring ( defaults , header = None , indent = None , footer = None ) : \n    if indent is None : \n        indent = '' \n    if header is None : \n        header = '' \n    if footer is None : \n        footer = '' \n    pass \n    hbar = '\\n' \n    s = hbar + ( header ) + hbar \n    for key , value , desc in defaults : \n        if isinstance ( value , basestring ) : \n            value = \"'\" + value + \"'\" \n        if hasattr ( value , '__call__' ) : \n            value = \"<\" + value . __name__ + \">\" \n        s += indent + '%-12s\\n' % ( \"%s :\" % key ) \n        s += indent + indent + ( indent + 23 * ' ' ) . join ( desc . split ( '\\n' ) ) \n        s += ' [%s]\\n\\n' % str ( value ) \n    s += hbar \n    s += footer \n    return s "}
{"12773": "\ndef fix_emails ( text ) : \n    emails = bracket_emails . findall ( text ) \n    pass \n    for email in emails : \n        _email = email . replace ( \"<\" , \"&lt;\" ) . replace ( \">\" , \"&gt;\" ) \n        text = text . replace ( email , _email ) \n    return text "}
{"12883": "\ndef _check_inputs ( self ) : \n    try : \n        pass \n    except TypeError : \n        raise RuntimeError ( \"inputs should be iterable but found type='{0}', value=\" \"'{1}'\" . format ( type ( self . _inputs ) , str ( self . _inputs ) ) ) \n    from melody . inputs import Input \n    for check_input in self . _inputs : \n        if not isinstance ( check_input , Input ) : \n            raise RuntimeError ( \"input should be a subclass of the Input class but \" \"found type='{0}', value='{1}'\" . format ( type ( check_input ) , str ( check_input ) ) ) "}
{"12893": "\ndef save_into_qrcode ( text , out_filepath , color = '' , box_size = 10 , pixel_size = 1850 ) : \n    try : \n        qr = qrcode . QRCode ( version = 1 , error_correction = qrcode . constants . ERROR_CORRECT_L , box_size = box_size , border = 0 , ) \n        qr . add_data ( text ) \n        qr . make ( fit = True ) \n    except Exception as exc : \n        raise Exception ( 'Error trying to generate QR code ' ' from `vcard_string`: {}' . format ( text ) ) from exc \n    else : \n        img = qr . make_image ( image_factory = qrcode . image . svg . SvgPathImage ) \n    pass \n    if color : \n        replace_file_content ( out_filepath , 'fill:#000000' , 'fill:#{}' . format ( color ) ) "}
{"12894": "\ndef launch ( option ) : \n    from melody . inputs import create_input \n    pass \n    success = True \n    results = None \n    if success : \n        results = { \"rate\" : { \"value\" : 35 , \"units\" : \"ns/day\" } , } \n    return success , results "}
{"12935": "\ndef geom_to_array ( geom , size , affine ) : \n    driver = ImageDriver ( 'MEM' ) \n    rast = driver . raster ( driver . ShortName , size ) \n    rast . affine = affine \n    rast . sref = geom . GetSpatialReference ( ) \n    with MemoryLayer . from_records ( [ ( 1 , geom ) ] ) as ml : \n        pass \n    arr = rast . array ( ) \n    rast . close ( ) \n    return arr "}
{"12936": "\ndef rasterize ( layer , rast ) : \n    driver = ImageDriver ( 'MEM' ) \n    r2 = driver . raster ( driver . ShortName , rast . size ) \n    r2 . affine = rast . affine \n    sref = rast . sref \n    if not sref . srid : \n        sref = SpatialReference ( 4326 ) \n    r2 . sref = sref \n    ml = MemoryLayer ( sref , layer . GetGeomType ( ) ) \n    ml . load ( layer ) \n    pass \n    ml . close ( ) \n    return r2 "}
{"13048": "\ndef render_row ( num , columns , widths , column_colors = None ) : \n    pass \n    cell_strs = [ ] \n    for i , column in enumerate ( columns ) : \n        try : \n            cell = column [ num ] \n            spaces = ' ' * ( widths [ i ] - len ( cell ) ) \n            if column_colors is not None and column_colors [ i ] is not None : \n                cell = column_colors [ i ] ( cell ) \n            cell_strs . append ( ' %s%s ' % ( cell , spaces ) ) \n        except IndexError : \n            cell_strs . append ( ' ' * ( widths [ i ] + 2 ) ) \n    return '|%s|' % '|' . join ( cell_strs ) "}
{"13219": "\ndef output ( self , fieldNames = None , datemap = None , time_format = None ) : \n    pass "}
{"13268": "\ndef _cmdRegex ( self , cmd_grp = None ) : \n    cmd_grp = cmd_grp or \"cmd\" \n    help_opts = ( \"-h\" , \"--help\" ) \n    cmd = self . name ( ) \n    pass \n    opts = [ ] \n    for action in self . parser . _actions : \n        opts += [ a for a in action . option_strings if a not in help_opts ] \n    opts_re = \"|\" . join ( [ re . escape ( o ) for o in opts ] ) \n    if opts_re : \n        opts_re = rf\"(\\s+(?P<{cmd_grp}_opts>{opts_re}))*\" \n    pass \n    pass \n    completers = { } \n    if opts_re : \n        completers [ f\"{cmd_grp}_opts\" ] = WordCompleter ( opts ) \n    return tuple ( [ rf\"\"\"(?P<{cmd_grp}>{names}){opts_re}{help_re}\"\"\" , completers ] ) "}
{"13273": "\ndef dump ( filename , dbname , username = None , password = None , host = None , port = None , tempdir = '/tmp' , pg_dump_path = 'pg_dump' , format = 'p' ) : \n    pass \n    cmd = pg_dump_path \n    cmd += ' --format %s' % format \n    cmd += ' --file ' + os . path . join ( tempdir , filename ) \n    if username : \n        cmd += ' --username %s' % username \n    if host : \n        cmd += ' --host %s' % host \n    if port : \n        cmd += ' --port %s' % port \n    cmd += ' ' + dbname \n    if password : \n        os . environ [ \"PGPASSWORD\" ] = password \n    return sh ( cmd ) "}
{"13331": "\ndef dump ( filename , dbname , username = None , password = None , host = None , port = None , tempdir = '/tmp' , mysqldump_path = 'mysqldump' ) : \n    pass \n    cmd = mysqldump_path \n    cmd += ' --result-file=' + os . path . join ( tempdir , filename ) \n    if username : \n        cmd += ' --user=%s' % username \n    if host : \n        cmd += ' --host=%s' % host \n    if port : \n        cmd += ' --port=%s' % port \n    if password : \n        cmd += ' --password=%s' % password \n    cmd += ' ' + dbname \n    return sh ( cmd ) "}
{"13342": "\ndef scrape ( ctx , url ) : \n    data = load_feed ( url ) \n    pass \n    entries = data [ 'entries' ] \n    pass \n    pass \n    for entry in entries : \n        pass \n        pass \n        pass \n        pass \n        pass \n        pass \n        pass \n        ipdb . set_trace ( ) "}
{"13437": "\nasync def fetch_bikes ( ) -> List [ dict ] : \n    async with ClientSession ( ) as session : \n        try : \n            async with session . get ( 'https://www.bikeregister.com/stolen-bikes' ) as request : \n                document = document_fromstring ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        token = document . xpath ( \"//input[@name='_token']\" ) \n        if len ( token ) != 1 : \n            raise ApiError ( f\"Couldn't extract token from page.\" ) \n        else : \n            token = token [ 0 ] . value \n        pass \n        pass \n        headers = { 'cookie' : f'XSRF-TOKEN={xsrf_token}; laravel_session={laravel_session}' , 'origin' : 'https://www.bikeregister.com' , 'accept-encoding' : 'gzip, deflate, br' , 'accept-language' : 'en-GB,en-US;q=0.9,en;q=0.8' , 'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0' , 'content-type' : 'application/x-www-form-urlencoded; charset=UTF-8' , 'accept' : '*/*' , 'referer' : 'https://www.bikeregister.com/stolen-bikes' , 'authority' : 'www.bikeregister.com' , 'x-requested-with' : 'XMLHttpRequest' , } \n        data = [ ( '_token' , token ) , ( 'make' , '' ) , ( 'model' , '' ) , ( 'colour' , '' ) , ( 'reporting_period' , '1' ) , ] \n        try : \n            async with session . post ( 'https://www.bikeregister.com/stolen-bikes' , headers = headers , data = data ) as request : \n                bikes = json . loads ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        except json . JSONDecodeError as dec_err : \n            logger . error ( f\"Could not decode data: {dec_err.msg}\" ) \n            raise ApiError ( f\"Could not decode data: {dec_err.msg}\" ) \n        return bikes \n    return [ ] "}
{"13672": "\ndef _ofind ( self , oname , namespaces = None ) : \n    oname = oname . strip ( ) \n    if not oname . startswith ( ESC_MAGIC ) and not oname . startswith ( ESC_MAGIC2 ) and not py3compat . isidentifier ( oname , dotted = True ) : \n        return dict ( found = False ) \n    alias_ns = None \n    if namespaces is None : \n        namespaces = [ ( 'Interactive' , self . user_ns ) , ( 'Interactive (global)' , self . user_global_ns ) , ( 'Python builtin' , builtin_mod . __dict__ ) , ( 'Alias' , self . alias_manager . alias_table ) , ] \n        alias_ns = self . alias_manager . alias_table \n    found = False ; \n    obj = None ; \n    ospace = None ; \n    pass ; \n    ismagic = False ; \n    isalias = False ; \n    parent = None \n    if ( oname == 'print' and not py3compat . PY3 and not ( self . compile . compiler_flags & __future__ . CO_FUTURE_PRINT_FUNCTION ) ) : \n        return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n    oname_parts = oname . split ( '.' ) \n    oname_head , oname_rest = oname_parts [ 0 ] , oname_parts [ 1 : ] \n    for nsname , ns in namespaces : \n        try : \n            obj = ns [ oname_head ] \n        except KeyError : \n            continue \n        else : \n            for part in oname_rest : \n                try : \n                    parent = obj \n                    obj = getattr ( obj , part ) \n                except : \n                    break \n            else : \n                found = True \n                ospace = nsname \n                if ns == alias_ns : \n                    isalias = True \n                break \n    if not found : \n        obj = None \n        if oname . startswith ( ESC_MAGIC2 ) : \n            oname = oname . lstrip ( ESC_MAGIC2 ) \n            obj = self . find_cell_magic ( oname ) \n        elif oname . startswith ( ESC_MAGIC ) : \n            oname = oname . lstrip ( ESC_MAGIC ) \n            obj = self . find_line_magic ( oname ) \n        else : \n            obj = self . find_line_magic ( oname ) \n            if obj is None : \n                obj = self . find_cell_magic ( oname ) \n        if obj is not None : \n            found = True \n            ospace = 'IPython internal' \n            ismagic = True \n    if not found and oname_head in [ \"''\" , '\"\"' , '[]' , '{}' , '()' ] : \n        obj = eval ( oname_head ) \n        found = True \n        ospace = 'Interactive' \n    return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } "}
{"13696": "\ndef run_ast_nodes ( self , nodelist , cell_name , interactivity = 'last_expr' ) : \n    if not nodelist : \n        return \n    if interactivity == 'last_expr' : \n        if isinstance ( nodelist [ - 1 ] , ast . Expr ) : \n            interactivity = \"last\" \n        else : \n            interactivity = \"none\" \n    if interactivity == 'none' : \n        to_run_exec , to_run_interactive = nodelist , [ ] \n    elif interactivity == 'last' : \n        to_run_exec , to_run_interactive = nodelist [ : - 1 ] , nodelist [ - 1 : ] \n    elif interactivity == 'all' : \n        to_run_exec , to_run_interactive = [ ] , nodelist \n    else : \n        raise ValueError ( \"Interactivity was %r\" % interactivity ) \n    pass \n    try : \n        for i , node in enumerate ( to_run_exec ) : \n            mod = ast . Module ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"exec\" ) \n            if self . run_code ( code ) : \n                return True \n        for i , node in enumerate ( to_run_interactive ) : \n            mod = ast . Interactive ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"single\" ) \n            if self . run_code ( code ) : \n                return True \n        if softspace ( sys . stdout , 0 ) : \n            print \n    except : \n        self . showtraceback ( ) \n    return False "}
{"13722": "\ndef log_listener ( log : logging . Logger = None , level = logging . INFO ) : \n    if log is None : \n        log = logging . getLogger ( \"ProgressMonitor\" ) \n    def listen ( monitor ) : \n        name = \"{}: \" . format ( monitor . name ) if monitor . name is not None else \"\" \n        pass \n        msg = \"[{name}{perc:3d}%] {monitor.message}\" . format ( ** locals ( ) ) \n        log . log ( level , msg ) \n    return listen "}
{"13723": "\ndef unpack_directory ( filename , extract_dir , progress_filter = default_filter ) : \n    if not os . path . isdir ( filename ) : \n        raise UnrecognizedFormat ( \"%s is not a directory\" % ( filename , ) ) \n    paths = { filename : ( '' , extract_dir ) } \n    for base , dirs , files in os . walk ( filename ) : \n        src , dst = paths [ base ] \n        for d in dirs : \n            paths [ os . path . join ( base , d ) ] = src + d + '/' , os . path . join ( dst , d ) \n        for f in files : \n            pass \n            target = os . path . join ( dst , f ) \n            target = progress_filter ( src + f , target ) \n            if not target : \n                continue \n            ensure_directory ( target ) \n            f = os . path . join ( base , f ) \n            shutil . copyfile ( f , target ) \n            shutil . copystat ( f , target ) "}
{"13788": "\ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    pass \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = 0 ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while True : \n        try : \n            idents , msg = session . recv ( sub , mode = 0 ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ 0 ] \n        if msg [ 'msg_type' ] == 'stream' : \n            print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) \n        elif msg [ 'msg_type' ] == 'pyerr' : \n            c = msg [ 'content' ] \n            print ( topic + ':' ) \n            for line in c [ 'traceback' ] : \n                print ( '    ' + line ) "}
{"13808": "\ndef class_get_help ( cls , inst = None ) : \n    assert inst is None or isinstance ( inst , cls ) \n    pass \n    final_help = [ ] \n    final_help . append ( u'%s options' % cls . __name__ ) \n    final_help . append ( len ( final_help [ 0 ] ) * u'-' ) \n    for k , v in sorted ( cls . class_traits ( config = True ) . iteritems ( ) ) : \n        help = cls . class_get_trait_help ( v , inst ) \n        final_help . append ( help ) \n    return '\\n' . join ( final_help ) "}
{"13845": "\ndef create_interrupt_event ( ) : \n    class SECURITY_ATTRIBUTES ( ctypes . Structure ) : \n        pass \n    sa = SECURITY_ATTRIBUTES ( ) \n    sa_p = ctypes . pointer ( sa ) \n    sa . nLength = ctypes . sizeof ( SECURITY_ATTRIBUTES ) \n    sa . lpSecurityDescriptor = 0 \n    sa . bInheritHandle = 1 \n    return ctypes . windll . kernel32 . CreateEventA ( sa_p , False , False , '' ) "}
{"14005": "\ndef dispatch_control ( self , msg ) : \n    idents , msg = self . session . feed_identities ( msg , copy = False ) \n    try : \n        msg = self . session . unserialize ( msg , content = True , copy = False ) \n    except : \n        self . log . error ( \"Invalid Control Message\" , exc_info = True ) \n        return \n    self . log . debug ( \"Control received: %s\" , msg ) \n    header = msg [ 'header' ] \n    pass \n    msg_type = header [ 'msg_type' ] \n    handler = self . control_handlers . get ( msg_type , None ) \n    if handler is None : \n        self . log . error ( \"UNKNOWN CONTROL MESSAGE TYPE: %r\" , msg_type ) \n    else : \n        try : \n            handler ( self . control_stream , idents , msg ) \n        except Exception : \n            self . log . error ( \"Exception in control handler:\" , exc_info = True ) "}
{"14006": "\ndef dispatch_shell ( self , stream , msg ) : \n    if self . control_stream : \n        self . control_stream . flush ( ) \n    idents , msg = self . session . feed_identities ( msg , copy = False ) \n    try : \n        msg = self . session . unserialize ( msg , content = True , copy = False ) \n    except : \n        self . log . error ( \"Invalid Message\" , exc_info = True ) \n        return \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    self . log . debug ( '\\n*** MESSAGE TYPE:%s***' , msg_type ) \n    self . log . debug ( '   Content: %s\\n   --->\\n   ' , msg [ 'content' ] ) \n    if msg_id in self . aborted : \n        self . aborted . remove ( msg_id ) \n        reply_type = msg_type . split ( '_' ) [ 0 ] + '_reply' \n        status = { 'status' : 'aborted' } \n        sub = { 'engine' : self . ident } \n        sub . update ( status ) \n        pass \n        return \n    handler = self . shell_handlers . get ( msg_type , None ) \n    if handler is None : \n        self . log . error ( \"UNKNOWN MESSAGE TYPE: %r\" , msg_type ) \n    else : \n        sig = signal ( SIGINT , default_int_handler ) \n        try : \n            handler ( stream , idents , msg ) \n        except Exception : \n            self . log . error ( \"Exception in message handler:\" , exc_info = True ) \n        finally : \n            signal ( SIGINT , sig ) "}
{"14011": "\ndef clear_request ( self , stream , idents , parent ) : \n    self . shell . reset ( False ) \n    pass "}
{"14116": "\ndef Rconverter ( Robj , dataframe = False ) : \n    pass \n    colnames = ro . r ( 'colnames' ) \n    pass \n    names = ro . r ( 'names' ) \n    if dataframe : \n        as_data_frame = ro . r ( 'as.data.frame' ) \n        cols = colnames ( Robj ) \n        _names = names ( Robj ) \n        if cols != ri . NULL : \n            Robj = as_data_frame ( Robj ) \n            names = tuple ( np . array ( cols ) ) \n        elif _names != ri . NULL : \n            names = tuple ( np . array ( _names ) ) \n        else : \n            return np . asarray ( Robj ) \n        Robj = np . rec . fromarrays ( Robj , names = names ) \n    return np . asarray ( Robj ) "}
{"14191": "\ndef save_connection_dict ( self , fname , cdict ) : \n    pass \n    url = cdict [ 'url' ] \n    location = cdict [ 'location' ] \n    if not location : \n        try : \n            proto , ip , port = split_url ( url ) \n        except AssertionError : \n            pass \n        else : \n            try : \n                location = socket . gethostbyname_ex ( socket . gethostname ( ) ) [ 2 ] [ - 1 ] \n            except ( socket . gaierror , IndexError ) : \n                self . log . warn ( \"Could not identify this machine's IP, assuming 127.0.0.1.\" \" You may need to specify '--location=<external_ip_address>' to help\" \" IPython decide when to connect via loopback.\" ) \n                location = '127.0.0.1' \n        cdict [ 'location' ] = location \n    fname = os . path . join ( self . profile_dir . security_dir , fname ) \n    self . log . info ( \"writing connection info to %s\" , fname ) \n    with open ( fname , 'w' ) as f : \n        f . write ( json . dumps ( cdict , indent = 2 ) ) \n    os . chmod ( fname , stat . S_IRUSR | stat . S_IWUSR ) "}
{"14232": "\ndef _flush_iopub ( self , sock ) : \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        if self . debug : \n            pprint ( msg ) \n        parent = msg [ 'parent_header' ] \n        if not parent : \n            continue \n        msg_id = parent [ 'msg_id' ] \n        content = msg [ 'content' ] \n        pass \n        msg_type = msg [ 'header' ] [ 'msg_type' ] \n        md = self . metadata [ msg_id ] \n        if msg_type == 'stream' : \n            name = content [ 'name' ] \n            s = md [ name ] or '' \n            md [ name ] = s + content [ 'data' ] \n        elif msg_type == 'pyerr' : \n            md . update ( { 'pyerr' : self . _unwrap_exception ( content ) } ) \n        elif msg_type == 'pyin' : \n            md . update ( { 'pyin' : content [ 'code' ] } ) \n        elif msg_type == 'display_data' : \n            md [ 'outputs' ] . append ( content ) \n        elif msg_type == 'pyout' : \n            md [ 'pyout' ] = content \n        elif msg_type == 'status' : \n            if content [ 'execution_state' ] == 'idle' : \n                md [ 'outputs_ready' ] = True \n        else : \n            pass \n        self . metadata [ msg_id ] = md \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14282": "\ndef _handle_display_data ( self , msg ) : \n    self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        pass \n        data = msg [ 'content' ] [ 'data' ] \n        pass \n        if data . has_key ( 'text/html' ) : \n            html = data [ 'text/html' ] \n            self . _append_html ( html , True ) \n        elif data . has_key ( 'text/plain' ) : \n            text = data [ 'text/plain' ] \n            self . _append_plain_text ( text , True ) \n        self . _append_plain_text ( u'\\n' , True ) "}
{"14498": "\ndef _event_filter_page_keypress ( self , event ) : \n    key = event . key ( ) \n    ctrl_down = self . _control_key_down ( event . modifiers ( ) ) \n    alt_down = event . modifiers ( ) & QtCore . Qt . AltModifier \n    if ctrl_down : \n        if key == QtCore . Qt . Key_O : \n            self . _control . setFocus ( ) \n            pass \n    elif alt_down : \n        if key == QtCore . Qt . Key_Greater : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . End ) \n            pass \n        elif key == QtCore . Qt . Key_Less : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            pass \n    elif key in ( QtCore . Qt . Key_Q , QtCore . Qt . Key_Escape ) : \n        if self . _splitter : \n            self . _page_control . hide ( ) \n            self . _control . setFocus ( ) \n        else : \n            self . layout ( ) . setCurrentWidget ( self . _control ) \n        return True \n    elif key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return , QtCore . Qt . Key_Tab ) : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageDown , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return True \n    elif key == QtCore . Qt . Key_Backspace : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageUp , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return True \n    return False "}
{"14535": "\ndef timeUnit ( elapsed , avg , est_end ) : \n    minute = 60 \n    hr = 3600 \n    pass \n    if elapsed <= 3 * minute : \n        unit_elapsed = ( elapsed , \"secs\" ) \n    if elapsed > 3 * minute : \n        unit_elapsed = ( ( elapsed / 60 ) , \"mins\" ) \n    if elapsed > 3 * hr : \n        unit_elapsed = ( ( elapsed / 3600 ) , \"hr\" ) \n    if avg <= 3 * minute : \n        unit_avg = ( avg , \"secs\" ) \n    if avg > 3 * minute : \n        unit_avg = ( ( avg / 60 ) , \"mins\" ) \n    if avg > 3 * hr : \n        unit_avg = ( ( avg / 3600 ) , \"hr\" ) \n    if est_end <= 3 * minute : \n        unit_estEnd = ( est_end , \"secs\" ) \n    if est_end > 3 * minute : \n        unit_estEnd = ( ( est_end / 60 ) , \"mins\" ) \n    if est_end > 3 * hr : \n        unit_estEnd = ( ( est_end / 3600 ) , \"hr\" ) \n    return [ unit_elapsed , unit_avg , unit_estEnd ] "}
{"14680": "\ndef show_all ( self ) : \n    pass \n    title = self . title \n    nblocks = self . nblocks \n    silent = self . _silent \n    marquee = self . marquee \n    for index , block in enumerate ( self . src_blocks_colored ) : \n        if silent [ index ] : \n            print >> io . stdout , marquee ( '<%s> SILENT block # %s (%s remaining)' % ( title , index , nblocks - index - 1 ) ) \n        else : \n            print >> io . stdout , marquee ( '<%s> block # %s (%s remaining)' % ( title , index , nblocks - index - 1 ) ) \n        print >> io . stdout , block , \n    sys . stdout . flush ( ) "}
{"14698": "\ndef prop ( func = None , * , field = _UNSET , get : bool = True , set : bool = True , del_ : bool = False , default = _UNSET , types : tuple = _UNSET ) : \n    def wrap ( func ) : \n        if not callable ( func ) : \n            raise TypeError \n        prop_name = func . __name__ \n        key = field \n        if key is _UNSET : \n            key = '_' + prop_name \n        fget , fset , fdel = None , None , None \n        if get : \n            def fget ( self ) : \n                try : \n                    return self . __dict__ [ key ] \n                except KeyError : \n                    if default is not _UNSET : \n                        return default \n                    raise AttributeError ( f\"'{type(self).__name__}' object has no attribute '{key}'\" ) \n        if set : \n            def fset ( self , val ) : \n                if types is not _UNSET and not isinstance ( val , types ) : \n                    if isinstance ( types , tuple ) : \n                        pass \n                    else : \n                        pass \n                    raise TypeError ( f'type of {type(self).__name__}.{prop_name} must be {types_name}; ' f'got {type(val).__name__} instead' ) \n                self . __dict__ [ key ] = val \n        if del_ : \n            def fdel ( self ) : \n                del self . __dict__ [ key ] \n        return property ( fget , fset , fdel , func . __doc__ ) \n    return wrap ( func ) if func else wrap "}
{"14805": "\ndef _check_packers ( self ) : \n    pack = self . pack \n    unpack = self . unpack \n    msg = dict ( a = [ 1 , 'hi' ] ) \n    try : \n        packed = pack ( msg ) \n    except Exception : \n        raise ValueError ( \"packer could not serialize a simple message\" ) \n    if not isinstance ( packed , bytes ) : \n        raise ValueError ( \"message packed to %r, but bytes are required\" % type ( packed ) ) \n    try : \n        pass \n    except Exception : \n        raise ValueError ( \"unpacker could not handle the packer's output\" ) \n    msg = dict ( t = datetime . now ( ) ) \n    try : \n        pass \n    except Exception : \n        self . pack = lambda o : pack ( squash_dates ( o ) ) \n        self . unpack = lambda s : extract_dates ( unpack ( s ) ) "}
{"14899": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        self . log . critical ( 'Could not read pid file, cluster is probably not running.' ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    if not self . check_pid ( pid ) : \n        self . log . critical ( 'Cluster [pid=%r] is not running.' % pid ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    elif os . name == 'posix' : \n        sig = self . signal \n        self . log . info ( \"Stopping cluster [pid=%r] with [signal=%r]\" % ( pid , sig ) ) \n        try : \n            os . kill ( pid , sig ) \n        except OSError : \n            self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = True ) \n            self . remove_pid_file ( ) \n    elif os . name == 'nt' : \n        try : \n            pass \n        except ( CalledProcessError , OSError ) : \n            self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = True ) \n        self . remove_pid_file ( ) "}
