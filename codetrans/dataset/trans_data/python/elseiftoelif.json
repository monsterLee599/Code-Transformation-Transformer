{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256 \n    for index , _ in enumerate ( data ) : \n        t = index \n        for i in range ( 8 ) : \n            t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) \n        e = o = r = - 1 \n        i , a = 0 , len ( path ) \n        while i < a : \n            e = ord ( path [ i ] ) \n            i += 1 \n            if e < 128 : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] \n            elif e < 2048 : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n            elif 55296 <= e < 57344 : \n                e = ( 1023 & e ) + 64 \n                i += 1 \n                o = 1023 & t . url ( i ) \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] \n            else : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) \n    while 1 : \n        url = tmp ( ) \n        if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : \n            return url "}
{"14": "\ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    elif 'info_only' in kwargs and kwargs [ 'info_only' ] : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n        elif 'index' not in kwargs : \n            self . p ( [ ] ) \n        else : \n            stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n            self . p_i ( stream_id ) \n    else : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n        else : \n            stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n        if 'index' not in kwargs : \n            self . p ( stream_id ) \n        else : \n            self . p_i ( stream_id ) \n        if stream_id in self . streams : \n            urls = self . streams [ stream_id ] [ 'src' ] \n            ext = self . streams [ stream_id ] [ 'container' ] \n            total_size = self . streams [ stream_id ] [ 'size' ] \n        else : \n            urls = self . dash_streams [ stream_id ] [ 'src' ] \n            ext = self . dash_streams [ stream_id ] [ 'container' ] \n            total_size = self . dash_streams [ stream_id ] [ 'size' ] \n        if not urls : \n            log . wtf ( '[Failed] Cannot extract video source.' ) \n        download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping captions.' ) \n            return \n        for lang in self . caption_tracks : \n            filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n            print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) \n            srt = self . caption_tracks [ lang ] \n            with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( srt ) \n            print ( 'Done.' ) "}
{"210": "\ndef get_task_instances ( self , state = None , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    tis = session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . execution_date == self . execution_date , ) \n    if state : \n        if isinstance ( state , six . string_types ) : \n            tis = tis . filter ( TaskInstance . state == state ) \n        elif None in state : \n            tis = tis . filter ( or_ ( TaskInstance . state . in_ ( state ) , TaskInstance . state . is_ ( None ) ) ) \n        else : \n            tis = tis . filter ( TaskInstance . state . in_ ( state ) ) \n    if self . dag and self . dag . partial : \n        tis = tis . filter ( TaskInstance . task_id . in_ ( self . dag . task_ids ) ) \n    return tis . all ( ) "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > 0 : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            elif num > 0 : \n                start_date += delta \n            else : \n                start_date -= delta \n    return sorted ( dates ) "}
{"557": "\ndef health ( self , session = None ) : \n    BJ = jobs . BaseJob \n    payload = { } \n    scheduler_health_check_threshold = timedelta ( seconds = conf . getint ( 'scheduler' , 'scheduler_health_check_threshold' ) ) \n    latest_scheduler_heartbeat = None \n    payload [ 'metadatabase' ] = { 'status' : 'healthy' } \n    try : \n        latest_scheduler_heartbeat = session . query ( func . max ( BJ . latest_heartbeat ) ) . filter ( BJ . state == 'running' , BJ . job_type == 'SchedulerJob' ) . scalar ( ) \n    except Exception : \n        payload [ 'metadatabase' ] [ 'status' ] = 'unhealthy' \n    if not latest_scheduler_heartbeat : \n        scheduler_status = 'unhealthy' \n    elif timezone . utcnow ( ) - latest_scheduler_heartbeat <= scheduler_health_check_threshold : \n        scheduler_status = 'healthy' \n    else : \n        scheduler_status = 'unhealthy' \n    payload [ 'scheduler' ] = { 'status' : scheduler_status , 'latest_scheduler_heartbeat' : str ( latest_scheduler_heartbeat ) } \n    return wwwutils . json_response ( payload ) "}
{"1015": "\ndef sample_chain ( num_results , current_state , previous_kernel_results = None , kernel = None , num_burnin_steps = 0 , num_steps_between_results = 0 , trace_fn = lambda current_state , kernel_results : kernel_results , return_final_kernel_results = False , parallel_iterations = 10 , name = None , ) : \n    if not kernel . is_calibrated : \n        warnings . warn ( \"supplied `TransitionKernel` is not calibrated. Markov \" \"chain may not converge to intended target distribution.\" ) \n    with tf . compat . v1 . name_scope ( name , \"mcmc_sample_chain\" , [ num_results , num_burnin_steps , num_steps_between_results ] ) : \n        num_results = tf . convert_to_tensor ( value = num_results , dtype = tf . int32 , name = \"num_results\" ) \n        num_burnin_steps = tf . convert_to_tensor ( value = num_burnin_steps , dtype = tf . int32 , name = \"num_burnin_steps\" ) \n        num_steps_between_results = tf . convert_to_tensor ( value = num_steps_between_results , dtype = tf . int32 , name = \"num_steps_between_results\" ) \n        current_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = \"current_state\" ) , current_state ) \n        if previous_kernel_results is None : \n            previous_kernel_results = kernel . bootstrap_results ( current_state ) \n        if trace_fn is None : \n            trace_fn = lambda * args : ( ) \n            no_trace = True \n        else : \n            no_trace = False \n        if trace_fn is sample_chain . __defaults__ [ 4 ] : \n            warnings . warn ( \"Tracing all kernel results by default is deprecated. Set \" \"the `trace_fn` argument to None (the future default \" \"value) or an explicit callback that traces the values \" \"you are interested in.\" ) \n        def _trace_scan_fn ( state_and_results , num_steps ) : \n            next_state , current_kernel_results = mcmc_util . smart_for_loop ( loop_num_iter = num_steps , body_fn = kernel . one_step , initial_loop_vars = list ( state_and_results ) , parallel_iterations = parallel_iterations ) \n            return next_state , current_kernel_results \n        ( _ , final_kernel_results ) , ( all_states , trace ) = mcmc_util . trace_scan ( loop_fn = _trace_scan_fn , initial_state = ( current_state , previous_kernel_results ) , elems = tf . one_hot ( indices = 0 , depth = num_results , on_value = 1 + num_burnin_steps , off_value = 1 + num_steps_between_results , dtype = tf . int32 ) , trace_fn = lambda state_and_results : ( state_and_results [ 0 ] , trace_fn ( * state_and_results ) ) , parallel_iterations = parallel_iterations ) \n        if return_final_kernel_results : \n            return CheckpointableStatesAndTrace ( all_states = all_states , trace = trace , final_kernel_results = final_kernel_results ) \n        elif no_trace : \n            return all_states \n        else : \n            return StatesAndTrace ( all_states = all_states , trace = trace ) "}
{"1133": "\ndef optimal_variational_posterior ( kernel , inducing_index_points , observation_index_points , observations , observation_noise_variance , mean_fn = None , jitter = 1e-6 , name = None ) : \n    with tf . name_scope ( name or 'optimal_variational_posterior' ) : \n        dtype = dtype_util . common_dtype ( [ inducing_index_points , observation_index_points , observations , observation_noise_variance , jitter ] , tf . float32 ) \n        inducing_index_points = tf . convert_to_tensor ( value = inducing_index_points , dtype = dtype , name = 'inducing_index_points' ) \n        observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = dtype , name = 'observation_index_points' ) \n        observations = tf . convert_to_tensor ( value = observations , dtype = dtype , name = 'observations' ) \n        observation_noise_variance = tf . convert_to_tensor ( value = observation_noise_variance , dtype = dtype , name = 'observation_noise_variance' ) \n        jitter = tf . convert_to_tensor ( value = jitter , dtype = dtype , name = 'jitter' ) \n        if mean_fn is None : \n            mean_fn = lambda x : tf . zeros ( [ 1 ] , dtype = dtype ) \n        elif not callable ( mean_fn ) : \n            raise ValueError ( '`mean_fn` must be a Python callable' ) \n        kzz = kernel . matrix ( inducing_index_points , inducing_index_points ) \n        kzx = kernel . matrix ( inducing_index_points , observation_index_points ) \n        noise_var_inv = tf . math . reciprocal ( observation_noise_variance ) \n        sigma_inv = _add_diagonal_shift ( kzz + noise_var_inv * tf . matmul ( kzx , kzx , adjoint_b = True ) , jitter ) \n        chol_sigma_inv = tf . linalg . cholesky ( sigma_inv ) \n        kzx_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzx ) \n        kzx_obs = kzx_lin_op . matvec ( observations - mean_fn ( observation_index_points ) ) \n        kzz_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzz ) \n        loc = ( mean_fn ( inducing_index_points ) + noise_var_inv * kzz_lin_op . matvec ( _solve_cholesky_factored_system_vec ( chol_sigma_inv , kzx_obs ) ) ) \n        chol_sigma_inv_lin_op = tf . linalg . LinearOperatorLowerTriangular ( chol_sigma_inv ) \n        scale = chol_sigma_inv_lin_op . solve ( kzz ) \n        return loc , scale "}
{"1156": "\ndef predict ( self , x , distributed = True ) : \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            features = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            features = x \n        else : \n            raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) \n        return self . predict_distributed ( features ) \n    elif isinstance ( x , np . ndarray ) : \n        return self . predict_local ( x ) \n    else : \n        raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) "}
{"1175": "\ndef predict ( self , x , batch_size = None , verbose = None , is_distributed = False ) : \n    if batch_size or verbose : \n        raise Exception ( \"we don't support batch_size or verbose for now\" ) \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            input = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            input = x \n        return self . bmodel . predict ( input ) \n    elif isinstance ( x , np . ndarray ) : \n        return self . bmodel . predict_local ( x ) \n    raise Exception ( \"not supported type: %s\" % x ) "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if possible_matches > 0 : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        elif possible_matches_by_order [ i ] > 0 : \n            precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n        else : \n            precisions [ i ] = 0.0 \n    if min ( precisions ) > 0 : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = False \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if len ( found ) > 0 : \n                    if len ( g_current_testname ) > 0 : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( len ( temp_strings ) >= 6 ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : \n                if g_toContinue == True : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = False \n            elif g_toContinue : \n                tempMessage += each_line \n            if ( ( len ( temp_strings ) > 5 ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) \n                if found and ( len ( endStr . strip ( ) ) > 0 ) : \n                    tempMessage += endStr \n                    messageType = temp_strings [ 5 ] \n                    g_toContinue = True \n        java_file . close ( ) "}
{"1567": "\ndef update_message_dict ( message_dict , action ) : \n    global g_ok_java_messages \n    allKeys = g_ok_java_messages . keys ( ) \n    for key in message_dict . keys ( ) : \n        if key in allKeys : \n            for message in message_dict [ key ] : \n                if action == 1 : \n                    if message not in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . append ( message ) \n                if action == 2 : \n                    if message in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . remove ( message ) \n        elif action == 1 : \n            g_ok_java_messages [ key ] = message_dict [ key ] "}
{"1932": "\ndef handle_url ( ) : \n    try : \n        plugin = streamlink . resolve_url ( args . url ) \n        setup_plugin_options ( streamlink , plugin ) \n        log . info ( \"Found matching plugin {0} for URL {1}\" , plugin . module , args . url ) \n        plugin_args = [ ] \n        for parg in plugin . arguments : \n            value = plugin . get_option ( parg . dest ) \n            if value : \n                plugin_args . append ( ( parg , value ) ) \n        if plugin_args : \n            log . debug ( \"Plugin specific arguments:\" ) \n            for parg , value in plugin_args : \n                log . debug ( \" {0}={1} ({2})\" . format ( parg . argument_name ( plugin . module ) , value if not parg . sensitive else ( \"*\" * 8 ) , parg . dest ) ) \n        if args . retry_max or args . retry_streams : \n            retry_streams = 1 \n            retry_max = 0 \n            if args . retry_streams : \n                retry_streams = args . retry_streams \n            if args . retry_max : \n                retry_max = args . retry_max \n            streams = fetch_streams_with_retry ( plugin , retry_streams , retry_max ) \n        else : \n            streams = fetch_streams ( plugin ) \n    except NoPluginError : \n        console . exit ( \"No plugin can handle URL: {0}\" , args . url ) \n    except PluginError as err : \n        console . exit ( u\"{0}\" , err ) \n    if not streams : \n        console . exit ( \"No playable streams found on this URL: {0}\" , args . url ) \n    if args . default_stream and not args . stream and not args . json : \n        args . stream = args . default_stream \n    if args . stream : \n        validstreams = format_valid_streams ( plugin , streams ) \n        for stream_name in args . stream : \n            if stream_name in streams : \n                log . info ( \"Available streams: {0}\" , validstreams ) \n                handle_stream ( plugin , streams , stream_name ) \n                return \n        err = ( \"The specified stream(s) '{0}' could not be \" \"found\" . format ( \", \" . join ( args . stream ) ) ) \n        if console . json : \n            console . msg_json ( dict ( streams = streams , plugin = plugin . module , error = err ) ) \n        else : \n            console . exit ( \"{0}.\\n       Available streams: {1}\" , err , validstreams ) \n    elif console . json : \n        console . msg_json ( dict ( streams = streams , plugin = plugin . module ) ) \n    else : \n        validstreams = format_valid_streams ( plugin , streams ) \n        console . msg ( \"Available streams: {0}\" , validstreams ) "}
{"2078": "\ndef trim ( self , inplace = False ) : \n    df = self if inplace else self . copy ( ) \n    for name in df : \n        column = df . columns . get ( name ) \n        if column is not None : \n            if self . _index_start == 0 and len ( column ) == self . _index_end : \n                pass \n            elif isinstance ( column , np . ndarray ) : \n                df . columns [ name ] = column [ self . _index_start : self . _index_end ] \n            else : \n                df . columns [ name ] = column . trim ( self . _index_start , self . _index_end ) \n    df . _length_original = self . length_unfiltered ( ) \n    df . _length_unfiltered = df . _length_original \n    df . _index_start = 0 \n    df . _index_end = df . _length_original \n    df . _active_fraction = 1 \n    return df "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = True , axis = - 1 ) : \n    axis = np . abs ( axis ) \n    if rec . ndim != 2 or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = 1 - axis \n        lag_format = rec . format \n        if axis == 0 : \n            rec = rec . tocsc ( ) \n        elif axis in ( - 1 , 1 ) : \n            rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    elif pad : \n        padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] \n        padding [ ( 1 - axis ) ] = ( 0 , t ) \n        lag = np . pad ( rec , padding , mode = 'constant' ) \n    else : \n        lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2522": "\ndef latex ( self , prec = 15 , nested_scope = None ) : \n    if not nested_scope : \n        return \"\\textrm{\" + self . name + \"}\" \n    elif self . name not in nested_scope [ - 1 ] : \n        raise NodeException ( \"Expected local parameter name: \" , \"name=%s, \" % self . name , \"line=%s, \" % self . line , \"file=%s\" % self . file ) \n    else : \n        return nested_scope [ - 1 ] [ self . name ] . latex ( prec , nested_scope [ 0 : - 1 ] ) "}
{"2527": "\ndef _html_checker ( job_var , interval , status , header , _interval_set = False ) : \n    job_status = job_var . status ( ) \n    job_status_name = job_status . name \n    job_status_msg = job_status . value \n    status . value = header % ( job_status_msg ) \n    while job_status_name not in [ 'DONE' , 'CANCELLED' ] : \n        time . sleep ( interval ) \n        job_status = job_var . status ( ) \n        job_status_name = job_status . name \n        job_status_msg = job_status . value \n        if job_status_name == 'ERROR' : \n            break \n        else : \n            if job_status_name == 'QUEUED' : \n                job_status_msg += ' (%s)' % job_var . queue_position ( ) \n                if not _interval_set : \n                    interval = max ( job_var . queue_position ( ) , 2 ) \n            elif not _interval_set : \n                interval = 2 \n            status . value = header % ( job_status_msg ) \n    status . value = header % ( job_status_msg ) "}
{"2699": "\ndef _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = True ) : \n    add_regs = set ( ) \n    reg_frag_chk = { } \n    for v in keyregs . values ( ) : \n        reg_frag_chk [ v ] = { j : False for j in range ( len ( v ) ) } \n    for k in edge_map . keys ( ) : \n        if k [ 0 ] . name in keyregs : \n            reg_frag_chk [ k [ 0 ] ] [ k [ 1 ] ] = True \n    for k , v in reg_frag_chk . items ( ) : \n        s = set ( v . values ( ) ) \n        if len ( s ) == 2 : \n            raise DAGCircuitError ( \"edge_map fragments reg %s\" % k ) \n        elif s == set ( [ False ] ) : \n            if k in self . qregs . values ( ) or k in self . cregs . values ( ) : \n                raise DAGCircuitError ( \"unmapped duplicate reg %s\" % k ) \n            else : \n                add_regs . add ( k ) \n        elif valreg : \n            if not edge_map [ ( k , 0 ) ] [ 0 ] . name in valregs : \n                size = max ( map ( lambda x : x [ 1 ] , filter ( lambda x : x [ 0 ] == edge_map [ ( k , 0 ) ] [ 0 ] , edge_map . values ( ) ) ) ) \n                qreg = QuantumRegister ( size + 1 , edge_map [ ( k , 0 ) ] [ 0 ] . name ) \n                add_regs . add ( qreg ) \n    return add_regs "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        elif not _interval_set : \n            interval = 2 \n        if len ( msg ) < msg_len : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif len ( msg ) > msg_len : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2745": "\ndef euler_angles_1q ( unitary_matrix ) : \n    if unitary_matrix . shape != ( 2 , 2 ) : \n        raise QiskitError ( \"euler_angles_1q: expected 2x2 matrix\" ) \n    phase = la . det ( unitary_matrix ) ** ( - 1.0 / 2.0 ) \n    U = phase * unitary_matrix \n    if abs ( U [ 0 , 0 ] ) > _CUTOFF_PRECISION : \n        theta = 2 * math . acos ( abs ( U [ 0 , 0 ] ) ) \n    else : \n        theta = 2 * math . asin ( abs ( U [ 1 , 0 ] ) ) \n    phase11 = 0.0 \n    phase10 = 0.0 \n    if abs ( math . cos ( theta / 2.0 ) ) > _CUTOFF_PRECISION : \n        phase11 = U [ 1 , 1 ] / math . cos ( theta / 2.0 ) \n    if abs ( math . sin ( theta / 2.0 ) ) > _CUTOFF_PRECISION : \n        phase10 = U [ 1 , 0 ] / math . sin ( theta / 2.0 ) \n    phiplambda = 2 * math . atan2 ( np . imag ( phase11 ) , np . real ( phase11 ) ) \n    phimlambda = 2 * math . atan2 ( np . imag ( phase10 ) , np . real ( phase10 ) ) \n    phi = 0.0 \n    if abs ( U [ 0 , 0 ] ) > _CUTOFF_PRECISION and abs ( U [ 1 , 0 ] ) > _CUTOFF_PRECISION : \n        phi = ( phiplambda + phimlambda ) / 2.0 \n        lamb = ( phiplambda - phimlambda ) / 2.0 \n    elif abs ( U [ 0 , 0 ] ) < _CUTOFF_PRECISION : \n        lamb = - phimlambda \n    else : \n        lamb = phiplambda \n    Rzphi = np . array ( [ [ np . exp ( - 1j * phi / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * phi / 2.0 ) ] ] , dtype = complex ) \n    Rytheta = np . array ( [ [ np . cos ( theta / 2.0 ) , - np . sin ( theta / 2.0 ) ] , [ np . sin ( theta / 2.0 ) , np . cos ( theta / 2.0 ) ] ] , dtype = complex ) \n    Rzlambda = np . array ( [ [ np . exp ( - 1j * lamb / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * lamb / 2.0 ) ] ] , dtype = complex ) \n    V = np . dot ( Rzphi , np . dot ( Rytheta , Rzlambda ) ) \n    if la . norm ( V - U ) > _CUTOFF_PRECISION : \n        raise QiskitError ( \"euler_angles_1q: incorrect result\" ) \n    return theta , phi , lamb "}
{"2784": "\ndef verify_exp_list ( self , obj ) : \n    if obj . children is not None : \n        for children in obj . children : \n            if isinstance ( children , node . Id ) : \n                if children . name in self . external_functions : \n                    continue \n                if children . name not in self . current_symtab : \n                    raise QasmError ( \"Argument '\" + children . name + \"' in expression cannot be \" + \"found, line\" , str ( children . line ) , \"file\" , children . file ) \n            elif hasattr ( children , \"children\" ) : \n                self . verify_exp_list ( children ) "}
{"2785": "\ndef verify_as_gate ( self , obj , bitlist , arglist = None ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( \"Cannot find gate definition for '\" + obj . name + \"', line\" , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if not ( g_sym . type == 'gate' or g_sym . type == 'opaque' ) : \n        raise QasmError ( \"'\" + obj . name + \"' is used as a gate \" + \"or opaque call but the symbol is neither;\" + \" it is a '\" + g_sym . type + \"' line\" , str ( obj . line ) , 'file' , obj . file ) \n    if g_sym . n_bits ( ) != bitlist . size ( ) : \n        raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( bitlist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_bits ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    if arglist : \n        if g_sym . n_args ( ) != arglist . size ( ) : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( arglist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    elif g_sym . n_args ( ) > 0 : \n        raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' has no arguments but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ True if i == ' ' else False for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( True ) \n            elif is_space ( chunk ) : \n                result . append ( True ) \n            else : \n                result . append ( False ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result += chunk \n        elif left > 0 : \n            if len ( chunk ) <= left : \n                result += chunk \n                left -= len ( chunk ) \n            else : \n                leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] \n                was_cut = not is_empty ( leftover , markups ) \n                if was_cut : \n                    result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' \n                else : \n                    result += chunk [ : left ] \n                left = 0 \n    return result "}
{"3225": "\ndef next_retrieve_group_item ( self , last_item = None , entry = None ) : \n    next_item = None \n    gerrit_version = self . version \n    if gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] > 9 : \n        if last_item is None : \n            next_item = 0 \n        else : \n            next_item = last_item \n    elif gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] == 9 : \n        cause = \"Gerrit 2.9.0 does not support pagination\" \n        raise BackendError ( cause = cause ) \n    elif entry is not None : \n        next_item = entry [ 'sortKey' ] \n    return next_item "}
{"3512": "\ndef put_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    if target [ - 1 ] == PATH_SEP : \n        for src in source : \n            self . put_single_file ( pool , src , os . path . join ( target , self . get_basename ( src ) ) ) \n    elif len ( source ) == 1 : \n        self . put_single_file ( pool , source [ 0 ] , target ) \n    else : \n        raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n    pool . join ( ) "}
{"3517": "\ndef get_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if os . path . isdir ( target ) : \n        for src in source : \n            self . get_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) ) \n    elif len ( source ) > 1 : \n        raise Failure ( 'Target \"%s\" is not a directory.' % target ) \n    elif len ( source ) == 1 : \n        self . get_single_file ( pool , source [ 0 ] , target ) \n    else : \n        pass \n    pool . join ( ) "}
{"3519": "\ndef cp_files ( self , source , target , delete_source = False ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if target [ - 1 ] == PATH_SEP : \n        for src in source : \n            self . cp_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) , delete_source ) \n    elif len ( source ) > 1 : \n        raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n    elif len ( source ) == 1 : \n        self . cp_single_file ( pool , source [ 0 ] , target , delete_source ) \n    else : \n        pass \n    pool . join ( ) "}
{"3747": "\ndef _check_unpacking ( self , infered , node , targets ) : \n    if utils . is_inside_abstract_class ( node ) : \n        return \n    if utils . is_comprehension ( node ) : \n        return \n    if infered is astroid . Uninferable : \n        return \n    if ( isinstance ( infered . parent , astroid . Arguments ) and isinstance ( node . value , astroid . Name ) and node . value . name == infered . parent . vararg ) : \n        return \n    if isinstance ( infered , ( astroid . Tuple , astroid . List ) ) : \n        values = infered . itered ( ) \n        if len ( targets ) != len ( values ) : \n            if any ( isinstance ( target , astroid . Starred ) for target in targets ) : \n                return \n            self . add_message ( \"unbalanced-tuple-unpacking\" , node = node , args = ( _get_unpacking_extra_info ( node , infered ) , len ( targets ) , len ( values ) , ) , ) \n    elif not utils . is_iterable ( infered ) : \n        self . add_message ( \"unpacking-non-sequence\" , node = node , args = ( _get_unpacking_extra_info ( node , infered ) , ) , ) "}
{"3843": "\ndef visit_default ( self , node ) : \n    if not node . is_statement : \n        return \n    if not node . root ( ) . pure_python : \n        return \n    prev_sibl = node . previous_sibling ( ) \n    if prev_sibl is not None : \n        prev_line = prev_sibl . fromlineno \n    elif ( isinstance ( node . parent , nodes . TryFinally ) and node in node . parent . finalbody ) : \n        prev_line = node . parent . body [ 0 ] . tolineno + 1 \n    else : \n        prev_line = node . parent . statement ( ) . fromlineno \n    line = node . fromlineno \n    assert line , node \n    if prev_line == line and self . _visited_lines . get ( line ) != 2 : \n        self . _check_multi_statement_line ( node , line ) \n        return \n    if line in self . _visited_lines : \n        return \n    try : \n        tolineno = node . blockstart_tolineno \n    except AttributeError : \n        tolineno = node . tolineno \n    assert tolineno , node \n    lines = [ ] \n    for line in range ( line , tolineno + 1 ) : \n        self . _visited_lines [ line ] = 1 \n        try : \n            lines . append ( self . _lines [ line ] . rstrip ( ) ) \n        except KeyError : \n            lines . append ( \"\" ) "}
{"3898": "\ndef visit_assignname ( self , node ) : \n    self . _check_assign_to_new_keyword_violation ( node . name , node ) \n    frame = node . frame ( ) \n    assign_type = node . assign_type ( ) \n    if isinstance ( assign_type , astroid . Comprehension ) : \n        self . _check_name ( \"inlinevar\" , node . name , node ) \n    elif isinstance ( frame , astroid . Module ) : \n        if isinstance ( assign_type , astroid . Assign ) and not in_loop ( assign_type ) : \n            if isinstance ( utils . safe_infer ( assign_type . value ) , astroid . ClassDef ) : \n                self . _check_name ( \"class\" , node . name , node ) \n            elif not _redefines_import ( node ) : \n                self . _check_name ( \"const\" , node . name , node ) \n        elif isinstance ( assign_type , astroid . ExceptHandler ) : \n            self . _check_name ( \"variable\" , node . name , node ) \n    elif isinstance ( frame , astroid . FunctionDef ) : \n        if node . name in frame and node . name not in frame . argnames ( ) : \n            if not _redefines_import ( node ) : \n                self . _check_name ( \"variable\" , node . name , node ) \n    elif isinstance ( frame , astroid . ClassDef ) : \n        if not list ( frame . local_attr_ancestors ( node . name ) ) : \n            self . _check_name ( \"class_attribute\" , node . name , node ) "}
{"3977": "\ndef validate_grant_type ( self , client_id , grant_type , client , request , * args , ** kwargs ) : \n    if self . _usergetter is None and grant_type == 'password' : \n        log . debug ( 'Password credential authorization is disabled.' ) \n        return False \n    default_grant_types = ( 'authorization_code' , 'password' , 'client_credentials' , 'refresh_token' , ) \n    if hasattr ( client , 'allowed_grant_types' ) : \n        if grant_type not in client . allowed_grant_types : \n            return False \n    elif grant_type not in default_grant_types : \n        return False \n    if grant_type == 'client_credentials' : \n        if not hasattr ( client , 'user' ) : \n            log . debug ( 'Client should have a user property' ) \n            return False \n        request . user = client . user \n    return True "}
{"4315": "\ndef lines_without_stdlib ( self ) : \n    prev_line = None \n    current_module_path = inspect . getabsfile ( inspect . currentframe ( ) ) \n    for module_path , lineno , runtime in self . lines : \n        module_abspath = os . path . abspath ( module_path ) \n        if not prev_line : \n            prev_line = [ module_abspath , lineno , runtime ] \n        elif ( not check_standard_dir ( module_path ) and module_abspath != current_module_path ) : \n            yield prev_line \n            prev_line = [ module_abspath , lineno , runtime ] \n        else : \n            prev_line [ 2 ] += runtime \n    yield prev_line "}
{"4376": "\ndef convert ( self , txn ) : \n    ofxid = self . mk_ofxid ( txn . id ) \n    metadata = { } \n    posting_metadata = { \"ofxid\" : ofxid } \n    if isinstance ( txn , OfxTransaction ) : \n        posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) \n        return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) \n    elif isinstance ( txn , InvestmentTransaction ) : \n        acct1 = self . name \n        acct2 = self . name \n        posting1 = None \n        posting2 = None \n        security = self . maybe_get_ticker ( txn . security ) \n        if isinstance ( txn . type , str ) : \n            if re . match ( '^(buy|sell)' , txn . type ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif txn . type == 'transfer' : \n                acct2 = 'Transfer' \n            elif txn . type == 'reinvest' : \n                acct2 = 'Income:Interest' \n            elif txn . type == 'income' and txn . income_type == 'DIV' : \n                metadata [ 'dividend_from' ] = security \n                acct2 = 'Income:Dividends' \n                posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) \n                posting2 = posting1 . clone_inverted ( acct2 ) \n            else : \n                pass \n        elif ( txn . type in [ 0 , 1 , 3 , 4 ] ) : \n            acct2 = self . unknownaccount or 'Assets:Unknown' \n        elif ( txn . type == 2 ) : \n            acct2 = 'Income:Interest' \n        else : \n            pass \n        aux_date = None \n        if txn . settleDate is not None and txn . settleDate != txn . tradeDate : \n            aux_date = txn . settleDate \n        if posting1 is None and posting2 is None : \n            posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = True ) , metadata = posting_metadata ) \n            posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = True ) ) \n        else : \n            pass \n        return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] ) "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = True ) : \n    if masker is not None : \n        masker = masker \n    elif isinstance ( dataset , Dataset ) : \n        masker = dataset . masker \n    elif not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = False ) \n    if regions . ndim == 2 : \n        m = regions \n        for i in range ( regions . shape [ 1 ] ) : \n            _nz = np . nonzero ( m [ : , i ] ) [ 0 ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        for i in range ( n_regions ) : \n            if isinstance ( threshold , int ) : \n                m [ regions == labels [ i ] , i ] = 1.0 \n            else : \n                m [ regions == labels [ i ] , i ] = 1.0 / np . sum ( regions == labels [ i ] ) \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ result < threshold ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4596": "\ndef _get_structure ( self ) : \n    structure_file = \"\" \n    req = \"\" \n    if PyFunceble . path . isfile ( self . structure ) : \n        structure_file = self . structure \n    elif PyFunceble . path . isfile ( self . base + \"dir_structure_production.json\" ) : \n        structure_file = self . base + \"dir_structure_production.json\" \n    elif \"dev\" not in PyFunceble . VERSION : \n        req = PyFunceble . requests . get ( PyFunceble . LINKS [ \"dir_structure\" ] . replace ( \"dev\" , \"master\" ) ) \n    else : \n        req = PyFunceble . requests . get ( PyFunceble . LINKS [ \"dir_structure\" ] . replace ( \"master\" , \"dev\" ) ) \n    if structure_file . endswith ( \"_production.json\" ) : \n        return self . _update_structure_from_config ( Dict ( ) . from_json ( File ( structure_file ) . read ( ) ) ) \n    if structure_file . endswith ( \".json\" ) : \n        return self . _update_structure_from_config ( Dict ( ) . from_json ( File ( structure_file ) . read ( ) ) ) \n    return self . _update_structure_from_config ( Dict ( ) . from_json ( req . text ) ) "}
{"4651": "\ndef get ( self ) : \n    result = { } \n    if self . algorithm in self . valid_algorithms : \n        if self . algorithm == \"all\" : \n            del self . valid_algorithms [ 0 ] \n            for algo in self . valid_algorithms : \n                if self . path and path . isfile ( self . path ) : \n                    result [ algo ] = self . _hash_file ( algo ) \n                elif self . data : \n                    result [ algo ] = self . _hash_data ( algo ) \n                else : \n                    return None \n        elif self . path and path . isfile ( self . path ) : \n            result [ self . algorithm ] = self . _hash_file ( self . algorithm ) \n        elif self . data : \n            result [ self . algorithm ] = self . _hash_data ( self . algorithm ) \n        else : \n            return None \n    else : \n        return None \n    if self . algorithm != \"all\" and self . only_hash : \n        return result [ self . algorithm ] \n    return result "}
{"4687": "\ndef nslookup ( cls ) : \n    try : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            if not Check ( ) . is_ip_valid ( ) : \n                request = PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n                for sequence in request : \n                    PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] . append ( sequence [ - 1 ] [ 0 ] ) \n            else : \n                request = PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ 0 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ 1 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2 ] \n        elif not Check ( ) . is_ip_valid ( ) : \n            PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n        else : \n            PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n        return True \n    except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : \n        return False "}
{"4721": "\ndef find_files ( filenames , recursive , exclude ) : \n    while filenames : \n        name = filenames . pop ( 0 ) \n        if recursive and os . path . isdir ( name ) : \n            for root , directories , children in os . walk ( name ) : \n                filenames += [ os . path . join ( root , f ) for f in children if match_file ( os . path . join ( root , f ) , exclude ) ] \n                directories [ : ] = [ d for d in directories if match_file ( os . path . join ( root , d ) , exclude ) ] \n        elif not is_exclude_file ( name , exclude ) : \n            yield name "}
{"4743": "\ndef process_request ( self , request , credential = None ) : \n    self . _client_identity = [ None , None ] \n    header = request . request_header \n    self . _set_protocol_version ( header . protocol_version ) \n    max_response_size = None \n    if header . maximum_response_size : \n        max_response_size = header . maximum_response_size . value \n    now = int ( time . time ( ) ) \n    if header . time_stamp : \n        then = header . time_stamp . value \n        if ( now >= then ) and ( ( now - then ) < 60 ) : \n            self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( then ) ) ) ) \n        elif now < then : \n            self . _logger . warning ( \"Received request with future timestamp. Received \" \"timestamp: {0}, Current timestamp: {1}\" . format ( then , now ) ) \n            raise exceptions . InvalidMessage ( \"Future request rejected by server.\" ) \n        else : \n            self . _logger . warning ( \"Received request with old timestamp. Possible \" \"replay attack. Received timestamp: {0}, Current \" \"timestamp: {1}\" . format ( then , now ) ) \n            raise exceptions . InvalidMessage ( \"Stale request rejected by server.\" ) \n    else : \n        self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( now ) ) ) ) \n    self . is_asynchronous = False \n    if header . asynchronous_indicator is not None : \n        self . is_asynchronous = header . asynchronous_indicator . value \n    if self . is_asynchronous : \n        raise exceptions . InvalidMessage ( \"Asynchronous operations are not supported.\" ) \n    if header . authentication : \n        if header . authentication . credentials : \n            auth_credentials = header . authentication . credentials [ 0 ] \n        else : \n            auth_credentials = None \n    else : \n        auth_credentials = None \n    self . _verify_credential ( auth_credentials , credential ) \n    batch_error_option = enums . BatchErrorContinuationOption . STOP \n    if header . batch_error_cont_option is not None : \n        batch_error_option = header . batch_error_cont_option . value \n    if batch_error_option == enums . BatchErrorContinuationOption . UNDO : \n        raise exceptions . InvalidMessage ( \"Undo option for batch handling is not supported.\" ) \n    batch_order_option = False \n    if header . batch_order_option : \n        batch_order_option = header . batch_order_option . value \n    response_batch = self . _process_batch ( request . batch_items , batch_error_option , batch_order_option ) \n    response = self . _build_response ( header . protocol_version , response_batch ) \n    return response , max_response_size , header . protocol_version "}
{"4759": "\ndef validate ( self ) : \n    if self . value is not None : \n        if not isinstance ( self . value , six . integer_types ) : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        elif self . value > LongInteger . MAX : \n            raise ValueError ( 'long integer value greater than accepted max' ) \n        elif self . value < LongInteger . MIN : \n            raise ValueError ( 'long integer value less than accepted min' ) "}
{"4763": "\ndef validate ( self ) : \n    if not isinstance ( self . enum , enumeration . EnumMeta ) : \n        raise TypeError ( 'enumeration type {0} must be of type EnumMeta' . format ( self . enum ) ) \n    if self . value is not None : \n        if not isinstance ( self . value , self . enum ) : \n            raise TypeError ( 'enumeration {0} must be of type {1}' . format ( self . value , self . enum ) ) \n        if type ( self . value . value ) not in six . integer_types : \n            raise TypeError ( 'enumeration value must be an int' ) \n        elif self . value . value > Enumeration . MAX : \n            raise ValueError ( 'enumeration value greater than accepted max' ) \n        elif self . value . value < Enumeration . MIN : \n            raise ValueError ( 'enumeration value less than accepted min' ) "}
{"4769": "\ndef validate ( self ) : \n    if self . value is not None : \n        if type ( self . value ) not in six . integer_types : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        elif self . value > Interval . MAX : \n            raise ValueError ( 'interval value greater than accepted max' ) \n        elif self . value < Interval . MIN : \n            raise ValueError ( 'interval value less than accepted min' ) "}
{"4780": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateKeyPairRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . is_tag_next ( enums . Tags . COMMON_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _common_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n            self . _common_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    elif self . is_tag_next ( enums . Tags . COMMON_ATTRIBUTES , local_buffer ) : \n        attributes = objects . Attributes ( tag = enums . Tags . COMMON_ATTRIBUTES ) \n        attributes . read ( local_buffer , kmip_version = kmip_version ) \n        self . _common_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _private_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _private_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    elif self . is_tag_next ( enums . Tags . PRIVATE_KEY_ATTRIBUTES , local_buffer ) : \n        attributes = objects . Attributes ( tag = enums . Tags . PRIVATE_KEY_ATTRIBUTES ) \n        attributes . read ( local_buffer , kmip_version = kmip_version ) \n        self . _private_key_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _public_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _public_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    elif self . is_tag_next ( enums . Tags . PUBLIC_KEY_ATTRIBUTES , local_buffer ) : \n        attributes = objects . Attributes ( tag = enums . Tags . PUBLIC_KEY_ATTRIBUTES ) \n        attributes . read ( local_buffer , kmip_version = kmip_version ) \n        self . _public_key_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    self . is_oversized ( local_buffer ) "}
{"4781": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . _common_template_attribute is not None : \n            self . _common_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    elif self . _common_template_attribute is not None : \n        attributes = objects . convert_template_attribute_to_attributes ( self . _common_template_attribute ) \n        attributes . write ( local_buffer , kmip_version = kmip_version ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . _private_key_template_attribute is not None : \n            self . _private_key_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    elif self . _private_key_template_attribute is not None : \n        attributes = objects . convert_template_attribute_to_attributes ( self . _private_key_template_attribute ) \n        attributes . write ( local_buffer , kmip_version = kmip_version ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . _public_key_template_attribute is not None : \n            self . _public_key_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    elif self . _public_key_template_attribute is not None : \n        attributes = objects . convert_template_attribute_to_attributes ( self . _public_key_template_attribute ) \n        attributes . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CreateKeyPairRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4795": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the object \" \"type.\" ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the \" \"template attribute.\" ) \n    elif self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n        attributes = objects . Attributes ( ) \n        attributes . read ( local_buffer , kmip_version = kmip_version ) \n        value = objects . convert_attributes_to_template_attribute ( attributes ) \n        self . _template_attribute = value \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the \" \"attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4796": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create request payload is missing the object type field.\" ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Create request payload is missing the template \" \"attribute field.\" ) \n    elif self . _template_attribute : \n        attributes = objects . convert_template_attribute_to_attributes ( self . _template_attribute ) \n        attributes . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create request payload is missing the template \" \"attribute field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( CreateRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4801": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( DeriveKeyRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the object \" \"type.\" ) \n    unique_identifiers = [ ] \n    while self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n        unique_identifiers . append ( unique_identifier ) \n    if not unique_identifiers : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the unique \" \"identifiers.\" ) \n    else : \n        self . _unique_identifiers = unique_identifiers \n    if self . is_tag_next ( enums . Tags . DERIVATION_METHOD , local_buffer ) : \n        self . _derivation_method = primitives . Enumeration ( enums . DerivationMethod , tag = enums . Tags . DERIVATION_METHOD ) \n        self . _derivation_method . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"derivation method.\" ) \n    if self . is_tag_next ( enums . Tags . DERIVATION_PARAMETERS , local_buffer ) : \n        self . _derivation_parameters = attributes . DerivationParameters ( ) \n        self . _derivation_parameters . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"derivation parameters.\" ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"template attribute.\" ) \n    elif self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n        attrs = objects . Attributes ( ) \n        attrs . read ( local_buffer , kmip_version = kmip_version ) \n        value = objects . convert_attributes_to_template_attribute ( attrs ) \n        self . _template_attribute = value \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4802": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the object type \" \"field.\" ) \n    if self . _unique_identifiers : \n        for unique_identifier in self . _unique_identifiers : \n            unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the unique \" \"identifiers field.\" ) \n    if self . _derivation_method : \n        self . _derivation_method . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the derivation \" \"method field.\" ) \n    if self . _derivation_parameters : \n        self . _derivation_parameters . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the derivation \" \"parameters field.\" ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the template \" \"attribute field.\" ) \n    elif self . _template_attribute : \n        attrs = objects . convert_template_attribute_to_attributes ( self . _template_attribute ) \n        attrs . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the template \" \"attribute field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( DeriveKeyRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4847": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( LocateRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . MAXIMUM_ITEMS , local_buffer ) : \n        self . _maximum_items = primitives . Integer ( tag = enums . Tags . MAXIMUM_ITEMS ) \n        self . _maximum_items . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . OFFSET_ITEMS , local_buffer ) : \n        self . _offset_items = primitives . Integer ( tag = enums . Tags . OFFSET_ITEMS ) \n        self . _offset_items . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . STORAGE_STATUS_MASK , local_buffer ) : \n        self . _storage_status_mask = primitives . Integer ( tag = enums . Tags . STORAGE_STATUS_MASK ) \n        self . _storage_status_mask . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . OBJECT_GROUP_MEMBER , local_buffer ) : \n        self . _object_group_member = primitives . Enumeration ( enums . ObjectGroupMember , tag = enums . Tags . OBJECT_GROUP_MEMBER ) \n        self . _object_group_member . read ( local_buffer , kmip_version = kmip_version ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE , local_buffer ) : \n            attribute = objects . Attribute ( ) \n            attribute . read ( local_buffer , kmip_version = kmip_version ) \n            self . _attributes . append ( attribute ) \n    elif self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n        attributes = objects . Attributes ( ) \n        attributes . read ( local_buffer , kmip_version = kmip_version ) \n        temp_attr = objects . convert_attributes_to_template_attribute ( attributes ) \n        self . _attributes = temp_attr . attributes \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Locate request payload encoding is missing the \" \"attributes structure.\" ) "}
{"4848": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _maximum_items : \n        self . _maximum_items . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _offset_items : \n        self . _offset_items . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _storage_status_mask : \n        self . _storage_status_mask . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _object_group_member : \n        self . _object_group_member . write ( local_buffer , kmip_version = kmip_version ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        if self . _attributes : \n            for attribute in self . attributes : \n                attribute . write ( local_buffer , kmip_version = kmip_version ) \n    elif self . _attributes : \n        template_attribute = objects . TemplateAttribute ( attributes = self . attributes ) \n        attributes = objects . convert_template_attribute_to_attributes ( template_attribute ) \n        attributes . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Locate request payload is missing the attributes \" \"list.\" ) \n    self . length = local_buffer . length ( ) \n    super ( LocateRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4907": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributesResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n        self . unique_identifier = unique_identifier . value \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributes response payload encoding is missing the \" \"unique identifier.\" ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        self . _attributes = list ( ) \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE , local_buffer ) : \n            attribute = objects . Attribute ( ) \n            attribute . read ( local_buffer , kmip_version = kmip_version ) \n            self . _attributes . append ( attribute ) \n    elif self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n        attributes = objects . Attributes ( ) \n        attributes . read ( local_buffer , kmip_version = kmip_version ) \n        temp_attr = objects . convert_attributes_to_template_attribute ( attributes ) \n        self . _attributes = temp_attr . attributes \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributes response payload encoding is missing \" \"the attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4908": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributes response payload is missing the unique \" \"identifier field.\" ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        for attribute in self . _attributes : \n            attribute . write ( local_buffer , kmip_version = kmip_version ) \n    elif self . _attributes : \n        template_attribute = objects . TemplateAttribute ( attributes = self . attributes ) \n        attributes = objects . convert_template_attribute_to_attributes ( template_attribute ) \n        attributes . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributes response payload is missing the \" \"attributes list.\" ) \n    self . length = local_buffer . length ( ) \n    super ( GetAttributesResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4977": "\ndef calculate_item_depth ( self , tree_alias , item_id , depth = 0 ) : \n    item = self . get_item_by_id ( tree_alias , item_id ) \n    if hasattr ( item , 'depth' ) : \n        depth = item . depth + depth \n    elif item . parent is not None : \n        depth = self . calculate_item_depth ( tree_alias , item . parent . id , depth + 1 ) \n    return depth "}
{"4983": "\ndef menu ( self , tree_alias , tree_branches , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return '' \n    tree_branches = self . resolve_var ( tree_branches ) \n    parent_isnull = False \n    parent_ids = [ ] \n    parent_aliases = [ ] \n    current_item = self . get_tree_current_item ( tree_alias ) \n    self . tree_climber ( tree_alias , current_item ) \n    for branch_id in tree_branches . split ( ',' ) : \n        branch_id = branch_id . strip ( ) \n        if branch_id == ALIAS_TRUNK : \n            parent_isnull = True \n        elif branch_id == ALIAS_THIS_CHILDREN and current_item is not None : \n            branch_id = current_item . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_ANCESTOR_CHILDREN and current_item is not None : \n            branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_SIBLINGS and current_item is not None and current_item . parent is not None : \n            branch_id = current_item . parent . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_PARENT_SIBLINGS and current_item is not None : \n            branch_id = self . get_ancestor_level ( current_item , depth = 2 ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id . isdigit ( ) : \n            parent_ids . append ( int ( branch_id ) ) \n        else : \n            parent_aliases . append ( branch_id ) \n    check_access = self . check_access \n    menu_items = [ ] \n    for item in sitetree_items : \n        if not item . hidden and item . inmenu and check_access ( item , context ) : \n            if item . parent is None : \n                if parent_isnull : \n                    menu_items . append ( item ) \n            elif item . parent . id in parent_ids or item . parent . alias in parent_aliases : \n                menu_items . append ( item ) \n    menu_items = self . apply_hook ( menu_items , 'menu' ) \n    self . update_has_children ( tree_alias , menu_items , 'menu' ) \n    return menu_items "}
{"4984": "\ndef check_access ( self , item , context ) : \n    if hasattr ( self . current_request . user . is_authenticated , '__call__' ) : \n        authenticated = self . current_request . user . is_authenticated ( ) \n    else : \n        authenticated = self . current_request . user . is_authenticated \n    if item . access_loggedin and not authenticated : \n        return False \n    if item . access_guest and authenticated : \n        return False \n    if item . access_restricted : \n        user_perms = self . _current_user_permissions \n        if user_perms is _UNSET : \n            user_perms = set ( context [ 'user' ] . get_all_permissions ( ) ) \n            self . _current_user_permissions = user_perms \n        if item . access_perm_type == MODEL_TREE_ITEM_CLASS . PERM_TYPE_ALL : \n            if len ( item . perms ) != len ( item . perms . intersection ( user_perms ) ) : \n                return False \n        elif not len ( item . perms . intersection ( user_perms ) ) : \n            return False \n    return True "}
{"5206": "\ndef _build_list_of_Intervals ( cls , data_dict , stop = None , points = False , include = None , exclude = None , ignore = None , lexicon = None ) : \n    include = include or { } \n    exclude = exclude or { } \n    ignore = ignore or [ ] \n    all_data = [ ] \n    for data in zip ( * data_dict . values ( ) ) : \n        all_data . append ( { k : v for k , v in zip ( data_dict . keys ( ) , data ) } ) \n    all_data = sorted ( all_data , key = lambda x : x [ 'top' ] ) \n    wanted_data = [ ] \n    for dictionary in all_data : \n        keep = True \n        delete = [ ] \n        for k , v in dictionary . items ( ) : \n            incl = include . get ( k , utils . null_default ( True ) ) \n            excl = exclude . get ( k , utils . null_default ( False ) ) \n            if k in ignore : \n                delete . append ( k ) \n            if not incl ( v ) : \n                keep = False \n            if excl ( v ) : \n                keep = False \n        if delete : \n            for key in delete : \n                _ = dictionary . pop ( key , None ) \n        if keep : \n            wanted_data . append ( dictionary ) \n    if not points : \n        for i , iv in enumerate ( wanted_data ) : \n            if iv . get ( 'base' , None ) is None : \n                try : \n                    iv [ 'base' ] = wanted_data [ i + 1 ] [ 'top' ] \n                except ( IndexError , KeyError ) : \n                    if stop is not None : \n                        thick = stop - iv [ 'top' ] \n                    else : \n                        thick = 1 \n                    iv [ 'base' ] = iv [ 'top' ] + thick \n    list_of_Intervals = [ ] \n    for iv in wanted_data : \n        top = iv . pop ( 'top' ) \n        base = iv . pop ( 'base' , None ) \n        descr = iv . pop ( 'description' , '' ) \n        if iv : \n            c , d = { } , { } \n            for k , v in iv . items ( ) : \n                if ( k [ : 5 ] . lower ( ) == 'comp ' ) or ( k [ : 9 ] . lower ( ) == 'component' ) : \n                    k = re . sub ( r'comp(?:onent)? ' , '' , k , flags = re . I ) \n                    c [ k ] = v \n                elif v is not None : \n                    d [ k ] = v \n            comp = [ Component ( c ) ] if c else None \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'data' : d , 'components' : comp } ) \n        else : \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'lexicon' : lexicon } ) \n        list_of_Intervals . append ( this ) \n    return list_of_Intervals "}
{"5227": "\ndef hist ( self , lumping = None , summary = False , sort = True , plot = True , legend = None , ax = None ) : \n    comps = [ ] \n    labels = [ ] \n    entries = defaultdict ( int ) \n    for i in self : \n        if lumping : \n            k = i . primary [ lumping ] \n        elif summary : \n            k = i . primary . summary ( ) \n        else : \n            k = i . primary \n        comps . append ( i . primary ) \n        labels . append ( i . primary . summary ( ) ) \n        entries [ k ] += i . thickness \n    if sort : \n        allitems = sorted ( entries . items ( ) , key = lambda i : i [ 1 ] , reverse = True ) \n        ents , counts = zip ( * allitems ) \n    else : \n        ents , counts = tuple ( entries . keys ( ) ) , tuple ( entries . values ( ) ) \n    if plot : \n        if ax is None : \n            fig , ax = plt . subplots ( ) \n            return_ax = False \n        else : \n            return_ax = True \n        ind = np . arange ( len ( ents ) ) \n        bars = ax . bar ( ind , counts , align = 'center' ) \n        ax . set_xticks ( ind ) \n        ax . set_xticklabels ( labels ) \n        if legend : \n            colours = [ legend . get_colour ( c ) for c in comps ] \n            for b , c in zip ( bars , colours ) : \n                b . set_color ( c ) \n        ax . set_ylabel ( 'Thickness [m]' ) \n    else : \n        bars = [ ] \n    if plot and return_ax : \n        return counts , ents , ax \n    return counts , ents , bars "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = True ) : \n            if type ( d ) == dict : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = False ) \n                    elif type ( d [ k ] ) == str : \n                        if _decode : \n                            d [ k ] = decode ( d [ k ] ) \n                        else : \n                            d [ k ] = encode ( d [ k ] ) \n            elif type ( d ) == list : \n                arr = [ ] \n                for e in d : \n                    if type ( e ) == str : \n                        if _decode : \n                            arr . append ( decode ( e ) ) \n                        else : \n                            arr . append ( encode ( e ) ) \n                    elif type ( e ) in [ dict , list ] : \n                        if _decode : \n                            arr . append ( encode_decode_all ( e ) ) \n                        else : \n                            arr . append ( encode_decode_all ( e , _decode = False ) ) \n                    else : \n                        arr . append ( e ) \n                return arr \n            elif _decode : \n                return decode ( d ) \n            else : \n                return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if sys . version_info >= ( 3 , 0 ) : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if sys . version_info >= ( 3 , 0 ) : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5 ) ) , _decode = False ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = False ) \n    return func_wrapper "}
{"5293": "\ndef fuzz_elements ( self , element ) : \n    try : \n        if type ( element ) == dict : \n            tmp_element = { } \n            for key in element : \n                if len ( self . config . parameters ) > 0 : \n                    if self . config . exclude_parameters : \n                        fuzz = key not in self . config . parameters \n                    else : \n                        fuzz = key in self . config . parameters \n                else : \n                    fuzz = True \n                if fuzz : \n                    if type ( element [ key ] ) == dict : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    elif type ( element [ key ] ) == list : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    else : \n                        tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) \n                else : \n                    tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n            element = tmp_element \n            del tmp_element \n        elif type ( element ) == list : \n            arr = [ ] \n            for key in element : \n                if type ( key ) == dict : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                elif type ( key ) == list : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                elif len ( self . config . parameters ) <= 0 : \n                    arr . append ( self . mutator . fuzz ( key ) ) \n                else : \n                    arr . append ( key ) \n            element = arr \n            del arr \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) \n    return element "}
{"5294": "\ndef fuzzed ( self ) : \n    try : \n        if self . config . strong_fuzz : \n            fuzzer = PJFMutators ( self . config ) \n            if self . config . url_encode : \n                if sys . version_info >= ( 3 , 0 ) : \n                    return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n                else : \n                    return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n            elif type ( self . config . json ) in [ list , dict ] : \n                return fuzzer . fuzz ( json . dumps ( self . config . json ) ) \n            else : \n                return fuzzer . fuzz ( self . config . json ) \n        elif self . config . url_encode : \n            if sys . version_info >= ( 3 , 0 ) : \n                return urllib . parse . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n            else : \n                return urllib . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n        else : \n            return self . get_fuzzed ( self . config . indent , self . config . utf8 ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5379": "\ndef set_arg ( self , name : str , value : str , positional : bool = None , before : str = None , after : str = None , preserve_spacing : bool = True ) -> None : \n    args = list ( reversed ( self . arguments ) ) \n    arg = get_arg ( name , args ) \n    if arg : \n        if positional : \n            arg . positional = positional \n        if preserve_spacing : \n            val = arg . value \n            arg . value = val . replace ( val . strip ( WS ) , value ) \n        else : \n            arg . value = value \n        return \n    if not name and positional is None : \n        positional = True \n    if not positional and preserve_spacing and args : \n        before_names = [ ] \n        name_lengths = [ ] \n        before_values = [ ] \n        after_values = [ ] \n        for arg in args : \n            aname = arg . name \n            name_len = len ( aname ) \n            name_lengths . append ( name_len ) \n            before_names . append ( STARTING_WS_MATCH ( aname ) [ 0 ] ) \n            arg_value = arg . value \n            before_values . append ( STARTING_WS_MATCH ( arg_value ) [ 0 ] ) \n            after_values . append ( ENDING_WS_MATCH ( arg_value ) [ 0 ] ) \n        pre_name_ws_mode = mode ( before_names ) \n        name_length_mode = mode ( name_lengths ) \n        post_value_ws_mode = mode ( [ SPACE_AFTER_SEARCH ( self . string ) [ 0 ] ] + after_values [ 1 : ] ) \n        pre_value_ws_mode = mode ( before_values ) \n    else : \n        preserve_spacing = False \n    if positional : \n        addstring = '|' + value \n    elif preserve_spacing : \n        addstring = ( '|' + ( pre_name_ws_mode + name . strip ( WS ) ) . ljust ( name_length_mode ) + '=' + pre_value_ws_mode + value + post_value_ws_mode ) \n    else : \n        addstring = '|' + name + '=' + value \n    if before : \n        arg = get_arg ( before , args ) \n        arg . insert ( 0 , addstring ) \n    elif after : \n        arg = get_arg ( after , args ) \n        arg . insert ( len ( arg . string ) , addstring ) \n    elif args and not positional : \n        arg = args [ 0 ] \n        arg_string = arg . string \n        if preserve_spacing : \n            arg [ 0 : len ( arg_string ) ] = ( arg . string . rstrip ( WS ) + post_value_ws_mode + addstring . rstrip ( WS ) + after_values [ 0 ] ) \n        else : \n            arg . insert ( len ( arg_string ) , addstring ) \n    else : \n        self . insert ( - 2 , addstring ) "}
{"5641": "\ndef POST ( self ) : \n    json_data = web . data ( ) \n    print ( \"\\nWEBHOOK POST RECEIVED:\" ) \n    print ( json_data , \"\\n\" ) \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    print ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) \n    print ( \"FROM '{}'\" . format ( person . displayName ) ) \n    print ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) \n    me = api . people . me ( ) \n    if message . personId == me . id : \n        return 'OK' \n    elif \"/CAT\" in message . text : \n        print ( \"FOUND '/CAT'\" ) \n        cat_fact = get_catfact ( ) \n        print ( \"SENDING CAT FACT '{}'\" . format ( cat_fact ) ) \n        api . messages . create ( room . id , text = cat_fact ) \n    return 'OK' "}
{"5755": "\ndef _on_message ( self , ws , message ) : \n    self . _stop_timers ( ) \n    raw , received_at = message , time . time ( ) \n    self . log . debug ( \"_on_message(): Received new message %s at %s\" , raw , received_at ) \n    try : \n        data = json . loads ( raw ) \n    except json . JSONDecodeError : \n        return \n    if isinstance ( data , dict ) : \n        self . _system_handler ( data , received_at ) \n    elif data [ 1 ] == 'hb' : \n        self . _heartbeat_handler ( ) \n    else : \n        self . _data_handler ( data , received_at ) \n    self . _start_timers ( ) "}
{"5903": "\ndef get_contacts ( address_books , query , method = \"all\" , reverse = False , group = False , sort = \"first_name\" ) : \n    contacts = [ ] \n    for address_book in address_books : \n        contacts . extend ( address_book . search ( query , method = method ) ) \n    if group : \n        if sort == \"first_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) ) \n        elif sort == \"last_name\" : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) ) \n        else : \n            raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) \n    elif sort == \"first_name\" : \n        return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) \n    elif sort == \"last_name\" : \n        return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) \n    else : \n        raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) "}
{"5908": "\ndef birthdays_subcommand ( vcard_list , parsable ) : \n    vcard_list = [ vcard for vcard in vcard_list if vcard . get_birthday ( ) is not None ] \n    vcard_list . sort ( key = lambda x : ( x . get_birthday ( ) . month , x . get_birthday ( ) . day ) if isinstance ( x . get_birthday ( ) , datetime . datetime ) else ( 0 , 0 , x . get_birthday ( ) ) ) \n    birthday_list = [ ] \n    for vcard in vcard_list : \n        date = vcard . get_birthday ( ) \n        if parsable : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_first_name_last_name ( ) ) ) \n            else : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_last_name_first_name ( ) ) ) \n        elif config . display_by_name ( ) == \"first_name\" : \n            birthday_list . append ( \"%s\\t%s\" % ( vcard . get_first_name_last_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n        else : \n            birthday_list . append ( \"%s\\t%s\" % ( vcard . get_last_name_first_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n    if birthday_list : \n        if parsable : \n            print ( '\\n' . join ( birthday_list ) ) \n        else : \n            list_birthdays ( birthday_list ) \n    else : \n        if not parsable : \n            print ( \"Found no birthdays\" ) \n        sys . exit ( 1 ) "}
{"5915": "\ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no contact\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"%s contact %s from address book %s\" % ( action . title ( ) , source_vcard , source_vcard . address_book ) ) \n    if len ( target_address_book_list ) == 1 and target_address_book_list [ 0 ] == source_vcard . address_book : \n        print ( \"The address book %s already contains the contact %s\" % ( target_address_book_list [ 0 ] , source_vcard ) ) \n        sys . exit ( 1 ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if abook != source_vcard . address_book ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            print ( \"Error: address book list is empty\" ) \n            sys . exit ( 1 ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , True ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n    elif source_vcard == target_vcard : \n        print ( \"Target contact: %s\" % target_vcard ) \n        if action == \"move\" : \n            copy_contact ( source_vcard , selected_target_address_book , True ) \n        else : \n            print ( \"The selected contacts are already identical\" ) \n    else : \n        print ( \"The address book %s already contains the contact %s\\n\\n\" \"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n\" \"Possible actions:\\n\" \"  a: %s anyway\\n\" \"  m: Merge from source into target contact\\n\" \"  o: Overwrite target contact\\n\" \"  q: Quit\" % ( target_vcard . address_book , source_vcard , source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , \"Move\" if action == \"move\" else \"Copy\" ) ) \n        while True : \n            input_string = input ( \"Your choice: \" ) \n            if input_string . lower ( ) == \"a\" : \n                copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                break \n            if input_string . lower ( ) == \"o\" : \n                copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                target_vcard . delete_vcard_file ( ) \n                break \n            if input_string . lower ( ) == \"m\" : \n                merge_existing_contacts ( source_vcard , target_vcard , action == \"move\" ) \n                break \n            if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                break "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = 0 \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            elif type . lower ( ) == \"pref\" : \n                pref += 1 \n            elif re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                pref += int ( type . split ( \"=\" ) [ 1 ] ) \n            elif type . lower ( ) . startswith ( \"x-\" ) : \n                custom_types . append ( type [ 2 : ] ) \n                standard_types . append ( type ) \n            else : \n                custom_types . append ( type ) \n                standard_types . append ( \"X-{}\" . format ( type ) ) \n    return ( standard_types , custom_types , pref ) "}
{"5941": "\ndef safe_input ( prompt ) : \n    if sys . version_info < ( 3 , 0 ) : \n        if isinstance ( prompt , compat . text_type ) : \n            encoding = locale . getpreferredencoding ( ) or 'utf-8' \n            prompt = prompt . encode ( encoding ) \n    elif not isinstance ( prompt , compat . text_type ) : \n        prompt = prompt . decode ( ) \n    return _input ( prompt ) "}
{"5942": "\ndef encode_output ( value , output_file ) : \n    if sys . version_info > ( 3 , 0 ) : \n        return compat . text_type ( value ) \n    else : \n        stream_encoding = getattr ( output_file , 'encoding' , None ) \n        if stream_encoding : \n            if stream_encoding . upper ( ) == 'UTF-8' : \n                return compat . text_type ( value ) \n            else : \n                return value . encode ( stream_encoding , 'ignore' ) \n        elif isinstance ( value , compat . text_type ) : \n            return value . encode ( 'utf-8' ) \n        else : \n            return str ( value ) "}
{"5972": "\ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = True ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while True : \n        try : \n            call_item = call_queue . get ( block = True , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = False ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                print ( previous_tb ) \n            sys . exit ( 1 ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        elif ( ( _last_memory_leak_check is None ) or ( time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY ) ) : \n            gc . collect ( ) \n            _last_memory_leak_check = time ( ) "}
{"6085": "\ndef open_json_or_csv_somehow ( filename , date_format = None ) : \n    fileformat = None \n    if filename . endswith ( '.csv' ) : \n        fileformat = 'csv' \n    elif filename . endswith ( '.jsons' ) : \n        fileformat = 'jsons' \n    else : \n        with open ( filename ) as opened : \n            line = opened . readline ( ) \n            if line [ 0 ] not in '{[' and not filename . endswith ( '.json' ) : \n                fileformat = 'csv' \n            elif ( line . count ( '{' ) == line . count ( '}' ) and line . count ( '[' ) == line . count ( ']' ) ) : \n                char = ' ' \n                while char . isspace ( ) : \n                    char = opened . read ( ) \n                    if char == '' : \n                        fileformat = 'json' \n                        break \n                if fileformat is None : \n                    fileformat = 'jsons' \n            else : \n                fileformat = 'json' \n    if fileformat == 'json' : \n        stream = json . load ( open ( filename ) , encoding = 'utf-8' ) \n    elif fileformat == 'csv' : \n        stream = open_csv_somehow ( filename ) \n    else : \n        stream = stream_json_lines ( filename ) \n    return _normalize_data ( stream , date_format = date_format ) "}
{"6402": "\ndef toRtl ( unitOrCls : Unit , name : str = None , serializer : GenericSerializer = VhdlSerializer , targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : \n    if not isinstance ( unitOrCls , Unit ) : \n        u = unitOrCls ( ) \n    else : \n        u = unitOrCls \n    u . _loadDeclarations ( ) \n    if name is not None : \n        assert isinstance ( name , str ) \n        u . _name = name \n    globScope = serializer . getBaseNameScope ( ) \n    mouduleScopes = { } \n    serializedClasses = { } \n    serializedConfiguredUnits = { } \n    doSerialize = True \n    createFiles = saveTo is not None \n    if createFiles : \n        os . makedirs ( saveTo , exist_ok = True ) \n        files = UniqList ( ) \n    else : \n        codeBuff = [ ] \n    for obj in u . _toRtl ( targetPlatform ) : \n        doSerialize = serializer . serializationDecision ( obj , serializedClasses , serializedConfiguredUnits ) \n        if doSerialize : \n            if isinstance ( obj , Entity ) : \n                s = globScope . fork ( 1 ) \n                s . setLevel ( 2 ) \n                ctx = serializer . getBaseContext ( ) \n                ctx . scope = s \n                mouduleScopes [ obj ] = ctx \n                ctx . currentUnit = obj . origin \n                sc = serializer . Entity ( obj , ctx ) \n                if createFiles : \n                    fName = obj . name + serializer . fileExtension \n                    fileMode = 'w' \n            elif isinstance ( obj , Architecture ) : \n                try : \n                    ctx = mouduleScopes [ obj . entity ] \n                except KeyError : \n                    raise SerializerException ( \"Entity should be serialized\" \" before architecture of %s\" % ( obj . getEntityName ( ) ) ) \n                sc = serializer . Architecture ( obj , ctx ) \n                if createFiles : \n                    fName = obj . getEntityName ( ) + serializer . fileExtension \n                    fileMode = 'a' \n            elif hasattr ( obj , \"_hdlSources\" ) : \n                for fn in obj . _hdlSources : \n                    if isinstance ( fn , str ) : \n                        shutil . copy2 ( fn , saveTo ) \n                        files . append ( fn ) \n                        continue \n            else : \n                sc = serializer . asHdl ( obj ) \n            if sc : \n                if createFiles : \n                    fp = os . path . join ( saveTo , fName ) \n                    files . append ( fp ) \n                    with open ( fp , fileMode ) as f : \n                        if fileMode == 'a' : \n                            f . write ( \"\\n\" ) \n                        f . write ( serializer . formatter ( sc ) ) \n                else : \n                    codeBuff . append ( sc ) \n        elif not createFiles : \n            try : \n                name = '\"%s\"' % obj . name \n            except AttributeError : \n                name = \"\" \n            codeBuff . append ( serializer . comment ( \"Object of class %s, %s was not serialized as specified\" % ( obj . __class__ . __name__ , name ) ) ) \n    if createFiles : \n        return files \n    else : \n        return serializer . formatter ( \"\\n\" . join ( codeBuff ) ) "}
{"6412": "\ndef _signalsForInterface ( self , context , prefix = '' , typeTransform = None ) : \n    sigs = [ ] \n    if self . _interfaces : \n        for intf in self . _interfaces : \n            sigs . extend ( intf . _signalsForInterface ( context , prefix , typeTransform = typeTransform ) ) \n    elif hasattr ( self , '_sig' ) : \n        sigs = [ self . _sig ] \n    else : \n        t = self . _dtype \n        if typeTransform is not None : \n            t = typeTransform ( t ) \n        s = context . sig ( prefix + self . _getPhysicalName ( ) , t ) \n        s . _interface = self \n        self . _sig = s \n        if hasattr ( self , '_boundedEntityPort' ) : \n            self . _boundedEntityPort . connectSig ( self . _sig ) \n        sigs = [ s ] \n    return sigs "}
{"6429": "\ndef _on_reduce ( self , self_reduced : bool , io_changed : bool , result_statements : List [ \"HdlStatement\" ] ) -> None : \n    parentStm = self . parentStm \n    if self_reduced : \n        was_top = parentStm is None \n        if was_top : \n            ctx = self . _get_rtl_context ( ) \n            ctx . statements . remove ( self ) \n            ctx . statements . update ( result_statements ) \n            for i in self . _inputs : \n                i . endpoints . discard ( self ) \n            for o in self . _outputs : \n                o . drivers . remove ( self ) \n        for stm in result_statements : \n            stm . parentStm = parentStm \n            if parentStm is None : \n                for inp in stm . _inputs : \n                    inp . endpoints . append ( stm ) \n                for outp in stm . _outputs : \n                    outp . drivers . append ( stm ) \n    elif io_changed : \n        self . _inputs = UniqList ( ) \n        self . _outputs = UniqList ( ) \n        self . _collect_io ( ) "}
{"6462": "\ndef packIntf ( intf , masterDirEqTo = DIRECTION . OUT , exclude = None ) : \n    if not intf . _interfaces : \n        if intf . _masterDir == masterDirEqTo : \n            return intf . _sig \n        return None \n    res = None \n    for i in intf . _interfaces : \n        if exclude is not None and i in exclude : \n            continue \n        if i . _interfaces : \n            if i . _masterDir == DIRECTION . IN : \n                d = DIRECTION . opposite ( masterDirEqTo ) \n            else : \n                d = masterDirEqTo \n            s = i . _pack ( d , exclude = exclude ) \n        elif i . _masterDir == masterDirEqTo : \n            s = i . _sig \n        else : \n            s = None \n        if s is not None : \n            if res is None : \n                res = s \n            else : \n                res = res . _concat ( s ) \n    return res "}
{"6702": "\ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    print ( results ) \n    if name is not None : \n        for result in results : \n            matches = False \n            if name in result . name : \n                matches = True \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = True \n            if matches is True : \n                content = self . _print_log ( result . name ) \n    elif len ( results ) > 0 : \n        latest = results [ 0 ] \n        for result in results : \n            if result . time_created >= latest . time_created : \n                latest = result \n        content = self . _print_log ( result . name ) \n    return content "}
{"6711": "\ndef delete_backend ( backend ) : \n    settings = read_client_secrets ( ) \n    if backend in settings : \n        del settings [ backend ] \n        if 'SREGISTRY_CLIENT' in settings : \n            if settings [ 'SREGISTRY_CLIENT' ] == backend : \n                del settings [ 'SREGISTRY_CLIENT' ] \n        update_secrets ( settings ) \n        print ( '[delete] %s' % backend ) \n    elif backend is not None : \n        print ( '%s is not a known client.' % backend ) \n    else : \n        print ( 'Please specify a backend to delete.' ) "}
{"7133": "\ndef read_features ( self , tol = 1e-3 ) : \n    try : \n        with open ( self . file_struct . features_file ) as f : \n            feats = json . load ( f ) \n        if self . dur is None : \n            self . dur = float ( feats [ \"globals\" ] [ \"dur\" ] ) \n        assert ( np . isclose ( self . dur , float ( feats [ \"globals\" ] [ \"dur\" ] ) , rtol = tol ) ) \n        assert ( self . sr == int ( feats [ \"globals\" ] [ \"sample_rate\" ] ) ) \n        assert ( self . hop_length == int ( feats [ \"globals\" ] [ \"hop_length\" ] ) ) \n        assert ( os . path . basename ( self . file_struct . audio_file ) == os . path . basename ( feats [ \"globals\" ] [ \"audio_file\" ] ) ) \n        feat_params_err = FeatureParamsError ( \"Couldn't find features for %s id in file %s\" % ( self . get_id ( ) , self . file_struct . features_file ) ) \n        if self . get_id ( ) not in feats . keys ( ) : \n            raise feat_params_err \n        for param_name in self . get_param_names ( ) : \n            value = getattr ( self , param_name ) \n            if hasattr ( value , '__call__' ) : \n                if value . __name__ != feats [ self . get_id ( ) ] [ \"params\" ] [ param_name ] : \n                    raise feat_params_err \n            elif str ( value ) != feats [ self . get_id ( ) ] [ \"params\" ] [ param_name ] : \n                raise feat_params_err \n        self . _est_beats_times = np . array ( feats [ \"est_beats\" ] ) \n        self . _est_beatsync_times = np . array ( feats [ \"est_beatsync_times\" ] ) \n        self . _est_beats_frames = librosa . core . time_to_frames ( self . _est_beats_times , sr = self . sr , hop_length = self . hop_length ) \n        self . _framesync_features = np . array ( feats [ self . get_id ( ) ] [ \"framesync\" ] ) \n        self . _est_beatsync_features = np . array ( feats [ self . get_id ( ) ] [ \"est_beatsync\" ] ) \n        if \"ann_beats\" in feats . keys ( ) : \n            self . _ann_beats_times = np . array ( feats [ \"ann_beats\" ] ) \n            self . _ann_beatsync_times = np . array ( feats [ \"ann_beatsync_times\" ] ) \n            self . _ann_beats_frames = librosa . core . time_to_frames ( self . _ann_beats_times , sr = self . sr , hop_length = self . hop_length ) \n            self . _ann_beatsync_features = np . array ( feats [ self . get_id ( ) ] [ \"ann_beatsync\" ] ) \n    except KeyError : \n        raise WrongFeaturesFormatError ( \"The features file %s is not correctly formatted\" % self . file_struct . features_file ) \n    except AssertionError : \n        raise FeaturesNotFound ( \"The features for the given parameters were not found in \" \"features file %s\" % self . file_struct . features_file ) \n    except IOError : \n        raise NoFeaturesFileError ( \"Could not find features file %s\" , self . file_struct . features_file ) "}
{"7306": "\ndef add_coalescent_model ( self , Tc , ** kwargs ) : \n    from . merger_models import Coalescent \n    self . logger ( 'TreeTime.run: adding coalescent prior with Tc=' + str ( Tc ) , 1 ) \n    self . merger_model = Coalescent ( self . tree , date2dist = self . date2dist , logger = self . logger ) \n    if Tc == 'skyline' : \n        self . merger_model . optimize_skyline ( ** kwargs ) \n        self . logger ( \"optimized a skyline \" , 2 ) \n    elif Tc in [ 'opt' , 'const' ] : \n        self . merger_model . optimize_Tc ( ) \n        self . logger ( \"optimized Tc to %f\" % self . merger_model . Tc . y [ 0 ] , 2 ) \n    else : \n        try : \n            self . merger_model . set_Tc ( Tc ) \n        except : \n            self . logger ( \"setting of coalescent time scale failed\" , 1 , warn = True ) \n    self . merger_model . attach_to_tree ( ) "}
{"7440": "\ndef get_length ( alt_len , ref_len , category , pos , end , svtype = None , svlen = None ) : \n    length = - 1 \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        if ref_len == alt_len : \n            length = alt_len \n        else : \n            length = abs ( ref_len - alt_len ) \n    elif category == 'sv' : \n        if svtype == 'bnd' : \n            length = int ( 10e10 ) \n        elif svlen : \n            length = abs ( int ( svlen ) ) \n        elif end : \n            if end != pos : \n                length = end - pos \n    return length "}
{"7504": "\ndef add_peddy_information ( config_data ) : \n    ped_info = { } \n    ped_check = { } \n    sex_check = { } \n    relations = [ ] \n    if config_data . get ( 'peddy_ped' ) : \n        file_handle = open ( config_data [ 'peddy_ped' ] , 'r' ) \n        for ind_info in parse_peddy_ped ( file_handle ) : \n            ped_info [ ind_info [ 'sample_id' ] ] = ind_info \n    if config_data . get ( 'peddy_ped_check' ) : \n        file_handle = open ( config_data [ 'peddy_ped_check' ] , 'r' ) \n        for pair_info in parse_peddy_ped_check ( file_handle ) : \n            ped_check [ ( pair_info [ 'sample_a' ] , pair_info [ 'sample_b' ] ) ] = pair_info \n    if config_data . get ( 'peddy_sex_check' ) : \n        file_handle = open ( config_data [ 'peddy_sex_check' ] , 'r' ) \n        for ind_info in parse_peddy_sex_check ( file_handle ) : \n            sex_check [ ind_info [ 'sample_id' ] ] = ind_info \n    if not ped_info : \n        return \n    analysis_inds = { } \n    for ind in config_data [ 'samples' ] : \n        ind_id = ind [ 'sample_id' ] \n        analysis_inds [ ind_id ] = ind \n    for ind_id in analysis_inds : \n        ind = analysis_inds [ ind_id ] \n        if ind_id in ped_info : \n            ind [ 'predicted_ancestry' ] = ped_info [ ind_id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) \n        if ind_id in sex_check : \n            if sex_check [ ind_id ] [ 'error' ] : \n                ind [ 'confirmed_sex' ] = False \n            else : \n                ind [ 'confirmed_sex' ] = True \n        for parent in [ 'mother' , 'father' ] : \n            if ind [ parent ] != '0' : \n                for pair in ped_check : \n                    if ( ind_id in pair and ind [ parent ] in pair ) : \n                        if ped_check [ pair ] [ 'parent_error' ] : \n                            analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = False \n                        elif 'confirmed_parent' not in analysis_inds [ ind [ parent ] ] : \n                            analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = True "}
{"7598": "\ndef get_date ( date , date_format = None ) : \n    date_obj = datetime . datetime . now ( ) \n    if date : \n        if date_format : \n            date_obj = datetime . datetime . strptime ( date , date_format ) \n        elif match_date ( date ) : \n            if len ( date . split ( '-' ) ) == 3 : \n                date = date . split ( '-' ) \n            elif len ( date . split ( ' ' ) ) == 3 : \n                date = date . split ( ' ' ) \n            elif len ( date . split ( '.' ) ) == 3 : \n                date = date . split ( '.' ) \n            else : \n                date = date . split ( '/' ) \n            date_obj = datetime . datetime ( * ( int ( number ) for number in date ) ) \n        else : \n            raise ValueError ( \"Date %s is invalid\" % date ) \n    return date_obj "}
{"7604": "\ndef load_delivery_report ( adapter : MongoAdapter , report_path : str , case_id : str , update : bool = False ) : \n    case_obj = adapter . case ( case_id = case_id , ) \n    if case_obj is None : \n        raise DataNotFoundError ( \"no case found\" ) \n    if not case_obj . get ( 'delivery_report' ) : \n        _put_report_in_case_root ( case_obj , report_path ) \n    elif update : \n        _put_report_in_case_root ( case_obj , report_path ) \n    else : \n        raise IntegrityError ( 'Existing delivery report found, use update = True to ' 'overwrite' ) \n    logger . info ( 'Saving report for case {} in database' . format ( case_obj [ '_id' ] ) ) \n    return adapter . replace_case ( case_obj ) "}
{"7697": "\ndef events ( institute_id , case_name , event_id = None ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    link = request . form . get ( 'link' ) \n    content = request . form . get ( 'content' ) \n    variant_id = request . args . get ( 'variant_id' ) \n    user_obj = store . user ( current_user . email ) \n    if event_id : \n        store . delete_event ( event_id ) \n    elif variant_id : \n        variant_obj = store . variant ( variant_id ) \n        level = request . form . get ( 'level' , 'specific' ) \n        store . comment ( institute_obj , case_obj , user_obj , link , variant = variant_obj , content = content , comment_level = level ) \n    else : \n        store . comment ( institute_obj , case_obj , user_obj , link , content = content ) \n    return redirect ( request . referrer ) "}
{"7769": "\ndef init_log ( logger , filename = None , loglevel = None ) : \n    template = '[%(asctime)s] %(levelname)-8s: %(name)-25s: %(message)s' \n    formatter = logging . Formatter ( template ) \n    if loglevel : \n        logger . setLevel ( getattr ( logging , loglevel ) ) \n    console = logging . StreamHandler ( ) \n    console . setLevel ( 'WARNING' ) \n    console . setFormatter ( formatter ) \n    if filename : \n        file_handler = logging . FileHandler ( filename , encoding = 'utf-8' ) \n        if loglevel : \n            file_handler . setLevel ( getattr ( logging , loglevel ) ) \n        file_handler . setFormatter ( formatter ) \n        logger . addHandler ( file_handler ) \n    elif loglevel : \n        console . setLevel ( getattr ( logging , loglevel ) ) \n    logger . addHandler ( console ) "}
{"7933": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and self . _args . verbose > 2 : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        print ( \"{}D layer condition:\" . format ( dimension ) , file = output_file ) \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            print ( cache + \": \" , end = '' , file = output_file ) \n            if lc_solution [ 'lt' ] is sympy . true : \n                print ( \"unconditionally fulfilled\" , file = output_file ) \n            elif lc_solution [ 'eq' ] is None : \n                print ( \"{}\" . format ( lc_solution [ 'lt' ] ) , file = output_file ) \n            elif type ( lc_solution [ 'eq' ] ) is not list : \n                print ( \"{}\" . format ( lc_solution [ 'eq' ] ) , file = output_file ) \n            else : \n                for solu in lc_solution [ 'eq' ] : \n                    for s , v in solu . items ( ) : \n                        print ( \"{} <= {}\" . format ( s , v ) , file = output_file ) "}
{"7996": "\ndef build_executable ( self , lflags = None , verbose = False , openmp = False ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = True ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            print ( 'Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) \n            sys . exit ( 1 ) \n        compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] \n        if os . environ . get ( 'LIKWID_LIB' ) == '' : \n            compiler_args = compiler_args [ : - 1 ] \n        if lflags is None : \n            lflags = [ ] \n        lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            print ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            print ( \"Build failed:\" , e , file = sys . stderr ) \n            sys . exit ( 1 ) \n    elif verbose : \n        print ( 'Executing (build_executable): ' , 'using cached' , out_filename ) \n    return out_filename "}
{"8291": "\ndef _node_to_msg ( store_load , node ) : \n    if node . v_is_leaf : \n        if store_load == STORE : \n            return pypetconstants . LEAF \n        elif store_load == LOAD : \n            return pypetconstants . LEAF \n        elif store_load == REMOVE : \n            return pypetconstants . DELETE \n    elif store_load == STORE : \n        return pypetconstants . GROUP \n    elif store_load == LOAD : \n        return pypetconstants . GROUP \n    elif store_load == REMOVE : \n        return pypetconstants . DELETE "}
{"8308": "\ndef _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    if predicate is None : \n        predicate = lambda x : True \n    iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) \n    start = True \n    visited_linked_nodes = set ( [ ] ) \n    while True : \n        try : \n            depth , name , item = next ( iterator_queue ) \n            full_name = item . _full_name \n            if start or predicate ( item ) : \n                if full_name in visited_linked_nodes : \n                    if in_search : \n                        yield depth , name , item \n                elif depth <= max_depth : \n                    if start : \n                        start = False \n                    elif in_search : \n                        yield depth , name , item \n                    else : \n                        yield item \n                    if full_name in linked_by : \n                        visited_linked_nodes . add ( full_name ) \n                    if not item . _is_leaf and depth < max_depth : \n                        child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) \n                        iterator_queue . add ( child_iterator ) \n        except StopIteration : \n            break "}
{"8576": "\ndef config_from_file ( filename , config = None ) : \n    if config : \n        try : \n            with open ( filename , 'w' ) as fdesc : \n                fdesc . write ( json . dumps ( config ) ) \n        except IOError as error : \n            logger . exception ( error ) \n            return False \n        return True \n    elif os . path . isfile ( filename ) : \n        try : \n            with open ( filename , 'r' ) as fdesc : \n                return json . loads ( fdesc . read ( ) ) \n        except IOError as error : \n            return False \n    else : \n        return { } "}
{"8625": "\ndef get_status ( options ) : \n    payload = { \"username\" : options . username , \"password\" : options . password , \"server\" : options . server , \"port\" : options . port , } \n    try : \n        if options . server . startswith ( \"/\" ) and stat . S_ISSOCK ( os . stat ( options . server ) . st_mode ) : \n            try : \n                import supervisor . xmlrpc \n            except ImportError as error : \n                sys . stderr . write ( \"ERROR: Couldn't load module. {error}\\n\" . format ( error = error ) ) \n                sys . stderr . write ( \"ERROR: Unix socket support not available! Please install nagios-check-supervisord with unix socket support: 'nagios-check-supervisord[unix-socket-support]' or install 'supervisor' separately.\\n\" ) \n                sys . exit ( - 1 ) \n            if all ( [ options . username , options . password , ] ) : \n                connection = xmlrpclib . ServerProxy ( \"https://\" , transport = supervisor . xmlrpc . SupervisorTransport ( options . username , options . password , serverurl = URI [ URI_TPL_SOCKET ] . format ( ** payload ) ) ) \n            else : \n                connection = xmlrpclib . ServerProxy ( \"https://\" , transport = supervisor . xmlrpc . SupervisorTransport ( None , None , serverurl = URI [ URI_TPL_SOCKET ] . format ( ** payload ) ) ) \n        elif all ( [ options . username , options . password , ] ) : \n            connection = xmlrpclib . Server ( URI [ URI_TPL_HTTP_AUTH ] . format ( ** payload ) ) \n        else : \n            connection = xmlrpclib . Server ( URI [ URI_TPL_HTTP ] . format ( ** payload ) ) \n        return connection . supervisor . getAllProcessInfo ( ) \n    except Exception as error : \n        if not options . quiet : \n            sys . stdout . write ( \"ERROR: Server communication problem. {error}\\n\" . format ( error = error ) ) \n        sys . exit ( EXIT_CODES . get ( options . network_errors_exit_code , EXIT_CODE_UNKNOWN ) ) "}
{"9180": "\ndef _extract_arg_value ( cls , arg_name , arg_type , remaining ) : \n    next_arg = None \n    should_consume = False \n    if len ( remaining ) > 0 : \n        next_arg = remaining [ 0 ] \n        should_consume = True \n        if next_arg == '--' : \n            next_arg = None \n    if arg_type == \"bool\" : \n        if next_arg is None or next_arg . startswith ( '-' ) : \n            next_arg = True \n            should_consume = False \n    elif next_arg is None : \n        raise ArgumentError ( \"Could not find value for keyword argument\" , argument = arg_name ) \n    if should_consume : \n        remaining . pop ( 0 ) \n    return next_arg "}
{"9188": "\ndef _join_paragraphs ( cls , lines , use_indent = False , leading_blanks = False , trailing_blanks = False ) : \n    curr_para = [ ] \n    paragraphs = [ ] \n    for line in lines : \n        if use_indent : \n            if line . startswith ( ' ' ) : \n                curr_para . append ( line . lstrip ( ) ) \n                continue \n            elif line == '' : \n                continue \n            else : \n                if len ( curr_para ) > 0 : \n                    paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ line . lstrip ( ) ] \n        elif len ( line ) != 0 : \n            curr_para . append ( line ) \n        else : \n            paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n            curr_para = [ ] \n    if len ( curr_para ) > 0 : \n        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n    return paragraphs "}
{"9324": "\ndef GetParam ( tag , param , default = __SENTINEL ) : \n    if tag . HasParam ( param ) : \n        return tag . GetParam ( param ) \n    elif default == __SENTINEL : \n        raise KeyError \n    else : \n        return default "}
{"9548": "\ndef build_list ( li_nodes , meta_data ) : \n    ol_dict = { } \n    current_ilvl = - 1 \n    current_numId = - 1 \n    current_ol = None \n    root_ol = None \n    visited_nodes = [ ] \n    list_contents = [ ] \n    def _build_li ( list_contents ) : \n        data = '<br />' . join ( t for t in list_contents if t is not None ) \n        return etree . XML ( '<li>%s</li>' % data ) \n    def _build_non_li_content ( el , meta_data ) : \n        w_namespace = get_namespace ( el , 'w' ) \n        if el . tag == '%stbl' % w_namespace : \n            new_el , visited_nodes = build_table ( el , meta_data ) \n            return etree . tostring ( new_el ) , visited_nodes \n        elif el . tag == '%sp' % w_namespace : \n            return get_element_content ( el , meta_data ) , [ el ] \n        if has_text ( el ) : \n            raise UnintendedTag ( 'Did not expect %s' % el . tag ) \n    def _merge_lists ( ilvl , current_ilvl , ol_dict , current_ol ) : \n        for i in reversed ( range ( ilvl , current_ilvl ) ) : \n            if i not in ol_dict : \n                continue \n            if ol_dict [ i ] is not current_ol : \n                if ol_dict [ i ] is current_ol : \n                    continue \n                ol_dict [ i ] [ - 1 ] . append ( current_ol ) \n                current_ol = ol_dict [ i ] \n        for key in list ( ol_dict ) : \n            if key > ilvl : \n                del ol_dict [ key ] \n        return current_ol \n    for li_node in li_nodes : \n        w_namespace = get_namespace ( li_node , 'w' ) \n        if not is_li ( li_node , meta_data ) : \n            new_el , el_visited_nodes = _build_non_li_content ( li_node , meta_data , ) \n            list_contents . append ( new_el ) \n            visited_nodes . extend ( el_visited_nodes ) \n            continue \n        if list_contents : \n            li_el = _build_li ( list_contents ) \n            list_contents = [ ] \n            current_ol . append ( li_el ) \n        list_contents . append ( get_element_content ( li_node , meta_data , ) ) \n        ilvl = get_ilvl ( li_node , w_namespace ) \n        numId = get_numId ( li_node , w_namespace ) \n        list_type = get_ordered_list_type ( meta_data , numId , ilvl ) \n        if ( ilvl > current_ilvl ) or ( numId != current_numId ) : \n            ol_dict [ ilvl ] = create_list ( list_type ) \n            current_ol = ol_dict [ ilvl ] \n            current_ilvl = ilvl \n            current_numId = numId \n        else : \n            current_ol = _merge_lists ( ilvl = ilvl , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n        if root_ol is None : \n            root_ol = current_ol \n        if ilvl in ol_dict : \n            current_ol = ol_dict [ ilvl ] \n        elif current_ol is not root_ol : \n            root_ol [ - 1 ] . append ( current_ol ) \n            current_ol = create_list ( list_type ) \n        visited_nodes . extend ( list ( li_node . iter ( ) ) ) \n    if list_contents : \n        li_el = _build_li ( list_contents ) \n        list_contents = [ ] \n        current_ol . append ( li_el ) \n    current_ol = _merge_lists ( ilvl = 0 , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n    return root_ol , visited_nodes "}
{"9624": "\ndef indent ( el , level = 0 ) : \n    i = '\\n' + level * '\\t' \n    if len ( el ) : \n        if not el . text or not el . text . strip ( ) : \n            el . text = i + '\\t' \n        if not el . tail or not el . tail . strip ( ) : \n            el . tail = i \n        for elem in el : \n            indent ( elem , level + 1 ) \n        if not el . tail or not el . tail . strip ( ) : \n            el . tail = i \n    elif level and ( not el . tail or not el . tail . strip ( ) ) : \n        el . tail = i "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != 0 : \n                overflow_counter += 1 \n                continue \n            new_level = level_func ( embedding_level ) \n            if new_level < EXPLICIT_LEVEL_LIMIT : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif embedding_level == EXPLICIT_LEVEL_LIMIT - 2 : \n                almost_overflow_counter += 1 \n            else : \n                overflow_counter += 1 \n        elif bidi_type not in X6_IGNORED : \n            _ch [ 'level' ] = embedding_level \n            if directional_override != 'N' : \n                _ch [ 'type' ] = directional_override \n        elif bidi_type == 'PDF' : \n            if overflow_counter : \n                overflow_counter -= 1 \n            elif almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - 1 : \n                almost_overflow_counter -= 1 \n            elif levels : \n                embedding_level , directional_override = levels . pop ( ) \n        elif bidi_type == 'B' : \n            levels . clear ( ) \n            overflow_counter = almost_overflow_counter = 0 \n            embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n            directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9664": "\ndef resolve_neutral_types ( storage , debug ) : \n    for run in storage [ 'runs' ] : \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = [ { 'type' : run [ 'sor' ] } ] + storage [ 'chars' ] [ start : start + length ] + [ { 'type' : run [ 'eor' ] } ] \n        total_chars = len ( chars ) \n        seq_start = None \n        for idx in range ( total_chars ) : \n            _ch = chars [ idx ] \n            if _ch [ 'type' ] in ( 'B' , 'S' , 'WS' , 'ON' ) : \n                if seq_start is None : \n                    seq_start = idx \n                    prev_bidi_type = chars [ idx - 1 ] [ 'type' ] \n            elif seq_start is not None : \n                next_bidi_type = chars [ idx ] [ 'type' ] \n                if prev_bidi_type in ( 'AN' , 'EN' ) : \n                    prev_bidi_type = 'R' \n                if next_bidi_type in ( 'AN' , 'EN' ) : \n                    next_bidi_type = 'R' \n                for seq_idx in range ( seq_start , idx ) : \n                    if prev_bidi_type == next_bidi_type : \n                        chars [ seq_idx ] [ 'type' ] = prev_bidi_type \n                    else : \n                        chars [ seq_idx ] [ 'type' ] = _embedding_direction ( chars [ seq_idx ] [ 'level' ] ) \n                seq_start = None \n    if debug : \n        debug_storage ( storage ) "}
{"9665": "\ndef reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) : \n    for level in range ( highest_level , lowest_odd_level - 1 , - 1 ) : \n        _start = _end = None \n        for run_idx in range ( line_start , line_end + 1 ) : \n            run_ch = chars [ run_idx ] \n            if run_ch [ 'level' ] >= level : \n                if _start is None : \n                    _start = _end = run_idx \n                else : \n                    _end = run_idx \n            elif _end : \n                chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) \n                _start = _end = None \n        if _start is not None : \n            chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) "}
{"9775": "\ndef _execute_get_url ( self , request_url , append_sid = True ) : \n    self . _debuglog ( \"Requesting URL: '\" + request_url + \"'\" ) \n    if append_sid : \n        self . _debuglog ( \"Appending access_token (SID: \" + self . access_token + \") to url\" ) \n        request_url = \"%s&_sid=%s\" % ( request_url , self . access_token ) \n    try : \n        resp = self . _session . get ( request_url ) \n        self . _debuglog ( \"Request executed: \" + str ( resp . status_code ) ) \n        if resp . status_code == 200 : \n            json_data = json . loads ( resp . text ) \n            if json_data [ \"success\" ] : \n                self . _debuglog ( \"Succesfull returning data\" ) \n                self . _debuglog ( str ( json_data ) ) \n                return json_data \n            elif json_data [ \"error\" ] [ \"code\" ] in { 105 , 106 , 107 , 119 } : \n                self . _debuglog ( \"Session error: \" + str ( json_data [ \"error\" ] [ \"code\" ] ) ) \n                self . _session_error = True \n            else : \n                self . _debuglog ( \"Failed: \" + resp . text ) \n        else : \n            return None \n    except : \n        return None "}
{"10023": "\ndef fuzzmatch ( self , fuzzkey , multi = False ) : \n    keys , ratios = np . array ( [ ( f , seqm ( None , fuzzkey , f ) . ratio ( ) ) for f in self . components . keys ( ) ] ) . T \n    mratio = max ( ratios ) \n    if multi : \n        return keys [ ratios == mratio ] \n    elif sum ( ratios == mratio ) == 1 : \n        return keys [ ratios == mratio ] [ 0 ] \n    else : \n        raise ValueError ( \"\\nThe filter key provided ('{:}') matches two or more filter names equally well:\\n\" . format ( fuzzkey ) + ', ' . join ( keys [ ratios == mratio ] ) + \"\\nPlease be more specific!\" ) "}
{"10025": "\ndef grab_filt ( self , filt , analyte = None ) : \n    if isinstance ( filt , str ) : \n        if filt in self . components : \n            if analyte is None : \n                return self . components [ filt ] \n            elif self . switches [ analyte ] [ filt ] : \n                return self . components [ filt ] \n        else : \n            try : \n                ind = self . make_fromkey ( filt ) \n            except KeyError : \n                print ( ( \"\\n\\n***Filter key invalid. Please consult \" \"manual and try again.\" ) ) \n    elif isinstance ( filt , dict ) : \n        try : \n            ind = self . make_fromkey ( filt [ analyte ] ) \n        except ValueError : \n            print ( ( \"\\n\\n***Filter key invalid. Please consult manual \" \"and try again.\\nOR\\nAnalyte missing from filter \" \"key dict.\" ) ) \n    elif filt : \n        ind = self . make ( analyte ) \n    else : \n        ind = ~ np . zeros ( self . size , dtype = bool ) \n    return ind "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if period < 1 or period > 3 : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + \"2903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10249": "\ndef setMeterPassword ( self , new_pwd , pwd = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMeterPassword\" ) \n    try : \n        if len ( new_pwd ) != 8 or len ( pwd ) != 8 : \n            self . writeCmdMsg ( \"Passwords must be exactly eight characters.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Pre command read failed: check serial line.\" ) \n        elif not self . serialCmdPwdAuth ( pwd ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_pwd = binascii . hexlify ( new_pwd . zfill ( 8 ) ) \n            req_str = \"015731023030323028\" + req_pwd + \"2903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success(setMeterPassword): 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10256": "\ndef setCTRatio ( self , new_ct , password = \"00000000\" ) : \n    ret = False \n    self . setContext ( \"setCTRatio\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if ( ( new_ct != CTRatio . Amps_100 ) and ( new_ct != CTRatio . Amps_200 ) and ( new_ct != CTRatio . Amps_400 ) and ( new_ct != CTRatio . Amps_600 ) and ( new_ct != CTRatio . Amps_800 ) and ( new_ct != CTRatio . Amps_1000 ) and ( new_ct != CTRatio . Amps_1200 ) and ( new_ct != CTRatio . Amps_1500 ) and ( new_ct != CTRatio . Amps_2000 ) and ( new_ct != CTRatio . Amps_3000 ) and ( new_ct != CTRatio . Amps_4000 ) and ( new_ct != CTRatio . Amps_5000 ) ) : \n            self . writeCmdMsg ( \"Legal CT Ratios: 100, 200, 400, 600, \" + \"800, 1000, 1200, 1500, 2000, 3000, 4000 and 5000\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_str = \"015731023030443028\" + binascii . hexlify ( str ( new_ct ) . zfill ( 4 ) ) + \"2903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success(setCTRatio): 06 returned.\" ) \n                ret = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return ret "}
{"10259": "\ndef setSeasonSchedules ( self , cmd_dict = None , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setSeasonSchedules\" ) \n    if not cmd_dict : \n        cmd_dict = self . m_seasons_sched_params \n    try : \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_table = \"\" \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Month\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Day\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Schedule\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Month\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Day\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Schedule\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Month\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Day\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Schedule\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Month\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Day\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Schedule\" ] ) . zfill ( 2 ) ) \n            req_table += binascii . hexlify ( str ( 0 ) . zfill ( 24 ) ) \n            req_str = \"015731023030383028\" + req_table + \"2903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success(setSeasonSchedules): 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if seconds < 0 or seconds > 9999 : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_str = \"\" \n            req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + \"2903\" ) \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10280": "\ndef setPulseInputRatio ( self , line_in , new_cnst , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setPulseInputRatio\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4 ) ) \n            line_const = binascii . hexlify ( str ( line_in - 1 ) ) \n            req_str = \"01573102303041\" + line_const + \"28\" + req_const + \"2903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10281": "\ndef setZeroResettableKWH ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setZeroResettableKWH\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_str = \"0157310230304433282903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        elif not self . serialCmdPwdAuth ( password ) : \n            self . writeCmdMsg ( \"Password failure\" ) \n        else : \n            req_table = \"\" \n            fill_len = 40 - len ( self . m_lcd_items ) \n            for lcdid in self . m_lcd_items : \n                append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) \n                req_table += append_val \n            for i in range ( 0 , fill_len ) : \n                append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2 ) ) \n                req_table += append_val \n            req_str = \"015731023030443228\" + req_table + \"2903\" \n            req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n            self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n            if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10284": "\ndef iterate_schema ( fields , schema , path = None ) : \n    for field_schema in schema : \n        name = field_schema [ 'name' ] \n        if 'group' in field_schema : \n            for rvals in iterate_schema ( fields [ name ] if name in fields else { } , field_schema [ 'group' ] , None if path is None else '{}.{}' . format ( path , name ) ) : \n                yield rvals \n        elif path is None : \n            yield ( field_schema , fields ) \n        else : \n            yield ( field_schema , fields , '{}.{}' . format ( path , name ) ) "}
{"10381": "\ndef selector_production ( self , tokens ) : \n    validators = [ ] \n    if self . peek ( tokens , 'type' ) : \n        type_ = self . match ( tokens , 'type' ) \n        validators . append ( self . type_production ( type_ ) ) \n    if self . peek ( tokens , 'identifier' ) : \n        key = self . match ( tokens , 'identifier' ) \n        validators . append ( self . key_production ( key ) ) \n    if self . peek ( tokens , 'pclass' ) : \n        pclass = self . match ( tokens , 'pclass' ) \n        validators . append ( self . pclass_production ( pclass ) ) \n    if self . peek ( tokens , 'nth_func' ) : \n        nth_func = self . match ( tokens , 'nth_func' ) \n        validators . append ( self . nth_child_production ( nth_func , tokens ) ) \n    if self . peek ( tokens , 'pclass_func' ) : \n        pclass_func = self . match ( tokens , 'pclass_func' ) \n        validators . append ( self . pclass_func_production ( pclass_func , tokens ) ) \n    if not len ( validators ) : \n        raise SelectorSyntaxError ( 'no selector recognized.' ) \n    results = self . _match_nodes ( validators , self . obj ) \n    if self . peek ( tokens , 'operator' ) : \n        operator = self . match ( tokens , 'operator' ) \n        rvals = self . selector_production ( tokens ) \n        if operator == ',' : \n            results . extend ( rvals ) \n        elif operator == '>' : \n            results = self . parents ( results , rvals ) \n        elif operator == '~' : \n            results = self . siblings ( results , rvals ) \n        elif operator == ' ' : \n            results = self . ancestors ( results , rvals ) \n        else : \n            raise SelectorSyntaxError ( \"unrecognized operator '%s'\" % operator ) \n    elif len ( tokens ) : \n        rvals = self . selector_production ( tokens ) \n        results = self . ancestors ( results , rvals ) \n    return results "}
{"10396": "\ndef rendered_content ( self ) : \n    template = self . resolve_template ( self . template_name ) \n    if django . VERSION [ 1 ] < 8 : \n        if template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    elif template . template . name . endswith ( '.min' ) : \n        return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = jsmin . jsmin ( content ) \n    return content "}
{"10563": "\nasync def search_vndb ( self , stype , term ) : \n    fstype = \"\" \n    if stype not in [ 'v' , 'r' , 'p' , 's' , 'c' , 'g' , 'i' , 'u' ] : \n        raise VNDBBadStype ( stype ) \n    elif stype in [ 'v' , 'p' , 's' , 'c' , 'u' ] : \n        fstype = '/{}/all' . format ( stype ) \n    elif stype in [ 'g' , 'i' ] : \n        fstype = '/{}/list' . format ( stype ) \n    elif stype == 'r' : \n        fstype = '/r' \n    async with self . session . get ( self . base_url + \"{}\" . format ( fstype ) , params = { \"q\" : term } , headers = self . headers ) as response : \n        if response . status == 404 : \n            raise aiohttp . HttpBadRequest ( \"VN Not Found\" ) \n        elif 'q=' not in response . url : \n            raise VNDBOneResult ( term , response . url . rsplit ( '/' , 1 ) [ 1 ] ) \n        text = await response . text ( ) \n        if 'No Results' in text : \n            raise VNDBNoResults ( term ) \n        soup = BeautifulSoup ( text , 'lxml' ) \n        resp = await self . parse_search ( stype , soup ) \n        if resp == [ ] : \n            raise VNDBNoResults ( term ) \n        return resp "}
{"10726": "\ndef url_for ( endpoint , ** values ) : \n    appctx = _app_ctx_stack . top \n    reqctx = _request_ctx_stack . top \n    if appctx is None : \n        raise RuntimeError ( 'Attempted to generate a URL without the ' 'application context being pushed. This has to be ' 'executed when application context is available.' ) \n    if reqctx is not None : \n        url_adapter = reqctx . url_adapter \n        blueprint_name = request . blueprint \n        if not reqctx . request . _is_old_module : \n            if endpoint [ : 1 ] == '.' : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + endpoint \n                else : \n                    endpoint = endpoint [ 1 : ] \n        elif '.' not in endpoint : \n            if blueprint_name is not None : \n                endpoint = blueprint_name + '.' + endpoint \n        elif endpoint . startswith ( '.' ) : \n            endpoint = endpoint [ 1 : ] \n        external = values . pop ( '_external' , False ) \n    else : \n        url_adapter = appctx . url_adapter \n        if url_adapter is None : \n            raise RuntimeError ( 'Application was not able to create a URL ' 'adapter for request independent URL generation. ' 'You might be able to fix this by setting ' 'the SERVER_NAME config variable.' ) \n        external = values . pop ( '_external' , True ) \n    anchor = values . pop ( '_anchor' , None ) \n    method = values . pop ( '_method' , None ) \n    scheme = values . pop ( '_scheme' , None ) \n    appctx . app . inject_url_defaults ( endpoint , values ) \n    if scheme is not None : \n        if not external : \n            raise ValueError ( 'When specifying _scheme, _external must be True' ) \n        url_adapter . url_scheme = scheme \n    try : \n        rv = url_adapter . build ( endpoint , values , method = method , force_external = external ) \n    except BuildError as error : \n        values [ '_external' ] = external \n        values [ '_anchor' ] = anchor \n        values [ '_method' ] = method \n        return appctx . app . handle_url_build_error ( error , endpoint , values ) \n    if anchor is not None : \n        rv += '#' + url_quote ( anchor ) \n    return rv "}
{"10843": "\ndef method_names ( self ) : \n    for c in self . classes ( ) : \n        ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) \n        method_name = getattr ( self , 'method_name' , '' ) \n        method_regex = '' \n        if method_name : \n            if method_name . startswith ( self . method_prefix ) : \n                method_regex = re . compile ( r'^{}' . format ( method_name ) , flags = re . I ) \n            elif method_name . startswith ( \"*\" ) : \n                method_name = method_name . strip ( \"*\" ) \n                method_regex = re . compile ( r'^{}[_]{{0,1}}.*?{}' . format ( self . method_prefix , method_name ) , flags = re . I ) \n            else : \n                method_regex = re . compile ( r'^{}[_]{{0,1}}{}' . format ( self . method_prefix , method_name ) , flags = re . I ) \n        for m_name , m in ms : \n            if not m_name . startswith ( self . method_prefix ) : \n                continue \n            can_yield = True \n            if method_regex and not method_regex . match ( m_name ) : \n                can_yield = False \n            if can_yield : \n                logger . debug ( 'method: {} matches {}' . format ( m_name , method_name ) ) \n                yield c , m_name "}
{"10844": "\ndef _find_basename ( self , name , basenames , is_prefix = False ) : \n    ret = \"\" \n    fileroots = [ ( os . path . splitext ( n ) [ 0 ] , n ) for n in basenames ] \n    glob = False \n    if name . startswith ( \"*\" ) : \n        glob = True \n    name = name . strip ( \"*\" ) \n    for fileroot , basename in fileroots : \n        if name in fileroot or fileroot in name : \n            for pf in self . module_postfixes : \n                logger . debug ( 'Checking if basename {} starts with {} and ends with {}' . format ( basename , name , pf ) ) \n                if glob : \n                    if name in fileroot and fileroot . endswith ( pf ) : \n                        ret = basename \n                        break \n                elif fileroot . startswith ( name ) and fileroot . endswith ( pf ) : \n                    ret = basename \n                    break \n            if not ret : \n                for pf in self . module_prefixes : \n                    n = pf + name \n                    logger . debug ( 'Checking if basename {} starts with {}' . format ( basename , n ) ) \n                    if glob : \n                        if fileroot . startswith ( pf ) and name in fileroot : \n                            ret = basename \n                            break \n                    elif fileroot . startswith ( n ) : \n                        ret = basename \n                        break \n            if not ret : \n                if is_prefix : \n                    logger . debug ( 'Checking if basename {} starts with {}' . format ( basename , name ) ) \n                    if basename . startswith ( name ) or ( glob and name in basename ) : \n                        ret = basename \n                    else : \n                        logger . debug ( 'Checking if basename {} starts with {} and is a test module' . format ( basename , name ) ) \n                        if glob : \n                            if name in basename and self . _is_module_path ( basename ) : \n                                ret = basename \n                        elif basename . startswith ( name ) and self . _is_module_path ( basename ) : \n                            ret = basename \n            if ret : \n                logger . debug ( 'Found basename {}' . format ( ret ) ) \n                break \n    return ret "}
{"11015": "\ndef build_chain ( self , source , chain ) : \n    for group in WalkByGroup ( source , chain . order + 1 ) : \n        pre = group [ : - 1 ] \n        res = group [ - 1 ] \n        if pre not in chain . content : \n            chain . content [ pre ] = { res : 1 } \n        elif res not in chain . content [ pre ] : \n            chain . content [ pre ] [ res ] = 1 \n        else : \n            chain . content [ pre ] [ res ] += 1 \n    chain . decache ( ) "}
{"11238": "\ndef execute ( self , command , coerce_floats = True , parse_dates = False , header = False , sanitize = True , silent = False , panic = None , multi_statement = False , prepare_only = False ) : \n    if panic is None : \n        panic = self . panic \n    self . options ( \"panic\" , panic ) \n    self . options ( \"multi-statement mode\" , multi_statement , 3 ) \n    if isfile ( command ) : \n        self . options ( \"file\" , command , 2 ) \n        with open ( command , 'r' ) as f : \n            command = f . read ( ) \n    elif log . level >= VERBOSE : \n        self . options ( \"query\" , command , 2 ) \n    else : \n        self . options ( \"query\" , truncate ( command ) , 2 ) \n    if not silent and not self . silent : \n        log . info ( \"Command\" , \"Executing ...\" ) \n        log . info ( self . options ) \n    if sanitize : \n        command = prepare_statement ( command ) \n        log . debug ( \"Debug[2]\" , \"Command (sanitized): {!r}\" . format ( command ) ) \n    self . cmd . set_encoding ( ENCODER_SETTINGS_DEFAULT ) \n    return Cursor ( self . cmd , command , multi_statement = multi_statement , header = header , prepare_only = prepare_only , coerce_floats = coerce_floats , parse_dates = parse_dates , panic = panic ) "}
{"11448": "\ndef djfrontend_ga ( account = None ) : \n    if account is None : \n        account = getattr ( settings , 'DJFRONTEND_GA' , False ) \n    if account : \n        if getattr ( settings , 'TEMPLATE_DEBUG' , False ) : \n            return '' \n        elif getattr ( settings , 'DJFRONTEND_GA_SETDOMAINNAME' , False ) : \n            if getattr ( settings , 'DJFRONTEND_GA_SETALLOWLINKER' , False ) : \n                return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"require\", \"linker\");ga(\"linker:autoLink\", [\"%s\"]);ga(\"create\", \"%s\", \"auto\", {\"allowLinker\": true});ga(\"send\", \"pageview\");</script>' % ( settings . DJFRONTEND_GA_SETDOMAINNAME , account ) ) \n            else : \n                return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"create\", \"%s\", \"%s\");ga(\"send\", \"pageview\");</script>' % ( account , settings . DJFRONTEND_GA_SETDOMAINNAME ) ) \n        else : \n            return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"create\", \"%s\", \"auto\");ga(\"send\", \"pageview\");</script>' % account ) \n    else : \n        return '' "}
{"11666": "\ndef parse_file ( self , filename ) : \n    self . reset ( ) \n    self . filename = filename \n    fileinput . close ( ) \n    self . format = None \n    self . lineno = 0 \n    self . lines = [ ] \n    for line in fileinput . input ( filename ) : \n        if line [ - 1 ] == '\\012' : \n            line = line [ 0 : - 1 ] \n        if self . format == None : \n            self . process_normal_line ( line ) \n        elif self . format . end . match ( line ) : \n            self . lines . append ( line ) \n            self . add_block_lines ( ) \n        elif self . format . column . match ( line ) : \n            self . lines . append ( line ) \n        else : \n            self . add_block_lines ( ) \n            self . process_normal_line ( line ) \n    self . add_block_lines ( ) "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    lnum = 1 \n    try : \n        while True : \n            if maxlines : \n                if lnum > maxlines : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += 1 \n            elif maxlength : \n                lines . append ( string_input ( '' ) [ : maxlength ] ) \n            else : \n                lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    inum = 1 \n    try : \n        while True : \n            if maxitems : \n                if inum > maxitems : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += 1 \n            elif maxlength : \n                lines . append ( string_input ( '' ) [ : maxlength ] ) \n            else : \n                lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11840": "\ndef _Streamer__read_process ( self , path , read_size , cbuf , stop , barrier , cyclic , offset , read_skip , sync ) : \n    import tables as tb \n    h5_file = tb . open_file ( self . filename , 'r' , ** self . h5_kw_args ) \n    ary = h5_file . get_node ( path ) \n    i = offset \n    while not stop . is_set ( ) : \n        vals = ary [ i : i + read_size ] \n        if i + read_size > len ( ary ) : \n            vals = np . concatenate ( [ vals , ary [ 0 : read_size - len ( vals ) ] ] ) \n        if sync is None : \n            with cbuf . put_direct ( ) as put_ary : \n                put_ary [ : ] = vals \n        else : \n            with sync . do ( cbuf . put_direct ( ) , i , ( i + read_size ) % len ( ary ) ) as put_ary : \n                put_ary [ : ] = vals \n        i += read_skip \n        if cyclic : \n            if i >= len ( ary ) : \n                i %= len ( ary ) \n                barrier . wait ( ) \n        elif i + read_size > len ( ary ) : \n            break "}
{"11967": "\ndef _deprecated_register_to_python ( self , cd , name , converter = None ) : \n    if converter is None : \n        if isclass ( cd ) and issubclass ( cd , om . OMAny ) : \n            self . _conv_to_py [ cd ] = name \n        else : \n            raise TypeError ( 'Two-arguments form expects subclass of openmath.OMAny, found %r' % cd ) \n    elif isinstance ( cd , str ) and isinstance ( name , str ) : \n        self . _conv_sym_to_py [ ( cd , name ) ] = converter \n    else : \n        raise TypeError ( 'Three-arguments form expects string, found %r' % cd . __class__ ) "}
{"11995": "\ndef encode ( data ) : \n    if len ( data ) == 0 : \n        return data \n    if len ( data ) == 1 : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = 0 \n    repeat_count = 0 \n    MAX_LENGTH = 127 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if len ( buf ) == 0 : \n            return \n        result . append ( len ( buf ) - 1 ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256 - ( repeat_count - 1 ) ) \n        result . append ( data [ pos ] ) \n    while pos < len ( data ) - 1 : \n        current_byte = data [ pos ] \n        if data [ pos ] == data [ pos + 1 ] : \n            if state == 'RAW' : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = 1 \n            elif state == 'RLE' : \n                if repeat_count == MAX_LENGTH : \n                    finish_rle ( ) \n                    repeat_count = 0 \n                repeat_count += 1 \n        elif state == 'RLE' : \n            repeat_count += 1 \n            finish_rle ( ) \n            state = 'RAW' \n            repeat_count = 0 \n        elif state == 'RAW' : \n            if len ( buf ) == MAX_LENGTH : \n                finish_raw ( ) \n            buf . append ( current_byte ) \n        pos += 1 \n    if state == 'RAW' : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count += 1 \n        finish_rle ( ) \n    return bytes ( result ) "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return 0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return 0 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( _PROTON_VERSION < ( 0 , 10 ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        elif self . _pn_sasl . outcome is not None : \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12355": "\ndef infix_to_postfix ( nodes , * , recurse_types = None ) : \n    output = [ ] \n    operators = [ ] \n    for node in nodes : \n        if isinstance ( node , OperatorNode ) : \n            cmp_operator = node . operator \n            while operators : \n                current_operator = operators [ - 1 ] . operator \n                if current_operator . precedence > cmp_operator . precedence or current_operator . precedence == cmp_operator . precedence and current_operator . association == Association . left : \n                    output . append ( operators . pop ( ) ) \n                else : \n                    break \n            operators . append ( node ) \n        elif recurse_types is not None and node . node_type in recurse_types : \n            output . extend ( infix_to_postfix ( node . children , recurse_types = recurse_types ) ) \n        else : \n            output . append ( node ) \n    return output + list ( reversed ( operators ) ) "}
{"12570": "\ndef loadcommon ( sources , load_task , asynchronous = True , predicate = None , task_args = None , task_kwargs = None ) : \n    task_args = tuple ( ) if task_args is None else task_args \n    task_kwargs = dict ( ) if task_kwargs is None else task_kwargs \n    click . echo ( 'Loading dumps started.' ) \n    for idx , source in enumerate ( sources , 1 ) : \n        click . echo ( 'Opening dump file {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        with click . progressbar ( data ) as data_bar : \n            for d in data_bar : \n                if predicate is not None : \n                    if predicate ( d ) : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply ( throw = True ) \n                        click . echo ( \"Loaded a single record.\" ) \n                        return \n                elif asynchronous : \n                    load_task . s ( d , * task_args , ** task_kwargs ) . apply_async ( ) \n                else : \n                    load_task . s ( d , * task_args , ** task_kwargs ) . apply ( throw = True ) "}
{"12686": "\ndef parse_response ( self , response ) : \n    payload = None \n    try : \n        if isinstance ( response . json , collections . Callable ) : \n            payload = response . json ( ) \n        else : \n            payload = response . json \n    except ValueError : \n        payload = response . content \n    if not self . _raise_errors : \n        return payload \n    elif response . status_code == 401 : \n        raise AuthenticationError ( payload [ 'message' ] ) \n    elif response . status_code == 500 : \n        raise ServerError ( payload [ 'message' ] ) \n    elif isinstance ( payload , dict ) and not payload [ 'success' ] : \n        raise APIError ( payload [ 'message' ] ) \n    else : \n        return payload "}
{"12825": "\ndef make_pretty ( elem , depth = 0 , indent = '  ' ) : \n    depth += 1 \n    updated_child_list = [ ] \n    updated_child_ix = 0 \n    for child in elem . xml_children : \n        if isinstance ( child , element ) : \n            if updated_child_ix % 2 : \n                updated_child_list . append ( child ) \n                updated_child_ix += 1 \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_list . append ( child ) \n                updated_child_ix += 2 \n            make_pretty ( child , depth ) \n        elif child . xml_value . strip ( ) : \n            updated_child_list . append ( child ) \n            updated_child_ix += 1 \n        else : \n            new_text = text ( '\\n' + indent * depth , elem ) \n            updated_child_list . append ( new_text ) \n            updated_child_ix += 1 \n    if not ( updated_child_ix % 2 ) : \n        new_text = text ( '\\n' + indent * ( depth - 1 ) , elem ) \n        updated_child_list . append ( new_text ) \n    elem . xml_children = updated_child_list \n    return elem "}
{"13133": "\ndef _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = False ) ) : \n    if scale is None : \n        scale = Scale ( ) \n    xfieldtype = xfield [ 1 ] \n    yfieldtype = yfield [ 1 ] \n    x_options = None \n    if len ( xfield ) > 2 : \n        x_options = xfield [ 2 ] \n    y_options = None \n    if len ( yfield ) > 2 : \n        y_options = yfield [ 2 ] \n    if time_unit is not None : \n        if x_options is None : \n            xencode = X ( xfieldtype , timeUnit = time_unit ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , timeUnit = time_unit , scale = scale ) \n    elif x_options is None : \n        xencode = X ( xfieldtype ) \n    else : \n        xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , scale = scale ) \n    if y_options is None : \n        yencode = Y ( yfieldtype , scale = scale ) \n    else : \n        yencode = Y ( yfieldtype , axis = Axis ( ** y_options ) , scale = scale ) \n    return xencode , yencode "}
{"13258": "\ndef unpad ( padded_data , block_size , style = 'pkcs7' ) : \n    pdata_len = len ( padded_data ) \n    if pdata_len % block_size : \n        raise ValueError ( \"Input data is not padded\" ) \n    if style in ( 'pkcs7' , 'x923' ) : \n        padding_len = bord ( padded_data [ - 1 ] ) \n        if padding_len < 1 or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if style == 'pkcs7' : \n            if padded_data [ - padding_len : ] != bchr ( padding_len ) * padding_len : \n                raise ValueError ( \"PKCS#7 padding is incorrect.\" ) \n        elif padded_data [ - padding_len : - 1 ] != bchr ( 0 ) * ( padding_len - 1 ) : \n            raise ValueError ( \"ANSI X.923 padding is incorrect.\" ) \n    elif style == 'iso7816' : \n        padding_len = pdata_len - padded_data . rfind ( bchr ( 128 ) ) \n        if padding_len < 1 or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if padding_len > 1 and padded_data [ 1 - padding_len : ] != bchr ( 0 ) * ( padding_len - 1 ) : \n            raise ValueError ( \"ISO 7816-4 padding is incorrect.\" ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return padded_data [ : - padding_len ] "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ 1 : ] \n    if takes_context : \n        if 'context' in params [ : 1 ] : \n            params = params [ 1 : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = False \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if len ( bits ) > 1 and bits [ - 2 ] == 'as' : \n        output_var = bits [ - 1 ] \n        if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > 0 : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            elif kw in handled_params : \n                raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                kwargs [ str ( kw ) ] = var \n                kwarg_found = True \n                handled_params . append ( kw ) \n        elif kwarg_found : \n            raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n        else : \n            args . append ( bit ) \n            try : \n                handled_params . append ( unhandled_params . pop ( 0 ) ) \n            except IndexError : \n                if not takes_var_args : \n                    raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if len ( unhandled_params ) == 1 : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ 0 ] ) ) \n    elif len ( unhandled_params ) > 1 : \n        raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13571": "\ndef wait_for_kernel ( self , timeout = None ) : \n    tic = time . time ( ) \n    self . km . hb_channel . unpause ( ) \n    while True : \n        self . run_cell ( '1' , False ) \n        if self . km . hb_channel . is_beating ( ) : \n            break \n        elif timeout is not None and ( time . time ( ) - tic ) > timeout : \n            return False \n    return True "}
{"13617": "\ndef _render_expression ( self , check ) : \n    expressions = [ ] \n    args = [ ] \n    skeys = set ( check . keys ( ) ) \n    skeys . difference_update ( set ( self . _keys ) ) \n    skeys . difference_update ( set ( [ 'buffers' , 'result_buffers' ] ) ) \n    if skeys : \n        raise KeyError ( \"Illegal testing key(s): %s\" % skeys ) \n    for name , sub_check in check . iteritems ( ) : \n        if isinstance ( sub_check , dict ) : \n            for test , value in sub_check . iteritems ( ) : \n                try : \n                    op = operators [ test ] \n                except KeyError : \n                    raise KeyError ( \"Unsupported operator: %r\" % test ) \n                if isinstance ( op , tuple ) : \n                    op , join = op \n                if value is None and op in null_operators : \n                    expr = \"%s %s\" % ( name , null_operators [ op ] ) \n                else : \n                    expr = \"%s %s ?\" % ( name , op ) \n                    if isinstance ( value , ( tuple , list ) ) : \n                        if op in null_operators and any ( [ v is None for v in value ] ) : \n                            raise ValueError ( \"Cannot use %r test with NULL values on SQLite backend\" % test ) \n                        expr = '( %s )' % ( join . join ( [ expr ] * len ( value ) ) ) \n                        args . extend ( value ) \n                    else : \n                        args . append ( value ) \n                expressions . append ( expr ) \n        elif sub_check is None : \n            expressions . append ( \"%s IS NULL\" % name ) \n        else : \n            expressions . append ( \"%s = ?\" % name ) \n            args . append ( sub_check ) \n    expr = \" AND \" . join ( expressions ) \n    return expr , args "}
{"13771": "\ndef pretty ( self , obj ) : \n    obj_id = id ( obj ) \n    cycle = obj_id in self . stack \n    self . stack . append ( obj_id ) \n    self . begin_group ( ) \n    try : \n        obj_class = getattr ( obj , '__class__' , None ) or type ( obj ) \n        try : \n            printer = self . singleton_pprinters [ obj_id ] \n        except ( TypeError , KeyError ) : \n            pass \n        else : \n            return printer ( obj , self , cycle ) \n        for cls in _get_mro ( obj_class ) : \n            if cls in self . type_pprinters : \n                return self . type_pprinters [ cls ] ( obj , self , cycle ) \n            else : \n                printer = self . _in_deferred_types ( cls ) \n                if printer is not None : \n                    return printer ( obj , self , cycle ) \n                elif '_repr_pretty_' in obj_class . __dict__ : \n                    meth = obj_class . _repr_pretty_ \n                    if callable ( meth ) : \n                        return meth ( obj , self , cycle ) \n        return _default_pprint ( obj , self , cycle ) \n    finally : \n        self . end_group ( ) \n        self . stack . pop ( ) "}
{"13833": "\ndef task_with_callable ( the_callable , label = None , schedule = DEFAULT_SCHEDULE , userdata = None , pk_override = None ) : \n    task = Task ( ) \n    if isinstance ( the_callable , str ) : \n        if pk_override is not None : \n            components = the_callable . split ( '.' ) \n            info = dict ( func_type = 'instancemethod' , module_name = '.' . join ( components [ : - 2 ] ) , class_name = components [ - 2 ] , class_path = '.' . join ( components [ : - 1 ] ) , model_pk = pk_override , func_name = components [ - 1 ] , func_path = the_callable , ) \n            task . funcinfo = info \n        else : \n            task . funcinfo = get_func_info ( func_from_string ( the_callable ) ) \n    else : \n        task . funcinfo = get_func_info ( the_callable ) \n    if label is None : \n        task . label = task . funcinfo [ 'func_path' ] \n    else : \n        task . label = label \n    task . schedule = schedule \n    if not croniter . is_valid ( task . schedule ) : \n        raise ValueError ( f\"Cron schedule {task.schedule} is not valid\" ) \n    if userdata is None : \n        task . userdata = dict ( ) \n    elif isinstance ( userdata , dict ) : \n        task . userdata = userdata \n    else : \n        raise ValueError ( \"Userdata must be a dictionary of JSON-serializable data\" ) \n    return task "}
{"13863": "\ndef base_launch_kernel ( code , fname , stdin = None , stdout = None , stderr = None , executable = None , independent = False , extra_arguments = [ ] , cwd = None ) : \n    if executable is None : \n        executable = sys . executable \n    arguments = [ executable , '-c' , code , '-f' , fname ] \n    arguments . extend ( extra_arguments ) \n    redirect_in = True \n    _stdin = PIPE if stdin is None else stdin \n    redirect_out = sys . executable . endswith ( 'pythonw.exe' ) \n    if redirect_out : \n        _stdout = PIPE if stdout is None else stdout \n        _stderr = PIPE if stderr is None else stderr \n    else : \n        _stdout , _stderr = stdout , stderr \n    if sys . platform == 'win32' : \n        interrupt_event = ParentPollerWindows . create_interrupt_event ( ) \n        arguments += [ '--interrupt=%i' % interrupt_event ] \n        if executable . endswith ( 'pythonw.exe' ) : \n            if stdout is None : \n                arguments . append ( '--no-stdout' ) \n            if stderr is None : \n                arguments . append ( '--no-stderr' ) \n        if independent : \n            proc = Popen ( arguments , creationflags = 512 , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        else : \n            try : \n                from _winapi import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            except : \n                from _subprocess import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            pid = GetCurrentProcess ( ) \n            handle = DuplicateHandle ( pid , pid , pid , 0 , True , DUPLICATE_SAME_ACCESS ) \n            proc = Popen ( arguments + [ '--parent=%i' % int ( handle ) ] , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        proc . win32_interrupt_event = interrupt_event \n    elif independent : \n        proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    else : \n        proc = Popen ( arguments + [ '--parent=1' ] , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    if redirect_in : \n        if stdin is None : \n            proc . stdin . close ( ) \n    if redirect_out : \n        if stdout is None : \n            proc . stdout . close ( ) \n        if stderr is None : \n            proc . stderr . close ( ) \n    return proc "}
{"13989": "\ndef file_matches ( self , text ) : \n    if text . startswith ( '!' ) : \n        text = text [ 1 : ] \n        text_prefix = '!' \n    else : \n        text_prefix = '' \n    text_until_cursor = self . text_until_cursor \n    open_quotes = has_open_quotes ( text_until_cursor ) \n    if '(' in text_until_cursor or '[' in text_until_cursor : \n        lsplit = text \n    else : \n        try : \n            lsplit = arg_split ( text_until_cursor ) [ - 1 ] \n        except ValueError : \n            if open_quotes : \n                lsplit = text_until_cursor . split ( open_quotes ) [ - 1 ] \n            else : \n                return [ ] \n        except IndexError : \n            lsplit = \"\" \n    if not open_quotes and lsplit != protect_filename ( lsplit ) : \n        has_protectables = True \n        text0 , text = text , lsplit \n    else : \n        has_protectables = False \n        text = os . path . expanduser ( text ) \n    if text == \"\" : \n        return [ text_prefix + protect_filename ( f ) for f in self . glob ( \"*\" ) ] \n    m0 = self . clean_glob ( text . replace ( '\\\\' , '' ) ) \n    if has_protectables : \n        len_lsplit = len ( lsplit ) \n        matches = [ text_prefix + text0 + protect_filename ( f [ len_lsplit : ] ) for f in m0 ] \n    elif open_quotes : \n        matches = m0 \n    else : \n        matches = [ text_prefix + protect_filename ( f ) for f in m0 ] \n    matches = [ x + '/' if os . path . isdir ( x ) else x for x in matches ] \n    return matches "}
{"13993": "\ndef complete ( self , text = None , line_buffer = None , cursor_pos = None ) : \n    if cursor_pos is None : \n        cursor_pos = len ( line_buffer ) if text is None else len ( text ) \n    if not text : \n        text = self . splitter . split_line ( line_buffer , cursor_pos ) \n    if line_buffer is None : \n        line_buffer = text \n    self . line_buffer = line_buffer \n    self . text_until_cursor = self . line_buffer [ : cursor_pos ] \n    self . matches [ : ] = [ ] \n    custom_res = self . dispatch_custom_completer ( text ) \n    if custom_res is not None : \n        self . matches = custom_res \n    elif self . merge_completions : \n        self . matches = [ ] \n        for matcher in self . matchers : \n            try : \n                self . matches . extend ( matcher ( text ) ) \n            except : \n                sys . excepthook ( * sys . exc_info ( ) ) \n    else : \n        for matcher in self . matchers : \n            self . matches = matcher ( text ) \n            if self . matches : \n                break \n    self . matches = sorted ( set ( self . matches ) ) \n    return text , self . matches "}
{"14199": "\ndef run_task ( message ) : \n    task = Task . objects . get ( pk = message [ 'id' ] ) \n    if task . allow_overlap : \n        task . run ( message ) \n    elif not task . running : \n        task . running = True \n        task . save ( ) \n        try : \n            task . run ( message ) \n        finally : \n            task . running = False \n            task . save ( ) "}
{"14401": "\ndef process_iter ( ) : \n    def add ( pid ) : \n        proc = Process ( pid ) \n        _pmap [ proc . pid ] = proc \n        return proc \n    def remove ( pid ) : \n        _pmap . pop ( pid , None ) \n    a = set ( get_pid_list ( ) ) \n    b = set ( _pmap . keys ( ) ) \n    new_pids = a - b \n    gone_pids = b - a \n    for pid in gone_pids : \n        remove ( pid ) \n    for pid , proc in sorted ( list ( _pmap . items ( ) ) + list ( dict . fromkeys ( new_pids ) . items ( ) ) ) : \n        try : \n            if proc is None : \n                yield add ( pid ) \n            elif proc . is_running ( ) : \n                yield proc \n            else : \n                yield add ( pid ) \n        except NoSuchProcess : \n            remove ( pid ) \n        except AccessDenied : \n            yield proc "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = True \n    if esc == ESC_QUOTE : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    elif esc == ESC_QUOTE2 : \n        newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n    elif esc == ESC_PAREN : \n        newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n    else : \n        if force_auto : \n            do_rewrite = not the_rest . startswith ( '(' ) \n        elif not the_rest : \n            do_rewrite = ( self . shell . autocall >= 2 ) \n        elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n            do_rewrite = False \n        else : \n            do_rewrite = True \n        if do_rewrite : \n            if the_rest . endswith ( ';' ) : \n                newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) \n            else : \n                newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n        else : \n            normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n            return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14480": "\ndef execute ( self , source = None , hidden = False , interactive = False ) : \n    if source is None : \n        source = self . input_buffer \n        if not hidden : \n            source += '\\n' \n    elif not hidden : \n        self . input_buffer = source \n    complete = self . _is_complete ( source , interactive ) \n    if hidden : \n        if complete : \n            self . _execute ( source , hidden ) \n        else : \n            error = 'Incomplete noninteractive input: \"%s\"' \n            raise RuntimeError ( error % source ) \n    elif complete : \n        self . _append_plain_text ( '\\n' ) \n        self . _input_buffer_executing = self . input_buffer \n        self . _executing = True \n        self . _prompt_finished ( ) \n        self . _control . document ( ) . setMaximumBlockCount ( self . buffer_size ) \n        self . _control . setUndoRedoEnabled ( False ) \n        self . _execute ( source , hidden ) \n    else : \n        cursor = self . _get_end_cursor ( ) \n        cursor . beginEditBlock ( ) \n        cursor . insertText ( '\\n' ) \n        self . _insert_continuation_prompt ( cursor ) \n        cursor . endEditBlock ( ) \n        self . _control . moveCursor ( QtGui . QTextCursor . End ) \n    return complete "}
{"14516": "\ndef _show_prompt ( self , prompt = None , html = False , newline = True ) : \n    cursor = self . _get_end_cursor ( ) \n    self . _append_before_prompt_pos = cursor . position ( ) \n    if newline and cursor . position ( ) > 0 : \n        cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor ) \n        if cursor . selection ( ) . toPlainText ( ) != '\\n' : \n            self . _append_plain_text ( '\\n' ) \n    self . _append_plain_text ( self . _prompt_sep ) \n    if prompt is None : \n        if self . _prompt_html is None : \n            self . _append_plain_text ( self . _prompt ) \n        else : \n            self . _append_html ( self . _prompt_html ) \n    elif html : \n        self . _prompt = self . _append_html_fetching_plain_text ( prompt ) \n        self . _prompt_html = prompt \n    else : \n        self . _append_plain_text ( prompt ) \n        self . _prompt = prompt \n        self . _prompt_html = None \n    self . _prompt_pos = self . _get_end_cursor ( ) . position ( ) \n    self . _prompt_started ( ) "}
{"14664": "\ndef validateAttrib ( self , method , cls = None ) : \n    any = False \n    for group in self . attribs : \n        match = True \n        for key , value in group : \n            attr = get_method_attr ( method , cls , key ) \n            if callable ( value ) : \n                if not value ( key , method , cls ) : \n                    match = False \n                    break \n            elif value is True : \n                if not bool ( attr ) : \n                    match = False \n                    break \n            elif value is False : \n                if bool ( attr ) : \n                    match = False \n                    break \n            elif type ( attr ) in ( list , tuple ) : \n                if not str ( value ) . lower ( ) in [ str ( x ) . lower ( ) for x in attr ] : \n                    match = False \n                    break \n            elif ( value != attr and str ( value ) . lower ( ) != str ( attr ) . lower ( ) ) : \n                match = False \n                break \n        any = any or match \n    if any : \n        return None \n    return False "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255 , 255 , 255 ) , size = 12 , antialias = False , center = False ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2 ) \n        y = y - ( height // 2 ) \n    for ystep in range ( height ) : \n        for xstep in range ( width ) : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if normalisation != 0 : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            elif normalisation > .5 : \n                set_color ( x + xstep , y + ystep , color ) "}
