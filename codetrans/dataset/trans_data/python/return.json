{"20": "\ndef parse_host ( host ) : \n    if re . match ( r'^(\\d+)$' , host ) is not None : \n        return_identifier_0 = ( \"0.0.0.0\" , int ( host ) ) \n        return return_identifier_0 \n    if re . match ( r'^(\\w+)://' , host ) is None : \n        host = \"//\" + host \n    o = parse . urlparse ( host ) \n    hostname = o . hostname or \"0.0.0.0\" \n    port = o . port or 0 \n    return_identifier_1 = ( hostname , port ) \n    return return_identifier_1 "}
{"25": "\ndef has_task ( self , task_instance ) : \n    if task_instance . key in self . queued_tasks or task_instance . key in self . running : \n        return_identifier_0 = True \n        return return_identifier_0 "}
{"86": "\ndef does_collection_exist ( self , collection_name , database_name = None ) : \n    if collection_name is None : \n        raise AirflowBadRequest ( \"Collection name cannot be None.\" ) \n    existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) \n    if len ( existing_container ) == 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"88": "\ndef does_database_exist ( self , database_name ) : \n    if database_name is None : \n        raise AirflowBadRequest ( \"Database name cannot be None.\" ) \n    existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) \n    if len ( existing_database ) == 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"109": "\ndef list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : \n    if include_examples is None : \n        include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) \n    file_paths = [ ] \n    if directory is None : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    elif os . path . isfile ( directory ) : \n        return_identifier_1 = [ directory ] \n        return return_identifier_1 \n    elif os . path . isdir ( directory ) : \n        patterns_by_dir = { } \n        for root , dirs , files in os . walk ( directory , followlinks = True ) : \n            patterns = patterns_by_dir . get ( root , [ ] ) \n            ignore_file = os . path . join ( root , '.airflowignore' ) \n            if os . path . isfile ( ignore_file ) : \n                with open ( ignore_file , 'r' ) as f : \n                    patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] \n            dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] \n            for d in dirs : \n                patterns_by_dir [ os . path . join ( root , d ) ] = patterns \n            for f in files : \n                try : \n                    file_path = os . path . join ( root , f ) \n                    if not os . path . isfile ( file_path ) : \n                        continue \n                    mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) \n                    if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : \n                        continue \n                    if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : \n                        continue \n                    might_contain_dag = True \n                    if safe_mode and not zipfile . is_zipfile ( file_path ) : \n                        with open ( file_path , 'rb' ) as fp : \n                            content = fp . read ( ) \n                            might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) \n                    if not might_contain_dag : \n                        continue \n                    file_paths . append ( file_path ) \n                except Exception : \n                    log = LoggingMixin ( ) . log \n                    log . exception ( \"Error while examining %s\" , f ) \n    if include_examples : \n        import airflow . example_dags \n        example_dag_folder = airflow . example_dags . __path__ [ 0 ] \n        file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) \n    return file_paths "}
{"188": "\ndef write_batch_data ( self , items ) : \n    dynamodb_conn = self . get_conn ( ) \n    try : \n        table = dynamodb_conn . Table ( self . table_name ) \n        with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : \n            for item in items : \n                batch . put_item ( Item = item ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except Exception as general_error : \n        raise AirflowException ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) ) "}
{"201": "\ndef get_state_exitcode_details ( self , resource_group , name ) : \n    current_state = self . _get_instance_view ( resource_group , name ) . current_state \n    return_identifier_0 = ( current_state . state , current_state . exit_code , current_state . detail_status ) \n    return return_identifier_0 "}
{"205": "\ndef exists ( self , resource_group , name ) : \n    for container in self . connection . container_groups . list_by_resource_group ( resource_group ) : \n        if container . name == name : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"208": "\ndef poke ( self , context ) : \n    self . log . info ( 'RedisPubSubSensor checking for message on channels: %s' , self . channels ) \n    message = self . pubsub . get_message ( ) \n    self . log . info ( 'Message %s from channel %s' , message , self . channels ) \n    if message and message [ 'type' ] == 'message' : \n        context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) \n        self . pubsub . unsubscribe ( self . channels ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"216": "\ndef jenkins_request_with_headers ( jenkins_server , req ) : \n    try : \n        response = jenkins_server . jenkins_request ( req ) \n        response_body = response . content \n        response_headers = response . headers \n        if response_body is None : \n            raise jenkins . EmptyResponseException ( \"Error communicating with server[%s]: \" \"empty response\" % jenkins_server . server ) \n        return_identifier_0 = { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } \n        return return_identifier_0 \n    except HTTPError as e : \n        if e . code in [ 401 , 403 , 500 ] : \n            raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) \n        elif e . code == 404 : \n            raise jenkins . NotFoundException ( 'Requested item could not be found' ) \n        else : \n            raise \n    except socket . timeout as e : \n        raise jenkins . TimeoutException ( 'Error in request: %s' % e ) \n    except URLError as e : \n        if str ( e . reason ) == \"timed out\" : \n            raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) \n        raise JenkinsException ( 'Error in request: %s' % e . reason ) "}
{"238": "\ndef are_dependents_done ( self , session = None ) : \n    task = self . task \n    if not task . downstream_task_ids : \n        return_identifier_0 = True \n        return return_identifier_0 \n    ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance . execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) \n    count = ti [ 0 ] [ 0 ] \n    return count == len ( task . downstream_task_ids ) "}
{"241": "\ndef pool_full ( self , session ) : \n    if not self . task . pool : \n        return_identifier_0 = False \n        return return_identifier_0 \n    pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) \n    if not pool : \n        return_identifier_1 = False \n        return return_identifier_1 \n    open_slots = pool . open_slots ( session = session ) \n    return open_slots <= 0 "}
{"255": "\ndef check_for_bucket ( self , bucket_name ) : \n    try : \n        self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except ClientError as e : \n        self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"260": "\ndef check_for_key ( self , key , bucket_name = None ) : \n    if not bucket_name : \n        ( bucket_name , key ) = self . parse_s3_url ( key ) \n    try : \n        self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except ClientError as e : \n        self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"278": "\ndef is_valid_plugin ( plugin_obj , existing_plugins ) : \n    if ( inspect . isclass ( plugin_obj ) and issubclass ( plugin_obj , AirflowPlugin ) and ( plugin_obj is not AirflowPlugin ) ) : \n        plugin_obj . validate ( ) \n        return plugin_obj not in existing_plugins \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"281": "\ndef check_for_file ( self , file_path ) : \n    try : \n        files = self . connection . glob ( file_path , details = False , invalidate_cache = True ) \n        return len ( files ) == 1 \n    except FileNotFoundError : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"338": "\ndef is_updated_after ( self , bucket_name , object_name , ts ) : \n    client = self . get_conn ( ) \n    bucket = storage . Bucket ( client = client , name = bucket_name ) \n    blob = bucket . get_blob ( blob_name = object_name ) \n    blob . reload ( ) \n    blob_update_time = blob . updated \n    if blob_update_time is not None : \n        import dateutil . tz \n        if not ts . tzinfo : \n            ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) \n        self . log . info ( \"Verify object date: %s > %s\" , blob_update_time , ts ) \n        if blob_update_time > ts : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"346": "\ndef secondary_training_status_changed ( current_job_description , prev_job_description ) : \n    current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) \n    if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None \n    last_message = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and len ( prev_job_secondary_status_transitions ) > 0 else '' \n    message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] \n    return message != last_message "}
{"347": "\ndef secondary_training_status_message ( job_description , prev_description ) : \n    if job_description is None or job_description . get ( 'SecondaryStatusTransitions' ) is None or len ( job_description . get ( 'SecondaryStatusTransitions' ) ) == 0 : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    prev_description_secondary_transitions = prev_description . get ( 'SecondaryStatusTransitions' ) if prev_description is not None else None \n    prev_transitions_num = len ( prev_description [ 'SecondaryStatusTransitions' ] ) if prev_description_secondary_transitions is not None else 0 \n    current_transitions = job_description [ 'SecondaryStatusTransitions' ] \n    transitions_to_print = current_transitions [ - 1 : ] if len ( current_transitions ) == prev_transitions_num else current_transitions [ prev_transitions_num - len ( current_transitions ) : ] \n    status_strs = [ ] \n    for transition in transitions_to_print : \n        message = transition [ 'StatusMessage' ] \n        time_str = timezone . convert_to_utc ( job_description [ 'LastModifiedTime' ] ) . strftime ( '%Y-%m-%d %H:%M:%S' ) \n        status_strs . append ( '{} {} - {}' . format ( time_str , transition [ 'Status' ] , message ) ) \n    return '\\n' . join ( status_strs ) "}
{"350": "\ndef check_s3_url ( self , s3url ) : \n    bucket , key = S3Hook . parse_s3_url ( s3url ) \n    if not self . s3_hook . check_for_bucket ( bucket_name = bucket ) : \n        raise AirflowException ( \"The input S3 Bucket {} does not exist \" . format ( bucket ) ) \n    if key and not self . s3_hook . check_for_key ( key = key , bucket_name = bucket ) and not self . s3_hook . check_for_prefix ( prefix = key , bucket_name = bucket , delimiter = '/' ) : \n        raise AirflowException ( \"The input S3 Key \" \"or Prefix {} does not exist in the Bucket {}\" . format ( s3url , bucket ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"368": "\ndef _prepare_hiveconf ( d ) : \n    if not d : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    return as_flattened_list ( zip ( [ \"-hiveconf\" ] * len ( d ) , [ \"{}={}\" . format ( k , v ) for k , v in d . items ( ) ] ) ) "}
{"373": "\ndef table_exists ( self , table_name , db = 'default' ) : \n    try : \n        self . get_table ( table_name , db ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except Exception : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"388": "\ndef table_exists ( self , project_id , dataset_id , table_id ) : \n    service = self . get_service ( ) \n    try : \n        service . tables ( ) . get ( projectId = project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except HttpError as e : \n        if e . resp [ 'status' ] == '404' : \n            return_identifier_1 = False \n            return return_identifier_1 \n        raise "}
{"439": "\ndef cluster_status ( self , cluster_identifier ) : \n    conn = self . get_conn ( ) \n    try : \n        response = conn . describe_clusters ( ClusterIdentifier = cluster_identifier ) [ 'Clusters' ] \n        return response [ 0 ] [ 'ClusterStatus' ] if response else None \n    except conn . exceptions . ClusterNotFoundFault : \n        return_identifier_0 = 'cluster_not_found' \n        return return_identifier_0 "}
{"448": "\ndef execute ( self , context ) : \n    s3_conn = S3Hook ( self . s3_conn_id ) \n    if self . is_pipeline : \n        results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , mongo_db = self . mongo_db ) \n    else : \n        results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) \n    docs_str = self . _stringify ( self . transform ( results ) ) \n    s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"468": "\ndef _get_init_containers ( self ) : \n    if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] \n    if self . kube_config . git_user : \n        init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) \n    if self . kube_config . git_password : \n        init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) \n    volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : False } ] \n    if self . kube_config . git_ssh_key_secret_name : \n        volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) \n        init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) \n    if self . kube_config . git_ssh_known_hosts_configmap_name : \n        volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) \n        init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) \n    else : \n        init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'false' } ) \n    return_identifier_1 = [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533 } , 'env' : init_environment , 'volumeMounts' : volume_mounts } ] \n    return return_identifier_1 "}
{"476": "\ndef done ( self ) : \n    if self . _process is None : \n        raise AirflowException ( \"Tried to see if it's done before starting!\" ) \n    if self . _done : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if self . _result_queue and not self . _result_queue . empty ( ) : \n        self . _result = self . _result_queue . get_nowait ( ) \n        self . _done = True \n        self . log . debug ( \"Waiting for %s\" , self . _process ) \n        self . _process . join ( ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    if self . _result_queue and not self . _process . is_alive ( ) : \n        self . _done = True \n        if not self . _result_queue . empty ( ) : \n            self . _result = self . _result_queue . get_nowait ( ) \n        self . log . debug ( \"Waiting for %s\" , self . _process ) \n        self . _process . join ( ) \n        return_identifier_2 = True \n        return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"482": "\ndef _change_state_for_executable_task_instances ( self , task_instances , acceptable_states , session = None ) : \n    if len ( task_instances ) == 0 : \n        session . commit ( ) \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    TI = models . TaskInstance \n    filter_for_ti_state_change = ( [ and_ ( TI . dag_id == ti . dag_id , TI . task_id == ti . task_id , TI . execution_date == ti . execution_date ) for ti in task_instances ] ) \n    ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) \n    if None in acceptable_states : \n        ti_query = ti_query . filter ( or_ ( TI . state == None , TI . state . in_ ( acceptable_states ) ) ) \n    else : \n        ti_query = ti_query . filter ( TI . state . in_ ( acceptable_states ) ) \n    tis_to_set_to_queued = ( ti_query . with_for_update ( ) . all ( ) ) \n    if len ( tis_to_set_to_queued ) == 0 : \n        self . log . info ( \"No tasks were able to have their state changed to queued.\" ) \n        session . commit ( ) \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    for task_instance in tis_to_set_to_queued : \n        task_instance . state = State . QUEUED \n        task_instance . queued_dttm = ( timezone . utcnow ( ) if not task_instance . queued_dttm else task_instance . queued_dttm ) \n        session . merge ( task_instance ) \n    simple_task_instances = [ SimpleTaskInstance ( ti ) for ti in tis_to_set_to_queued ] \n    task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_queued ] ) \n    session . commit ( ) \n    self . log . info ( \"Setting the following %s tasks to queued state:\\n\\t%s\" , len ( tis_to_set_to_queued ) , task_instance_str ) \n    return simple_task_instances "}
{"487": "\ndef process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : \n    self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) \n    simple_dags = [ ] \n    try : \n        dagbag = models . DagBag ( file_path , include_examples = False ) \n    except Exception : \n        self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) \n        Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    if len ( dagbag . dags ) > 0 : \n        self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) \n    else : \n        self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) \n        self . update_import_errors ( session , dagbag ) \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    for dag in dagbag . dags . values ( ) : \n        dag . sync_to_db ( ) \n    paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] \n    for dag_id in dagbag . dags : \n        if dag_id not in paused_dag_ids : \n            dag = dagbag . get_dag ( dag_id ) \n            pickle_id = None \n            if pickle_dags : \n                pickle_id = dag . pickle ( session ) . id \n            simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) \n    if len ( self . dag_ids ) > 0 : \n        dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] \n    else : \n        dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] \n    ti_keys_to_schedule = [ ] \n    self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) \n    for ti_key in ti_keys_to_schedule : \n        dag = dagbag . dags [ ti_key [ 0 ] ] \n        task = dag . get_task ( ti_key [ 1 ] ) \n        ti = models . TaskInstance ( task , ti_key [ 2 ] ) \n        ti . refresh_from_db ( session = session , lock_for_update = True ) \n        dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) \n        if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : \n            ti . state = State . SCHEDULED \n        self . log . info ( \"Creating / updating %s in ORM\" , ti ) \n        session . merge ( ti ) \n    session . commit ( ) \n    try : \n        self . update_import_errors ( session , dagbag ) \n    except Exception : \n        self . log . exception ( \"Error logging import errors!\" ) \n    try : \n        dagbag . kill_zombies ( zombies ) \n    except Exception : \n        self . log . exception ( \"Error killing zombies!\" ) \n    return simple_dags "}
{"507": "\ndef prepare_additional_parameters ( additional_properties , language_hints , web_detection_params ) : \n    if language_hints is None and web_detection_params is None : \n        return additional_properties \n    if additional_properties is None : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    merged_additional_parameters = deepcopy ( additional_properties ) \n    if 'image_context' not in merged_additional_parameters : \n        merged_additional_parameters [ 'image_context' ] = { } \n    merged_additional_parameters [ 'image_context' ] [ 'language_hints' ] = merged_additional_parameters [ 'image_context' ] . get ( 'language_hints' , language_hints ) \n    merged_additional_parameters [ 'image_context' ] [ 'web_detection_params' ] = merged_additional_parameters [ 'image_context' ] . get ( 'web_detection_params' , web_detection_params ) \n    return merged_additional_parameters "}
{"510": "\ndef record_exists ( self , table , keys ) : \n    keyspace = self . keyspace \n    if '.' in table : \n        keyspace , table = table . split ( '.' , 1 ) \n    ks = \" AND \" . join ( \"{}=%({})s\" . format ( key , key ) for key in keys . keys ( ) ) \n    cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\" . format ( keyspace = keyspace , table = table , keys = ks ) \n    try : \n        rs = self . get_conn ( ) . execute ( cql , keys ) \n        return rs . one ( ) is not None \n    except Exception : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > 0 : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if num > 0 : \n                    start_date += delta \n                else : \n                    start_date -= delta \n    return sorted ( dates ) "}
{"533": "\ndef _has_perm ( self , permission_name , view_menu_name ) : \n    if hasattr ( self , 'perms' ) : \n        if ( permission_name , view_menu_name ) in self . perms : \n            return_identifier_0 = True \n            return return_identifier_0 \n    self . _get_and_cache_perms ( ) \n    return ( permission_name , view_menu_name ) in self . perms "}
{"542": "\ndef poke ( self , context ) : \n    sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) \n    sqs_conn = sqs_hook . get_conn ( ) \n    self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) \n    messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) \n    self . log . info ( \"reveived message %s\" , str ( messages ) ) \n    if 'Messages' in messages and len ( messages [ 'Messages' ] ) > 0 : \n        entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] \n        result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) \n        if 'Successful' in result : \n            context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"549": "\ndef _convert_date_to_dict ( field_date ) : \n    return_identifier_0 = { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year } \n    return return_identifier_0 "}
{"550": "\ndef _convert_time_to_dict ( time ) : \n    return_identifier_0 = { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second } \n    return return_identifier_0 "}
{"565": "\ndef unfinished ( cls ) : \n    return_identifier_0 = [ cls . NONE , cls . SCHEDULED , cls . QUEUED , cls . RUNNING , cls . SHUTDOWN , cls . UP_FOR_RETRY , cls . UP_FOR_RESCHEDULE ] \n    return return_identifier_0 "}
{"576": "\ndef five_crop ( img , size ) : \n    if isinstance ( size , numbers . Number ) : \n        size = ( int ( size ) , int ( size ) ) \n    else : \n        assert len ( size ) == 2 , \"Please provide only two dimensions (h, w) for size.\" \n    w , h = img . size \n    crop_h , crop_w = size \n    if crop_w > w or crop_h > h : \n        raise ValueError ( \"Requested crop size {} is bigger than input size {}\" . format ( size , ( h , w ) ) ) \n    tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) \n    tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) \n    bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) \n    br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) \n    center = center_crop ( img , ( crop_h , crop_w ) ) \n    return_identifier_0 = ( tl , tr , bl , br , center ) \n    return return_identifier_0 "}
{"608": "\ndef searx_bang ( full_query ) : \n    if len ( full_query . getSearchQuery ( ) ) == 0 : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    results = [ ] \n    first_char = full_query . getSearchQuery ( ) [ 0 ] \n    if first_char == '!' or first_char == '?' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( first_char + \"images\" ) \n            results . append ( first_char + \"wikipedia\" ) \n            results . append ( first_char + \"osm\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for categorie in categories : \n                if categorie . startswith ( engine_query ) : \n                    results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) \n            for engine in engines : \n                if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) \n            for engine_shortcut in engine_shortcuts : \n                if engine_shortcut . startswith ( engine_query ) : \n                    results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) \n    elif first_char == ':' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( \":en\" ) \n            results . append ( \":en_us\" ) \n            results . append ( \":english\" ) \n            results . append ( \":united_kingdom\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for lc in language_codes : \n                lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) \n                if lang_id . startswith ( engine_query ) : \n                    if len ( engine_query ) <= 2 : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) \n                    else : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) \n                if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : \n                    results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) \n                if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) \n    result_set = set ( results ) \n    for query_part in full_query . query_parts : \n        if query_part in result_set : \n            result_set . remove ( query_part ) \n    return list ( result_set ) "}
{"672": "\ndef _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : \n    iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) \n    def _cond ( i , val_c , to_fix ) : \n        del val_c \n        return ( i < iter_max ) & tf . reduce_any ( input_tensor = to_fix ) \n    def _body ( i , val_c , to_fix ) : \n        next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) \n        next_val_c = value_and_gradients_function ( next_c ) \n        still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) \n        return_identifier_0 = ( i + 1 , next_val_c , still_to_fix ) \n        return return_identifier_0 \n    to_fix = active & ~ hzl . is_finite ( val_c_input ) \n    return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( 0 , val_c_input , to_fix ) ) "}
{"674": "\ndef _line_search_after_bracketing ( value_and_gradients_function , search_interval , val_0 , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param ) : \n    def _loop_cond ( curr_interval ) : \n        active = ~ ( curr_interval . converged | curr_interval . failed ) \n        return ( curr_interval . iterations < max_iterations ) & tf . reduce_any ( input_tensor = active ) \n    def _loop_body ( curr_interval ) : \n        secant2_raw_result = hzl . secant2 ( value_and_gradients_function , val_0 , curr_interval , f_lim , sufficient_decrease_param , curvature_param ) \n        secant2_result = HagerZhangLineSearchResult ( converged = secant2_raw_result . converged , failed = secant2_raw_result . failed , iterations = curr_interval . iterations + 1 , func_evals = secant2_raw_result . num_evals , left = secant2_raw_result . left , right = secant2_raw_result . right ) \n        should_check_shrinkage = ~ ( secant2_result . converged | secant2_result . failed ) \n        def _do_check_shrinkage ( ) : \n            old_width = curr_interval . right . x - curr_interval . left . x \n            new_width = secant2_result . right . x - secant2_result . left . x \n            sufficient_shrinkage = new_width < old_width * shrinkage_param \n            func_is_flat = ( _very_close ( curr_interval . left . f , curr_interval . right . f ) & _very_close ( secant2_result . left . f , secant2_result . right . f ) ) \n            new_converged = ( should_check_shrinkage & sufficient_shrinkage & func_is_flat ) \n            needs_inner_bisect = should_check_shrinkage & ~ sufficient_shrinkage \n            inner_bisect_args = secant2_result . _replace ( converged = secant2_result . converged | new_converged ) \n            def _apply_inner_bisect ( ) : \n                return _line_search_inner_bisection ( value_and_gradients_function , inner_bisect_args , needs_inner_bisect , f_lim ) \n            return prefer_static . cond ( tf . reduce_any ( input_tensor = needs_inner_bisect ) , _apply_inner_bisect , lambda : inner_bisect_args ) \n        next_args = prefer_static . cond ( tf . reduce_any ( input_tensor = should_check_shrinkage ) , _do_check_shrinkage , lambda : secant2_result ) \n        interval_shrunk = ( ~ next_args . failed & _very_close ( next_args . left . x , next_args . right . x ) ) \n        return_identifier_0 = [ next_args . _replace ( converged = next_args . converged | interval_shrunk ) ] \n        return return_identifier_0 \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ search_interval ] , parallel_iterations = 1 ) [ 0 ] "}
{"681": "\ndef determine_batch_event_shapes ( grid , endpoint_affine ) : \n    with tf . name_scope ( \"determine_batch_event_shapes\" ) : \n        batch_shape = grid . shape [ : - 2 ] \n        batch_shape_tensor = tf . shape ( input = grid ) [ : - 2 ] \n        event_shape = None \n        event_shape_tensor = None \n        def _set_event_shape ( shape , shape_tensor ) : \n            if event_shape is None : \n                return shape , shape_tensor \n            return_identifier_0 = ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) \n            return return_identifier_0 \n        for aff in endpoint_affine : \n            if aff . shift is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - 1 ] ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) \n                event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) \n            if aff . scale is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) \n                event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) \n        return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor "}
{"692": "\ndef posterior_mode ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_mode\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_prob = log_init + observation_log_probs [ 0 ] \n                if self . _num_steps == 1 : \n                    most_likely_end = tf . argmax ( input = log_prob , axis = - 1 ) \n                    return most_likely_end [ ... , tf . newaxis ] \n                def forward_step ( previous_step_pair , log_prob_observation ) : \n                    log_prob_previous = previous_step_pair [ 0 ] \n                    log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) \n                    most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2 ) \n                    max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) \n                    return_identifier_0 = ( max_log_p_given_successor , most_likely_given_successor ) \n                    return return_identifier_0 \n                forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) \n                most_likely_end = tf . argmax ( input = forward_log_probs [ - 1 ] , axis = - 1 ) \n                def backward_step ( most_likely_successor , most_likely_given_successor ) : \n                    return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) \n                backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) \n                most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) \n                return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 ) "}
{"694": "\ndef _sample_next ( target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_next' , [ current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank ] ) : \n        direction = _choose_random_direction ( current_state_parts , batch_rank = batch_rank , seed = seed ) \n        reduce_axes = [ tf . range ( batch_rank , tf . rank ( dirn_part ) ) for dirn_part in direction ] \n        components = [ tf . reduce_sum ( input_tensor = ( dirn_part / step_size ) ** 2 , axis = reduce_axes [ i ] ) for i , ( step_size , dirn_part ) in enumerate ( zip ( step_sizes , direction ) ) ] \n        step_size = tf . math . rsqrt ( tf . add_n ( components ) ) \n        def _get_rank ( x ) : \n            return ( len ( x . shape . as_list ( ) ) if x . shape . dims is not None else tf . rank ( x ) ) \n        state_part_ranks = [ _get_rank ( part ) for part in current_state_parts ] \n        def _step_along_direction ( alpha ) : \n            padded_alphas = [ _right_pad ( alpha , final_rank = part_rank ) for part_rank in state_part_ranks ] \n            state_parts = [ state_part + padded_alpha * direction_part for state_part , direction_part , padded_alpha in zip ( current_state_parts , direction , padded_alphas ) ] \n            return state_parts \n        def projected_target_log_prob_fn ( alpha ) : \n            return target_log_prob_fn ( * _step_along_direction ( alpha ) ) \n        alpha_init = tf . zeros_like ( current_target_log_prob , dtype = current_state_parts [ 0 ] . dtype . base_dtype ) \n        [ next_alpha , next_target_log_prob , bounds_satisfied , upper_bounds , lower_bounds ] = ssu . slice_sampler_one_dim ( projected_target_log_prob_fn , x_initial = alpha_init , max_doublings = max_doublings , step_size = step_size , seed = seed ) \n        return_identifier_0 = [ _step_along_direction ( next_alpha ) , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] \n        return return_identifier_0 "}
{"697": "\ndef one_step ( self , current_state , previous_kernel_results ) : \n    with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'slice' , 'one_step' ) , values = [ self . step_size , self . max_doublings , self . _seed_stream , current_state , previous_kernel_results . target_log_prob ] ) : \n        with tf . compat . v1 . name_scope ( 'initialize' ) : \n            [ current_state_parts , step_sizes , current_target_log_prob ] = _prepare_args ( self . target_log_prob_fn , current_state , self . step_size , previous_kernel_results . target_log_prob , maybe_expand = True ) \n            max_doublings = tf . convert_to_tensor ( value = self . max_doublings , dtype = tf . int32 , name = 'max_doublings' ) \n        independent_chain_ndims = distribution_util . prefer_static_rank ( current_target_log_prob ) \n        [ next_state_parts , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] = _sample_next ( self . target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , independent_chain_ndims , seed = self . _seed_stream ( ) ) \n        def maybe_flatten ( x ) : \n            return x if mcmc_util . is_list_like ( current_state ) else x [ 0 ] \n        return_identifier_0 = [ maybe_flatten ( next_state_parts ) , SliceSamplerKernelResults ( target_log_prob = next_target_log_prob , bounds_satisfied = bounds_satisfied , direction = direction , upper_bounds = upper_bounds , lower_bounds = lower_bounds ) , ] \n        return return_identifier_0 "}
{"796": "\ndef slice_sampler_one_dim ( target_log_prob , x_initial , step_size = 0.01 , max_doublings = 30 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_sampler_one_dim' , [ x_initial , step_size , max_doublings ] ) : \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        dtype = x_initial . dtype . base_dtype \n        log_slice_heights = target_log_prob ( x_initial ) - tf . random . gamma ( tf . shape ( input = x_initial ) , alpha = 1 , dtype = dtype , seed = seed ) \n        upper_bounds , lower_bounds , bounds_satisfied = slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = seed ) \n        retval = _sample_with_shrinkage ( x_initial , target_log_prob = target_log_prob , log_slice_heights = log_slice_heights , step_size = step_size , lower_bounds = lower_bounds , upper_bounds = upper_bounds , seed = seed ) \n        return_identifier_0 = ( retval , target_log_prob ( retval ) , bounds_satisfied , upper_bounds , lower_bounds ) \n        return return_identifier_0 "}
{"801": "\ndef _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : \n    if depth == 0 : \n        [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) \n        next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) \n        num_states = tf . cast ( next_log_joint > log_slice_sample , dtype = tf . int32 ) \n        continue_trajectory = ( next_log_joint > log_slice_sample - max_simulation_error ) \n        return_identifier_0 = [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] \n        return return_identifier_0 \n    seed_stream = tfd . SeedStream ( seed , \"build_tree\" ) \n    [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n    if continue_trajectory : \n        if direction < 0 : \n            [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        else : \n            [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        num_states += far_num_states \n        accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) \n        if accept_far_state : \n            next_state = far_state \n            next_target_log_prob = far_target_log_prob \n            next_grads_target_log_prob = far_grads_target_log_prob \n        has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) \n        continue_trajectory = far_continue_trajectory and has_no_u_turn \n    return_identifier_1 = [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] \n    return return_identifier_1 "}
{"804": "\ndef _leapfrog ( value_and_gradients_fn , current_state , current_grads_target_log_prob , current_momentum , step_size ) : \n    mid_momentum = [ m + 0.5 * step * g for m , step , g in zip ( current_momentum , step_size , current_grads_target_log_prob ) ] \n    next_state = [ s + step * m for s , step , m in zip ( current_state , step_size , mid_momentum ) ] \n    next_target_log_prob , next_grads_target_log_prob = value_and_gradients_fn ( * next_state ) \n    next_momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid_momentum , step_size , next_grads_target_log_prob ) ] \n    return_identifier_0 = [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] \n    return return_identifier_0 "}
{"858": "\ndef minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = False , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = 1 , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : \n        ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) \n        domain_dtype = simplex . dtype \n        ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) \n        closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) \n        def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : \n            ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , ** closure_kwargs ) \n            return_identifier_0 = ( converged , iterations + 1 , next_simplex , next_objective , num_evaluations + evaluations ) \n            return return_identifier_0 \n        initial_args = ( False , 0 , simplex , objective_at_simplex , num_evaluations ) \n        def _is_converged ( converged , num_iterations , * ignored_args ) : \n            not_converged = tf . logical_not ( converged ) \n            return ( not_converged if max_iterations is None else ( not_converged & ( num_iterations < max_iterations ) ) ) \n        ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) \n        order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = True ) \n        best_index = order [ 0 ] \n        return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) \n        ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex \n        face_centroid /= tf . cast ( dim , domain_dtype ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = 1 \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return_identifier_0 = ( True , current_simplex , current_objective_values , 0 ) \n            return return_identifier_0 \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = objective_at_reflected < best_objective_value \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return_identifier_1 = ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) \n        return return_identifier_1 "}
{"862": "\ndef _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : \n    def _contraction ( ) : \n        contracted = face_centroid + contraction * ( reflected - face_centroid ) \n        objective_at_contracted = objective_function ( contracted ) \n        is_contracted_acceptable = objective_at_contracted <= objective_at_reflected \n        def _accept_contraction ( ) : \n            next_simplex = _replace_at_index ( simplex , worst_index , contracted ) \n            objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) \n            return_identifier_0 = ( False , next_simplex , objective_at_next_simplex , 1 ) \n            return return_identifier_0 \n        def _reject_contraction ( ) : \n            return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) \n        return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) \n    return _contraction "}
{"863": "\ndef _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : \n    best_vertex = simplex [ best_index ] \n    shrunk_simplex = best_vertex + shrinkage * ( simplex - best_vertex ) \n    objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) \n    return_identifier_0 = ( False , shrunk_simplex , objective_at_shrunk_simplex , evals ) \n    return return_identifier_0 "}
{"867": "\ndef _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : \n    initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) \n    num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] \n    dim = num_vertices - 1 \n    num_evaluations = 0 \n    if objective_at_initial_simplex is None : \n        objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) \n        num_evaluations += n_evals \n    objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) \n    return_identifier_0 = ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations ) \n    return return_identifier_0 "}
{"868": "\ndef _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : \n    dim = tf . size ( input = initial_vertex ) \n    num_vertices = dim + 1 \n    unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial_vertex ) ] , axis = 0 ) ) \n    simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes \n    simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = 0 ) , simplex_face ] , axis = 0 ) \n    num_evaluations = 0 \n    if objective_at_initial_vertex is None : \n        objective_at_initial_vertex = objective_function ( initial_vertex ) \n        num_evaluations += 1 \n    objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) \n    num_evaluations += num_evals \n    objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = 0 ) , objective_at_simplex_face ] , axis = 0 ) \n    return_identifier_0 = ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations ) \n    return return_identifier_0 "}
{"873": "\ndef get_config ( self ) : \n    return_identifier_0 = { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate_args' : self . validate_args , } \n    return return_identifier_0 "}
{"892": "\ndef fit ( model_matrix , response , model , model_coefficients_start = None , predicted_linear_response_start = None , l2_regularizer = None , dispersion = None , offset = None , convergence_criteria_fn = None , learning_rate = None , fast_unsafe_numerics = True , maximum_iterations = None , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , dispersion , offset , learning_rate , maximum_iterations ] \n    with tf . compat . v1 . name_scope ( name , 'fit' , graph_deps ) : \n        [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset , ] = prepare_args ( model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset ) \n        if convergence_criteria_fn is None : \n            convergence_criteria_fn = ( convergence_criteria_small_relative_norm_weights_change ( ) ) \n        def _body ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous ) : \n            model_coefficients_next , predicted_linear_response_next = fit_one_step ( model_matrix , response , model , model_coefficients_previous , predicted_linear_response_previous , l2_regularizer , dispersion , offset , learning_rate , fast_unsafe_numerics ) \n            is_converged_next = convergence_criteria_fn ( is_converged_previous = is_converged_previous , iter_ = iter_ , model_coefficients_previous = model_coefficients_previous , predicted_linear_response_previous = predicted_linear_response_previous , model_coefficients_next = model_coefficients_next , predicted_linear_response_next = predicted_linear_response_next , response = response , model = model , dispersion = dispersion ) \n            return_identifier_0 = [ is_converged_next , iter_ + 1 , model_coefficients_next , predicted_linear_response_next , ] \n            return return_identifier_0 \n        [ is_converged , iter_ , model_coefficients , predicted_linear_response , ] = tf . while_loop ( cond = lambda is_converged , * args : tf . logical_not ( is_converged ) , body = _body , loop_vars = [ tf . zeros ( [ ] , np . bool ) , tf . zeros ( [ ] , np . int32 ) , model_coefficients_start , predicted_linear_response_start , ] , maximum_iterations = maximum_iterations ) \n        return_identifier_1 = [ model_coefficients , predicted_linear_response , is_converged , iter_ ] \n        return return_identifier_1 "}
{"894": "\ndef prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] \n    with tf . compat . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : \n        dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) \n        model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) \n        if offset is not None : \n            offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) \n        response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) \n        use_default_model_coefficients = model_coefficients is None \n        if use_default_model_coefficients : \n            batch_shape = tf . shape ( input = model_matrix ) [ : - 2 ] \n            num_columns = tf . shape ( input = model_matrix ) [ - 1 ] \n            model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = 0 ) , dtype = dtype , name = 'model_coefficients' ) \n        else : \n            model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) \n        if predicted_linear_response is None : \n            if use_default_model_coefficients : \n                if offset is None : \n                    predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) \n                else : \n                    predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( input = response ) , name = 'predicted_linear_response' ) \n            else : \n                predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) \n        else : \n            predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) \n    return_identifier_0 = [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ] \n    return return_identifier_0 "}
{"917": "\ndef _smallest_integer_by_dtype ( dt ) : \n    if not _is_known_dtype ( dt ) : \n        raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) \n    if _is_known_unsigned_by_dtype ( dt ) : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    return - 1 * _largest_integer_by_dtype ( dt ) "}
{"941": "\ndef backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) : \n    tmp_gain_cov = transition_matrix . matmul ( filtered_cov ) \n    predicted_cov_chol = tf . linalg . cholesky ( predicted_cov ) \n    gain_transpose = tf . linalg . cholesky_solve ( predicted_cov_chol , tmp_gain_cov ) \n    posterior_mean = ( filtered_mean + tf . linalg . matmul ( gain_transpose , next_posterior_mean - predicted_mean , adjoint_a = True ) ) \n    posterior_cov = ( filtered_cov + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( next_posterior_cov - predicted_cov , gain_transpose ) , adjoint_a = True ) ) \n    return_identifier_0 = ( posterior_mean , posterior_cov ) \n    return return_identifier_0 "}
{"945": "\ndef build_kalman_mean_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : \n    def mean_step ( previous_means , t ) : \n        previous_latent_mean , _ = previous_means \n        latent_mean = _propagate_mean ( previous_latent_mean , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) \n        observation_mean = _propagate_mean ( latent_mean , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) \n        return_identifier_0 = ( latent_mean , observation_mean ) \n        return return_identifier_0 \n    return mean_step "}
{"946": "\ndef build_kalman_cov_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : \n    def cov_step ( previous_covs , t ) : \n        previous_latent_cov , _ = previous_covs \n        latent_cov = _propagate_cov ( previous_latent_cov , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) \n        observation_cov = _propagate_cov ( latent_cov , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) \n        return_identifier_0 = ( latent_cov , observation_cov ) \n        return return_identifier_0 \n    return cov_step "}
{"947": "\ndef build_kalman_sample_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep , full_sample_and_batch_shape , stream , validate_args = False ) : \n    def sample_step ( sampled_prev , t ) : \n        latent_prev , _ = sampled_prev \n        transition_matrix = get_transition_matrix_for_timestep ( t - 1 ) \n        transition_noise = get_transition_noise_for_timestep ( t - 1 ) \n        latent_pred = transition_matrix . matmul ( latent_prev ) \n        latent_sampled = latent_pred + transition_noise . sample ( sample_shape = _augment_sample_shape ( transition_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] \n        observation_matrix = get_observation_matrix_for_timestep ( t ) \n        observation_noise = get_observation_noise_for_timestep ( t ) \n        observation_pred = observation_matrix . matmul ( latent_sampled ) \n        observation_sampled = observation_pred + observation_noise . sample ( sample_shape = _augment_sample_shape ( observation_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] \n        return_identifier_0 = ( latent_sampled , observation_sampled ) \n        return return_identifier_0 \n    return sample_step "}
{"950": "\ndef backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : \n    with tf . name_scope ( \"backward_pass\" ) : \n        filtered_means = tf . convert_to_tensor ( value = filtered_means , name = \"filtered_means\" ) \n        filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = \"filtered_covs\" ) \n        predicted_means = tf . convert_to_tensor ( value = predicted_means , name = \"predicted_means\" ) \n        predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = \"predicted_covs\" ) \n        filtered_means = distribution_util . move_dimension ( filtered_means , - 2 , 0 ) \n        filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3 , 0 ) \n        predicted_means = distribution_util . move_dimension ( predicted_means , - 2 , 0 ) \n        predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3 , 0 ) \n        filtered_means = filtered_means [ ... , tf . newaxis ] \n        predicted_means = predicted_means [ ... , tf . newaxis ] \n        initial_backward_mean = predicted_means [ - 1 , ... ] \n        initial_backward_cov = predicted_covs [ - 1 , ... ] \n        num_timesteps = tf . shape ( input = filtered_means ) [ 0 ] \n        initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - 1 ) \n        update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) \n        posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = True ) \n        posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , 0 ] , 0 , - 2 ) \n        posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , 0 , - 3 ) \n        return_identifier_0 = ( posterior_means , posterior_covs ) \n        return return_identifier_0 "}
{"952": "\ndef posterior_marginals ( self , x , mask = None ) : \n    with tf . name_scope ( \"smooth\" ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        ( _ , filtered_means , filtered_covs , predicted_means , predicted_covs , _ , _ ) = self . forward_filter ( x , mask = mask ) \n        ( smoothed_means , smoothed_covs ) = self . backward_smoothing_pass ( filtered_means , filtered_covs , predicted_means , predicted_covs ) \n        return_identifier_0 = ( smoothed_means , smoothed_covs ) \n        return return_identifier_0 "}
{"964": "\ndef is_namedtuple_like ( x ) : \n    try : \n        for fn in x . _fields : \n            _ = getattr ( x , fn ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except AttributeError : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"999": "\ndef bracket ( value_and_gradients_function , search_interval , f_lim , max_iterations , expansion_param = 5.0 ) : \n    already_stopped = search_interval . failed | search_interval . converged \n    bracketed = search_interval . right . df >= 0 \n    needs_bisect = ( search_interval . right . df < 0 ) & ( search_interval . right . f > f_lim ) \n    initial_args = _IntermediateResult ( iteration = search_interval . iterations , stopped = already_stopped | bracketed | needs_bisect , failed = search_interval . failed , num_evals = search_interval . func_evals , left = search_interval . left , right = search_interval . right ) \n    def _loop_cond ( curr ) : \n        return ( curr . iteration < max_iterations ) & ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        new_right = value_and_gradients_function ( expansion_param * curr . right . x ) \n        left = val_where ( curr . stopped , curr . left , curr . right ) \n        right = val_where ( curr . stopped , curr . right , new_right ) \n        failed = curr . failed | ~ is_finite ( right ) \n        bracketed = right . df >= 0 \n        needs_bisect = ( right . df < 0 ) & ( right . f > f_lim ) \n        return_identifier_0 = [ _IntermediateResult ( iteration = curr . iteration + 1 , stopped = curr . stopped | failed | bracketed | needs_bisect , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n        return return_identifier_0 \n    bracket_result = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] \n    needs_bisect = ( ( bracket_result . right . df < 0 ) & ( bracket_result . right . f > f_lim ) ) \n    stopped = already_stopped | bracket_result . failed | ~ needs_bisect \n    left = val_where ( stopped , bracket_result . left , search_interval . left ) \n    bisect_args = bracket_result . _replace ( stopped = stopped , left = left ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1001": "\ndef _bisect ( value_and_gradients_function , initial_args , f_lim ) : \n    def _loop_cond ( curr ) : \n        return ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) \n        failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) \n        to_update = ~ ( curr . stopped | failed ) \n        update_left = ( mid . df < 0 ) & ( mid . f <= f_lim ) \n        left = val_where ( to_update & update_left , mid , curr . left ) \n        right = val_where ( to_update & ~ update_left , mid , curr . right ) \n        stopped = curr . stopped | failed | ( right . df >= 0 ) \n        return_identifier_0 = [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n        return return_identifier_0 \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] "}
{"1006": "\ndef _leapfrog_integrator_one_step ( target_log_prob_fn , independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts , state_gradients_are_stopped = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'hmc_leapfrog_integrator_one_step' , [ independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts ] ) : \n        proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( current_momentum_parts , step_sizes , current_target_log_prob_grad_parts ) ] \n        proposed_state_parts = [ x + tf . cast ( eps , v . dtype ) * v for x , eps , v in zip ( current_state_parts , step_sizes , proposed_momentum_parts ) ] \n        if state_gradients_are_stopped : \n            proposed_state_parts = [ tf . stop_gradient ( x ) for x in proposed_state_parts ] \n        [ proposed_target_log_prob , proposed_target_log_prob_grad_parts , ] = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , proposed_state_parts ) \n        if not proposed_target_log_prob . dtype . is_floating : \n            raise TypeError ( '`target_log_prob_fn` must produce a `Tensor` ' 'with `float` `dtype`.' ) \n        if any ( g is None for g in proposed_target_log_prob_grad_parts ) : \n            raise ValueError ( 'Encountered `None` gradient. Does your target `target_log_prob_fn` ' 'access all `tf.Variable`s via `tf.get_variable`?\\n' '  current_state_parts: {}\\n' '  proposed_state_parts: {}\\n' '  proposed_target_log_prob_grad_parts: {}' . format ( current_state_parts , proposed_state_parts , proposed_target_log_prob_grad_parts ) ) \n        proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( proposed_momentum_parts , step_sizes , proposed_target_log_prob_grad_parts ) ] \n        return_identifier_0 = [ proposed_momentum_parts , proposed_state_parts , proposed_target_log_prob , proposed_target_log_prob_grad_parts , ] \n        return return_identifier_0 "}
{"1027": "\ndef create_sprites_dataset ( characters , actions , directions , channels = 3 , length = 8 , shuffle = False , fake_data = False ) : \n    if fake_data : \n        dummy_image = tf . random . normal ( [ HEIGHT , WIDTH , CHANNELS ] ) \n    else : \n        basedir = download_sprites ( ) \n    action_names = [ action . name for action in actions ] \n    action_metadata = [ ( action . start_row , action . frames ) for action in actions ] \n    direction_rows = [ direction . row_offset for direction in directions ] \n    chars = tf . data . Dataset . from_tensor_slices ( characters ) \n    act_names = tf . data . Dataset . from_tensor_slices ( action_names ) . repeat ( ) \n    acts_metadata = tf . data . Dataset . from_tensor_slices ( action_metadata ) . repeat ( ) \n    dir_rows = tf . data . Dataset . from_tensor_slices ( direction_rows ) . repeat ( ) \n    if shuffle : \n        chars = chars . shuffle ( len ( characters ) ) \n    dataset = tf . data . Dataset . zip ( ( chars , act_names , acts_metadata , dir_rows ) ) \n    skin_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( SKIN_COLORS ) ) \n    hair_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( HAIRSTYLES ) ) \n    top_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( TOPS ) ) \n    pants_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( PANTS ) ) \n    action_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( action_names ) ) \n    def process_example ( attrs , act_name , act_metadata , dir_row_offset ) : \n        skin_name = attrs [ 0 ] \n        hair_name = attrs [ 1 ] \n        top_name = attrs [ 2 ] \n        pants_name = attrs [ 3 ] \n        if fake_data : \n            char = dummy_image \n        else : \n            skin = read_image ( basedir + os . sep + skin_name ) \n            hair = read_image ( basedir + os . sep + hair_name ) \n            top = read_image ( basedir + os . sep + top_name ) \n            pants = read_image ( basedir + os . sep + pants_name ) \n            char = create_character ( skin , hair , top , pants ) \n        if shuffle : \n            seq = create_random_seq ( char , act_metadata , dir_row_offset , length ) \n        else : \n            seq = create_seq ( char , act_metadata , dir_row_offset , length ) \n        seq = seq [ ... , : channels ] \n        skin_idx = skin_table . lookup ( skin_name ) \n        hair_idx = hair_table . lookup ( hair_name ) \n        top_idx = top_table . lookup ( top_name ) \n        pants_idx = pants_table . lookup ( pants_name ) \n        act_idx = action_table . lookup ( act_name ) \n        return_identifier_0 = ( seq , skin_idx , hair_idx , top_idx , pants_idx , act_idx , skin_name , hair_name , top_name , pants_name , act_name ) \n        return return_identifier_0 \n    dataset = dataset . map ( process_example ) \n    return dataset "}
{"1061": "\ndef minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        input_shape = distribution_util . prefer_static_shape ( initial_position ) \n        batch_shape , domain_size = input_shape [ : - 1 ] , input_shape [ - 1 ] \n        if stopping_condition is None : \n            stopping_condition = bfgs_utils . converged_all \n        control_inputs = None \n        if initial_inverse_hessian_estimate is None : \n            initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) \n        else : \n            initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) \n            control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) \n            hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , 0 ) \n            initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) \n        def _cond ( state ) : \n            return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( state ) : \n            search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) \n            derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) \n            needs_reset = ( ~ state . failed & ~ state . converged & ( derivative_at_start_pt >= 0 ) ) \n            search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) \n            actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) \n            actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) \n            current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            return_identifier_0 = [ _update_inv_hessian ( current_state , next_state ) ] \n            return return_identifier_0 \n        kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) \n        kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian \n        initial_state = BfgsOptimizerResults ( ** kwargs ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1062": "\ndef _inv_hessian_control_inputs ( inv_hessian ) : \n    is_positive_definite = tf . reduce_all ( input_tensor = tf . math . is_finite ( tf . linalg . cholesky ( inv_hessian ) ) , axis = [ - 1 , - 2 ] ) \n    is_symmetric = tf . equal ( bfgs_utils . norm ( inv_hessian - _batch_transpose ( inv_hessian ) , dims = 2 ) , 0 ) \n    return_identifier_0 = [ tf . Assert ( is_positive_definite , [ 'Initial inverse Hessian is not positive definite.' , inv_hessian ] ) , tf . Assert ( is_symmetric , [ 'Initial inverse Hessian is not symmetric' , inv_hessian ] ) ] \n    return return_identifier_0 "}
{"1072": "\ndef minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10 , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    if initial_inverse_hessian_estimate is not None : \n        raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) \n    if stopping_condition is None : \n        stopping_condition = bfgs_utils . converged_all \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        def _cond ( state ) : \n            return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( current_state ) : \n            search_direction = _get_search_direction ( current_state ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            should_update = ~ ( next_state . converged | next_state . failed ) \n            state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) \n            return_identifier_0 = [ state_after_inv_hessian_update ] \n            return return_identifier_0 \n        initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1074": "\ndef _get_search_direction ( state ) : \n    num_elements = tf . minimum ( state . num_iterations , distribution_util . prefer_static_shape ( state . position_deltas ) [ 0 ] ) \n    def _two_loop_algorithm ( ) : \n        position_deltas = state . position_deltas [ - num_elements : ] \n        gradient_deltas = state . gradient_deltas [ - num_elements : ] \n        inv_rhos = tf . reduce_sum ( input_tensor = gradient_deltas * position_deltas , axis = - 1 ) \n        def first_loop ( acc , args ) : \n            _ , q_direction = acc \n            position_delta , gradient_delta , inv_rho = args \n            alpha = tf . reduce_sum ( input_tensor = position_delta * q_direction , axis = - 1 ) / inv_rho \n            direction_delta = tf . expand_dims ( alpha , axis = - 1 ) * gradient_delta \n            return_identifier_0 = ( alpha , q_direction - direction_delta ) \n            return return_identifier_0 \n        zero = tf . zeros_like ( inv_rhos [ 0 ] ) \n        alphas , q_directions = tf . scan ( first_loop , [ position_deltas , gradient_deltas , inv_rhos ] , initializer = ( zero , state . objective_gradient ) , reverse = True ) \n        gamma_k = inv_rhos [ - 1 ] / tf . reduce_sum ( input_tensor = gradient_deltas [ - 1 ] * gradient_deltas [ - 1 ] , axis = - 1 ) \n        r_direction = tf . expand_dims ( gamma_k , axis = - 1 ) * q_directions [ 0 ] \n        def second_loop ( r_direction , args ) : \n            alpha , position_delta , gradient_delta , inv_rho = args \n            beta = tf . reduce_sum ( input_tensor = gradient_delta * r_direction , axis = - 1 ) / inv_rho \n            direction_delta = tf . expand_dims ( alpha - beta , axis = - 1 ) * position_delta \n            return r_direction + direction_delta \n        r_directions = tf . scan ( second_loop , [ alphas , position_deltas , gradient_deltas , inv_rhos ] , initializer = r_direction ) \n        return - r_directions [ - 1 ] \n    return prefer_static . cond ( tf . equal ( num_elements , 0 ) , ( lambda : - state . objective_gradient ) , _two_loop_algorithm ) "}
{"1081": "\ndef _clopper_pearson_confidence_interval ( samples , error_rate ) : \n    if optimize is None or stats is None : \n        raise ValueError ( \"Scipy is required for computing Clopper-Pearson confidence intervals\" ) \n    if len ( samples . shape ) != 1 : \n        raise ValueError ( \"Batch semantics not implemented\" ) \n    n = len ( samples ) \n    low = np . amin ( samples ) \n    high = np . amax ( samples ) \n    successes = np . count_nonzero ( samples - low ) \n    failures = np . count_nonzero ( samples - high ) \n    if successes + failures != n : \n        uniques = np . unique ( samples ) \n        msg = ( \"Purportedly Bernoulli distribution had distinct samples\" \" {}, {}, and {}\" . format ( uniques [ 0 ] , uniques [ 1 ] , uniques [ 2 ] ) ) \n        raise ValueError ( msg ) \n    def p_small_enough ( p ) : \n        prob = stats . binom . logcdf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    def p_big_enough ( p ) : \n        prob = stats . binom . logsf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    high_p = optimize . brentq ( p_small_enough , float ( successes ) / n , 1. , rtol = 1e-9 ) \n    low_p = optimize . brentq ( p_big_enough , 0. , float ( successes ) / n , rtol = 1e-9 ) \n    low_interval = low + ( high - low ) * low_p \n    high_interval = low + ( high - low ) * high_p \n    return_identifier_0 = ( low_interval , high_interval ) \n    return return_identifier_0 "}
{"1086": "\ndef minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    if initial_population is None and initial_position is None : \n        raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) \n    if initial_population is not None and initial_position is not None : \n        raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) \n    with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : \n        ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) \n        def evolve_body ( loop_vars ) : \n            next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) \n            converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) \n            failed = _check_failure ( next_population_values ) \n            return_identifier_0 = [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] \n            return return_identifier_0 \n        def evolve_cond ( loop_vars ) : \n            should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and loop_vars . num_iterations >= max_iterations ) ) \n            return ~ should_stop \n        initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) \n        final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] \n        best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) \n        final_population = final_state . population \n        if not was_iterable : \n            final_population = final_population [ 0 ] \n            best_position = best_position [ 0 ] \n        return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations ) "}
{"1087": "\ndef _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) : \n    was_iterable = False \n    if initial_position is not None : \n        initial_position , was_iterable = _ensure_list ( initial_position ) \n    if initial_population is not None : \n        initial_population , was_iterable = _ensure_list ( initial_population ) \n    population = _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed = seed ) \n    differential_weight = tf . convert_to_tensor ( value = differential_weight , dtype = population [ 0 ] . dtype . base_dtype ) \n    crossover_prob = tf . convert_to_tensor ( value = crossover_prob ) \n    population_values = objective_function ( * population ) \n    if max_iterations is not None : \n        max_iterations = tf . convert_to_tensor ( value = max_iterations ) \n    func_tolerance = tf . convert_to_tensor ( value = func_tolerance , dtype = population_values . dtype . base_dtype ) \n    position_tolerance = tf . convert_to_tensor ( value = position_tolerance , dtype = population [ 0 ] . dtype . base_dtype ) \n    return_identifier_0 = ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) \n    return return_identifier_0 "}
{"1088": "\ndef _find_best_in_population ( population , values ) : \n    best_value = tf . math . reduce_min ( input_tensor = values ) \n    best_index = tf . where ( tf . math . equal ( values , best_value ) ) [ 0 , 0 ] \n    return_identifier_0 = ( [ population_part [ best_index ] for population_part in population ] , best_value ) \n    return return_identifier_0 "}
{"1139": "\ndef _is_empty_observation_data ( feature_ndims , observation_index_points , observations ) : \n    if observation_index_points is None and observations is None : \n        return_identifier_0 = True \n        return return_identifier_0 \n    num_obs = tf . compat . dimension_value ( observation_index_points . shape [ - ( feature_ndims + 1 ) ] ) \n    if num_obs is not None and num_obs == 0 : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"1195": "\ndef get_bigdl_classpath ( ) : \n    if os . getenv ( \"BIGDL_CLASSPATH\" ) : \n        return os . environ [ \"BIGDL_CLASSPATH\" ] \n    jar_dir = os . path . abspath ( __file__ + \"/../../\" ) \n    jar_paths = glob . glob ( os . path . join ( jar_dir , \"share/lib/*.jar\" ) ) \n    if jar_paths : \n        assert len ( jar_paths ) == 1 , \"Expecting one jar: %s\" % len ( jar_paths ) \n        return jar_paths [ 0 ] \n    return_identifier_0 = \"\" \n    return return_identifier_0 "}
{"1196": "\ndef is_spark_below_2_2 ( ) : \n    import pyspark \n    if ( hasattr ( pyspark , \"version\" ) ) : \n        full_version = pyspark . version . __version__ \n        parts = full_version . split ( \".\" ) \n        spark_version = parts [ 0 ] + \".\" + parts [ 1 ] \n        if ( compare_version ( spark_version , \"2.2\" ) >= 0 ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if possible_matches > 0 : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if possible_matches_by_order [ i ] > 0 : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if min ( precisions ) > 0 : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return_identifier_0 = ( bleu , precisions , bp , ratio , translation_length , reference_length ) \n    return return_identifier_0 "}
{"1263": "\ndef search ( self , word , d , allow_spaces = True , return_cost = True ) : \n    if not all ( ( c in self . alphabet or ( c == \" \" and self . allow_spaces ) ) for c in word ) : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    return self . _trie_search ( word , d , allow_spaces = allow_spaces , return_cost = return_cost ) "}
{"1269": "\ndef _handle_intent ( self , request : dict ) -> dict : \n    intent_name = self . config [ 'intent_name' ] \n    slot_name = self . config [ 'slot_name' ] \n    request_id = request [ 'request' ] [ 'requestId' ] \n    request_intent : dict = request [ 'request' ] [ 'intent' ] \n    if intent_name != request_intent [ 'name' ] : \n        log . error ( f\"Wrong intent name received: {request_intent['name']} in request {request_id}\" ) \n        return_identifier_0 = { 'error' : 'wrong intent name' } \n        return return_identifier_0 \n    if slot_name not in request_intent [ 'slots' ] . keys ( ) : \n        log . error ( f'No slot named {slot_name} found in request {request_id}' ) \n        return_identifier_1 = { 'error' : 'no slot found' } \n        return return_identifier_1 \n    utterance = request_intent [ 'slots' ] [ slot_name ] [ 'value' ] \n    agent_response = self . _act ( utterance ) \n    if not agent_response : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return_identifier_2 = { 'error' : 'error during response generation' } \n        return return_identifier_2 \n    prediction : RichMessage = agent_response [ 0 ] \n    prediction : list = prediction . alexa ( ) \n    if not prediction : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return_identifier_3 = { 'error' : 'error during response generation' } \n        return return_identifier_3 \n    response = self . _generate_response ( prediction [ 0 ] , request ) \n    return response "}
{"1284": "\ndef read_requirements ( ) : \n    reqs_path = os . path . join ( __location__ , 'requirements.txt' ) \n    with open ( reqs_path , encoding = 'utf8' ) as f : \n        reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] \n    names = [ ] \n    links = [ ] \n    for req in reqs : \n        if '://' in req : \n            links . append ( req ) \n        else : \n            names . append ( req ) \n    return_identifier_0 = { 'install_requires' : names , 'dependency_links' : links } \n    return return_identifier_0 "}
{"1311": "\ndef _verify_request ( self , signature_chain_url : str , signature : str , request_body : bytes ) -> bool : \n    if signature_chain_url not in self . valid_certificates . keys ( ) : \n        amazon_cert : X509 = verify_cert ( signature_chain_url ) \n        if amazon_cert : \n            amazon_cert_lifetime : timedelta = self . config [ 'amazon_cert_lifetime' ] \n            expiration_timestamp = datetime . utcnow ( ) + amazon_cert_lifetime \n            validated_cert = ValidatedCert ( cert = amazon_cert , expiration_timestamp = expiration_timestamp ) \n            self . valid_certificates [ signature_chain_url ] = validated_cert \n            log . info ( f'Certificate {signature_chain_url} validated' ) \n        else : \n            log . error ( f'Certificate {signature_chain_url} validation failed' ) \n            return_identifier_0 = False \n            return return_identifier_0 \n    else : \n        validated_cert : ValidatedCert = self . valid_certificates [ signature_chain_url ] \n        amazon_cert : X509 = validated_cert . cert \n    if verify_signature ( amazon_cert , signature , request_body ) : \n        result = True \n    else : \n        log . error ( f'Failed signature verification for request: {request_body.decode(\"utf-8\", \"replace\")}' ) \n        result = False \n    return result "}
{"1312": "\ndef _handle_request ( self , request : dict ) -> dict : \n    request_body : bytes = request [ 'request_body' ] \n    signature_chain_url : str = request [ 'signature_chain_url' ] \n    signature : str = request [ 'signature' ] \n    alexa_request : dict = request [ 'alexa_request' ] \n    if not self . _verify_request ( signature_chain_url , signature , request_body ) : \n        return_identifier_0 = { 'error' : 'failed certificate/signature check' } \n        return return_identifier_0 \n    timestamp_str = alexa_request [ 'request' ] [ 'timestamp' ] \n    timestamp_datetime = datetime . strptime ( timestamp_str , '%Y-%m-%dT%H:%M:%SZ' ) \n    now = datetime . utcnow ( ) \n    delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now \n    if abs ( delta . seconds ) > REQUEST_TIMESTAMP_TOLERANCE_SECS : \n        log . error ( f'Failed timestamp check for request: {request_body.decode(\"utf-8\", \"replace\")}' ) \n        return_identifier_1 = { 'error' : 'failed request timestamp check' } \n        return return_identifier_1 \n    conversation_key = alexa_request [ 'session' ] [ 'user' ] [ 'userId' ] \n    if conversation_key not in self . conversations . keys ( ) : \n        if self . config [ 'multi_instance' ] : \n            conv_agent = self . _init_agent ( ) \n            log . info ( 'New conversation instance level agent initiated' ) \n        else : \n            conv_agent = self . agent \n        self . conversations [ conversation_key ] = Conversation ( config = self . config , agent = conv_agent , conversation_key = conversation_key , self_destruct_callback = lambda : self . _del_conversation ( conversation_key ) ) \n        log . info ( f'Created new conversation, key: {conversation_key}' ) \n    conversation = self . conversations [ conversation_key ] \n    response = conversation . handle_request ( alexa_request ) \n    return response "}
{"1320": "\ndef is_running ( self ) : \n    try : \n        if h2o . connection ( ) . local_server and not h2o . connection ( ) . local_server . is_running ( ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        h2o . api ( \"GET /\" ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    except ( H2OConnectionError , H2OServerError ) : \n        return_identifier_2 = False \n        return return_identifier_2 "}
{"1341": "\ndef _get_method_full_name ( func ) : \n    if hasattr ( func , \"__qualname__\" ) : \n        return func . __qualname__ \n    module = inspect . getmodule ( func ) \n    if module is None : \n        return \"?.%s\" % getattr ( func , \"__name__\" , \"?\" ) \n    for cls_name in dir ( module ) : \n        cls = getattr ( module , cls_name ) \n        if not inspect . isclass ( cls ) : \n            continue \n        for method_name in dir ( cls ) : \n            cls_method = getattr ( cls , method_name ) \n            if cls_method == func : \n                return \"%s.%s\" % ( cls_name , method_name ) \n    if hasattr ( func , \"__name__\" ) : \n        return \"%s.%s\" % ( module . __name__ , func . __name__ ) \n    return_identifier_0 = \"<unknown>\" \n    return return_identifier_0 "}
{"1343": "\ndef _get_args_str ( func , highlight = None ) : \n    if not func : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    s = str ( inspect . signature ( func ) ) [ 1 : - 1 ] \n    if highlight : \n        s = re . sub ( r\"\\b%s\\b\" % highlight , Style . BRIGHT + Fore . WHITE + highlight + Fore . LIGHTBLACK_EX + Style . NORMAL , s ) \n    return s "}
{"1369": "\ndef _get_type_name ( vtype , dump = None ) : \n    if vtype is None : \n        return_identifier_0 = \"None\" \n        return return_identifier_0 \n    if vtype is str : \n        return_identifier_1 = \"string\" \n        return return_identifier_1 \n    if vtype is int : \n        return_identifier_2 = \"integer\" \n        return return_identifier_2 \n    if vtype is numeric : \n        return_identifier_3 = \"numeric\" \n        return return_identifier_3 \n    if is_type ( vtype , str ) : \n        return '\"%s\"' % repr ( vtype ) [ 1 : - 1 ] \n    if is_type ( vtype , int ) : \n        return str ( vtype ) \n    if isinstance ( vtype , MagicType ) : \n        return vtype . name ( dump ) \n    if isinstance ( vtype , type ) : \n        return vtype . __name__ \n    if isinstance ( vtype , list ) : \n        return \"list(%s)\" % _get_type_name ( U ( * vtype ) , dump ) \n    if isinstance ( vtype , set ) : \n        return \"set(%s)\" % _get_type_name ( U ( * vtype ) , dump ) \n    if isinstance ( vtype , tuple ) : \n        return \"(%s)\" % \", \" . join ( _get_type_name ( item , dump ) for item in vtype ) \n    if isinstance ( vtype , dict ) : \n        return \"dict(%s)\" % \", \" . join ( \"%s: %s\" % ( _get_type_name ( tk , dump ) , _get_type_name ( tv , dump ) ) for tk , tv in viewitems ( vtype ) ) \n    if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : \n        if vtype . __name__ == \"<lambda>\" : \n            return _get_lambda_source_code ( vtype , dump ) \n        else : \n            return vtype . __name__ \n    raise RuntimeError ( \"Unexpected `vtype`: %r\" % vtype ) "}
{"1370": "\ndef _get_lambda_source_code ( lambda_fn , src ) : \n    def gen_lambdas ( ) : \n        def gen ( ) : \n            yield src + \"\\n\" \n        g = gen ( ) \n        step = 0 \n        tokens = [ ] \n        for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : \n            if step == 0 : \n                if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : \n                    step = 1 \n                    tokens = [ tok ] \n                    level = 0 \n            elif step == 1 : \n                if tok [ 0 ] == tokenize . NAME : \n                    tokens . append ( tok ) \n                    step = 2 \n                else : \n                    step = 0 \n            elif step == 2 : \n                if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : \n                    tokens . append ( tok ) \n                    step = 3 \n                else : \n                    step = 0 \n            elif step == 3 : \n                if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : \n                    yield tokenize . untokenize ( tokens ) . strip ( ) \n                    step = 0 \n                else : \n                    tokens . append ( tok ) \n                    if tok [ 0 ] == tokenize . OP : \n                        if tok [ 1 ] in \"[({\" : \n                            level += 1 \n                        if tok [ 1 ] in \"])}\" : \n                            level -= 1 \n        assert not tokens \n    actual_code = lambda_fn . __code__ . co_code \n    for lambda_src in gen_lambdas ( ) : \n        try : \n            fn = eval ( lambda_src , globals ( ) , locals ( ) ) \n            if fn . __code__ . co_code == actual_code : \n                return lambda_src . split ( \":\" , 1 ) [ 1 ] . strip ( ) \n        except Exception : \n            pass \n    return_identifier_0 = \"<lambda>\" \n    return return_identifier_0 "}
{"1372": "\ndef check ( self , var ) : \n    if not isinstance ( var , _str_type ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return _enum_mangle ( var ) in self . _consts "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    for _ in range ( 20 ) : \n        if v == 0 : \n            return_identifier_0 = 1e20 \n            return return_identifier_0 \n        t += ( x_target - x ) / v \n        x , v = self . _compute_progress_at_time ( t ) \n        if abs ( x - x_target ) < 1e-3 : \n            return t \n    return time . time ( ) + 100 "}
{"1385": "\ndef _get_terminal_size ( ) : \n    if not sys . stdout . isatty ( ) : \n        return_identifier_0 = 80 \n        return return_identifier_0 \n    try : \n        import subprocess \n        ret = subprocess . check_output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) \n        if len ( ret ) == 2 : \n            return int ( ret [ 1 ] ) \n    except : \n        pass \n    try : \n        from termios import TIOCGWINSZ \n        from fcntl import ioctl \n        from struct import unpack \n        res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) \n        return int ( res [ 1 ] ) \n    except : \n        pass \n    return int ( os . environ . get ( \"COLUMNS\" , 80 ) ) "}
{"1477": "\ndef get_human_readable_bytes ( size ) : \n    if size == 0 : \n        return_identifier_0 = \"0\" \n        return return_identifier_0 \n    if size is None : \n        return_identifier_1 = \"\" \n        return return_identifier_1 \n    assert_is_type ( size , int ) \n    assert size >= 0 , \"`size` cannot be negative, got %d\" % size \n    suffixes = \"PTGMk\" \n    maxl = len ( suffixes ) \n    for i in range ( maxl + 1 ) : \n        shift = ( maxl - i ) * 10 \n        if size >> shift == 0 : \n            continue \n        ndigits = 0 \n        for nd in [ 3 , 2 , 1 ] : \n            if size >> ( shift + 12 - nd * 3 ) == 0 : \n                ndigits = nd \n                break \n        if ndigits == 0 or size == ( size >> shift ) << shift : \n            rounded_val = str ( size >> shift ) \n        else : \n            rounded_val = \"%.*f\" % ( ndigits , size / ( 1 << shift ) ) \n        return \"%s %sb\" % ( rounded_val , suffixes [ i ] if i < maxl else \"\" ) "}
{"1502": "\ndef _prepare_file_payload ( filename ) : \n    if not filename : \n        return None \n    absfilename = os . path . abspath ( filename ) \n    if not os . path . exists ( absfilename ) : \n        raise H2OValueError ( \"File %s does not exist\" % filename , skip_frames = 1 ) \n    return_identifier_0 = { os . path . basename ( absfilename ) : open ( absfilename , \"rb\" ) } \n    return return_identifier_0 "}
{"1515": "\ndef find_node_name ( each_line , temp_func_list ) : \n    global g_node_name \n    global g_failed_test_info_dict \n    if g_node_name in each_line : \n        temp_strings = each_line . split ( ) \n        [ start , found , endstr ] = each_line . partition ( g_node_name ) \n        if found : \n            temp_strings = endstr . split ( ) \n            g_failed_test_info_dict [ \"6.node_name\" ] = extract_true_string ( temp_strings [ 1 ] ) \n            temp_func_list . remove ( find_node_name ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"1516": "\ndef find_git_hash_branch ( each_line , temp_func_list ) : \n    global g_git_hash_branch \n    global g_failed_test_info_dict \n    if g_git_hash_branch in each_line : \n        [ start , found , endstr ] = each_line . partition ( g_git_hash_branch ) \n        temp_strings = endstr . strip ( ) . split ( ) \n        if len ( temp_strings ) > 1 : \n            g_failed_test_info_dict [ \"4.git_hash\" ] = temp_strings [ 0 ] \n            g_failed_test_info_dict [ \"5.git_branch\" ] = temp_strings [ 1 ] \n        temp_func_list . remove ( find_git_hash_branch ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"1517": "\ndef find_build_timeout ( each_line , temp_func_list ) : \n    global g_build_timeout \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    if g_build_timeout in each_line : \n        g_failed_test_info_dict [ \"8.build_timeout\" ] = 'Yes' \n        g_failure_occurred = True \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"1518": "\ndef find_build_failure ( each_line , temp_func_list ) : \n    global g_build_success \n    global g_build_success_tests \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    global g_build_failed_message \n    for ind in range ( 0 , len ( g_build_failed_message ) ) : \n        if g_build_failed_message [ ind ] in each_line . lower ( ) : \n            if ( ( ind == 0 ) and ( len ( g_failed_jobs ) > 0 ) ) : \n                continue \n            else : \n                g_failure_occurred = True \n                g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' \n                temp_func_list . remove ( find_build_failure ) \n                return_identifier_0 = False \n                return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"1519": "\ndef find_build_id ( each_line , temp_func_list ) : \n    global g_before_java_file \n    global g_java_filenames \n    global g_build_id_text \n    global g_jenkins_url \n    global g_output_filename \n    global g_output_pickle_filename \n    if g_build_id_text in each_line : \n        [ startStr , found , endStr ] = each_line . partition ( g_build_id_text ) \n        g_failed_test_info_dict [ \"2.build_id\" ] = endStr . strip ( ) \n        temp_func_list . remove ( find_build_id ) \n        g_jenkins_url = os . path . join ( 'http://' , g_jenkins_url , 'view' , g_view_name , 'job' , g_failed_test_info_dict [ \"1.jobName\" ] , g_failed_test_info_dict [ \"2.build_id\" ] , 'artifact' ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"1564": "\ndef to_list ( self ) : \n    return_identifier_0 = [ [ int ( self . table . cell_values [ 0 ] [ 1 ] ) , int ( self . table . cell_values [ 0 ] [ 2 ] ) ] , [ int ( self . table . cell_values [ 1 ] [ 1 ] ) , int ( self . table . cell_values [ 1 ] [ 2 ] ) ] ] \n    return return_identifier_0 "}
{"1581": "\ndef available ( ) : \n    builder_json = h2o . api ( \"GET /3/ModelBuilders\" , data = { \"algo\" : \"deepwater\" } ) \n    visibility = builder_json [ \"model_builders\" ] [ \"deepwater\" ] [ \"visibility\" ] \n    if visibility == \"Experimental\" : \n        print ( \"Cannot build a Deep Water model - no backend found.\" ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"1586": "\ndef comment_user ( self , user_id , amount = None ) : \n    if not self . check_user ( user_id , filter_closed_acc = True ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . logger . info ( \"Going to comment user_%s's feed:\" % user_id ) \n    user_id = self . convert_to_user_id ( user_id ) \n    medias = self . get_user_medias ( user_id , is_comment = True ) \n    if not medias : \n        self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    return self . comment_medias ( medias [ : amount ] ) "}
{"1588": "\ndef like_user ( self , user_id , amount = None , filtration = True ) : \n    if filtration : \n        if not self . check_user ( user_id ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    self . logger . info ( \"Liking user_%s's feed:\" % user_id ) \n    user_id = self . convert_to_user_id ( user_id ) \n    medias = self . get_user_medias ( user_id , filtration = filtration ) \n    if not medias : \n        self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    return self . like_medias ( medias [ : amount ] ) "}
{"1590": "\ndef check_not_bot ( self , user_id ) : \n    self . small_delay ( ) \n    user_id = self . convert_to_user_id ( user_id ) \n    if not user_id : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if user_id in self . whitelist : \n        return_identifier_1 = True \n        return return_identifier_1 \n    if user_id in self . blacklist : \n        return_identifier_2 = False \n        return return_identifier_2 \n    user_info = self . get_user_info ( user_id ) \n    if not user_info : \n        return_identifier_3 = True \n        return return_identifier_3 \n    skipped = self . skipped_file \n    if \"following_count\" in user_info and user_info [ \"following_count\" ] > self . max_following_to_block : \n        msg = 'following_count > bot.max_following_to_block, skipping!' \n        self . console_print ( msg , 'red' ) \n        skipped . append ( user_id ) \n        return_identifier_4 = False \n        return return_identifier_4 \n    if search_stop_words_in_user ( self , user_info ) : \n        msg = '`bot.search_stop_words_in_user` found in user, skipping!' \n        skipped . append ( user_id ) \n        return_identifier_5 = False \n        return return_identifier_5 \n    return_identifier_6 = True \n    return return_identifier_6 "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if sys . version_info [ 0 ] < 3 : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        print ( str ( exception ) ) \n        return_identifier_1 = [ ] \n        return return_identifier_1 "}
{"1595": "\ndef guess_service_info_from_path ( spec_path ) : \n    spec_path = spec_path . lower ( ) \n    spec_path = spec_path [ spec_path . index ( \"specification\" ) : ] \n    split_spec_path = spec_path . split ( \"/\" ) \n    rp_name = split_spec_path [ 1 ] \n    is_arm = split_spec_path [ 2 ] == \"resource-manager\" \n    return_identifier_0 = { \"rp_name\" : rp_name , \"is_arm\" : is_arm } \n    return return_identifier_0 "}
{"1618": "\ndef response_body ( self ) : \n    var_respbody = VARIANT ( ) \n    _WinHttpRequest . _ResponseBody ( self , byref ( var_respbody ) ) \n    if var_respbody . is_safearray_of_bytes ( ) : \n        respbody = var_respbody . str_from_safearray ( ) \n        return respbody \n    else : \n        return_identifier_0 = '' \n        return return_identifier_0 "}
{"1629": "\ndef get_children_from_path ( node , * path ) : \n    cur = node \n    for index , child in enumerate ( path ) : \n        if isinstance ( child , _strtype ) : \n            next = _MinidomXmlToObject . get_child_nodes ( cur , child ) \n        else : \n            next = _MinidomXmlToObject . _get_child_nodesNS ( cur , * child ) \n        if index == len ( path ) - 1 : \n            return next \n        elif not next : \n            break \n        cur = next [ 0 ] \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"1630": "\ndef _find_namespaces_from_child ( parent , child , namespaces ) : \n    for cur_child in parent . childNodes : \n        if cur_child is child : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if _MinidomXmlToObject . _find_namespaces_from_child ( cur_child , child , namespaces ) : \n            for key in cur_child . attributes . keys ( ) : \n                if key . startswith ( 'xmlns:' ) or key == 'xmlns' : \n                    namespaces [ key ] = cur_child . attributes [ key ] \n            break \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"1739": "\ndef travis_build_package ( ) : \n    travis_tag = os . environ . get ( 'TRAVIS_TAG' ) \n    if not travis_tag : \n        print ( \"TRAVIS_TAG environment variable is not present\" ) \n        return_identifier_0 = \"TRAVIS_TAG environment variable is not present\" \n        return return_identifier_0 \n    try : \n        name , version = travis_tag . split ( \"_\" ) \n    except ValueError : \n        print ( \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\" . format ( travis_tag ) ) \n        return \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\" . format ( travis_tag ) \n    try : \n        version = Version ( version ) \n    except InvalidVersion : \n        print ( \"Version must be a valid PEP440 version (version is: {})\" . format ( version ) ) \n        return \"Version must be a valid PEP440 version (version is: {})\" . format ( version ) \n    if name . lower ( ) in OMITTED_RELEASE_PACKAGES : \n        print ( \"The input package {} has been disabled for release from Travis.CI.\" . format ( name ) ) \n        return \n    abs_dist_path = Path ( os . environ [ 'TRAVIS_BUILD_DIR' ] , 'dist' ) \n    create_package ( name , str ( abs_dist_path ) ) \n    print ( \"Produced:\\n{}\" . format ( list ( abs_dist_path . glob ( '*' ) ) ) ) \n    pattern = \"*{}*\" . format ( version ) \n    packages = list ( abs_dist_path . glob ( pattern ) ) \n    if not packages : \n        return \"Package version does not match tag {}, abort\" . format ( version ) \n    pypi_server = os . environ . get ( \"PYPI_SERVER\" , \"default PyPI server\" ) \n    print ( \"Package created as expected and will be pushed to {}\" . format ( pypi_server ) ) "}
{"1779": "\ndef create_queue ( self , queue_name , queue = None , fail_on_exist = False ) : \n    _validate_not_none ( 'queue_name' , queue_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( queue_name ) + '' \n    request . body = _get_request_body ( _convert_queue_to_xml ( queue ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        self . _perform_request ( request ) \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"1780": "\ndef delete_queue ( self , queue_name , fail_not_exist = False ) : \n    _validate_not_none ( 'queue_name' , queue_name ) \n    request = HTTPRequest ( ) \n    request . method = 'DELETE' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( queue_name ) + '' \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_not_exist : \n        try : \n            self . _perform_request ( request ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except AzureHttpError as ex : \n            _dont_fail_not_exist ( ex ) \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        self . _perform_request ( request ) \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"1782": "\ndef create_topic ( self , topic_name , topic = None , fail_on_exist = False ) : \n    _validate_not_none ( 'topic_name' , topic_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( topic_name ) + '' \n    request . body = _get_request_body ( _convert_topic_to_xml ( topic ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        self . _perform_request ( request ) \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"1784": "\ndef create_rule ( self , topic_name , subscription_name , rule_name , rule = None , fail_on_exist = False ) : \n    _validate_not_none ( 'topic_name' , topic_name ) \n    _validate_not_none ( 'subscription_name' , subscription_name ) \n    _validate_not_none ( 'rule_name' , rule_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( topic_name ) + '/subscriptions/' + _str ( subscription_name ) + '/rules/' + _str ( rule_name ) + '' \n    request . body = _get_request_body ( _convert_rule_to_xml ( rule ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        self . _perform_request ( request ) \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"1787": "\ndef create_subscription ( self , topic_name , subscription_name , subscription = None , fail_on_exist = False ) : \n    _validate_not_none ( 'topic_name' , topic_name ) \n    _validate_not_none ( 'subscription_name' , subscription_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( topic_name ) + '/subscriptions/' + _str ( subscription_name ) + '' \n    request . body = _get_request_body ( _convert_subscription_to_xml ( subscription ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        self . _perform_request ( request ) \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"1796": "\ndef create_event_hub ( self , hub_name , hub = None , fail_on_exist = False ) : \n    _validate_not_none ( 'hub_name' , hub_name ) \n    request = HTTPRequest ( ) \n    request . method = 'PUT' \n    request . host = self . _get_host ( ) \n    request . path = '/' + _str ( hub_name ) + '?api-version=2014-01' \n    request . body = _get_request_body ( _convert_event_hub_to_xml ( hub ) ) \n    request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) \n    request . headers = self . _update_service_bus_header ( request ) \n    if not fail_on_exist : \n        try : \n            self . _perform_request ( request ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except AzureHttpError as ex : \n            _dont_fail_on_exist ( ex ) \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        self . _perform_request ( request ) \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"1835": "\ndef expired ( self ) : \n    if self . locked_until and self . locked_until <= datetime . datetime . now ( ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"1905": "\ndef determine_json_encoding ( cls , sample ) : \n    nulls_at = [ i for i , j in enumerate ( bytearray ( sample [ : 4 ] ) ) if j == 0 ] \n    if nulls_at == [ 0 , 1 , 2 ] : \n        return_identifier_0 = \"UTF-32BE\" \n        return return_identifier_0 \n    elif nulls_at == [ 0 , 2 ] : \n        return_identifier_1 = \"UTF-16BE\" \n        return return_identifier_1 \n    elif nulls_at == [ 1 , 2 , 3 ] : \n        return_identifier_2 = \"UTF-32LE\" \n        return return_identifier_2 \n    elif nulls_at == [ 1 , 3 ] : \n        return_identifier_3 = \"UTF-16LE\" \n        return return_identifier_3 \n    else : \n        return_identifier_4 = \"UTF-8\" \n        return return_identifier_4 "}
{"1923": "\ndef output_stream_passthrough ( plugin , stream ) : \n    global output \n    title = create_title ( plugin ) \n    filename = '\"{0}\"' . format ( stream_to_url ( stream ) ) \n    output = PlayerOutput ( args . player , args = args . player_args , filename = filename , call = True , quiet = not args . verbose_player , title = title ) \n    try : \n        log . info ( \"Starting player: {0}\" , args . player ) \n        output . open ( ) \n    except OSError as err : \n        console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"1925": "\ndef output_stream ( plugin , stream ) : \n    global output \n    success_open = False \n    for i in range ( args . retry_open ) : \n        try : \n            stream_fd , prebuffer = open_stream ( stream ) \n            success_open = True \n            break \n        except StreamError as err : \n            log . error ( \"Try {0}/{1}: Could not open stream {2} ({3})\" , i + 1 , args . retry_open , stream , err ) \n    if not success_open : \n        console . exit ( \"Could not open stream {0}, tried {1} times, exiting\" , stream , args . retry_open ) \n    output = create_output ( plugin ) \n    try : \n        output . open ( ) \n    except ( IOError , OSError ) as err : \n        if isinstance ( output , PlayerOutput ) : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n        else : \n            console . exit ( \"Failed to open output: {0} ({1})\" , args . output , err ) \n    with closing ( output ) : \n        log . debug ( \"Writing stream to output\" ) \n        read_stream ( stream_fd , output , prebuffer ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"1951": "\ndef startswith ( string ) : \n    def starts_with ( value ) : \n        validate ( text , value ) \n        if not value . startswith ( string ) : \n            raise ValueError ( \"'{0}' does not start with '{1}'\" . format ( value , string ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return starts_with "}
{"1952": "\ndef endswith ( string ) : \n    def ends_with ( value ) : \n        validate ( text , value ) \n        if not value . endswith ( string ) : \n            raise ValueError ( \"'{0}' does not end with '{1}'\" . format ( value , string ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return ends_with "}
{"1953": "\ndef contains ( string ) : \n    def contains_str ( value ) : \n        validate ( text , value ) \n        if string not in value : \n            raise ValueError ( \"'{0}' does not contain '{1}'\" . format ( value , string ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return contains_str "}
{"1957": "\ndef url ( ** attributes ) : \n    def check_url ( value ) : \n        validate ( text , value ) \n        parsed = urlparse ( value ) \n        if not parsed . netloc : \n            raise ValueError ( \"'{0}' is not a valid URL\" . format ( value ) ) \n        for name , schema in attributes . items ( ) : \n            if not _hasattr ( parsed , name ) : \n                raise ValueError ( \"Invalid URL attribute '{0}'\" . format ( name ) ) \n            try : \n                validate ( schema , _getattr ( parsed , name ) ) \n            except ValueError as err : \n                raise ValueError ( \"Unable to validate URL attribute '{0}': {1}\" . format ( name , err ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    if attributes . get ( \"scheme\" ) == \"http\" : \n        attributes [ \"scheme\" ] = any ( \"http\" , \"https\" ) \n    return check_url "}
{"1963": "\ndef dologin ( self , email , password , emailauth = \"\" , emailsteamid = \"\" , captchagid = \"-1\" , captcha_text = \"\" , twofactorcode = \"\" ) : \n    epassword , rsatimestamp = self . encrypt_password ( email , password ) \n    login_data = { 'username' : email , \"password\" : epassword , \"emailauth\" : emailauth , \"loginfriendlyname\" : \"Streamlink\" , \"captchagid\" : captchagid , \"captcha_text\" : captcha_text , \"emailsteamid\" : emailsteamid , \"rsatimestamp\" : rsatimestamp , \"remember_login\" : True , \"donotcache\" : self . donotcache , \"twofactorcode\" : twofactorcode } \n    res = self . session . http . post ( self . _dologin_url , data = login_data ) \n    resp = self . session . http . json ( res , schema = self . _dologin_schema ) \n    if not resp [ u\"success\" ] : \n        if resp . get ( u\"captcha_needed\" ) : \n            captchagid = resp [ u\"captcha_gid\" ] \n            log . error ( \"Captcha result required, open this URL to see the captcha: {}\" . format ( self . _captcha_url . format ( captchagid ) ) ) \n            try : \n                captcha_text = self . input_ask ( \"Captcha text\" ) \n            except FatalPluginError : \n                captcha_text = None \n            if not captcha_text : \n                return_identifier_0 = False \n                return return_identifier_0 \n        else : \n            if resp . get ( u\"emailauth_needed\" ) : \n                if not emailauth : \n                    try : \n                        emailauth = self . input_ask ( \"Email auth code required\" ) \n                    except FatalPluginError : \n                        emailauth = None \n                    if not emailauth : \n                        return_identifier_1 = False \n                        return return_identifier_1 \n                else : \n                    raise SteamLoginFailed ( \"Email auth key error\" ) \n            if resp . get ( u\"requires_twofactor\" ) : \n                try : \n                    twofactorcode = self . input_ask ( \"Two factor auth code required\" ) \n                except FatalPluginError : \n                    twofactorcode = None \n                if not twofactorcode : \n                    return_identifier_2 = False \n                    return return_identifier_2 \n            if resp . get ( u\"message\" ) : \n                raise SteamLoginFailed ( resp [ u\"message\" ] ) \n        return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u\"emailsteamid\" , u\"\" ) , captcha_text = captcha_text , captchagid = captchagid , twofactorcode = twofactorcode ) \n    elif resp . get ( \"login_complete\" ) : \n        return_identifier_3 = True \n        return return_identifier_3 \n    else : \n        log . error ( \"Something when wrong when logging in to Steam\" ) \n        return_identifier_4 = False \n        return return_identifier_4 "}
{"1966": "\ndef _login ( self , username , password ) : \n    self . logger . debug ( 'login ...' ) \n    res = self . session . http . get ( self . login_url ) \n    input_list = self . _input_re . findall ( res . text ) \n    if not input_list : \n        raise PluginError ( 'Missing input data on login website.' ) \n    data = { } \n    for _input_data in input_list : \n        try : \n            _input_name = self . _name_re . search ( _input_data ) . group ( 1 ) \n        except AttributeError : \n            continue \n        try : \n            _input_value = self . _value_re . search ( _input_data ) . group ( 1 ) \n        except AttributeError : \n            _input_value = '' \n        data [ _input_name ] = _input_value \n    login_data = { 'ctl00$Login1$UserName' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$LoginButton.x' : '0' , 'ctl00$Login1$LoginButton.y' : '0' } \n    data . update ( login_data ) \n    res = self . session . http . post ( self . login_url , data = data ) \n    for cookie in self . session . http . cookies : \n        self . _session_attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) \n    if self . _session_attributes . get ( 'ASP.NET_SessionId' ) and self . _session_attributes . get ( '.abportail1' ) : \n        self . logger . debug ( 'New session data' ) \n        self . set_expires_time_cache ( ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        self . logger . error ( 'Failed to login, check your username/password' ) \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"1973": "\ndef outputCharFormatter ( c ) : \n    if 32 < c < 127 : \n        return chr ( c ) \n    elif c == 10 : \n        return_identifier_0 = '\\\\n' \n        return return_identifier_0 \n    elif c == 13 : \n        return_identifier_1 = '\\\\r' \n        return return_identifier_1 \n    elif c == 32 : \n        return_identifier_2 = '\" \"' \n        return return_identifier_2 \n    else : \n        return '\\\\x{:02x}' . format ( c ) "}
{"1992": "\ndef metablockLength ( self ) : \n    self . MLEN = self . verboseRead ( MetablockLengthAlphabet ( ) ) \n    if self . MLEN : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . verboseRead ( ReservedAlphabet ( ) ) \n    MSKIP = self . verboseRead ( SkipLengthAlphabet ( ) ) \n    self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n    self . stream . pos += 8 * MSKIP \n    print ( \"Skipping to {:x}\" . format ( self . stream . pos >> 3 ) ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"2004": "\ndef value_counts ( self , dropna = False , dropnull = True , ascending = False , progress = False ) : \n    from pandas import Series \n    dtype = self . dtype \n    transient = self . transient or self . ds . filtered or self . ds . is_masked ( self . expression ) \n    if self . dtype == str_type and not transient : \n        ar = self . ds . columns [ self . expression ] \n        if not isinstance ( ar , ColumnString ) : \n            transient = True \n    counter_type = counter_type_from_dtype ( self . dtype , transient ) \n    counters = [ None ] * self . ds . executor . thread_pool . nthreads \n    def map ( thread_index , i1 , i2 , ar ) : \n        if counters [ thread_index ] is None : \n            counters [ thread_index ] = counter_type ( ) \n        if dtype == str_type : \n            previous_ar = ar \n            ar = _to_string_sequence ( ar ) \n            if not transient : \n                assert ar is previous_ar . string_sequence \n        if np . ma . isMaskedArray ( ar ) : \n            mask = np . ma . getmaskarray ( ar ) \n            counters [ thread_index ] . update ( ar , mask ) \n        else : \n            counters [ thread_index ] . update ( ar ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    def reduce ( a , b ) : \n        return a + b \n    self . ds . map_reduce ( map , reduce , [ self . expression ] , delay = False , progress = progress , name = 'value_counts' , info = True , to_numpy = False ) \n    counters = [ k for k in counters if k is not None ] \n    counter0 = counters [ 0 ] \n    for other in counters [ 1 : ] : \n        counter0 . merge ( other ) \n    value_counts = counter0 . extract ( ) \n    index = np . array ( list ( value_counts . keys ( ) ) ) \n    counts = np . array ( list ( value_counts . values ( ) ) ) \n    order = np . argsort ( counts ) \n    if not ascending : \n        order = order [ : : - 1 ] \n    counts = counts [ order ] \n    index = index [ order ] \n    if not dropna or not dropnull : \n        index = index . tolist ( ) \n        counts = counts . tolist ( ) \n        if not dropna and counter0 . nan_count : \n            index = [ np . nan ] + index \n            counts = [ counter0 . nan_count ] + counts \n        if not dropnull and counter0 . null_count : \n            index = [ 'null' ] + index \n            counts = [ counter0 . null_count ] + counts \n    return Series ( counts , index = index ) "}
{"2022": "\ndef _task ( self , task , progressbar = False ) : \n    if self . delay : \n        return self . executor . schedule ( task ) \n    else : \n        import vaex . utils \n        callback = None \n        try : \n            if progressbar == True : \n                def update ( fraction ) : \n                    bar . update ( fraction ) \n                    return_identifier_0 = True \n                    return return_identifier_0 \n                bar = vaex . utils . progressbar ( task . name ) \n                callback = self . executor . signal_progress . connect ( update ) \n            elif progressbar : \n                callback = self . executor . signal_progress . connect ( progressbar ) \n            result = self . executor . run ( task ) \n            if progressbar == True : \n                bar . finish ( ) \n                sys . stdout . write ( '\\n' ) \n            return result \n        finally : \n            if callback : \n                self . executor . signal_progress . disconnect ( callback ) "}
{"2045": "\ndef state_get ( self ) : \n    virtual_names = list ( self . virtual_columns . keys ( ) ) + list ( self . variables . keys ( ) ) \n    units = { key : str ( value ) for key , value in self . units . items ( ) } \n    ucds = { key : value for key , value in self . ucds . items ( ) if key in virtual_names } \n    descriptions = { key : value for key , value in self . descriptions . items ( ) } \n    import vaex . serialize \n    def check ( key , value ) : \n        if not vaex . serialize . can_serialize ( value . f ) : \n            warnings . warn ( 'Cannot serialize function for virtual column {} (use vaex.serialize.register)' . format ( key ) ) \n            return_identifier_0 = False \n            return return_identifier_0 \n        return_identifier_1 = True \n        return return_identifier_1 \n    def clean ( value ) : \n        return vaex . serialize . to_dict ( value . f ) \n    functions = { key : clean ( value ) for key , value in self . functions . items ( ) if check ( key , value ) } \n    virtual_columns = { key : value for key , value in self . virtual_columns . items ( ) } \n    selections = { name : self . get_selection ( name ) for name , history in self . selection_histories . items ( ) } \n    selections = { name : selection . to_dict ( ) if selection is not None else None for name , selection in selections . items ( ) } \n    state = dict ( virtual_columns = virtual_columns , column_names = self . column_names , renamed_columns = self . _renamed_columns , variables = self . variables , functions = functions , selections = selections , ucds = ucds , units = units , descriptions = descriptions , description = self . description , active_range = [ self . _index_start , self . _index_end ] ) \n    return state "}
{"2050": "\ndef subspaces ( self , expressions_list = None , dimensions = None , exclude = None , ** kwargs ) : \n    if dimensions is not None : \n        expressions_list = list ( itertools . combinations ( self . get_column_names ( ) , dimensions ) ) \n        if exclude is not None : \n            import six \n            def excluded ( expressions ) : \n                if callable ( exclude ) : \n                    return exclude ( expressions ) \n                elif isinstance ( exclude , six . string_types ) : \n                    return exclude in expressions \n                elif isinstance ( exclude , ( list , tuple ) ) : \n                    for e in exclude : \n                        if isinstance ( e , six . string_types ) : \n                            if e in expressions : \n                                return_identifier_0 = True \n                                return return_identifier_0 \n                        elif isinstance ( e , ( list , tuple ) ) : \n                            if set ( e ) . issubset ( expressions ) : \n                                return_identifier_1 = True \n                                return return_identifier_1 \n                        else : \n                            raise ValueError ( \"elements of exclude should contain a string or a sequence of strings\" ) \n                else : \n                    raise ValueError ( \"exclude should contain a string, a sequence of strings, or should be a callable\" ) \n                return_identifier_2 = False \n                return return_identifier_2 \n            expressions_list = [ expr for expr in expressions_list if not excluded ( expr ) ] \n        logger . debug ( \"expression list generated: %r\" , expressions_list ) \n    import vaex . legacy \n    return vaex . legacy . Subspaces ( [ self ( * expressions , ** kwargs ) for expressions in expressions_list ] ) "}
{"2077": "\ndef get_column_names ( self , virtual = True , strings = True , hidden = False , regex = None ) : \n    def column_filter ( name ) : \n        if regex and not re . match ( regex , name ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if not virtual and name in self . virtual_columns : \n            return_identifier_1 = False \n            return return_identifier_1 \n        if not strings and ( self . dtype ( name ) == str_type or self . dtype ( name ) . type == np . string_ ) : \n            return_identifier_2 = False \n            return return_identifier_2 \n        if not hidden and name . startswith ( '__' ) : \n            return_identifier_3 = False \n            return return_identifier_3 \n        return_identifier_4 = True \n        return return_identifier_4 \n    return [ name for name in self . column_names if column_filter ( name ) ] "}
{"2241": "\ndef __jaccard ( int_a , int_b ) : \n    ends = [ int_a [ 1 ] , int_b [ 1 ] ] \n    if ends [ 1 ] < ends [ 0 ] : \n        ends . reverse ( ) \n    starts = [ int_a [ 0 ] , int_b [ 0 ] ] \n    if starts [ 1 ] < starts [ 0 ] : \n        starts . reverse ( ) \n    intersection = ends [ 0 ] - starts [ 1 ] \n    if intersection < 0 : \n        intersection = 0. \n    union = ends [ 1 ] - starts [ 0 ] \n    if union > 0 : \n        return intersection / union \n    return_identifier_0 = 0.0 \n    return return_identifier_0 "}
{"2263": "\ndef beat_track ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , start_bpm = 120.0 , tightness = 100 , trim = True , bpm = None , units = 'frames' ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset . onset_strength ( y = y , sr = sr , hop_length = hop_length , aggregate = np . median ) \n    if not onset_envelope . any ( ) : \n        return_identifier_0 = ( 0 , np . array ( [ ] , dtype = int ) ) \n        return return_identifier_0 \n    if bpm is None : \n        bpm = tempo ( onset_envelope = onset_envelope , sr = sr , hop_length = hop_length , start_bpm = start_bpm ) [ 0 ] \n    beats = __beat_tracker ( onset_envelope , bpm , float ( sr ) / hop_length , tightness , trim ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        beats = core . frames_to_samples ( beats , hop_length = hop_length ) \n    elif units == 'time' : \n        beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return_identifier_1 = ( bpm , beats ) \n    return return_identifier_1 "}
{"2276": "\ndef valid_audio ( y , mono = True ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and y . ndim != 1 : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    elif y . ndim > 2 or y . ndim == 0 : \n        raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"2339": "\ndef is_seq_of ( seq , expected_type , seq_type = None ) : \n    if seq_type is None : \n        exp_seq_type = collections_abc . Sequence \n    else : \n        assert isinstance ( seq_type , type ) \n        exp_seq_type = seq_type \n    if not isinstance ( seq , exp_seq_type ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    for item in seq : \n        if not isinstance ( item , expected_type ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"2345": "\ndef check_time ( timer_id ) : \n    if timer_id not in _g_timers : \n        _g_timers [ timer_id ] = Timer ( ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    else : \n        return _g_timers [ timer_id ] . since_last_check ( ) "}
{"2389": "\ndef get_channel_id ( turn_context : TurnContext ) -> str : \n    if turn_context . activity . channel_id is None : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    else : \n        return turn_context . activity . channel_id "}
{"2390": "\ndef is_token_from_emulator ( auth_header : str ) -> bool : \n    if not auth_header : \n        return_identifier_0 = False \n        return return_identifier_0 \n    parts = auth_header . split ( ' ' ) \n    if len ( parts ) != 2 : \n        return_identifier_1 = False \n        return return_identifier_1 \n    auth_scheme = parts [ 0 ] \n    bearer_token = parts [ 1 ] \n    if auth_scheme != 'Bearer' : \n        return_identifier_2 = False \n        return return_identifier_2 \n    token = jwt . decode ( bearer_token , verify = False ) \n    if not token : \n        return_identifier_3 = False \n        return return_identifier_3 \n    issuer = token [ 'iss' ] \n    if not issuer : \n        return_identifier_4 = False \n        return return_identifier_4 \n    issuer_list = EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS . issuer \n    if issuer_list and not issuer in issuer_list : \n        return_identifier_5 = False \n        return return_identifier_5 \n    return_identifier_6 = True \n    return return_identifier_6 "}
{"2417": "\ndef compose_u3 ( theta1 , phi1 , lambda1 , theta2 , phi2 , lambda2 ) : \n    thetap , phip , lambdap = Optimize1qGates . yzy_to_zyz ( ( lambda1 + phi2 ) , theta1 , theta2 ) \n    ( theta , phi , lamb ) = ( thetap , phi1 + phip , lambda2 + lambdap ) \n    return_identifier_0 = ( theta , phi , lamb ) \n    return return_identifier_0 "}
{"2427": "\ndef _process_bit_id ( self , node ) : \n    reg = None \n    if node . name in self . dag . qregs : \n        reg = self . dag . qregs [ node . name ] \n    elif node . name in self . dag . cregs : \n        reg = self . dag . cregs [ node . name ] \n    else : \n        raise QiskitError ( \"expected qreg or creg name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    if node . type == \"indexed_id\" : \n        return_identifier_0 = [ ( reg , node . index ) ] \n        return return_identifier_0 \n    elif node . type == \"id\" : \n        if not self . bit_stack [ - 1 ] : \n            return [ ( reg , j ) for j in range ( reg . size ) ] \n        else : \n            if node . name in self . bit_stack [ - 1 ] : \n                return_identifier_1 = [ self . bit_stack [ - 1 ] [ node . name ] ] \n                return return_identifier_1 \n            raise QiskitError ( \"expected local bit name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    return None "}
{"2455": "\ndef entropy ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == 1 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    evals = np . maximum ( np . linalg . eigvalsh ( state ) , 0. ) \n    return shannon_entropy ( evals , base = np . e ) "}
{"2478": "\ndef is_square_matrix ( mat ) : \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    shape = mat . shape \n    return shape [ 0 ] == shape [ 1 ] "}
{"2479": "\ndef is_diagonal_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol ) "}
{"2480": "\ndef is_symmetric_matrix ( op , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( op ) \n    if mat . ndim != 2 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return np . allclose ( mat , mat . T , rtol = rtol , atol = atol ) "}
{"2481": "\ndef is_hermitian_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol ) "}
{"2482": "\ndef is_positive_semidefinite_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    if not is_hermitian_matrix ( mat , rtol = rtol , atol = atol ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    vals = np . linalg . eigvalsh ( mat ) \n    for v in vals : \n        if v < - atol : \n            return_identifier_1 = False \n            return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"2483": "\ndef is_identity_matrix ( mat , ignore_phase = False , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if ignore_phase : \n        theta = np . angle ( mat [ 0 , 0 ] ) \n        mat = np . exp ( - 1j * theta ) * mat \n    iden = np . eye ( len ( mat ) ) \n    return np . allclose ( mat , iden , rtol = rtol , atol = atol ) "}
{"2492": "\ndef _from_operator ( rep , data , input_dim , output_dim ) : \n    if rep == 'Operator' : \n        return data \n    if rep == 'SuperOp' : \n        return np . kron ( np . conj ( data ) , data ) \n    if rep == 'Choi' : \n        vec = np . ravel ( data , order = 'F' ) \n        return np . outer ( vec , np . conj ( vec ) ) \n    if rep == 'Kraus' : \n        return_identifier_0 = ( [ data ] , None ) \n        return return_identifier_0 \n    if rep == 'Stinespring' : \n        return_identifier_1 = ( data , None ) \n        return return_identifier_1 \n    if rep == 'Chi' : \n        _check_nqubit_dim ( input_dim , output_dim ) \n        data = _from_operator ( 'Choi' , data , input_dim , output_dim ) \n        return _choi_to_chi ( data , input_dim , output_dim ) \n    if rep == 'PTM' : \n        _check_nqubit_dim ( input_dim , output_dim ) \n        data = _from_operator ( 'SuperOp' , data , input_dim , output_dim ) \n        return _superop_to_ptm ( data , input_dim , output_dim ) \n    raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) ) "}
{"2497": "\ndef _choi_to_kraus ( data , input_dim , output_dim , atol = ATOL_DEFAULT ) : \n    if is_hermitian_matrix ( data , atol = atol ) : \n        w , v = la . eigh ( data ) \n        if len ( w [ w < - atol ] ) == 0 : \n            kraus = [ ] \n            for val , vec in zip ( w , v . T ) : \n                if abs ( val ) > atol : \n                    k = np . sqrt ( val ) * vec . reshape ( ( output_dim , input_dim ) , order = 'F' ) \n                    kraus . append ( k ) \n            if not kraus : \n                kraus . append ( np . zeros ( ( output_dim , input_dim ) , dtype = complex ) ) \n            return_identifier_0 = ( kraus , None ) \n            return return_identifier_0 \n    mat_u , svals , mat_vh = la . svd ( data ) \n    kraus_l = [ ] \n    kraus_r = [ ] \n    for val , vec_l , vec_r in zip ( svals , mat_u . T , mat_vh . conj ( ) ) : \n        kraus_l . append ( np . sqrt ( val ) * vec_l . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n        kraus_r . append ( np . sqrt ( val ) * vec_r . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n    return_identifier_1 = ( kraus_l , kraus_r ) \n    return return_identifier_1 "}
{"2526": "\ndef _has_connection ( hostname , port ) : \n    try : \n        host = socket . gethostbyname ( hostname ) \n        socket . create_connection ( ( host , port ) , 2 ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except Exception : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"2557": "\ndef _search_forward_n_swaps ( layout , gates , coupling_map , depth = SEARCH_DEPTH , width = SEARCH_WIDTH ) : \n    gates_mapped , gates_remaining = _map_free_gates ( layout , gates , coupling_map ) \n    base_step = { 'layout' : layout , 'swaps_added' : 0 , 'gates_mapped' : gates_mapped , 'gates_remaining' : gates_remaining } \n    if not gates_remaining or depth == 0 : \n        return base_step \n    possible_swaps = coupling_map . get_edges ( ) \n    def _score_swap ( swap ) : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        return _calc_layout_distance ( gates , coupling_map , trial_layout ) \n    ranked_swaps = sorted ( possible_swaps , key = _score_swap ) \n    best_swap , best_step = None , None \n    for swap in ranked_swaps [ : width ] : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        next_step = _search_forward_n_swaps ( trial_layout , gates_remaining , coupling_map , depth - 1 , width ) \n        if best_swap is None or _score_step ( next_step ) > _score_step ( best_step ) : \n            best_swap , best_step = swap , next_step \n    best_swap_gate = _swap_ops_from_edge ( best_swap , layout ) \n    return_identifier_0 = { 'layout' : best_step [ 'layout' ] , 'swaps_added' : 1 + best_step [ 'swaps_added' ] , 'gates_remaining' : best_step [ 'gates_remaining' ] , 'gates_mapped' : gates_mapped + best_swap_gate + best_step [ 'gates_mapped' ] , } \n    return return_identifier_0 "}
{"2563": "\ndef _swap_ops_from_edge ( edge , layout ) : \n    device_qreg = QuantumRegister ( len ( layout . get_physical_bits ( ) ) , 'q' ) \n    qreg_edge = [ ( device_qreg , i ) for i in edge ] \n    return_identifier_0 = [ DAGNode ( { 'op' : SwapGate ( ) , 'qargs' : qreg_edge , 'cargs' : [ ] , 'type' : 'op' } ) ] \n    return return_identifier_0 "}
{"2569": "\ndef is_connected ( self ) : \n    try : \n        return nx . is_weakly_connected ( self . graph ) \n    except nx . exception . NetworkXException : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"2577": "\ndef subscribe ( self , event , callback ) : \n    if not callable ( callback ) : \n        raise QiskitError ( \"Callback is not a callable!\" ) \n    if event not in self . _subscribers : \n        self . _subscribers [ event ] = [ ] \n    new_subscription = self . _Subscription ( event , callback ) \n    if new_subscription in self . _subscribers [ event ] : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _subscribers [ event ] . append ( new_subscription ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"2579": "\ndef unsubscribe ( self , event , callback ) : \n    try : \n        self . _subscribers [ event ] . remove ( self . _Subscription ( event , callback ) ) \n    except KeyError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"2624": "\ndef _is_bit ( obj ) : \n    if isinstance ( obj , tuple ) and len ( obj ) == 2 : \n        if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"2626": "\ndef has_overlap ( self , interval : 'Interval' ) -> bool : \n    if self . begin < interval . end and interval . begin < self . end : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"2629": "\ndef ch_start_time ( self , * channels : List [ Channel ] ) -> int : \n    intervals = list ( itertools . chain ( * ( self . _table [ chan ] for chan in channels if chan in self . _table ) ) ) \n    if intervals : \n        return min ( ( interval . begin for interval in intervals ) ) \n    return_identifier_0 = 0 \n    return return_identifier_0 "}
{"2630": "\ndef ch_stop_time ( self , * channels : List [ Channel ] ) -> int : \n    intervals = list ( itertools . chain ( * ( self . _table [ chan ] for chan in channels if chan in self . _table ) ) ) \n    if intervals : \n        return max ( ( interval . end for interval in intervals ) ) \n    return_identifier_0 = 0 \n    return return_identifier_0 "}
{"2631": "\ndef is_mergeable_with ( self , timeslots : 'TimeslotCollection' ) -> bool : \n    for slot in timeslots . timeslots : \n        for interval in self . _table [ slot . channel ] : \n            if slot . interval . has_overlap ( interval ) : \n                return_identifier_0 = False \n                return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"2666": "\ndef includes ( self , lo_freq : float ) -> bool : \n    if self . _lb <= lo_freq <= self . _ub : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"2672": "\ndef n_choose_k ( n , k ) : \n    if n == 0 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    return reduce ( lambda x , y : x * y [ 0 ] / y [ 1 ] , zip ( range ( n - k + 1 , n + 1 ) , range ( 1 , k + 1 ) ) , 1 ) "}
{"2731": "\ndef tomography_set ( meas_qubits , meas_basis = 'Pauli' , prep_qubits = None , prep_basis = None ) : \n    if not isinstance ( meas_qubits , list ) : \n        raise QiskitError ( 'Qubits argument must be a list' ) \n    num_of_qubits = len ( meas_qubits ) \n    if prep_qubits is None : \n        prep_qubits = meas_qubits \n    if not isinstance ( prep_qubits , list ) : \n        raise QiskitError ( 'prep_qubits argument must be a list' ) \n    if len ( prep_qubits ) != len ( meas_qubits ) : \n        raise QiskitError ( 'meas_qubits and prep_qubitsare different length' ) \n    if isinstance ( meas_basis , str ) : \n        if meas_basis . lower ( ) == 'pauli' : \n            meas_basis = PAULI_BASIS \n    if isinstance ( prep_basis , str ) : \n        if prep_basis . lower ( ) == 'pauli' : \n            prep_basis = PAULI_BASIS \n        elif prep_basis . lower ( ) == 'sic' : \n            prep_basis = SIC_BASIS \n    circuits = [ ] \n    circuit_labels = [ ] \n    if prep_basis is None : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'meas' : meas } ) \n            label = '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ 0 ] , qubit ) \n            circuit_labels . append ( label ) \n        return_identifier_0 = { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'meas_basis' : meas_basis } \n        return return_identifier_0 \n    num_of_s = len ( list ( prep_basis . values ( ) ) [ 0 ] ) \n    plst_single = [ ( b , s ) for b in prep_basis . keys ( ) for s in range ( num_of_s ) ] \n    for plst_product in product ( plst_single , repeat = num_of_qubits ) : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            prep = dict ( zip ( prep_qubits , plst_product ) ) \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'prep' : prep , 'meas' : meas } ) \n            label = '_prep_' \n            for qubit , op in prep . items ( ) : \n                label += '%s%d(%d)' % ( op [ 0 ] , op [ 1 ] , qubit ) \n            label += '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ 0 ] , qubit ) \n            circuit_labels . append ( label ) \n    return_identifier_1 = { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'prep_basis' : prep_basis , 'meas_basis' : meas_basis } \n    return return_identifier_1 "}
{"2788": "\ndef find_column ( self , input_ , token ) : \n    if token is None : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    last_cr = input_ . rfind ( '\\n' , 0 , token . lexpos ) \n    if last_cr < 0 : \n        last_cr = 0 \n    column = ( token . lexpos - last_cr ) + 1 \n    return column "}
{"2796": "\ndef purity ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == 1 : \n        return_identifier_0 = 1.0 \n        return return_identifier_0 \n    return np . real ( np . trace ( rho . dot ( rho ) ) ) "}
{"2803": "\ndef _bipartite_shape ( self ) : \n    return_identifier_0 = ( self . _input_dim , self . _output_dim , self . _input_dim , self . _output_dim ) \n    return return_identifier_0 "}
{"2805": "\ndef _truncate_float ( matchobj , format_str = '0.2g' ) : \n    if matchobj . group ( 0 ) : \n        return format ( float ( matchobj . group ( 0 ) ) , format_str ) \n    return_identifier_0 = '' \n    return return_identifier_0 "}
{"2808": "\ndef _get_beamer_page ( self ) : \n    PIL_limit = 40000 \n    beamer_limit = 550 \n    aspect_ratio = self . sum_row_heights / self . sum_column_widths \n    margin_factor = 1.5 \n    height = min ( self . sum_row_heights * margin_factor , beamer_limit ) \n    width = min ( self . sum_column_widths * margin_factor , beamer_limit ) \n    if height * width > PIL_limit : \n        height = min ( np . sqrt ( PIL_limit * aspect_ratio ) , beamer_limit ) \n        width = min ( np . sqrt ( PIL_limit / aspect_ratio ) , beamer_limit ) \n    height = max ( height , 10 ) \n    width = max ( width , 10 ) \n    return_identifier_0 = ( height , width , self . scale ) \n    return return_identifier_0 "}
{"2841": "\ndef _automatic_dims ( cls , dims , size ) : \n    if dims is None : \n        dims = size \n    elif np . product ( dims ) != size : \n        raise QiskitError ( \"dimensions do not match size.\" ) \n    if isinstance ( dims , ( int , np . integer ) ) : \n        num_qubits = int ( np . log2 ( dims ) ) \n        if 2 ** num_qubits == size : \n            return num_qubits * ( 2 , ) \n        return_identifier_0 = ( dims , ) \n        return return_identifier_0 \n    return tuple ( dims ) "}
{"2850": "\ndef is_unitary ( self , atol = None , rtol = None ) : \n    try : \n        op = self . to_operator ( ) \n        return op . is_unitary ( atol = atol , rtol = rtol ) \n    except QiskitError : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"2858": "\ndef are_tokens_valid ( self , tokens ) : \n    match_relative_regex = self . _get_match_relative_regex_cache ( ) \n    for token in tokens : \n        if any ( [ match_relative_regex . match ( token ) , token in self , token . isdigit ( ) ] ) : \n            continue \n        else : \n            return_identifier_0 = False \n            return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"2864": "\ndef parse_with_formats ( date_string , date_formats , settings ) : \n    period = 'day' \n    for date_format in date_formats : \n        try : \n            date_obj = datetime . strptime ( date_string , date_format ) \n        except ValueError : \n            continue \n        else : \n            if '%d' not in date_format : \n                period = 'month' \n                date_obj = date_obj . replace ( day = get_last_day_of_month ( date_obj . year , date_obj . month ) ) \n            if not ( '%y' in date_format or '%Y' in date_format ) : \n                today = datetime . today ( ) \n                date_obj = date_obj . replace ( year = today . year ) \n            date_obj = apply_timezone_from_settings ( date_obj , settings ) \n            return_identifier_0 = { 'date_obj' : date_obj , 'period' : period } \n            return return_identifier_0 \n    else : \n        return_identifier_1 = { 'date_obj' : None , 'period' : period } \n        return return_identifier_1 "}
{"2866": "\ndef _exc_to_net ( param1 , success ) : \n    if len ( param1 ) <= 3 : \n        if success : \n            return_identifier_0 = 0 \n            return return_identifier_0 \n        else : \n            return_identifier_1 = 314 \n            return return_identifier_1 \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return KNOWN_EXC [ exc ] \n    else : \n        logger . warning ( \"Unknown Java exception, consider adding it to dictionary: %s\" , param1 ) \n        return_identifier_2 = 41 \n        return return_identifier_2 "}
{"2867": "\ndef _exc_to_http ( param1 ) : \n    if len ( param1 ) <= 3 : \n        try : \n            int ( param1 ) \n        except BaseException : \n            logger . error ( \"JMeter wrote some strange data into codes column: %s\" , param1 ) \n        else : \n            return int ( param1 ) \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    else : \n        logger . warning ( \"Unknown Java exception. %s\" , param1 ) \n        return_identifier_1 = 0 \n        return return_identifier_1 "}
{"2881": "\ndef rps_at ( self , t ) : \n    if 0 <= t <= self . duration : \n        return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration \n    else : \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"2895": "\ndef get_marker ( marker_type , enum_ammo = False ) : \n    try : \n        limit = int ( marker_type ) \n        if limit : \n            marker = __UriMarker ( limit ) \n        else : \n            def marker ( m ) : \n                return_identifier_0 = '' \n                return return_identifier_0 \n    except ValueError : \n        if marker_type in __markers : \n            marker = __markers [ marker_type ] \n        else : \n            raise NotImplementedError ( 'No such marker: \"%s\"' % marker_type ) \n    if enum_ammo : \n        marker = __Enumerator ( marker ) \n    return marker "}
{"2914": "\ndef fill_rectangle ( self , prepared ) : \n    result = [ ] \n    width = max ( [ self . clean_len ( line ) for line in prepared ] ) \n    for line in prepared : \n        spacer = ' ' * ( width - self . clean_len ( line ) ) \n        result . append ( line + ( self . screen . markup . RESET , spacer ) ) \n    return_identifier_0 = ( width , result ) \n    return return_identifier_0 "}
{"2933": "\ndef poll ( self ) : \n    start_time = time . time ( ) \n    for agent in self . agents : \n        for collect in agent . reader : \n            if not collect : \n                return_identifier_0 = 0 \n                return return_identifier_0 \n            for chunk in collect : \n                ts , prepared_results = chunk \n                if self . load_start_time and int ( ts ) >= self . load_start_time : \n                    ready_to_send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash_hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared_results } } } \n                    self . __collected_data . append ( ready_to_send ) \n    logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000 ) \n    collected_data_length = len ( self . __collected_data ) \n    if not self . first_data_received and self . __collected_data : \n        self . first_data_received = True \n        logger . info ( \"Monitoring received first data.\" ) \n    else : \n        self . send_collected_data ( ) \n    return collected_data_length "}
{"2935": "\ndef __detect_configuration ( self ) : \n    try : \n        is_telegraf = self . core . get_option ( 'telegraf' , \"config\" ) \n    except KeyError : \n        is_telegraf = None \n    try : \n        is_monitoring = self . core . get_option ( 'monitoring' , \"config\" ) \n    except KeyError : \n        is_monitoring = None \n    if is_telegraf and is_monitoring : \n        raise ValueError ( 'Both telegraf and monitoring configs specified. ' 'Clean up your config and delete one of them' ) \n    if is_telegraf and not is_monitoring : \n        return_identifier_0 = 'telegraf' \n        return return_identifier_0 \n    if not is_telegraf and is_monitoring : \n        return_identifier_1 = 'monitoring' \n        return return_identifier_1 \n    if not is_telegraf and not is_monitoring : \n        try : \n            is_telegraf_dt = self . core . get_option ( 'telegraf' ) \n        except NoOptionError : \n            is_telegraf_dt = None \n        try : \n            is_monitoring_dt = self . core . get_option ( 'monitoring' ) \n        except BaseException : \n            is_monitoring_dt = None \n        if is_telegraf_dt and is_monitoring_dt : \n            raise ValueError ( 'Both telegraf and monitoring default targets specified. ' 'Clean up your config and delete one of them' ) \n        if is_telegraf_dt and not is_monitoring_dt : \n            return \n        if not is_telegraf_dt and is_monitoring_dt : \n            self . core . set_option ( \"telegraf\" , \"default_target\" , is_monitoring_dt ) \n        if not is_telegraf_dt and not is_monitoring_dt : \n            return "}
{"2937": "\ndef _decode_agents_data ( self , block ) : \n    collect = [ ] \n    if block : \n        for chunk in block . split ( '\\n' ) : \n            try : \n                if chunk : \n                    prepared_results = { } \n                    jsn = json . loads ( chunk ) \n                    for ts , values in jsn . iteritems ( ) : \n                        for key , value in values . iteritems ( ) : \n                            try : \n                                key_group , key_name = key . split ( '_' ) [ 0 ] . split ( '-' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) \n                            except : \n                                key_group , key_name = key . split ( '_' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) \n                            if key_group in decoder . diff_metrics . keys ( ) : \n                                if key_name in decoder . diff_metrics [ key_group ] : \n                                    decoded_key = decoder . find_common_names ( key ) \n                                    if self . prev_check : \n                                        try : \n                                            value = jsn [ ts ] [ key ] - self . prev_check [ key ] \n                                        except KeyError : \n                                            logger . debug ( 'There is no diff value for metric %s.\\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc_info = True ) \n                                            value = 0 \n                                        prepared_results [ decoded_key ] = value \n                                else : \n                                    decoded_key = decoder . find_common_names ( key ) \n                                    prepared_results [ decoded_key ] = value \n                            else : \n                                decoded_key = decoder . find_common_names ( key ) \n                                prepared_results [ decoded_key ] = value \n                        self . prev_check = jsn [ ts ] \n                        collect . append ( ( ts , prepared_results ) ) \n            except ValueError : \n                logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) \n                logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc_info = True ) \n                return_identifier_0 = [ ] \n                return return_identifier_0 \n            except BaseException : \n                logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc_info = True ) \n                return_identifier_1 = [ ] \n                return return_identifier_1 \n        if collect : \n            return collect "}
{"2949": "\ndef opt_dict_param ( obj , param_name , key_type = None , value_type = None , value_class = None ) : \n    if obj is not None and not isinstance ( obj , dict ) : \n        raise_with_traceback ( _param_type_mismatch_exception ( obj , dict , param_name ) ) \n    if not obj : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    if value_class : \n        return _check_key_value_types ( obj , key_type , value_type = value_class , value_check = issubclass ) \n    return _check_key_value_types ( obj , key_type , value_type ) "}
{"2959": "\ndef join ( self ) : \n    while True : \n        with self . _processes_lock : \n            if not self . _processes and self . _processing_semaphore . locked ( ) : \n                return_identifier_0 = True \n                return return_identifier_0 \n        gevent . sleep ( 0.1 ) "}
{"2967": "\ndef passthrough_context_definition ( context_params ) : \n    check . inst_param ( context_params , 'context' , ExecutionContext ) \n    context_definition = PipelineContextDefinition ( context_fn = lambda * _args : context_params ) \n    return_identifier_0 = { DEFAULT_CONTEXT_NAME : context_definition } \n    return return_identifier_0 "}
{"2977": "\ndef success ( self ) : \n    any_success = False \n    for step_event in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) : \n        if step_event . event_type == DagsterEventType . STEP_FAILURE : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if step_event . event_type == DagsterEventType . STEP_SUCCESS : \n            any_success = True \n    return any_success "}
{"2982": "\ndef PermissiveDict ( fields = None ) : \n    if fields : \n        check_user_facing_fields_dict ( fields , 'PermissiveDict' ) \n    class _PermissiveDict ( _ConfigComposite ) : \n        def __init__ ( self ) : \n            key = 'PermissiveDict.' + str ( DictCounter . get_next_count ( ) ) \n            super ( _PermissiveDict , self ) . __init__ ( name = None , key = key , fields = fields or dict ( ) , description = 'A configuration dictionary with typed fields' , type_attributes = ConfigTypeAttributes ( is_builtin = True ) , ) \n        \n        @ property \n        def is_permissive_composite ( self ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return _PermissiveDict "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if num_part < 2 : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if num_part > len ( sizes ) : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if num_part == 2 : \n        total_nfps , u = min ( ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( 0 , len ( sizes ) - 1 ) ) \n        return [ ( sizes [ 0 ] , sizes [ u ] ) , ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2 ) ) \n    p2i = lambda p : p - 2 \n    for p in range ( 2 , num_part ) : \n        for u in range ( p - 1 , len ( sizes ) ) : \n            if p == 2 : \n                cost [ u , p2i ( p ) ] = min ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , u ] for u1 in range ( ( p - 1 ) - 1 , u ) ) \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , len ( sizes ) - 1 ) ) \n    partitions = [ ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] \n    p -= 1 \n    while p > 1 : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + 1 , u ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , u ) ) \n        partitions . insert ( 0 , ( sizes [ u1_best + 1 ] , sizes [ u ] ) ) \n        u = u1_best \n        p -= 1 \n    partitions . insert ( 0 , ( sizes [ 0 ] , sizes [ u ] ) ) \n    return_identifier_0 = [ partitions , total_nfps , cost ] \n    return return_identifier_0 "}
{"3002": "\ndef optimal_partitions ( sizes , counts , num_part ) : \n    if num_part < 2 : \n        return_identifier_0 = [ ( sizes [ 0 ] , sizes [ - 1 ] ) ] \n        return return_identifier_0 \n    if num_part >= len ( sizes ) : \n        partitions = [ ( x , x ) for x in sizes ] \n        return partitions \n    nfps = _compute_nfps_real ( counts , sizes ) \n    partitions , _ , _ = _compute_best_partitions ( num_part , sizes , nfps ) \n    return partitions "}
{"3019": "\ndef apk ( actual , predicted , k = 10 ) : \n    if len ( predicted ) > k : \n        predicted = predicted [ : k ] \n    score = 0.0 \n    num_hits = 0.0 \n    for i , p in enumerate ( predicted ) : \n        if p in actual and p not in predicted [ : i ] : \n            num_hits += 1.0 \n            score += num_hits / ( i + 1.0 ) \n    if len ( actual ) == 0 : \n        return_identifier_0 = 0.0 \n        return return_identifier_0 \n    return score / min ( len ( actual ) , k ) "}
{"3035": "\ndef _markdownify_operative_config_str ( self , string ) : \n    def process ( line ) : \n        if not line . startswith ( '#' ) : \n            return '    ' + line \n        line = line [ 2 : ] \n        if line . startswith ( '====' ) : \n            return_identifier_0 = '' \n            return return_identifier_0 \n        if line . startswith ( 'None' ) : \n            return_identifier_1 = '    # None.' \n            return return_identifier_1 \n        if line . endswith ( ':' ) : \n            return '#### ' + line \n        return line \n    output_lines = [ ] \n    for line in string . splitlines ( ) : \n        procd_line = process ( line ) \n        if procd_line is not None : \n            output_lines . append ( procd_line ) \n    return '\\n' . join ( output_lines ) "}
{"3043": "\ndef _might_have_parameter ( fn_or_cls , arg_name ) : \n    if inspect . isclass ( fn_or_cls ) : \n        fn = _find_class_construction_fn ( fn_or_cls ) \n    else : \n        fn = fn_or_cls \n    while hasattr ( fn , '__wrapped__' ) : \n        fn = fn . __wrapped__ \n    arg_spec = _get_cached_arg_spec ( fn ) \n    if six . PY3 : \n        if arg_spec . varkw : \n            return_identifier_0 = True \n            return return_identifier_0 \n        return arg_name in arg_spec . args or arg_name in arg_spec . kwonlyargs \n    else : \n        if arg_spec . keywords : \n            return_identifier_1 = True \n            return return_identifier_1 \n        return arg_name in arg_spec . args "}
{"3061": "\ndef matching_selectors ( self , partial_selector ) : \n    if partial_selector in self . _selector_map : \n        return_identifier_0 = [ partial_selector ] \n        return return_identifier_0 \n    selector_components = partial_selector . split ( '.' ) \n    node = self . _selector_tree \n    for component in reversed ( selector_components ) : \n        if component not in node : \n            return_identifier_1 = [ ] \n            return return_identifier_1 \n        node = node [ component ] \n    selectors = [ ] \n    dfs_stack = [ node ] \n    while dfs_stack : \n        node = dfs_stack . pop ( ) . copy ( ) \n        selector = node . pop ( _TERMINAL_KEY , None ) \n        dfs_stack . extend ( node . values ( ) ) \n        if selector : \n            selectors . append ( selector ) \n    return selectors "}
{"3100": "\ndef remove_action ( self , action_name , action_id ) : \n    action = self . get_action ( action_name , action_id ) \n    if action is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    action . cancel ( ) \n    self . actions [ action_name ] . remove ( action ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"3137": "\ndef _rewrite_insert_update ( self , sql , params , returning ) : \n    update_columns = ', ' . join ( [ '{0} = EXCLUDED.{0}' . format ( self . qn ( field . column ) ) for field in self . query . update_fields ] ) \n    conflict_target = self . _build_conflict_target ( ) \n    index_predicate = self . query . index_predicate \n    sql_template = ( '{insert} ON CONFLICT {conflict_target} DO UPDATE ' 'SET {update_columns} RETURNING {returning}' ) \n    if index_predicate : \n        sql_template = ( '{insert} ON CONFLICT {conflict_target} WHERE {index_predicate} DO UPDATE ' 'SET {update_columns} RETURNING {returning}' ) \n    return_identifier_0 = ( sql_template . format ( insert = sql , conflict_target = conflict_target , update_columns = update_columns , returning = returning , index_predicate = index_predicate , ) , params ) \n    return return_identifier_0 "}
{"3138": "\ndef _rewrite_insert_nothing ( self , sql , params , returning ) : \n    conflict_target = self . _build_conflict_target ( ) \n    where_clause = ' AND ' . join ( [ '{0} = %s' . format ( self . _format_field_name ( field_name ) ) for field_name in self . query . conflict_target ] ) \n    where_clause_params = [ self . _format_field_value ( field_name ) for field_name in self . query . conflict_target ] \n    params = params + tuple ( where_clause_params ) \n    return_identifier_0 = ( ( 'WITH insdata AS (' '{insert} ON CONFLICT {conflict_target} DO UPDATE' ' SET {pk_column} = NULL WHERE FALSE RETURNING {returning})' ' SELECT * FROM insdata UNION ALL' ' SELECT {returning} FROM {table} WHERE {where_clause} LIMIT 1;' ) . format ( insert = sql , conflict_target = conflict_target , pk_column = self . qn ( self . query . model . _meta . pk . column ) , returning = returning , table = self . query . objs [ 0 ] . _meta . db_table , where_clause = where_clause ) , params ) \n    return return_identifier_0 "}
{"3150": "\ndef tdist95conf_level ( df ) : \n    df = int ( round ( df ) ) \n    highest_table_df = len ( _T_DIST_95_CONF_LEVELS ) \n    if df >= 200 : \n        return_identifier_0 = 1.960 \n        return return_identifier_0 \n    if df >= 100 : \n        return_identifier_1 = 1.984 \n        return return_identifier_1 \n    if df >= 80 : \n        return_identifier_2 = 1.990 \n        return return_identifier_2 \n    if df >= 60 : \n        return_identifier_3 = 2.000 \n        return return_identifier_3 \n    if df >= 50 : \n        return_identifier_4 = 2.009 \n        return return_identifier_4 \n    if df >= 40 : \n        return_identifier_5 = 2.021 \n        return return_identifier_5 \n    if df >= highest_table_df : \n        return _T_DIST_95_CONF_LEVELS [ highest_table_df - 1 ] \n    return _T_DIST_95_CONF_LEVELS [ df ] "}
{"3153": "\ndef is_significant ( sample1 , sample2 ) : \n    deg_freedom = len ( sample1 ) + len ( sample2 ) - 2 \n    critical_value = tdist95conf_level ( deg_freedom ) \n    t_score = tscore ( sample1 , sample2 ) \n    return_identifier_0 = ( abs ( t_score ) >= critical_value , t_score ) \n    return return_identifier_0 "}
{"3163": "\ndef GetDomain ( self ) : \n    return_identifier_0 = ( self . knots [ self . degree - 1 ] , self . knots [ len ( self . knots ) - self . degree ] ) \n    return return_identifier_0 "}
{"3218": "\ndef parse_number_of_html_pages ( html_question ) : \n    bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) \n    try : \n        bs_question . select ( 'div.paginator' ) [ 0 ] \n    except IndexError : \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    else : \n        return int ( bs_question . select ( 'div.paginator' ) [ 0 ] . attrs [ 'data-num-pages' ] ) "}
{"3261": "\ndef _validate_message ( self , message ) : \n    if self . MESSAGE_ID_FIELD not in message : \n        logger . warning ( \"Field 'Message-ID' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not message [ self . MESSAGE_ID_FIELD ] : \n        logger . warning ( \"Field 'Message-ID' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    if self . DATE_FIELD not in message : \n        logger . warning ( \"Field 'Date' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    if not message [ self . DATE_FIELD ] : \n        logger . warning ( \"Field 'Date' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) \n        return_identifier_3 = False \n        return return_identifier_3 \n    try : \n        str_to_datetime ( message [ self . DATE_FIELD ] ) \n    except InvalidDateError : \n        logger . warning ( \"Invalid date %s in message %s; ignoring\" , message [ self . DATE_FIELD ] , message [ 'unixfrom' ] ) \n        return_identifier_4 = False \n        return return_identifier_4 \n    return_identifier_5 = True \n    return return_identifier_5 "}
{"3272": "\ndef is_detached ( self ) : \n    cmd_sym = [ 'git' , 'symbolic-ref' , 'HEAD' ] \n    try : \n        self . _exec ( cmd_sym , cwd = self . dirpath , env = self . gitenv ) \n    except RepositoryError as e : \n        if e . msg . find ( \"ref HEAD is not a symbolic ref\" ) == - 1 : \n            raise e \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"3278": "\ndef _fetch_pack ( self ) : \n    def prepare_refs ( refs ) : \n        return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] \n    def determine_wants ( refs ) : \n        remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) \n        local_refs = prepare_refs ( self . _discover_refs ( ) ) \n        wants = [ ref for ref in remote_refs if ref not in local_refs ] \n        return wants \n    client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) \n    repo = dulwich . repo . Repo ( self . dirpath ) \n    fd = io . BytesIO ( ) \n    local_refs = self . _discover_refs ( ) \n    graph_walker = _GraphWalker ( local_refs ) \n    result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) \n    refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] \n    if len ( fd . getvalue ( ) ) > 0 : \n        fd . seek ( 0 ) \n        pack = repo . object_store . add_thin_pack ( fd . read , None ) \n        pack_name = pack . name ( ) . decode ( 'utf-8' ) \n    else : \n        pack_name = None \n    return_identifier_0 = ( pack_name , refs ) \n    return return_identifier_0 "}
{"3317": "\ndef _need_check_tokens ( self ) : \n    if self . n_tokens <= 1 or self . rate_limit is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    elif self . last_rate_limit_checked is None : \n        self . last_rate_limit_checked = self . rate_limit \n        return_identifier_1 = True \n        return return_identifier_1 \n    approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 \n    if self . rate_limit <= approaching_limit : \n        self . last_rate_limit_checked = self . rate_limit \n        return_identifier_2 = True \n        return return_identifier_2 \n    ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) \n    if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : \n        self . last_rate_limit_checked = self . rate_limit \n        return_identifier_3 = True \n        return return_identifier_3 \n    elif ratio > 1.0 : \n        self . last_rate_limit_checked = self . rate_limit \n        return_identifier_4 = False \n        return return_identifier_4 \n    else : \n        return_identifier_5 = False \n        return return_identifier_5 "}
{"3345": "\ndef map_custom_field ( custom_fields , fields ) : \n    def build_cf ( cf , v ) : \n        return_identifier_0 = { 'id' : cf [ 'id' ] , 'name' : cf [ 'name' ] , 'value' : v } \n        return return_identifier_0 \n    return { k : build_cf ( custom_fields [ k ] , v ) for k , v in fields . items ( ) if k in custom_fields } "}
{"3365": "\ndef _filter_message_by_chats ( self , message , chats ) : \n    if chats is None : \n        return_identifier_0 = True \n        return return_identifier_0 \n    chat_id = message [ 'message' ] [ 'chat' ] [ 'id' ] \n    return chat_id in chats "}
{"3466": "\ndef cook_refs ( refs , n = 4 ) : \n    refs = [ normalize ( ref ) for ref in refs ] \n    maxcounts = { } \n    for ref in refs : \n        counts = count_ngrams ( ref , n ) \n        for ( ngram , count ) in list ( counts . items ( ) ) : \n            maxcounts [ ngram ] = max ( maxcounts . get ( ngram , 0 ) , count ) \n    return_identifier_0 = ( [ len ( ref ) for ref in refs ] , maxcounts ) \n    return return_identifier_0 "}
{"3467": "\ndef cook_ref_set ( ref , n = 4 ) : \n    ref = normalize ( ref ) \n    counts = count_ngrams ( ref , n ) \n    return_identifier_0 = ( len ( ref ) , counts , frozenset ( counts ) ) \n    return return_identifier_0 "}
{"3475": "\ndef to_json ( self ) : \n    d , ps = self . _to_json ( ) \n    if len ( ps ) == 0 : \n        return_identifier_0 = { \"name\" : d } \n        return return_identifier_0 \n    else : \n        return_identifier_1 = { \"name\" : d , \"args\" : ps } \n        return return_identifier_1 "}
{"3495": "\ndef get_legal_params ( self , method ) : \n    if method not in self . client . meta . method_to_api_mapping : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    api = self . client . meta . method_to_api_mapping [ method ] \n    shape = self . client . meta . service_model . operation_model ( api ) . input_shape \n    if shape is None : \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    return shape . members . keys ( ) "}
{"3526": "\ndef sync_check ( self , md5cache , remoteKey ) : \n    if not remoteKey : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not os . path . exists ( md5cache . filename ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    localmd5 = md5cache . get_md5 ( ) \n    return ( 'ETag' in remoteKey and remoteKey [ 'ETag' ] == '\"%s\"' % localmd5 ) or ( 'md5' in remoteKey and remoteKey [ 'md5' ] == localmd5 ) or ( 'md5' in remoteKey [ 'Metadata' ] and remoteKey [ 'Metadata' ] [ 'md5' ] == localmd5 ) "}
{"3527": "\ndef partial_match ( self , path , filter_path ) : \n    if not path or not filter_path : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if path [ - 1 ] == PATH_SEP : \n        path = path [ 0 : - 1 ] \n    if filter_path [ - 1 ] == PATH_SEP : \n        filter_path += '*' \n    pi = path . split ( PATH_SEP ) \n    fi = filter_path . split ( PATH_SEP ) \n    min_len = min ( len ( pi ) , len ( fi ) ) \n    matched = fnmatch . fnmatch ( PATH_SEP . join ( pi [ 0 : min_len ] ) , PATH_SEP . join ( fi [ 0 : min_len ] ) ) \n    return matched and ( self . opt . recursive or len ( pi ) <= len ( fi ) ) "}
{"3551": "\ndef match_date ( self , value ) : \n    m = self . REGEX_DATE . search ( value ) \n    date = datetime . datetime . utcnow ( ) . date ( ) \n    if m : \n        date = datetime . date ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) , int ( m . group ( 3 ) ) ) \n        value = self . REGEX_DATE . sub ( '' , value ) \n    return_identifier_0 = ( date , value ) \n    return return_identifier_0 "}
{"3552": "\ndef match_time ( self , value ) : \n    m = self . REGEX_TIME . search ( value ) \n    time = datetime . datetime . utcnow ( ) . time ( ) \n    if m : \n        time = datetime . time ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) ) \n        value = self . REGEX_TIME . sub ( '' , value ) \n    return_identifier_0 = ( time , value ) \n    return return_identifier_0 "}
{"3553": "\ndef match_delta ( self , value ) : \n    m = self . REGEX_DELTA . search ( value ) \n    delta = datetime . timedelta ( days = 0 ) \n    if m : \n        d = int ( m . group ( 1 ) ) \n        if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : \n            d = - d \n        if m . group ( 2 ) == 'minute' : \n            delta = datetime . timedelta ( minutes = d ) \n        elif m . group ( 2 ) == 'hour' : \n            delta = datetime . timedelta ( hours = d ) \n        elif m . group ( 2 ) == 'day' : \n            delta = datetime . timedelta ( days = d ) \n        elif m . group ( 2 ) == 'week' : \n            delta = datetime . timedelta ( weeks = d ) \n        value = self . REGEX_DELTA . sub ( '' , value ) \n    return_identifier_0 = ( delta , value ) \n    return return_identifier_0 "}
{"3558": "\ndef push_data ( self , data ) : \n    if not _validate_data ( data ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else _list2map ( data [ 'params' ] ) \n    if jdata is None : \n        return_identifier_1 = False \n        return return_identifier_1 \n    sid = data [ 'sid' ] \n    for func in self . callbacks [ sid ] : \n        func ( jdata , data ) \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"3560": "\ndef exception_handler ( job , * exc_info ) : \n    job_info = job . to_dict ( ) \n    job_info [ 'data' ] = repr ( job_info [ 'data' ] ) \n    extra_data = { 'job' : job_info } \n    payload_data = { 'framework' : 'rq' } \n    rollbar . report_exc_info ( exc_info , extra_data = extra_data , payload_data = payload_data ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"3567": "\ndef search_items ( title , return_fields = None , access_token = None , endpoint = None , ** search_fields ) : \n    if not title : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    if return_fields is not None : \n        return_fields = ',' . join ( return_fields ) \n    return _get_api ( 'search/' , title = title , fields = return_fields , access_token = access_token , endpoint = endpoint , ** search_fields ) "}
{"3569": "\ndef _build_person_data ( request ) : \n    if hasattr ( request , 'rollbar_person' ) : \n        rollbar_person_prop = request . rollbar_person \n        try : \n            person = rollbar_person_prop ( ) \n        except TypeError : \n            person = rollbar_person_prop \n        if person and isinstance ( person , dict ) : \n            return person \n        else : \n            return None \n    if hasattr ( request , 'user' ) : \n        user_prop = request . user \n        try : \n            user = user_prop ( ) \n        except TypeError : \n            user = user_prop \n        if not user : \n            return None \n        elif isinstance ( user , dict ) : \n            return user \n        else : \n            retval = { } \n            if getattr ( user , 'id' , None ) : \n                retval [ 'id' ] = text ( user . id ) \n            elif getattr ( user , 'user_id' , None ) : \n                retval [ 'id' ] = text ( user . user_id ) \n            if retval . get ( 'id' ) : \n                username = getattr ( user , 'username' , None ) \n                email = getattr ( user , 'email' , None ) \n                retval . update ( { 'username' : username , 'email' : email } ) \n            return retval \n    if hasattr ( request , 'user_id' ) : \n        user_id_prop = request . user_id \n        try : \n            user_id = user_id_prop ( ) \n        except TypeError : \n            user_id = user_id_prop \n        if not user_id : \n            return None \n        return_identifier_0 = { 'id' : text ( user_id ) } \n        return return_identifier_0 "}
{"3582": "\ndef is_inside_lambda ( node : astroid . node_classes . NodeNG ) -> bool : \n    parent = node . parent \n    while parent is not None : \n        if isinstance ( parent , astroid . Lambda ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        parent = parent . parent \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3584": "\ndef clobber_in_except ( node : astroid . node_classes . NodeNG ) -> Tuple [ bool , Tuple [ str , str ] ] : \n    if isinstance ( node , astroid . AssignAttr ) : \n        return True , ( node . attrname , \"object %r\" % ( node . expr . as_string ( ) , ) ) \n    if isinstance ( node , astroid . AssignName ) : \n        name = node . name \n        if is_builtin ( name ) : \n            return_identifier_0 = ( True , ( name , \"builtins\" ) ) \n            return return_identifier_0 \n        stmts = node . lookup ( name ) [ 1 ] \n        if stmts and not isinstance ( stmts [ 0 ] . assign_type ( ) , ( astroid . Assign , astroid . AugAssign , astroid . ExceptHandler ) , ) : \n            return True , ( name , \"outer scope (line %s)\" % stmts [ 0 ] . fromlineno ) \n    return False , None "}
{"3585": "\ndef is_super ( node : astroid . node_classes . NodeNG ) -> bool : \n    if getattr ( node , \"name\" , None ) == \"super\" and node . root ( ) . name == BUILTINS_NAME : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3586": "\ndef is_error ( node : astroid . node_classes . NodeNG ) -> bool : \n    for child_node in node . get_children ( ) : \n        if isinstance ( child_node , astroid . Raise ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3587": "\ndef is_default_argument ( node : astroid . node_classes . NodeNG ) -> bool : \n    parent = node . scope ( ) \n    if isinstance ( parent , ( astroid . FunctionDef , astroid . Lambda ) ) : \n        for default_node in parent . args . defaults : \n            for default_name_node in default_node . nodes_of_class ( astroid . Name ) : \n                if default_name_node is node : \n                    return_identifier_0 = True \n                    return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3588": "\ndef is_func_decorator ( node : astroid . node_classes . NodeNG ) -> bool : \n    parent = node . parent \n    while parent is not None : \n        if isinstance ( parent , astroid . Decorators ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if parent . is_statement or isinstance ( parent , ( astroid . Lambda , scoped_nodes . ComprehensionScope , scoped_nodes . ListComp ) , ) : \n            break \n        parent = parent . parent \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3589": "\ndef is_ancestor_name ( frame : astroid . node_classes . NodeNG , node : astroid . node_classes . NodeNG ) -> bool : \n    try : \n        bases = frame . bases \n    except AttributeError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    for base in bases : \n        if node in base . nodes_of_class ( astroid . Name ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"3594": "\ndef inherit_from_std_ex ( node : astroid . node_classes . NodeNG ) -> bool : \n    ancestors = node . ancestors ( ) if hasattr ( node , \"ancestors\" ) else [ ] \n    for ancestor in itertools . chain ( [ node ] , ancestors ) : \n        if ( ancestor . name in ( \"Exception\" , \"BaseException\" ) and ancestor . root ( ) . name == EXCEPTIONS_MODULE ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3595": "\ndef error_of_type ( handler : astroid . ExceptHandler , error_type ) -> bool : \n    def stringify_error ( error ) : \n        if not isinstance ( error , str ) : \n            return error . __name__ \n        return error \n    if not isinstance ( error_type , tuple ) : \n        error_type = ( error_type , ) \n    expected_errors = { stringify_error ( error ) for error in error_type } \n    if not handler . type : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return handler . catch ( expected_errors ) "}
{"3596": "\ndef decorated_with_property ( node : astroid . FunctionDef ) -> bool : \n    if not node . decorators : \n        return_identifier_0 = False \n        return return_identifier_0 \n    for decorator in node . decorators . nodes : \n        if not isinstance ( decorator , astroid . Name ) : \n            continue \n        try : \n            if _is_property_decorator ( decorator ) : \n                return_identifier_1 = True \n                return return_identifier_1 \n        except astroid . InferenceError : \n            pass \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"3597": "\ndef decorated_with ( func : astroid . FunctionDef , qnames : Iterable [ str ] ) -> bool : \n    decorators = func . decorators . nodes if func . decorators else [ ] \n    for decorator_node in decorators : \n        try : \n            if any ( i is not None and i . qname ( ) in qnames for i in decorator_node . infer ( ) ) : \n                return_identifier_0 = True \n                return return_identifier_0 \n        except astroid . InferenceError : \n            continue \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3599": "\ndef is_from_fallback_block ( node : astroid . node_classes . NodeNG ) -> bool : \n    context = find_try_except_wrapper_node ( node ) \n    if not context : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if isinstance ( context , astroid . ExceptHandler ) : \n        other_body = context . parent . body \n        handlers = context . parent . handlers \n    else : \n        other_body = itertools . chain . from_iterable ( handler . body for handler in context . handlers ) \n        handlers = context . handlers \n    has_fallback_imports = any ( isinstance ( import_node , ( astroid . ImportFrom , astroid . Import ) ) for import_node in other_body ) \n    ignores_import_error = _except_handlers_ignores_exception ( handlers , ImportError ) \n    return ignores_import_error or has_fallback_imports "}
{"3601": "\ndef node_ignores_exception ( node : astroid . node_classes . NodeNG , exception = Exception ) -> bool : \n    managing_handlers = get_exception_handlers ( node , exception ) \n    if not managing_handlers : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return any ( managing_handlers ) "}
{"3602": "\ndef class_is_abstract ( node : astroid . ClassDef ) -> bool : \n    for method in node . methods ( ) : \n        if method . parent . frame ( ) is node : \n            if method . is_abstract ( pass_is_abstract = False ) : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3605": "\ndef is_registered_in_singledispatch_function ( node : astroid . FunctionDef ) -> bool : \n    singledispatch_qnames = ( \"functools.singledispatch\" , \"singledispatch.singledispatch\" , ) \n    if not isinstance ( node , astroid . FunctionDef ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    decorators = node . decorators . nodes if node . decorators else [ ] \n    for decorator in decorators : \n        if not isinstance ( decorator , astroid . Call ) : \n            continue \n        func = decorator . func \n        if not isinstance ( func , astroid . Attribute ) or func . attrname != \"register\" : \n            continue \n        try : \n            func_def = next ( func . expr . infer ( ) ) \n        except astroid . InferenceError : \n            continue \n        if isinstance ( func_def , astroid . FunctionDef ) : \n            return decorated_with ( func_def , singledispatch_qnames ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3616": "\ndef _check_relative_import ( self , modnode , importnode , importedmodnode , importedasname ) : \n    if not self . linter . is_message_enabled ( \"relative-import\" ) : \n        return None \n    if importedmodnode . file is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if modnode is importedmodnode : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if modnode . absolute_import_activated ( ) or getattr ( importnode , \"level\" , None ) : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if importedmodnode . name != importedasname : \n        self . add_message ( \"relative-import\" , args = ( importedasname , importedmodnode . name ) , node = importnode , ) \n        return None \n    return None "}
{"3635": "\ndef show_node ( self , node ) : \n    if self . config . show_builtin : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return node . root ( ) . name != BUILTINS_NAME "}
{"3640": "\ndef leave_project ( self , node ) : \n    if self . pkgdiagram : \n        return self . pkgdiagram , self . classdiagram \n    return_identifier_0 = ( self . classdiagram , ) \n    return return_identifier_0 "}
{"3644": "\ndef _is_owner_ignored ( owner , name , ignored_classes , ignored_modules ) : \n    ignored_modules = set ( ignored_modules ) \n    module_name = owner . root ( ) . name \n    module_qname = owner . root ( ) . qname ( ) \n    if any ( module_name in ignored_modules or module_qname in ignored_modules or fnmatch . fnmatch ( module_qname , ignore ) for ignore in ignored_modules ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    ignored_classes = set ( ignored_classes ) \n    if hasattr ( owner , \"qname\" ) : \n        qname = owner . qname ( ) \n    else : \n        qname = \"\" \n    return any ( ignore in ( name , qname ) for ignore in ignored_classes ) "}
{"3646": "\ndef _emit_no_member ( node , owner , owner_name , ignored_mixins = True , ignored_none = True ) : \n    if node_ignores_exception ( node , AttributeError ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if ignored_none and isinstance ( owner , astroid . Const ) and owner . value is None : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if is_super ( owner ) or getattr ( owner , \"type\" , None ) == \"metaclass\" : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if ignored_mixins and owner_name [ - 5 : ] . lower ( ) == \"mixin\" : \n        return_identifier_3 = False \n        return return_identifier_3 \n    if isinstance ( owner , astroid . FunctionDef ) and owner . decorators : \n        return_identifier_4 = False \n        return return_identifier_4 \n    if isinstance ( owner , ( astroid . Instance , astroid . ClassDef ) ) : \n        if owner . has_dynamic_getattr ( ) : \n            try : \n                metaclass = owner . metaclass ( ) \n            except exceptions . MroError : \n                return_identifier_5 = False \n                return return_identifier_5 \n            if metaclass : \n                return metaclass . qname ( ) == \"enum.EnumMeta\" \n            return_identifier_6 = False \n            return return_identifier_6 \n        if not has_known_bases ( owner ) : \n            return_identifier_7 = False \n            return return_identifier_7 \n    if isinstance ( owner , objects . Super ) : \n        try : \n            owner . super_mro ( ) \n        except ( exceptions . MroError , exceptions . SuperError ) : \n            return_identifier_8 = False \n            return return_identifier_8 \n        if not all ( map ( has_known_bases , owner . type . mro ( ) ) ) : \n            return_identifier_9 = False \n            return return_identifier_9 \n    if isinstance ( owner , astroid . Module ) : \n        try : \n            owner . getattr ( \"__getattr__\" ) \n            return_identifier_10 = False \n            return return_identifier_10 \n        except astroid . NotFoundError : \n            pass \n    if node . attrname . startswith ( \"_\" + owner_name ) : \n        unmangled_name = node . attrname . split ( \"_\" + owner_name ) [ - 1 ] \n        try : \n            if owner . getattr ( unmangled_name , context = None ) is not None : \n                return_identifier_11 = False \n                return return_identifier_11 \n        except astroid . NotFoundError : \n            return_identifier_12 = True \n            return return_identifier_12 \n    return_identifier_13 = True \n    return return_identifier_13 "}
{"3649": "\ndef _no_context_variadic ( node , variadic_name , variadic_type , variadics ) : \n    statement = node . statement ( ) \n    for name in statement . nodes_of_class ( astroid . Name ) : \n        if name . name != variadic_name : \n            continue \n        inferred = safe_infer ( name ) \n        if isinstance ( inferred , ( astroid . List , astroid . Tuple ) ) : \n            length = len ( inferred . elts ) \n        elif isinstance ( inferred , astroid . Dict ) : \n            length = len ( inferred . items ) \n        else : \n            continue \n        inferred_statement = inferred . statement ( ) \n        if not length and isinstance ( inferred_statement , astroid . FunctionDef ) : \n            is_in_starred_context = _has_parent_of_type ( node , variadic_type , statement ) \n            used_as_starred_argument = _is_name_used_as_variadic ( name , variadics ) \n            if is_in_starred_context or used_as_starred_argument : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3662": "\ndef compute_module ( self , context_name , mod_path ) : \n    package_dir = os . path . dirname ( self . project . path ) \n    if context_name == mod_path : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    if modutils . is_standard_module ( mod_path , ( package_dir , ) ) : \n        return_identifier_1 = 1 \n        return return_identifier_1 \n    return_identifier_2 = 0 \n    return return_identifier_2 "}
{"3664": "\ndef _get_ansi_code ( color = None , style = None ) : \n    ansi_code = [ ] \n    if style : \n        style_attrs = utils . _splitstrip ( style ) \n        for effect in style_attrs : \n            ansi_code . append ( ANSI_STYLES [ effect ] ) \n    if color : \n        if color . isdigit ( ) : \n            ansi_code . extend ( [ \"38\" , \"5\" ] ) \n            ansi_code . append ( color ) \n        else : \n            ansi_code . append ( ANSI_COLORS [ color ] ) \n    if ansi_code : \n        return ANSI_PREFIX + \";\" . join ( ansi_code ) + ANSI_END \n    return_identifier_0 = \"\" \n    return return_identifier_0 "}
{"3685": "\ndef get_message_definitions ( self , msgid_or_symbol : str ) -> list : \n    if msgid_or_symbol [ 1 : ] . isdigit ( ) : \n        msgid_or_symbol = msgid_or_symbol . upper ( ) \n    for source in ( self . _alternative_names , self . _messages_definitions ) : \n        try : \n            return_identifier_0 = [ source [ msgid_or_symbol ] ] \n            return return_identifier_0 \n        except KeyError : \n            pass \n    error_msg = \"No such message id or symbol '{msgid_or_symbol}'.\" . format ( msgid_or_symbol = msgid_or_symbol ) \n    raise UnknownMessageError ( error_msg ) "}
{"3690": "\ndef _cpu_count ( ) -> int : \n    sched_getaffinity = getattr ( os , \"sched_getaffinity\" , None ) \n    if sched_getaffinity : \n        return len ( sched_getaffinity ( 0 ) ) \n    if multiprocessing : \n        return multiprocessing . cpu_count ( ) \n    return_identifier_0 = 1 \n    return return_identifier_0 "}
{"3704": "\ndef check_astroid_module ( self , ast_node , walker , rawcheckers , tokencheckers ) : \n    try : \n        tokens = utils . tokenize_module ( ast_node ) \n    except tokenize . TokenError as ex : \n        self . add_message ( \"syntax-error\" , line = ex . args [ 1 ] [ 0 ] , args = ex . args [ 0 ] ) \n        return None \n    if not ast_node . pure_python : \n        self . add_message ( \"raw-checker-failed\" , args = ast_node . name ) \n    else : \n        self . process_tokens ( tokens ) \n        if self . _ignore_file : \n            return_identifier_0 = False \n            return return_identifier_0 \n        self . file_state . collect_block_lines ( self . msgs_store , ast_node ) \n        for checker in rawcheckers : \n            checker . process_module ( ast_node ) \n        for checker in tokencheckers : \n            checker . process_tokens ( tokens ) \n    walker . walk ( ast_node ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"3713": "\ndef _basename_in_blacklist_re ( base_name , black_list_re ) : \n    for file_pattern in black_list_re : \n        if file_pattern . match ( base_name ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3721": "\ndef parents ( self ) : \n    assert self . parent is not self \n    if self . parent is None : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    return [ self . parent ] + self . parent . parents ( ) "}
{"3732": "\ndef returns_something ( return_node ) : \n    returns = return_node . value \n    if returns is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return not ( isinstance ( returns , astroid . Const ) and returns . value is None ) "}
{"3737": "\ndef _is_from_future_import ( stmt , name ) : \n    try : \n        module = stmt . do_import_module ( stmt . modname ) \n    except astroid . AstroidBuildingException : \n        return None \n    for local_node in module . locals . get ( name , [ ] ) : \n        if isinstance ( local_node , astroid . ImportFrom ) and local_node . modname == FUTURE : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return None "}
{"3741": "\ndef _detect_global_scope ( node , frame , defframe ) : \n    def_scope = scope = None \n    if frame and frame . parent : \n        scope = frame . parent . scope ( ) \n    if defframe and defframe . parent : \n        def_scope = defframe . parent . scope ( ) \n    if isinstance ( frame , astroid . FunctionDef ) : \n        if not isinstance ( node . parent , ( astroid . FunctionDef , astroid . Arguments ) ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    elif any ( not isinstance ( f , ( astroid . ClassDef , astroid . Module ) ) for f in ( frame , defframe ) ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    break_scopes = [ ] \n    for s in ( scope , def_scope ) : \n        parent_scope = s \n        while parent_scope : \n            if not isinstance ( parent_scope , ( astroid . ClassDef , astroid . Module ) ) : \n                break_scopes . append ( parent_scope ) \n                break \n            if parent_scope . parent : \n                parent_scope = parent_scope . parent . scope ( ) \n            else : \n                break \n    if break_scopes and len ( set ( break_scopes ) ) != 1 : \n        return_identifier_2 = False \n        return return_identifier_2 \n    return frame . lineno < defframe . lineno "}
{"3746": "\ndef _has_homonym_in_upper_function_scope ( self , node , index ) : \n    for _consumer in self . _to_consume [ index - 1 : : - 1 ] : \n        if _consumer . scope_type == \"function\" and node . name in _consumer . to_consume : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3763": "\ndef _has_different_parameters_default_value ( original , overridden ) : \n    if original . args is None or overridden . args is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    all_args = chain ( original . args , original . kwonlyargs ) \n    original_param_names = [ param . name for param in all_args ] \n    default_missing = object ( ) \n    for param_name in original_param_names : \n        try : \n            original_default = original . default_value ( param_name ) \n        except astroid . exceptions . NoDefault : \n            original_default = default_missing \n        try : \n            overridden_default = overridden . default_value ( param_name ) \n        except astroid . exceptions . NoDefault : \n            overridden_default = default_missing \n        default_list = [ arg == default_missing for arg in ( original_default , overridden_default ) ] \n        if any ( default_list ) and not all ( default_list ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n        astroid_type_compared_attr = { astroid . Const : \"value\" , astroid . ClassDef : \"name\" , astroid . Tuple : \"elts\" , astroid . List : \"elts\" , } \n        handled_types = tuple ( astroid_type for astroid_type in astroid_type_compared_attr ) \n        original_type = _get_node_type ( original_default , handled_types ) \n        if original_type : \n            if not isinstance ( overridden_default , original_type ) : \n                return_identifier_2 = True \n                return return_identifier_2 \n            if not _check_arg_equality ( original_default , overridden_default , astroid_type_compared_attr [ original_type ] , ) : \n                return_identifier_3 = True \n                return return_identifier_3 \n    return_identifier_4 = False \n    return return_identifier_4 "}
{"3779": "\ndef _is_raising ( body : typing . List ) -> bool : \n    for node in body : \n        if isinstance ( node , astroid . Raise ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3783": "\ndef _is_typing_namedtuple ( node : astroid . ClassDef ) -> bool : \n    for base in node . ancestors ( ) : \n        if base . qname ( ) == TYPING_NAMEDTUPLE : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3784": "\ndef _is_enum_class ( node : astroid . ClassDef ) -> bool : \n    for base in node . bases : \n        try : \n            inferred_bases = base . inferred ( ) \n        except astroid . InferenceError : \n            continue \n        for ancestor in inferred_bases : \n            if not isinstance ( ancestor , astroid . ClassDef ) : \n                continue \n            if ancestor . name == \"Enum\" and ancestor . root ( ) . name == \"enum\" : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3785": "\ndef _is_dataclass ( node : astroid . ClassDef ) -> bool : \n    if not node . decorators : \n        return_identifier_0 = False \n        return return_identifier_0 \n    root_locals = node . root ( ) . locals \n    for decorator in node . decorators . nodes : \n        if isinstance ( decorator , astroid . Call ) : \n            decorator = decorator . func \n        if not isinstance ( decorator , ( astroid . Name , astroid . Attribute ) ) : \n            continue \n        if isinstance ( decorator , astroid . Name ) : \n            name = decorator . name \n        else : \n            name = decorator . attrname \n        if name == DATACLASS_DECORATOR and DATACLASS_DECORATOR in root_locals : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"3793": "\ndef _is_trailing_comma ( tokens , index ) : \n    token = tokens [ index ] \n    if token . exact_type != tokenize . COMMA : \n        return_identifier_0 = False \n        return return_identifier_0 \n    left_tokens = itertools . islice ( tokens , index + 1 , None ) \n    same_line_remaining_tokens = list ( itertools . takewhile ( lambda other_token , _token = token : other_token . start [ 0 ] == _token . start [ 0 ] , left_tokens , ) ) \n    is_last_element = all ( other_token . type in ( tokenize . NEWLINE , tokenize . COMMENT ) for other_token in same_line_remaining_tokens ) \n    if not same_line_remaining_tokens or not is_last_element : \n        return_identifier_1 = False \n        return return_identifier_1 \n    def get_curline_index_start ( ) : \n        for subindex , token in enumerate ( reversed ( tokens [ : index ] ) ) : \n            if token . type in ( tokenize . NEWLINE , tokenize . NL ) : \n                return index - subindex \n        return_identifier_2 = 0 \n        return return_identifier_2 \n    curline_start = get_curline_index_start ( ) \n    expected_tokens = { \"return\" , \"yield\" } \n    for prevtoken in tokens [ curline_start : index ] : \n        if \"=\" in prevtoken . string or prevtoken . string in expected_tokens : \n            return_identifier_3 = True \n            return return_identifier_3 \n    return_identifier_4 = False \n    return return_identifier_4 "}
{"3794": "\ndef _is_actual_elif ( self , node ) : \n    if isinstance ( node . parent , astroid . If ) : \n        orelse = node . parent . orelse \n        if orelse and orelse == [ node ] : \n            if ( node . lineno , node . col_offset ) in self . _elifs : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3798": "\ndef _check_raising_stopiteration_in_generator_next_call ( self , node ) : \n    def _looks_like_infinite_iterator ( param ) : \n        inferred = utils . safe_infer ( param ) \n        if inferred : \n            return inferred . qname ( ) in KNOWN_INFINITE_ITERATORS \n        return_identifier_0 = False \n        return return_identifier_0 \n    if isinstance ( node . func , astroid . Attribute ) : \n        return \n    inferred = utils . safe_infer ( node . func ) \n    if getattr ( inferred , \"name\" , \"\" ) == \"next\" : \n        frame = node . frame ( ) \n        has_sentinel_value = len ( node . args ) > 1 \n        if ( isinstance ( frame , astroid . FunctionDef ) and frame . is_generator ( ) and not has_sentinel_value and not utils . node_ignores_exception ( node , StopIteration ) and not _looks_like_infinite_iterator ( node . args [ 0 ] ) ) : \n            self . add_message ( \"stop-iteration-return\" , node = node ) "}
{"3805": "\ndef _is_node_return_ended ( self , node ) : \n    if isinstance ( node , astroid . Return ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if isinstance ( node , astroid . Call ) : \n        try : \n            funcdef_node = node . func . inferred ( ) [ 0 ] \n            if self . _is_function_def_never_returning ( funcdef_node ) : \n                return_identifier_1 = True \n                return return_identifier_1 \n        except astroid . InferenceError : \n            pass \n    if isinstance ( node , astroid . While ) : \n        return_identifier_2 = True \n        return return_identifier_2 \n    if isinstance ( node , astroid . Raise ) : \n        if not node . exc : \n            return_identifier_3 = True \n            return return_identifier_3 \n        if not utils . is_node_inside_try_except ( node ) : \n            return_identifier_4 = True \n            return return_identifier_4 \n        exc = utils . safe_infer ( node . exc ) \n        if exc is None or exc is astroid . Uninferable : \n            return_identifier_5 = False \n            return return_identifier_5 \n        exc_name = exc . pytype ( ) . split ( \".\" ) [ - 1 ] \n        handlers = utils . get_exception_handlers ( node , exc_name ) \n        handlers = list ( handlers ) if handlers is not None else [ ] \n        if handlers : \n            return any ( self . _is_node_return_ended ( _handler ) for _handler in handlers ) \n        return_identifier_6 = True \n        return return_identifier_6 \n    if isinstance ( node , astroid . If ) : \n        is_orelse_returning = any ( self . _is_node_return_ended ( _ore ) for _ore in node . orelse if not isinstance ( _ore , astroid . FunctionDef ) ) \n        is_if_returning = any ( self . _is_node_return_ended ( _ifn ) for _ifn in node . body if not isinstance ( _ifn , astroid . FunctionDef ) ) \n        return is_if_returning and is_orelse_returning \n    return any ( self . _is_node_return_ended ( _child ) for _child in node . get_children ( ) if not isinstance ( _child , astroid . ExceptHandler ) ) "}
{"3808": "\ndef run ( self , args ) : \n    if not args : \n        print ( self . help ( ) ) \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    sys . path . insert ( 0 , os . getcwd ( ) ) \n    try : \n        project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) \n        linker = Linker ( project , tag = True ) \n        handler = DiadefsHandler ( self . config ) \n        diadefs = handler . get_diadefs ( project , linker ) \n    finally : \n        sys . path . pop ( 0 ) \n    if self . config . output_format == \"vcg\" : \n        writer . VCGWriter ( self . config ) . write ( diadefs ) \n    else : \n        writer . DotWriter ( self . config ) . write ( diadefs ) \n    return_identifier_1 = 0 \n    return return_identifier_1 "}
{"3813": "\ndef may_be_emitted ( self ) : \n    if self . minversion is not None and self . minversion > sys . version_info : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if self . maxversion is not None and self . maxversion <= sys . version_info : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"3826": "\ndef is_message_enabled ( self , msg_descr , line = None , confidence = None ) : \n    if self . config . confidence and confidence : \n        if confidence . name not in self . config . confidence : \n            return_identifier_0 = False \n            return return_identifier_0 \n    try : \n        message_definitions = self . msgs_store . get_message_definitions ( msg_descr ) \n        msgids = [ md . msgid for md in message_definitions ] \n    except UnknownMessageError : \n        msgids = [ msg_descr ] \n    for msgid in msgids : \n        if self . is_one_message_enabled ( msgid , line ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"3831": "\ndef _get_indent_hint_line ( bar_positions , bad_position ) : \n    if not bar_positions : \n        return_identifier_0 = ( \"\" , \"\" ) \n        return return_identifier_0 \n    bar_positions = [ _get_indent_length ( indent ) for indent in bar_positions ] \n    bad_position = _get_indent_length ( bad_position ) \n    delta_message = \"\" \n    markers = [ ( pos , \"|\" ) for pos in bar_positions ] \n    if len ( markers ) == 1 : \n        expected_position = markers [ 0 ] [ 0 ] \n        delta = abs ( expected_position - bad_position ) \n        direction = \"add\" if expected_position > bad_position else \"remove\" \n        delta_message = _CONTINUATION_HINT_MESSAGE % ( direction , delta , \"s\" if delta > 1 else \"\" , ) \n    markers . append ( ( bad_position , \"^\" ) ) \n    markers . sort ( ) \n    line = [ \" \" ] * ( markers [ - 1 ] [ 0 ] + 1 ) \n    for position , marker in markers : \n        line [ position ] = marker \n    return_identifier_1 = ( \"\" . join ( line ) , delta_message ) \n    return return_identifier_1 "}
{"3840": "\ndef _has_valid_type_annotation ( self , tokens , i ) : \n    if not self . _inside_brackets ( \"(\" ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    bracket_level = 0 \n    for token in tokens [ i - 1 : : - 1 ] : \n        if token [ 1 ] == \":\" : \n            return_identifier_1 = True \n            return return_identifier_1 \n        if token [ 1 ] == \"(\" : \n            return_identifier_2 = False \n            return return_identifier_2 \n        if token [ 1 ] == \"]\" : \n            bracket_level += 1 \n        elif token [ 1 ] == \"[\" : \n            bracket_level -= 1 \n        elif token [ 1 ] == \",\" : \n            if not bracket_level : \n                return_identifier_3 = False \n                return return_identifier_3 \n        elif token [ 1 ] in ( \".\" , \"...\" ) : \n            continue \n        elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : \n            return_identifier_4 = False \n            return return_identifier_4 \n    return_identifier_5 = False \n    return return_identifier_5 "}
{"3847": "\ndef _in_iterating_context ( node ) : \n    parent = node . parent \n    if isinstance ( parent , astroid . For ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if isinstance ( parent , astroid . Comprehension ) : \n        if parent . iter == node : \n            return_identifier_1 = True \n            return return_identifier_1 \n    elif isinstance ( parent , astroid . Call ) : \n        if isinstance ( parent . func , astroid . Name ) : \n            parent_scope = parent . func . lookup ( parent . func . name ) [ 0 ] \n            if _is_builtin ( parent_scope ) and parent . func . name in _ACCEPTS_ITERATOR : \n                return_identifier_2 = True \n                return return_identifier_2 \n        elif isinstance ( parent . func , astroid . Attribute ) : \n            if parent . func . attrname in ATTRIBUTES_ACCEPTS_ITERATOR : \n                return_identifier_3 = True \n                return return_identifier_3 \n        inferred = utils . safe_infer ( parent . func ) \n        if inferred : \n            if inferred . qname ( ) in _BUILTIN_METHOD_ACCEPTS_ITERATOR : \n                return_identifier_4 = True \n                return return_identifier_4 \n            root = inferred . root ( ) \n            if root and root . name == \"itertools\" : \n                return_identifier_5 = True \n                return return_identifier_5 \n    elif isinstance ( parent , astroid . Assign ) and isinstance ( parent . targets [ 0 ] , ( astroid . List , astroid . Tuple ) ) : \n        if len ( parent . targets [ 0 ] . elts ) > 1 : \n            return_identifier_6 = True \n            return return_identifier_6 \n    elif ( isinstance ( parent , astroid . Compare ) and len ( parent . ops ) == 1 and parent . ops [ 0 ] [ 0 ] == \"in\" ) : \n        return_identifier_7 = True \n        return return_identifier_7 \n    elif isinstance ( parent , astroid . YieldFrom ) : \n        return_identifier_8 = True \n        return return_identifier_8 \n    if isinstance ( parent , astroid . Starred ) : \n        return_identifier_9 = True \n        return return_identifier_9 \n    return_identifier_10 = False \n    return return_identifier_10 "}
{"3871": "\ndef is_complex_format_str ( node ) : \n    inferred = utils . safe_infer ( node ) \n    if inferred is None or not isinstance ( inferred . value , str ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    try : \n        parsed = list ( string . Formatter ( ) . parse ( inferred . value ) ) \n    except ValueError : \n        return_identifier_1 = False \n        return return_identifier_1 \n    for _ , _ , format_spec , _ in parsed : \n        if format_spec : \n            return_identifier_2 = True \n            return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"3877": "\ndef in_loop ( node ) : \n    parent = node . parent \n    while parent is not None : \n        if isinstance ( parent , ( astroid . For , astroid . ListComp , astroid . SetComp , astroid . DictComp , astroid . GeneratorExp , ) , ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        parent = parent . parent \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3881": "\ndef _determine_function_name_type ( node , config = None ) : \n    property_classes , property_names = _get_properties ( config ) \n    if not node . is_method ( ) : \n        return_identifier_0 = \"function\" \n        return return_identifier_0 \n    if node . decorators : \n        decorators = node . decorators . nodes \n    else : \n        decorators = [ ] \n    for decorator in decorators : \n        if isinstance ( decorator , astroid . Name ) or ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname in property_names ) : \n            infered = utils . safe_infer ( decorator ) \n            if infered and infered . qname ( ) in property_classes : \n                return_identifier_1 = \"attr\" \n                return return_identifier_1 \n        elif isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"setter\" , \"deleter\" , ) : \n            return_identifier_2 = \"attr\" \n            return return_identifier_2 \n    return_identifier_3 = \"method\" \n    return return_identifier_3 "}
{"3883": "\ndef redefined_by_decorator ( node ) : \n    if node . decorators : \n        for decorator in node . decorators . nodes : \n            if ( isinstance ( decorator , astroid . Attribute ) and getattr ( decorator . expr , \"name\" , None ) == node . name ) : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3899": "\ndef _check_name ( self , node_type , name , node , confidence = interfaces . HIGH ) : \n    def _should_exempt_from_invalid_name ( node ) : \n        if node_type == \"variable\" : \n            inferred = utils . safe_infer ( node ) \n            if isinstance ( inferred , astroid . ClassDef ) : \n                return_identifier_0 = True \n                return return_identifier_0 \n        return_identifier_1 = False \n        return return_identifier_1 \n    if utils . is_inside_except ( node ) : \n        clobbering , _ = utils . clobber_in_except ( node ) \n        if clobbering : \n            return \n    if name in self . config . good_names : \n        return \n    if name in self . config . bad_names : \n        self . stats [ \"badname_\" + node_type ] += 1 \n        self . add_message ( \"blacklisted-name\" , node = node , args = name ) \n        return \n    regexp = self . _name_regexps [ node_type ] \n    match = regexp . match ( name ) \n    if _is_multi_naming_match ( match , node_type , confidence ) : \n        name_group = self . _find_name_group ( node_type ) \n        bad_name_group = self . _bad_names . setdefault ( name_group , { } ) \n        warnings = bad_name_group . setdefault ( match . lastgroup , [ ] ) \n        warnings . append ( ( node , node_type , name , confidence ) ) \n    if match is None and not _should_exempt_from_invalid_name ( node ) : \n        self . _raise_name_warning ( node , node_type , name , confidence ) "}
{"3935": "\ndef _hash_token ( application , token ) : \n    if isinstance ( token , dict ) : \n        hashed_token = tuple ( sorted ( token . items ( ) ) ) \n    elif isinstance ( token , tuple ) : \n        hashed_token = token \n    else : \n        raise TypeError ( '%r is unknown type of token' % token ) \n    return_identifier_0 = ( application . __class__ . __name__ , application . name , hashed_token ) \n    return return_identifier_0 "}
{"3944": "\ndef get_default_realms ( self , client_key , request ) : \n    log . debug ( 'Get realms for %r' , client_key ) \n    if not request . client : \n        request . client = self . _clientgetter ( client_key = client_key ) \n    client = request . client \n    if hasattr ( client , 'default_realms' ) : \n        return client . default_realms \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"3945": "\ndef get_realms ( self , token , request ) : \n    log . debug ( 'Get realms of %r' , token ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if not tok : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    request . request_token = tok \n    if hasattr ( tok , 'realms' ) : \n        return tok . realms or [ ] \n    return_identifier_1 = [ ] \n    return return_identifier_1 "}
{"3948": "\ndef validate_client_key ( self , client_key , request ) : \n    log . debug ( 'Validate client key for %r' , client_key ) \n    if not request . client : \n        request . client = self . _clientgetter ( client_key = client_key ) \n    if request . client : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3949": "\ndef validate_request_token ( self , client_key , token , request ) : \n    log . debug ( 'Validate request token %r for %r' , token , client_key ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if tok and tok . client_key == client_key : \n        request . request_token = tok \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3950": "\ndef validate_access_token ( self , client_key , token , request ) : \n    log . debug ( 'Validate access token %r for %r' , token , client_key ) \n    tok = request . access_token or self . _tokengetter ( client_key = client_key , token = token , ) \n    if tok : \n        request . access_token = tok \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3951": "\ndef validate_timestamp_and_nonce ( self , client_key , timestamp , nonce , request , request_token = None , access_token = None ) : \n    log . debug ( 'Validate timestamp and nonce %r' , client_key ) \n    nonce_exists = self . _noncegetter ( client_key = client_key , timestamp = timestamp , nonce = nonce , request_token = request_token , access_token = access_token ) \n    if nonce_exists : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _noncesetter ( client_key = client_key , timestamp = timestamp , nonce = nonce , request_token = request_token , access_token = access_token ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"3952": "\ndef validate_redirect_uri ( self , client_key , redirect_uri , request ) : \n    log . debug ( 'Validate redirect_uri %r for %r' , redirect_uri , client_key ) \n    if not request . client : \n        request . client = self . _clientgetter ( client_key = client_key ) \n    if not request . client : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not request . client . redirect_uris and redirect_uri is None : \n        return_identifier_1 = True \n        return return_identifier_1 \n    request . redirect_uri = redirect_uri \n    return redirect_uri in request . client . redirect_uris "}
{"3953": "\ndef validate_realms ( self , client_key , token , request , uri = None , realms = None ) : \n    log . debug ( 'Validate realms %r for %r' , realms , client_key ) \n    if request . access_token : \n        tok = request . access_token \n    else : \n        tok = self . _tokengetter ( client_key = client_key , token = token ) \n        request . access_token = tok \n    if not tok : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return set ( tok . realms ) . issuperset ( set ( realms ) ) "}
{"3954": "\ndef validate_verifier ( self , client_key , token , verifier , request ) : \n    log . debug ( 'Validate verifier %r for %r' , verifier , client_key ) \n    data = self . _verifiergetter ( verifier = verifier , token = token ) \n    if not data : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not hasattr ( data , 'user' ) : \n        log . debug ( 'Verifier should has user attribute' ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    request . user = data . user \n    if hasattr ( data , 'client_key' ) : \n        return data . client_key == client_key \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"3955": "\ndef verify_request_token ( self , token , request ) : \n    log . debug ( 'Verify request token %r' , token ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if tok : \n        request . request_token = tok \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3956": "\ndef verify_realms ( self , token , realms , request ) : \n    log . debug ( 'Verify realms %r' , realms ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if not tok : \n        return_identifier_0 = False \n        return return_identifier_0 \n    request . request_token = tok \n    if not hasattr ( tok , 'realms' ) : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return set ( tok . realms ) == set ( realms ) "}
{"3960": "\ndef error_uri ( self ) : \n    error_uri = self . app . config . get ( 'OAUTH2_PROVIDER_ERROR_URI' ) \n    if error_uri : \n        return error_uri \n    error_endpoint = self . app . config . get ( 'OAUTH2_PROVIDER_ERROR_ENDPOINT' ) \n    if error_endpoint : \n        return url_for ( error_endpoint ) \n    return_identifier_0 = '/oauth/errors' \n    return return_identifier_0 "}
{"3964": "\ndef client_authentication_required ( self , request , * args , ** kwargs ) : \n    def is_confidential ( client ) : \n        if hasattr ( client , 'is_confidential' ) : \n            return client . is_confidential \n        client_type = getattr ( client , 'client_type' , None ) \n        if client_type : \n            return client_type == 'confidential' \n        return_identifier_0 = True \n        return return_identifier_0 \n    grant_types = ( 'password' , 'authorization_code' , 'refresh_token' ) \n    client_id , _ = self . _get_client_creds_from_request ( request ) \n    if client_id and request . grant_type in grant_types : \n        client = self . _clientgetter ( client_id ) \n        if client : \n            return is_confidential ( client ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3965": "\ndef authenticate_client ( self , request , * args , ** kwargs ) : \n    client_id , client_secret = self . _get_client_creds_from_request ( request ) \n    log . debug ( 'Authenticate client %r' , client_id ) \n    client = self . _clientgetter ( client_id ) \n    if not client : \n        log . debug ( 'Authenticate client failed, client not found.' ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    request . client = client \n    if hasattr ( client , 'client_secret' ) and client . client_secret != client_secret : \n        log . debug ( 'Authenticate client failed, secret not match.' ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    log . debug ( 'Authenticate client success.' ) \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"3966": "\ndef authenticate_client_id ( self , client_id , request , * args , ** kwargs ) : \n    if client_id is None : \n        client_id , _ = self . _get_client_creds_from_request ( request ) \n    log . debug ( 'Authenticate client %r.' , client_id ) \n    client = request . client or self . _clientgetter ( client_id ) \n    if not client : \n        log . debug ( 'Authenticate failed, client not found.' ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    request . client = client \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"3968": "\ndef confirm_scopes ( self , refresh_token , scopes , request , * args , ** kwargs ) : \n    if not scopes : \n        log . debug ( 'Scope omitted for refresh token %r' , refresh_token ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    log . debug ( 'Confirm scopes %r for refresh token %r' , scopes , refresh_token ) \n    tok = self . _tokengetter ( refresh_token = refresh_token ) \n    return set ( tok . scopes ) == set ( scopes ) "}
{"3974": "\ndef validate_bearer_token ( self , token , scopes , request ) : \n    log . debug ( 'Validate bearer token %r' , token ) \n    tok = self . _tokengetter ( access_token = token ) \n    if not tok : \n        msg = 'Bearer token not found.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if tok . expires is not None and datetime . datetime . utcnow ( ) > tok . expires : \n        msg = 'Bearer token is expired.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    if scopes and not set ( tok . scopes ) & set ( scopes ) : \n        msg = 'Bearer token scope not valid.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    request . access_token = tok \n    request . user = tok . user \n    request . scopes = scopes \n    if hasattr ( tok , 'client' ) : \n        request . client = tok . client \n    elif hasattr ( tok , 'client_id' ) : \n        request . client = self . _clientgetter ( tok . client_id ) \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"3975": "\ndef validate_client_id ( self , client_id , request , * args , ** kwargs ) : \n    log . debug ( 'Validate client %r' , client_id ) \n    client = request . client or self . _clientgetter ( client_id ) \n    if client : \n        request . client = client \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3976": "\ndef validate_code ( self , client_id , code , client , request , * args , ** kwargs ) : \n    client = client or self . _clientgetter ( client_id ) \n    log . debug ( 'Validate code for client %r and code %r' , client . client_id , code ) \n    grant = self . _grantgetter ( client_id = client . client_id , code = code ) \n    if not grant : \n        log . debug ( 'Grant not found.' ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : \n        log . debug ( 'Grant is expired.' ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    request . state = kwargs . get ( 'state' ) \n    request . user = grant . user \n    request . scopes = grant . scopes \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"3977": "\ndef validate_grant_type ( self , client_id , grant_type , client , request , * args , ** kwargs ) : \n    if self . _usergetter is None and grant_type == 'password' : \n        log . debug ( 'Password credential authorization is disabled.' ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    default_grant_types = ( 'authorization_code' , 'password' , 'client_credentials' , 'refresh_token' , ) \n    if hasattr ( client , 'allowed_grant_types' ) : \n        if grant_type not in client . allowed_grant_types : \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        if grant_type not in default_grant_types : \n            return_identifier_2 = False \n            return return_identifier_2 \n    if grant_type == 'client_credentials' : \n        if not hasattr ( client , 'user' ) : \n            log . debug ( 'Client should have a user property' ) \n            return_identifier_3 = False \n            return return_identifier_3 \n        request . user = client . user \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"3978": "\ndef validate_refresh_token ( self , refresh_token , client , request , * args , ** kwargs ) : \n    token = self . _tokengetter ( refresh_token = refresh_token ) \n    if token and token . client_id == client . client_id : \n        request . client_id = token . client_id \n        request . user = token . user \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"3979": "\ndef validate_response_type ( self , client_id , response_type , client , request , * args , ** kwargs ) : \n    if response_type not in ( 'code' , 'token' ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if hasattr ( client , 'allowed_response_types' ) : \n        return response_type in client . allowed_response_types \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"3981": "\ndef validate_user ( self , username , password , client , request , * args , ** kwargs ) : \n    log . debug ( 'Validating username %r and its password' , username ) \n    if self . _usergetter is not None : \n        user = self . _usergetter ( username , password , client , request , * args , ** kwargs ) \n        if user : \n            request . user = user \n            return_identifier_0 = True \n            return return_identifier_0 \n        return_identifier_1 = False \n        return return_identifier_1 \n    log . debug ( 'Password credential authorization is disabled.' ) \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"3982": "\ndef revoke_token ( self , token , token_type_hint , request , * args , ** kwargs ) : \n    if token_type_hint : \n        tok = self . _tokengetter ( ** { token_type_hint : token } ) \n    else : \n        tok = self . _tokengetter ( access_token = token ) \n        if not tok : \n            tok = self . _tokengetter ( refresh_token = token ) \n    if tok : \n        request . client_id = tok . client_id \n        request . user = tok . user \n        tok . delete ( ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    msg = 'Invalid token supplied.' \n    log . debug ( msg ) \n    request . error_message = msg \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4019": "\ndef set_tlsext_servername_callback ( self , callback ) : \n    \n    @ wraps ( callback ) \n    def wrapper ( ssl , alert , arg ) : \n        callback ( Connection . _reverse_mapping [ ssl ] ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    self . _tlsext_servername_callback = _ffi . callback ( \"int (*)(SSL *, int *, void *)\" , wrapper ) \n    _lib . SSL_CTX_set_tlsext_servername_callback ( self . _context , self . _tlsext_servername_callback ) "}
{"4033": "\ndef renegotiate ( self ) : \n    if not self . renegotiate_pending ( ) : \n        _openssl_assert ( _lib . SSL_renegotiate ( self . _ssl ) == 1 ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4034": "\ndef shutdown ( self ) : \n    result = _lib . SSL_shutdown ( self . _ssl ) \n    if result < 0 : \n        self . _raise_ssl_error ( self . _ssl , result ) \n    elif result > 0 : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"4036": "\ndef get_client_ca_list ( self ) : \n    ca_names = _lib . SSL_get_client_CA_list ( self . _ssl ) \n    if ca_names == _ffi . NULL : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    result = [ ] \n    for i in range ( _lib . sk_X509_NAME_num ( ca_names ) ) : \n        name = _lib . sk_X509_NAME_value ( ca_names , i ) \n        copy = _lib . X509_NAME_dup ( name ) \n        _openssl_assert ( copy != _ffi . NULL ) \n        pyname = X509Name . __new__ ( X509Name ) \n        pyname . _name = _ffi . gc ( copy , _lib . X509_NAME_free ) \n        result . append ( pyname ) \n    return result "}
{"4049": "\ndef get_alpn_proto_negotiated ( self ) : \n    data = _ffi . new ( \"unsigned char **\" ) \n    data_len = _ffi . new ( \"unsigned int *\" ) \n    _lib . SSL_get0_alpn_selected ( self . _ssl , data , data_len ) \n    if not data_len : \n        return_identifier_0 = b'' \n        return return_identifier_0 \n    return _ffi . buffer ( data [ 0 ] , data_len [ 0 ] ) [ : ] "}
{"4062": "\ndef check ( self ) : \n    if self . _only_public : \n        raise TypeError ( \"public key only\" ) \n    if _lib . EVP_PKEY_type ( self . type ( ) ) != _lib . EVP_PKEY_RSA : \n        raise TypeError ( \"key type unsupported\" ) \n    rsa = _lib . EVP_PKEY_get1_RSA ( self . _pkey ) \n    rsa = _ffi . gc ( rsa , _lib . RSA_free ) \n    result = _lib . RSA_check_key ( rsa ) \n    if result : \n        return_identifier_0 = True \n        return return_identifier_0 \n    _raise_current_error ( ) "}
{"4115": "\ndef verify ( self , key ) : \n    answer = _lib . NETSCAPE_SPKI_verify ( self . _spki , key . _pkey ) \n    if answer <= 0 : \n        _raise_current_error ( ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"4122": "\ndef get_service_metadata ( self ) : \n    return_identifier_0 = { 'import_labels_as_tags' : self . config . get ( 'import_labels_as_tags' , False , asbool ) , 'label_template' : self . config . get ( 'label_template' , DEFAULT_LABEL_TEMPLATE ) , } \n    return return_identifier_0 "}
{"4138": "\ndef include ( self , issue ) : \n    only_if_assigned = self . config . get ( 'only_if_assigned' , None ) \n    if only_if_assigned : \n        owner = self . get_owner ( issue ) \n        include_owners = [ only_if_assigned ] \n        if self . config . get ( 'also_unassigned' , None , asbool ) : \n            include_owners . append ( None ) \n        return owner in include_owners \n    only_if_author = self . config . get ( 'only_if_author' , None ) \n    if only_if_author : \n        return self . get_author ( issue ) == only_if_author \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"4147": "\ndef build_uda_config_overrides ( targets ) : \n    from bugwarrior . services import get_service \n    targets_udas = { } \n    for target in targets : \n        targets_udas . update ( get_service ( target ) . ISSUE_CLASS . UDAS ) \n    return_identifier_0 = { 'uda' : targets_udas } \n    return return_identifier_0 "}
{"4252": "\ndef v_cross ( u , v ) : \n    i = '(({u1})*({v2}) - ({u2})*({v1}))' . format ( u1 = u [ 1 ] , u2 = u [ 2 ] , v1 = v [ 1 ] , v2 = v [ 2 ] ) \n    j = '(({u2})*({v0}) - ({u0})*({v2}))' . format ( u0 = u [ 0 ] , u2 = u [ 2 ] , v0 = v [ 0 ] , v2 = v [ 2 ] ) \n    k = '(({u0})*({v1}) - ({u1})*({v0}))' . format ( u0 = u [ 0 ] , u1 = u [ 1 ] , v0 = v [ 0 ] , v1 = v [ 1 ] ) \n    return_identifier_0 = [ i , j , k ] \n    return return_identifier_0 "}
{"4283": "\ndef run ( self ) : \n    existing_objects = _get_in_memory_objects ( ) \n    prof , result = self . profile ( ) \n    new_objects = _get_in_memory_objects ( ) \n    new_obj_count = _get_obj_count_difference ( new_objects , existing_objects ) \n    result_obj_count = new_obj_count - prof . obj_overhead \n    result_obj_count [ list ] -= 1 \n    pretty_obj_count = _format_obj_count ( result_obj_count ) \n    return_identifier_0 = { 'objectName' : self . _object_name , 'codeEvents' : prof . code_events , 'totalEvents' : len ( prof . code_events ) , 'objectsCount' : pretty_obj_count , 'result' : result , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4286": "\ndef get_run_object_type ( run_object ) : \n    if isinstance ( run_object , tuple ) : \n        return_identifier_0 = 'function' \n        return return_identifier_0 \n    run_object , _ , _ = run_object . partition ( ' ' ) \n    if os . path . isdir ( run_object ) : \n        return_identifier_1 = 'package' \n        return return_identifier_1 \n    return_identifier_2 = 'module' \n    return return_identifier_2 "}
{"4294": "\ndef _format_tree ( self , node , total_samples ) : \n    funcname , filename , _ = node [ 'stack' ] \n    sample_percent = self . _get_percentage ( node [ 'sampleCount' ] , total_samples ) \n    color_hash = base_profiler . hash_name ( '%s @ %s' % ( funcname , filename ) ) \n    return_identifier_0 = { 'stack' : node [ 'stack' ] , 'children' : [ self . _format_tree ( child , total_samples ) for child in node [ 'children' ] ] , 'sampleCount' : node [ 'sampleCount' ] , 'samplePercentage' : sample_percent , 'colorHash' : color_hash } \n    return return_identifier_0 "}
{"4295": "\ndef call_tree ( self ) : \n    call_tree = { 'stack' : 'base' , 'sampleCount' : 0 , 'children' : [ ] } \n    for stack , sample_count in self . _stats . items ( ) : \n        self . _insert_stack ( reversed ( stack ) , sample_count , call_tree ) \n    self . _fill_sample_count ( call_tree ) \n    if not call_tree [ 'children' ] : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    return self . _format_tree ( call_tree [ 'children' ] [ 0 ] , call_tree [ 'sampleCount' ] ) "}
{"4296": "\ndef _profile_package ( self ) : \n    with _StatProfiler ( ) as prof : \n        prof . base_frame = inspect . currentframe ( ) \n        try : \n            runpy . run_path ( self . _run_object , run_name = '__main__' ) \n        except SystemExit : \n            pass \n    call_tree = prof . call_tree \n    return_identifier_0 = { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , 0 ) , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4297": "\ndef _profile_module ( self ) : \n    with open ( self . _run_object , 'rb' ) as srcfile , _StatProfiler ( ) as prof : \n        code = compile ( srcfile . read ( ) , self . _run_object , 'exec' ) \n        prof . base_frame = inspect . currentframe ( ) \n        try : \n            exec ( code , self . _globs , None ) \n        except SystemExit : \n            pass \n    call_tree = prof . call_tree \n    return_identifier_0 = { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , 0 ) , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4298": "\ndef profile_function ( self ) : \n    with _StatProfiler ( ) as prof : \n        result = self . _run_object ( * self . _run_args , ** self . _run_kwargs ) \n    call_tree = prof . call_tree \n    return_identifier_0 = { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , 0 ) , 'result' : result , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4300": "\ndef _profile_package ( self ) : \n    prof = cProfile . Profile ( ) \n    prof . enable ( ) \n    try : \n        runpy . run_path ( self . _run_object , run_name = '__main__' ) \n    except SystemExit : \n        pass \n    prof . disable ( ) \n    prof_stats = pstats . Stats ( prof ) \n    prof_stats . calc_callees ( ) \n    return_identifier_0 = { 'objectName' : self . _object_name , 'callStats' : self . _transform_stats ( prof_stats ) , 'totalTime' : prof_stats . total_tt , 'primitiveCalls' : prof_stats . prim_calls , 'totalCalls' : prof_stats . total_calls , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4301": "\ndef _profile_module ( self ) : \n    prof = cProfile . Profile ( ) \n    try : \n        with open ( self . _run_object , 'rb' ) as srcfile : \n            code = compile ( srcfile . read ( ) , self . _run_object , 'exec' ) \n        prof . runctx ( code , self . _globs , None ) \n    except SystemExit : \n        pass \n    prof_stats = pstats . Stats ( prof ) \n    prof_stats . calc_callees ( ) \n    return_identifier_0 = { 'objectName' : self . _object_name , 'callStats' : self . _transform_stats ( prof_stats ) , 'totalTime' : prof_stats . total_tt , 'primitiveCalls' : prof_stats . prim_calls , 'totalCalls' : prof_stats . total_calls , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4302": "\ndef profile_function ( self ) : \n    prof = cProfile . Profile ( ) \n    prof . enable ( ) \n    result = self . _run_object ( * self . _run_args , ** self . _run_kwargs ) \n    prof . disable ( ) \n    prof_stats = pstats . Stats ( prof ) \n    prof_stats . calc_callees ( ) \n    return_identifier_0 = { 'objectName' : self . _object_name , 'callStats' : self . _transform_stats ( prof_stats ) , 'totalTime' : prof_stats . total_tt , 'primitiveCalls' : prof_stats . prim_calls , 'totalCalls' : prof_stats . total_calls , 'result' : result , 'timestamp' : int ( time . time ( ) ) } \n    return return_identifier_0 "}
{"4313": "\ndef check_standard_dir ( module_path ) : \n    if 'site-packages' in module_path : \n        return_identifier_0 = True \n        return return_identifier_0 \n    for stdlib_path in _STDLIB_PATHS : \n        if fnmatch . fnmatchcase ( module_path , stdlib_path + '*' ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"4318": "\ndef _profile_package ( self ) : \n    with _CodeHeatmapCalculator ( ) as prof : \n        try : \n            runpy . run_path ( self . _run_object , run_name = '__main__' ) \n        except SystemExit : \n            pass \n    heatmaps = [ ] \n    for filename , heatmap in prof . heatmap . items ( ) : \n        if os . path . isfile ( filename ) : \n            heatmaps . append ( self . _format_heatmap ( filename , heatmap , prof . execution_count [ filename ] ) ) \n    run_time = sum ( heatmap [ 'runTime' ] for heatmap in heatmaps ) \n    return_identifier_0 = { 'objectName' : self . _run_object , 'runTime' : run_time , 'heatmaps' : heatmaps } \n    return return_identifier_0 "}
{"4319": "\ndef _format_heatmap ( self , filename , heatmap , execution_count ) : \n    with open ( filename ) as src_file : \n        file_source = src_file . read ( ) . split ( '\\n' ) \n        skip_map = self . _calc_skips ( heatmap , len ( file_source ) ) \n    run_time = sum ( time for time in heatmap . values ( ) ) \n    return_identifier_0 = { 'name' : filename , 'heatmap' : heatmap , 'executionCount' : execution_count , 'srcCode' : self . _skip_lines ( file_source , skip_map ) , 'runTime' : run_time } \n    return return_identifier_0 "}
{"4320": "\ndef _profile_module ( self ) : \n    with open ( self . _run_object , 'r' ) as srcfile : \n        src_code = srcfile . read ( ) \n        code = compile ( src_code , self . _run_object , 'exec' ) \n    try : \n        with _CodeHeatmapCalculator ( ) as prof : \n            exec ( code , self . _globs , None ) \n    except SystemExit : \n        pass \n    heatmaps = [ ] \n    for filename , heatmap in prof . heatmap . items ( ) : \n        if os . path . isfile ( filename ) : \n            heatmaps . append ( self . _format_heatmap ( filename , heatmap , prof . execution_count [ filename ] ) ) \n    run_time = sum ( heatmap [ 'runTime' ] for heatmap in heatmaps ) \n    return_identifier_0 = { 'objectName' : self . _run_object , 'runTime' : run_time , 'heatmaps' : heatmaps } \n    return return_identifier_0 "}
{"4321": "\ndef profile_function ( self ) : \n    with _CodeHeatmapCalculator ( ) as prof : \n        result = self . _run_object ( * self . _run_args , ** self . _run_kwargs ) \n    code_lines , start_line = inspect . getsourcelines ( self . _run_object ) \n    source_lines = [ ] \n    for line in code_lines : \n        source_lines . append ( ( 'line' , start_line , line ) ) \n        start_line += 1 \n    filename = os . path . abspath ( inspect . getsourcefile ( self . _run_object ) ) \n    heatmap = prof . heatmap [ filename ] \n    run_time = sum ( time for time in heatmap . values ( ) ) \n    return_identifier_0 = { 'objectName' : self . _object_name , 'runTime' : run_time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . _object_name , 'heatmap' : heatmap , 'executionCount' : prof . execution_count [ filename ] , 'srcCode' : source_lines , 'runTime' : run_time } ] } \n    return return_identifier_0 "}
{"4346": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    def mapper ( X ) : \n        X = check_array ( X , ( 'csr' , 'csc' ) , dtype = np . float64 ) \n        if hasattr ( X , \"toarray\" ) : \n            mean , var = mean_variance_axis ( X , axis = 0 ) \n        else : \n            mean , var = np . mean ( X , axis = 0 ) , np . var ( X , axis = 0 ) \n        return X . shape [ 0 ] , mean , var \n    def reducer ( a , b ) : \n        n_a , mean_a , var_a = a \n        n_b , mean_b , var_b = b \n        n_ab = n_a + n_b \n        mean_ab = ( ( mean_a * n_a ) + ( mean_b * n_b ) ) / n_ab \n        var_ab = ( ( ( n_a * var_a ) + ( n_b * var_b ) ) / n_ab ) + ( ( n_a * n_b ) * ( ( mean_b - mean_a ) / n_ab ) ** 2 ) \n        return_identifier_0 = ( n_ab , mean_ab , var_ab ) \n        return return_identifier_0 \n    _ , _ , self . variances_ = X . map ( mapper ) . treeReduce ( reducer ) \n    if np . all ( self . variances_ <= self . threshold ) : \n        msg = \"No feature in X meets the variance threshold {0:.5f}\" \n        if X . shape [ 0 ] == 1 : \n            msg += \" (X contains only one sample)\" \n        raise ValueError ( msg . format ( self . threshold ) ) \n    return self "}
{"4357": "\ndef only_root_write ( path ) : \n    s = os . stat ( path ) \n    for ug , bp in [ ( s . st_uid , bitperm ( s , 'w' , 'usr' ) ) , ( s . st_gid , bitperm ( s , 'w' , 'grp' ) ) ] : \n        if ug and bp : \n            return_identifier_0 = False \n            return return_identifier_0 \n    if bitperm ( s , 'w' , 'oth' ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"4403": "\ndef _connectIfNecessarySingle ( self , node ) : \n    if node in self . _connections and self . _connections [ node ] . state != CONNECTION_STATE . DISCONNECTED : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if not self . _shouldConnect ( node ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    assert node in self . _connections \n    if node in self . _lastConnectAttempt and time . time ( ) - self . _lastConnectAttempt [ node ] < self . _syncObj . conf . connectionRetryTime : \n        return_identifier_2 = False \n        return return_identifier_2 \n    self . _lastConnectAttempt [ node ] = time . time ( ) \n    return self . _connections [ node ] . connect ( node . ip , node . port ) "}
{"4409": "\ndef send ( self , node , message ) : \n    if node not in self . _connections or self . _connections [ node ] . state != CONNECTION_STATE . CONNECTED : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _connections [ node ] . send ( message ) \n    if self . _connections [ node ] . state != CONNECTION_STATE . CONNECTED : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"4411": "\ndef put ( self , item ) : \n    if self . __maxsize and len ( self . __data ) >= self . __maxsize : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . __data . append ( item ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"4412": "\ndef put ( self , item ) : \n    if self . __maxsize and len ( self . __data ) >= self . __maxsize : \n        return_identifier_0 = False \n        return return_identifier_0 \n    heapq . heappush ( self . __data , item ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"4418": "\ndef token_required ( view_func ) : \n    def _parse_auth_header ( auth_header ) : \n        reg = re . compile ( '(\\w+)[=] ?\"?([\\w-]+)\"?' ) \n        header_dict = dict ( reg . findall ( auth_header ) ) \n        return header_dict [ 'Token' ] \n    def _get_passed_token ( request ) : \n        try : \n            auth_header = request . META [ 'HTTP_AUTHORIZATION' ] \n            token = _parse_auth_header ( auth_header ) \n        except KeyError : \n            token = request . GET . get ( settings . WATCHMAN_TOKEN_NAME ) \n        return token \n    def _validate_token ( request ) : \n        if settings . WATCHMAN_TOKENS : \n            watchman_tokens = settings . WATCHMAN_TOKENS . split ( ',' ) \n        elif settings . WATCHMAN_TOKEN : \n            watchman_tokens = [ settings . WATCHMAN_TOKEN , ] \n        else : \n            return_identifier_0 = True \n            return return_identifier_0 \n        return _get_passed_token ( request ) in watchman_tokens \n    \n    @ csrf_exempt \n    @ wraps ( view_func ) \n    def _wrapped_view ( request , * args , ** kwargs ) : \n        if _validate_token ( request ) : \n            return view_func ( request , * args , ** kwargs ) \n        return HttpResponseForbidden ( ) \n    return _wrapped_view "}
{"4467": "\ndef get_studies_by_regions ( dataset , masks , threshold = 0.08 , remove_overlap = True , studies = None , features = None , regularization = \"scale\" ) : \n    import nibabel as nib \n    import os \n    try : \n        loaded_masks = [ nib . load ( os . path . relpath ( m ) ) for m in masks ] \n    except OSError : \n        print ( 'Error loading masks. Check the path' ) \n    grouped_ids = [ dataset . get_studies ( mask = m , activation_threshold = threshold ) for m in loaded_masks ] \n    flat_ids = reduce ( lambda a , b : a + b , grouped_ids ) \n    if remove_overlap : \n        import collections \n        flat_ids = [ id for ( id , count ) in collections . Counter ( flat_ids ) . items ( ) if count == 1 ] \n        grouped_ids = [ [ x for x in m if x in flat_ids ] for m in grouped_ids ] \n    y = [ [ idx ] * len ( ids ) for ( idx , ids ) in enumerate ( grouped_ids ) ] \n    y = reduce ( lambda a , b : a + b , y ) \n    y = np . array ( y ) \n    X = [ dataset . get_feature_data ( ids = group_ids , features = features ) for group_ids in grouped_ids ] \n    X = np . vstack ( tuple ( X ) ) \n    if regularization : \n        X = regularize ( X , method = regularization ) \n    return_identifier_0 = ( X , y ) \n    return return_identifier_0 "}
{"4561": "\ndef get ( self ) : \n    domain_validation = self . checker . is_domain_valid ( ) \n    ip_validation = self . checker . is_ip_valid ( ) \n    if \"current_test_data\" in PyFunceble . INTERN : \n        PyFunceble . INTERN [ \"current_test_data\" ] . update ( { \"domain_syntax_validation\" : domain_validation , \"ip4_syntax_validation\" : ip_validation , } ) \n    if ( domain_validation and not ip_validation or domain_validation or PyFunceble . CONFIGURATION [ \"local\" ] ) : \n        PyFunceble . INTERN . update ( { \"http_code\" : HTTPCode ( ) . get ( ) , \"referer\" : Referer ( ) . get ( ) } ) \n        if not PyFunceble . INTERN [ \"referer\" ] : \n            return PyFunceble . INTERN [ \"referer\" ] \n        if PyFunceble . INTERN [ \"referer\" ] and not self . checker . is_subdomain ( ) : \n            return self . _extract ( ) \n        Logs ( ) . whois ( self . whois_record ) \n        return None \n    if ( ip_validation and not domain_validation or ip_validation or PyFunceble . CONFIGURATION [ \"local\" ] ) : \n        PyFunceble . INTERN [ \"http_code\" ] = HTTPCode ( ) . get ( ) \n        Logs ( ) . whois ( self . whois_record ) \n        return None \n    Logs ( ) . whois ( self . whois_record ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"4564": "\ndef _is_version_greater ( self ) : \n    checked = Version ( True ) . check_versions ( self . current_version [ 0 ] , self . version_yaml ) \n    if checked is not None and not checked : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4565": "\ndef is_dev_version ( cls ) : \n    command = \"git branch\" \n    command_result = Command ( command ) . execute ( ) \n    for branch in command_result . split ( \"\\n\" ) : \n        if branch . startswith ( \"*\" ) and \"dev\" in branch : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4566": "\ndef _does_require_deprecation ( self ) : \n    for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2 ] ) : \n        if version_number > self . version_yaml [ index ] : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4569": "\ndef _is_to_ignore ( cls , line ) : \n    to_ignore = [ r\"(^!|^@@|^\\/|^\\[|^\\.|^-|^_|^\\?|^&)\" ] \n    for element in to_ignore : \n        if Regex ( line , element , return_data = False ) . match ( ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4570": "\ndef _handle_options ( self , options ) : \n    result = [ ] \n    regex_domain_option = r\"domain=(.*)\" \n    for option in options : \n        try : \n            domains = Regex ( option , regex_domain_option , return_data = True , rematch = True , group = 0 ) . match ( ) [ - 1 ] \n            if domains : \n                if self . aggressive : \n                    result . extend ( [ x for x in domains . split ( \"|\" ) if x and not x . startswith ( \"~\" ) ] ) \n                else : \n                    return_identifier_0 = True \n                    return return_identifier_0 \n        except TypeError : \n            pass \n    return result "}
{"4582": "\ndef _entry_management_url_download ( self , passed ) : \n    if passed and self . checker . is_url_valid ( passed ) : \n        file_to_test = passed . split ( \"/\" ) [ - 1 ] \n        if ( not PyFunceble . path . isfile ( file_to_test ) or PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] == 0 ) : \n            Download ( passed , file_to_test ) . text ( ) \n        PyFunceble . INTERN [ \"file_to_test\" ] = file_to_test \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4589": "\ndef _format_domain ( cls , extracted_domain ) : \n    if not extracted_domain . startswith ( \"#\" ) : \n        if \"#\" in extracted_domain : \n            extracted_domain = extracted_domain [ : extracted_domain . find ( \"#\" ) ] . strip ( ) \n        if \" \" in extracted_domain or \"\\t\" in extracted_domain : \n            splited_line = extracted_domain . split ( ) \n            index = 1 \n            while index < len ( splited_line ) : \n                if splited_line [ index ] : \n                    break \n                index += 1 \n            return splited_line [ index ] \n        return extracted_domain \n    return_identifier_0 = \"\" \n    return return_identifier_0 "}
{"4593": "\ndef switch ( cls , variable , custom = False ) : \n    if not custom : \n        current_state = dict . get ( PyFunceble . CONFIGURATION , variable ) \n    else : \n        current_state = variable \n    if isinstance ( current_state , bool ) : \n        if current_state : \n            return_identifier_0 = False \n            return return_identifier_0 \n        return_identifier_1 = True \n        return return_identifier_1 \n    to_print = \"Impossible to switch %s. Please post an issue to %s\" \n    raise Exception ( to_print % ( repr ( variable ) , PyFunceble . LINKS [ \"repo\" ] + \"/issues.\" ) ) "}
{"4599": "\ndef _set_path_to_configs ( cls , path_to_config ) : \n    if not path_to_config . endswith ( PyFunceble . directory_separator ) : \n        default = parsed = path_to_config + PyFunceble . directory_separator \n    else : \n        default = parsed = path_to_config \n    parsed += PyFunceble . CONFIGURATION_FILENAME \n    default += PyFunceble . DEFAULT_CONFIGURATION_FILENAME \n    return_identifier_0 = ( parsed , default ) \n    return return_identifier_0 "}
{"4604": "\ndef _install_directory_structure_file ( cls ) : \n    dir_structure_link = PyFunceble . CONFIGURATION [ \"links\" ] [ \"dir_structure\" ] \n    dir_structure_link = Version ( True ) . right_url_from_version ( dir_structure_link ) \n    destination = ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . CONFIGURATION [ \"outputs\" ] [ \"default_files\" ] [ \"dir_structure\" ] ) \n    if not Version ( True ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : \n        data = Download ( dir_structure_link , destination , return_data = True ) . text ( ) \n        File ( destination ) . write ( data , overwrite = True ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return None "}
{"4607": "\ndef split_versions ( cls , version , return_non_digits = False ) : \n    splited_version = version . split ( \".\" ) \n    digits = [ x for x in splited_version if x . isdigit ( ) ] \n    if not return_non_digits : \n        return digits \n    non_digits = [ x for x in splited_version if not x . isdigit ( ) ] \n    return_identifier_0 = ( digits , non_digits [ 0 ] ) \n    return return_identifier_0 "}
{"4608": "\ndef check_versions ( cls , local , upstream ) : \n    status = [ None , None , None ] \n    for index , version_number in enumerate ( local ) : \n        if int ( version_number ) < int ( upstream [ index ] ) : \n            status [ index ] = True \n        elif int ( version_number ) > int ( upstream [ index ] ) : \n            status [ index ] = False \n    if False in status : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if True in status : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return None "}
{"4609": "\ndef is_cloned ( cls ) : \n    if not PyFunceble . path . isdir ( \".git\" ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    list_of_file = [ \".coveragerc\" , \".coveralls.yml\" , \".gitignore\" , \".PyFunceble_production.yaml\" , \".travis.yml\" , \"CODE_OF_CONDUCT.md\" , \"CONTRIBUTING.md\" , \"dir_structure_production.json\" , \"MANIFEST.in\" , \"README.rst\" , \"requirements.txt\" , \"setup.py\" , \"version.yaml\" , ] \n    list_of_dir = [ \"docs\" , \"PyFunceble\" , \"tests\" ] \n    for file in list_of_file : \n        if not PyFunceble . path . isfile ( file ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n    for directory in list_of_dir : \n        if not PyFunceble . path . isdir ( directory ) : \n            return_identifier_2 = False \n            return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"4614": "\ndef _do_not_produce_file ( self ) : \n    if ( Inactive ( ) . is_present ( ) and self . domain_status in [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , ] and PyFunceble . INTERN [ \"to_test\" ] not in PyFunceble . INTERN [ \"extracted_list_to_test\" ] ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4630": "\ndef _get_content ( cls , file ) : \n    if PyFunceble . path . isfile ( file ) : \n        return Dict ( ) . from_json ( File ( file ) . read ( ) ) \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"4636": "\ndef _header_constructor ( cls , data_to_print , header_separator = \"-\" , column_separator = \" \" ) : \n    header_data = [ ] \n    header_size = \"\" \n    before_size = \"%-\" \n    after_size = \"s\" \n    if header_separator : \n        header_separator_data = [ ] \n    length_data_to_print = len ( data_to_print ) - 1 \n    i = 0 \n    for data in data_to_print : \n        size = data_to_print [ data ] \n        header_data . append ( data ) \n        header_size += before_size + str ( size ) + after_size \n        if i < length_data_to_print : \n            header_size += column_separator \n        if header_separator : \n            header_separator_data . append ( header_separator * size ) \n        i += 1 \n    if header_separator : \n        return_identifier_0 = [ header_size % tuple ( header_data ) , header_size % tuple ( header_separator_data ) , ] \n        return return_identifier_0 \n    return_identifier_1 = [ header_size % tuple ( header_data ) ] \n    return return_identifier_1 "}
{"4664": "\ndef match ( self ) : \n    result = [ ] \n    to_match = comp ( self . regex ) \n    if self . rematch : \n        pre_result = to_match . findall ( self . data ) \n    else : \n        pre_result = to_match . search ( self . data ) \n    if self . return_data and pre_result : \n        if self . rematch : \n            for data in pre_result : \n                if isinstance ( data , tuple ) : \n                    result . extend ( list ( data ) ) \n                else : \n                    result . append ( data ) \n            if self . group != 0 : \n                return result [ self . group ] \n        else : \n            result = pre_result . group ( self . group ) . strip ( ) \n        return result \n    if not self . return_data and pre_result : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4669": "\ndef is_url_valid ( self , url = None , return_base = False , return_formatted = False ) : \n    initial_base = None \n    if url : \n        to_test = url \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    if to_test . startswith ( \"http\" ) : \n        try : \n            regex = r\"(^(http:\\/\\/|https:\\/\\/)(.+?(?=\\/)|.+?$))\" \n            initial_base = base = Regex ( to_test , regex , return_data = True , rematch = True ) . match ( ) [ 2 ] \n            if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n                base = domain2idna ( base ) \n            domain_status = self . is_domain_valid ( base ) \n            ip_status = self . is_ip_valid ( base ) \n            if domain_status or ip_status : \n                if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] and return_formatted : \n                    return Regex ( to_test , initial_base , escape = True , return_data = True , replace_with = base , occurences = 1 , ) . replace ( ) \n                if return_formatted : \n                    return to_test \n                if return_base : \n                    return base \n                return_identifier_0 = True \n                return return_identifier_0 \n        except TypeError : \n            pass \n    if return_formatted : \n        return to_test \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4670": "\ndef is_domain_valid ( self , domain = None , subdomain_check = False ) : \n    regex_valid_domains = r\"^(?=.{0,253}$)(([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9](?:\\.)?|[a-z0-9](?:\\.)?))$\" \n    regex_valid_subdomains = r\"^(?=.{0,253}$)(([a-z0-9_][a-z0-9-_]{0,61}[a-z0-9_-]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9]))$\" \n    if domain : \n        to_test = domain \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    try : \n        last_point_index = to_test . rindex ( \".\" ) \n        extension = to_test [ last_point_index + 1 : ] \n        if not extension and to_test . endswith ( \".\" ) : \n            try : \n                extension = [ x for x in to_test . split ( \".\" ) if x ] [ - 1 ] \n            except IndexError : \n                pass \n        if not extension or extension not in PyFunceble . INTERN [ \"iana_db\" ] : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if ( Regex ( to_test , regex_valid_domains , return_data = False ) . match ( ) and not subdomain_check ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n        if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n            for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                try : \n                    suffix_index = to_test . rindex ( \".\" + suffix ) \n                    to_check = to_test [ : suffix_index ] \n                    if \".\" not in to_check and subdomain_check : \n                        return_identifier_2 = False \n                        return return_identifier_2 \n                    if \".\" in to_check and subdomain_check : \n                        return_identifier_3 = True \n                        return return_identifier_3 \n                    if \".\" in to_check : \n                        return Regex ( to_check , regex_valid_subdomains , return_data = False ) . match ( ) \n                except ValueError : \n                    pass \n        to_check = to_test [ : last_point_index ] \n        if \".\" in to_check and subdomain_check : \n            return_identifier_4 = True \n            return return_identifier_4 \n        if \".\" in to_check : \n            return Regex ( to_check , regex_valid_subdomains , return_data = False ) . match ( ) \n    except ( ValueError , AttributeError ) : \n        pass \n    return_identifier_5 = False \n    return return_identifier_5 "}
{"4678": "\ndef is_present ( cls ) : \n    if PyFunceble . CONFIGURATION [ \"inactive_database\" ] : \n        if PyFunceble . INTERN [ \"to_test\" ] in PyFunceble . INTERN [ \"flatten_inactive_db\" ] or ( PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"inactive_db\" ] and PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] and \"to_test\" in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] and PyFunceble . INTERN [ \"to_test\" ] in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ \"to_test\" ] ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4681": "\ndef is_in_database ( self ) : \n    if ( self . _authorization ( ) and PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] and PyFunceble . INTERN [ \"to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4682": "\ndef is_time_older ( self ) : \n    if ( self . _authorization ( ) and self . is_in_database ( ) and int ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) < int ( PyFunceble . time ( ) ) ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"4687": "\ndef nslookup ( cls ) : \n    try : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            if not Check ( ) . is_ip_valid ( ) : \n                request = PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n                for sequence in request : \n                    PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] . append ( sequence [ - 1 ] [ 0 ] ) \n            else : \n                request = PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ 0 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ 1 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2 ] \n        else : \n            if not Check ( ) . is_ip_valid ( ) : \n                PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n            else : \n                PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"4690": "\ndef get ( self ) : \n    if not PyFunceble . CONFIGURATION [ \"local\" ] : \n        if self . domain_extension not in self . ignored_extension : \n            referer = None \n            if self . domain_extension in PyFunceble . INTERN [ \"iana_db\" ] : \n                if not PyFunceble . CONFIGURATION [ \"no_whois\" ] : \n                    referer = PyFunceble . INTERN [ \"iana_db\" ] [ self . domain_extension ] \n                    if not referer : \n                        Logs ( ) . referer_not_found ( self . domain_extension ) \n                        return None \n                    return referer \n                return None \n            return_identifier_0 = False \n            return return_identifier_0 \n        return None \n    return None "}
{"4701": "\ndef check ( source ) : \n    if sys . version_info [ 0 ] == 2 and isinstance ( source , unicode ) : \n        try : \n            source = source . encode ( 'utf-8' ) \n        except UnicodeError : \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n    reporter = ListReporter ( ) \n    try : \n        pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) \n    except ( AttributeError , RecursionError , UnicodeDecodeError ) : \n        pass \n    return reporter . messages "}
{"4703": "\ndef multiline_import ( line , previous_line = '' ) : \n    for symbol in '()' : \n        if symbol in line : \n            return_identifier_0 = True \n            return return_identifier_0 \n    if line . lstrip ( ) . startswith ( '>' ) : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return multiline_statement ( line , previous_line ) "}
{"4704": "\ndef multiline_statement ( line , previous_line = '' ) : \n    for symbol in '\\\\:;' : \n        if symbol in line : \n            return_identifier_0 = True \n            return return_identifier_0 \n    sio = io . StringIO ( line ) \n    try : \n        list ( tokenize . generate_tokens ( sio . readline ) ) \n        return previous_line . rstrip ( ) . endswith ( '\\\\' ) \n    except ( SyntaxError , tokenize . TokenError ) : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"4710": "\ndef filter_duplicate_key ( line , message , line_number , marked_line_numbers , source , previous_line = '' ) : \n    if marked_line_numbers and line_number == sorted ( marked_line_numbers ) [ 0 ] : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    return line "}
{"4711": "\ndef dict_entry_has_key ( line , key ) : \n    if '#' in line : \n        return_identifier_0 = False \n        return return_identifier_0 \n    result = re . match ( r'\\s*(.*)\\s*:\\s*(.*),\\s*$' , line ) \n    if not result : \n        return_identifier_1 = False \n        return return_identifier_1 \n    try : \n        candidate_key = ast . literal_eval ( result . group ( 1 ) ) \n    except ( SyntaxError , ValueError ) : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if multiline_statement ( result . group ( 2 ) ) : \n        return_identifier_3 = False \n        return return_identifier_3 \n    return candidate_key == key "}
{"4712": "\ndef is_literal_or_name ( value ) : \n    try : \n        ast . literal_eval ( value ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except ( SyntaxError , ValueError ) : \n        pass \n    if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return re . match ( r'^\\w+\\s*$' , value ) "}
{"4715": "\ndef get_indentation ( line ) : \n    if line . strip ( ) : \n        non_whitespace_index = len ( line ) - len ( line . lstrip ( ) ) \n        return line [ : non_whitespace_index ] \n    else : \n        return_identifier_0 = '' \n        return return_identifier_0 "}
{"4716": "\ndef get_line_ending ( line ) : \n    non_whitespace_index = len ( line . rstrip ( ) ) - len ( line ) \n    if not non_whitespace_index : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    else : \n        return line [ non_whitespace_index : ] "}
{"4719": "\ndef is_python_file ( filename ) : \n    if filename . endswith ( '.py' ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    try : \n        with open_with_encoding ( filename , None , limit_byte_check = MAX_PYTHON_FILE_DETECTION_BYTES ) as f : \n            text = f . read ( MAX_PYTHON_FILE_DETECTION_BYTES ) \n            if not text : \n                return_identifier_1 = False \n                return return_identifier_1 \n            first_line = text . splitlines ( ) [ 0 ] \n    except ( IOError , IndexError ) : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if not PYTHON_SHEBANG_REGEX . match ( first_line ) : \n        return_identifier_3 = False \n        return return_identifier_3 \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"4720": "\ndef is_exclude_file ( filename , exclude ) : \n    base_name = os . path . basename ( filename ) \n    if base_name . startswith ( '.' ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    for pattern in exclude : \n        if fnmatch . fnmatch ( base_name , pattern ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n        if fnmatch . fnmatch ( filename , pattern ) : \n            return_identifier_2 = True \n            return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"4722": "\ndef _main ( argv , standard_out , standard_error ) : \n    import argparse \n    parser = argparse . ArgumentParser ( description = __doc__ , prog = 'autoflake' ) \n    parser . add_argument ( '-c' , '--check' , action = 'store_true' , help = 'return error code if changes are needed' ) \n    parser . add_argument ( '-i' , '--in-place' , action = 'store_true' , help = 'make changes to files instead of printing diffs' ) \n    parser . add_argument ( '-r' , '--recursive' , action = 'store_true' , help = 'drill down directories recursively' ) \n    parser . add_argument ( '--exclude' , metavar = 'globs' , help = 'exclude file/directory names that match these ' 'comma-separated globs' ) \n    parser . add_argument ( '--imports' , help = 'by default, only unused standard library ' 'imports are removed; specify a comma-separated ' 'list of additional modules/packages' ) \n    parser . add_argument ( '--expand-star-imports' , action = 'store_true' , help = 'expand wildcard star imports with undefined ' 'names; this only triggers if there is only ' 'one star import in the file; this is skipped if ' 'there are any uses of `__all__` or `del` in the ' 'file' ) \n    parser . add_argument ( '--remove-all-unused-imports' , action = 'store_true' , help = 'remove all unused imports (not just those from ' 'the standard library)' ) \n    parser . add_argument ( '--ignore-init-module-imports' , action = 'store_true' , help = 'exclude __init__.py when removing unused ' 'imports' ) \n    parser . add_argument ( '--remove-duplicate-keys' , action = 'store_true' , help = 'remove all duplicate keys in objects' ) \n    parser . add_argument ( '--remove-unused-variables' , action = 'store_true' , help = 'remove unused variables' ) \n    parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s ' + __version__ ) \n    parser . add_argument ( 'files' , nargs = '+' , help = 'files to format' ) \n    args = parser . parse_args ( argv [ 1 : ] ) \n    if args . remove_all_unused_imports and args . imports : \n        print ( 'Using both --remove-all and --imports is redundant' , file = standard_error ) \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    if args . exclude : \n        args . exclude = _split_comma_separated ( args . exclude ) \n    else : \n        args . exclude = set ( [ ] ) \n    filenames = list ( set ( args . files ) ) \n    failure = False \n    for name in find_files ( filenames , args . recursive , args . exclude ) : \n        try : \n            fix_file ( name , args = args , standard_out = standard_out ) \n        except IOError as exception : \n            print ( unicode ( exception ) , file = standard_error ) \n            failure = True \n    return 1 if failure else 0 "}
{"4750": "\ndef is_allowed ( self , policy_name , session_user , session_group , object_owner , object_type , operation ) : \n    policy_section = self . get_relevant_policy_section ( policy_name , session_group ) \n    if policy_section is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    object_policy = policy_section . get ( object_type ) \n    if not object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} objects.\" . format ( policy_name , self . _get_enum_string ( object_type ) ) ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    operation_object_policy = object_policy . get ( operation ) \n    if not operation_object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} operations on {2} \" \"objects.\" . format ( policy_name , self . _get_enum_string ( operation ) , self . _get_enum_string ( object_type ) ) ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    if operation_object_policy == enums . Policy . ALLOW_ALL : \n        return_identifier_3 = True \n        return return_identifier_3 \n    elif operation_object_policy == enums . Policy . ALLOW_OWNER : \n        if session_user == object_owner : \n            return_identifier_4 = True \n            return return_identifier_4 \n        else : \n            return_identifier_5 = False \n            return return_identifier_5 \n    elif operation_object_policy == enums . Policy . DISALLOW_ALL : \n        return_identifier_6 = False \n        return return_identifier_6 \n    else : \n        return_identifier_7 = False \n        return return_identifier_7 "}
{"4779": "\ndef is_bit_mask ( enumeration , potential_mask ) : \n    if not isinstance ( potential_mask , six . integer_types ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    mask_enumerations = ( CryptographicUsageMask , ProtectionStorageMask , StorageStatusMask ) \n    if enumeration not in mask_enumerations : \n        return_identifier_1 = False \n        return return_identifier_1 \n    mask = 0 \n    for value in [ e . value for e in enumeration ] : \n        if ( value & potential_mask ) == value : \n            mask |= value \n    if mask != potential_mask : \n        return_identifier_2 = False \n        return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"4803": "\ndef is_attribute_supported ( self , attribute ) : \n    if attribute not in self . _attribute_rule_sets . keys ( ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if self . _version >= rule_set . version_added : \n        return_identifier_1 = True \n        return return_identifier_1 \n    else : \n        return_identifier_2 = False \n        return return_identifier_2 "}
{"4804": "\ndef is_attribute_deprecated ( self , attribute ) : \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if rule_set . version_deprecated : \n        if self . _version >= rule_set . version_deprecated : \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            return_identifier_1 = False \n            return return_identifier_1 \n    else : \n        return_identifier_2 = False \n        return return_identifier_2 "}
{"4805": "\ndef is_attribute_applicable_to_object_type ( self , attribute , object_type ) : \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if object_type in rule_set . applies_to_object_types : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"4851": "\ndef create_symmetric_key ( self , algorithm , length ) : \n    if algorithm not in self . _symmetric_key_algorithms . keys ( ) : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm {0} is not a supported symmetric \" \"key algorithm.\" . format ( algorithm ) ) \n    cryptography_algorithm = self . _symmetric_key_algorithms . get ( algorithm ) \n    if length not in cryptography_algorithm . key_sizes : \n        raise exceptions . InvalidField ( \"The cryptographic length ({0}) is not valid for \" \"the cryptographic algorithm ({1}).\" . format ( length , algorithm . name ) ) \n    self . logger . info ( \"Generating a {0} symmetric key with length: {1}\" . format ( algorithm . name , length ) ) \n    key_bytes = os . urandom ( length // 8 ) \n    try : \n        cryptography_algorithm ( key_bytes ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"Invalid bytes for the provided cryptographic algorithm.\" ) \n    return_identifier_0 = { 'value' : key_bytes , 'format' : enums . KeyFormatType . RAW } \n    return return_identifier_0 "}
{"4855": "\ndef _encrypt_symmetric ( self , encryption_algorithm , encryption_key , plain_text , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    algorithm = self . _symmetric_key_algorithms . get ( encryption_algorithm , None ) \n    if algorithm is None : \n        raise exceptions . InvalidField ( \"Encryption algorithm '{0}' is not a supported symmetric \" \"encryption algorithm.\" . format ( encryption_algorithm ) ) \n    try : \n        algorithm = algorithm ( encryption_key ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"Invalid key bytes for the specified encryption algorithm.\" ) \n    return_iv_nonce = False \n    if encryption_algorithm == enums . CryptographicAlgorithm . RC4 : \n        mode = None \n    else : \n        if cipher_mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode is required.\" ) \n        mode = self . _modes . get ( cipher_mode , None ) \n        if mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode '{0}' is not a supported mode.\" . format ( cipher_mode ) ) \n        if hasattr ( mode , 'initialization_vector' ) or hasattr ( mode , 'nonce' ) : \n            if iv_nonce is None : \n                iv_nonce = os . urandom ( algorithm . block_size // 8 ) \n                return_iv_nonce = True \n            mode = mode ( iv_nonce ) \n        else : \n            mode = mode ( ) \n    if cipher_mode in [ enums . BlockCipherMode . CBC , enums . BlockCipherMode . ECB ] : \n        plain_text = self . _handle_symmetric_padding ( self . _symmetric_key_algorithms . get ( encryption_algorithm ) , plain_text , padding_method ) \n    cipher = ciphers . Cipher ( algorithm , mode , backend = default_backend ( ) ) \n    encryptor = cipher . encryptor ( ) \n    cipher_text = encryptor . update ( plain_text ) + encryptor . finalize ( ) \n    if return_iv_nonce : \n        return_identifier_0 = { 'cipher_text' : cipher_text , 'iv_nonce' : iv_nonce } \n        return return_identifier_0 \n    else : \n        return_identifier_1 = { 'cipher_text' : cipher_text } \n        return return_identifier_1 "}
{"4856": "\ndef _encrypt_asymmetric ( self , encryption_algorithm , encryption_key , plain_text , padding_method , hashing_algorithm = None ) : \n    if encryption_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . OAEP : \n            hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n            if hash_algorithm is None : \n                raise exceptions . InvalidField ( \"The hashing algorithm '{0}' is not supported for \" \"asymmetric encryption.\" . format ( hashing_algorithm ) ) \n            padding_method = asymmetric_padding . OAEP ( mgf = asymmetric_padding . MGF1 ( algorithm = hash_algorithm ( ) ) , algorithm = hash_algorithm ( ) , label = None ) \n        elif padding_method == enums . PaddingMethod . PKCS1v15 : \n            padding_method = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for asymmetric \" \"encryption.\" . format ( padding_method ) ) \n        backend = default_backend ( ) \n        try : \n            public_key = backend . load_der_public_key ( encryption_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( encryption_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The public key bytes could not be loaded.\" ) \n        cipher_text = public_key . encrypt ( plain_text , padding_method ) \n        return_identifier_0 = { 'cipher_text' : cipher_text } \n        return return_identifier_0 \n    else : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm '{0}' is not supported for \" \"asymmetric encryption.\" . format ( encryption_algorithm ) ) "}
{"4861": "\ndef verify_signature ( self , signing_key , message , signature , padding_method , signing_algorithm = None , hashing_algorithm = None , digital_signature_algorithm = None ) : \n    backend = default_backend ( ) \n    hash_algorithm = None \n    dsa_hash_algorithm = None \n    dsa_signing_algorithm = None \n    if hashing_algorithm : \n        hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n    if digital_signature_algorithm : \n        algorithm_pair = self . _digital_signature_algorithms . get ( digital_signature_algorithm ) \n        if algorithm_pair : \n            dsa_hash_algorithm = algorithm_pair [ 0 ] \n            dsa_signing_algorithm = algorithm_pair [ 1 ] \n    if dsa_hash_algorithm and dsa_signing_algorithm : \n        if hash_algorithm and ( hash_algorithm != dsa_hash_algorithm ) : \n            raise exceptions . InvalidField ( \"The hashing algorithm does not match the digital \" \"signature algorithm.\" ) \n        if ( signing_algorithm and ( signing_algorithm != dsa_signing_algorithm ) ) : \n            raise exceptions . InvalidField ( \"The signing algorithm does not match the digital \" \"signature algorithm.\" ) \n        signing_algorithm = dsa_signing_algorithm \n        hash_algorithm = dsa_hash_algorithm \n    if signing_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . PSS : \n            if hash_algorithm : \n                padding = asymmetric_padding . PSS ( mgf = asymmetric_padding . MGF1 ( hash_algorithm ( ) ) , salt_length = asymmetric_padding . PSS . MAX_LENGTH ) \n            else : \n                raise exceptions . InvalidField ( \"A hashing algorithm must be specified for PSS \" \"padding.\" ) \n        elif padding_method == enums . PaddingMethod . PKCS1v15 : \n            padding = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for signature \" \"verification.\" . format ( padding_method ) ) \n        try : \n            public_key = backend . load_der_public_key ( signing_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( signing_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The signing key bytes could not be loaded.\" ) \n        try : \n            public_key . verify ( signature , message , padding , hash_algorithm ( ) ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except errors . InvalidSignature : \n            return_identifier_1 = False \n            return return_identifier_1 \n        except Exception : \n            raise exceptions . CryptographicFailure ( \"The signature verification process failed.\" ) \n    else : \n        raise exceptions . InvalidField ( \"The signing algorithm '{0}' is not supported for \" \"signature verification.\" . format ( signing_algorithm ) ) "}
{"4923": "\ndef timesince ( value ) : \n    if not value : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    if not isinstance ( value , datetime . date ) : \n        return value \n    now = datetime . datetime . now ( ) \n    delta = now - value \n    if value > now : \n        return_identifier_1 = \"right now\" \n        return return_identifier_1 \n    elif delta . days > 365 : \n        return '%d years ago' % ( delta . days / 365 ) \n    elif delta . days > 30 : \n        return '%d months ago' % ( delta . days / 30 ) \n    elif delta . days > 0 : \n        return '%d days ago' % delta . days \n    elif delta . seconds > 3600 : \n        return '%d hours ago' % ( delta . seconds / 3600 ) \n    elif delta . seconds > 60 : \n        return '%d minutes ago' % ( delta . seconds / 60 ) \n    else : \n        return_identifier_2 = 'right now' \n        return return_identifier_2 "}
{"4963": "\ndef _cubic_bernstein_extrema ( p0 , p1 , p2 , p3 ) : \n    a = 3. * ( p3 - p0 + 3. * ( p1 - p2 ) ) \n    b = 6. * ( p0 + p2 - 2. * p1 ) \n    c = 3. * ( p1 - p0 ) \n    if a == 0 : \n        if b == 0 : \n            return_identifier_0 = ( ) \n            return return_identifier_0 \n        return_identifier_1 = ( - c / b , ) \n        return return_identifier_1 \n    d = b * b - 4. * a * c \n    if d < 0 : \n        return_identifier_2 = ( ) \n        return return_identifier_2 \n    k = - 2. * a \n    if d == 0 : \n        return_identifier_3 = ( b / k , ) \n        return return_identifier_3 \n    r = math . sqrt ( d ) \n    return_identifier_4 = ( ( b + r ) / k , ( b - r ) / k ) \n    return return_identifier_4 "}
{"4968": "\ndef compose_dynamic_tree ( src , target_tree_alias = None , parent_tree_item_alias = None , include_trees = None ) : \n    def result ( sitetrees = src ) : \n        if include_trees is not None : \n            sitetrees = [ tree for tree in sitetrees if tree . alias in include_trees ] \n        return_identifier_0 = { 'app' : src , 'sitetrees' : sitetrees , 'tree' : target_tree_alias , 'parent_item' : parent_tree_item_alias } \n        return return_identifier_0 \n    if isinstance ( src , six . string_types ) : \n        try : \n            module = import_app_sitetree_module ( src ) \n            return None if module is None else result ( getattr ( module , 'sitetrees' , None ) ) \n        except ImportError as e : \n            if settings . DEBUG : \n                warnings . warn ( 'Unable to register dynamic sitetree(s) for `%s` application: %s. ' % ( src , e ) ) \n            return None \n    return result ( ) "}
{"4981": "\ndef get_current_page_attr ( self , attr_name , tree_alias , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    current_item = self . get_tree_current_item ( tree_alias ) \n    if current_item is None : \n        if settings . DEBUG and RAISE_ITEMS_ERRORS_ON_DEBUG : \n            raise SiteTreeError ( 'Unable to resolve current sitetree item to get a `%s` for current page. Check whether ' 'there is an appropriate sitetree item defined for current URL.' % attr_name ) \n        return_identifier_0 = '' \n        return return_identifier_0 \n    return getattr ( current_item , attr_name , '' ) "}
{"4983": "\ndef menu ( self , tree_alias , tree_branches , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    tree_branches = self . resolve_var ( tree_branches ) \n    parent_isnull = False \n    parent_ids = [ ] \n    parent_aliases = [ ] \n    current_item = self . get_tree_current_item ( tree_alias ) \n    self . tree_climber ( tree_alias , current_item ) \n    for branch_id in tree_branches . split ( ',' ) : \n        branch_id = branch_id . strip ( ) \n        if branch_id == ALIAS_TRUNK : \n            parent_isnull = True \n        elif branch_id == ALIAS_THIS_CHILDREN and current_item is not None : \n            branch_id = current_item . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_ANCESTOR_CHILDREN and current_item is not None : \n            branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_SIBLINGS and current_item is not None and current_item . parent is not None : \n            branch_id = current_item . parent . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_PARENT_SIBLINGS and current_item is not None : \n            branch_id = self . get_ancestor_level ( current_item , depth = 2 ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id . isdigit ( ) : \n            parent_ids . append ( int ( branch_id ) ) \n        else : \n            parent_aliases . append ( branch_id ) \n    check_access = self . check_access \n    menu_items = [ ] \n    for item in sitetree_items : \n        if not item . hidden and item . inmenu and check_access ( item , context ) : \n            if item . parent is None : \n                if parent_isnull : \n                    menu_items . append ( item ) \n            else : \n                if item . parent . id in parent_ids or item . parent . alias in parent_aliases : \n                    menu_items . append ( item ) \n    menu_items = self . apply_hook ( menu_items , 'menu' ) \n    self . update_has_children ( tree_alias , menu_items , 'menu' ) \n    return menu_items "}
{"4984": "\ndef check_access ( self , item , context ) : \n    if hasattr ( self . current_request . user . is_authenticated , '__call__' ) : \n        authenticated = self . current_request . user . is_authenticated ( ) \n    else : \n        authenticated = self . current_request . user . is_authenticated \n    if item . access_loggedin and not authenticated : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if item . access_guest and authenticated : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if item . access_restricted : \n        user_perms = self . _current_user_permissions \n        if user_perms is _UNSET : \n            user_perms = set ( context [ 'user' ] . get_all_permissions ( ) ) \n            self . _current_user_permissions = user_perms \n        if item . access_perm_type == MODEL_TREE_ITEM_CLASS . PERM_TYPE_ALL : \n            if len ( item . perms ) != len ( item . perms . intersection ( user_perms ) ) : \n                return_identifier_2 = False \n                return return_identifier_2 \n        else : \n            if not len ( item . perms . intersection ( user_perms ) ) : \n                return_identifier_3 = False \n                return return_identifier_3 \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"4985": "\ndef breadcrumbs ( self , tree_alias , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    current_item = self . get_tree_current_item ( tree_alias ) \n    breadcrumbs = [ ] \n    if current_item is not None : \n        context_ = self . current_page_context \n        check_access = self . check_access \n        get_item_by_id = self . get_item_by_id \n        def climb ( base_item ) : \n            if base_item . inbreadcrumbs and not base_item . hidden and check_access ( base_item , context_ ) : \n                breadcrumbs . append ( base_item ) \n            if hasattr ( base_item , 'parent' ) and base_item . parent is not None : \n                climb ( get_item_by_id ( tree_alias , base_item . parent . id ) ) \n        climb ( current_item ) \n        breadcrumbs . reverse ( ) \n    items = self . apply_hook ( breadcrumbs , 'breadcrumbs' ) \n    self . update_has_children ( tree_alias , items , 'breadcrumbs' ) \n    return items "}
{"4986": "\ndef tree ( self , tree_alias , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    tree_items = self . filter_items ( self . get_children ( tree_alias , None ) , 'sitetree' ) \n    tree_items = self . apply_hook ( tree_items , 'sitetree' ) \n    self . update_has_children ( tree_alias , tree_items , 'sitetree' ) \n    return tree_items "}
{"5021": "\ndef create_http_headers_for_new_span ( context_stack = None , tracer = None ) : \n    if tracer : \n        zipkin_attrs = tracer . get_zipkin_attrs ( ) \n    elif context_stack : \n        zipkin_attrs = context_stack . get ( ) \n    else : \n        zipkin_attrs = get_default_tracer ( ) . get_zipkin_attrs ( ) \n    if not zipkin_attrs : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    return_identifier_1 = { 'X-B3-TraceId' : zipkin_attrs . trace_id , 'X-B3-SpanId' : generate_random_64bit_string ( ) , 'X-B3-ParentSpanId' : zipkin_attrs . span_id , 'X-B3-Flags' : '0' , 'X-B3-Sampled' : '1' if zipkin_attrs . is_sampled else '0' , } \n    return return_identifier_1 "}
{"5065": "\ndef parse ( src , encoding = None ) : \n    def safe_is_file ( filename ) : \n        try : \n            return os . path . isfile ( src ) \n        except ValueError : \n            return_identifier_0 = False \n            return return_identifier_0 \n    if hasattr ( src , 'read' ) : \n        data = src . read ( ) \n    elif safe_is_file ( src ) : \n        with open ( src , 'rb' ) as fh : \n            data = fh . read ( ) \n    else : \n        data = src \n    if hasattr ( data , 'decode' ) : \n        exception = None \n        encodings = [ encoding , 'utf-8' , 'cp852' , 'iso8859-15' , 'latin1' ] \n        for encoding in encodings : \n            if not encoding : \n                continue \n            try : \n                data = data . decode ( encoding ) \n                break \n            except UnicodeDecodeError as e : \n                exception = e \n            except UnicodeEncodeError : \n                break \n        else : \n            raise exception \n    transactions = mt940 . models . Transactions ( ) \n    transactions . parse ( data ) \n    return transactions "}
{"5091": "\ndef has_changed ( self ) : \n    if self . formsets : \n        for formset in self . formsets . values ( ) : \n            for form in formset . forms : \n                if form . has_changed ( ) : \n                    return_identifier_0 = True \n                    return return_identifier_0 \n    return bool ( self . changed_data ) "}
{"5132": "\ndef find_transaction_objects ( adapter , ** kwargs ) : \n    ft_response = FindTransactionsCommand ( adapter ) ( ** kwargs ) \n    hashes = ft_response [ 'hashes' ] \n    if hashes : \n        gt_response = GetTrytesCommand ( adapter ) ( hashes = hashes ) \n        return list ( map ( Transaction . from_tryte_string , gt_response . get ( 'trytes' ) or [ ] , ) ) \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"5134": "\ndef get_bundles_from_transaction_hashes ( adapter , transaction_hashes , inclusion_states , ) : \n    transaction_hashes = list ( transaction_hashes ) \n    if not transaction_hashes : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    my_bundles = [ ] \n    tail_transaction_hashes = set ( ) \n    non_tail_bundle_hashes = set ( ) \n    gt_response = GetTrytesCommand ( adapter ) ( hashes = transaction_hashes ) \n    all_transactions = list ( map ( Transaction . from_tryte_string , gt_response [ 'trytes' ] , ) ) \n    for txn in all_transactions : \n        if txn . is_tail : \n            tail_transaction_hashes . add ( txn . hash ) \n        else : \n            non_tail_bundle_hashes . add ( txn . bundle_hash ) \n    if non_tail_bundle_hashes : \n        for txn in find_transaction_objects ( adapter = adapter , bundles = list ( non_tail_bundle_hashes ) , ) : \n            if txn . is_tail : \n                if txn . hash not in tail_transaction_hashes : \n                    all_transactions . append ( txn ) \n                    tail_transaction_hashes . add ( txn . hash ) \n    tail_transactions = [ txn for txn in all_transactions if txn . hash in tail_transaction_hashes ] \n    if inclusion_states : \n        gli_response = GetLatestInclusionCommand ( adapter ) ( hashes = list ( tail_transaction_hashes ) , ) \n        for txn in tail_transactions : \n            txn . is_confirmed = gli_response [ 'states' ] . get ( txn . hash ) \n    for txn in tail_transactions : \n        gb_response = GetBundlesCommand ( adapter ) ( transaction = txn . hash ) \n        txn_bundles = gb_response [ 'bundles' ] \n        if inclusion_states : \n            for bundle in txn_bundles : \n                bundle . is_confirmed = txn . is_confirmed \n        my_bundles . extend ( txn_bundles ) \n    return list ( sorted ( my_bundles , key = lambda bundle_ : bundle_ . tail_transaction . timestamp , ) ) "}
{"5139": "\ndef _find_addresses ( self , seed , index , count , security_level , checksum ) : \n    generator = AddressGenerator ( seed , security_level , checksum ) \n    if count is None : \n        for addy in generator . create_iterator ( start = index ) : \n            response = FindTransactionsCommand ( self . adapter ) ( addresses = [ addy . address ] , ) \n            if not response . get ( 'hashes' ) : \n                return_identifier_0 = [ addy ] \n                return return_identifier_0 \n    return generator . get_addresses ( start = index , count = count ) "}
{"5142": "\ndef as_json_compatible ( self ) : \n    return_identifier_0 = { 'hash_' : self . hash , 'signature_message_fragment' : self . signature_message_fragment , 'address' : self . address , 'value' : self . value , 'legacy_tag' : self . legacy_tag , 'timestamp' : self . timestamp , 'current_index' : self . current_index , 'last_index' : self . last_index , 'bundle_hash' : self . bundle_hash , 'trunk_transaction_hash' : self . trunk_transaction_hash , 'branch_transaction_hash' : self . branch_transaction_hash , 'tag' : self . tag , 'attachment_timestamp' : self . attachment_timestamp , 'attachment_timestamp_lower_bound' : self . attachment_timestamp_lower_bound , 'attachment_timestamp_upper_bound' : self . attachment_timestamp_upper_bound , 'nonce' : self . nonce , } \n    return return_identifier_0 "}
{"5155": "\ndef _get_bundle_signature_errors ( self , groups ) : \n    current_pos = None \n    current_errors = [ ] \n    for current_pos , group in enumerate ( groups ) : \n        error = self . _get_group_signature_error ( group , SUPPORTED_SPONGE ) \n        if error : \n            current_errors . append ( error ) \n            break \n    if current_errors and LEGACY_SPONGE : \n        for group in groups : \n            if self . _get_group_signature_error ( group , LEGACY_SPONGE ) : \n                break \n        else : \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n    current_errors . extend ( filter ( None , ( self . _get_group_signature_error ( group , SUPPORTED_SPONGE ) for group in groups [ current_pos + 1 : ] ) ) ) \n    return current_errors "}
{"5157": "\ndef _traverse_bundle ( self , txn_hash , target_bundle_hash = None ) : \n    trytes = ( GetTrytesCommand ( self . adapter ) ( hashes = [ txn_hash ] ) [ 'trytes' ] ) \n    if not trytes : \n        raise with_context ( exc = BadApiResponse ( 'Bundle transactions not visible ' '(``exc.context`` has more info).' , ) , context = { 'transaction_hash' : txn_hash , 'target_bundle_hash' : target_bundle_hash , } , ) \n    transaction = Transaction . from_tryte_string ( trytes [ 0 ] ) \n    if ( not target_bundle_hash ) and transaction . current_index : \n        raise with_context ( exc = BadApiResponse ( '``_traverse_bundle`` started with a non-tail transaction ' '(``exc.context`` has more info).' , ) , context = { 'transaction_object' : transaction , 'target_bundle_hash' : target_bundle_hash , } , ) \n    if target_bundle_hash : \n        if target_bundle_hash != transaction . bundle_hash : \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n    else : \n        target_bundle_hash = transaction . bundle_hash \n    if transaction . current_index == transaction . last_index == 0 : \n        return_identifier_1 = [ transaction ] \n        return return_identifier_1 \n    return [ transaction ] + self . _traverse_bundle ( txn_hash = transaction . trunk_transaction_hash , target_bundle_hash = target_bundle_hash ) "}
{"5175": "\ndef decompress_G1 ( z : G1Compressed ) -> G1Uncompressed : \n    b_flag = ( z % POW_2_383 ) // POW_2_382 \n    if b_flag == 1 : \n        return Z1 \n    x = z % POW_2_381 \n    y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) \n    if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : \n        raise ValueError ( \"The given point is not on G1: y**2 = x**3 + b\" ) \n    a_flag = ( z % POW_2_382 ) // POW_2_381 \n    if ( y * 2 ) // q != a_flag : \n        y = q - y \n    return_identifier_0 = ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) ) \n    return return_identifier_0 "}
{"5176": "\ndef prime_field_inv ( a : int , n : int ) -> int : \n    if a == 0 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    lm , hm = 1 , 0 \n    low , high = a % n , n \n    while low > 1 : \n        r = high // low \n        nm , new = hm - lm * r , high - low * r \n        lm , low , hm , high = nm , new , lm , low \n    return lm % n "}
{"5191": "\ndef max_width ( self ) : \n    try : \n        maximum = max ( [ row . width for row in self . __list if row . width is not None ] ) \n        return maximum \n    except : \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"5233": "\ndef get_field ( self , field_name , args , kwargs ) : \n    try : \n        s = super ( CustomFormatter , self ) \n        return s . get_field ( field_name , args , kwargs ) \n    except KeyError : \n        return_identifier_0 = ( \"_\" , field_name ) \n        return return_identifier_0 \n    except IndexError : \n        return_identifier_1 = ( \"_\" , field_name ) \n        return return_identifier_1 "}
{"5277": "\ndef execute ( self , obj ) : \n    try : \n        if self . config . stdin : \n            self . spawn ( self . config . command , stdin_content = obj , stdin = True , timeout = 1 ) \n        else : \n            if \"@@\" not in self . config . command : \n                raise PJFMissingArgument ( \"Missing @@ filename indicator while using non-stdin fuzzing method\" ) \n            for x in self . config . command : \n                if \"@@\" in x : \n                    self . config . command [ self . config . command . index ( x ) ] = x . replace ( \"@@\" , obj ) \n            self . spawn ( self . config . command , timeout = 2 ) \n        self . logger . debug ( \"[{0}] - PJFExternalFuzzer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) \n        return self . _out \n    except KeyboardInterrupt : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5346": "\ndef arguments ( self ) -> List [ Argument ] : \n    shadow = self . _shadow \n    split_spans = self . _args_matcher ( shadow ) . spans ( 'arg' ) \n    if not split_spans : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    arguments = [ ] \n    arguments_append = arguments . append \n    type_to_spans = self . _type_to_spans \n    ss , se = span = self . _span \n    type_ = id ( span ) \n    lststr = self . _lststr \n    string = lststr [ 0 ] \n    arg_spans = type_to_spans . setdefault ( type_ , [ ] ) \n    span_tuple_to_span_get = { ( s [ 0 ] , s [ 1 ] ) : s for s in arg_spans } . get \n    for arg_self_start , arg_self_end in split_spans : \n        s , e = arg_span = [ ss + arg_self_start , ss + arg_self_end ] \n        old_span = span_tuple_to_span_get ( ( s , e ) ) \n        if old_span is None : \n            insort ( arg_spans , arg_span ) \n        else : \n            arg_span = old_span \n        arg = Argument ( lststr , type_to_spans , arg_span , type_ ) \n        arg . _shadow_cache = ( string [ s : e ] , shadow [ arg_self_start : arg_self_end ] ) \n        arguments_append ( arg ) \n    return arguments "}
{"5349": "\ndef _pattern ( trie : dict ) -> str : \n    if '' in trie : \n        if len ( trie ) == 1 : \n            return_identifier_0 = '' \n            return return_identifier_0 \n        optional = True \n        del trie [ '' ] \n    else : \n        optional = False \n    subpattern_to_chars = _defaultdict ( list ) \n    for char , sub_trie in trie . items ( ) : \n        subpattern = _pattern ( sub_trie ) \n        subpattern_to_chars [ subpattern ] . append ( char ) \n    alts = [ ] \n    for subpattern , chars in subpattern_to_chars . items ( ) : \n        if len ( chars ) == 1 : \n            alts . append ( chars [ 0 ] + subpattern ) \n        else : \n            chars . sort ( reverse = True ) \n            alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) \n    if len ( alts ) == 1 : \n        result = alts [ 0 ] \n        if optional : \n            if len ( result ) == 1 : \n                result += '?+' \n            else : \n                result = '(?:' + result + ')?+' \n    else : \n        alts . sort ( reverse = True ) \n        result = '(?>' + '|' . join ( alts ) + ')' \n        if optional : \n            result += '?+' \n    return result "}
{"5381": "\ndef has_arg ( self , name : str , value : str = None ) -> bool : \n    for arg in reversed ( self . arguments ) : \n        if arg . name . strip ( WS ) == name . strip ( WS ) : \n            if value : \n                if arg . positional : \n                    if arg . value == value : \n                        return_identifier_0 = True \n                        return return_identifier_0 \n                    return_identifier_1 = False \n                    return return_identifier_1 \n                if arg . value . strip ( WS ) == value . strip ( WS ) : \n                    return_identifier_2 = True \n                    return return_identifier_2 \n                return_identifier_3 = False \n                return return_identifier_3 \n            return_identifier_4 = True \n            return return_identifier_4 \n    return_identifier_5 = False \n    return return_identifier_5 "}
{"5459": "\nasync def connect ( self ) : \n    _LOGGER . debug ( \"Connecting...\" ) \n    try : \n        self . _reader , self . _writer = await asyncio . open_connection ( self . _host , self . _port , loop = self . _loop ) \n        _LOGGER . debug ( \"sucess connecting...\" ) \n    except Exception as e : \n        _LOGGER . warning ( \"Exception during connecting: %s.\" , e ) \n        self . _writer = None \n        self . _reader = None \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"5468": "\ndef guess_type ( self , path , allow_directory = True ) : \n    if path . endswith ( '.ipynb' ) : \n        return_identifier_0 = 'notebook' \n        return return_identifier_0 \n    elif allow_directory and self . dir_exists ( path ) : \n        return_identifier_1 = 'directory' \n        return return_identifier_1 \n    else : \n        return_identifier_2 = 'file' \n        return return_identifier_2 "}
{"5490": "\ndef _file_default_fields ( ) : \n    return_identifier_0 = [ files . c . name , files . c . created_at , files . c . parent_name , ] \n    return return_identifier_0 "}
{"5494": "\ndef file_exists ( db , user_id , path ) : \n    try : \n        get_file ( db , user_id , path , include_content = False , decrypt_func = unused_decrypt_func , ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except NoSuchFile : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"5529": "\ndef _decode_unknown_from_base64 ( path , bcontent ) : \n    content = b64decode ( bcontent ) \n    try : \n        return_identifier_0 = ( content . decode ( 'utf-8' ) , 'text' ) \n        return return_identifier_0 \n    except UnicodeError : \n        pass \n    return bcontent . decode ( 'ascii' ) , 'base64' "}
{"5560": "\ndef is_valid_uuid ( id ) : \n    if not isinstance ( id , basestring ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        val = UUID ( id , version = 4 ) \n    except ValueError : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"5570": "\ndef get_settings ( self , integration_id ) : \n    try : \n        integration = self . get ( integration_id = integration_id ) \n        return json . loads ( integration . settings ) \n    except ( self . model . DoesNotExist , ValueError ) : \n        return_identifier_0 = { } \n        return return_identifier_0 "}
{"5574": "\ndef zone ( zone_id , ** kwargs ) : \n    try : \n        zone = ThemeManager . Zones . get ( zone_id ) \n    except ZoneNotFound : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    try : \n        return zone . widget . render ( add_context = kwargs ) \n    except ( WidgetNotFound , AttributeError ) : \n        pass \n    return_identifier_1 = '' \n    return return_identifier_1 "}
{"5587": "\ndef get_projected_rate ( self , amt , time_at_consumption ) : \n    if self . _last_time is None : \n        return_identifier_0 = 0.0 \n        return return_identifier_0 \n    return self . _calculate_exponential_moving_average_rate ( amt , time_at_consumption ) "}
{"5595": "\ndef is_special_file ( cls , filename ) : \n    if not os . path . exists ( filename ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    mode = os . stat ( filename ) . st_mode \n    if stat . S_ISCHR ( mode ) : \n        return_identifier_1 = True \n        return return_identifier_1 \n    if stat . S_ISBLK ( mode ) : \n        return_identifier_2 = True \n        return return_identifier_2 \n    if stat . S_ISFIFO ( mode ) : \n        return_identifier_3 = True \n        return return_identifier_3 \n    if stat . S_ISSOCK ( mode ) : \n        return_identifier_4 = True \n        return return_identifier_4 \n    return_identifier_5 = False \n    return return_identifier_5 "}
{"5604": "\ndef request_writes ( self , offset , data ) : \n    if offset < self . _next_offset : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    writes = [ ] \n    if offset in self . _pending_offsets : \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    heapq . heappush ( self . _writes , ( offset , data ) ) \n    self . _pending_offsets . add ( offset ) \n    while self . _writes and self . _writes [ 0 ] [ 0 ] == self . _next_offset : \n        next_write = heapq . heappop ( self . _writes ) \n        writes . append ( { 'offset' : next_write [ 0 ] , 'data' : next_write [ 1 ] } ) \n        self . _pending_offsets . remove ( next_write [ 0 ] ) \n        self . _next_offset += len ( next_write [ 1 ] ) \n    return writes "}
{"5605": "\ndef seekable ( fileobj ) : \n    if hasattr ( fileobj , 'seekable' ) : \n        return fileobj . seekable ( ) \n    elif hasattr ( fileobj , 'seek' ) and hasattr ( fileobj , 'tell' ) : \n        try : \n            fileobj . seek ( 0 , 1 ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        except ( OSError , IOError ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"5641": "\ndef POST ( self ) : \n    json_data = web . data ( ) \n    print ( \"\\nWEBHOOK POST RECEIVED:\" ) \n    print ( json_data , \"\\n\" ) \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    print ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) \n    print ( \"FROM '{}'\" . format ( person . displayName ) ) \n    print ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) \n    me = api . people . me ( ) \n    if message . personId == me . id : \n        return_identifier_0 = 'OK' \n        return return_identifier_0 \n    else : \n        if \"/CAT\" in message . text : \n            print ( \"FOUND '/CAT'\" ) \n            cat_fact = get_catfact ( ) \n            print ( \"SENDING CAT FACT '{}'\" . format ( cat_fact ) ) \n            api . messages . create ( room . id , text = cat_fact ) \n    return_identifier_1 = 'OK' \n    return return_identifier_1 "}
{"5690": "\ndef post_events_service ( request ) : \n    json_data = request . json \n    log . info ( \"\\n\" ) \n    log . info ( \"WEBHOOK POST RECEIVED:\" ) \n    log . info ( json_data ) \n    log . info ( \"\\n\" ) \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    log . info ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) \n    log . info ( \"FROM '{}'\" . format ( person . displayName ) ) \n    log . info ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) \n    me = api . people . me ( ) \n    if message . personId == me . id : \n        return_identifier_0 = { 'Message' : 'OK' } \n        return return_identifier_0 \n    else : \n        if \"/CAT\" in message . text : \n            log . info ( \"FOUND '/CAT'\" ) \n            catfact = get_catfact ( ) \n            log . info ( \"SENDING CAT FACT'{}'\" . format ( catfact ) ) \n            api . messages . create ( room . id , text = catfact ) \n        return_identifier_1 = { 'Message' : 'OK' } \n        return return_identifier_1 "}
{"5720": "\ndef _translate_glob_part ( pat ) : \n    if pat == '**' : \n        return_identifier_0 = '.*' \n        return return_identifier_0 \n    i , n = 0 , len ( pat ) \n    res = [ ] \n    while i < n : \n        c = pat [ i ] \n        i = i + 1 \n        if c == '*' : \n            res . append ( '[^%s]*' % SEPARATORS ) \n        elif c == '?' : \n            res . append ( '[^%s]?' % SEPARATORS ) \n        elif c == '[' : \n            j = i \n            if j < n and pat [ j ] == '!' : \n                j = j + 1 \n            if j < n and pat [ j ] == ']' : \n                j = j + 1 \n            while j < n and pat [ j ] != ']' : \n                j = j + 1 \n            if j >= n : \n                res . append ( '\\\\[' ) \n            else : \n                stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) \n                i = j + 1 \n                if stuff [ 0 ] == '!' : \n                    stuff = '^' + stuff [ 1 : ] \n                elif stuff [ 0 ] == '^' : \n                    stuff = '\\\\' + stuff \n                res . append ( '[%s]' % stuff ) \n        else : \n            res . append ( re . escape ( c ) ) \n    return '' . join ( res ) "}
{"5734": "\ndef _build_extra_predicate ( self , extra_predicate ) : \n    if extra_predicate is None : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    if not isinstance ( extra_predicate [ 1 ] , ( list , dict , tuple ) ) : \n        extra_predicate = [ extra_predicate [ 0 ] , ( extra_predicate [ 1 ] , ) ] \n    extra_predicate = database . escape_query ( * extra_predicate ) \n    return 'AND (' + extra_predicate + ')' "}
{"5796": "\ndef subscribeToDeviceEvents ( self , typeId = \"+\" , deviceId = \"+\" , eventId = \"+\" , msgFormat = \"+\" , qos = 0 ) : \n    if self . _config . isQuickstart ( ) and deviceId == \"+\" : \n        self . logger . warning ( \"QuickStart applications do not support wildcard subscription to events from all devices\" ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    topic = \"iot-2/type/%s/id/%s/evt/%s/fmt/%s\" % ( typeId , deviceId , eventId , msgFormat ) \n    return self . _subscribe ( topic , qos ) "}
{"5797": "\ndef subscribeToDeviceStatus ( self , typeId = \"+\" , deviceId = \"+\" ) : \n    if self . _config . isQuickstart ( ) and deviceId == \"+\" : \n        self . logger . warning ( \"QuickStart applications do not support wildcard subscription to device status\" ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    topic = \"iot-2/type/%s/id/%s/mon\" % ( typeId , deviceId ) \n    return self . _subscribe ( topic , 0 ) "}
{"5798": "\ndef subscribeToDeviceCommands ( self , typeId = \"+\" , deviceId = \"+\" , commandId = \"+\" , msgFormat = \"+\" ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support commands\" ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n    return self . _subscribe ( topic , 0 ) "}
{"5799": "\ndef publishCommand ( self , typeId , deviceId , commandId , msgFormat , data = None , qos = 0 , on_publish = None ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support sending commands\" ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not self . connectEvent . wait ( timeout = 10 ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    else : \n        topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n        if self . getMessageCodec ( msgFormat ) is None : \n            raise MissingMessageEncoderException ( msgFormat ) \n        payload = self . getMessageCodec ( msgFormat ) . encode ( data , datetime . now ( ) ) \n        result = self . client . publish ( topic , payload = payload , qos = qos , retain = False ) \n        if result [ 0 ] == paho . MQTT_ERR_SUCCESS : \n            with self . _messagesLock : \n                if result [ 1 ] in self . _onPublishCallbacks : \n                    del self . _onPublishCallbacks [ result [ 1 ] ] \n                    if on_publish is not None : \n                        on_publish ( ) \n                else : \n                    self . _onPublishCallbacks [ result [ 1 ] ] = on_publish \n            return_identifier_2 = True \n            return return_identifier_2 \n        else : \n            return_identifier_3 = False \n            return return_identifier_3 "}
{"5814": "\ndef _create_idx_from_stream ( self , stream ) : \n    stream_iter = iter ( stream ) \n    dimension = self . properties . dimension \n    darray = ctypes . c_double * dimension \n    mins = darray ( ) \n    maxs = darray ( ) \n    no_data = ctypes . cast ( ctypes . pointer ( ctypes . c_ubyte ( 0 ) ) , ctypes . POINTER ( ctypes . c_ubyte ) ) \n    def py_next_item ( p_id , p_mins , p_maxs , p_dimension , p_data , p_length ) : \n        try : \n            p_id [ 0 ] , coordinates , obj = next ( stream_iter ) \n        except StopIteration : \n            return - 1 \n        except Exception as exc : \n            self . _exception = exc \n            return - 1 \n        if self . interleaved : \n            coordinates = Index . deinterleave ( coordinates ) \n        for i in range ( dimension ) : \n            mins [ i ] = coordinates [ i * 2 ] \n            maxs [ i ] = coordinates [ ( i * 2 ) + 1 ] \n        p_mins [ 0 ] = ctypes . cast ( mins , ctypes . POINTER ( ctypes . c_double ) ) \n        p_maxs [ 0 ] = ctypes . cast ( maxs , ctypes . POINTER ( ctypes . c_double ) ) \n        p_dimension [ 0 ] = dimension \n        if obj is None : \n            p_data [ 0 ] = no_data \n            p_length [ 0 ] = 0 \n        else : \n            p_length [ 0 ] , data , _ = self . _serialize ( obj ) \n            p_data [ 0 ] = ctypes . cast ( data , ctypes . POINTER ( ctypes . c_ubyte ) ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    stream = core . NEXTFUNC ( py_next_item ) \n    return IndexStreamHandle ( self . properties . handle , stream ) "}
{"5815": "\ndef loadByteArray ( self , page , returnError ) : \n    returnError . contents . value = self . IllegalStateError \n    raise NotImplementedError ( \"You must override this method.\" ) \n    return_identifier_0 = '' \n    return return_identifier_0 "}
{"5817": "\ndef check_return ( result , func , cargs ) : \n    if result != 0 : \n        s = rt . Error_GetLastErrorMsg ( ) . decode ( ) \n        msg = 'LASError in \"%s\": %s' % ( func . __name__ , s ) \n        rt . Error_Reset ( ) \n        raise RTreeError ( msg ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"5843": "\ndef preprocess_GIF ( self , image , ** kwargs ) : \n    if 'transparency' in image . info : \n        save_kwargs = { 'transparency' : image . info [ 'transparency' ] } \n    else : \n        save_kwargs = { } \n    return_identifier_0 = ( image , save_kwargs ) \n    return return_identifier_0 "}
{"5844": "\ndef preprocess_JPEG ( self , image , ** kwargs ) : \n    save_kwargs = { 'progressive' : VERSATILEIMAGEFIELD_PROGRESSIVE_JPEG , 'quality' : QUAL } \n    if image . mode != 'RGB' : \n        image = image . convert ( 'RGB' ) \n    return_identifier_0 = ( image , save_kwargs ) \n    return return_identifier_0 "}
{"5845": "\ndef retrieve_image ( self , path_to_image ) : \n    image = self . storage . open ( path_to_image , 'rb' ) \n    file_ext = path_to_image . rsplit ( '.' ) [ - 1 ] \n    image_format , mime_type = get_image_metadata_from_file_ext ( file_ext ) \n    return_identifier_0 = ( Image . open ( image ) , file_ext , image_format , mime_type ) \n    return return_identifier_0 "}
{"5866": "\ndef parse_agi_result ( line ) : \n    if line == 'HANGUP' : \n        return_identifier_0 = { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n        return return_identifier_0 \n    kwargs = dict ( code = 0 , response = \"\" , line = line ) \n    m = re_code . search ( line ) \n    try : \n        kwargs . update ( m . groupdict ( ) ) \n    except AttributeError : \n        pass \n    return agi_code_check ( ** kwargs ) "}
{"5867": "\ndef agi_code_check ( code = None , response = None , line = None ) : \n    code = int ( code ) \n    response = response or \"\" \n    result = { 'status_code' : code , 'result' : ( '' , '' ) , 'msg' : '' } \n    if code == 100 : \n        result [ 'msg' ] = line \n    elif code == 200 : \n        for key , value , data in re_kv . findall ( response ) : \n            result [ key ] = ( value , data ) \n            if data == 'hangup' : \n                return_identifier_0 = { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n                return return_identifier_0 \n            elif key == 'result' and value == '-1' : \n                return_identifier_1 = { 'error' : 'AGIAppError' , 'msg' : 'Error executing application, or hangup' } \n                return return_identifier_1 \n    elif code == 510 : \n        result [ 'error' ] = 'AGIInvalidCommand' \n    elif code == 520 : \n        result [ 'error' ] = 'AGIUsageError' \n        result [ 'msg' ] = line \n    else : \n        result [ 'error' ] = 'AGIUnknownError' \n        result [ 'msg' ] = line \n    return result "}
{"5892": "\ndef map_param_type ( param_type ) : \n    main_type , sub_type = TYPE_INFO_RE . match ( param_type ) . groups ( ) \n    if main_type in ( 'list' , 'array' ) : \n        if sub_type is not None : \n            sub_type = sub_type . strip ( ) \n        if not sub_type : \n            sub_type = 'str' \n        sub_match = TYPE_INFO_RE . match ( sub_type ) \n        if sub_match : \n            sub_type = sub_match . group ( 1 ) . lower ( ) \n        return_identifier_0 = [ PARAM_TYPE_MAP . setdefault ( sub_type , string_types ) ] \n        return return_identifier_0 \n    return PARAM_TYPE_MAP . setdefault ( main_type , string_types ) "}
{"5898": "\ndef equals_order_sensitive ( self , other ) : \n    if not isinstance ( other , Mapping ) or len ( self ) != len ( other ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return all ( i == j for ( i , j ) in izip ( iteritems ( self ) , iteritems ( other ) ) ) "}
{"5922": "\ndef _get_names_part ( self , part ) : \n    try : \n        the_list = getattr ( self . vcard . n . value , part ) \n    except AttributeError : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    else : \n        if not '' . join ( the_list ) : \n            return_identifier_1 = [ ] \n            return return_identifier_1 \n    return the_list if isinstance ( the_list , list ) else [ the_list ] "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = 0 \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            elif type . lower ( ) == \"pref\" : \n                pref += 1 \n            elif re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                pref += int ( type . split ( \"=\" ) [ 1 ] ) \n            else : \n                if type . lower ( ) . startswith ( \"x-\" ) : \n                    custom_types . append ( type [ 2 : ] ) \n                    standard_types . append ( type ) \n                else : \n                    custom_types . append ( type ) \n                    standard_types . append ( \"X-{}\" . format ( type ) ) \n    return_identifier_0 = ( standard_types , custom_types , pref ) \n    return return_identifier_0 "}
{"5933": "\ndef get_short_uid ( self , uid ) : \n    if uid : \n        short_uids = self . get_short_uid_dict ( ) \n        for length_of_uid in range ( len ( uid ) , 0 , - 1 ) : \n            if short_uids . get ( uid [ : length_of_uid ] ) is not None : \n                return uid [ : length_of_uid ] \n    return_identifier_0 = \"\" \n    return return_identifier_0 "}
{"5937": "\ndef avail_archs ( self ) : \n    return_identifier_0 = { ARM32 : ( KS_ARCH_ARM , KS_MODE_ARM ) , ARM64 : ( KS_ARCH_ARM64 , KS_MODE_LITTLE_ENDIAN ) , ARM_TB : ( KS_ARCH_ARM , KS_MODE_THUMB ) , HEXAGON : ( KS_ARCH_HEXAGON , KS_MODE_BIG_ENDIAN ) , MIPS32 : ( KS_ARCH_MIPS , KS_MODE_MIPS32 ) , MIPS64 : ( KS_ARCH_MIPS , KS_MODE_MIPS64 ) , PPC32 : ( KS_ARCH_PPC , KS_MODE_PPC32 ) , PPC64 : ( KS_ARCH_PPC , KS_MODE_PPC64 ) , SPARC32 : ( KS_ARCH_SPARC , KS_MODE_SPARC32 ) , SPARC64 : ( KS_ARCH_SPARC , KS_MODE_SPARC64 ) , SYSTEMZ : ( KS_ARCH_SYSTEMZ , KS_MODE_BIG_ENDIAN ) , X86_16 : ( KS_ARCH_X86 , KS_MODE_16 ) , X86_32 : ( KS_ARCH_X86 , KS_MODE_32 ) , X86_64 : ( KS_ARCH_X86 , KS_MODE_64 ) , } \n    return return_identifier_0 "}
{"5938": "\ndef avail_archs ( self ) : \n    return_identifier_0 = { ARM32 : ( CS_ARCH_ARM , CS_MODE_ARM ) , ARM64 : ( CS_ARCH_ARM64 , CS_MODE_LITTLE_ENDIAN ) , ARM_TB : ( CS_ARCH_ARM , CS_MODE_THUMB ) , MIPS32 : ( CS_ARCH_MIPS , CS_MODE_MIPS32 ) , MIPS64 : ( CS_ARCH_MIPS , CS_MODE_MIPS64 ) , SPARC32 : ( CS_ARCH_SPARC , CS_MODE_BIG_ENDIAN ) , SPARC64 : ( CS_ARCH_SPARC , CS_MODE_V9 ) , SYSTEMZ : ( CS_ARCH_SYSZ , CS_MODE_BIG_ENDIAN ) , X86_16 : ( CS_ARCH_X86 , CS_MODE_16 ) , X86_32 : ( CS_ARCH_X86 , CS_MODE_32 ) , X86_64 : ( CS_ARCH_X86 , CS_MODE_64 ) , } \n    return return_identifier_0 "}
{"5947": "\ndef confirm ( action , default = None , skip = False ) : \n    MAX_ITERATIONS = 3 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , True : ( 'Y' , 'n' ) , False : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = 1 \n                while not choice and cnt < MAX_ITERATIONS : \n                    choice = safe_input ( prompt ) \n                    cnt += 1 \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if default is not None : \n        return default \n    return None "}
{"5990": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        curframe = self . proc . curframe \n        if curframe : \n            line_no = inspect . getlineno ( curframe ) \n            offset = curframe . f_lasti \n            self . msg ( \"PC offset is %d.\" % offset ) \n            offset = max ( offset , 0 ) \n            code = curframe . f_code \n            co_code = code . co_code \n            disassemble_bytes ( self . msg , self . msg_nocr , co_code , offset , line_no , line_no - 1 , line_no + 1 , constants = code . co_consts , cells = code . co_cellvars , varnames = code . co_varnames , freevars = code . co_freevars , linestarts = dict ( findlinestarts ( code ) ) , end_offset = offset + 10 ) \n            pass \n        pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"5994": "\ndef run_hooks ( obj , hooks , * args ) : \n    for hook in hooks : \n        if hook ( obj , * args ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        pass \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"6000": "\ndef next_token ( str , start_pos ) : \n    look_at = str [ start_pos : ] \n    match = re . search ( '\\S' , look_at ) \n    if match : \n        pos = match . start ( ) \n    else : \n        pos = 0 \n        pass \n    next_nonblank_pos = start_pos + pos \n    next_match = re . search ( '\\s' , str [ next_nonblank_pos : ] ) \n    if next_match : \n        next_blank_pos = next_nonblank_pos + next_match . start ( ) \n    else : \n        next_blank_pos = len ( str ) \n        pass \n    return_identifier_0 = [ next_blank_pos , str [ next_nonblank_pos : next_blank_pos + 1 ] . rstrip ( ) ] \n    return return_identifier_0 "}
{"6015": "\ndef run ( self , args ) : \n    if not self . proc . curframe : \n        self . errmsg ( \"No line number information available.\" ) \n        return \n    if len ( args ) == 3 : \n        answer = self . lineinfo ( args [ 2 ] ) \n        if answer [ 0 ] : \n            item , filename , lineno = answer \n            if not os . path . isfile ( filename ) : \n                filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n            self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) \n        return \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    if not os . path . isfile ( filename ) : \n        filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n        pass \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) \n    msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) \n    if self . proc . event : \n        msg2 += ', %s event' % self . proc . event \n        pass \n    self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"6019": "\ndef get_onoff ( errmsg , arg , default = None , print_error = True ) : \n    if not arg : \n        if default is None : \n            if print_error : \n                errmsg ( \"Expecting 'on', 1, 'off', or 0. Got nothing.\" ) \n                pass \n            raise ValueError \n        return default \n    if arg == '1' or arg == 'on' : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if arg == '0' or arg == 'off' : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if print_error : \n        errmsg ( \"Expecting 'on', 1, 'off', or 0. Got: %s.\" % str ( arg ) ) \n    raise ValueError "}
{"6024": "\ndef run_show_val ( obj , name ) : \n    val = obj . debugger . settings [ obj . name ] \n    obj . msg ( \"%s is %s.\" % ( obj . name , obj . cmd . proc . _saferepr ( val ) , ) ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"6028": "\ndef set_default_bg ( ) : \n    term = environ . get ( 'TERM' , None ) \n    if term : \n        if ( term . startswith ( 'xterm' , ) or term . startswith ( 'eterm' ) or term == 'dtterm' ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"6029": "\ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        print ( \"midpoint\" , midpoint , 'vs' , ( 16 * 5 + 16 * g + 16 * b ) ) \n        midpoint = 383 if term and term == 'xterm-256color' else 117963 \n    if ( ( 16 * 5 + 16 * g + 16 * b ) < midpoint ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"6030": "\ndef signature ( frame ) : \n    if not frame : \n        return None \n    code = frame . f_code \n    return_identifier_0 = ( code . co_name , code . co_filename , code . co_firstlineno ) \n    return return_identifier_0 "}
{"6037": "\ndef run ( self , args ) : \n    if len ( args ) == 1 : \n        position_str = '0' \n    elif len ( args ) == 2 : \n        name_or_id = args [ 1 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id , False ) \n        if frame is None : \n            position_str = name_or_id \n        else : \n            position_str = '0' \n            self . find_and_set_debugged_frame ( frame , thread_id ) \n            pass \n    elif len ( args ) == 3 : \n        name_or_id = args [ 1 ] \n        position_str = args [ 2 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id ) \n        if frame is None : \n            return \n        self . find_and_set_debugged_frame ( frame , thread_id ) \n        pass \n    self . one_arg_run ( position_str ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"6038": "\ndef pprint_simple_array ( val , displaywidth , msg_nocr , msg , lineprefix = '' ) : \n    if type ( val ) != list : \n        return_identifier_0 = False \n        return return_identifier_0 \n    numeric = True \n    for i in range ( len ( val ) ) : \n        if not ( type ( val [ i ] ) in [ bool , float , int ] ) : \n            numeric = False \n            if not ( type ( val [ i ] ) in [ bool , float , int , bytes ] ) : \n                return_identifier_1 = False \n                return return_identifier_1 \n            pass \n        pass \n    mess = columnize ( [ repr ( v ) for v in val ] , opts = { \"arrange_array\" : True , \"lineprefix\" : lineprefix , \"displaywidth\" : int ( displaywidth ) - 3 , 'ljust' : not numeric } ) \n    msg_nocr ( mess ) \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"6041": "\ndef canonic_signame ( name_num ) : \n    signum = lookup_signum ( name_num ) \n    if signum is None : \n        try : \n            num = int ( name_num ) \n            signame = lookup_signame ( num ) \n            if signame is None : \n                return None \n        except : \n            return_identifier_0 = False \n            return return_identifier_0 \n        return signame \n    signame = name_num . upper ( ) \n    if not signame . startswith ( 'SIG' ) : \n        return 'SIG' + signame \n    return signame "}
{"6042": "\ndef set_signal_replacement ( self , signum , handle ) : \n    signame = lookup_signame ( signum ) \n    if signame is None : \n        self . dbgr . intf [ - 1 ] . errmsg ( ( \"%s is not a signal number\" \" I know about.\" ) % signum ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . sigs [ signame ] . pass_along = True \n    if self . check_and_adjust_sighandler ( signame , self . sigs ) : \n        self . sigs [ signame ] . old_handler = handle \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"6044": "\ndef info_signal ( self , args ) : \n    if len ( args ) == 0 : \n        return None \n    signame = args [ 0 ] \n    if signame in [ 'handle' , 'signal' ] : \n        if len ( args ) == 1 : \n            self . dbgr . core . processor . section ( self . header ) \n            for signame in self . siglist : \n                self . print_info_signal_entry ( signame ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            signame = args [ 1 ] \n            pass \n        pass \n    signame = self . is_name_or_number ( signame ) \n    self . dbgr . core . processor . section ( self . header ) \n    self . print_info_signal_entry ( signame ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    args = arg . split ( ) \n    signame = args [ 0 ] \n    signame = self . is_name_or_number ( args [ 0 ] ) \n    if not signame : \n        return \n    if len ( args ) == 1 : \n        self . info_signal ( [ signame ] ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ 1 : ] : \n        if attr . startswith ( 'no' ) : \n            on = False \n            attr = attr [ 2 : ] \n        else : \n            on = True \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        elif 'print' . startswith ( attr ) and len ( attr ) >= 2 : \n            self . handle_print ( signame , on ) \n        elif 'pass' . startswith ( attr ) : \n            self . handle_pass ( signame , on ) \n        elif 'ignore' . startswith ( attr ) : \n            self . handle_ignore ( signame , on ) \n        elif 'stack' . startswith ( attr ) : \n            self . handle_print_stack ( signame , on ) \n        else : \n            self . dbgr . intf [ - 1 ] . errmsg ( 'Invalid arguments' ) \n            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6053": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        if mainfile : \n            part1 = \"Python program '%s' is stopped\" % mainfile \n        else : \n            part1 = 'Program is stopped' \n            pass \n        if self . proc . event : \n            msg = 'via a %s event.' % self . proc . event \n        else : \n            msg = '.' \n        self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        if self . proc . curframe : \n            self . msg ( \"PC offset is %d.\" % self . proc . curframe . f_lasti ) \n        if self . proc . event == 'return' : \n            val = self . proc . event_arg \n            part1 = 'Return value is' \n            self . msg ( Mmisc . wrapped_lines ( part1 , self . proc . _saferepr ( val ) , self . settings [ 'width' ] ) ) \n            pass \n        elif self . proc . event == 'exception' : \n            exc_type , exc_value , exc_tb = self . proc . event_arg \n            self . msg ( 'Exception type: %s' % self . proc . _saferepr ( exc_type ) ) \n            if exc_value : \n                self . msg ( 'Exception value: %s' % self . proc . _saferepr ( exc_value ) ) \n                pass \n            pass \n        self . msg ( 'It stopped %s.' % self . core . stop_reason ) \n        if self . proc . event in [ 'signal' , 'exception' , 'c_exception' ] : \n            self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) \n            pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"6058": "\ndef complete_identifier ( cmd , prefix ) : \n    if not cmd . proc . curframe : \n        return_identifier_0 = [ None ] \n        return return_identifier_0 \n    ns = cmd . proc . curframe . f_globals . copy ( ) \n    ns . update ( cmd . proc . curframe . f_locals ) \n    if '.' in prefix : \n        dotted = prefix . split ( '.' ) \n        try : \n            obj = ns [ dotted [ 0 ] ] \n            for part in dotted [ 1 : - 1 ] : \n                obj = getattr ( obj , part ) \n        except ( KeyError , AttributeError ) : \n            return_identifier_1 = [ ] \n            return return_identifier_1 \n        pre_prefix = '.' . join ( dotted [ : - 1 ] ) + '.' \n        return [ pre_prefix + n for n in dir ( obj ) if n . startswith ( dotted [ - 1 ] ) ] \n    else : \n        return Mcomplete . complete_token ( ns . keys ( ) , prefix ) "}
{"6064": "\ndef is_stop_here ( self , frame , event , arg ) : \n    lineno = frame . f_lineno \n    filename = frame . f_code . co_filename \n    if self . different_line and event == 'line' : \n        if self . last_lineno == lineno and self . last_filename == filename : \n            return_identifier_0 = False \n            return return_identifier_0 \n        pass \n    self . last_lineno = lineno \n    self . last_filename = filename \n    if self . stop_level is not None : \n        if frame != self . last_frame : \n            self . last_level = Mstack . count_frames ( frame ) \n            self . last_frame = frame \n            pass \n        if self . last_level > self . stop_level : \n            return_identifier_1 = False \n            return return_identifier_1 \n        elif self . last_level == self . stop_level and self . stop_on_finish and event in [ 'return' , 'c_return' ] : \n            self . stop_level = None \n            self . stop_reason = \"in return for 'finish' command\" \n            return_identifier_2 = True \n            return return_identifier_2 \n        pass \n    if self . _is_step_next_stop ( event ) : \n        self . stop_reason = 'at a stepping statement' \n        return_identifier_3 = True \n        return return_identifier_3 \n    return_identifier_4 = False \n    return return_identifier_4 "}
{"6067": "\ndef run ( self , args ) : \n    if len ( args ) == 0 : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return_identifier_0 = False \n            return return_identifier_0 \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ 0 ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m += \" cached in debugger\" \n        if filename_cache [ filename ] != filename : \n            m += ' as:' \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m += '.' \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( len ( matches ) > 1 ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        elif len ( matches ) == 1 : \n            canonic_name = pyficache . unmap_file ( matches [ 0 ] ) \n            m += \" matched debugger cache file:\\n  \" + canonic_name \n            self . msg ( m ) \n        else : \n            self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if name == v ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ 1 : ] : \n        processed_arg = False \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = False , arrange_vertical = False , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = True \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6068": "\ndef checkfuncname ( b , frame ) : \n    if not b . funcname : \n        if b . line != frame . f_lineno : \n            return_identifier_0 = False \n            return return_identifier_0 \n        return_identifier_1 = True \n        return return_identifier_1 \n    if frame . f_code . co_name != b . funcname : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if not b . func_first_executable_line : \n        b . func_first_executable_line = frame . f_lineno \n    if b . func_first_executable_line != frame . f_lineno : \n        return_identifier_3 = False \n        return return_identifier_3 \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"6069": "\ndef delete_breakpoint ( self , bp ) : \n    bpnum = bp . number \n    self . bpbynumber [ bpnum ] = None \n    index = ( bp . filename , bp . line ) \n    if index not in self . bplist : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . bplist [ index ] . remove ( bp ) \n    if not self . bplist [ index ] : \n        del self . bplist [ index ] \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"6070": "\ndef delete_breakpoint_by_number ( self , bpnum ) : \n    success , msg , bp = self . get_breakpoint ( bpnum ) \n    if not success : \n        return False , msg \n    self . delete_breakpoint ( bp ) \n    return_identifier_0 = ( True , '' ) \n    return return_identifier_0 "}
{"6072": "\ndef en_disable_breakpoint_by_number ( self , bpnum , do_enable = True ) : \n    success , msg , bp = self . get_breakpoint ( bpnum ) \n    if not success : \n        return success , msg \n    if do_enable : \n        endis = 'en' \n    else : \n        endis = 'dis' \n        pass \n    if bp . enabled == do_enable : \n        return_identifier_0 = ( False , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) \n        return return_identifier_0 \n    bp . enabled = do_enable \n    return_identifier_1 = ( True , '' ) \n    return return_identifier_1 "}
{"6073": "\ndef delete_breakpoints_by_lineno ( self , filename , lineno ) : \n    if ( filename , lineno ) not in self . bplist : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    breakpoints = self . bplist [ ( filename , lineno ) ] \n    bpnums = [ bp . number for bp in breakpoints ] \n    for bp in list ( breakpoints ) : \n        self . delete_breakpoint ( bp ) \n    return bpnums "}
{"6076": "\ndef confirm ( self , prompt , default ) : \n    while True : \n        try : \n            self . write_confirm ( prompt , default ) \n            reply = self . readline ( '' ) . strip ( ) . lower ( ) \n        except EOFError : \n            return default \n        if reply in ( 'y' , 'yes' ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        elif reply in ( 'n' , 'no' ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n        else : \n            self . msg ( \"Please answer y or n.\" ) \n            pass \n        pass \n    return default "}
{"6110": "\ndef _simplify_doc ( doc ) : \n    doc = dict ( doc ) \n    if 'text' not in doc : \n        raise ValueError ( \"The document {!r} has no text field\" . format ( doc ) ) \n    return_identifier_0 = { 'text' : doc [ 'text' ] , 'metadata' : doc . get ( 'metadata' , [ ] ) , 'title' : doc . get ( 'title' , '' ) } \n    return return_identifier_0 "}
{"6119": "\ndef _post_login_page ( self ) : \n    data = { 'IDToken1' : self . username , 'IDToken2' : self . password , 'SunQueryParamsString' : base64 . b64encode ( b'realm=particuliers' ) , 'encoded' : 'true' , 'gx_charset' : 'UTF-8' } \n    try : \n        self . _session . post ( LOGIN_URL , data = data , allow_redirects = False , timeout = self . _timeout ) \n    except OSError : \n        raise PyLinkyError ( \"Can not submit login form\" ) \n    if 'iPlanetDirectoryPro' not in self . _session . cookies : \n        raise PyLinkyError ( \"Login error: Please check your username/password.\" ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"6165": "\ndef add_exit ( self ) : \n    if self . items : \n        if self . items [ - 1 ] is not self . exit_item : \n            self . items . append ( self . exit_item ) \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"6277": "\ndef valid ( self ) : \n    if self . expiration_time : \n        return self . expiration_time > int ( time . time ( ) ) \n    else : \n        return_identifier_0 = True \n        return return_identifier_0 "}
{"6278": "\ndef expire_soon ( self , seconds ) : \n    if self . expiration_time : \n        return self . expiration_time < int ( time . time ( ) ) + int ( seconds ) \n    else : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"6296": "\ndef ensure_iterable ( inst ) : \n    if isinstance ( inst , str ) : \n        return_identifier_0 = [ inst ] \n        return return_identifier_0 \n    elif not isinstance ( inst , collections . abc . Iterable ) : \n        return_identifier_1 = [ inst ] \n        return return_identifier_1 \n    else : \n        return inst "}
{"6297": "\ndef isclose ( a , b , * , rel_tol = 1e-09 , abs_tol = 0.0 ) : \n    try : \n        return math . isclose ( a , b , rel_tol = rel_tol , abs_tol = abs_tol ) \n    except AttributeError : \n        if ( rel_tol < 0.0 ) or ( abs_tol < 0.0 ) : \n            raise ValueError ( \"Tolerances must be non-negative, but are rel_tol: {} and abs_tol: {}\" . format ( rel_tol , abs_tol ) ) \n        if math . isnan ( a ) or math . isnan ( b ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if ( a == b ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n        if math . isinf ( a ) or math . isinf ( b ) : \n            return_identifier_2 = False \n            return return_identifier_2 \n        diff = abs ( a - b ) \n        return ( diff <= rel_tol * abs ( b ) ) or ( diff <= rel_tol * abs ( a ) ) or ( diff <= abs_tol ) "}
{"6331": "\ndef equal_ignore_order ( a , b ) : \n    unmatched = list ( b ) \n    for element in a : \n        try : \n            unmatched . remove ( element ) \n        except ValueError : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return not unmatched "}
{"6381": "\ndef StaticForEach ( parentUnit , items , bodyFn , name = \"\" ) : \n    items = list ( items ) \n    itemsCnt = len ( items ) \n    if itemsCnt == 0 : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    elif itemsCnt == 1 : \n        return bodyFn ( items [ 0 ] , 0 ) \n    else : \n        index = parentUnit . _reg ( name + \"for_index\" , Bits ( log2ceil ( itemsCnt + 1 ) , signed = False ) , defVal = 0 ) \n        ackSig = parentUnit . _sig ( name + \"for_ack\" ) \n        statementLists = [ ] \n        for i , ( statementList , ack ) in [ ( i , bodyFn ( item , i ) ) for i , item in enumerate ( items ) ] : \n            statementLists . append ( statementList + [ ( ackSig ( ack ) ) , ] ) \n        If ( ackSig , If ( index . _eq ( itemsCnt - 1 ) , index ( 0 ) ) . Else ( index ( index + 1 ) ) ) \n        return Switch ( index ) . addCases ( enumerate ( statementLists ) ) . Default ( bodyFn ( items [ 0 ] , 0 ) [ 0 ] , ackSig ( True ) ) "}
{"6398": "\ndef checkIfIsTooSimple ( proc ) : \n    try : \n        a , = proc . statements \n        if isinstance ( a , Assignment ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    except ValueError : \n        pass \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"6403": "\ndef name_for_process_and_mark_outputs ( statements : List [ HdlStatement ] ) -> str : \n    out_names = [ ] \n    for stm in statements : \n        for sig in stm . _outputs : \n            if not sig . hasGenericName : \n                out_names . append ( sig . name ) \n    if out_names : \n        return min ( out_names ) \n    else : \n        return_identifier_0 = \"\" \n        return return_identifier_0 "}
{"6406": "\ndef synthesize ( self , name , interfaces , targetPlatform ) : \n    ent = Entity ( name ) \n    ent . _name = name + \"_inst\" \n    for _ , v in self . params . items ( ) : \n        ent . generics . append ( v ) \n    if isinstance ( interfaces , set ) : \n        intfSet = interfaces \n    else : \n        intfSet = set ( interfaces ) \n    for s in interfaces : \n        pi = portItemfromSignal ( s , ent ) \n        pi . registerInternSig ( s ) \n        ent . ports . append ( pi ) \n        s . hidden = False \n    removeUnconnectedSignals ( self ) \n    markVisibilityOfSignals ( self , name , self . signals , intfSet ) \n    for proc in targetPlatform . beforeHdlArchGeneration : \n        proc ( self ) \n    arch = Architecture ( ent ) \n    for p in statements_to_HWProcesses ( self . statements ) : \n        arch . processes . append ( p ) \n    for s in self . signals : \n        if s not in intfSet and not s . hidden : \n            arch . variables . append ( s ) \n    for u in self . subUnits : \n        arch . componentInstances . append ( u ) \n    for su in distinctBy ( self . subUnits , lambda x : x . name ) : \n        arch . components . append ( su ) \n    self . synthesised = True \n    return_identifier_0 = [ ent , arch ] \n    return return_identifier_0 "}
{"6431": "\ndef _is_mergable_statement_list ( cls , stmsA , stmsB ) : \n    if stmsA is None and stmsB is None : \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif stmsA is None or stmsB is None : \n        return_identifier_1 = False \n        return return_identifier_1 \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = _get_stm_with_branches ( a_it ) \n    b = _get_stm_with_branches ( b_it ) \n    while a is not None or b is not None : \n        if a is None or b is None or not a . _is_mergable ( b ) : \n            return_identifier_2 = False \n            return return_identifier_2 \n        a = _get_stm_with_branches ( a_it ) \n        b = _get_stm_with_branches ( b_it ) \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"6448": "\ndef mkUpdater ( nextVal : Value , invalidate : bool ) : \n    def updater ( currentVal ) : \n        _nextVal = nextVal . clone ( ) \n        if invalidate : \n            _nextVal . vldMask = 0 \n        return_identifier_0 = ( valueHasChanged ( currentVal , _nextVal ) , _nextVal ) \n        return return_identifier_0 \n    return updater "}
{"6449": "\ndef mkArrayUpdater ( nextItemVal : Value , indexes : Tuple [ Value ] , invalidate : bool ) : \n    def updater ( currentVal ) : \n        if len ( indexes ) > 1 : \n            raise NotImplementedError ( \"[TODO] implement for more indexes\" ) \n        _nextItemVal = nextItemVal . clone ( ) \n        if invalidate : \n            _nextItemVal . vldMask = 0 \n        index = indexes [ 0 ] \n        change = valueHasChanged ( currentVal . _getitem__val ( index ) , _nextItemVal ) \n        currentVal . _setitem__val ( index , _nextItemVal ) \n        return_identifier_0 = ( change , currentVal ) \n        return return_identifier_0 \n    return updater "}
{"6469": "\ndef serializationDecision ( cls , obj , serializedClasses , serializedConfiguredUnits ) : \n    isDeclaration = isinstance ( obj , Entity ) \n    isDefinition = isinstance ( obj , Architecture ) \n    if isDeclaration : \n        unit = obj . origin \n    elif isDefinition : \n        unit = obj . entity . origin \n    else : \n        return_identifier_0 = True \n        return return_identifier_0 \n    assert isinstance ( unit , Unit ) \n    sd = unit . _serializeDecision \n    if sd is None : \n        return_identifier_1 = True \n        return return_identifier_1 \n    else : \n        prevPriv = serializedClasses . get ( unit . __class__ , None ) \n        seriazlize , nextPriv = sd ( unit , obj , isDeclaration , prevPriv ) \n        serializedClasses [ unit . __class__ ] = nextPriv \n        return seriazlize "}
{"6472": "\ndef getBaseCond ( c ) : \n    isNegated = False \n    try : \n        drivers = c . drivers \n    except AttributeError : \n        return_identifier_0 = ( c , isNegated ) \n        return return_identifier_0 \n    if len ( drivers ) == 1 : \n        d = list ( c . drivers ) [ 0 ] \n        if isinstance ( d , Operator ) and d . operator == AllOps . NOT : \n            c = d . operands [ 0 ] \n            isNegated = True \n    return_identifier_1 = ( c , isNegated ) \n    return return_identifier_1 "}
{"6515": "\ndef isEvDependentOn ( sig , process ) -> bool : \n    if sig is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return process in sig . simFallingSensProcs or process in sig . simRisingSensProcs "}
{"6520": "\ndef _conflictResolveStrategy ( self , newValue : set ) -> Tuple [ Callable [ [ Value ] , bool ] , bool ] : \n    invalidate = False \n    resLen = len ( newValue ) \n    if resLen == 3 : \n        val , indexes , isEvDependent = newValue \n        return_identifier_0 = ( mkArrayUpdater ( val , indexes , invalidate ) , isEvDependent ) \n        return return_identifier_0 \n    else : \n        val , isEvDependent = newValue \n        return_identifier_1 = ( mkUpdater ( val , invalidate ) , isEvDependent ) \n        return return_identifier_1 "}
{"6546": "\ndef is_game_over ( self ) : \n    try : \n        next ( self . generate_legal_moves ( ) . __iter__ ( ) ) \n    except StopIteration : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if self . is_fourfold_repetition ( ) : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"6547": "\ndef is_checkmate ( self ) : \n    if not self . is_check ( ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        next ( self . generate_legal_moves ( ) . __iter__ ( ) ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    except StopIteration : \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"6548": "\ndef is_fourfold_repetition ( self ) : \n    zobrist_hash = self . zobrist_hash ( ) \n    if self . transpositions [ zobrist_hash ] < 4 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"6555": "\ndef usi ( self ) : \n    if self : \n        if self . drop_piece_type : \n            return '{0}*{1}' . format ( PIECE_SYMBOLS [ self . drop_piece_type ] . upper ( ) , SQUARE_NAMES [ self . to_square ] ) \n        else : \n            return SQUARE_NAMES [ self . from_square ] + SQUARE_NAMES [ self . to_square ] + ( '+' if self . promotion else '' ) \n    else : \n        return_identifier_0 = '0000' \n        return return_identifier_0 "}
{"6571": "\ndef _get_rate ( self , currency , date ) : \n    if currency == self . ref_currency : \n        return_identifier_0 = 1.0 \n        return return_identifier_0 \n    if date not in self . _rates [ currency ] : \n        first_date , last_date = self . bounds [ currency ] \n        if not self . fallback_on_wrong_date : \n            raise RateNotFoundError ( '{0} not in {1} bounds {2}/{3}' . format ( date , currency , first_date , last_date ) ) \n        if date < first_date : \n            fallback_date = first_date \n        elif date > last_date : \n            fallback_date = last_date \n        else : \n            raise AssertionError ( 'Should never happen, bug in the code!' ) \n        if self . verbose : \n            print ( r'/!\\ {0} not in {1} bounds {2}/{3}, falling back to {4}' . format ( date , currency , first_date , last_date , fallback_date ) ) \n        date = fallback_date \n    rate = self . _rates [ currency ] [ date ] \n    if rate is None : \n        raise RateNotFoundError ( '{0} has no rate for {1}' . format ( currency , date ) ) \n    return rate "}
{"6578": "\ndef comments ( self ) : \n    record_numbers = range ( 2 , self . fward ) \n    if not record_numbers : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    data = b'' . join ( self . read_record ( n ) [ 0 : 1000 ] for n in record_numbers ) \n    try : \n        return data [ : data . find ( b'\\4' ) ] . decode ( 'ascii' ) . replace ( '\\0' , '\\n' ) \n    except IndexError : \n        raise ValueError ( 'DAF file comment area is missing its EOT byte' ) \n    except UnicodeDecodeError : \n        raise ValueError ( 'DAF file comment area is not ASCII text' ) "}
{"6724": "\ndef check_install ( software = None , quiet = True ) : \n    if software is None : \n        software = \"singularity\" \n    cmd = [ software , '--version' ] \n    try : \n        version = run_command ( cmd , software ) \n    except : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if version is not None : \n        if quiet is False and version [ 'return_code' ] == 0 : \n            version = version [ 'message' ] \n            bot . info ( \"Found %s version %s\" % ( software . upper ( ) , version ) ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"6788": "\ndef parse_header ( recipe , header = \"from\" , remove_header = True ) : \n    parsed_header = None \n    fromline = [ x for x in recipe . split ( '\\n' ) if \"%s:\" % header in x . lower ( ) ] \n    if len ( fromline ) == 0 : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    if len ( fromline ) > 0 : \n        fromline = fromline [ 0 ] \n        parsed_header = fromline . strip ( ) \n    if remove_header is True : \n        parsed_header = fromline . split ( ':' , 1 ) [ - 1 ] . strip ( ) \n    return parsed_header "}
{"6794": "\ndef useColor ( self ) : \n    COLORIZE = get_user_color_preference ( ) \n    if COLORIZE is not None : \n        return COLORIZE \n    streams = [ self . errorStream , self . outputStream ] \n    for stream in streams : \n        if not hasattr ( stream , 'isatty' ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if not stream . isatty ( ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"6795": "\ndef emitError ( self , level ) : \n    if level in [ ABORT , ERROR , WARNING , VERBOSE , VERBOSE1 , VERBOSE2 , VERBOSE3 , DEBUG ] : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"6806": "\ndef gravatar_url ( user_or_email , size = GRAVATAR_DEFAULT_SIZE ) : \n    if hasattr ( user_or_email , 'email' ) : \n        email = user_or_email . email \n    else : \n        email = user_or_email \n    try : \n        return escape ( get_gravatar_url ( email = email , size = size ) ) \n    except : \n        return_identifier_0 = '' \n        return return_identifier_0 "}
{"6808": "\ndef has_gravatar ( email ) : \n    url = get_gravatar_url ( email , default = GRAVATAR_DEFAULT_IMAGE_404 ) \n    try : \n        request = Request ( url ) \n        request . get_method = lambda : 'HEAD' \n        return 200 == urlopen ( request ) . code \n    except ( HTTPError , URLError ) : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"6881": "\ndef _submitQuery ( self , gitquery , gitvars = { } , verbose = False , rest = False ) : \n    errOut = DEVNULL if not verbose else None \n    authhead = 'Authorization: bearer ' + self . __githubApiToken \n    bashcurl = 'curl -iH TMPauthhead -X POST -d TMPgitquery https://api.github.com/graphql' if not rest else 'curl -iH TMPauthhead https://api.github.com' + gitquery \n    bashcurl_list = bashcurl . split ( ) \n    bashcurl_list [ 2 ] = authhead \n    if not rest : \n        gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) \n        bashcurl_list [ 6 ] = gitqueryJSON \n    fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) \n    _vPrint ( verbose , \"\\n\" + fullResponse ) \n    fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) \n    heads = fullResponse [ 0 ] . split ( '\\r\\n' ) \n    if len ( fullResponse ) > 1 : \n        result = fullResponse [ 1 ] \n    else : \n        result = \"\" \n    http = heads [ 0 ] . split ( ) \n    statusNum = int ( http [ 1 ] ) \n    headDict = { } \n    headDict [ \"http\" ] = heads [ 0 ] \n    for header in heads [ 1 : ] : \n        h = header . split ( ': ' ) \n        headDict [ h [ 0 ] ] = h [ 1 ] \n    linkDict = None \n    if \"Link\" in headDict : \n        linkProperties = headDict [ \"Link\" ] . split ( ', ' ) \n        propDict = { } \n        for item in linkProperties : \n            divided = re . split ( r'<https://api.github.com|>; rel=\"|\"' , item ) \n            propDict [ divided [ 2 ] ] = divided [ 1 ] \n        linkDict = propDict \n    return_identifier_0 = { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } \n    return return_identifier_0 "}
{"6909": "\ndef _resolve_path ( obj , path ) : \n    if obj . __class__ not in path . context . accept : \n        result = set ( ) \n        for ctx in path . context . accept : \n            result |= { e for u in obj [ ctx ] for e in _resolve_path ( u , path ) } \n        return result \n    if isinstance ( obj , Text ) : \n        if path . index is not None : \n            return_identifier_0 = { obj . children [ path . index ] } \n            return return_identifier_0 \n        return set ( obj . children ) \n    if isinstance ( obj , ( Fact , Theory ) ) : \n        return _resolve_path_tree_graph ( obj . tree_graph , path ) \n    if isinstance ( obj , Topic ) : \n        if path . kind == 'r' : \n            if path . index is not None : \n                return_identifier_1 = { obj . root [ path . index ] } \n                return return_identifier_1 \n            return set ( obj . root ) \n        else : \n            if path . index is not None : \n                return_identifier_2 = { obj . flexing [ path . index ] } \n                return return_identifier_2 \n            return set ( obj . flexing ) "}
{"6911": "\ndef mean ( self ) : \n    if self . counter . value > 0 : \n        return self . sum . value / self . counter . value \n    return_identifier_0 = 0.0 \n    return return_identifier_0 "}
{"6913": "\ndef mean_rate ( self ) : \n    if self . counter . value == 0 : \n        return_identifier_0 = 0.0 \n        return return_identifier_0 \n    else : \n        elapsed = time ( ) - self . start_time \n        return self . counter . value / elapsed "}
{"6922": "\ndef get_ledger_code_to_guid_map ( self , codes ) : \n    if codes : \n        codes = set ( str ( i ) for i in codes ) \n        ledger_ids = self . _api . ledgeraccounts . filter ( code__in = codes ) \n        ret = dict ( ( str ( i [ 'Code' ] ) , i [ 'ID' ] ) for i in ledger_ids ) \n        found = set ( ret . keys ( ) ) \n        missing = ( codes - found ) \n        if missing : \n            raise UnknownLedgerCodes ( missing ) \n        return ret \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"6945": "\ndef is_starved ( self ) : \n    for conn in itervalues ( self . conns ) : \n        if conn . in_flight > 0 and conn . in_flight >= ( conn . last_rdy * 0.85 ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"6955": "\ndef prediction ( self , input_data = '' , mode = 'test_data' ) : \n    prediction = { } \n    if ( self . status != 'train' ) : \n        print ( \"Please load train data and init W then train the W first.\" ) \n        return prediction \n    if ( input_data == '' ) : \n        print ( \"Please input test data for prediction.\" ) \n        return prediction \n    if mode == 'future_data' : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return_identifier_0 = { \"input_data_x\" : input_data_x , \"input_data_y\" : None , \"prediction\" : prediction } \n        return return_identifier_0 \n    else : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data [ : - 1 ] ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        input_data_y = float ( data [ - 1 ] ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return_identifier_1 = { \"input_data_x\" : input_data_x , \"input_data_y\" : input_data_y , \"prediction\" : prediction } \n        return return_identifier_1 "}
{"6965": "\ndef get_user_channel ( self , input_channel , input_type = None ) : \n    res = { \"input_channel\" : input_channel } \n    itype = input_type if input_type else self . input_type \n    if itype in self . RAW_MAPPING : \n        channel_info = self . RAW_MAPPING [ itype ] \n        return_identifier_0 = { ** res , ** channel_info } \n        return return_identifier_0 "}
{"6978": "\ndef _get_gc_content ( sequence , length ) : \n    at = sum ( map ( sequence . count , [ \"A\" , \"T\" ] ) ) \n    gc = sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) \n    n = length - ( at + gc ) \n    at_prop = at / length \n    gc_prop = gc / length \n    n_prop = n / length \n    return_identifier_0 = { \"at\" : at , \"gc\" : gc , \"n\" : n , \"at_prop\" : at_prop , \"gc_prop\" : gc_prop , \"n_prop\" : n_prop } \n    return return_identifier_0 "}
{"6987": "\ndef get_source_lane ( fork_process , pipeline_list ) : \n    fork_source = fork_process [ - 1 ] \n    fork_sig = [ x for x in fork_process if x != \"__init__\" ] \n    for position , p in enumerate ( pipeline_list [ : : - 1 ] ) : \n        if p [ \"output\" ] [ \"process\" ] == fork_source : \n            lane = p [ \"output\" ] [ \"lane\" ] \n            logger . debug ( \"Possible source match found in position {} in lane\" \" {}\" . format ( position , lane ) ) \n            lane_sequence = [ x [ \"output\" ] [ \"process\" ] for x in pipeline_list if x [ \"output\" ] [ \"lane\" ] == lane ] \n            logger . debug ( \"Testing lane sequence '{}' against fork signature\" \" '{}'\" . format ( lane_sequence , fork_sig ) ) \n            if lane_sequence == fork_sig : \n                return p [ \"output\" ] [ \"lane\" ] \n    return_identifier_0 = 0 \n    return return_identifier_0 "}
{"6995": "\ndef _hms ( s ) : \n    if s == \"-\" : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    if s . endswith ( \"ms\" ) : \n        return float ( s . rstrip ( \"ms\" ) ) / 1000 \n    fields = list ( map ( float , re . split ( \"[dhms]\" , s ) [ : - 1 ] ) ) \n    if len ( fields ) == 4 : \n        return fields [ 0 ] * 24 * 3600 + fields [ 1 ] * 3600 + fields [ 2 ] * 60 + fields [ 3 ] \n    if len ( fields ) == 3 : \n        return fields [ 0 ] * 3600 + fields [ 1 ] * 60 + fields [ 2 ] \n    elif len ( fields ) == 2 : \n        return fields [ 0 ] * 60 + fields [ 1 ] \n    else : \n        return fields [ 0 ] "}
{"7018": "\ndef validate_pipeline ( pipeline_string ) : \n    if \"(\" in pipeline_string or \")\" in pipeline_string or \"|\" in pipeline_string : \n        logger . error ( colored_print ( \"Please provide a valid task list!\" , \"red_bold\" ) ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"7042": "\ndef get_trim_index ( biased_list ) : \n    if set ( biased_list ) == { False } : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    if set ( biased_list [ : 5 ] ) == { False } : \n        return_identifier_1 = 0 \n        return return_identifier_1 \n    for i , val in enumerate ( biased_list ) : \n        if val and set ( biased_list [ i + 1 : i + 3 ] ) == { False } : \n            return i + 1 \n    return len ( biased_list ) "}
{"7050": "\ndef _search_tree_backwards ( self , template , parent_lanes ) : \n    for p in self . processes [ : : - 1 ] : \n        if p . lane not in parent_lanes : \n            continue \n        if p . template == template : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"7115": "\ndef feat_segments_to_2dfmc_max ( feat_segments , offset = 4 ) : \n    if len ( feat_segments ) == 0 : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    max_len = max ( [ feat_segment . shape [ 0 ] for feat_segment in feat_segments ] ) \n    fmcs = [ ] \n    for feat_segment in feat_segments : \n        X = np . zeros ( ( max_len , feat_segment . shape [ 1 ] ) ) \n        if feat_segment . shape [ 0 ] <= offset or offset == 0 : \n            X [ : feat_segment . shape [ 0 ] , : ] = feat_segment \n        else : \n            X [ : feat_segment . shape [ 0 ] - offset , : ] = feat_segment [ offset // 2 : - offset // 2 , : ] \n        try : \n            fmcs . append ( utils2d . compute_ffmc2d ( X ) ) \n        except : \n            logging . warning ( \"Couldn't compute the 2D Fourier Transform\" ) \n            fmcs . append ( np . zeros ( ( X . shape [ 0 ] * X . shape [ 1 ] ) // 2 + 1 ) ) \n    return np . asarray ( fmcs ) "}
{"7152": "\ndef gaussian_cost ( X ) : \n    d , n = X . shape \n    if n < 2 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    sigma = np . var ( X , axis = 1 , ddof = 1 ) \n    cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) \n    return cost "}
{"7166": "\ndef compute_bic ( self , D , means , labels , K , R ) : \n    D = vq . whiten ( D ) \n    Rn = D . shape [ 0 ] \n    M = D . shape [ 1 ] \n    if R == K : \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    mle_var = 0 \n    for k in range ( len ( means ) ) : \n        X = D [ np . argwhere ( labels == k ) ] \n        X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) \n        for x in X : \n            mle_var += distance . euclidean ( x , means [ k ] ) \n    mle_var /= float ( R - K ) \n    l_D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) \n    p = ( K - 1 ) + M * K + mle_var \n    return l_D - p / 2. * np . log ( R ) "}
{"7173": "\ndef compute_labels ( X , rank , R , bound_idxs , niter = 300 ) : \n    try : \n        F , G = cnmf ( X , rank , niter = niter , hull = False ) \n    except : \n        return_identifier_0 = [ 1 ] \n        return return_identifier_0 \n    label_frames = filter_activation_matrix ( G . T , R ) \n    label_frames = np . asarray ( label_frames , dtype = int ) \n    labels = [ ] \n    bound_inters = zip ( bound_idxs [ : - 1 ] , bound_idxs [ 1 : ] ) \n    for bound_inter in bound_inters : \n        if bound_inter [ 1 ] - bound_inter [ 0 ] <= 0 : \n            labels . append ( np . max ( label_frames ) + 1 ) \n        else : \n            labels . append ( most_frequent ( label_frames [ bound_inter [ 0 ] : bound_inter [ 1 ] ] ) ) \n    return labels "}
{"7207": "\ndef isregex ( value ) : \n    if not value : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return any ( ( isregex_expr ( value ) , isinstance ( value , retype ) ) ) "}
{"7210": "\ndef compare ( expr , value , regex_expr = False ) : \n    if expr == value : \n        return_identifier_0 = True \n        return return_identifier_0 \n    negate = False \n    if isinstance ( expr , str ) : \n        negate = expr . startswith ( NEGATE ) \n        expr = strip_negate ( expr ) if negate else expr \n    try : \n        test ( expr , value , regex_expr = regex_expr ) \n    except Exception as err : \n        if negate : \n            return_identifier_1 = True \n            return return_identifier_1 \n        else : \n            raise err \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"7212": "\ndef match ( self , request ) : \n    errors = [ ] \n    def match ( matcher ) : \n        try : \n            return matcher . match ( request ) \n        except Exception as err : \n            err = '{}: {}' . format ( type ( matcher ) . __name__ , err ) \n            errors . append ( err ) \n            return_identifier_0 = False \n            return return_identifier_0 \n    return all ( [ match ( matcher ) for matcher in self ] ) , errors "}
{"7242": "\ndef match ( self , request ) : \n    for test in self . filters : \n        if not test ( request , self ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    match_errors = [ ] \n    for mock in self . mocks [ : ] : \n        try : \n            matches , errors = mock . match ( request . copy ( ) ) \n            if len ( errors ) : \n                match_errors += errors \n            if matches : \n                return mock \n        except PookExpiredMock : \n            self . mocks . remove ( mock ) \n    if not self . should_use_network ( request ) : \n        msg = 'pook error!\\n\\n' \n        msg += ( '=> Cannot match any mock for the ' 'following request:\\n{}' . format ( request ) ) \n        if self . debug : \n            err = '\\n\\n' . join ( [ str ( err ) for err in match_errors ] ) \n            if err : \n                msg += '\\n\\n=> Detailed matching errors:\\n{}\\n' . format ( err ) \n        raise PookNoMatches ( msg ) \n    self . unmatched_reqs . append ( request ) "}
{"7247": "\ndef remove_interceptor ( self , name ) : \n    for index , interceptor in enumerate ( self . interceptors ) : \n        matches = ( type ( interceptor ) . __name__ == name or getattr ( interceptor , 'name' ) == name ) \n        if matches : \n            self . interceptors . pop ( index ) \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"7251": "\ndef get_placeholder ( self , value = None , compiler = None , connection = None ) : \n    if value is None or value . startswith ( '\\\\x' ) : \n        return_identifier_0 = '%s' \n        return return_identifier_0 \n    return self . get_encrypt_sql ( connection ) "}
{"7255": "\ndef base_regression ( Q , slope = None ) : \n    if slope is None : \n        slope = ( Q [ dtavgii ] - Q [ tavgii ] * Q [ davgii ] / Q [ sii ] ) / ( Q [ tsqii ] - Q [ tavgii ] ** 2 / Q [ sii ] ) \n        only_intercept = False \n    else : \n        only_intercept = True \n    intercept = ( Q [ davgii ] - Q [ tavgii ] * slope ) / Q [ sii ] \n    if only_intercept : \n        return_identifier_0 = { 'slope' : slope , 'intercept' : intercept , 'chisq' : 0.5 * ( Q [ dsqii ] / Q [ sii ] - Q [ davgii ] ** 2 / Q [ sii ] ** 2 ) } \n        return return_identifier_0 \n    chisq = 0.5 * ( Q [ dsqii ] - Q [ davgii ] ** 2 / Q [ sii ] - ( Q [ dtavgii ] - Q [ davgii ] * Q [ tavgii ] / Q [ sii ] ) ** 2 / ( Q [ tsqii ] - Q [ tavgii ] ** 2 / Q [ sii ] ) ) \n    estimator_hessian = np . array ( [ [ Q [ tsqii ] , Q [ tavgii ] ] , [ Q [ tavgii ] , Q [ sii ] ] ] ) \n    return_identifier_1 = { 'slope' : slope , 'intercept' : intercept , 'chisq' : chisq , 'hessian' : estimator_hessian , 'cov' : np . linalg . inv ( estimator_hessian ) } \n    return return_identifier_1 "}
{"7269": "\ndef normalize_profile ( in_profile , log = False , return_offset = True ) : \n    if log : \n        tmp_prefactor = in_profile . max ( axis = 1 ) \n        tmp_prof = np . exp ( in_profile . T - tmp_prefactor ) . T \n    else : \n        tmp_prefactor = 0.0 \n        tmp_prof = in_profile \n    norm_vector = tmp_prof . sum ( axis = 1 ) \n    return_identifier_0 = ( np . copy ( np . einsum ( 'ai,a->ai' , tmp_prof , 1.0 / norm_vector ) ) , ( np . log ( norm_vector ) + tmp_prefactor ) if return_offset else None ) \n    return return_identifier_0 "}
{"7287": "\ndef optimize_branch_length_global ( self , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length_global: running branch length optimization...\" , 1 ) \n    def neg_log ( s ) : \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            n . branch_length = si ** 2 \n        self . infer_ancestral_sequences ( marginal = True ) \n        gradient = [ ] \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            if n . up : \n                pp , pc = self . marginal_branch_profile ( n ) \n                Qtds = self . gtr . expQsds ( si ) . T \n                Qt = self . gtr . expQs ( si ) . T \n                res = pp . dot ( Qt ) \n                overlap = np . sum ( res * pc , axis = 1 ) \n                res_ds = pp . dot ( Qtds ) \n                overlap_ds = np . sum ( res_ds * pc , axis = 1 ) \n                logP = np . sum ( self . multiplicity * overlap_ds / overlap ) \n                gradient . append ( logP ) \n            else : \n                gradient . append ( 2 * ( si ** 2 - 0.001 ) ) \n        print ( - self . tree . sequence_marginal_LH ) \n        return_identifier_0 = ( - self . tree . sequence_marginal_LH + ( s [ 0 ] ** 2 - 0.001 ) ** 2 , - 1.0 * np . array ( gradient ) ) \n        return return_identifier_0 \n    from scipy . optimize import minimize \n    x0 = np . sqrt ( [ n . branch_length for n in self . tree . find_clades ( order = 'preorder' ) ] ) \n    sol = minimize ( neg_log , x0 , jac = True ) \n    for new_len , node in zip ( sol [ 'x' ] , self . tree . find_clades ( ) ) : \n        self . logger ( \"Optimization results: old_len=%.4f, new_len=%.4f \" \" Updating branch length...\" % ( node . branch_length , new_len ) , 5 ) \n        node . branch_length = new_len ** 2 \n        node . mutation_length = new_len ** 2 \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    self . _prepare_nodes ( ) "}
{"7308": "\ndef assure_tree ( params , tmp_dir = 'treetime_tmp' ) : \n    if params . tree is None : \n        params . tree = os . path . basename ( params . aln ) + '.nwk' \n        print ( \"No tree given: inferring tree\" ) \n        utils . tree_inference ( params . aln , params . tree , tmp_dir = tmp_dir ) \n    if os . path . isdir ( tmp_dir ) : \n        shutil . rmtree ( tmp_dir ) \n    try : \n        tt = TreeAnc ( params . tree ) \n    except : \n        print ( \"Tree loading/building failed.\" ) \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    return_identifier_1 = 0 \n    return return_identifier_1 "}
{"7311": "\ndef ancestral_reconstruction ( params ) : \n    if assure_tree ( params , tmp_dir = 'ancestral_tmp' ) : \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    outdir = get_outdir ( params , '_ancestral' ) \n    basename = get_basename ( params , outdir ) \n    gtr = create_gtr ( params ) \n    aln , ref , fixed_pi = read_if_vcf ( params ) \n    is_vcf = True if ref is not None else False \n    treeanc = TreeAnc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = 1 , fill_overhangs = not params . keep_overhangs ) \n    ndiff = treeanc . infer_ancestral_sequences ( 'ml' , infer_gtr = params . gtr == 'infer' , marginal = params . marginal , fixed_pi = fixed_pi ) \n    if ndiff == ttconf . ERROR : \n        return_identifier_1 = 1 \n        return return_identifier_1 \n    if params . gtr == \"infer\" : \n        print ( '\\nInferred GTR model:' ) \n        print ( treeanc . gtr ) \n    export_sequences_and_tree ( treeanc , basename , is_vcf , params . zero_based , report_ambiguous = params . report_ambiguous ) \n    return_identifier_2 = 0 \n    return return_identifier_2 "}
{"7341": "\ndef _ask_for_credentials ( ) : \n    _print_msg ( 'Please enter your SolveBio credentials' ) \n    domain = raw_input ( 'Domain (e.g. <domain>.solvebio.com): ' ) \n    try : \n        account = client . request ( 'get' , '/p/accounts/{}' . format ( domain ) ) \n        auth = account [ 'authentication' ] \n    except : \n        raise SolveError ( 'Invalid domain: {}' . format ( domain ) ) \n    if auth . get ( 'login' ) or auth . get ( 'SAML' , { } ) . get ( 'simple_login' ) : \n        email = raw_input ( 'Email: ' ) \n        password = getpass . getpass ( 'Password (typing will be hidden): ' ) \n        return_identifier_0 = ( domain , email , password ) \n        return return_identifier_0 \n    else : \n        _print_msg ( 'Your domain uses Single Sign-On (SSO). ' 'Please visit https://{}.solvebio.com/settings/security ' 'for instructions on how to log in.' . format ( domain ) ) \n        sys . exit ( 1 ) "}
{"7361": "\ndef row_to_dict ( self , row , allele , alternate_alleles ) : \n    def _variant_sbid ( ** kwargs ) : \n        return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( ** kwargs ) . upper ( ) \n    if allele == '.' : \n        allele = row . REF or allele \n    genomic_coordinates = { 'build' : self . genome_build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - 1 } \n    variant_sbid = _variant_sbid ( allele = allele , ** genomic_coordinates ) \n    return_identifier_0 = { 'genomic_coordinates' : genomic_coordinates , 'variant' : variant_sbid , 'allele' : allele , 'row_id' : row . ID , 'reference_allele' : row . REF , 'alternate_alleles' : alternate_alleles , 'info' : self . _parse_info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER } \n    return return_identifier_0 "}
{"7374": "\ndef validate_api_host_url ( url ) : \n    if not url : \n        raise SolveError ( 'No SolveBio API host is set' ) \n    parsed = urlparse ( url ) \n    if parsed . scheme not in [ 'http' , 'https' ] : \n        raise SolveError ( 'Invalid API host: %s. ' 'Missing url scheme (HTTP or HTTPS).' % url ) \n    elif not parsed . netloc : \n        raise SolveError ( 'Invalid API host: %s.' % url ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"7383": "\ndef get_system_config_dirs ( app_name , app_author , force_xdg = True ) : \n    if WIN : \n        folder = os . environ . get ( 'PROGRAMDATA' ) \n        return_identifier_0 = [ os . path . join ( folder , app_author , app_name ) ] \n        return return_identifier_0 \n    if MAC and not force_xdg : \n        return_identifier_1 = [ os . path . join ( '/Library/Application Support' , app_name ) ] \n        return return_identifier_1 \n    dirs = os . environ . get ( 'XDG_CONFIG_DIRS' , '/etc/xdg' ) \n    paths = [ os . path . expanduser ( x ) for x in dirs . split ( os . pathsep ) ] \n    return [ os . path . join ( d , _pathify ( app_name ) ) for d in paths ] "}
{"7399": "\ndef truncate_string ( data , headers , max_field_width = None , ** _ ) : \n    return_identifier_0 = ( ( [ utils . truncate_string ( v , max_field_width ) for v in row ] for row in data ) , [ utils . truncate_string ( h , max_field_width ) for h in headers ] ) \n    return return_identifier_0 "}
{"7405": "\ndef isNum ( self , type ) : \n    if type in ( CKA_CERTIFICATE_TYPE , CKA_CLASS , CKA_KEY_GEN_MECHANISM , CKA_KEY_TYPE , CKA_MODULUS_BITS , CKA_VALUE_BITS , CKA_VALUE_LEN ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"7406": "\ndef isBool ( self , type ) : \n    if type in ( CKA_ALWAYS_SENSITIVE , CKA_DECRYPT , CKA_DERIVE , CKA_ENCRYPT , CKA_EXTRACTABLE , CKA_HAS_RESET , CKA_LOCAL , CKA_MODIFIABLE , CKA_NEVER_EXTRACTABLE , CKA_PRIVATE , CKA_RESET_ON_INIT , CKA_SECONDARY_AUTH , CKA_SENSITIVE , CKA_SIGN , CKA_SIGN_RECOVER , CKA_TOKEN , CKA_TRUSTED , CKA_UNWRAP , CKA_VERIFY , CKA_VERIFY_RECOVER , CKA_WRAP , CKA_WRAP_WITH_TRUSTED ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"7457": "\ndef sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : \n    skip_count = ( per_page * max ( page - 1 , 0 ) ) \n    more_variants = True if variants_query . count ( ) > ( skip_count + per_page ) else False \n    genome_build = case_obj . get ( 'genome_build' , '37' ) \n    if genome_build not in [ '37' , '38' ] : \n        genome_build = '37' \n    return_identifier_0 = { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , } \n    return return_identifier_0 "}
{"7459": "\ndef str_variant ( store , institute_id , case_name , variant_id ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    variant_obj = store . variant ( variant_id ) \n    variant_case ( store , case_obj , variant_obj ) \n    variant_obj [ 'callers' ] = callers ( variant_obj , category = 'str' ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = True ) \n    return_identifier_0 = { 'institute' : institute_obj , 'case' : case_obj , 'variant' : variant_obj , 'overlapping_snvs' : overlapping_snvs , 'manual_rank_options' : MANUAL_RANK_OPTIONS , 'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS } \n    return return_identifier_0 "}
{"7460": "\ndef sv_variant ( store , institute_id , case_name , variant_id = None , variant_obj = None , add_case = True , get_overlapping = True ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    if not variant_obj : \n        variant_obj = store . variant ( variant_id ) \n    if add_case : \n        variant_case ( store , case_obj , variant_obj ) \n    variant_obj [ 'frequencies' ] = [ ( '1000G' , variant_obj . get ( 'thousand_genomes_frequency' ) ) , ( '1000G (left)' , variant_obj . get ( 'thousand_genomes_frequency_left' ) ) , ( '1000G (right)' , variant_obj . get ( 'thousand_genomes_frequency_right' ) ) , ( 'ClinGen CGH (benign)' , variant_obj . get ( 'clingen_cgh_benign' ) ) , ( 'ClinGen CGH (pathogenic)' , variant_obj . get ( 'clingen_cgh_pathogenic' ) ) , ( 'ClinGen NGI' , variant_obj . get ( 'clingen_ngi' ) ) , ( 'SweGen' , variant_obj . get ( 'swegen' ) ) , ( 'Decipher' , variant_obj . get ( 'decipher' ) ) , ] \n    variant_obj [ 'callers' ] = callers ( variant_obj , category = 'sv' ) \n    overlapping_snvs = [ ] \n    if get_overlapping : \n        overlapping_snvs = ( parse_variant ( store , institute_obj , case_obj , variant ) for variant in store . overlapping ( variant_obj ) ) \n    for gene_obj in variant_obj [ 'genes' ] : \n        if gene_obj . get ( 'common' ) : \n            ensembl_id = gene_obj [ 'common' ] [ 'ensembl_id' ] \n            try : \n                build = int ( gene_obj [ 'common' ] . get ( 'build' , '37' ) ) \n            except Exception : \n                build = 37 \n            gene_obj [ 'ensembl_link' ] = ensembl ( ensembl_id , build = build ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = True ) \n    case_clinvars = store . case_to_clinVars ( case_obj . get ( 'display_name' ) ) \n    if variant_id in case_clinvars : \n        variant_obj [ 'clinvar_clinsig' ] = case_clinvars . get ( variant_id ) [ 'clinsig' ] \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return_identifier_0 = { 'institute' : institute_obj , 'case' : case_obj , 'variant' : variant_obj , 'overlapping_snvs' : overlapping_snvs , 'manual_rank_options' : MANUAL_RANK_OPTIONS , 'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS } \n    return return_identifier_0 "}
{"7471": "\ndef frequency ( variant_obj ) : \n    most_common_frequency = max ( variant_obj . get ( 'thousand_genomes_frequency' ) or 0 , variant_obj . get ( 'exac_frequency' ) or 0 ) \n    if most_common_frequency > .05 : \n        return_identifier_0 = 'common' \n        return return_identifier_0 \n    elif most_common_frequency > .01 : \n        return_identifier_1 = 'uncommon' \n        return return_identifier_1 \n    else : \n        return_identifier_2 = 'rare' \n        return return_identifier_2 "}
{"7477": "\ndef spidex_human ( variant_obj ) : \n    if variant_obj . get ( 'spidex' ) is None : \n        return_identifier_0 = 'not_reported' \n        return return_identifier_0 \n    elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : \n        return_identifier_1 = 'low' \n        return return_identifier_1 \n    elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : \n        return_identifier_2 = 'medium' \n        return return_identifier_2 \n    else : \n        return_identifier_3 = 'high' \n        return return_identifier_3 "}
{"7511": "\ndef is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if ps_terms : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if pm_terms : \n            if pp_terms : \n                return_identifier_1 = True \n                return return_identifier_1 \n            if len ( pm_terms ) >= 2 : \n                return_identifier_2 = True \n                return return_identifier_2 \n        if len ( pp_terms ) >= 2 : \n            return_identifier_3 = True \n            return return_identifier_3 \n    if ps_terms : \n        if len ( ps_terms ) >= 2 : \n            return_identifier_4 = True \n            return return_identifier_4 \n        if pm_terms : \n            if len ( pm_terms ) >= 3 : \n                return_identifier_5 = True \n                return return_identifier_5 \n            elif len ( pm_terms ) >= 2 : \n                if len ( pp_terms ) >= 2 : \n                    return_identifier_6 = True \n                    return return_identifier_6 \n            elif len ( pp_terms ) >= 4 : \n                return_identifier_7 = True \n                return return_identifier_7 \n    return_identifier_8 = False \n    return return_identifier_8 "}
{"7512": "\ndef is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if pm_terms : \n            return_identifier_0 = True \n            return return_identifier_0 \n    if ps_terms : \n        if pm_terms : \n            return_identifier_1 = True \n            return return_identifier_1 \n        if len ( pp_terms ) >= 2 : \n            return_identifier_2 = True \n            return return_identifier_2 \n    if pm_terms : \n        if len ( pm_terms ) >= 3 : \n            return_identifier_3 = True \n            return return_identifier_3 \n        elif len ( pm_terms ) >= 2 : \n            if len ( pp_terms ) >= 2 : \n                return_identifier_4 = True \n                return return_identifier_4 \n        elif len ( pp_terms ) >= 4 : \n            return_identifier_5 = True \n            return return_identifier_5 \n    return_identifier_6 = False \n    return return_identifier_6 "}
{"7513": "\ndef is_likely_benign ( bs_terms , bp_terms ) : \n    if bs_terms : \n        if bp_terms : \n            return_identifier_0 = True \n            return return_identifier_0 \n    if len ( bp_terms ) >= 2 : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"7522": "\ndef check_causatives ( self , case_obj = None , institute_obj = None ) : \n    institute_id = case_obj [ 'owner' ] if case_obj else institute_obj [ '_id' ] \n    institute_causative_variant_ids = self . get_causatives ( institute_id ) \n    if len ( institute_causative_variant_ids ) == 0 : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    if case_obj : \n        case_causative_ids = set ( case_obj . get ( 'causatives' , [ ] ) ) \n        institute_causative_variant_ids = list ( set ( institute_causative_variant_ids ) . difference ( case_causative_ids ) ) \n    query = self . variant_collection . find ( { '_id' : { '$in' : institute_causative_variant_ids } } , { 'variant_id' : 1 } ) \n    positional_variant_ids = [ item [ 'variant_id' ] for item in query ] \n    filters = { 'variant_id' : { '$in' : positional_variant_ids } } \n    if case_obj : \n        filters [ 'case_id' ] = case_obj [ '_id' ] \n    else : \n        filters [ 'institute' ] = institute_obj [ '_id' ] \n    return self . variant_collection . find ( filters ) "}
{"7556": "\ndef check_coordinates ( chromosome , pos , coordinates ) : \n    chrom_match = CHR_PATTERN . match ( chromosome ) \n    chrom = chrom_match . group ( 2 ) \n    if chrom != coordinates [ 'chrom' ] : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if ( pos >= coordinates [ 'start' ] and pos <= coordinates [ 'end' ] ) : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"7597": "\ndef match_date ( date ) : \n    date_pattern = re . compile ( \"^(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])\" ) \n    if re . match ( date_pattern , date ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"7602": "\ndef check_connection ( host = 'localhost' , port = 27017 , username = None , password = None , authdb = None , max_delay = 1 ) : \n    if username and password : \n        uri = ( \"mongodb://{}:{}@{}:{}/{}\" . format ( quote_plus ( username ) , quote_plus ( password ) , host , port , authdb ) ) \n        log_uri = ( \"mongodb://{}:****@{}:{}/{}\" . format ( quote_plus ( username ) , host , port , authdb ) ) \n    else : \n        log_uri = uri = \"mongodb://%s:%s\" % ( host , port ) \n    LOG . info ( \"Test connection with uri: %s\" , log_uri ) \n    client = MongoClient ( uri , serverSelectionTimeoutMS = max_delay ) \n    try : \n        client . server_info ( ) \n    except ( ServerSelectionTimeoutError , OperationFailure ) as err : \n        LOG . warning ( err ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"7716": "\ndef vcf2cytosure ( store , institute_id , case_name , individual_id ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    for individual in case_obj [ 'individuals' ] : \n        if individual [ 'individual_id' ] == individual_id : \n            individual_obj = individual \n    return_identifier_0 = ( individual_obj [ 'display_name' ] , individual_obj [ 'vcf2cytosure' ] ) \n    return return_identifier_0 "}
{"7719": "\ndef mme_add ( store , user_obj , case_obj , add_gender , add_features , add_disorders , genes_only , mme_base_url , mme_accepts , mme_token ) : \n    if not mme_base_url or not mme_accepts or not mme_token : \n        return_identifier_0 = 'Please check that Matchmaker connection parameters are valid' \n        return return_identifier_0 \n    url = '' . join ( [ mme_base_url , '/patient/add' ] ) \n    features = [ ] \n    disorders = [ ] \n    g_features = [ ] \n    contact_info = { 'name' : user_obj [ 'name' ] , 'href' : '' . join ( [ 'mailto:' , user_obj [ 'email' ] ] ) , 'institution' : 'Scout software user, Science For Life Laboratory, Stockholm, Sweden' } \n    if add_features : \n        features = hpo_terms ( case_obj ) \n    if add_disorders : \n        disorders = omim_terms ( case_obj ) \n    server_responses = [ ] \n    submitted_info = { 'contact' : contact_info , 'sex' : add_gender , 'features' : features , 'disorders' : disorders , 'genes_only' : genes_only , 'patient_id' : [ ] } \n    for individual in case_obj . get ( 'individuals' ) : \n        if not individual [ 'phenotype' ] in [ 2 , 'affected' ] : \n            continue \n        patient = { 'contact' : contact_info , 'id' : '.' . join ( [ case_obj [ '_id' ] , individual . get ( 'individual_id' ) ] ) , 'label' : '.' . join ( [ case_obj [ 'display_name' ] , individual . get ( 'display_name' ) ] ) , 'features' : features , 'disorders' : disorders } \n        if add_gender : \n            if individual [ 'sex' ] == '1' : \n                patient [ 'sex' ] = 'MALE' \n            else : \n                patient [ 'sex' ] = 'FEMALE' \n        if case_obj . get ( 'suspects' ) : \n            g_features = genomic_features ( store , case_obj , individual . get ( 'display_name' ) , genes_only ) \n            patient [ 'genomicFeatures' ] = g_features \n        resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' , content_type = mme_accepts , accept = 'application/json' , data = { 'patient' : patient } ) \n        server_responses . append ( { 'patient' : patient , 'message' : resp . get ( 'message' ) , 'status_code' : resp . get ( 'status_code' ) } ) \n    submitted_info [ 'server_responses' ] = server_responses \n    return submitted_info "}
{"7720": "\ndef mme_delete ( case_obj , mme_base_url , mme_token ) : \n    server_responses = [ ] \n    if not mme_base_url or not mme_token : \n        return_identifier_0 = 'Please check that Matchmaker connection parameters are valid' \n        return return_identifier_0 \n    for patient in case_obj [ 'mme_submission' ] [ 'patients' ] : \n        patient_id = patient [ 'id' ] \n        url = '' . join ( [ mme_base_url , '/patient/delete/' , patient_id ] ) \n        resp = matchmaker_request ( url = url , token = mme_token , method = 'DELETE' , ) \n        server_responses . append ( { 'patient_id' : patient_id , 'message' : resp . get ( 'message' ) , 'status_code' : resp . get ( 'status_code' ) } ) \n    return server_responses "}
{"7833": "\ndef isheavy ( self ) : \n    selection_size_bytes = self . _calc_selection_size ( ) \n    if selection_size_bytes > self . MAX_DATA_ARRAY_SIZE : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"7869": "\ndef to_dict ( self ) : \n    if self . tb_next is None : \n        tb_next = None \n    else : \n        tb_next = self . tb_next . to_dict ( ) \n    code = { 'co_filename' : self . tb_frame . f_code . co_filename , 'co_name' : self . tb_frame . f_code . co_name , } \n    frame = { 'f_globals' : self . tb_frame . f_globals , 'f_code' : code , } \n    return_identifier_0 = { 'tb_frame' : frame , 'tb_lineno' : self . tb_lineno , 'tb_next' : tb_next , } \n    return return_identifier_0 "}
{"7900": "\ndef escape_filter ( o ) : \n    if o is None : \n        return_identifier_0 = u'NULL' \n        return return_identifier_0 \n    if isinstance ( o , int ) : \n        return str ( o ) \n    if not isinstance ( o , six . string_types ) : \n        raise ValueError ( 'Filters take only None, int or a string type' ) \n    if not o : \n        return_identifier_1 = u\"''\" \n        return return_identifier_1 \n    o = unicode_process ( o ) \n    if u'\"' not in o : \n        return u'\"' + o + u'\"' \n    elif u\"'\" not in o : \n        return u\"'\" + o + u\"'\" \n    else : \n        first_char = o [ 0 ] \n        last_char = o [ - 1 ] \n        if first_char in QUOTES and last_char in QUOTES : \n            if first_char == last_char : \n                quote = give_another_quote ( first_char ) \n                return quote + o + quote \n            else : \n                return u\"'\" + o + u\"'\" \n        elif first_char not in QUOTES and last_char not in QUOTES : \n            return u\"'\" + o + u\"'\" \n        else : \n            if first_char in QUOTES : \n                quote = give_another_quote ( first_char ) \n            else : \n                quote = give_another_quote ( last_char ) \n            return quote + o + quote "}
{"7926": "\ndef serach_path ( ) : \n    operating_system = get_os ( ) \n    return_identifier_0 = [ os . path . expanduser ( \"~/.kerncraft/iaca/{}/\" . format ( operating_system ) ) , os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/iaca/{}/' . format ( operating_system ) ] \n    return return_identifier_0 "}
{"7958": "\ndef find_node_type ( ast , node_type ) : \n    if type ( ast ) is node_type : \n        return_identifier_0 = [ ast ] \n        return return_identifier_0 \n    elif type ( ast ) is list : \n        return reduce ( operator . add , list ( map ( lambda a : find_node_type ( a , node_type ) , ast ) ) , [ ] ) \n    elif ast is None : \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    else : \n        return reduce ( operator . add , [ find_node_type ( o [ 1 ] , node_type ) for o in ast . children ( ) ] , [ ] ) "}
{"7959": "\ndef force_iterable ( f ) : \n    def wrapper ( * args , ** kwargs ) : \n        r = f ( * args , ** kwargs ) \n        if hasattr ( r , '__iter__' ) : \n            return r \n        else : \n            return_identifier_0 = [ r ] \n            return return_identifier_0 \n    return wrapper "}
{"8016": "\ndef parse_description ( ) : \n    from os . path import dirname , join , exists \n    readme_fpath = join ( dirname ( __file__ ) , 'README.md' ) \n    if exists ( readme_fpath ) : \n        textlines = [ ] \n        with open ( readme_fpath , 'r' ) as f : \n            capture = False \n            for line in f . readlines ( ) : \n                if '# Purpose' in line : \n                    capture = True \n                elif line . startswith ( '##' ) : \n                    break \n                elif capture : \n                    textlines += [ line ] \n        text = '' . join ( textlines ) . strip ( ) \n        text = text . replace ( '\\n\\n' , '_NLHACK_' ) \n        text = text . replace ( '\\n' , ' ' ) \n        text = text . replace ( '_NLHACK_' , '\\n\\n' ) \n        return text \n    return_identifier_0 = '' \n    return return_identifier_0 "}
{"8019": "\ndef _record_purchase ( sailthru_client , email , item , purchase_incomplete , message_id , options ) : \n    try : \n        sailthru_response = sailthru_client . purchase ( email , [ item ] , incomplete = purchase_incomplete , message_id = message_id , options = options ) \n        if not sailthru_response . is_ok ( ) : \n            error = sailthru_response . get_error ( ) \n            logger . error ( \"Error attempting to record purchase in Sailthru: %s\" , error . get_message ( ) ) \n            return not can_retry_sailthru_request ( error ) \n    except SailthruClientError as exc : \n        logger . exception ( \"Exception attempting to record purchase for %s in Sailthru - %s\" , email , text_type ( exc ) ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"8021": "\ndef _get_course_content_from_ecommerce ( course_id , site_code = None ) : \n    api = get_ecommerce_client ( site_code = site_code ) \n    try : \n        api_response = api . courses ( course_id ) . get ( ) \n    except Exception : \n        logger . exception ( 'An error occurred while retrieving data for course run [%s] from the Catalog API.' , course_id , exc_info = True ) \n        return_identifier_0 = { } \n        return return_identifier_0 \n    return_identifier_1 = { 'title' : api_response . get ( 'name' ) , 'verification_deadline' : api_response . get ( 'verification_deadline' ) } \n    return return_identifier_1 "}
{"8022": "\ndef _update_unenrolled_list ( sailthru_client , email , course_url , unenroll ) : \n    try : \n        sailthru_response = sailthru_client . api_get ( \"user\" , { \"id\" : email , \"fields\" : { \"vars\" : 1 } } ) \n        if not sailthru_response . is_ok ( ) : \n            error = sailthru_response . get_error ( ) \n            logger . error ( \"Error attempting to read user record from Sailthru: %s\" , error . get_message ( ) ) \n            return not can_retry_sailthru_request ( error ) \n        response_json = sailthru_response . json \n        unenroll_list = [ ] \n        if response_json and \"vars\" in response_json and response_json [ \"vars\" ] and \"unenrolled\" in response_json [ \"vars\" ] : \n            unenroll_list = response_json [ \"vars\" ] [ \"unenrolled\" ] \n        changed = False \n        if unenroll : \n            if course_url not in unenroll_list : \n                unenroll_list . append ( course_url ) \n                changed = True \n        elif course_url in unenroll_list : \n            unenroll_list . remove ( course_url ) \n            changed = True \n        if changed : \n            sailthru_response = sailthru_client . api_post ( 'user' , { 'id' : email , 'key' : 'email' , 'vars' : { 'unenrolled' : unenroll_list } } ) \n            if not sailthru_response . is_ok ( ) : \n                error = sailthru_response . get_error ( ) \n                logger . error ( \"Error attempting to update user record in Sailthru: %s\" , error . get_message ( ) ) \n                return not can_retry_sailthru_request ( error ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except SailthruClientError as exc : \n        logger . exception ( \"Exception attempting to update user record for %s in Sailthru - %s\" , email , text_type ( exc ) ) \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"8054": "\ndef serialize ( self ) : \n    return_identifier_0 = { 'type' : 'event' , 'id' : self . uid , 'attributes' : { 'start' : self . start , 'end' : self . end , 'uid' : self . uid , 'title' : self . title , 'data' : self . get_data ( ) , 'status' : Status . str ( self . status ) } } \n    return return_identifier_0 "}
{"8063": "\ndef configuration_file ( cfgfile ) : \n    if cfgfile is not None : \n        return cfgfile \n    cfg = './etc/pyca.conf' \n    if not os . path . isfile ( cfg ) : \n        return_identifier_0 = '/etc/pyca.conf' \n        return return_identifier_0 \n    return cfg "}
{"8086": "\ndef to_dict ( self ) : \n    return_identifier_0 = { 'content' : self . content , 'resources' : [ r . _asdict ( ) for r in self . resources ] , 'js_init_fn' : self . js_init_fn , 'js_init_version' : self . js_init_version , 'json_init_args' : self . json_init_args } \n    return return_identifier_0 "}
{"8115": "\ndef delete_group ( self , group_id ) : \n    self . _valid_group_id ( group_id ) \n    url = \"{}/group/{}\" . format ( self . API , group_id ) \n    self . _delete_resource ( url ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"8117": "\ndef update_members ( self , group_id , members ) : \n    self . _valid_group_id ( group_id ) \n    body = { \"data\" : [ m . json_data ( ) for m in members ] } \n    headers = { \"If-Match\" : \"*\" } \n    url = \"{}/group/{}/member\" . format ( self . API , group_id ) \n    data = self . _put_resource ( url , headers , body ) \n    errors = data . get ( \"errors\" , [ ] ) \n    if len ( errors ) : \n        return errors [ 0 ] . get ( \"notFound\" , [ ] ) \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"8119": "\ndef is_effective_member ( self , group_id , netid ) : \n    self . _valid_group_id ( group_id ) \n    netid = re . sub ( '@washington.edu' , '' , netid ) \n    url = \"{}/group/{}/effective_member/{}\" . format ( self . API , group_id , netid ) \n    try : \n        data = self . _get_resource ( url ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except DataFailureException as ex : \n        if ex . status == 404 : \n            return_identifier_1 = False \n            return return_identifier_1 \n        else : \n            raise "}
{"8141": "\ndef delete_account ( self , headers = None , yes_i_mean_delete_the_account = False , query = None , cdn = False , body = None ) : \n    if not yes_i_mean_delete_the_account and ( not body or not query or 'bulk-delete' not in query ) : \n        return_identifier_0 = ( 0 , 'yes_i_mean_delete_the_account was not set to True' , { } , '' ) \n        return return_identifier_0 \n    return self . request ( 'DELETE' , '' , body or '' , headers , query = query , cdn = cdn ) "}
{"8165": "\ndef is_empty ( self ) : \n    something = self . read ( 1 ) \n    if something : \n        if self . buf : \n            self . buf = something + self . buf \n        else : \n            self . buf = something \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"8172": "\ndef render_hero_slider ( context ) : \n    req = context . get ( 'request' ) \n    qs = SliderItem . objects . published ( req ) . order_by ( 'position' ) \n    return_identifier_0 = { 'slider_items' : qs , } \n    return return_identifier_0 "}
{"8186": "\ndef create_index ( idx_url , clean = False ) : \n    try : \n        r = requests . get ( idx_url ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s)\" % idx_url \n        raise ElasticSearchError ( cause = cause ) \n    if r . status_code != 200 : \n        r = requests . put ( idx_url ) \n        if r . status_code != 200 : \n            logger . info ( \"Can't create index %s (%s)\" , idx_url , r . status_code ) \n            cause = \"Error creating Elastic Search index %s\" % idx_url \n            raise ElasticSearchError ( cause = cause ) \n        logger . info ( \"Index %s created\" , idx_url ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif r . status_code == 200 and clean : \n        requests . delete ( idx_url ) \n        requests . put ( idx_url ) \n        logger . info ( \"Index deleted and created (index: %s)\" , idx_url ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"8191": "\ndef remove_task ( self , task_id ) : \n    try : \n        self . _scheduler . cancel_task ( task_id ) \n    except NotFoundError as e : \n        logger . info ( \"Cannot cancel %s task because it does not exist.\" , task_id ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"8221": "\ndef parse_bool ( value ) : \n    boolean = parse_str ( value ) . capitalize ( ) \n    if boolean in ( \"True\" , \"Yes\" , \"On\" , \"1\" ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif boolean in ( \"False\" , \"No\" , \"Off\" , \"0\" ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    else : \n        raise ValueError ( 'Unable to parse boolean value \"{}\"' . format ( value ) ) "}
{"8250": "\ndef results_equal ( a , b ) : \n    if a . v_is_parameter and b . v_is_parameter : \n        raise ValueError ( 'Both inputs are not results.' ) \n    if a . v_is_parameter or b . v_is_parameter : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if a . v_full_name != b . v_full_name : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if hasattr ( a , '_data' ) and not hasattr ( b , '_data' ) : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if hasattr ( a , '_data' ) : \n        akeyset = set ( a . _data . keys ( ) ) \n        bkeyset = set ( b . _data . keys ( ) ) \n        if akeyset != bkeyset : \n            return_identifier_3 = False \n            return return_identifier_3 \n        for key in a . _data : \n            val = a . _data [ key ] \n            bval = b . _data [ key ] \n            if not nested_equal ( val , bval ) : \n                return_identifier_4 = False \n                return return_identifier_4 \n    return_identifier_5 = True \n    return return_identifier_5 "}
{"8251": "\ndef parameters_equal ( a , b ) : \n    if ( not b . v_is_parameter and not a . v_is_parameter ) : \n        raise ValueError ( 'Both inputs are not parameters' ) \n    if ( not b . v_is_parameter or not a . v_is_parameter ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if a . v_full_name != b . v_full_name : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if a . f_is_empty ( ) and b . f_is_empty ( ) : \n        return_identifier_2 = True \n        return return_identifier_2 \n    if a . f_is_empty ( ) != b . f_is_empty ( ) : \n        return_identifier_3 = False \n        return return_identifier_3 \n    if not a . _values_of_same_type ( a . f_get ( ) , b . f_get ( ) ) : \n        return_identifier_4 = False \n        return return_identifier_4 \n    if not a . _equal_values ( a . f_get ( ) , b . f_get ( ) ) : \n        return_identifier_5 = False \n        return return_identifier_5 \n    if a . f_has_range ( ) != b . f_has_range ( ) : \n        return_identifier_6 = False \n        return return_identifier_6 \n    if a . f_has_range ( ) : \n        if a . f_get_range_length ( ) != b . f_get_range_length ( ) : \n            return_identifier_7 = False \n            return return_identifier_7 \n        for myitem , bitem in zip ( a . f_get_range ( copy = False ) , b . f_get_range ( copy = False ) ) : \n            if not a . _values_of_same_type ( myitem , bitem ) : \n                return_identifier_8 = False \n                return return_identifier_8 \n            if not a . _equal_values ( myitem , bitem ) : \n                return_identifier_9 = False \n                return return_identifier_9 \n    return_identifier_10 = True \n    return return_identifier_10 "}
{"8276": "\ndef _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : \n    assert ( end_time >= start_time + time_window ) \n    bins = ( end_time - start_time ) / time_window \n    bins = int ( np . floor ( bins ) ) \n    binned_spikes = np . zeros ( bins ) \n    spike_array_neuron = spike_res . t [ spike_res . i == neuron_id ] \n    for bin in range ( bins ) : \n        lower_time = start_time + time_window * bin \n        upper_time = start_time + time_window * ( bin + 1 ) \n        spike_array_interval = spike_array_neuron [ spike_array_neuron >= lower_time ] \n        spike_array_interval = spike_array_interval [ spike_array_interval < upper_time ] \n        spikes = len ( spike_array_interval ) \n        binned_spikes [ bin ] = spikes \n    var = np . var ( binned_spikes ) \n    avg = np . mean ( binned_spikes ) \n    if avg > 0 : \n        return var / float ( avg ) \n    else : \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"8292": "\ndef _remove_subtree ( self , start_node , name , predicate = None ) : \n    def _delete_from_children ( node , child_name ) : \n        del node . _children [ child_name ] \n        if child_name in node . _groups : \n            del node . _groups [ child_name ] \n        elif child_name in node . _leaves : \n            del node . _leaves [ child_name ] \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    def _remove_subtree_inner ( node , predicate ) : \n        if not predicate ( node ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        elif node . v_is_group : \n            for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , list ( node . _groups . keys ( ) ) ) : \n                child_ = node . _children [ name_ ] \n                child_deleted = _remove_subtree_inner ( child_ , predicate ) \n                if child_deleted : \n                    _delete_from_children ( node , name_ ) \n                    del child_ \n            for link_ in list ( node . _links . keys ( ) ) : \n                node . f_remove_link ( link_ ) \n            if len ( node . _children ) == 0 : \n                self . _delete_node ( node ) \n                return_identifier_1 = True \n                return return_identifier_1 \n            else : \n                return_identifier_2 = False \n                return return_identifier_2 \n        else : \n            self . _delete_node ( node ) \n            return_identifier_3 = True \n            return return_identifier_3 \n    if name in start_node . _links : \n        start_node . f_remove_link ( name ) \n    else : \n        child = start_node . _children [ name ] \n        if predicate is None : \n            predicate = lambda x : True \n        if _remove_subtree_inner ( child , predicate ) : \n            _delete_from_children ( start_node , name ) \n            del child \n            return_identifier_4 = True \n            return return_identifier_4 \n        else : \n            return_identifier_5 = False \n            return return_identifier_5 "}
{"8295": "\ndef _remove_along_branch ( self , actual_node , split_name , recursive = False ) : \n    if len ( split_name ) == 0 : \n        if actual_node . v_is_group and actual_node . f_has_children ( ) : \n            if recursive : \n                for child in list ( actual_node . _children . keys ( ) ) : \n                    actual_node . f_remove_child ( child , recursive = True ) \n            else : \n                raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) \n        self . _delete_node ( actual_node ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    name = split_name . popleft ( ) \n    if name in actual_node . _links : \n        if len ( split_name ) > 0 : \n            raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) \n        actual_node . f_remove_link ( name ) \n    else : \n        child = actual_node . _children [ name ] \n        if self . _remove_along_branch ( child , split_name , recursive = recursive ) : \n            del actual_node . _children [ name ] \n            if name in actual_node . _groups : \n                del actual_node . _groups [ name ] \n            elif name in actual_node . _leaves : \n                del actual_node . _leaves [ name ] \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n            del child \n            return_identifier_1 = False \n            return return_identifier_1 "}
{"8324": "\ndef f_contains ( self , item , with_links = True , shortcuts = False , max_depth = None ) : \n    try : \n        search_string = item . v_full_name \n        parent_full_name = self . v_full_name \n        if not search_string . startswith ( parent_full_name ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if parent_full_name != '' : \n            search_string = search_string [ len ( parent_full_name ) + 1 : ] \n        else : \n            search_string = search_string \n        shortcuts = False \n    except AttributeError : \n        search_string = item \n        item = None \n    if search_string == '' : \n        return_identifier_1 = False \n        return return_identifier_1 \n    try : \n        result = self . f_get ( search_string , shortcuts = shortcuts , max_depth = max_depth , with_links = with_links ) \n    except AttributeError : \n        return_identifier_2 = False \n        return return_identifier_2 \n    if item is not None : \n        return id ( item ) == id ( result ) \n    else : \n        return_identifier_3 = True \n        return return_identifier_3 "}
{"8342": "\ndef eval_one_max ( traj , individual ) : \n    traj . f_add_result ( '$set.$.individual' , list ( individual ) ) \n    fitness = sum ( individual ) \n    traj . f_add_result ( '$set.$.fitness' , fitness ) \n    traj . f_store ( ) \n    return_identifier_0 = ( fitness , ) \n    return return_identifier_0 "}
{"8371": "\ndef acquire ( self ) : \n    self . start ( test_connection = False ) \n    while True : \n        str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) \n        response = str_response . split ( LockerServer . DELIMITER ) \n        if response [ 0 ] == LockerServer . GO : \n            return_identifier_0 = True \n            return return_identifier_0 \n        elif response [ 0 ] == LockerServer . LOCK_ERROR and retries > 0 : \n            self . _logger . error ( str_response + '; Probably due to retry' ) \n            return_identifier_1 = True \n            return return_identifier_1 \n        elif response [ 0 ] == LockerServer . WAIT : \n            time . sleep ( self . SLEEP ) \n        else : \n            raise RuntimeError ( 'Response `%s` not understood' % response ) "}
{"8399": "\ndef _srvc_closing_routine ( self , closing ) : \n    if ( not self . _keep_open and closing and self . is_open ) : \n        f_fd = self . _hdf5file . fileno ( ) \n        self . _hdf5file . flush ( ) \n        try : \n            os . fsync ( f_fd ) \n            try : \n                self . _hdf5store . flush ( fsync = True ) \n            except TypeError : \n                f_fd = self . _hdf5store . _handle . fileno ( ) \n                self . _hdf5store . flush ( ) \n                os . fsync ( f_fd ) \n        except OSError as exc : \n            errmsg = ( 'Encountered OSError while flushing file.' 'If you are using Windows, don`t worry! ' 'I will ignore the error and try to close the file. ' 'Original error: %s' % repr ( exc ) ) \n            self . _logger . debug ( errmsg ) \n        self . _hdf5store . close ( ) \n        if self . _hdf5file . isopen : \n            self . _logger . error ( 'Could not close HDF5 file!' ) \n        self . _hdf5file = None \n        self . _hdf5store = None \n        self . _trajectory_group = None \n        self . _trajectory_name = None \n        self . _trajectory_index = None \n        self . _overview_group_ = None \n        self . _logger . debug ( 'Closing HDF5 file' ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"8433": "\ndef _prm_meta_add_summary ( self , instance ) : \n    if instance . v_comment == '' : \n        return_identifier_0 = False \n        return return_identifier_0 \n    where = instance . v_branch \n    definitely_store_comment = True \n    bytes_comment = instance . v_comment . encode ( 'utf-8' ) \n    hexdigest = hashlib . sha1 ( bytes_comment ) . hexdigest ( ) \n    hexdigest = hexdigest . encode ( 'utf-8' ) \n    table_name = where + '_summary' \n    if table_name in self . _overview_group : \n        table = getattr ( self . _overview_group , table_name ) \n    else : \n        return definitely_store_comment \n    try : \n        condvars = { 'hexdigestcol' : table . cols . hexdigest , 'hexdigest' : hexdigest } \n        condition = \"\"\"(hexdigestcol == hexdigest)\"\"\" \n        row_iterator = table . where ( condition , condvars = condvars ) \n        row = None \n        try : \n            row = next ( row_iterator ) \n        except StopIteration : \n            pass \n        if row is None : \n            self . _all_store_param_or_result_table_entry ( instance , table , flags = ( HDF5StorageService . ADD_ROW , ) , additional_info = { 'hexdigest' : hexdigest } ) \n            definitely_store_comment = True \n        else : \n            definitely_store_comment = False \n            self . _all_kill_iterator ( row_iterator ) \n    except pt . NoSuchNodeError : \n        definitely_store_comment = True \n    return definitely_store_comment "}
{"8524": "\ndef f_val_to_str ( self ) : \n    old_locked = self . _locked \n    try : \n        return repr ( self . f_get ( ) ) \n    except Exception : \n        return_identifier_0 = 'No Evaluation possible (yet)!' \n        return return_identifier_0 \n    finally : \n        self . _locked = old_locked "}
{"8525": "\ndef _equal_values ( self , val1 , val2 ) : \n    if self . f_supports ( val1 ) != self . f_supports ( val2 ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not self . f_supports ( val1 ) and not self . f_supports ( val2 ) : \n        raise TypeError ( 'I do not support the types of both inputs (`%s` and `%s`), ' 'therefore I cannot judge whether ' 'the two are equal.' % ( str ( type ( val1 ) ) , str ( type ( val2 ) ) ) ) \n    if not self . _values_of_same_type ( val1 , val2 ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return comparisons . nested_equal ( val1 , val2 ) "}
{"8533": "\ndef _equal_values ( self , val1 , val2 ) : \n    if self . _is_supported_matrix ( val1 ) : \n        if self . _is_supported_matrix ( val2 ) : \n            _ , _ , hash_tuple_1 = self . _serialize_matrix ( val1 ) \n            _ , _ , hash_tuple_2 = self . _serialize_matrix ( val2 ) \n            return hash ( hash_tuple_1 ) == hash ( hash_tuple_2 ) \n        else : \n            return_identifier_0 = False \n            return return_identifier_0 \n    else : \n        return super ( SparseParameter , self ) . _equal_values ( val1 , val2 ) "}
{"8547": "\ndef _supports ( self , item ) : \n    if SparseParameter . _is_supported_matrix ( item ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return super ( SparseResult , self ) . _supports ( item ) "}
{"8576": "\ndef config_from_file ( filename , config = None ) : \n    if config : \n        try : \n            with open ( filename , 'w' ) as fdesc : \n                fdesc . write ( json . dumps ( config ) ) \n        except IOError as error : \n            logger . exception ( error ) \n            return_identifier_0 = False \n            return return_identifier_0 \n        return_identifier_1 = True \n        return return_identifier_1 \n    else : \n        if os . path . isfile ( filename ) : \n            try : \n                with open ( filename , 'r' ) as fdesc : \n                    return json . loads ( fdesc . read ( ) ) \n            except IOError as error : \n                return_identifier_2 = False \n                return return_identifier_2 \n        else : \n            return_identifier_3 = { } \n            return return_identifier_3 "}
{"8579": "\ndef refresh_tokens ( self ) : \n    url = 'https://api.ecobee.com/token' \n    params = { 'grant_type' : 'refresh_token' , 'refresh_token' : self . refresh_token , 'client_id' : self . api_key } \n    request = requests . post ( url , params = params ) \n    if request . status_code == requests . codes . ok : \n        self . access_token = request . json ( ) [ 'access_token' ] \n        self . refresh_token = request . json ( ) [ 'refresh_token' ] \n        self . write_tokens_to_file ( ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        self . request_pin ( ) "}
{"8602": "\ndef process_received_ack ( self , pkt ) : \n    if isack ( pkt ) : \n        try : \n            self . event = self . client . handle_ack ( pkt , self . time_sent_request ) \n        except AddrFormatError as err : \n            logger . error ( err ) \n            raise self . SELECTING ( ) \n        logger . info ( 'DHCPACK of %s from %s' % ( self . client . client_ip , self . client . server_ip ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"8603": "\ndef process_received_nak ( self , pkt ) : \n    if isnak ( pkt ) : \n        logger . info ( 'DHCPNAK of %s from %s' , self . client . client_ip , self . client . server_ip ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"8632": "\ndef equal ( self , cwd ) : \n    cmd = [ \"diff\" ] \n    cmd . append ( \"-q\" ) \n    cmd . append ( self . left . get_name ( ) ) \n    cmd . append ( self . right . get_name ( ) ) \n    try : \n        Process ( cmd ) . run ( cwd = cwd , suppress_output = True ) \n    except SubprocessError as e : \n        if e . get_returncode ( ) == 1 : \n            return_identifier_0 = False \n            return return_identifier_0 \n        else : \n            raise e \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"8636": "\ndef _file_in_patch ( self , filename , patch , ignore ) : \n    file = self . quilt_pc + File ( os . path . join ( patch . get_name ( ) , filename ) ) \n    if file . exists ( ) : \n        if ignore : \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            raise QuiltError ( \"File %s is already in patch %s\" % ( filename , patch . get_name ( ) ) ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"8694": "\ndef register ( device , data , facet ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if data [ 'version' ] != VERSION : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    client_data = { 'typ' : 'navigator.id.finishEnrollment' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param \n    p1 = 0x03 \n    p2 = 0 \n    response = device . send_apdu ( INS_ENROLL , p1 , p2 , request ) \n    return_identifier_0 = { 'registrationData' : websafe_encode ( response ) , 'clientData' : websafe_encode ( client_data ) } \n    return return_identifier_0 "}
{"8695": "\ndef authenticate ( device , data , facet , check_only = False ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if data [ 'version' ] != VERSION : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    key_handle = websafe_decode ( data [ 'keyHandle' ] ) \n    client_data = { 'typ' : 'navigator.id.getAssertion' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param + int2byte ( len ( key_handle ) ) + key_handle \n    p1 = 0x07 if check_only else 0x03 \n    p2 = 0 \n    response = device . send_apdu ( INS_SIGN , p1 , p2 , request ) \n    return_identifier_0 = { 'clientData' : websafe_encode ( client_data ) , 'signatureData' : websafe_encode ( response ) , 'keyHandle' : data [ 'keyHandle' ] } \n    return return_identifier_0 "}
{"8700": "\ndef _matches_filepath_pattern ( self , filepath ) : \n    if not self . only_blame_patterns : \n        return_identifier_0 = True \n        return return_identifier_0 \n    for pattern in self . only_blame_patterns : \n        if pattern . match ( filepath ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"8713": "\ndef parse_totals ( self , item , field_name , source_name ) : \n    val = self . get_value ( item , source_name ) \n    try : \n        return int ( val ) \n    except : \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"8759": "\ndef _delete_external_tool ( self , context , context_id , external_tool_id ) : \n    url = context . format ( context_id ) + \"/external_tools/{}\" . format ( external_tool_id ) \n    response = self . _delete_resource ( url ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"8777": "\ndef delete_admin ( self , account_id , user_id , role ) : \n    url = ADMINS_API . format ( account_id ) + \"/{}?role={}\" . format ( user_id , quote ( role ) ) \n    response = self . _delete_resource ( url ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"8810": "\ndef delete_report ( self , report ) : \n    url = ACCOUNTS_API . format ( report . account_id ) + \"/reports/{}/{}\" . format ( report . type , report . report_id ) \n    response = self . _delete_resource ( url ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"8853": "\ndef get_config ( app , prefix = 'hive_' ) : \n    items = app . config . items ( ) \n    prefix = prefix . upper ( ) \n    def strip_prefix ( tup ) : \n        return_identifier_0 = ( tup [ 0 ] . replace ( prefix , '' ) , tup [ 1 ] ) \n        return return_identifier_0 \n    return dict ( [ strip_prefix ( i ) for i in items if i [ 0 ] . startswith ( prefix ) ] ) "}
{"8878": "\ndef map_lrepr ( entries : Callable [ [ ] , Iterable [ Tuple [ Any , Any ] ] ] , start : str , end : str , meta = None , ** kwargs , ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and print_level < 1 : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    def entry_reprs ( ) : \n        for k , v in entries ( ) : \n            yield \"{k} {v}\" . format ( k = lrepr ( k , ** kwargs ) , v = lrepr ( v , ** kwargs ) ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( entry_reprs ( ) ) . take ( print_length + 1 ) . to_list ( ) \n        if len ( items ) > print_length : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = list ( entry_reprs ( ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return_identifier_0 = f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n        return return_identifier_0 \n    return_identifier_1 = f\"{start}{seq_lrepr}{end}\" \n    return return_identifier_1 "}
{"8879": "\ndef seq_lrepr ( iterable : Iterable [ Any ] , start : str , end : str , meta = None , ** kwargs ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and print_level < 1 : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( iterable ) . take ( print_length + 1 ) . to_list ( ) \n        if len ( items ) > print_length : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = iterable \n    items = list ( map ( lambda o : lrepr ( o , ** kwargs ) , items ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return_identifier_0 = f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n        return return_identifier_0 \n    return_identifier_1 = f\"{start}{seq_lrepr}{end}\" \n    return return_identifier_1 "}
{"8888": "\ndef munge ( s : str , allow_builtins : bool = False ) -> str : \n    new_str = [ ] \n    for c in s : \n        new_str . append ( _MUNGE_REPLACEMENTS . get ( c , c ) ) \n    new_s = \"\" . join ( new_str ) \n    if keyword . iskeyword ( new_s ) : \n        return_identifier_0 = f\"{new_s}_\" \n        return return_identifier_0 \n    if not allow_builtins and new_s in builtins . __dict__ : \n        return_identifier_1 = f\"{new_s}_\" \n        return return_identifier_1 \n    return new_s "}
{"8902": "\ndef _read_sym ( ctx : ReaderContext ) -> MaybeSymbol : \n    ns , name = _read_namespaced ( ctx , allowed_suffix = \"#\" ) \n    if not ctx . is_syntax_quoted and name . endswith ( \"#\" ) : \n        raise SyntaxError ( \"Gensym may not appear outside syntax quote\" ) \n    if ns is not None : \n        if any ( map ( lambda s : len ( s ) == 0 , ns . split ( \".\" ) ) ) : \n            raise SyntaxError ( \"All '.' separated segments of a namespace \" \"must contain at least one character.\" ) \n    if name . startswith ( \".\" ) and ns is not None : \n        raise SyntaxError ( \"Symbols starting with '.' may not have a namespace\" ) \n    if ns is None : \n        if name == \"nil\" : \n            return None \n        elif name == \"true\" : \n            return_identifier_0 = True \n            return return_identifier_0 \n        elif name == \"false\" : \n            return_identifier_1 = False \n            return return_identifier_1 \n    if ctx . is_syntax_quoted and not name . endswith ( \"#\" ) : \n        return ctx . resolve ( symbol . symbol ( name , ns ) ) \n    return symbol . symbol ( name , ns = ns ) "}
{"8905": "\ndef _read_function ( ctx : ReaderContext ) -> llist . List : \n    if ctx . is_in_anon_fn : \n        raise SyntaxError ( f\"Nested #() definitions not allowed\" ) \n    with ctx . in_anon_fn ( ) : \n        form = _read_list ( ctx ) \n    arg_set = set ( ) \n    def arg_suffix ( arg_num ) : \n        if arg_num is None : \n            return_identifier_0 = \"1\" \n            return return_identifier_0 \n        elif arg_num == \"&\" : \n            return_identifier_1 = \"rest\" \n            return return_identifier_1 \n        else : \n            return arg_num \n    def sym_replacement ( arg_num ) : \n        suffix = arg_suffix ( arg_num ) \n        return symbol . symbol ( f\"arg-{suffix}\" ) \n    def identify_and_replace ( f ) : \n        if isinstance ( f , symbol . Symbol ) : \n            if f . ns is None : \n                match = fn_macro_args . match ( f . name ) \n                if match is not None : \n                    arg_num = match . group ( 2 ) \n                    suffix = arg_suffix ( arg_num ) \n                    arg_set . add ( suffix ) \n                    return sym_replacement ( arg_num ) \n        return f \n    body = walk . postwalk ( identify_and_replace , form ) if len ( form ) > 0 else None \n    arg_list : List [ symbol . Symbol ] = [ ] \n    numbered_args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg_set ) ) ) \n    if len ( numbered_args ) > 0 : \n        max_arg = max ( numbered_args ) \n        arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] \n        if \"rest\" in arg_set : \n            arg_list . append ( _AMPERSAND ) \n            arg_list . append ( sym_replacement ( \"rest\" ) ) \n    return llist . l ( _FN , vector . vector ( arg_list ) , body ) "}
{"8946": "\ndef __should_warn_on_redef ( ctx : GeneratorContext , defsym : sym . Symbol , safe_name : str , def_meta : lmap . Map ) -> bool : \n    no_warn_on_redef = def_meta . entry ( SYM_NO_WARN_ON_REDEF_META_KEY , False ) \n    if no_warn_on_redef : \n        return_identifier_0 = False \n        return return_identifier_0 \n    elif safe_name in ctx . current_ns . module . __dict__ : \n        return_identifier_1 = True \n        return return_identifier_1 \n    elif defsym in ctx . current_ns . interns : \n        var = ctx . current_ns . find ( defsym ) \n        assert var is not None , f\"Var {defsym} cannot be none here\" \n        if var . meta is not None and var . meta . entry ( SYM_REDEF_META_KEY ) : \n            return_identifier_2 = False \n            return return_identifier_2 \n        elif var . is_bound : \n            return_identifier_3 = True \n            return return_identifier_3 \n        else : \n            return_identifier_4 = False \n            return return_identifier_4 \n    else : \n        return_identifier_5 = False \n        return return_identifier_5 "}
{"9036": "\ndef args ( self ) -> Tuple : \n    if not self . _has_varargs : \n        return self . _args \n    try : \n        final = self . _args [ - 1 ] \n        if isinstance ( final , ISeq ) : \n            inits = self . _args [ : - 1 ] \n            return tuple ( itertools . chain ( inits , final ) ) \n        return self . _args \n    except IndexError : \n        return_identifier_0 = ( ) \n        return return_identifier_0 "}
{"9043": "\ndef to_python ( self ) : \n    return_identifier_0 = ( self . selector , COMPARISON_MAP . get ( self . comparison , self . comparison ) , self . argument ) \n    return return_identifier_0 "}
{"9076": "\ndef _homogenize_waves ( wave_a , wave_b ) : \n    indep_vector = _get_indep_vector ( wave_a , wave_b ) \n    dep_vector_a = _interp_dep_vector ( wave_a , indep_vector ) \n    dep_vector_b = _interp_dep_vector ( wave_b , indep_vector ) \n    return_identifier_0 = ( indep_vector , dep_vector_a , dep_vector_b ) \n    return return_identifier_0 "}
{"9109": "\ndef _build_units ( indep_units , dep_units , op ) : \n    if ( not dep_units ) and ( not indep_units ) : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    if dep_units and ( not indep_units ) : \n        return dep_units \n    if ( not dep_units ) and indep_units : \n        return ( remove_extra_delims ( \"1{0}({1})\" . format ( op , indep_units ) ) if op == \"/\" else remove_extra_delims ( \"({0})\" . format ( indep_units ) ) ) \n    return remove_extra_delims ( \"({0}){1}({2})\" . format ( dep_units , op , indep_units ) ) "}
{"9144": "\ndef find ( self , path , all = False ) : \n    bits = path . split ( '/' ) \n    dirs_to_serve = [ 'jspm_packages' , settings . SYSTEMJS_OUTPUT_DIR ] \n    if not bits or bits [ 0 ] not in dirs_to_serve : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    return super ( SystemFinder , self ) . find ( path , all = all ) "}
{"9145": "\ndef get_short_desc ( long_desc ) : \n    found = False \n    olines = [ ] \n    for line in [ item . rstrip ( ) for item in long_desc . split ( \"\\n\" ) ] : \n        if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : \n            olines . append ( line ) \n        elif found and olines and ( not line ) : \n            return ( \" \" . join ( olines ) . split ( \".\" ) [ 0 ] ) . strip ( ) \n        found = line == \".. [[[end]]]\" if not found else found \n    return_identifier_0 = \"\" \n    return return_identifier_0 "}
{"9157": "\ndef peng_frac ( snum ) : \n    snum = snum . rstrip ( ) \n    pindex = snum . find ( \".\" ) \n    if pindex == - 1 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    return int ( snum [ pindex + 1 : ] if snum [ - 1 ] . isdigit ( ) else snum [ pindex + 1 : - 1 ] ) "}
{"9163": "\ndef to_scientific_tuple ( number ) : \n    convert = not isinstance ( number , str ) \n    if ( convert and ( number == 0 ) ) or ( ( not convert ) and ( not number . strip ( \"0\" ) . strip ( \".\" ) ) ) : \n        return_identifier_0 = ( \"0\" , 0 ) \n        return return_identifier_0 \n    sign , digits , exp = Decimal ( str ( number ) if convert else number ) . as_tuple ( ) \n    mant = ( \"{sign}{itg}{frac}\" . format ( sign = \"-\" if sign else \"\" , itg = digits [ 0 ] , frac = ( \".{frac}\" . format ( frac = \"\" . join ( [ str ( num ) for num in digits [ 1 : ] ] ) ) if len ( digits ) > 1 else \"\" ) , ) . rstrip ( \"0\" ) . rstrip ( \".\" ) ) \n    exp += len ( digits ) - 1 \n    return NumComp ( mant , exp ) "}
{"9165": "\ndef needs_ext ( self ) : \n    if settings . SYSTEMJS_DEFAULT_JS_EXTENSIONS : \n        name , ext = posixpath . splitext ( self . app ) \n        if not ext : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"9168": "\ndef hashes_match ( self , dep_tree ) : \n    hashes = self . get_hashes ( ) \n    for module , info in dep_tree . items ( ) : \n        md5 = self . get_hash ( info [ 'path' ] ) \n        if md5 != hashes [ info [ 'path' ] ] : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"9178": "\ndef _is_flag ( cls , arg ) : \n    if arg == '--' : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not arg . startswith ( '-' ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if arg . startswith ( '--' ) : \n        first_char = arg [ 2 ] \n    else : \n        first_char = arg [ 1 ] \n    if not first_char . isalpha ( ) : \n        return_identifier_2 = False \n        return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"9183": "\ndef invoke_string ( self , line ) : \n    line = str ( line ) \n    if len ( line ) == 0 : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if line [ 0 ] == u'#' : \n        return_identifier_1 = True \n        return return_identifier_1 \n    args = self . _split_line ( line ) \n    return self . invoke ( args ) "}
{"9192": "\ndef get_type_size ( self , type ) : \n    typeobj = self . get_type ( type ) \n    if hasattr ( typeobj , 'size' ) : \n        return typeobj . size ( ) \n    return_identifier_0 = 0 \n    return return_identifier_0 "}
{"9195": "\ndef is_known_type ( self , type_name ) : \n    type_name = str ( type_name ) \n    if type_name in self . known_types : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"9199": "\ndef is_known_format ( self , type , format ) : \n    typeobj = self . get_type ( type ) \n    formatter = \"format_%s\" % str ( format ) \n    if not hasattr ( typeobj , formatter ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"9226": "\ndef short_description ( func ) : \n    doc = inspect . getdoc ( func ) \n    if doc is not None : \n        doc = inspect . cleandoc ( doc ) \n        lines = doc . splitlines ( ) \n        return lines [ 0 ] \n    return_identifier_0 = \"\" \n    return return_identifier_0 "}
{"9282": "\ndef is_hash256 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE ) "}
{"9283": "\ndef is_hash160 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not len ( s ) == 40 : \n        return_identifier_1 = False \n        return return_identifier_1 \n    for c in s : \n        if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : \n            return_identifier_2 = False \n            return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"9367": "\ndef IsEnabled ( self , * args , ** kwargs ) : \n    for i in range ( self . GetMenuItemCount ( ) ) : \n        it = self . FindItemByPosition ( i ) \n        if not it . IsEnabled ( ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"9369": "\ndef IsEnabled ( self , * args , ** kwargs ) : \n    for i in range ( self . GetMenuCount ( ) ) : \n        if not self . IsEnabledTop ( i ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"9383": "\ndef find ( default = '' , whole_words = 0 , case_sensitive = 0 , parent = None ) : \n    result = dialogs . findDialog ( parent , default , whole_words , case_sensitive ) \n    return_identifier_0 = { 'text' : result . searchText , 'whole_words' : result . wholeWordsOnly , 'case_sensitive' : result . caseSensitive } \n    return return_identifier_0 "}
{"9403": "\ndef assert_current_path ( self , path , ** kwargs ) : \n    query = CurrentPathQuery ( path , ** kwargs ) \n    \n    @ self . document . synchronize \n    def assert_current_path ( ) : \n        if not query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . failure_message ) \n    assert_current_path ( ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"9404": "\ndef assert_no_current_path ( self , path , ** kwargs ) : \n    query = CurrentPathQuery ( path , ** kwargs ) \n    \n    @ self . document . synchronize \n    def assert_no_current_path ( ) : \n        if query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n    assert_no_current_path ( ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"9405": "\ndef has_current_path ( self , path , ** kwargs ) : \n    try : \n        return self . assert_current_path ( path , ** kwargs ) \n    except ExpectationNotMet : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"9406": "\ndef has_no_current_path ( self , path , ** kwargs ) : \n    try : \n        return self . assert_no_current_path ( path , ** kwargs ) \n    except ExpectationNotMet : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"9411": "\ndef matches_filters ( self , node ) : \n    visible = self . visible \n    if self . options [ \"text\" ] : \n        if isregex ( self . options [ \"text\" ] ) : \n            regex = self . options [ \"text\" ] \n        elif self . exact_text is True : \n            regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . options [ \"text\" ] ) ) ) \n        else : \n            regex = toregex ( self . options [ \"text\" ] ) \n        text = normalize_text ( node . all_text if visible == \"all\" else node . visible_text ) \n        if not regex . search ( text ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    if isinstance ( self . exact_text , ( bytes_ , str_ ) ) : \n        regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . exact_text ) ) ) \n        text = normalize_text ( node . all_text if visible == \"all\" else node . visible_text ) \n        if not regex . search ( text ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n    if visible == \"visible\" : \n        if not node . visible : \n            return_identifier_2 = False \n            return return_identifier_2 \n    elif visible == \"hidden\" : \n        if node . visible : \n            return_identifier_3 = False \n            return return_identifier_3 \n    for name , node_filter in iter ( self . _node_filters . items ( ) ) : \n        if name in self . filter_options : \n            if not node_filter . matches ( node , self . filter_options [ name ] ) : \n                return_identifier_4 = False \n                return return_identifier_4 \n        elif node_filter . has_default : \n            if not node_filter . matches ( node , node_filter . default ) : \n                return_identifier_5 = False \n                return return_identifier_5 \n    if self . options [ \"filter\" ] and not self . options [ \"filter\" ] ( node ) : \n        return_identifier_6 = False \n        return return_identifier_6 \n    return_identifier_7 = True \n    return return_identifier_7 "}
{"9421": "\ndef matches ( self , node , value ) : \n    if self . skip ( value ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if not self . _valid_value ( value ) : \n        msg = \"Invalid value {value} passed to filter {name} - \" . format ( value = repr ( value ) , name = self . name ) \n        if self . default is not None : \n            warn ( msg + \"defaulting to {}\" . format ( self . default ) ) \n            value = self . default \n        else : \n            warn ( msg + \"skipping\" ) \n            return_identifier_1 = True \n            return return_identifier_1 \n    return self . func ( node , value ) "}
{"9426": "\ndef assert_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_text ( ) : \n        count = query . resolve_for ( self ) \n        if not ( matches_count ( count , query . options ) and ( count > 0 or expects_none ( query . options ) ) ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return assert_text ( ) "}
{"9427": "\ndef assert_no_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_text ( ) : \n        count = query . resolve_for ( self ) \n        if matches_count ( count , query . options ) and ( count > 0 or expects_none ( query . options ) ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return assert_no_text ( ) "}
{"9428": "\ndef assert_title ( self , title , ** kwargs ) : \n    query = TitleQuery ( title , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_title ( ) : \n        if not query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return assert_title ( ) "}
{"9429": "\ndef assert_no_title ( self , title , ** kwargs ) : \n    query = TitleQuery ( title , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_title ( ) : \n        if query . resolves_for ( self ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return assert_no_title ( ) "}
{"9430": "\ndef has_title ( self , title , ** kwargs ) : \n    try : \n        self . assert_title ( title , ** kwargs ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except ExpectationNotMet : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"9431": "\ndef has_no_title ( self , title , ** kwargs ) : \n    try : \n        self . assert_no_title ( title , ** kwargs ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except ExpectationNotMet : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"9440": "\ndef compare_count ( self ) : \n    if self . query . options [ \"count\" ] is not None : \n        count_opt = int ( self . query . options [ \"count\" ] ) \n        self . _cache_at_least ( count_opt + 1 ) \n        return cmp ( len ( self . _result_cache ) , count_opt ) \n    if self . query . options [ \"minimum\" ] is not None : \n        min_opt = int ( self . query . options [ \"minimum\" ] ) \n        if not self . _cache_at_least ( min_opt ) : \n            return - 1 \n    if self . query . options [ \"maximum\" ] is not None : \n        max_opt = int ( self . query . options [ \"maximum\" ] ) \n        if self . _cache_at_least ( max_opt + 1 ) : \n            return_identifier_0 = 1 \n            return return_identifier_0 \n    if self . query . options [ \"between\" ] is not None : \n        between = self . query . options [ \"between\" ] \n        min_opt , max_opt = between [ 0 ] , between [ - 1 ] \n        if not self . _cache_at_least ( min_opt ) : \n            return - 1 \n        if self . _cache_at_least ( max_opt + 1 ) : \n            return_identifier_1 = 1 \n            return return_identifier_1 \n        return_identifier_2 = 0 \n        return return_identifier_2 \n    return_identifier_3 = 0 \n    return return_identifier_3 "}
{"9441": "\ndef _cache_at_least ( self , size ) : \n    try : \n        while len ( self . _result_cache ) < size : \n            self . _result_cache . append ( next ( self . _result_iter ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except StopIteration : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"9442": "\ndef expects_none ( options ) : \n    if any ( options . get ( key ) is not None for key in [ \"count\" , \"maximum\" , \"minimum\" , \"between\" ] ) : \n        return matches_count ( 0 , options ) \n    else : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"9444": "\ndef matches_count ( count , options ) : \n    if options . get ( \"count\" ) is not None : \n        return count == int ( options [ \"count\" ] ) \n    if options . get ( \"maximum\" ) is not None and int ( options [ \"maximum\" ] ) < count : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if options . get ( \"minimum\" ) is not None and int ( options [ \"minimum\" ] ) > count : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if options . get ( \"between\" ) is not None and count not in options [ \"between\" ] : \n        return_identifier_2 = False \n        return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"9445": "\ndef normalize_text ( value ) : \n    if value is None : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    text = decode_bytes ( value ) if isbytes ( value ) else str_ ( value ) \n    return normalize_whitespace ( text ) "}
{"9454": "\ndef __traceback ( self ) -> str : \n    if not self . log_traceback : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    exc_info = sys . exc_info ( ) \n    stack = traceback . extract_stack ( ) \n    exc_tb = traceback . extract_tb ( exc_info [ 2 ] ) \n    full_tb = stack [ : 1 ] + exc_tb \n    exc_line : typing . List [ str ] = traceback . format_exception_only ( * exc_info [ : 2 ] ) \n    tb_text = \"\\nTraceback (most recent call last):\\n\" + \"\" . join ( traceback . format_list ( full_tb ) ) + \"\" . join ( exc_line ) \n    return tb_text "}
{"9455": "\ndef __get_obj_source ( self , instance : typing . Any , owner : typing . Optional [ type ] = None ) -> str : \n    if self . log_object_repr : \n        return_identifier_0 = f\"{instance!r}\" \n        return return_identifier_0 \n    return_identifier_1 = f\"<{owner.__name__ if owner is not None else instance.__class__.__name__}() at 0x{id(instance):X}>\" \n    return return_identifier_1 "}
{"9484": "\ndef model_to_dict ( model , mode = \"\" , show_defaults = False ) : \n    def is_mode ( obj , mode ) : \n        if mode == \"\" : \n            return_identifier_0 = True \n            return return_identifier_0 \n        elif mode == \"config\" : \n            return obj . _yang_name == \"config\" or obj . _is_config \n        elif mode == \"state\" : \n            return obj . _yang_name == \"state\" or not obj . _is_config \n        else : \n            raise ValueError ( \"mode can only be config, state or ''. Passed: {}\" . format ( mode ) ) \n    def get_key ( key , model , parent_defining_module , show_defaults ) : \n        if not show_defaults : \n            key = \"{} {}\" . format ( key , \"[rw]\" if model . _is_config else \"[ro]\" ) \n        if parent_defining_module != model . _defining_module : \n            key = \"{}:{}\" . format ( model . _defining_module , key ) \n        return key \n    if model . _yang_type in ( \"container\" , \"list\" ) : \n        cls = model if model . _yang_type in ( \"container\" , ) else model . _contained_class ( ) \n        result = { } \n        for k , v in cls : \n            r = model_to_dict ( v , mode = mode , show_defaults = show_defaults ) \n            if r : \n                result [ get_key ( k , v , model . _defining_module , show_defaults ) ] = r \n        return result \n    else : \n        if show_defaults : \n            if model . _default is False : \n                if model . _yang_type != \"boolean\" : \n                    return None \n            return model . _default \n        return model . _yang_type if is_mode ( model , mode ) else None "}
{"9524": "\ndef uploading_request ( verb , path , data = None , files = None , timeout = conf . DEFAULT ) : \n    path = path . lstrip ( '/' ) \n    url = urljoin ( conf . upload_base , path ) \n    if data is None : \n        data = { } \n    data [ 'pub_key' ] = conf . pub_key \n    data [ 'UPLOADCARE_PUB_KEY' ] = conf . pub_key \n    headers = { 'User-Agent' : _build_user_agent ( ) , } \n    try : \n        response = session . request ( str ( verb ) , url , allow_redirects = True , verify = conf . verify_upload_ssl , data = data , files = files , headers = headers , timeout = _get_timeout ( timeout ) , ) \n    except requests . RequestException as exc : \n        raise APIConnectionError ( exc . args [ 0 ] ) \n    if response . status_code == 204 : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    if 200 <= response . status_code < 300 : \n        if _content_type_from_response ( response ) . endswith ( ( '/json' , '+json' ) ) : \n            try : \n                return response . json ( ) \n            except ValueError as exc : \n                raise APIError ( exc . args [ 0 ] ) \n    if response . status_code in ( 400 , 404 ) : \n        raise InvalidRequestError ( response . content ) \n    raise APIError ( response . content ) "}
{"9533": "\ndef is_last_li ( li , meta_data , current_numId ) : \n    if not is_li ( li , meta_data ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    w_namespace = get_namespace ( li , 'w' ) \n    next_el = li \n    while True : \n        if next_el is None : \n            return_identifier_1 = True \n            return return_identifier_1 \n        next_el = next_el . getnext ( ) \n        if not is_li ( next_el , meta_data ) : \n            continue \n        new_numId = get_numId ( next_el , w_namespace ) \n        if current_numId != new_numId : \n            return_identifier_2 = True \n            return return_identifier_2 \n        return_identifier_3 = False \n        return return_identifier_3 "}
{"9537": "\ndef get_grid_span ( tc ) : \n    w_namespace = get_namespace ( tc , 'w' ) \n    grid_spans = tc . xpath ( './/w:gridSpan' , namespaces = tc . nsmap ) \n    if len ( grid_spans ) != 1 : \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    grid_span = grid_spans [ 0 ] \n    return int ( grid_span . get ( '%sval' % w_namespace ) ) "}
{"9539": "\ndef style_is_false ( style ) : \n    if style is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    w_namespace = get_namespace ( style , 'w' ) \n    return style . get ( '%sval' % w_namespace ) != 'false' "}
{"9540": "\ndef is_bold ( r ) : \n    w_namespace = get_namespace ( r , 'w' ) \n    rpr = r . find ( '%srPr' % w_namespace ) \n    if rpr is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    bold = rpr . find ( '%sb' % w_namespace ) \n    return style_is_false ( bold ) "}
{"9541": "\ndef is_italics ( r ) : \n    w_namespace = get_namespace ( r , 'w' ) \n    rpr = r . find ( '%srPr' % w_namespace ) \n    if rpr is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    italics = rpr . find ( '%si' % w_namespace ) \n    return style_is_false ( italics ) "}
{"9542": "\ndef is_underlined ( r ) : \n    w_namespace = get_namespace ( r , 'w' ) \n    rpr = r . find ( '%srPr' % w_namespace ) \n    if rpr is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    underline = rpr . find ( '%su' % w_namespace ) \n    return style_is_false ( underline ) "}
{"9543": "\ndef is_title ( p ) : \n    w_namespace = get_namespace ( p , 'w' ) \n    styles = p . xpath ( './/w:pStyle' , namespaces = p . nsmap ) \n    if len ( styles ) == 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    style = styles [ 0 ] \n    return style . get ( '%sval' % w_namespace ) == 'Title' "}
{"9545": "\ndef get_relationship_info ( tree , media , image_sizes ) : \n    if tree is None : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    result = { } \n    for el in tree . iter ( ) : \n        el_id = el . get ( 'Id' ) \n        if el_id is None : \n            continue \n        target = el . get ( 'Target' ) \n        if any ( target . lower ( ) . endswith ( ext ) for ext in IMAGE_EXTENSIONS_TO_SKIP ) : \n            continue \n        if target in media : \n            image_size = image_sizes . get ( el_id ) \n            target = convert_image ( media [ target ] , image_size ) \n        result [ el_id ] = cgi . escape ( target ) \n    return result "}
{"9551": "\ndef get_t_tag_content ( t , parent , remove_bold , remove_italics , meta_data ) : \n    if t is None or t . text is None : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    text = cgi . escape ( t . text ) \n    el_is_bold = not remove_bold and ( is_bold ( parent ) or is_underlined ( parent ) ) \n    el_is_italics = not remove_italics and is_italics ( parent ) \n    if el_is_bold : \n        text = '<strong>%s</strong>' % text \n    if el_is_italics : \n        text = '<em>%s</em>' % text \n    return text "}
{"9554": "\ndef load_mnist ( flatten = True , labels = False ) : \n    fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) \n    h = gzip . open ( fn , 'rb' ) \n    if sys . version_info < ( 3 , ) : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) \n    else : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) \n    h . close ( ) \n    if not flatten : \n        timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n    if labels : \n        return_identifier_0 = ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) \n        return return_identifier_0 \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9561": "\ndef classifier_batches ( self , steps , batch_size , rng = None ) : \n    assert batch_size >= 2 , 'batch_size must be at least 2!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    T = np . arange ( steps ) \n    def batch ( ) : \n        inputs = np . zeros ( ( batch_size , steps , 1 + len ( self . alpha ) ) , 'f' ) \n        outputs = np . zeros ( ( batch_size , steps ) , 'i' ) \n        for b in range ( batch_size ) : \n            offset = rng . randint ( len ( self . text ) - steps - 1 ) \n            enc = self . encode ( self . text [ offset : offset + steps + 1 ] ) \n            inputs [ b , T , enc [ : - 1 ] ] = 1 \n            outputs [ b , T ] = enc [ 1 : ] \n        return_identifier_0 = [ inputs , outputs ] \n        return return_identifier_0 \n    return batch "}
{"9572": "\ndef batch_at ( features , labels , seq_begins , seq_lengths ) : \n    length = seq_lengths . max ( ) \n    feat = np . zeros ( ( BATCH_SIZE , length , features . shape [ - 1 ] ) , 'f' ) \n    labl = np . zeros ( ( BATCH_SIZE , length ) , 'i' ) \n    mask = np . zeros ( ( BATCH_SIZE , length ) , 'f' ) \n    for b , ( begin , length ) in enumerate ( zip ( seq_begins , seq_lengths ) ) : \n        feat [ b , : length ] = features [ begin : begin + length ] \n        labl [ b , : length ] = labels [ begin : begin + length ] \n        mask [ b , : length ] = 1 \n    return_identifier_0 = [ feat , labl , mask ] \n    return return_identifier_0 "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = 0 , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , 0 ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ 0 ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if algo == 'sample' : \n            algo = trainer . SampleTrainer ( self ) \n        elif algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n            algo = trainer . SupervisedPretrainer ( subalgo , self ) \n        elif algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n            algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n        else : \n            algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if isinstance ( save_every , float ) : \n            return elapsed > 60 * save_every \n        if isinstance ( save_every , int ) : \n            return iteration % save_every == 0 \n        return_identifier_1 = False \n        return return_identifier_1 \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9642": "\ndef get_ref_annotation_data_after_time ( self , id_tier , time ) : \n    befores = self . get_ref_annotation_data_between_times ( id_tier , time , self . get_full_time_interval ( ) ) \n    if befores : \n        return_identifier_0 = [ min ( befores , key = lambda x : x [ 0 ] ) ] \n        return return_identifier_0 \n    else : \n        return_identifier_1 = [ ] \n        return return_identifier_1 "}
{"9643": "\ndef get_ref_annotation_data_before_time ( self , id_tier , time ) : \n    befores = self . get_ref_annotation_data_between_times ( id_tier , 0 , time ) \n    if befores : \n        return_identifier_0 = [ max ( befores , key = lambda x : x [ 0 ] ) ] \n        return return_identifier_0 \n    else : \n        return_identifier_1 = [ ] \n        return return_identifier_1 "}
{"9707": "\ndef connect ( self ) : \n    self . _sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n    if self . use_ssl : \n        self . _sock = ssl . wrap_socket ( self . _sock ) \n    try : \n        self . _sock . connect ( ( self . server , self . port ) ) \n    except socket . error : \n        self . logger . error ( 'Unable to connect to %s on port %d' % ( self . server , self . port ) , exc_info = 1 ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _sock_file = self . _sock . makefile ( ) \n    if self . password : \n        self . set_password ( ) \n    self . register_nick ( ) \n    self . register ( ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"9709": "\ndef dispatch_patterns ( self ) : \n    return_identifier_0 = ( ( self . nick_re , self . new_nick ) , ( self . nick_change_re , self . handle_nick_change ) , ( self . ping_re , self . handle_ping ) , ( self . part_re , self . handle_part ) , ( self . join_re , self . handle_join ) , ( self . quit_re , self . handle_quit ) , ( self . chanmsg_re , self . handle_channel_message ) , ( self . privmsg_re , self . handle_private_message ) , ( self . registered_re , self . handle_registered ) , ) \n    return return_identifier_0 "}
{"9713": "\ndef enter_event_loop ( self ) : \n    patterns = self . dispatch_patterns ( ) \n    self . logger . debug ( 'entering receive loop' ) \n    while 1 : \n        try : \n            data = self . _sock_file . readline ( ) \n        except socket . error : \n            data = None \n        if not data : \n            self . logger . info ( 'server closed connection' ) \n            self . close ( ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        data = data . rstrip ( ) \n        for pattern , callback in patterns : \n            match = pattern . match ( data ) \n            if match : \n                callback ( ** match . groupdict ( ) ) "}
{"9717": "\ndef command_patterns ( self ) : \n    return_identifier_0 = ( ( '!register-success (?P<cmd_channel>.+)' , self . require_boss ( self . register_success ) ) , ( '!worker-execute (?:\\((?P<workers>.+?)\\) )?(?P<task_id>\\d+):(?P<command>.+)' , self . require_boss ( self . worker_execute ) ) , ( '!worker-ping' , self . require_boss ( self . worker_ping_handler ) ) , ( '!worker-stop' , self . require_boss ( self . worker_stop ) ) , ) \n    return return_identifier_0 "}
{"9726": "\ndef allow_request ( self , request , view ) : \n    if request . method != 'POST' : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return super ( PostRequestThrottleMixin , self ) . allow_request ( request , view ) "}
{"9729": "\ndef tls_client ( self ) : \n    if self . tls_cert and self . tls_key : \n        return_identifier_0 = ( self . tls_cert , self . tls_key ) \n        return return_identifier_0 \n    return None "}
{"9733": "\ndef poll ( self ) : \n    service = yield self . get_service ( ) \n    if not service : \n        self . log . warn ( \"Docker service not found\" ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    task_filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } \n    tasks = yield self . docker ( 'tasks' , task_filter ) \n    running_task = None \n    for task in tasks : \n        task_state = task [ 'Status' ] [ 'State' ] \n        self . log . debug ( \"Task %s of Docker service %s status: %s\" , task [ 'ID' ] [ : 7 ] , self . service_id [ : 7 ] , pformat ( task_state ) , ) \n        if task_state == 'running' : \n            running_task = task \n    if running_task is not None : \n        return None \n    else : \n        return_identifier_1 = 1 \n        return return_identifier_1 "}
{"9744": "\ndef password_reset_email_context ( notification ) : \n    return_identifier_0 = { 'protocol' : 'https' , 'uid' : notification . user . generate_uid ( ) , 'token' : notification . user . generate_token ( ) , 'site' : notification . site , } \n    return return_identifier_0 "}
{"9748": "\ndef authenticate ( self , request ) : \n    try : \n        key = request . data [ 'token' ] \n    except KeyError : \n        return \n    try : \n        token = AuthToken . objects . get ( key = key ) \n    except AuthToken . DoesNotExist : \n        return \n    return_identifier_0 = ( token . user , token ) \n    return return_identifier_0 "}
{"9749": "\ndef authenticate_credentials ( self , key ) : \n    user , token = super ( TokenAuthentication , self ) . authenticate_credentials ( key ) \n    if token . expires < timezone . now ( ) : \n        msg = _ ( 'Token has expired.' ) \n        raise exceptions . AuthenticationFailed ( msg ) \n    token . update_expiry ( ) \n    return_identifier_0 = ( user , token ) \n    return return_identifier_0 "}
{"9762": "\ndef bytes_to_readable ( num ) : \n    if num < 512 : \n        return_identifier_0 = \"0 Kb\" \n        return return_identifier_0 \n    elif num < 1024 : \n        return_identifier_1 = \"1 Kb\" \n        return return_identifier_1 \n    for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : \n        if abs ( num ) < 1024.0 : \n            return \"%3.1f%s\" % ( num , unit ) \n        num /= 1024.0 \n    return \"%.1f%s\" % ( num , 'Yb' ) "}
{"9773": "\ndef _login ( self ) : \n    api_path = \"%s/auth.cgi?api=SYNO.API.Auth&version=2\" % ( self . base_url , ) \n    login_path = \"method=login&%s\" % ( self . _encode_credentials ( ) ) \n    url = \"%s&%s&session=Core&format=cookie\" % ( api_path , login_path ) \n    result = self . _execute_get_url ( url , False ) \n    if result is not None : \n        self . access_token = result [ \"data\" ] [ \"sid\" ] \n        self . _debuglog ( \"Authentication Succesfull, token: \" + str ( self . access_token ) ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        self . _debuglog ( \"Authentication Failed\" ) \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"9797": "\ndef is_glacier ( s3_client , bucket , prefix ) : \n    response = s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix , MaxKeys = 3 ) \n    for key in response [ 'Contents' ] : \n        if key . get ( 'StorageClass' , 'STANDARD' ) == 'GLACIER' : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"9802": "\ndef extract_schema ( uri ) : \n    match = re . match ( SCHEMA_URI_REGEX , uri ) \n    if match : \n        return_identifier_0 = { 'vendor' : match . group ( 1 ) , 'name' : match . group ( 2 ) , 'format' : match . group ( 3 ) , 'version' : match . group ( 4 ) } \n        return return_identifier_0 \n    else : \n        raise SnowplowEventTransformationException ( [ \"Schema {} does not conform to regular expression {}\" . format ( uri , SCHEMA_URI ) ] ) "}
{"9805": "\ndef parse_unstruct ( unstruct ) : \n    my_json = json . loads ( unstruct ) \n    data = my_json [ 'data' ] \n    schema = data [ 'schema' ] \n    if 'data' in data : \n        inner_data = data [ 'data' ] \n    else : \n        raise SnowplowEventTransformationException ( [ \"Could not extract inner data field from unstructured event\" ] ) \n    fixed_schema = fix_schema ( \"unstruct_event\" , schema ) \n    return_identifier_0 = [ ( fixed_schema , inner_data ) ] \n    return return_identifier_0 "}
{"9813": "\ndef pformat_dict_summary_html ( dict ) : \n    if not dict : \n        return_identifier_0 = '   {}' \n        return return_identifier_0 \n    html = [ ] \n    for key , value in sorted ( six . iteritems ( dict ) ) : \n        if not isinstance ( value , DICT_EXPANDED_TYPES ) : \n            value = '...' \n        html . append ( _format_dict_item ( key , value ) ) \n    return mark_safe ( u'<br/>' . join ( html ) ) "}
{"9821": "\ndef read_input_file ( self , fn ) : \n    fnfull = os . path . realpath ( os . path . join ( self . tex_input_directory , fn ) ) \n    if self . strict_input : \n        dirfull = os . path . realpath ( self . tex_input_directory ) \n        if not fnfull . startswith ( dirfull ) : \n            logger . warning ( \"Can't access path '%s' leading outside of mandated directory [strict input mode]\" , fn ) \n            return_identifier_0 = '' \n            return return_identifier_0 \n    if not os . path . exists ( fnfull ) and os . path . exists ( fnfull + '.tex' ) : \n        fnfull = fnfull + '.tex' \n    if not os . path . exists ( fnfull ) and os . path . exists ( fnfull + '.latex' ) : \n        fnfull = fnfull + '.latex' \n    if not os . path . isfile ( fnfull ) : \n        logger . warning ( u\"Error, file doesn't exist: '%s'\" , fn ) \n        return_identifier_1 = '' \n        return return_identifier_1 \n    logger . debug ( \"Reading input file %r\" , fnfull ) \n    try : \n        with open ( fnfull ) as f : \n            return f . read ( ) \n    except IOError as e : \n        logger . warning ( u\"Error, can't access '%s': %s\" , fn , e ) \n        return_identifier_2 = '' \n        return return_identifier_2 "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and ord ( ch ) < 127 ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and lch [ 0 : 1 ] == '\\\\' else lch ) \n            elif ( ( ord ( ch ) >= 32 and ord ( ch ) <= 127 ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result += ch \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result += r'{\\bfseries ?}' \n                else : \n                    result += ch \n    return result "}
{"9896": "\ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    cli = CommandLineTool ( ) \n    try : \n        return cli . run ( argv ) \n    except KeyboardInterrupt : \n        print ( 'Canceled' ) \n        return_identifier_0 = 3 \n        return return_identifier_0 "}
{"9899": "\ndef priority ( self ) : \n    try : \n        __import__ ( 'argon2.low_level' ) \n    except ImportError : \n        raise RuntimeError ( \"argon2_cffi package required\" ) \n    try : \n        __import__ ( 'Crypto.Cipher.AES' ) \n    except ImportError : \n        raise RuntimeError ( \"PyCryptodome package required\" ) \n    if not json : \n        raise RuntimeError ( \"JSON implementation such as simplejson \" \"required.\" ) \n    return_identifier_0 = 2.5 \n    return return_identifier_0 "}
{"9906": "\ndef decodeString ( encoded ) : \n    length = encoded [ 0 ] * 256 + encoded [ 1 ] \n    return_identifier_0 = ( encoded [ 2 : 2 + length ] . decode ( 'utf-8' ) , encoded [ 2 + length : ] ) \n    return return_identifier_0 "}
{"10078": "\ndef has_bad_headers ( self , default_from = None ) : \n    sender = self . sender or default_from \n    reply_to = self . reply_to or '' \n    for val in [ self . subject , sender , reply_to ] + self . recipients : \n        for c in '\\r\\n' : \n            if c in val : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10089": "\ndef login ( self , username , password , generate = 'enabled' , proxies = None ) : \n    logger . debug ( \"login for: %s with generate: %s\" , username , generate ) \n    if not username or not password : \n        raise BackendException ( BACKEND_ERROR , \"Missing mandatory parameters\" ) \n    if proxies : \n        for key in proxies . keys ( ) : \n            try : \n                assert key in PROXY_PROTOCOLS \n            except AssertionError : \n                raise BackendException ( BACKEND_ERROR , \"Wrong proxy protocol \" , key ) \n    self . proxies = proxies \n    endpoint = 'login' \n    json = { u'username' : username , u'password' : password } \n    if generate == 'force' : \n        json [ 'action' ] = 'generate' \n        logger . debug ( \"Asking for generating new token\" ) \n    response = self . get_response ( method = 'POST' , endpoint = endpoint , json = json ) \n    if response . status_code == 401 : \n        logger . error ( \"Backend refused login with params %s\" , json ) \n        self . set_token ( token = None ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    resp = self . decode ( response = response ) \n    if 'token' in resp : \n        self . set_token ( token = resp [ 'token' ] ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    if generate == 'force' : \n        self . set_token ( token = None ) \n        raise BackendException ( BACKEND_ERROR , \"Token not provided\" ) \n    if generate == 'disabled' : \n        logger . error ( \"Token disabled ... to be implemented!\" ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    if generate == 'enabled' : \n        logger . warning ( \"Token enabled, but none provided, require new token generation\" ) \n        return self . login ( username , password , 'force' ) \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"10090": "\ndef get_domains ( self ) : \n    resp = self . get ( '' ) \n    if \"_links\" in resp : \n        _links = resp [ \"_links\" ] \n        if \"child\" in _links : \n            return _links [ \"child\" ] \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if self . processes == 1 : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if end > number_pages : \n                end = number_pages \n            begin += 1 \n            end += 1 \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return_identifier_0 = { '_items' : items , '_status' : 'OK' } \n    return return_identifier_0 "}
{"10098": "\ndef claim ( self , file_readers ) : \n    unclaimed_readers = [ ] \n    vcf_readers = [ ] \n    for file_reader in file_readers : \n        if self . _is_mutect_vcf ( file_reader ) : \n            vcf_reader = vcf . VcfReader ( file_reader ) \n            vcf_readers . append ( _MutectVcfReader ( vcf_reader ) ) \n        else : \n            unclaimed_readers . append ( file_reader ) \n    return_identifier_0 = ( unclaimed_readers , vcf_readers ) \n    return return_identifier_0 "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        if offset == where : \n            break \n        read_size = self . read_size if self . read_size <= where else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ 0 ] == b'\\n' [ 0 ] : \n            terminator_where = self . file . tell ( ) \n            if terminator_where > data_len + 1 : \n                self . file . seek ( where - offset - data_len - 1 , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( 1 ) \n                if terminator_data [ 0 ] == b'\\r' [ 0 ] : \n                    data_len += 1 \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while data_where > 0 : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and offset == 0 and data_where == data_len : \n                data_where -= len ( terminator ) \n            elif terminator : \n                self . file . seek ( where - offset - ( data_len - data_where ) ) \n                return self . file . tell ( ) \n            else : \n                data_where -= 1 \n        offset += data_len \n    if where == 0 : \n        return - 1 \n    else : \n        self . file . seek ( 0 ) \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"10108": "\ndef tail ( self , lines = 10 ) : \n    self . file . seek ( 0 , SEEK_END ) \n    for i in range ( lines ) : \n        if self . seek_previous_line ( ) == - 1 : \n            break \n    data = self . file . read ( ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return_identifier_0 = [ ] \n        return return_identifier_0 "}
{"10109": "\ndef head ( self , lines = 10 ) : \n    self . file . seek ( 0 ) \n    for i in range ( lines ) : \n        if self . seek_next_line ( ) == - 1 : \n            break \n    end_pos = self . file . tell ( ) \n    self . file . seek ( 0 ) \n    data = self . file . read ( end_pos ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return_identifier_0 = [ ] \n        return return_identifier_0 "}
{"10111": "\ndef claim ( self , file_readers ) : \n    ( prefix_to_reader , unclaimed_readers ) = self . _find_strelka_files ( file_readers ) \n    prefix_by_patients = self . _split_prefix_by_patient ( prefix_to_reader ) \n    self . _validate_vcf_readers ( prefix_by_patients ) \n    vcf_readers = self . _create_vcf_readers ( prefix_to_reader ) \n    return_identifier_0 = ( unclaimed_readers , vcf_readers ) \n    return return_identifier_0 "}
{"10117": "\ndef _sample_field ( self , sample ) : \n    tag_values = self . sample_tag_values [ sample ] . values ( ) \n    if tag_values : \n        return \":\" . join ( tag_values ) \n    else : \n        return_identifier_0 = \".\" \n        return return_identifier_0 "}
{"10130": "\ndef parse_osm_file ( f , parse_timestamps = True ) : \n    nodes = [ ] \n    ways = [ ] \n    relations = [ ] \n    for p in iter_osm_file ( f , parse_timestamps ) : \n        if type ( p ) == model . Node : \n            nodes . append ( p ) \n        elif type ( p ) == model . Way : \n            ways . append ( p ) \n        elif type ( p ) == model . Relation : \n            relations . append ( p ) \n    return_identifier_0 = ( nodes , ways , relations ) \n    return return_identifier_0 "}
{"10133": "\ndef is_met ( self , user , filtered = False ) : \n    if filtered : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return self . passes_filter ( user ) "}
{"10134": "\ndef user_quantity_remaining ( self , user , filtered = True ) : \n    if filtered : \n        if hasattr ( self . condition , \"remainder\" ) : \n            return self . condition . remainder \n    qs = type ( self . condition ) . objects . filter ( pk = self . condition . id ) \n    qs = self . pre_filter ( qs , user ) \n    if len ( qs ) > 0 : \n        return qs [ 0 ] . remainder \n    else : \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"10164": "\ndef speaker_registrations ( request , form ) : \n    kinds = form . cleaned_data [ \"kind\" ] \n    presentations = schedule_models . Presentation . objects . filter ( proposal_base__kind__in = kinds , ) . exclude ( cancelled = True , ) \n    users = User . objects . filter ( Q ( speaker_profile__presentations__in = presentations ) | Q ( speaker_profile__copresentations__in = presentations ) ) \n    paid_carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS_PAID ) \n    paid_carts = Case ( When ( cart__in = paid_carts , then = Value ( 1 ) ) , default = Value ( 0 ) , output_field = models . IntegerField ( ) , ) \n    users = users . annotate ( paid_carts = Sum ( paid_carts ) ) \n    users = users . order_by ( \"paid_carts\" ) \n    return QuerysetReport ( \"Speaker Registration Status\" , [ \"id\" , \"speaker_profile__name\" , \"email\" , \"paid_carts\" ] , users , link_view = attendee , ) \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"10174": "\ndef _handle_voucher ( request , prefix ) : \n    voucher_form = forms . VoucherForm ( request . POST or None , prefix = prefix ) \n    current_cart = CartController . for_user ( request . user ) \n    if ( voucher_form . is_valid ( ) and voucher_form . cleaned_data [ \"voucher\" ] . strip ( ) ) : \n        voucher = voucher_form . cleaned_data [ \"voucher\" ] \n        voucher = inventory . Voucher . normalise_code ( voucher ) \n        if len ( current_cart . cart . vouchers . filter ( code = voucher ) ) > 0 : \n            handled = False \n        else : \n            try : \n                current_cart . apply_voucher ( voucher ) \n            except Exception as e : \n                voucher_form . add_error ( \"voucher\" , e ) \n            handled = True \n    else : \n        handled = False \n    return_identifier_0 = ( voucher_form , handled ) \n    return return_identifier_0 "}
{"10198": "\ndef can_view ( self , user = None , access_code = None ) : \n    if user == self . invoice . user : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if user . is_staff : \n        return_identifier_1 = True \n        return return_identifier_1 \n    if self . invoice . user . attendee . access_code == access_code : \n        return_identifier_2 = True \n        return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"10203": "\ndef _invoice_matches_cart ( self ) : \n    self . _refresh ( ) \n    cart = self . invoice . cart \n    if not cart : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return cart . revision == self . invoice . cart_revision "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if prev_q [ 'chr' ] < current_q [ 'chr' ] : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if prev_q [ 'chr' ] > current_q [ 'chr' ] : \n            return_identifier_1 = False \n            return return_identifier_1 \n        if prev_q [ 'start' ] < current_q [ 'start' ] : \n            return_identifier_2 = True \n            return return_identifier_2 \n        if prev_q [ 'start' ] > current_q [ 'start' ] : \n            return_identifier_3 = False \n            return return_identifier_3 \n        if prev_q [ 'stop' ] < current_q [ 'stop' ] : \n            return_identifier_4 = True \n            return return_identifier_4 \n        if prev_q [ 'stop' ] > current_q [ 'stop' ] : \n            return_identifier_5 = False \n            return return_identifier_5 \n        return_identifier_6 = True \n        return return_identifier_6 \n    ct_pointer = 0 \n    query_pointer = 0 \n    last_query_pointer = - 1 \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while query_pointer < len ( sorted_queries ) and ct_pointer < len ( ct ) : \n        if last_query_pointer != query_pointer : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if q_chr < c_chr : \n            query_pointer += 1 \n            continue \n        if q_chr > c_chr : \n            ct_pointer += 1 \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if q_start > c_stop : \n            ct_pointer += 1 \n            continue \n        if q_stop < c_start : \n            query_pointer += 1 \n            continue \n        if search_mode == 'any' : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        elif search_mode == 'exact' and q_start == c_start and q_stop == c_stop : \n            q_alt = q . alt \n            c_alt = c . alt \n            if not ( q_alt and c_alt and q_alt != c_alt ) : \n                matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n        elif search_mode == 'include_smaller' : \n            raise NotImplementedError \n        elif search_mode == 'include_larger' : \n            raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer += 1 \n    return dict ( matches ) "}
{"10225": "\ndef update ( self , allow_partial = True , force = False , ** kwargs ) : \n    if kwargs : \n        self . __init__ ( partial = allow_partial , force = force , ** kwargs ) \n        return not self . _partial \n    if not force and CACHE . get ( hash ( self ) ) : \n        cached = CACHE [ hash ( self ) ] \n        for field in self . _SIMPLE_FIELDS | self . _COMPLEX_FIELDS : \n            v = getattr ( cached , field ) \n            setattr ( self , field , v ) \n        self . _partial = False \n        logging . info ( f'Loading {str(self)} from cache' ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    resp_dict = element_lookup_by_id ( self . type , self . id ) \n    self . __init__ ( partial = False , ** resp_dict ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"10227": "\ndef authenticate ( self ) : \n    if self . oauth : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . settings . apply ( 'api-asana' , self . args . asana_api , \"enter asana api key\" ) \n    self . settings . apply ( 'api-github' , self . args . github_api , \"enter github.com token\" ) \n    logging . debug ( \"authenticating asana api.\" ) \n    self . asana = Client . basic_auth ( self . settings [ 'api-asana' ] ) \n    self . asana_errors = asana_errors \n    self . asana_me = self . asana . users . me ( ) \n    logging . debug ( \"authenticating github api\" ) \n    self . github = Github ( self . settings [ 'api-github' ] ) \n    self . github_user = self . github . get_user ( ) \n    self . oauth = True "}
{"10242": "\ndef initPort ( self ) : \n    try : \n        self . m_ser = serial . Serial ( port = self . m_ttyport , baudrate = self . m_baudrate , timeout = 0 , parity = serial . PARITY_EVEN , stopbits = serial . STOPBITS_ONE , bytesize = serial . SEVENBITS , rtscts = False ) \n        ekm_log ( \"Pyserial version = \" + serial . VERSION ) \n        ekm_log ( \"Port = \" + self . m_ttyport ) \n        ekm_log ( \"Rate = \" + str ( self . m_baudrate ) ) \n        time . sleep ( self . m_init_wait ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = 0 \n    if kwh_scale == ScaleKWH . EmptyScale : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count += 1 \n            continue \n        if len ( contents ) == 0 : \n            count += 1 \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if fld_type == FieldType . Float : \n                float_data = float ( str ( raw_data ) ) \n                divisor = 1 \n                if fld_scale == ScaleType . KWH : \n                    divisor = 1 \n                    if kwh_scale == ScaleKWH . Scale10 : \n                        divisor = 10 \n                    elif kwh_scale == ScaleKWH . Scale100 : \n                        divisor = 100 \n                    elif ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : \n                        ekm_log ( \"Unrecognized kwh scale.\" ) \n                elif fld_scale == ScaleType . Div10 : \n                    divisor = 10 \n                elif fld_scale == ScaleType . Div100 : \n                    divisor = 100 \n                elif fld_scale != ScaleType . No : \n                    ekm_log ( \"Unrecognized float scale.\" ) \n                float_data /= divisor \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            elif fld_type == FieldType . Hex : \n                hex_data = raw_data . encode ( 'hex' ) \n                def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n            elif fld_type == FieldType . Int : \n                integer_data = int ( raw_data ) \n                integer_data_str = str ( integer_data ) \n                if len ( integer_data_str ) == 0 : \n                    integer_data_str = str ( 0 ) \n                def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n            elif fld_type == FieldType . String : \n                string_data = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n            elif fld_type == FieldType . PowerFactor : \n                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n            else : \n                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count += 1 \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"10252": "\ndef jsonRender ( self , def_buf ) : \n    try : \n        ret_dict = SerialBlock ( ) \n        ret_dict [ Field . Meter_Address ] = self . getMeterAddress ( ) \n        for fld in def_buf : \n            compare_fld = fld . upper ( ) \n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld : \n                ret_dict [ str ( fld ) ] = def_buf [ fld ] [ MeterData . StringValue ] \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    return json . dumps ( ret_dict , indent = 4 ) "}
{"10253": "\ndef crcMeterRead ( self , raw_read , def_buf ) : \n    try : \n        if len ( raw_read ) == 0 : \n            ekm_log ( \"(\" + self . m_context + \") Empty return read.\" ) \n            return_identifier_0 = False \n            return return_identifier_0 \n        sent_crc = self . calc_crc16 ( raw_read [ 1 : - 2 ] ) \n        logstr = \"(\" + self . m_context + \")CRC sent = \" + str ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] ) \n        logstr += \" CRC calc = \" + sent_crc \n        ekm_log ( logstr ) \n        if int ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] , 16 ) == int ( sent_crc , 16 ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    except struct . error : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    except TypeError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return_identifier_3 = False \n        return return_identifier_3 \n    except ValueError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return_identifier_4 = False \n        return return_identifier_4 \n    return_identifier_5 = False \n    return return_identifier_5 "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( hour < 0 ) or ( hour > 23 ) or ( minute < 0 ) or ( minute > 59 ) or ( tariff < 0 ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + 1 ) ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    period += 1 \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return_identifier_3 = False \n        return return_identifier_3 \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season += 1 \n    schedule += 1 \n    if ( ( season < 1 ) or ( season > Extents . Seasons ) or ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( month > 12 ) or ( month < 0 ) or ( day < 0 ) or ( day > 31 ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return_identifier_3 = False \n        return return_identifier_3 \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday += 1 \n    if ( month > 12 ) or ( month < 0 ) or ( day > 31 ) or ( day < 0 ) or ( holiday < 1 ) or ( holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if tableset == ReadSchedules . Schedules_1_To_4 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return_identifier_0 = True \n                return return_identifier_0 \n        elif tableset == ReadSchedules . Schedules_5_To_6 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n            self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                self . setContext ( \"\" ) \n                return_identifier_1 = True \n                return return_identifier_1 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"10263": "\ndef readMonthTariffs ( self , months_type ) : \n    self . setContext ( \"readMonthTariffs\" ) \n    try : \n        req_type = binascii . hexlify ( str ( months_type ) . zfill ( 1 ) ) \n        req_str = \"01523102303031\" + req_type + \"282903\" \n        work_table = self . m_mons \n        if months_type == ReadMonths . kWhReverse : \n            work_table = self . m_rev_mons \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , work_table ) \n        self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if str ( return_crc ) == str ( work_table [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Months CRC success, type = \" + str ( req_type ) ) \n            self . setContext ( \"\" ) \n            return_identifier_0 = True \n            return return_identifier_0 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10265": "\ndef readHolidayDates ( self ) : \n    self . setContext ( \"readHolidayDates\" ) \n    try : \n        req_str = \"0152310230304230282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) \n        self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if str ( return_crc ) == str ( self . m_hldy [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Holidays and Schedules CRC success\" ) \n            self . setContext ( \"\" ) \n            return_identifier_0 = True \n            return return_identifier_0 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10272": "\ndef request ( self , send_terminator = False ) : \n    try : \n        retA = self . requestA ( ) \n        retB = self . requestB ( ) \n        if retA and retB : \n            self . makeAB ( ) \n            self . calculateFields ( ) \n            self . updateObservers ( ) \n            return_identifier_0 = True \n            return return_identifier_0 \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10277": "\ndef setLCDCmd ( self , display_list , password = \"00000000\" ) : \n    result = False \n    try : \n        self . initLcd ( ) \n        item_cnt = len ( display_list ) \n        if ( item_cnt > 45 ) or ( item_cnt <= 0 ) : \n            ekm_log ( \"LCD item list must have between 1 and 40 items\" ) \n            return_identifier_0 = False \n            return return_identifier_0 \n        for display_item in display_list : \n            self . addLcdItem ( int ( display_item ) ) \n        result = self . setLCD ( password ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10286": "\ndef text ( length = None , at_least = 10 , at_most = 15 , lowercase = True , uppercase = True , digits = True , spaces = True , punctuation = False ) : \n    base_string = '' \n    if lowercase : \n        base_string += string . ascii_lowercase \n    if uppercase : \n        base_string += string . ascii_uppercase \n    if digits : \n        base_string += string . digits \n    if spaces : \n        base_string += ' ' \n    if punctuation : \n        base_string += string . punctuation \n    if len ( base_string ) == 0 : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    if not length : \n        length = random . randint ( at_least , at_most ) \n    result = '' \n    for i in xrange ( 0 , length ) : \n        result += random . choice ( base_string ) \n    return result "}
{"10341": "\ndef check ( self ) : \n    if self . closed : \n        raise ValueError ( \"Cannot check a closed state\" ) \n    self . _maybeReset ( ) \n    if self . url is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return self . _maybeCheck ( ) "}
{"10355": "\ndef get_context_data ( self , parent_context , * tag_args , ** tag_kwargs ) : \n    if 'template' not in self . allowed_kwargs : \n        tag_kwargs . pop ( 'template' , None ) \n    return_identifier_0 = { self . context_value_name : self . get_value ( parent_context , * tag_args , ** tag_kwargs ) } \n    return return_identifier_0 "}
{"10379": "\ndef select ( selector , obj ) : \n    parser = Parser ( obj ) \n    try : \n        return parser . parse ( selector ) \n    except SelectorSyntaxError as e : \n        log . exception ( e ) \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"10383": "\ndef ancestors ( self , lhs , rhs ) : \n    def _search ( node ) : \n        if node in lhs : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if not node . parent : \n            return_identifier_1 = False \n            return return_identifier_1 \n        return _search ( node . parent ) \n    return [ node for node in rhs if _search ( node ) ] "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = 1 if pat . group ( 5 ) == 'odd' else 0 \n    elif pat . group ( 6 ) : \n        a = 0 \n        b = int ( pat . group ( 6 ) ) \n    else : \n        sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n        coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 \n    reverse = False \n    if lexeme == 'nth-last-child' : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return_identifier_0 = False \n            return return_identifier_0 \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += 1 \n        if a == 0 : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and ( idx * a + b ) >= 0 \n        return m \n    return validate "}
{"10393": "\ndef chain_check ( cls , timestamp : int ) -> bool : \n    record = cls . get_record ( timestamp ) \n    if isinstance ( record , NistBeaconValue ) is False : \n        return_identifier_0 = False \n        return return_identifier_0 \n    prev_record = cls . get_previous ( record . timestamp ) \n    next_record = cls . get_next ( record . timestamp ) \n    if prev_record is None and next_record is None : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if ( isinstance ( prev_record , NistBeaconValue ) and isinstance ( next_record , NistBeaconValue ) ) : \n        return ( record . valid_signature and prev_record . valid_signature and next_record . valid_signature and record . previous_output_value == prev_record . output_value and next_record . previous_output_value == record . output_value ) \n    if ( prev_record is None and isinstance ( next_record , NistBeaconValue ) ) : \n        return ( record . valid_signature and next_record . valid_signature and cls . _INIT_RECORD == record and next_record . previous_output_value == record . output_value ) \n    if ( isinstance ( prev_record , NistBeaconValue ) and next_record is None ) : \n        return ( record . valid_signature and prev_record . valid_signature and record . previous_output_value == prev_record . output_value ) "}
{"10397": "\ndef get_fn ( self , fn , max_lines = None ) : \n    stat = os . stat ( self . logfile ) \n    if ( stat . st_ino == self . lastInode ) and ( stat . st_size == self . lastSize ) : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    if ( stat . st_ino != self . lastInode ) or ( stat . st_size < self . lastSize ) : \n        self . lastSize = 0 \n    fi = open ( self . logfile , 'rt' ) \n    fi . seek ( self . lastSize ) \n    self . lastInode = stat . st_ino \n    lines = 0 \n    for i in fi : \n        lines += 1 \n        if max_lines and ( lines > max_lines ) : \n            self . storeLast ( ) \n            fi . close ( ) \n            return \n        if '\\n' in i : \n            self . lastSize += len ( i ) \n            if self . parser : \n                line = self . parser ( i . strip ( '\\n' ) ) \n            else : \n                line = i . strip ( '\\n' ) \n            fn ( line ) \n    self . storeLast ( ) \n    fi . close ( ) "}
{"10425": "\ndef validate_token ( cls , token , expected_data ) : \n    data = SecretLinkFactory . validate_token ( token , expected_data = expected_data ) \n    if data : \n        link = cls . query . get ( data [ 'id' ] ) \n        if link and link . is_valid ( ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10426": "\ndef revoke ( self ) : \n    if self . revoked_at is None : \n        with db . session . begin_nested ( ) : \n            self . revoked_at = datetime . utcnow ( ) \n        link_revoked . send ( self ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10462": "\ndef log_time ( self ) : \n    if self . hot_loop and self . time_delta >= self . log_interval : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10480": "\ndef fetchChildren ( self ) : \n    if not self . canFetchMore ( ) : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    children = self . _fetchChildren ( ) \n    self . _fetched = True \n    return children "}
{"10493": "\ndef gravatar_get_img ( obj , size = 65 , default = 'identicon' ) : \n    url = get_gravatar_url ( obj , size = size , default = default ) \n    if url : \n        return safe ( '<img src=\"%s\" class=\"gravatar\">' % url ) \n    return_identifier_0 = '' \n    return return_identifier_0 "}
{"10494": "\ndef is_valid_filesys ( path ) : \n    if os . path . isabs ( path ) and os . path . isdir ( path ) and not os . path . isfile ( path ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        raise LocalPortValidationError ( 'Port value %s is not a valid filesystem location' % path ) "}
{"10495": "\ndef is_valid_s3_url ( url ) : \n    if url . startswith ( 'source:' ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    scheme , netloc , path , _ , _ , _ = urlparse ( url ) \n    port_except = RemotePortValidationError ( 'Port value %s is not a valid s3 location' % url ) \n    if len ( scheme ) < 2 : \n        raise port_except \n    if 's3' in scheme or 's3' in netloc or 's3' in path : \n        return_identifier_1 = True \n        return return_identifier_1 \n    else : \n        raise port_except "}
{"10507": "\ndef exists ( self , path ) : \n    self . __validate_storage_path ( path ) \n    try : \n        metadata = self . api_client . get_entity_by_query ( path = path ) \n    except StorageNotFoundException : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return metadata and 'uuid' in metadata "}
{"10529": "\ndef download_file_content ( self , file_id , etag = None ) : \n    if not is_valid_uuid ( file_id ) : \n        raise StorageArgumentException ( 'Invalid UUID for file_id: {0}' . format ( file_id ) ) \n    headers = { 'Accept' : '*/*' } \n    if etag : \n        headers [ 'If-None-Match' ] = etag \n    resp = self . _authenticated_request . to_endpoint ( 'file/{}/content/' . format ( file_id ) ) . with_headers ( headers ) . get ( ) \n    if resp . status_code == 304 : \n        return_identifier_0 = ( None , None ) \n        return return_identifier_0 \n    if 'ETag' not in resp . headers : \n        raise StorageException ( 'No ETag received from the service with the download' ) \n    return_identifier_1 = ( resp . headers [ 'ETag' ] , resp . content ) \n    return return_identifier_1 "}
{"10561": "\ndef streams ( self ) : \n    result = self . db . read ( self . path , { \"q\" : \"ls\" } ) \n    if result is None or result . json ( ) is None : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    streams = [ ] \n    for s in result . json ( ) : \n        strm = self [ s [ \"name\" ] ] \n        strm . metadata = s \n        streams . append ( strm ) \n    return streams "}
{"10567": "\ndef users ( self ) : \n    result = self . db . read ( \"\" , { \"q\" : \"ls\" } ) \n    if result is None or result . json ( ) is None : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    users = [ ] \n    for u in result . json ( ) : \n        usr = self ( u [ \"name\" ] ) \n        usr . metadata = u \n        users . append ( usr ) \n    return users "}
{"10603": "\ndef devices ( self ) : \n    result = self . db . read ( self . path , { \"q\" : \"ls\" } ) \n    if result is None or result . json ( ) is None : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    devices = [ ] \n    for d in result . json ( ) : \n        dev = self [ d [ \"name\" ] ] \n        dev . metadata = d \n        devices . append ( dev ) \n    return devices "}
{"10616": "\ndef subscribe ( self , stream , callback , transform = \"\" ) : \n    if self . status == \"disconnected\" or self . status == \"disconnecting\" or self . status == \"connecting\" : \n        self . connect ( ) \n    if self . status is not \"connected\" : \n        return_identifier_0 = False \n        return return_identifier_0 \n    logging . debug ( \"Subscribing to %s\" , stream ) \n    self . send ( { \"cmd\" : \"subscribe\" , \"arg\" : stream , \"transform\" : transform } ) \n    with self . subscription_lock : \n        self . subscriptions [ stream + \":\" + transform ] = callback \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"10617": "\ndef connect ( self ) : \n    self . ws_openlock . acquire ( ) \n    self . ws_openlock . release ( ) \n    if self . status == \"connected\" : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if self . status == \"disconnecting\" : \n        time . sleep ( 0.1 ) \n        return self . connect ( ) \n    if self . status == \"disconnected\" or self . status == \"reconnecting\" : \n        self . ws = websocket . WebSocketApp ( self . ws_url , header = self . headers , on_message = self . __on_message , on_ping = self . __on_ping , on_open = self . __on_open , on_close = self . __on_close , on_error = self . __on_error ) \n        self . ws_thread = threading . Thread ( target = self . ws . run_forever ) \n        self . ws_thread . daemon = True \n        self . status = \"connecting\" \n        self . ws_openlock . acquire ( ) \n        self . ws_thread . start ( ) \n    self . ws_openlock . acquire ( ) \n    self . ws_openlock . release ( ) \n    return self . status == \"connected\" "}
{"10630": "\ndef bam_quickcheck ( bam_path ) : \n    directory , bam_name = os . path . split ( bam_path ) \n    exit_code = subprocess . call ( [ 'docker' , 'run' , '-v' , directory + ':/data' , 'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c' , 'quickcheck' , '-vv' , '/data/' + bam_name ] ) \n    if exit_code != 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"10697": "\ndef _get_index_urls_locations ( self , project_name ) : \n    def mkurl_pypi_url ( url ) : \n        loc = posixpath . join ( url , project_url_name ) \n        if not loc . endswith ( '/' ) : \n            loc = loc + '/' \n        return loc \n    project_url_name = urllib_parse . quote ( project_name . lower ( ) ) \n    if self . index_urls : \n        main_index_url = Link ( mkurl_pypi_url ( self . index_urls [ 0 ] ) , trusted = True , ) \n        page = self . _get_page ( main_index_url ) \n        if page is None and PyPI . netloc not in str ( main_index_url ) : \n            warnings . warn ( \"Failed to find %r at %s. It is suggested to upgrade \" \"your index to support normalized names as the name in \" \"/simple/{name}.\" % ( project_name , main_index_url ) , RemovedInPip8Warning , ) \n            project_url_name = self . _find_url_name ( Link ( self . index_urls [ 0 ] , trusted = True ) , project_url_name , ) or project_url_name \n    if project_url_name is not None : \n        return [ mkurl_pypi_url ( url ) for url in self . index_urls ] \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"10701": "\ndef _get_content_type ( url , session ) : \n    scheme , netloc , path , query , fragment = urllib_parse . urlsplit ( url ) \n    if scheme not in ( 'http' , 'https' ) : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    resp = session . head ( url , allow_redirects = True ) \n    resp . raise_for_status ( ) \n    return resp . headers . get ( \"Content-Type\" , \"\" ) "}
{"10703": "\ndef verifiable ( self ) : \n    trusted = self . trusted or getattr ( self . comes_from , \"trusted\" , None ) \n    if trusted is not None and trusted : \n        try : \n            api_version = getattr ( self . comes_from , \"api_version\" , None ) \n            api_version = int ( api_version ) \n        except ( ValueError , TypeError ) : \n            api_version = None \n        if api_version is None or api_version <= 1 : \n            return \n        if self . hash : \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            return_identifier_1 = False \n            return return_identifier_1 \n    elif trusted is not None : \n        return_identifier_2 = False \n        return return_identifier_2 "}
{"10709": "\ndef compile ( marker ) : \n    try : \n        return _cache [ marker ] \n    except KeyError : \n        pass \n    if not marker . strip ( ) : \n        def marker_fn ( environment = None , override = None ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    else : \n        compiled_marker = compile_marker ( parse_marker ( marker ) ) \n        def marker_fn ( environment = None , override = None ) : \n            if override is None : \n                override = { } \n            if environment is None : \n                environment = default_environment ( ) \n            environment . update ( override ) \n            return eval ( compiled_marker , environment ) \n    marker_fn . __doc__ = marker \n    _cache [ marker ] = marker_fn \n    return _cache [ marker ] "}
{"10733": "\ndef root_is_purelib ( name , wheeldir ) : \n    name_folded = name . replace ( \"-\" , \"_\" ) \n    for item in os . listdir ( wheeldir ) : \n        match = dist_info_re . match ( item ) \n        if match and match . group ( 'name' ) == name_folded : \n            with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : \n                for line in wheel : \n                    line = line . lower ( ) . rstrip ( ) \n                    if line == \"root-is-purelib: true\" : \n                        return_identifier_0 = True \n                        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10741": "\ndef running_under_virtualenv ( ) : \n    if hasattr ( sys , 'real_prefix' ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif sys . prefix != getattr ( sys , \"base_prefix\" , sys . prefix ) : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and cc [ 'max-age' ] == 0 : \n        no_cache = True \n    if no_cache : \n        return_identifier_0 = False \n        return return_identifier_0 \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if resp . status == 301 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age += min_fresh \n    fresh = ( freshness_lifetime > current_age ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"10778": "\ndef get_decimal_precision ( number ) : \n    assert isinstance ( number , decimal . Decimal ) \n    decimal_tuple = number . normalize ( ) . as_tuple ( ) \n    if decimal_tuple . exponent >= 0 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    return abs ( decimal_tuple . exponent ) "}
{"10795": "\ndef add_requirement ( self , install_req , parent_req_name = None ) : \n    name = install_req . name \n    if not install_req . match_markers ( ) : \n        logger . warning ( \"Ignoring %s: markers %r don't match your \" \"environment\" , install_req . name , install_req . markers ) \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    install_req . as_egg = self . as_egg \n    install_req . use_user_site = self . use_user_site \n    install_req . target_dir = self . target_dir \n    install_req . pycompile = self . pycompile \n    if not name : \n        self . unnamed_requirements . append ( install_req ) \n        return_identifier_1 = [ install_req ] \n        return return_identifier_1 \n    else : \n        if parent_req_name is None and self . has_requirement ( name ) : \n            raise InstallationError ( 'Double requirement given: %s (already in %s, name=%r)' % ( install_req , self . get_requirement ( name ) , name ) ) \n        if not self . has_requirement ( name ) : \n            self . requirements [ name ] = install_req \n            if name . lower ( ) != name : \n                self . requirement_aliases [ name . lower ( ) ] = name \n            result = [ install_req ] \n        else : \n            install_req = self . get_requirement ( name ) \n            result = [ ] \n        if parent_req_name : \n            parent_req = self . get_requirement ( parent_req_name ) \n            self . _dependencies [ parent_req ] . append ( install_req ) \n        return result "}
{"10808": "\ndef fixup_chunks ( chunks ) : \n    tag_accum = [ ] \n    cur_word = None \n    result = [ ] \n    for chunk in chunks : \n        if isinstance ( chunk , tuple ) : \n            if chunk [ 0 ] == 'img' : \n                src = chunk [ 1 ] \n                tag , trailing_whitespace = split_trailing_whitespace ( chunk [ 2 ] ) \n                cur_word = tag_token ( 'img' , src , html_repr = tag , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            elif chunk [ 0 ] == 'href' : \n                href = chunk [ 1 ] \n                cur_word = href_token ( href , pre_tags = tag_accum , trailing_whitespace = \" \" ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            continue \n        if is_word ( chunk ) : \n            chunk , trailing_whitespace = split_trailing_whitespace ( chunk ) \n            cur_word = token ( chunk , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n            tag_accum = [ ] \n            result . append ( cur_word ) \n        elif is_start_tag ( chunk ) : \n            tag_accum . append ( chunk ) \n        elif is_end_tag ( chunk ) : \n            if tag_accum : \n                tag_accum . append ( chunk ) \n            else : \n                assert cur_word , ( \"Weird state, cur_word=%r, result=%r, chunks=%r of %r\" % ( cur_word , result , chunk , chunks ) ) \n                cur_word . post_tags . append ( chunk ) \n        else : \n            assert ( 0 ) \n    if not result : \n        return_identifier_0 = [ token ( '' , pre_tags = tag_accum ) ] \n        return return_identifier_0 \n    else : \n        result [ - 1 ] . post_tags . extend ( tag_accum ) \n    return result "}
{"10810": "\ndef split_words ( text ) : \n    if not text or not text . strip ( ) : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    words = split_words_re . findall ( text ) \n    return words "}
{"10825": "\ndef name ( self ) : \n    if self . import_name == '__main__' : \n        fn = getattr ( sys . modules [ '__main__' ] , '__file__' , None ) \n        if fn is None : \n            return_identifier_0 = '__main__' \n            return return_identifier_0 \n        return os . path . splitext ( os . path . basename ( fn ) ) [ 0 ] \n    return self . import_name "}
{"10830": "\ndef trap_http_exception ( self , e ) : \n    if self . config [ 'TRAP_HTTP_EXCEPTIONS' ] : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if self . config [ 'TRAP_BAD_REQUEST_ERRORS' ] : \n        return isinstance ( e , BadRequest ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10839": "\ndef requires_to_requires_dist ( requirement ) : \n    requires_dist = [ ] \n    for op , ver in requirement . specs : \n        requires_dist . append ( op + ver ) \n    if not requires_dist : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    return \" (%s)\" % ',' . join ( requires_dist ) "}
{"10868": "\ndef to_text ( s , blank_if_none = True ) : \n    if s is None : \n        if blank_if_none : \n            return_identifier_0 = \"\" \n            return return_identifier_0 \n        else : \n            return None \n    elif isinstance ( s , text_type ) : \n        return s \n    else : \n        return text_type ( s ) "}
{"10881": "\ndef distros_for_location ( location , basename , metadata = None ) : \n    if basename . endswith ( '.egg.zip' ) : \n        basename = basename [ : - 4 ] \n    if basename . endswith ( '.egg' ) and '-' in basename : \n        return_identifier_0 = [ Distribution . from_location ( location , basename , metadata ) ] \n        return return_identifier_0 \n    if basename . endswith ( '.exe' ) : \n        win_base , py_ver , platform = parse_bdist_wininst ( basename ) \n        if win_base is not None : \n            return interpret_distro_name ( location , win_base , metadata , py_ver , BINARY_DIST , platform ) \n    for ext in EXTENSIONS : \n        if basename . endswith ( ext ) : \n            basename = basename [ : - len ( ext ) ] \n            return interpret_distro_name ( location , basename , metadata ) \n    return_identifier_1 = [ ] \n    return return_identifier_1 "}
{"10889": "\ndef check_enableusersite ( ) : \n    if hasattr ( sys , 'flags' ) and getattr ( sys . flags , 'no_user_site' , False ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if hasattr ( os , \"getuid\" ) and hasattr ( os , \"geteuid\" ) : \n        if os . geteuid ( ) != os . getuid ( ) : \n            return None \n    if hasattr ( os , \"getgid\" ) and hasattr ( os , \"getegid\" ) : \n        if os . getegid ( ) != os . getgid ( ) : \n            return None \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"10897": "\ndef have_pyrex ( ) : \n    pyrex_impls = 'Cython.Distutils.build_ext' , 'Pyrex.Distutils.build_ext' \n    for pyrex_impl in pyrex_impls : \n        try : \n            __import__ ( pyrex_impl , fromlist = [ 'build_ext' ] ) . build_ext \n            return_identifier_0 = True \n            return return_identifier_0 \n        except Exception : \n            pass \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"10902": "\ndef is_url ( name ) : \n    if ':' not in name : \n        return_identifier_0 = False \n        return return_identifier_0 \n    scheme = name . split ( ':' , 1 ) [ 0 ] . lower ( ) \n    return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all_schemes "}
{"10914": "\ndef is_declared ( self , name ) : \n    if name in self . declared_locally or name in self . declared_parameter : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return name in self . declared "}
{"10947": "\ndef verify_signature ( self , value , sig ) : \n    key = self . derive_key ( ) \n    try : \n        sig = base64_decode ( sig ) \n    except Exception : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return self . algorithm . verify_signature ( key , value , sig ) "}
{"10950": "\ndef validate ( self , signed_value , max_age = None ) : \n    try : \n        self . unsign ( signed_value , max_age = max_age ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    except BadSignature : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"10974": "\ndef parse_xdot_data ( self , data ) : \n    parser = self . parser \n    if data : \n        return parser . parseString ( data ) \n    else : \n        return_identifier_0 = [ ] \n        return return_identifier_0 "}
{"10975": "\ndef proc_font ( self , tokens ) : \n    size = int ( tokens [ \"s\" ] ) \n    self . pen . font = \"%s %d\" % ( tokens [ \"b\" ] , size ) \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"10981": "\ndef render_grid_file ( context , f ) : \n    f . seek ( 0 ) \n    response = context . response \n    if __debug__ : \n        response . headers [ 'Grid-ID' ] = str ( f . _id ) \n        log . debug ( \"Serving GridFS file.\" , extra = dict ( identifier = str ( f . _id ) , filename = f . filename , length = f . length , mimetype = f . content_type ) ) \n    response . conditional_response = True \n    response . accept_ranges = 'bytes' \n    response . content_type = f . content_type \n    response . content_length = f . length \n    response . content_md5 = response . etag = f . md5 \n    response . last_modified = f . metadata . get ( 'modified' , None ) \n    response . content_disposition = 'attachment; filename=' + f . name \n    if context . request . if_range . match_response ( response ) : \n        response . body_file = f \n    else : \n        response . app_iter = iter ( f ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"11093": "\ndef split_line ( line , min_line_length = 30 , max_line_length = 100 ) : \n    if len ( line ) <= max_line_length : \n        return_identifier_0 = [ line ] \n        return return_identifier_0 \n    indent = 0 \n    while line [ indent ] == ' ' and indent < len ( line ) : \n        indent += 1 \n    i = max_line_length \n    split_point = None \n    while i > min_line_length : \n        if line [ i ] == ' ' : \n            split_point = i \n            break \n        i -= 1 \n    if split_point is None : \n        i = max_line_length + 1 \n        while i < len ( line ) : \n            if line [ i ] == ' ' : \n                split_point = i \n                break \n            i += 1 \n    if split_point is None : \n        return_identifier_1 = [ line ] \n        return return_identifier_1 \n    else : \n        line1 = line [ : split_point ] \n        line2 = ' ' * indent + line [ split_point + 1 : ] \n        return [ line1 ] + split_line ( line2 , min_line_length , max_line_length ) "}
{"11107": "\ndef get ( self , k , wait = False , wait_index = False , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if wait : \n        params [ 'index' ] = wait_index \n        params [ 'wait' ] = timeout \n    r = requests . get ( url , params = params ) \n    if r . status_code == 404 : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if r . status_code != 200 : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    try : \n        return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) \n    except TypeError as e : \n        return_identifier_0 = \"\" \n        return return_identifier_0 "}
{"11114": "\ndef is_christmas_period ( ) : \n    now = datetime . date . today ( ) \n    if now . month != 12 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if now . day < 15 : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if now . day > 27 : \n        return_identifier_2 = False \n        return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"11132": "\ndef reasonable_desired_version ( self , desired_version , allow_equal = False , allow_patch_skip = False ) : \n    try : \n        desired_version = desired_version . base_version \n    except : \n        pass \n    ( new_major , new_minor , new_patch ) = map ( int , desired_version . split ( '.' ) ) \n    tag_versions = self . _versions_from_tags ( ) \n    if not tag_versions : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    max_version = max ( self . _versions_from_tags ( ) ) . base_version \n    ( old_major , old_minor , old_patch ) = map ( int , str ( max_version ) . split ( '.' ) ) \n    update_str = str ( max_version ) + \" -> \" + str ( desired_version ) \n    v_desired = vers . Version ( desired_version ) \n    v_max = vers . Version ( max_version ) \n    if allow_equal and v_desired == v_max : \n        return_identifier_1 = \"\" \n        return return_identifier_1 \n    if v_desired < v_max : \n        return ( \"Bad update: New version doesn't increase on last tag: \" + update_str + \"\\n\" ) \n    bad_update = skipped_version ( ( old_major , old_minor , old_patch ) , ( new_major , new_minor , new_patch ) , allow_patch_skip ) \n    msg = \"\" \n    if bad_update : \n        msg = ( \"Bad update: Did you skip a version from \" + update_str + \"?\\n\" ) \n    return msg "}
{"11136": "\ndef parse_accept ( header_value ) : \n    next_explicit_q = decimal . ExtendedContext . next_plus ( decimal . Decimal ( '5.0' ) ) \n    headers = [ parse_content_type ( header ) for header in parse_list ( header_value ) ] \n    for header in headers : \n        q = header . parameters . pop ( 'q' , None ) \n        if q is None : \n            q = '1.0' \n        elif float ( q ) == 1.0 : \n            q = float ( next_explicit_q ) \n            next_explicit_q = next_explicit_q . next_minus ( ) \n        header . quality = float ( q ) \n    def ordering ( left , right ) : \n        if left . quality != right . quality : \n            return right . quality - left . quality \n        if left == right : \n            return_identifier_0 = 0 \n            return return_identifier_0 \n        if left > right : \n            return - 1 \n        return_identifier_1 = 1 \n        return return_identifier_1 \n    return sorted ( headers , key = functools . cmp_to_key ( ordering ) ) "}
{"11167": "\ndef pause ( self , signum , seconds = 0 , callback_function = None ) : \n    if callback_function is None : \n        callback_function = self . default_handler \n    if seconds > 0 : \n        self . log . info ( \"Signal handler pausing for {0} seconds or until it receives SIGALRM or SIGCONT\" . format ( seconds ) ) \n        signal . signal ( signal . SIGALRM , callback_function ) \n        signal . alarm ( seconds ) \n    else : \n        self . log . info ( 'Signal handler pausing until it receives SIGALRM or SIGCONT' ) \n    signal . signal ( signal . SIGCONT , callback_function ) \n    signal . pause ( ) \n    self . log . info ( 'Signal handler resuming from pause' ) \n    if signum == signal . SIGALRM : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"11171": "\ndef fetch_metric ( self , metric , start , end , tags = { } , aggregator = \"sum\" , downsample = None , ms_resolution = True ) : \n    query = \"{aggregator}:{downsample}{metric}{{{tags}}}\" . format ( aggregator = aggregator , downsample = downsample + \"-avg:\" if downsample else \"\" , metric = metric , tags = ',' . join ( \"%s=%s\" % ( k , v ) for k , v in tags . items ( ) ) ) \n    params = { 'ms' : ms_resolution , 'start' : '{0:.3f}' . format ( start . timestamp ( ) ) , 'end' : '{0:.3f}' . format ( end . timestamp ( ) ) , 'm' : query } \n    response = self . __request ( \"/query\" , params ) \n    if response . status_code == 200 : \n        try : \n            return response . json ( ) [ 0 ] [ 'dps' ] \n        except IndexError : \n            return_identifier_0 = { } \n            return return_identifier_0 \n    raise QueryError ( response . json ( ) ) "}
{"11178": "\ndef __new_argv ( self , * new_pargs , ** new_kargs ) : \n    new_argv = self . argv . copy ( ) \n    new_extra_argv = list ( self . extra_argv ) \n    for v in new_pargs : \n        arg_name = None \n        for name in self . pargl : \n            if not name in new_argv : \n                arg_name = name \n                break \n        if arg_name : \n            new_argv [ arg_name ] = v \n        elif self . var_pargs : \n            new_extra_argv . append ( v ) \n        else : \n            num_prev_pargs = len ( [ name for name in self . pargl if name in self . argv ] ) \n            raise TypeError ( \"%s() takes exactly %d positional arguments (%d given)\" % ( self . __name__ , len ( self . pargl ) , num_prev_pargs + len ( new_pargs ) ) ) \n    for k , v in new_kargs . items ( ) : \n        if not ( self . var_kargs or ( k in self . pargl ) or ( k in self . kargl ) ) : \n            raise TypeError ( \"%s() got an unexpected keyword argument '%s'\" % ( self . __name__ , k ) ) \n        new_argv [ k ] = v \n    return_identifier_0 = ( new_argv , new_extra_argv ) \n    return return_identifier_0 "}
{"11179": "\ndef ignore_certain_metainf_files ( filename ) : \n    ignore = ( \"META-INF/manifest.mf\" , \"META-INF/*.sf\" , \"META-INF/*.rsa\" , \"META-INF/*.dsa\" , \"META-INF/ids.json\" ) \n    for glob in ignore : \n        if fnmatch . fnmatchcase ( filename . upper ( ) , glob . upper ( ) ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"11180": "\ndef file_key ( filename ) : \n    prio = 4 \n    if filename == 'install.rdf' : \n        prio = 1 \n    elif filename in [ \"chrome.manifest\" , \"icon.png\" , \"icon64.png\" ] : \n        prio = 2 \n    elif filename in [ \"MPL\" , \"GPL\" , \"LGPL\" , \"COPYING\" , \"LICENSE\" , \"license.txt\" ] : \n        prio = 5 \n    return_identifier_0 = ( prio , os . path . split ( filename . lower ( ) ) ) \n    return return_identifier_0 "}
{"11207": "\ndef check ( self ) : \n    if not self . is_valid : \n        return_identifier_0 = True \n        return return_identifier_0 \n    test = ( self . has_quorum , self . has_quorum_possible , self . has_noquorum ) \n    assert 1 == len ( [ x for x in test if x is not None ] ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"11220": "\ndef update ( self , data ) : \n    if data not in self . filter : \n        self . filter . append ( data ) \n        if len ( self . filter ) > self . max_items : \n            self . filter . pop ( 0 ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        self . filter . append ( self . filter . pop ( 0 ) ) \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"11232": "\ndef tables ( self ) : \n    if self . table is None : \n        raise GiraffeError ( \"Target table has not been set.\" ) \n    return_identifier_0 = [ \"{}_wt\" . format ( self . table ) , \"{}_log\" . format ( self . table ) , \"{}_e1\" . format ( self . table ) , \"{}_e2\" . format ( self . table ) , ] \n    return return_identifier_0 "}
{"11245": "\ndef specific_gains ( string ) : \n    if not string : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    gains = { } \n    for gain in string . split ( ',' ) : \n        amp_name , value = gain . split ( '=' ) \n        gains [ amp_name . strip ( ) ] = float ( value . strip ( ) ) \n    return gains "}
{"11246": "\ndef device_settings ( string ) : \n    if not string : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    settings = { } \n    for setting in string . split ( ',' ) : \n        setting_name , value = setting . split ( '=' ) \n        settings [ setting_name . strip ( ) ] = value . strip ( ) \n    return settings "}
{"11248": "\ndef detect_devices ( soapy_args = '' ) : \n    devices = simplesoapy . detect_devices ( soapy_args , as_string = True ) \n    text = [ ] \n    text . append ( 'Detected SoapySDR devices:' ) \n    if devices : \n        for i , d in enumerate ( devices ) : \n            text . append ( '  {}' . format ( d ) ) \n    else : \n        text . append ( '  No devices found!' ) \n    return_identifier_0 = ( devices , '\\n' . join ( text ) ) \n    return return_identifier_0 "}
{"11250": "\ndef result ( self , psd_state ) : \n    freq_array = numpy . fft . fftshift ( psd_state [ 'freq_array' ] ) \n    pwr_array = numpy . fft . fftshift ( psd_state [ 'pwr_array' ] ) \n    if self . _crop_factor : \n        crop_bins_half = round ( ( self . _crop_factor * self . _bins ) / 2 ) \n        freq_array = freq_array [ crop_bins_half : - crop_bins_half ] \n        pwr_array = pwr_array [ crop_bins_half : - crop_bins_half ] \n    if psd_state [ 'repeats' ] > 1 : \n        pwr_array = pwr_array / psd_state [ 'repeats' ] \n    if self . _log_scale : \n        pwr_array = 10 * numpy . log10 ( pwr_array ) \n    return_identifier_0 = ( freq_array , pwr_array ) \n    return return_identifier_0 "}
{"11253": "\ndef read ( self , f ) : \n    magic = f . read ( len ( self . magic ) ) \n    if not magic : \n        return None \n    if magic != self . magic : \n        raise ValueError ( 'Magic bytes not found! Read data: {}' . format ( magic ) ) \n    header = self . header . _make ( self . header_struct . unpack ( f . read ( self . header_struct . size ) ) ) \n    pwr_array = numpy . fromstring ( f . read ( header . size ) , dtype = 'float32' ) \n    return_identifier_0 = ( header , pwr_array ) \n    return return_identifier_0 "}
{"11258": "\ndef create_buffer ( self , bins , repeats , base_buffer_size , max_buffer_size = 0 ) : \n    samples = bins * repeats \n    buffer_repeats = 1 \n    buffer_size = math . ceil ( samples / base_buffer_size ) * base_buffer_size \n    if not max_buffer_size : \n        max_buffer_size = ( 100 * 1024 ** 2 ) / 8 \n    if max_buffer_size > 0 : \n        max_buffer_size = math . ceil ( max_buffer_size / base_buffer_size ) * base_buffer_size \n        if buffer_size > max_buffer_size : \n            logger . warning ( 'Required buffer size ({}) will be shrinked to max_buffer_size ({})!' . format ( buffer_size , max_buffer_size ) ) \n            buffer_repeats = math . ceil ( buffer_size / max_buffer_size ) \n            buffer_size = max_buffer_size \n    logger . info ( 'repeats: {}' . format ( repeats ) ) \n    logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample_rate ) ) \n    if max_buffer_size > 0 : \n        logger . info ( 'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max_buffer_size , max_buffer_size / bins , max_buffer_size / self . device . sample_rate ) ) \n    else : \n        logger . info ( 'max_buffer_size (samples): UNLIMITED' ) \n    logger . info ( 'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer_size , buffer_size / bins , buffer_size / self . device . sample_rate ) ) \n    logger . info ( 'buffer_repeats: {}' . format ( buffer_repeats ) ) \n    return_identifier_0 = ( buffer_repeats , zeros ( buffer_size , numpy . complex64 ) ) \n    return return_identifier_0 "}
{"11261": "\ndef psd ( self , freq ) : \n    if not self . device . is_streaming : \n        raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) \n    logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) \n    t_freq = time . time ( ) \n    if self . device . freq != freq : \n        if self . _reset_stream : \n            self . device . device . deactivateStream ( self . device . stream ) \n        self . device . freq = freq \n        if self . _reset_stream : \n            self . device . device . activateStream ( self . device . stream ) \n        if self . _tune_delay : \n            t_delay = time . time ( ) \n            while True : \n                self . device . read_stream ( ) \n                t_delay_end = time . time ( ) \n                if t_delay_end - t_delay >= self . _tune_delay : \n                    break \n            logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) \n    else : \n        logger . debug ( '    Same frequency as before, tuning skipped' ) \n    psd_state = self . _psd . set_center_freq ( freq ) \n    t_freq_end = time . time ( ) \n    logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) \n    for repeat in range ( self . _buffer_repeats ) : \n        logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) \n        t_acq = time . time ( ) \n        acq_time_start = datetime . datetime . utcnow ( ) \n        self . device . read_stream_into_buffer ( self . _buffer ) \n        acq_time_stop = datetime . datetime . utcnow ( ) \n        t_acq_end = time . time ( ) \n        logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) \n        self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) \n        t_final = time . time ( ) \n        if _shutdown : \n            break \n    psd_future = self . _psd . result_async ( psd_state ) \n    logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) \n    return_identifier_0 = ( psd_future , acq_time_start , acq_time_stop ) \n    return return_identifier_0 "}
{"11303": "\ndef verify ( data ) : \n    if len ( data ) == 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    crc = VProCRC . get ( data ) \n    if crc : \n        log . info ( \"CRC Bad\" ) \n    else : \n        log . debug ( \"CRC OK\" ) \n    return not crc "}
{"11337": "\ndef permission_required ( perm , login_url = None , raise_exception = False ) : \n    def check_perms ( user ) : \n        if not getattr ( settings , 'DASHBOARD_REQUIRE_LOGIN' , app_settings . REQUIRE_LOGIN ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if user . has_perm ( perm ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n        if raise_exception : \n            raise PermissionDenied \n        return_identifier_2 = False \n        return return_identifier_2 \n    return user_passes_test ( check_perms , login_url = login_url ) "}
{"11346": "\ndef should_update ( self ) : \n    last_update = self . get_last_update ( ) \n    time_since = now ( ) - last_update . last_update \n    if time_since . seconds < self . update_interval : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"11350": "\ndef _argcheck ( * args , ** kwargs ) : \n    try : \n        from pyspark import SparkContext \n    except ImportError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    cond1 = any ( [ isinstance ( arg , SparkContext ) for arg in args ] ) \n    cond2 = isinstance ( kwargs . get ( 'context' , None ) , SparkContext ) \n    cond3 = any ( [ isinstance ( arg , BoltArraySpark ) for arg in args ] ) \n    cond4 = any ( [ any ( [ isinstance ( sub , BoltArraySpark ) for sub in arg ] ) if isinstance ( arg , ( tuple , list ) ) else False for arg in args ] ) \n    return cond1 or cond2 or cond3 or cond4 "}
{"11409": "\ndef discrete_alpha_mle ( data , xmin ) : \n    gexmin = ( data >= xmin ) \n    nn = gexmin . sum ( ) \n    if nn < 2 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    xx = data [ gexmin ] \n    alpha = 1.0 + float ( nn ) * ( sum ( log ( xx / ( float ( xmin ) - 0.5 ) ) ) ) ** - 1 \n    return alpha "}
{"11431": "\ndef store ( self , usage = None , mech = None , overwrite = False , default = False , cred_store = None ) : \n    if usage is None : \n        usage = self . usage \n    if isinstance ( mech , OID ) : \n        oid_ptr = ffi . addressof ( mech . _oid ) \n    else : \n        oid_ptr = ffi . cast ( 'gss_OID' , C . GSS_C_NO_OID ) \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    elements_stored = ffi . new ( 'gss_OID_set[1]' ) \n    usage_stored = ffi . new ( 'gss_cred_usage_t[1]' ) \n    if cred_store is None : \n        if not hasattr ( C , 'gss_store_cred' ) : \n            raise NotImplementedError ( \"The GSSAPI implementation does not support \" \"gss_store_cred\" ) \n        retval = C . gss_store_cred ( minor_status , self . _cred [ 0 ] , ffi . cast ( 'gss_cred_usage_t' , usage ) , oid_ptr , ffi . cast ( 'OM_uint32' , overwrite ) , ffi . cast ( 'OM_uint32' , default ) , elements_stored , usage_stored ) \n    else : \n        if not hasattr ( C , 'gss_store_cred_into' ) : \n            raise NotImplementedError ( \"The GSSAPI implementation does not support \" \"gss_store_cred_into\" ) \n        c_strings , elements , cred_store_kv_set = _make_kv_set ( cred_store ) \n        retval = C . gss_store_cred_into ( minor_status , self . _cred [ 0 ] , ffi . cast ( 'gss_cred_usage_t' , usage ) , oid_ptr , ffi . cast ( 'OM_uint32' , overwrite ) , ffi . cast ( 'OM_uint32' , default ) , cred_store_kv_set , elements_stored , usage_stored ) \n    try : \n        if GSS_ERROR ( retval ) : \n            if oid_ptr : \n                raise _exception_for_status ( retval , minor_status [ 0 ] , oid_ptr ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ 0 ] ) \n    except : \n        if elements_stored [ 0 ] : \n            C . gss_release_oid_set ( minor_status , elements_stored ) \n        raise \n    return_identifier_0 = ( OIDSet ( elements_stored ) , usage_stored [ 0 ] ) \n    return return_identifier_0 "}
{"11448": "\ndef djfrontend_ga ( account = None ) : \n    if account is None : \n        account = getattr ( settings , 'DJFRONTEND_GA' , False ) \n    if account : \n        if getattr ( settings , 'TEMPLATE_DEBUG' , False ) : \n            return_identifier_0 = '' \n            return return_identifier_0 \n        else : \n            if getattr ( settings , 'DJFRONTEND_GA_SETDOMAINNAME' , False ) : \n                if getattr ( settings , 'DJFRONTEND_GA_SETALLOWLINKER' , False ) : \n                    return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"require\", \"linker\");ga(\"linker:autoLink\", [\"%s\"]);ga(\"create\", \"%s\", \"auto\", {\"allowLinker\": true});ga(\"send\", \"pageview\");</script>' % ( settings . DJFRONTEND_GA_SETDOMAINNAME , account ) ) \n                else : \n                    return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"create\", \"%s\", \"%s\");ga(\"send\", \"pageview\");</script>' % ( account , settings . DJFRONTEND_GA_SETDOMAINNAME ) ) \n            else : \n                return mark_safe ( '<script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"//www.google-analytics.com/analytics.js\",\"ga\");ga(\"create\", \"%s\", \"auto\");ga(\"send\", \"pageview\");</script>' % account ) \n    else : \n        return_identifier_1 = '' \n        return return_identifier_1 "}
{"11452": "\ndef get_user_token ( user , purpose , minutes_valid ) : \n    token = '' . join ( dumps ( [ user . get_username ( ) , get_auth_hash ( user , purpose ) , ] ) . encode ( 'base64' ) . split ( '\\n' ) ) \n    return_identifier_0 = { 'id' : get_meteor_id ( user ) , 'token' : token , 'tokenExpires' : calc_expiry_time ( minutes_valid ) , } \n    return return_identifier_0 "}
{"11458": "\ndef check_secure ( ) : \n    if this . request . is_secure ( ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif this . request . META [ 'REMOTE_ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : \n        return_identifier_1 = True \n        return return_identifier_1 \n    raise MeteorError ( 403 , 'Authentication refused without SSL.' ) "}
{"11466": "\ndef change_password ( self , old_password , new_password ) : \n    try : \n        user = this . user \n    except self . user_model . DoesNotExist : \n        self . auth_failed ( ) \n    user = auth . authenticate ( username = user . get_username ( ) , password = self . get_password ( old_password ) , ) \n    if user is None : \n        self . auth_failed ( ) \n    else : \n        user . set_password ( self . get_password ( new_password ) ) \n        user . save ( ) \n        password_changed . send ( sender = __name__ , request = this . request , user = user , ) \n        return_identifier_0 = { \"passwordChanged\" : True } \n        return return_identifier_0 "}
{"11468": "\ndef reset_password ( self , token , new_password ) : \n    user = self . validated_user ( token , purpose = HashPurpose . PASSWORD_RESET , minutes_valid = HASH_MINUTES_VALID [ HashPurpose . PASSWORD_RESET ] , ) \n    user . set_password ( new_password ) \n    user . save ( ) \n    self . do_login ( user ) \n    return_identifier_0 = { \"userId\" : this . user_ddp_id } \n    return return_identifier_0 "}
{"11486": "\ndef state ( self ) : \n    return_identifier_0 = { 'c' : self . c , 's0' : self . s0 , 's1' : self . s1 , 's2' : self . s2 } \n    return return_identifier_0 "}
{"11530": "\ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    print ( \"Running fast forward selection algorithm\" ) \n    number_of_scenarios = scenarios . shape [ 1 ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ 1 / number_of_scenarios for i in range ( 0 , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( 0 , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( 0 , number_of_scenarios ) \n    if number_of_reduced_scenarios >= number_of_scenarios : \n        return_identifier_0 = ( scenarios , probability , J ) \n        return return_identifier_0 \n    for scenario_k in range ( 0 , number_of_scenarios ) : \n        for scenario_u in range ( 0 , number_of_scenarios ) : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n    for scenario_u in range ( 0 , number_of_scenarios ) : \n        summation = 0 \n        for scenario_k in range ( 0 , number_of_scenarios ) : \n            if scenario_k != scenario_u : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n        z [ scenario_u ] = summation \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    for _ in range ( 0 , number_of_scenarios - number_of_reduced_scenarios - 1 ) : \n        print ( \"Running {}\" . format ( _ ) ) \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = 0 \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if c [ deleted_scenario_number , scenario_j ] < lowest_value : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11545": "\ndef upload ( self , title , description = \"\" , keywords = \"\" , developer_tags = None , access_control = AccessControl . Public ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    my_media_group = gdata . media . Group ( title = gdata . media . Title ( text = title ) , description = gdata . media . Description ( description_type = 'plain' , text = description ) , keywords = gdata . media . Keywords ( text = keywords ) , category = [ gdata . media . Category ( text = 'Autos' , scheme = 'http://gdata.youtube.com/schemas/2007/categories.cat' , label = 'Autos' ) ] , ) \n    extension = self . _access_control ( access_control , my_media_group ) \n    video_entry = gdata . youtube . YouTubeVideoEntry ( media = my_media_group , extension_elements = extension ) \n    if developer_tags : \n        video_entry . AddDeveloperTags ( developer_tags ) \n    response = Api . yt_service . GetFormUploadToken ( video_entry ) \n    post_url = response [ 0 ] \n    youtube_token = response [ 1 ] \n    return_identifier_0 = { 'post_url' : post_url , 'youtube_token' : youtube_token } \n    return return_identifier_0 "}
{"11546": "\ndef check_upload_status ( self , video_id ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    entry = self . fetch_video ( video_id ) \n    upload_status = Api . yt_service . CheckUploadStatus ( entry ) \n    if upload_status is not None : \n        video_upload_state = upload_status [ 0 ] \n        detailed_message = upload_status [ 1 ] \n        return_identifier_0 = { \"upload_state\" : video_upload_state , \"detailed_message\" : detailed_message } \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"11548": "\ndef delete_video ( self , video_id ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    entry = self . fetch_video ( video_id ) \n    response = Api . yt_service . DeleteVideoEntry ( entry ) \n    if not response : \n        raise OperationError ( _ ( \"Cannot be deleted from Youtube\" ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"11631": "\ndef credential_property ( cred ) : \n    def getter ( instance ) : \n        return_identifier_0 = \"***obfuscated***\" \n        return return_identifier_0 \n    def setter ( instance , value ) : \n        private = \"_\" + cred \n        instance . __dict__ [ private ] = value \n    return property ( fget = getter , fset = setter ) "}
{"11633": "\ndef validate_input ( msg_type , attr , value ) : \n    try : \n        valid = { \"Email\" : validate_email , \"Twilio\" : validate_twilio , \"SlackWebhook\" : validate_slackwebhook , \"SlackPost\" : validate_slackpost , \"TelegramBot\" : validate_telegrambot , \"WhatsApp\" : validate_whatsapp , } [ msg_type ] ( attr , value ) \n    except KeyError : \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    else : \n        return_identifier_1 = 0 \n        return return_identifier_1 "}
{"11645": "\ndef get_server ( address = None ) : \n    if address : \n        domain = address . split ( \"@\" ) [ 1 ] \n        try : \n            return SMTP_SERVERS [ domain ] \n        except KeyError : \n            return_identifier_0 = ( \"smtp.\" + domain , 465 ) \n            return return_identifier_0 \n    return_identifier_1 = ( None , None ) \n    return return_identifier_1 "}
{"11657": "\ndef validate ( self ) : \n    if PY3 : \n        if not isinstance ( self . vendor , text_type ) : \n            raise ValueError \n        for key , value in self : \n            if not isinstance ( key , text_type ) : \n                raise ValueError \n            if not isinstance ( value , text_type ) : \n                raise ValueError \n    if not isinstance ( self . vendor , text_type ) : \n        try : \n            self . vendor . decode ( 'utf-8' ) \n        except UnicodeDecodeError : \n            raise ValueError \n    for key , value in self . _internal : \n        try : \n            if not is_valid_key ( key ) : \n                raise ValueError \n        except : \n            raise ValueError ( \"%r is not a valid key\" % key ) \n        if not isinstance ( value , text_type ) : \n            try : \n                value . decode ( \"utf-8\" ) \n            except : \n                raise ValueError ( \"%r is not a valid value\" % value ) \n    else : \n        return_identifier_0 = True \n        return return_identifier_0 "}
{"11683": "\ndef get_request_headers ( self ) : \n    date_header = time . asctime ( time . gmtime ( ) ) \n    signing_key = self . _hmac_sign_string ( date_header ) \n    auth_header = \"AWS3-HTTPS AWSAccessKeyId=%s,Algorithm=HmacSHA256,Signature=%s\" % ( self . connection . _aws_access_key_id , signing_key , ) \n    return_identifier_0 = { 'X-Amzn-Authorization' : auth_header , 'x-amz-date' : date_header , 'Host' : 'route53.amazonaws.com' , } \n    return return_identifier_0 "}
{"11705": "\ndef lock ( fileobj ) : \n    try : \n        import fcntl \n    except ImportError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        try : \n            fcntl . lockf ( fileobj , fcntl . LOCK_EX ) \n        except IOError : \n            return_identifier_1 = False \n            return return_identifier_1 \n        else : \n            return_identifier_2 = True \n            return return_identifier_2 "}
{"11710": "\ndef parse_change_info ( e_change_info ) : \n    if e_change_info is None : \n        return e_change_info \n    status = e_change_info . find ( './{*}Status' ) . text \n    submitted_at = e_change_info . find ( './{*}SubmittedAt' ) . text \n    submitted_at = parse_iso_8601_time_str ( submitted_at ) \n    return_identifier_0 = { 'request_id' : id , 'request_status' : status , 'request_submitted_at' : submitted_at } \n    return return_identifier_0 "}
{"11712": "\ndef is_modified ( self ) : \n    for key , val in self . _initial_vals . items ( ) : \n        if getattr ( self , key ) != val : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"11763": "\ndef args ( self ) : \n    if self . _args is None : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    if callable ( self . _args ) : \n        return dict ( self . _args ( ) ) \n    return dict ( self . _args ) "}
{"11765": "\ndef is_current ( self ) : \n    if not self . is_internal : \n        return_identifier_0 = False \n        return return_identifier_0 \n    has_same_endpoint = ( request . endpoint == self . endpoint ) \n    has_same_args = ( request . view_args == self . args ) \n    return has_same_endpoint and has_same_args "}
{"11775": "\ndef get_gecko_params ( request , uid = None , days_back = 0 , cumulative = True , frequency = settings . STATISTIC_FREQUENCY_DAILY , min_val = 0 , max_val = 100 , chart_type = 'standard' , percentage = 'show' , sort = False ) : \n    return_identifier_0 = { 'days_back' : int ( request . GET . get ( 'daysback' , days_back ) ) , 'uid' : request . GET . get ( 'uid' , uid ) , 'uids' : get_GET_array ( request , 'uids[]' ) , 'cumulative' : get_GET_bool ( request , 'cumulative' , cumulative ) , 'frequency' : request . GET . get ( 'frequency' , frequency ) , 'min' : request . GET . get ( 'min' , min_val ) , 'max' : request . GET . get ( 'max' , max_val ) , 'type' : request . GET . get ( 'type' , chart_type ) , 'percentage' : request . GET . get ( 'percentage' , percentage ) , 'sort' : get_GET_bool ( request , 'sort' , sort ) , } \n    return return_identifier_0 "}
{"11776": "\ndef geckoboard_number_widget ( request ) : \n    params = get_gecko_params ( request , days_back = 7 ) \n    metric = Metric . objects . get ( uid = params [ 'uid' ] ) \n    try : \n        latest_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order_by ( '-date_time' ) [ 0 ] \n    except IndexError : \n        return_identifier_0 = ( 0 , 0 ) \n        return return_identifier_0 \n    try : \n        prev_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date_time__lte = latest_stat . date_time - timedelta ( days = params [ 'days_back' ] ) ) . order_by ( '-date_time' ) [ 0 ] \n    except IndexError : \n        return ( latest_stat . cumulative_count , 0 ) if params [ 'cumulative' ] else ( latest_stat . count , 0 ) \n    return ( latest_stat . cumulative_count , prev_stat . cumulative_count ) if params [ 'cumulative' ] else ( latest_stat . count , prev_stat . count ) "}
{"11779": "\ndef geckoboard_geckometer ( request ) : \n    params = get_gecko_params ( request , cumulative = True ) \n    metric = Metric . objects . get ( uid = params [ 'uid' ] ) \n    return_identifier_0 = ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] ) \n    return return_identifier_0 "}
{"11780": "\ndef geckoboard_funnel ( request , frequency = settings . STATISTIC_FREQUENCY_DAILY ) : \n    params = get_gecko_params ( request , cumulative = True ) \n    metrics = Metric . objects . filter ( uid__in = params [ 'uids' ] ) \n    items = [ ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] \n    return_identifier_0 = { 'items' : items , 'type' : params [ 'type' ] , 'percentage' : params [ 'percentage' ] , 'sort' : params [ 'sort' ] , } \n    return return_identifier_0 "}
{"11795": "\ndef weather ( self ) : \n    doc = self . get_doc ( ) \n    table = doc ( 'table#game_info' ) \n    giTable = sportsref . utils . parse_info_table ( table ) \n    if 'weather' in giTable : \n        regex = ( r'(?:(?P<temp>\\-?\\d+) degrees )?' r'(?:relative humidity (?P<relHumidity>\\d+)%, )?' r'(?:wind (?P<windMPH>\\d+) mph, )?' r'(?:wind chill (?P<windChill>\\-?\\d+))?' ) \n        m = re . match ( regex , giTable [ 'weather' ] ) \n        d = m . groupdict ( ) \n        for k in d : \n            try : \n                d [ k ] = int ( d [ k ] ) \n            except TypeError : \n                pass \n        d [ 'windChill' ] = ( d [ 'windChill' ] if pd . notnull ( d [ 'windChill' ] ) else d [ 'temp' ] ) \n        d [ 'windMPH' ] = d [ 'windMPH' ] if pd . notnull ( d [ 'windMPH' ] ) else 0 \n        return d \n    else : \n        return_identifier_0 = { 'temp' : 70 , 'windChill' : 70 , 'relHumidity' : None , 'windMPH' : 0 } \n        return return_identifier_0 "}
{"11850": "\ndef _read_varint ( self ) : \n    buff = self . _fd . read ( 1 ) \n    if buff == b'' : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 == 1 : \n        new_byte = self . _fd . read ( 1 ) \n        if new_byte == b'' : \n            raise EOFError ( 'unexpected EOF.' ) \n        buff += new_byte \n    varint , _ = decodeVarint ( buff , 0 ) \n    return varint "}
{"11858": "\ndef evaluate_traits ( self ) : \n    return_value = [ ] \n    for trait in self . traits : \n        if not trait . condition ( ) : \n            if not self . traits_eager_evaluation : \n                return_identifier_0 = [ trait . description ] \n                return return_identifier_0 \n            else : \n                return_value . append ( trait . description ) \n    return return_value "}
{"11860": "\ndef until_traits_are_present ( self , element_with_traits ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    missing_traits_descriptions = None \n    while True : \n        missing_traits_descriptions = [ ] \n        try : \n            missing_traits_descriptions = element_with_traits . evaluate_traits ( ) \n            if len ( missing_traits_descriptions ) == 0 : \n                return_identifier_0 = True \n                return return_identifier_0 \n            else : \n                logger . debug ( \"#{0} - wait until all traits are present: <{1}>\" . format ( str ( count ) , '> <' . join ( missing_traits_descriptions ) ) ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0}: {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n            pass \n        time . sleep ( self . _poll ) \n        count += 1 \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"conditions \" + '<' + '> <' . join ( missing_traits_descriptions ) + '>' + \" not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11865": "\ndef status ( self ) : \n    nad_reply = self . _send ( self . POLL_VOLUME + self . POLL_POWER + self . POLL_MUTED + self . POLL_SOURCE , read_reply = True ) \n    if nad_reply is None : \n        return \n    num_chars = 10 \n    nad_status = [ nad_reply [ i : i + num_chars ] for i in range ( 0 , len ( nad_reply ) , num_chars ) ] \n    return_identifier_0 = { 'volume' : int ( nad_status [ 0 ] [ - 2 : ] , 16 ) , 'power' : nad_status [ 1 ] [ - 2 : ] == '01' , 'muted' : nad_status [ 2 ] [ - 2 : ] == '01' , 'source' : self . SOURCES_REVERSED [ nad_status [ 3 ] [ - 2 : ] ] } \n    return return_identifier_0 "}
{"11875": "\ndef check_if_song_name ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    a_list = soup . findAll ( 'a' , 'touch' ) \n    text = [ str ( x ) for x in a_list ] \n    text = '' . join ( text ) \n    text = text . lower ( ) \n    string1 = 'download in 48 kbps' \n    string2 = 'download in 128 kbps' \n    string3 = 'download in 320 kbps' \n    href = '' \n    if string3 in text : \n        href = a_list [ 2 ] . get ( 'href' ) \n    elif string2 in text : \n        href = a_list [ 1 ] . get ( 'href' ) \n    elif string1 in text : \n        href = a_list [ 0 ] . get ( 'href' ) \n    else : \n        return_identifier_0 = ( True , 'nothing' ) \n        return return_identifier_0 \n    return_identifier_1 = ( False , href ) \n    return return_identifier_1 "}
{"11883": "\ndef findStationCodesByCity ( city_name , token ) : \n    req = requests . get ( API_ENDPOINT_SEARCH , params = { 'token' : token , 'keyword' : city_name } ) \n    if req . status_code == 200 and req . json ( ) [ \"status\" ] == \"ok\" : \n        return [ result [ \"uid\" ] for result in req . json ( ) [ \"data\" ] ] \n    else : \n        return_identifier_0 = [ ] \n        return return_identifier_0 "}
{"11884": "\ndef get_location_observation ( lat , lng , token ) : \n    req = requests . get ( API_ENDPOINT_GEO % ( lat , lng ) , params = { 'token' : token } ) \n    if req . status_code == 200 and req . json ( ) [ \"status\" ] == \"ok\" : \n        return parse_observation_response ( req . json ( ) [ \"data\" ] ) \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"11886": "\ndef get_station_observation ( station_code , token ) : \n    req = requests . get ( API_ENDPOINT_OBS % ( station_code ) , params = { 'token' : token } ) \n    if req . status_code == 200 and req . json ( ) [ 'status' ] == \"ok\" : \n        return parse_observation_response ( req . json ( ) [ 'data' ] ) \n    else : \n        return_identifier_0 = { } \n        return return_identifier_0 "}
{"11983": "\ndef choice_field_data ( field , ** kwargs ) : \n    if field . choices : \n        return str ( random . choice ( list ( valid_choices ( field . choices ) ) ) ) \n    return_identifier_0 = 'None' \n    return return_identifier_0 "}
{"11984": "\ndef multiple_choice_field_data ( field , ** kwargs ) : \n    if field . choices : \n        from django_any . functions import valid_choices \n        l = list ( valid_choices ( field . choices ) ) \n        random . shuffle ( l ) \n        choices = [ ] \n        count = xunit . any_int ( min_value = 1 , max_value = len ( field . choices ) ) \n        for i in xrange ( 0 , count ) : \n            choices . append ( l [ i ] ) \n        return ' ' . join ( choices ) \n    return_identifier_0 = 'None' \n    return return_identifier_0 "}
{"12005": "\ndef repository_exists ( self , workspace , repo ) : \n    if not self . exists ( workspace ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    workspaces = self . list ( ) \n    return repo in workspaces [ workspace ] [ \"repositories\" ] "}
{"12018": "\ndef print_workspace ( self , name ) : \n    path_list = find_path ( name , self . config ) \n    if len ( path_list ) == 0 : \n        self . logger . error ( \"No matches for `%s`\" % name ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    for name , path in path_list . items ( ) : \n        self . print_status ( name , path ) "}
{"12021": "\ndef _post_cutout_no_chunking_blosc ( self , token , channel , x_start , y_start , z_start , data , resolution ) : \n    data = numpy . expand_dims ( data , axis = 0 ) \n    blosc_data = blosc . pack_array ( data ) \n    url = self . url ( \"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\" . format ( token , channel , resolution , x_start , x_start + data . shape [ 3 ] , y_start , y_start + data . shape [ 2 ] , z_start , z_start + data . shape [ 1 ] ) ) \n    req = self . remote_utils . post_url ( url , data = blosc_data , headers = { 'Content-Type' : 'application/octet-stream' } ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( req . text ) \n    else : \n        return_identifier_0 = True \n        return return_identifier_0 "}
{"12031": "\ndef merge_ids ( self , token , channel , ids , delete = False ) : \n    url = self . url ( ) + \"/merge/{}/\" . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( 'Could not merge ids {}' . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) ) \n    if delete : \n        self . delete_ramon ( token , channel , ids [ 1 : ] ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12032": "\ndef propagate ( self , token , channel ) : \n    if self . get_propagate_status ( token , channel ) != u'0' : \n        return \n    url = self . url ( 'sd/{}/{}/setPropagate/1/' . format ( token , channel ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( 'Propagate fail: {}' . format ( req . text ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12041": "\ndef _guess_format_from_extension ( ext ) : \n    ext = ext . strip ( '.' ) \n    formats = [ ] \n    for fmt in FILE_FORMATS : \n        if ext in FILE_FORMATS [ fmt ] : \n            formats . append ( fmt ) \n    if formats == [ ] or len ( formats ) > 1 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return formats [ 0 ] "}
{"12043": "\ndef convert ( in_file , out_file , in_fmt = \"\" , out_fmt = \"\" ) : \n    in_file = os . path . expanduser ( in_file ) \n    out_file = os . path . expanduser ( out_file ) \n    if not os . path . exists ( in_file ) : \n        raise IOError ( \"Input file {0} does not exist, stopping...\" . format ( in_file ) ) \n    in_fmt = in_fmt . lower ( ) or _guess_format_from_extension ( in_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    out_fmt = out_fmt . lower ( ) or _guess_format_from_extension ( out_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    if not in_fmt or not out_fmt : \n        raise ValueError ( \"Cannot determine conversion formats.\" ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if in_fmt is out_fmt : \n        shutil . copyfileobj ( in_file , out_file ) \n        return out_file \n    if in_fmt == 'hdf5' : \n        from . import hdf5 \n        data = hdf5 . load ( in_file ) \n    elif in_fmt == 'tiff' : \n        from . import tiff \n        data = tiff . load ( in_file ) \n    elif in_fmt == 'png' : \n        from . import png \n        data = png . load ( in_file ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    if out_fmt == 'hdf5' : \n        from . import hdf5 \n        return hdf5 . save ( out_file , data ) \n    elif out_fmt == 'tiff' : \n        from . import tiff \n        return tiff . save ( out_file , data ) \n    elif out_fmt == 'png' : \n        from . import png \n        return png . export_png ( out_file , data ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    return _fail_pair_conversion ( in_fmt , out_fmt ) "}
{"12057": "\ndef find_path ( name , config , wsonly = False ) : \n    workspace = Workspace ( config ) \n    config = config [ \"workspaces\" ] \n    path_list = { } \n    if name . find ( '/' ) != - 1 : \n        wsonly = False \n        try : \n            ws , repo = name . split ( '/' ) \n        except ValueError : \n            raise ValueError ( \"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\" ) \n        if ( workspace . exists ( ws ) ) : \n            if ( repo in config [ ws ] [ \"repositories\" ] ) : \n                path_name = \"%s/%s\" % ( ws , repo ) \n                path_list [ path_name ] = config [ ws ] [ \"repositories\" ] [ repo ] \n    for ws_name , ws in sorted ( config . items ( ) ) : \n        if ( name == ws_name ) : \n            if wsonly is True : \n                return_identifier_0 = { ws_name : ws [ \"path\" ] } \n                return return_identifier_0 \n            repositories = sorted ( config [ ws_name ] [ \"repositories\" ] . items ( ) ) \n            for name , path in repositories : \n                path_list [ \"%s/%s\" % ( ws_name , name ) ] = path \n            break \n        for repo_name , repo_path in sorted ( ws [ \"repositories\" ] . items ( ) ) : \n            if ( repo_name == name ) : \n                path_list [ \"%s/%s\" % ( ws_name , repo_name ) ] = repo_path \n    return path_list "}
{"12070": "\ndef _percent ( data , part , total ) : \n    try : \n        return round ( 100 * float ( data [ part ] ) / float ( data [ total ] ) , 1 ) \n    except ZeroDivisionError : \n        return_identifier_0 = 0 \n        return return_identifier_0 "}
{"12103": "\ndef _resolve_path ( self , create = False ) : \n    if type ( self . _path ) == str : \n        key_path = self . _path . split ( '.' ) \n    else : \n        key_path = [ self . _path ] \n    node = self . _root . _data \n    nodes = [ self . _root . _data ] \n    while len ( key_path ) : \n        key = key_path . pop ( 0 ) \n        try : \n            key = int ( key ) \n        except : \n            pass \n        if create : \n            if type ( node ) == dict and key not in node : \n                node [ key ] = { } \n            elif type ( node ) == list and type ( key ) == int and len ( node ) < key : \n                node . append ( [ None for i in range ( key - len ( node ) ) ] ) \n        nodes . append ( node ) \n        try : \n            node = node [ key ] \n        except TypeError : \n            if type ( key ) == int : \n                raise IndexError ( key ) \n            else : \n                raise KeyError ( key ) \n    return_identifier_0 = ( nodes [ - 1 ] , key ) \n    return return_identifier_0 "}
{"12112": "\ndef read_socket_input ( connection , socket_obj ) : \n    count = connection . needs_input \n    if count <= 0 : \n        return count \n    while True : \n        try : \n            sock_data = socket_obj . recv ( count ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return_identifier_0 = 0 \n                return return_identifier_0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if len ( sock_data ) > 0 : \n        count = connection . process_input ( sock_data ) \n    else : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_input ( ) \n        connection . close_output ( ) \n    return count "}
{"12113": "\ndef write_socket_output ( connection , socket_obj ) : \n    count = connection . has_output \n    if count <= 0 : \n        return count \n    data = connection . output_data ( ) \n    if not data : \n        return Connection . EOS \n    while True : \n        try : \n            count = socket_obj . send ( data ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return_identifier_0 = 0 \n                return return_identifier_0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if count > 0 : \n        connection . output_written ( count ) \n    elif data : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_output ( ) \n        connection . close_input ( ) \n    return count "}
{"12133": "\ndef need_processing ( self ) : \n    readers = [ ] \n    writers = [ ] \n    timer_heap = [ ] \n    for c in iter ( self . _connections . values ( ) ) : \n        if c . needs_input > 0 : \n            readers . append ( c ) \n        if c . has_output > 0 : \n            writers . append ( c ) \n        if c . deadline : \n            heapq . heappush ( timer_heap , ( c . next_tick , c ) ) \n    timers = [ ] \n    while timer_heap : \n        x = heapq . heappop ( timer_heap ) \n        timers . append ( x [ 1 ] ) \n    return_identifier_0 = ( readers , writers , timers ) \n    return return_identifier_0 "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return_identifier_1 = 0 \n        return return_identifier_1 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( _PROTON_VERSION < ( 0 , 10 ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = True \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12167": "\ndef rotatePoint ( self , pointX , pointY ) : \n    if ( self . angle == 0 or self . angle == None ) : \n        return_identifier_0 = ( pointX , pointY ) \n        return return_identifier_0 \n    length = math . sqrt ( ( pointX - self . xll ) ** 2 + ( pointY - self . yll ) ** 2 ) \n    beta = math . acos ( ( pointX - self . xll ) / length ) \n    if ( pointY < self . yll ) : \n        beta = math . pi * 2 - beta \n    offsetX = math . cos ( beta ) * length - math . cos ( self . _angle_rd + beta ) * length \n    offsetY = math . sin ( self . _angle_rd + beta ) * length - math . sin ( beta ) * length \n    return_identifier_1 = ( pointX - offsetX , pointY + offsetY ) \n    return return_identifier_1 "}
{"12181": "\ndef x_fit ( self , test_length ) : \n    if ( self . x + test_length ) >= self . xmax : \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"12182": "\ndef y_fit ( self , test_length ) : \n    if ( self . y + test_length ) >= self . ymax : \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"12183": "\ndef x_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if self . x > test_ordinate . x : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"12184": "\ndef y_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if self . y > test_ordinate . y : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"12247": "\ndef xfeature_compress_gzip ( self , terminator = False ) : \n    args = \"TERMINATOR\" if terminator else None \n    code , message = self . command ( \"XFEATURE COMPRESS GZIP\" , args ) \n    if code != 290 : \n        raise NNTPReplyError ( code , message ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12248": "\ndef post ( self , headers = { } , body = \"\" ) : \n    code , message = self . command ( \"POST\" ) \n    if code != 340 : \n        raise NNTPReplyError ( code , message ) \n    hdrs = utils . unparse_headers ( headers ) \n    self . socket . sendall ( hdrs ) \n    if isinstance ( body , basestring ) : \n        body = cStringIO . StringIO ( body ) \n    illegal = False \n    for line in body : \n        if line . startswith ( \".\" ) : \n            line = \".\" + line \n        if line . endswith ( \"\\r\\n\" ) : \n            line = line [ : - 2 ] \n        elif line . endswith ( \"\\n\" ) : \n            line = line [ : - 1 ] \n        if any ( c in line for c in \"\\0\\r\" ) : \n            illegal = True \n            break \n        self . socket . sendall ( line + \"\\r\\n\" ) \n    self . socket . sendall ( \".\\r\\n\" ) \n    code , message = self . status ( ) \n    if illegal : \n        raise NNTPDataError ( \"Illegal characters found\" ) \n    if code != 240 : \n        raise NNTPReplyError ( code , message ) \n    message_id = message . split ( None , 1 ) [ 0 ] \n    if message_id . startswith ( \"<\" ) and message_id . endswith ( \">\" ) : \n        return message_id \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12249": "\ndef _offset ( value ) : \n    o = int ( value ) \n    if o == 0 : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    a = abs ( o ) \n    s = a * 36 + ( a % 100 ) * 24 \n    return ( o // a ) * s "}
{"12265": "\ndef parse_header ( line ) : \n    if not line or line == \"\\r\\n\" : \n        return None \n    if line [ 0 ] in \" \\t\" : \n        return line [ 1 : ] . rstrip ( ) \n    name , value = line . split ( \":\" , 1 ) \n    return_identifier_0 = ( name . strip ( ) , value . strip ( ) ) \n    return return_identifier_0 "}
{"12276": "\ndef symmetric_error ( self ) : \n    if self . __errors__ is None : \n        return_identifier_0 = 0. \n        return return_identifier_0 \n    if np . isscalar ( self . __errors__ ) : \n        return self . __errors__ \n    return 0.5 * ( self . __errors__ [ 0 ] + self . __errors__ [ 1 ] ) "}
{"12324": "\ndef _get_imports ( self ) : \n    import_directives = [ d for d in self . directives if d . name == \"import\" ] \n    if import_directives : \n        return \"\\n\" + \"\\n\" . join ( d . args [ \"value\" ] for d in import_directives ) \n    else : \n        return_identifier_0 = \"\" \n        return return_identifier_0 "}
{"12330": "\ndef _get_rule_transform ( self , rule ) : \n    rd = self . _find_directive ( lambda d : d . name == \"rule\" and d . args . get ( \"name\" ) == rule . name ) \n    if rd : \n        args = rd . args \n    else : \n        args = { } \n    transform = args . get ( \"transform\" , \"retype\" ) \n    if transform == \"retype\" : \n        new_name = args . get ( \"to_type\" , \"TokenType.{0}\" . format ( rule . name ) ) \n        return \".retyped({0})\" . format ( new_name ) \n    elif transform == \"compress\" : \n        new_name = args . get ( \"to_type\" , \"TokenType.{0}\" . format ( rule . name ) ) \n        if new_name == \"identity\" : \n            return_identifier_0 = \".compressed()\" \n            return return_identifier_0 \n        else : \n            return \".compressed({0})\" . format ( new_name ) \n    elif transform == \"identity\" : \n        return_identifier_1 = \"\" \n        return return_identifier_1 "}
{"12337": "\ndef _ast_terminal_to_code ( self , terminal , ** kwargs ) : \n    value = _replace ( terminal . value ) \n    if self . use_terminal_shorthand : \n        return_identifier_0 = [ value ] \n        return return_identifier_0 \n    else : \n        return_identifier_1 = [ \"terminal({})\" . format ( value ) ] \n        return return_identifier_1 "}
{"12340": "\ndef _ast_special_handling_to_code ( self , special_handling , ** kwargs ) : \n    ident = special_handling . value . svalue \n    if ident in PB_SPECIAL_HANDLING : \n        return_identifier_0 = [ \"PB.{0}\" . format ( ident ) ] \n        return return_identifier_0 \n    else : \n        return_identifier_1 = [ \"self.{0}\" . format ( ident ) ] \n        return return_identifier_1 "}
{"12354": "\ndef _validate_arguments ( self ) : \n    if self . _email is None : \n        self . set_error_message ( \"E-mail for the account not provided\" ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if self . _api_token is None : \n        self . set_error_message ( \"API Token for the account not provided\" ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"12361": "\ndef getFieldsColumnLengths ( self ) : \n    nameLen = 0 \n    descLen = 0 \n    for f in self . fields : \n        nameLen = max ( nameLen , len ( f [ 'title' ] ) ) \n        descLen = max ( descLen , len ( f [ 'description' ] ) ) \n    return_identifier_0 = ( nameLen , descLen ) \n    return return_identifier_0 "}
{"12362": "\ndef getMetricsColumnLengths ( self ) : \n    displayLen = 0 \n    descLen = 0 \n    for m in self . metrics : \n        displayLen = max ( displayLen , len ( m [ 'displayName' ] ) ) \n        descLen = max ( descLen , len ( m [ 'description' ] ) ) \n    return_identifier_0 = ( displayLen , descLen ) \n    return return_identifier_0 "}
{"12386": "\ndef is_type ( self , value ) : \n    if isinstance ( value , tuple ) : \n        for opt in value : \n            if self . node_type == opt : \n                return_identifier_0 = True \n                return return_identifier_0 \n        return_identifier_1 = False \n        return return_identifier_1 \n    else : \n        return self . node_type == value "}
{"12399": "\ndef save_context ( self ) -> bool : \n    self . _contexts . append ( self . _cursor . position ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12400": "\ndef restore_context ( self ) -> bool : \n    self . _cursor . position = self . _contexts . pop ( ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"12414": "\ndef add ( self , it : Signature ) -> bool : \n    if isinstance ( it , Scope ) : \n        it . state = StateScope . EMBEDDED \n    txt = it . internal_name ( ) \n    it . set_parent ( self ) \n    if self . is_namespace : \n        txt = it . internal_name ( ) \n    if txt == \"\" : \n        txt = '_' + str ( len ( self . _hsig ) ) \n    if txt in self . _hsig : \n        raise KeyError ( \"Already exists %s\" % txt ) \n    self . _hsig [ txt ] = it \n    self . __update_count ( ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12415": "\ndef remove ( self , it : Signature ) -> bool : \n    txt = it . internal_name ( ) \n    if txt not in self . _hsig : \n        raise KeyError ( it . show_name ( ) + ' not in Set' ) \n    sig = self . _hsig [ txt ] \n    if isinstance ( sig , Scope ) : \n        sig . state = StateScope . LINKED \n    del self . _hsig [ txt ] \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12416": "\ndef discard ( self , it : Signature ) -> bool : \n    txt = it . internal_name ( ) \n    if txt in self . _hsig : \n        sig = self . _hsig [ txt ] \n        if isinstance ( sig , Scope ) : \n            sig . state = StateScope . LINKED \n        del self . _hsig [ txt ] \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"12428": "\ndef _hit_ok ( hit , min_hit_charge , max_hit_charge ) : \n    if hit [ 'charge' ] < min_hit_charge : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if max_hit_charge != 0 and hit [ 'charge' ] > max_hit_charge : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"12448": "\ndef bind ( self , dst : str , src : Node ) -> bool : \n    for m in self . rule_nodes . maps : \n        for k , v in m . items ( ) : \n            if k == dst : \n                m [ k ] = src \n                return_identifier_0 = True \n                return return_identifier_0 \n    raise Exception ( '%s not found' % dst ) "}
{"12449": "\ndef read_eol ( self ) -> bool : \n    if self . read_eof ( ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _stream . save_context ( ) \n    self . read_char ( '\\r' ) \n    if self . read_char ( '\\n' ) : \n        return self . _stream . validate_context ( ) \n    return self . _stream . restore_context ( ) "}
{"12450": "\ndef push_rule_nodes ( self ) -> bool : \n    if self . rule_nodes is None : \n        self . rule_nodes = collections . ChainMap ( ) \n        self . tag_cache = collections . ChainMap ( ) \n        self . id_cache = collections . ChainMap ( ) \n    else : \n        self . rule_nodes = self . rule_nodes . new_child ( ) \n        self . tag_cache = self . tag_cache . new_child ( ) \n        self . id_cache = self . id_cache . new_child ( ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12451": "\ndef pop_rule_nodes ( self ) -> bool : \n    self . rule_nodes = self . rule_nodes . parents \n    self . tag_cache = self . tag_cache . parents \n    self . id_cache = self . id_cache . parents \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12452": "\ndef value ( self , n : Node ) -> str : \n    id_n = id ( n ) \n    idcache = self . id_cache \n    if id_n not in idcache : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    name = idcache [ id_n ] \n    tag_cache = self . tag_cache \n    if name not in tag_cache : \n        raise Exception ( \"Incoherent tag cache\" ) \n    tag = tag_cache [ name ] \n    k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n    valcache = self . _streams [ - 1 ] . value_cache \n    if k not in valcache : \n        valcache [ k ] = str ( tag ) \n    return valcache [ k ] "}
{"12454": "\ndef begin_tag ( self , name : str ) -> Node : \n    self . tag_cache [ name ] = Tag ( self . _stream , self . _stream . index ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12455": "\ndef end_tag ( self , name : str ) -> Node : \n    self . tag_cache [ name ] . set_end ( self . _stream . index ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12456": "\ndef set_rules ( cls , rules : dict ) -> bool : \n    cls . _rules = cls . _rules . new_child ( ) \n    for rule_name , rule_pt in rules . items ( ) : \n        if '.' not in rule_name : \n            rule_name = cls . __module__ + '.' + cls . __name__ + '.' + rule_name \n        meta . set_one ( cls . _rules , rule_name , rule_pt ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12457": "\ndef set_hooks ( cls , hooks : dict ) -> bool : \n    cls . _hooks = cls . _hooks . new_child ( ) \n    for hook_name , hook_pt in hooks . items ( ) : \n        if '.' not in hook_name : \n            hook_name = cls . __module__ + '.' + cls . __name__ + '.' + hook_name \n        meta . set_one ( cls . _hooks , hook_name , hook_pt ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12458": "\ndef set_directives ( cls , directives : dict ) -> bool : \n    meta . _directives = meta . _directives . new_child ( ) \n    for dir_name , dir_pt in directives . items ( ) : \n        meta . set_one ( meta . _directives , dir_name , dir_pt ) \n        dir_pt . ns_name = dir_name \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12461": "\ndef peek_text ( self , text : str ) -> bool : \n    start = self . _stream . index \n    stop = start + len ( text ) \n    if stop > self . _stream . eos_index : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return self . _stream [ self . _stream . index : stop ] == text "}
{"12462": "\ndef one_char ( self ) -> bool : \n    if self . read_eof ( ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _stream . incpos ( ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"12463": "\ndef read_char ( self , c : str ) -> bool : \n    if self . read_eof ( ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _stream . save_context ( ) \n    if c == self . _stream . peek_char : \n        self . _stream . incpos ( ) \n        return self . _stream . validate_context ( ) \n    return self . _stream . restore_context ( ) "}
{"12464": "\ndef read_until_eof ( self ) -> bool : \n    if self . read_eof ( ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    self . _stream . save_context ( ) \n    while not self . read_eof ( ) : \n        self . _stream . incpos ( ) \n    return self . _stream . validate_context ( ) "}
{"12469": "\ndef add_ruleclause_name ( self , ns_name , rid ) -> bool : \n    ns_name . parser_tree = parsing . Rule ( self . value ( rid ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12470": "\ndef add_rules ( self , bnf , r ) -> bool : \n    bnf [ r . rulename ] = r . parser_tree \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12471": "\ndef add_rule ( self , rule , rn , alts ) -> bool : \n    rule . rulename = self . value ( rn ) \n    rule . parser_tree = alts . parser_tree \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12472": "\ndef add_sequences ( self , sequences , cla ) -> bool : \n    if not hasattr ( sequences , 'parser_tree' ) : \n        sequences . parser_tree = cla . parser_tree \n    else : \n        oldnode = sequences \n        if isinstance ( oldnode . parser_tree , parsing . Seq ) : \n            oldpt = list ( oldnode . parser_tree . ptlist ) \n        else : \n            oldpt = [ oldnode . parser_tree ] \n        oldpt . append ( cla . parser_tree ) \n        sequences . parser_tree = parsing . Seq ( * tuple ( oldpt ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12473": "\ndef add_alt ( self , alternatives , alt ) -> bool : \n    if not hasattr ( alternatives , 'parser_tree' ) : \n        if hasattr ( alt , 'parser_tree' ) : \n            alternatives . parser_tree = alt . parser_tree \n        else : \n            alternatives . parser_tree = alt \n    else : \n        oldnode = alternatives \n        if isinstance ( oldnode . parser_tree , parsing . Alt ) : \n            oldpt = list ( oldnode . parser_tree . ptlist ) \n        else : \n            oldpt = [ oldnode . parser_tree ] \n        oldpt . append ( alt . parser_tree ) \n        alternatives . parser_tree = parsing . Alt ( * tuple ( oldpt ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12474": "\ndef add_range ( self , sequence , begin , end ) : \n    sequence . parser_tree = parsing . Range ( self . value ( begin ) . strip ( \"'\" ) , self . value ( end ) . strip ( \"'\" ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12475": "\ndef add_rpt ( self , sequence , mod , pt ) : \n    modstr = self . value ( mod ) \n    if modstr == '!!' : \n        self . _stream . restore_context ( ) \n        self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a lookahead rule\" , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) \n        raise self . diagnostic \n    if modstr == '!' : \n        self . _stream . restore_context ( ) \n        self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a negated rule\" , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) \n        raise self . diagnostic \n    oldnode = sequence \n    sequence . parser_tree = pt . functor ( oldnode . parser_tree ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12476": "\ndef add_capture ( self , sequence , cpt ) : \n    cpt_value = self . value ( cpt ) \n    sequence . parser_tree = parsing . Capture ( cpt_value , sequence . parser_tree ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12477": "\ndef add_bind ( self , sequence , cpt ) : \n    cpt_value = self . value ( cpt ) \n    sequence . parser_tree = parsing . Bind ( cpt_value , sequence . parser_tree ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12478": "\ndef add_hook ( self , sequence , h ) : \n    sequence . parser_tree = parsing . Hook ( h . name , h . listparam ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12479": "\ndef param_num ( self , param , n ) : \n    param . pair = ( int ( self . value ( n ) ) , int ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12480": "\ndef param_str ( self , param , s ) : \n    param . pair = ( self . value ( s ) . strip ( '\"' ) , str ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12481": "\ndef param_char ( self , param , c ) : \n    param . pair = ( self . value ( c ) . strip ( \"'\" ) , str ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12482": "\ndef param_id ( self , param , i ) : \n    param . pair = ( self . value ( i ) , parsing . Node ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12483": "\ndef hook_name ( self , hook , n ) : \n    hook . name = self . value ( n ) \n    hook . listparam = [ ] \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12484": "\ndef hook_param ( self , hook , p ) : \n    hook . listparam . append ( p . pair ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12498": "\ndef dump_nodes ( self ) : \n    print ( \"DUMP NODE LOCAL INFOS\" ) \n    try : \n        print ( \"map Id->node name\" ) \n        for k , v in self . id_cache . items ( ) : \n            print ( \"[%d]=%s\" % ( k , v ) ) \n        print ( \"map tag->capture infos\" ) \n        for k , v in self . tag_cache . items ( ) : \n            print ( \"[%s]=%s\" % ( k , v ) ) \n        print ( \"map nodes->tag resolution\" ) \n        for k , v in self . rule_nodes . items ( ) : \n            txt = \"['%s']=%d\" % ( k , id ( v ) ) \n            if k in self . tag_cache : \n                tag = self . tag_cache [ k ] \n                txt += \" tag <%s>\" % tag \n                k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n                if k in self . _stream . value_cache : \n                    txt += \" cache <%s>\" % self . _stream . value_cache [ k ] \n            print ( txt ) \n    except Exception as err : \n        print ( \"RECV Exception %s\" % err ) \n    import sys \n    sys . stdout . flush ( ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12501": "\ndef _clause ( self , pt : parsing . ParserTree ) -> [ ast . stmt ] : \n    if isinstance ( pt , list ) : \n        return pt \n    return_identifier_0 = [ ast . If ( ast . UnaryOp ( ast . Not ( ) , pt ) , [ self . __exit_scope ( ) ] , [ ] ) ] \n    return return_identifier_0 "}
{"12508": "\ndef visit_Alt ( self , node : parsing . Alt ) -> [ ast . stmt ] : \n    clauses = [ self . visit ( clause ) for clause in node . ptlist ] \n    for clause in clauses : \n        if not isinstance ( clause , ast . expr ) : \n            break \n    else : \n        return ast . BoolOp ( ast . Or ( ) , clauses ) \n    res = ast . Try ( [ ] , [ ast . ExceptHandler ( ast . Name ( 'AltTrue' , ast . Load ( ) ) , None , [ ast . Pass ( ) ] ) ] , [ ] , [ ] ) \n    alt_true = [ ast . Raise ( ast . Call ( ast . Name ( 'AltTrue' , ast . Load ( ) ) , [ ] , [ ] , None , None ) , None ) ] \n    alt_false = [ ast . ExceptHandler ( ast . Name ( 'AltFalse' , ast . Load ( ) ) , None , [ ast . Pass ( ) ] ) ] \n    self . in_try += 1 \n    for clause in node . ptlist : \n        res . body . append ( ast . Try ( self . _clause ( self . visit ( clause ) ) + alt_true , alt_false , [ ] , [ ] ) ) \n    self . in_try -= 1 \n    res . body . append ( self . __exit_scope ( ) ) \n    return_identifier_0 = [ res ] \n    return return_identifier_0 "}
{"12511": "\ndef visit_Rep0N ( self , node : parsing . Rep0N ) -> [ ast . stmt ] : \n    cl_ast = self . visit ( node . pt ) \n    if isinstance ( cl_ast , ast . expr ) : \n        return_identifier_0 = [ ast . While ( cl_ast , [ ast . Pass ( ) ] , [ ] ) ] \n        return return_identifier_0 \n    self . in_loop += 1 \n    clause = self . _clause ( self . visit ( node . pt ) ) \n    self . in_loop -= 1 \n    return_identifier_1 = [ ast . While ( ast . Name ( 'True' , ast . Load ( ) ) , clause , [ ] ) ] \n    return return_identifier_1 "}
{"12516": "\ndef echo_nodes ( self , * rest ) : \n    txt = \"\" \n    for thing in rest : \n        if isinstance ( thing , Node ) : \n            txt += self . value ( thing ) \n        else : \n            txt += str ( thing ) \n    print ( txt ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12525": "\ndef set_node ( self , dst , src ) : \n    if not isinstance ( src , Node ) : \n        dst . value = src \n    else : \n        dst . set ( src ) \n        idsrc = id ( src ) \n        iddst = id ( dst ) \n        if iddst not in self . id_cache : \n            print ( \"DST: %s\" % repr ( dst ) ) \n            print ( \"RULE_NODES %s\" % repr ( self . rule_nodes ) ) \n            print ( \"IDCACHE %s\" % repr ( self . id_cache ) ) \n        if idsrc in self . id_cache : \n            k = self . id_cache [ idsrc ] \n            k2 = self . id_cache [ iddst ] \n            if k in self . rule_nodes : \n                self . tag_cache [ k2 ] = self . tag_cache [ k ] \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12526": "\ndef set_node_as_int ( self , dst , src ) : \n    dst . value = self . value ( src ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12527": "\ndef get_subnode ( self , dst , ast , expr ) : \n    dst . value = eval ( 'ast' + expr ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12535": "\ndef get_check ( ) : \n    try : \n        from invenio . dbquery import run_sql \n    except ImportError : \n        from invenio . legacy . dbquery import run_sql \n    return_identifier_0 = ( run_sql ( 'select count(id) from bibdoc' , run_on_slave = True ) [ 0 ] [ 0 ] , [ id [ 0 ] for id in run_sql ( 'select id from bibdoc' , run_on_slave = True ) ] , ) \n    return return_identifier_0 "}
{"12543": "\ndef _get_collection_restrictions ( collection ) : \n    try : \n        from invenio . dbquery import run_sql \n        from invenio . access_control_firerole import compile_role_definition \n    except ImportError : \n        from invenio . modules . access . firerole import compile_role_definition \n        from invenio . legacy . dbquery import run_sql \n    res = run_sql ( 'SELECT r.firerole_def_src, email ' 'FROM accROLE as r ' 'JOIN accROLE_accACTION_accARGUMENT ON r.id=id_accROLE ' 'JOIN accARGUMENT AS a ON a.id=id_accARGUMENT ' 'JOIN user_accROLE AS u ON r.id=u.id_accROLE ' 'JOIN user ON user.id=u.id_user ' 'WHERE a.keyword=\"collection\" AND ' 'a.value=%s AND ' 'id_accACTION=(select id from accACTION where name=\"viewrestrcoll\")' , ( collection , ) , run_on_slave = True ) \n    fireroles = set ( ) \n    users = set ( ) \n    for f , u in res : \n        fireroles . add ( compile_role_definition ( f ) ) \n        users . add ( u ) \n    return_identifier_0 = { 'fireroles' : list ( fireroles ) , 'users' : users } \n    return return_identifier_0 "}
{"12590": "\ndef create_files ( cls , record , files , existing_files ) : \n    default_bucket = None \n    for f in existing_files : \n        if 'bucket' in f : \n            default_bucket = f [ 'bucket' ] \n            break \n    if default_bucket is None : \n        b = Bucket . create ( ) \n        BucketTag . create ( b , 'record' , str ( record . id ) ) \n        default_bucket = str ( b . id ) \n        db . session . commit ( ) \n    else : \n        b = Bucket . get ( default_bucket ) \n    record [ '_files' ] = [ ] \n    for key , meta in files . items ( ) : \n        obj = cls . create_file ( b , key , meta ) \n        ext = splitext ( obj . key ) [ 1 ] . lower ( ) \n        if ext . startswith ( '.' ) : \n            ext = ext [ 1 : ] \n        record [ '_files' ] . append ( dict ( bucket = str ( obj . bucket . id ) , key = obj . key , version_id = str ( obj . version_id ) , size = obj . file . size , checksum = obj . file . checksum , type = ext , ) ) \n    db . session . add ( RecordsBuckets ( record_id = record . id , bucket_id = b . id ) ) \n    record . commit ( ) \n    db . session . commit ( ) \n    return_identifier_0 = [ b ] \n    return return_identifier_0 "}
{"12644": "\ndef loadModelData ( self , name ) : \n    path = self . resourceNameToPath ( name , \".json\" ) \n    try : \n        data = json . load ( open ( path , \"r\" ) ) \n    except Exception : \n        print ( \"Exception during model load: \" ) \n        import traceback ; \n        traceback . print_exc ( ) \n        return_identifier_0 = { } \n        return return_identifier_0 \n    out = { } \n    if data . get ( \"version\" , 1 ) == 1 : \n        out [ \"materials\" ] = { } \n        for name , matdata in data . get ( \"materials\" , { } ) . items ( ) : \n            m = model . Material ( self , name , matdata ) \n            out [ \"materials\" ] [ name ] = m \n        out [ \"default_material\" ] = out [ \"materials\" ] [ data . get ( \"default_material\" , list ( out [ \"materials\" ] . keys ( ) ) [ 0 ] ) ] \n        out [ \"bones\" ] = { \"__root__\" : model . RootBone ( self , \"__root__\" , { \"start_rot\" : [ 0 , 0 ] , \"length\" : 0 } ) } \n        for name , bonedata in data . get ( \"bones\" , { } ) . items ( ) : \n            b = model . Bone ( self , name , bonedata ) \n            out [ \"bones\" ] [ name ] = b \n        for name , bone in out [ \"bones\" ] . items ( ) : \n            if name == \"__root__\" : \n                continue \n            bone . setParent ( out [ \"bones\" ] [ bone . bonedata [ \"parent\" ] ] ) \n        out [ \"regions\" ] = { } \n        for name , regdata in data . get ( \"regions\" , { } ) . items ( ) : \n            r = model . Region ( self , name , regdata ) \n            r . material = out [ \"materials\" ] [ regdata . get ( \"material\" , out [ \"default_material\" ] ) ] \n            r . bone = out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] \n            out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] . addRegion ( r ) \n            out [ \"regions\" ] [ name ] = r \n        out [ \"animations\" ] = { } \n        out [ \"animations\" ] [ \"static\" ] = model . Animation ( self , \"static\" , { \"type\" : \"static\" , \"bones\" : { } } ) \n        for name , anidata in data . get ( \"animations\" , { } ) . items ( ) : \n            a = model . Animation ( self , name , anidata ) \n            a . setBones ( out [ \"bones\" ] ) \n            out [ \"animations\" ] [ name ] = a \n        out [ \"default_animation\" ] = out [ \"animations\" ] [ data . get ( \"default_animation\" , out [ \"animations\" ] [ \"static\" ] ) ] \n    else : \n        raise ValueError ( \"Unknown version %s of model '%s'\" % ( data . get ( \"version\" , 1 ) , name ) ) \n    self . modelcache [ name ] = out \n    return out "}
{"12691": "\ndef err_msg ( self , instance , value ) : \n    if not hasattr ( self , \"name\" ) : \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    return ( \"Attempted to set the {f_type} attribute {inst}.{attr} to the \" \"{val_type} value {val}, which does not satisfy the condition \" \"{f_type}.\" . format ( f_type = self . field_type , inst = instance . __class__ . __name__ , attr = self . name , val_type = value . __class__ . __name__ , val = value ) ) "}
{"12692": "\ndef exc_thrown_by_descriptor ( ) : \n    traceback = sys . exc_info ( ) [ 2 ] \n    tb_locals = traceback . tb_frame . f_locals \n    if \"self\" in tb_locals : \n        if not isinstance ( tb_locals [ \"self\" ] , Descriptor ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"12694": "\ndef _get_axis_mode ( self , axis ) : \n    if all ( [ isinstance ( getattr ( s , axis ) , TimeVariable ) for s in self . _series ] ) : \n        return_identifier_0 = 'time' \n        return return_identifier_0 \n    return None "}
{"12703": "\ndef is_matching_mime_type ( self , mime_type ) : \n    if len ( self . include_mime_types ) == 0 : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if mime_type is None : \n        return_identifier_1 = False \n        return return_identifier_1 \n    mime_type = mime_type . lower ( ) \n    return any ( mime_type . startswith ( mt ) for mt in self . include_mime_types ) "}
{"12704": "\ndef domain_name_cleanse ( raw_string ) : \n    try : \n        parts = urlparse ( raw_string ) \n        domain = parts . netloc . split ( ':' ) [ 0 ] \n    except : \n        domain = '' \n    if not domain : \n        domain = raw_string \n    if not domain : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    domain = re . sub ( '\\/' , '' , domain . strip ( ) . lower ( ) ) \n    return domain "}
{"12706": "\ndef make_hash_kw ( self , tok ) : \n    if isinstance ( tok , unicode ) : \n        tok = tok . encode ( 'utf-8' ) \n    h = mmh3 . hash ( tok ) \n    if h == DOCUMENT_HASH_KEY : \n        h = DOCUMENT_HASH_KEY_REPLACEMENT \n    return_identifier_0 = ( tok , h ) \n    return return_identifier_0 "}
{"12723": "\ndef is_subsequence ( needle , haystack ) : \n    it = iter ( haystack ) \n    for element in needle : \n        if element not in it : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"12729": "\ndef static ( self , root , path , media_type = None , charset = 'UTF-8' ) : \n    root = os . path . abspath ( os . path . join ( root , '' ) ) \n    path = os . path . abspath ( os . path . join ( root , path . lstrip ( '/\\\\' ) ) ) \n    self . response . state [ 'filename' ] = os . path . basename ( path ) \n    if not path . startswith ( root ) : \n        return_identifier_0 = 403 \n        return return_identifier_0 \n    elif not os . path . isfile ( path ) : \n        return_identifier_1 = 404 \n        return return_identifier_1 \n    if media_type is not None : \n        self . response . media_type = media_type \n    else : \n        self . response . media_type = mimetypes . guess_type ( path ) [ 0 ] \n    self . response . charset = charset \n    with open ( path , 'rb' ) as f : \n        return f . read ( ) "}
{"12735": "\ndef response ( self ) : \n    if isinstance ( self . body , bytes ) : \n        out = self . body \n    elif isinstance ( self . body , str ) : \n        out = self . body . encode ( self . charset ) \n    else : \n        out = b'' \n    self . add_header ( 'Content-Type' , self . content_type ) \n    self . add_header ( 'Content-Length' , str ( len ( out ) ) ) \n    self . start ( self . status_line , self . _headers ) \n    return_identifier_0 = [ out ] \n    return return_identifier_0 "}
{"12747": "\ndef _random_adjspecies_pair ( ) : \n    describer , desc_position = random_describer ( ) \n    if desc_position == 'prefix' : \n        return_identifier_0 = ( describer , random_species ( ) ) \n        return return_identifier_0 \n    elif desc_position == 'suffix' : \n        return_identifier_1 = ( random_species ( ) , describer ) \n        return return_identifier_1 "}
{"12757": "\ndef _init_stages ( self , config , name ) : \n    if name not in config : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    return [ self . create ( stage , config ) for stage in config [ name ] ] "}
{"12758": "\ndef _init_all_stages ( self , config ) : \n    reader = self . _init_stage ( config , 'reader' ) \n    incremental_transforms = self . _init_stages ( config , 'incremental_transforms' ) \n    batch_transforms = self . _init_stages ( config , 'batch_transforms' ) \n    post_batch_incremental_transforms = self . _init_stages ( config , 'post_batch_incremental_transforms' ) \n    writers = self . _init_stages ( config , 'writers' ) \n    tmp_dir_path = os . path . join ( config [ 'tmp_dir_path' ] , self . tmp_dir_suffix ) \n    return_identifier_0 = ( reader , incremental_transforms , batch_transforms , post_batch_incremental_transforms , writers , tmp_dir_path ) \n    return return_identifier_0 "}
{"12766": "\ndef ANY_mentions ( target_mentions , chain_mentions ) : \n    for name in target_mentions : \n        for chain_ment in chain_mentions : \n            if name in chain_ment : \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"12803": "\ndef verify_md5 ( md5_expected , data , other_errors = None ) : \n    md5_recv = hashlib . md5 ( data ) . hexdigest ( ) \n    if md5_expected != md5_recv : \n        if other_errors is not None : \n            logger . critical ( '\\n' . join ( other_errors ) ) \n        raise FailedVerification ( 'original md5 = %r != %r = received md5' % ( md5_expected , md5_recv ) ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"12807": "\ndef stream_id_to_kvlayer_key ( stream_id ) : \n    parts = stream_id . split ( '-' ) \n    if len ( parts ) != 2 : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    epoch_ticks_s = parts [ 0 ] \n    doc_id_s = parts [ 1 ] \n    if not epoch_ticks_s . isdigit ( ) : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    if doc_id_s . lstrip ( string . hexdigits ) != '' : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    return_identifier_0 = ( base64 . b16decode ( doc_id_s . upper ( ) ) , int ( epoch_ticks_s ) ) \n    return return_identifier_0 "}
{"12809": "\ndef key_for_stream_item ( si ) : \n    urlhash = hashlib . md5 ( si . abs_url ) . digest ( ) \n    return_identifier_0 = ( urlhash , int ( si . stream_time . epoch_ticks ) ) \n    return return_identifier_0 "}
{"12849": "\ndef create_event_datetimes ( options , config ) : \n    now = datetime . datetime . now ( ) \n    return_identifier_0 = { \"start\" : { \"dateTime\" : ( now + datetime . timedelta ( minutes = int ( config [ \"start\" ] ) ) ) . strftime ( DT_FORMAT ) , \"timeZone\" : options . timezone , } , \"end\" : { \"dateTime\" : ( now + datetime . timedelta ( minutes = int ( config [ \"end\" ] ) ) ) . strftime ( DT_FORMAT ) , \"timeZone\" : options . timezone , } , } \n    return return_identifier_0 "}
{"12986": "\ndef read_config_ ( self , cfile ) : \n    if not cfile . exists ( ) : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    try : \n        conf_dict = toml . load ( str ( cfile ) ) \n    except toml . TomlDecodeError : \n        return None \n    self . update_ ( conf_dict ) \n    return conf_dict "}
{"12989": "\ndef sections_list ( self , cmd = None ) : \n    sections = list ( self . common . sections ) \n    if not cmd : \n        if self . bare is not None : \n            sections . extend ( self . bare . sections ) \n            return sections \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    sections . extend ( self . subcmds [ cmd ] . sections ) \n    if cmd in self . _conf : \n        sections . append ( cmd ) \n    return sections "}
{"13023": "\ndef _match_regex ( regex , obj ) : \n    if isinstance ( obj , six . string_types ) : \n        return len ( regex . findall ( obj ) ) > 0 \n    elif isinstance ( obj , dict ) : \n        return _match_regex ( regex , obj . values ( ) ) \n    elif hasattr ( obj , '__iter__' ) : \n        return any ( _match_regex ( regex , s ) for s in obj if isinstance ( s , six . string_types ) ) \n    else : \n        return_identifier_0 = False \n        return return_identifier_0 "}
{"13029": "\ndef _get_attrib ( self , attr , convert_to_str = False ) : \n    if attr . startswith ( 'tags.' ) : \n        tag = attr [ len ( 'tags.' ) : ] \n        if tag in self . tags and self . tags [ tag ] != '' : \n            return self . tags [ tag ] \n        elif convert_to_str is True : \n            return_identifier_0 = '<not set>' \n            return return_identifier_0 \n        else : \n            return self . tags . get ( tag ) \n    elif not hasattr ( self , attr ) : \n        raise AttributeError ( 'Invalid attribute: {0}. Perhaps you meant ' '{1}?' . format ( red ( attr ) , green ( 'tags.' + attr ) ) ) \n    else : \n        result = getattr ( self , attr ) \n        if convert_to_str is True and not result : \n            return_identifier_1 = '<none>' \n            return return_identifier_1 \n        elif convert_to_str is True and isinstance ( result , list ) : \n            return ', ' . join ( result ) \n        elif convert_to_str is True : \n            return str ( result ) \n        else : \n            return result "}
{"13074": "\ndef _run_ssh_command ( entries , username , idfile , command , tunnel , parallel = False ) : \n    if len ( entries ) == 0 : \n        print ( '(No hosts to run command on)' ) \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    if command . strip ( ) == '' or command is None : \n        raise ValueError ( 'No command given' ) \n    print ( 'Running command {0} on {1} matching hosts' . format ( green ( repr ( command ) ) , len ( entries ) ) ) \n    shell_cmds = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_ssh_command ( hname , username , idfile , command , tunnel ) \n        shell_cmds . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( shell_cmds , parallel = parallel ) \n    print ( green ( 'All commands finished' ) ) "}
{"13105": "\ndef silence_without_namespace ( f ) : \n    \n    @ wraps ( f ) \n    def wrapped ( label = None ) : \n        if not home_namespace : \n            return_identifier_0 = '' \n            return return_identifier_0 \n        if label : \n            return f ( label ) \n        else : \n            return f ( home_label ) \n    return wrapped "}
{"13113": "\ndef _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n    travis_event = os . getenv ( 'TRAVIS_EVENT_TYPE' ) \n    if travis_event is None : \n        raise click . UsageError ( 'Using --travis but the TRAVIS_EVENT_TYPE ' 'environment variable is not detected.' ) \n    if travis_event == 'push' and on_travis_push is False : \n        click . echo ( 'Skipping upload on Travis push event.' ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif travis_event == 'pull_request' and on_travis_pr is False : \n        click . echo ( 'Skipping upload on Travis pull request event.' ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    elif travis_event == 'api' and on_travis_api is False : \n        click . echo ( 'Skipping upload on Travis pull request event.' ) \n        return_identifier_2 = True \n        return return_identifier_2 \n    elif travis_event == 'cron' and on_travis_cron is False : \n        click . echo ( 'Skipping upload on Travis cron event.' ) \n        return_identifier_3 = True \n        return return_identifier_3 \n    else : \n        return_identifier_4 = False \n        return return_identifier_4 "}
{"13145": "\ndef check_build_status ( self , build_id ) : \n    data = self . api_request ( 'GET' , '/app-setups/%s' % build_id ) \n    status = data . get ( 'status' ) \n    if status == 'pending' : \n        return_identifier_0 = False \n        return return_identifier_0 \n    elif status == 'succeeded' : \n        return_identifier_1 = True \n        return return_identifier_1 \n    else : \n        raise BuildError ( str ( data ) ) "}
{"13151": "\ndef get_description ( self ) : \n    if self . description : \n        return self . description \n    elif self . __doc__ and self . __doc__ . strip ( ) : \n        return self . __doc__ . strip ( ) . split ( '.' ) [ 0 ] + '.' \n    else : \n        return_identifier_0 = '' \n        return return_identifier_0 "}
{"13152": "\ndef get_help ( self ) : \n    if self . help : \n        return self . help \n    elif self . __doc__ and self . __doc__ . strip ( ) : \n        return self . __doc__ . strip ( ) \n    else : \n        return_identifier_0 = '' \n        return return_identifier_0 "}
{"13162": "\ndef create ( self , tarball_url , env = None , app_name = None ) : \n    data = self . _api . create_build ( tarball_url = tarball_url , env = env , app_name = app_name , ) \n    return_identifier_0 = ( data [ 'id' ] , data [ 'app' ] [ 'name' ] ) \n    return return_identifier_0 "}
{"13181": "\ndef signing_keys_as_jwks ( self ) : \n    _l = [ x . serialize ( ) for x in self . self_signer . keyjar . get_signing_key ( ) ] \n    if not _l : \n        _l = [ x . serialize ( ) for x in self . self_signer . keyjar . get_signing_key ( owner = self . iss ) ] \n    return_identifier_0 = { 'keys' : _l } \n    return return_identifier_0 "}
{"13195": "\ndef get_last_value_from_timeseries ( timeseries ) : \n    if not timeseries : \n        return_identifier_0 = 0 \n        return return_identifier_0 \n    for metric , points in timeseries . items ( ) : \n        return next ( ( p [ 'y' ] for p in reversed ( points ) if p [ 'y' ] > 0 ) , 0 ) "}
{"13240": "\ndef connect ( self , factory ) : \n    try : \n        factory = self . _factories [ factory ] \n    except KeyError : \n        raise NoSuchFactory ( ) \n    remote = self . getProtocol ( ) \n    addr = remote . transport . getPeer ( ) \n    proto = factory . buildProtocol ( addr ) \n    if proto is None : \n        raise ConnectionRefused ( ) \n    identifier = uuid4 ( ) . hex \n    transport = MultiplexedTransport ( identifier , remote ) \n    proto . makeConnection ( transport ) \n    self . _protocols [ identifier ] = proto \n    return_identifier_0 = { \"connection\" : identifier } \n    return return_identifier_0 "}
{"13241": "\ndef receiveData ( self , connection , data ) : \n    try : \n        protocol = self . _protocols [ connection ] \n    except KeyError : \n        raise NoSuchConnection ( ) \n    protocol . dataReceived ( data ) \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"13242": "\ndef disconnect ( self , connection ) : \n    proto = self . _protocols . pop ( connection ) \n    proto . transport = None \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"13250": "\ndef remoteDataReceived ( self , connection , data ) : \n    proto = self . getLocalProtocol ( connection ) \n    proto . transport . write ( data ) \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"13251": "\ndef disconnect ( self , connection ) : \n    proto = self . getLocalProtocol ( connection ) \n    proto . transport . loseConnection ( ) \n    return_identifier_0 = { } \n    return return_identifier_0 "}
{"13262": "\ndef argument_parser ( args ) : \n    parser = argparse . ArgumentParser ( prog = \"nagaram\" , description = \"Finds Scabble anagrams.\" , formatter_class = argparse . RawDescriptionHelpFormatter , add_help = False , ) \n    parser . add_argument ( \"-h\" , \"--help\" , dest = \"help\" , action = \"store_true\" , default = False , ) \n    parser . add_argument ( \"--sowpods\" , dest = \"sowpods\" , action = \"store_true\" , default = False , ) \n    parser . add_argument ( \"--length\" , \"-l\" , dest = \"length\" , action = \"store_true\" , default = False , ) \n    parser . add_argument ( \"--starts-with\" , \"-s\" , dest = \"starts_with\" , metavar = \"chars\" , default = \"\" , nargs = 1 , type = str , ) \n    parser . add_argument ( \"--ends-with\" , \"-e\" , dest = \"ends_with\" , metavar = \"chars\" , default = \"\" , nargs = 1 , type = str , ) \n    parser . add_argument ( \"--version\" , \"-v\" , action = \"version\" , version = \"Nagaram {0} (Released: {1})\" . format ( nagaram . __version__ , nagaram . __release_date__ , ) ) \n    parser . add_argument ( dest = \"wordlist\" , metavar = \"letters to find anagrams with (? for anything, _ for blanks)\" , nargs = argparse . REMAINDER , ) \n    settings = parser . parse_args ( args ) \n    if settings . help : \n        raise SystemExit ( nagaram . __doc__ . strip ( ) ) \n    if not settings . wordlist : \n        raise SystemExit ( parser . print_usage ( ) ) \n    if settings . starts_with : \n        settings . starts_with = settings . starts_with [ 0 ] \n    if settings . ends_with : \n        settings . ends_with = settings . ends_with [ 0 ] \n    return_identifier_0 = ( settings . wordlist , settings . sowpods , settings . length , settings . starts_with , settings . ends_with ) \n    return return_identifier_0 "}
{"13271": "\ndef verify ( self , ** kwargs ) : \n    super ( MetadataStatement , self ) . verify ( ** kwargs ) \n    if \"signing_keys\" in self : \n        if 'signing_keys_uri' in self : \n            raise VerificationError ( 'You can only have one of \"signing_keys\" and ' '\"signing_keys_uri\" in a metadata statement' ) \n        else : \n            kj = KeyJar ( ) \n            try : \n                kj . import_jwks ( self [ 'signing_keys' ] , '' ) \n            except Exception : \n                raise VerificationError ( '\"signing_keys\" not a proper JWKS' ) \n    if \"metadata_statements\" in self and \"metadata_statement_uris\" in self : \n        s = set ( self [ 'metadata_statements' ] . keys ( ) ) \n        t = set ( self [ 'metadata_statement_uris' ] . keys ( ) ) \n        if s . intersection ( t ) : \n            raise VerificationError ( 'You should not have the same key in \"metadata_statements\" ' 'and in \"metadata_statement_uris\"' ) \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9 , \"b\" : 2 , \"c\" : 2 , \"d\" : 4 , \"e\" : 12 , \"f\" : 2 , \"g\" : 3 , \"h\" : 2 , \"i\" : 9 , \"j\" : 1 , \"k\" : 1 , \"l\" : 4 , \"m\" : 2 , \"n\" : 6 , \"o\" : 8 , \"p\" : 2 , \"q\" : 1 , \"r\" : 6 , \"s\" : 4 , \"t\" : 6 , \"u\" : 4 , \"v\" : 2 , \"w\" : 2 , \"x\" : 1 , \"y\" : 2 , \"z\" : 1 , \"_\" : 2 , } \n    for letter in word : \n        if letter == \"?\" : \n            continue \n        try : \n            letters_in_bag [ letter ] -= 1 \n        except KeyError : \n            return_identifier_0 = False \n            return return_identifier_0 \n        if letters_in_bag [ letter ] < 0 : \n            letters_in_bag [ \"_\" ] -= 1 \n            if letters_in_bag [ \"_\" ] < 0 : \n                return_identifier_1 = False \n                return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"13339": "\ndef is_changed ( self , item ) : \n    fname = os . path . join ( self . fdir , item ) \n    if os . path . isfile ( fname ) : \n        mtime = self . get_mtime ( fname ) \n        try : \n            _ftime = self . fmtime [ item ] \n        except KeyError : \n            self . fmtime [ item ] = mtime \n            return_identifier_0 = True \n            return return_identifier_0 \n        if mtime > _ftime : \n            self . fmtime [ item ] = mtime \n            return_identifier_1 = True \n            return return_identifier_1 \n        else : \n            return_identifier_2 = False \n            return return_identifier_2 \n    else : \n        logger . error ( 'Could not access {}' . format ( fname ) ) \n        raise KeyError ( item ) "}
{"13344": "\ndef has_changed ( self ) : \n    request = urllib_request . Request ( self . url ) \n    request . get_method = lambda : 'HEAD' \n    response = urllib_request . urlopen ( request ) \n    information = response . info ( ) \n    if 'Last-Modified' in information : \n        last_modified = information [ 'Last-Modified' ] \n        if last_modified == self . image_last_modified : \n            return_identifier_0 = False \n            return return_identifier_0 \n    self . image_last_modified = last_modified \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"13350": "\ndef is_contained_in ( pe_pe , root ) : \n    if not pe_pe : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    ep_pkg = one ( pe_pe ) . EP_PKG [ 8000 ] ( ) \n    c_c = one ( pe_pe ) . C_C [ 8003 ] ( ) \n    if root in [ ep_pkg , c_c ] : \n        return_identifier_1 = True \n        return return_identifier_1 \n    elif is_contained_in ( ep_pkg , root ) : \n        return_identifier_2 = True \n        return return_identifier_2 \n    elif is_contained_in ( c_c , root ) : \n        return_identifier_3 = True \n        return return_identifier_3 \n    else : \n        return_identifier_4 = False \n        return return_identifier_4 "}
{"13351": "\ndef is_global ( pe_pe ) : \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    if one ( pe_pe ) . C_C [ 8003 ] ( ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    pe_pe = one ( pe_pe ) . EP_PKG [ 8000 ] . PE_PE [ 8001 ] ( ) \n    if not pe_pe : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return is_global ( pe_pe ) "}
{"13352": "\ndef _get_data_type_name ( s_dt ) : \n    s_cdt = one ( s_dt ) . S_CDT [ 17 ] ( ) \n    if s_cdt and s_cdt . Core_Typ in range ( 1 , 6 ) : \n        return s_dt . Name . upper ( ) \n    if one ( s_dt ) . S_EDT [ 17 ] ( ) : \n        return_identifier_0 = 'INTEGER' \n        return return_identifier_0 \n    s_dt = one ( s_dt ) . S_UDT [ 17 ] . S_DT [ 18 ] ( ) \n    if s_dt : \n        return _get_data_type_name ( s_dt ) "}
{"13373": "\ndef guess_type_name ( value ) : \n    value = str ( value ) \n    if value . upper ( ) in [ 'TRUE' , 'FALSE' ] : \n        return_identifier_0 = 'BOOLEAN' \n        return return_identifier_0 \n    elif re . match ( r'(-)?(\\d+)(\\.\\d+)' , value ) : \n        return_identifier_1 = 'REAL' \n        return return_identifier_1 \n    elif re . match ( r'(-)?(\\d+)' , value ) : \n        return_identifier_2 = 'INTEGER' \n        return return_identifier_2 \n    elif re . match ( r'\\'((\\'\\')|[^\\'])*\\'' , value ) : \n        return_identifier_3 = 'STRING' \n        return return_identifier_3 \n    elif re . match ( r'\\\"([^\\\\\\n]|(\\\\.))*?\\\"' , value ) : \n        return_identifier_4 = 'UNIQUE_ID' \n        return return_identifier_4 "}
{"13374": "\ndef deserialize_value ( ty , value ) : \n    uty = ty . upper ( ) \n    if uty == 'BOOLEAN' : \n        if value . isdigit ( ) : \n            return bool ( int ( value ) ) \n        elif value . upper ( ) == 'FALSE' : \n            return_identifier_0 = False \n            return return_identifier_0 \n        elif value . upper ( ) == 'TRUE' : \n            return_identifier_1 = True \n            return return_identifier_1 \n        else : \n            return None \n    elif uty == 'INTEGER' : \n        if '\"' in value : \n            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n        else : \n            return int ( value ) \n    elif uty == 'REAL' : \n        return float ( value ) \n    elif uty == 'STRING' : \n        return value [ 1 : - 1 ] . replace ( \"''\" , \"'\" ) \n    elif uty == 'UNIQUE_ID' : \n        if '\"' in value : \n            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n        else : \n            return int ( value ) "}
{"13392": "\ndef _source ( self , feature_names ) : \n    if feature_names is None : \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif isinstance ( feature_names , bool ) : \n        return feature_names \n    else : \n        return map ( lambda n : 'fc.' + n , feature_names ) "}
{"13393": "\ndef _range_filters ( self , * key_ranges ) : \n    filters = [ ] \n    for s , e in key_ranges : \n        if isinstance ( s , basestring ) : \n            s = eid ( s ) \n        if isinstance ( e , basestring ) : \n            e += u'\\U0010FFFF' \n            e = eid ( e ) \n        if s == ( ) and e == ( ) : \n            filters . append ( { 'match_all' : { } } ) \n        elif e == ( ) : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s } } } ) \n        elif s == ( ) : \n            filters . append ( { 'range' : { '_id' : { 'lte' : e } } } ) \n        else : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s , 'lte' : e } } } ) \n    if len ( filters ) == 0 : \n        return_identifier_0 = [ { 'match_all' : { } } ] \n        return return_identifier_0 \n    else : \n        return filters "}
{"13398": "\ndef _fc_index_disjunction_from_query ( self , query_fc , fname ) : \n    if len ( query_fc . get ( fname , [ ] ) ) == 0 : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    terms = query_fc [ fname ] . keys ( ) \n    disj = [ ] \n    for fname in self . indexes [ fname ] [ 'feature_names' ] : \n        disj . append ( { 'terms' : { fname_to_idx_name ( fname ) : terms } } ) \n    return disj "}
{"13424": "\ndef check_pypi_name ( pypi_package_name , pypi_registry_host = None ) : \n    if pypi_registry_host is None : \n        pypi_registry_host = 'pypi.python.org' \n    receive_buffer = bytearray ( b'------------' ) \n    context = ssl . create_default_context ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD /simple/\" , pypi_package_name . encode ( 'ascii' ) , b\"/ HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif b'HTTP/1.1 404' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    remaining_bytes = ssl_http_socket . recv ( 2048 ) \n    redirect_path_location_start = remaining_bytes . find ( b'Location:' ) + 10 \n    redirect_path_location_end = remaining_bytes . find ( b'\\r\\n' , redirect_path_location_start ) \n    redirect_path = remaining_bytes [ redirect_path_location_start : redirect_path_location_end ] + b'/' \n    ssl_http_socket . shutdown ( 1 ) \n    ssl_http_socket . close ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD \" , redirect_path , b\" HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        return_identifier_2 = True \n        return return_identifier_2 \n    elif b'HTTP/1.1 404' in receive_buffer : \n        return_identifier_3 = False \n        return return_identifier_3 \n    else : \n        NotImplementedError ( 'A definitive answer was not found by primary or secondary lookups.' ) "}
{"13437": "\nasync def fetch_bikes ( ) -> List [ dict ] : \n    async with ClientSession ( ) as session : \n        try : \n            async with session . get ( 'https://www.bikeregister.com/stolen-bikes' ) as request : \n                document = document_fromstring ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        token = document . xpath ( \"//input[@name='_token']\" ) \n        if len ( token ) != 1 : \n            raise ApiError ( f\"Couldn't extract token from page.\" ) \n        else : \n            token = token [ 0 ] . value \n        xsrf_token = request . cookies [ \"XSRF-TOKEN\" ] \n        laravel_session = request . cookies [ \"laravel_session\" ] \n        headers = { 'cookie' : f'XSRF-TOKEN={xsrf_token}; laravel_session={laravel_session}' , 'origin' : 'https://www.bikeregister.com' , 'accept-encoding' : 'gzip, deflate, br' , 'accept-language' : 'en-GB,en-US;q=0.9,en;q=0.8' , 'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0' , 'content-type' : 'application/x-www-form-urlencoded; charset=UTF-8' , 'accept' : '*/*' , 'referer' : 'https://www.bikeregister.com/stolen-bikes' , 'authority' : 'www.bikeregister.com' , 'x-requested-with' : 'XMLHttpRequest' , } \n        data = [ ( '_token' , token ) , ( 'make' , '' ) , ( 'model' , '' ) , ( 'colour' , '' ) , ( 'reporting_period' , '1' ) , ] \n        try : \n            async with session . post ( 'https://www.bikeregister.com/stolen-bikes' , headers = headers , data = data ) as request : \n                bikes = json . loads ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        except json . JSONDecodeError as dec_err : \n            logger . error ( f\"Could not decode data: {dec_err.msg}\" ) \n            raise ApiError ( f\"Could not decode data: {dec_err.msg}\" ) \n        return bikes \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"13460": "\ndef text_visible ( self ) : \n    words = self . read ( ) . split ( ) \n    for word in words : \n        if word . lstrip ( '-' ) . replace ( '.' , '' , 1 ) . isdigit ( ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        if word . isalpha ( ) and ( len ( word ) > 1 or len ( word ) <= 20 ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"13495": "\ndef create_inputhook_qt4 ( mgr , app = None ) : \n    if app is None : \n        app = QtCore . QCoreApplication . instance ( ) \n        if app is None : \n            app = QtGui . QApplication ( [ \" \" ] ) \n    ip = InteractiveShell . instance ( ) \n    if hasattr ( ip , '_inputhook_qt4' ) : \n        return app , ip . _inputhook_qt4 \n    got_kbdint = [ False ] \n    def inputhook_qt4 ( ) : \n        try : \n            allow_CTRL_C ( ) \n            app = QtCore . QCoreApplication . instance ( ) \n            if not app : \n                return_identifier_0 = 0 \n                return return_identifier_0 \n            app . processEvents ( QtCore . QEventLoop . AllEvents , 300 ) \n            if not stdin_ready ( ) : \n                timer = QtCore . QTimer ( ) \n                timer . timeout . connect ( app . quit ) \n                while not stdin_ready ( ) : \n                    timer . start ( 50 ) \n                    app . exec_ ( ) \n                    timer . stop ( ) \n        except KeyboardInterrupt : \n            ignore_CTRL_C ( ) \n            got_kbdint [ 0 ] = True \n            print ( \"\\nKeyboardInterrupt - Ctrl-C again for new prompt\" ) \n            mgr . clear_inputhook ( ) \n        except : \n            ignore_CTRL_C ( ) \n            from traceback import print_exc \n            print_exc ( ) \n            print ( \"Got exception from inputhook_qt4, unregistering.\" ) \n            mgr . clear_inputhook ( ) \n        finally : \n            allow_CTRL_C ( ) \n        return_identifier_1 = 0 \n        return return_identifier_1 \n    def preprompthook_qt4 ( ishell ) : \n        if got_kbdint [ 0 ] : \n            mgr . set_inputhook ( inputhook_qt4 ) \n        got_kbdint [ 0 ] = False \n    ip . _inputhook_qt4 = inputhook_qt4 \n    ip . set_hook ( 'pre_prompt_hook' , preprompthook_qt4 ) \n    return app , inputhook_qt4 "}
{"13503": "\ndef _up_pressed ( self , shift_modifier ) : \n    prompt_cursor = self . _get_prompt_cursor ( ) \n    if self . _get_cursor ( ) . blockNumber ( ) == prompt_cursor . blockNumber ( ) : \n        if self . _history_locked ( ) and not shift_modifier : \n            return_identifier_0 = False \n            return return_identifier_0 \n        col = self . _get_input_buffer_cursor_column ( ) \n        input_buffer = self . input_buffer \n        if self . _history_index == len ( self . _history ) or ( self . _history_prefix and col != len ( self . _history_prefix ) ) : \n            self . _history_index = len ( self . _history ) \n            self . _history_prefix = input_buffer [ : col ] \n        self . history_previous ( self . _history_prefix , as_prefix = not shift_modifier ) \n        cursor = self . _get_prompt_cursor ( ) \n        if self . _history_prefix : \n            cursor . movePosition ( QtGui . QTextCursor . Right , n = len ( self . _history_prefix ) ) \n        else : \n            cursor . movePosition ( QtGui . QTextCursor . EndOfLine ) \n        self . _set_cursor ( cursor ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"13504": "\ndef _down_pressed ( self , shift_modifier ) : \n    end_cursor = self . _get_end_cursor ( ) \n    if self . _get_cursor ( ) . blockNumber ( ) == end_cursor . blockNumber ( ) : \n        if self . _history_locked ( ) and not shift_modifier : \n            return_identifier_0 = False \n            return return_identifier_0 \n        replaced = self . history_next ( self . _history_prefix , as_prefix = not shift_modifier ) \n        if self . _history_prefix and replaced : \n            cursor = self . _get_prompt_cursor ( ) \n            cursor . movePosition ( QtGui . QTextCursor . Right , n = len ( self . _history_prefix ) ) \n            self . _set_cursor ( cursor ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"13526": "\ndef _override_setuptools ( req ) : \n    if req . project_name == 'setuptools' : \n        if not len ( req . specs ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n        for comparator , version in req . specs : \n            if comparator in [ '==' , '>=' , '>' ] : \n                if '0.7' in version : \n                    return_identifier_1 = False \n                    return return_identifier_1 \n        return_identifier_2 = True \n        return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"13553": "\ndef passwd_check ( hashed_passphrase , passphrase ) : \n    try : \n        algorithm , salt , pw_digest = hashed_passphrase . split ( ':' , 2 ) \n    except ( ValueError , TypeError ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        h = hashlib . new ( algorithm ) \n    except ValueError : \n        return_identifier_1 = False \n        return return_identifier_1 \n    if len ( pw_digest ) == 0 : \n        return_identifier_2 = False \n        return return_identifier_2 \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return h . hexdigest ( ) == pw_digest "}
{"13556": "\ndef _collect_editable_booleans ( self ) : \n    if hasattr ( self , '_ajax_editable_booleans' ) : \n        return \n    self . _ajax_editable_booleans = { } \n    for field in self . list_display : \n        item = getattr ( self . __class__ , field , None ) \n        if not item : \n            continue \n        attr = getattr ( item , 'editable_boolean_field' , None ) \n        if attr : \n            def _fn ( self , instance ) : \n                return_identifier_0 = [ ajax_editable_boolean_cell ( instance , _fn . attr ) ] \n                return return_identifier_0 \n            _fn . attr = attr \n            result_func = getattr ( item , 'editable_boolean_result' , _fn ) \n            self . _ajax_editable_booleans [ attr ] = result_func "}
{"13571": "\ndef wait_for_kernel ( self , timeout = None ) : \n    tic = time . time ( ) \n    self . km . hb_channel . unpause ( ) \n    while True : \n        self . run_cell ( '1' , False ) \n        if self . km . hb_channel . is_beating ( ) : \n            break \n        else : \n            if timeout is not None and ( time . time ( ) - tic ) > timeout : \n                return_identifier_0 = False \n                return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"13580": "\ndef last_blank ( src ) : \n    if not src : \n        return_identifier_0 = False \n        return return_identifier_0 \n    ll = src . splitlines ( ) [ - 1 ] \n    return ( ll == '' ) or ll . isspace ( ) "}
{"13581": "\ndef last_two_blanks ( src ) : \n    if not src : \n        return_identifier_0 = False \n        return return_identifier_0 \n    new_src = '\\n' . join ( [ '###\\n' ] + src . splitlines ( ) [ - 2 : ] ) \n    return ( bool ( last_two_blanks_re . match ( new_src ) ) or bool ( last_two_blanks_re2 . match ( new_src ) ) ) "}
{"13586": "\ndef push ( self , lines ) : \n    if self . input_mode == 'cell' : \n        self . reset ( ) \n    self . _store ( lines ) \n    source = self . source \n    self . code , self . _is_complete = None , None \n    if source . rstrip ( ) . endswith ( '\\\\' ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . _update_indent ( lines ) \n    try : \n        self . code = self . _compile ( source , symbol = \"exec\" ) \n    except ( SyntaxError , OverflowError , ValueError , TypeError , MemoryError ) : \n        self . _is_complete = True \n    else : \n        self . _is_complete = self . code is not None \n    return self . _is_complete "}
{"13587": "\ndef push_accepts_more ( self ) : \n    if not self . _is_complete : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if self . indent_spaces == 0 : \n        if self . input_mode == 'line' : \n            if not self . _full_dedent : \n                return_identifier_1 = False \n                return return_identifier_1 \n        else : \n            try : \n                code_ast = ast . parse ( u'' . join ( self . _buffer ) ) \n            except Exception : \n                return_identifier_2 = False \n                return return_identifier_2 \n            else : \n                if len ( code_ast . body ) == 1 : \n                    return_identifier_3 = False \n                    return return_identifier_3 \n    last_line = self . source . splitlines ( ) [ - 1 ] \n    return bool ( last_line and not last_line . isspace ( ) ) "}
{"13615": "\ndef _check_table ( self ) : \n    cursor = self . _db . execute ( \"PRAGMA table_info(%s)\" % self . table ) \n    lines = cursor . fetchall ( ) \n    if not lines : \n        return_identifier_0 = True \n        return return_identifier_0 \n    types = { } \n    keys = [ ] \n    for line in lines : \n        keys . append ( line [ 1 ] ) \n        types [ line [ 1 ] ] = line [ 2 ] \n    if self . _keys != keys : \n        self . log . warn ( 'keys mismatch' ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    for key in self . _keys : \n        if types [ key ] != self . _types [ key ] : \n            self . log . warn ( 'type mismatch: %s: %s != %s' % ( key , types [ key ] , self . _types [ key ] ) ) \n            return_identifier_2 = False \n            return return_identifier_2 \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"13625": "\ndef sub ( value , arg ) : \n    try : \n        return valid_numeric ( value ) - valid_numeric ( arg ) \n    except ( ValueError , TypeError ) : \n        try : \n            return value - arg \n        except Exception : \n            return_identifier_0 = '' \n            return return_identifier_0 "}
{"13626": "\ndef mul ( value , arg ) : \n    try : \n        return valid_numeric ( value ) * valid_numeric ( arg ) \n    except ( ValueError , TypeError ) : \n        try : \n            return value * arg \n        except Exception : \n            return_identifier_0 = '' \n            return return_identifier_0 "}
{"13627": "\ndef div ( value , arg ) : \n    try : \n        return valid_numeric ( value ) / valid_numeric ( arg ) \n    except ( ValueError , TypeError ) : \n        try : \n            return value / arg \n        except Exception : \n            return_identifier_0 = '' \n            return return_identifier_0 "}
{"13628": "\ndef mod ( value , arg ) : \n    try : \n        return valid_numeric ( value ) % valid_numeric ( arg ) \n    except ( ValueError , TypeError ) : \n        try : \n            return value % arg \n        except Exception : \n            return_identifier_0 = '' \n            return return_identifier_0 "}
{"13643": "\ndef do_help ( self , options , args , parser ) : \n    if options . help : \n        if self . classic : \n            self . help_fn ( topic = 'help' ) \n        else : \n            self . help_fn ( parser = parser ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    if \"help\" in options . actions : \n        if args : \n            for a in args : \n                parser = CMDS . get ( a ) \n                if parser : \n                    self . help_fn ( parser = parser ) \n                else : \n                    self . help_fn ( topic = a ) \n        else : \n            self . help_fn ( topic = 'help' ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    if options . version : \n        self . help_fn ( topic = 'version' ) \n        return_identifier_2 = True \n        return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"13644": "\ndef args_ok ( self , options , args ) : \n    for i in [ 'erase' , 'execute' ] : \n        for j in [ 'annotate' , 'html' , 'report' , 'combine' ] : \n            if ( i in options . actions ) and ( j in options . actions ) : \n                self . help_fn ( \"You can't specify the '%s' and '%s' \" \"options at the same time.\" % ( i , j ) ) \n                return_identifier_0 = False \n                return return_identifier_0 \n    if not options . actions : \n        self . help_fn ( \"You must specify at least one of -e, -x, -c, -r, -a, or -b.\" ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    args_allowed = ( 'execute' in options . actions or 'annotate' in options . actions or 'html' in options . actions or 'debug' in options . actions or 'report' in options . actions or 'xml' in options . actions ) \n    if not args_allowed and args : \n        self . help_fn ( \"Unexpected arguments: %s\" % \" \" . join ( args ) ) \n        return_identifier_2 = False \n        return return_identifier_2 \n    if 'execute' in options . actions and not args : \n        self . help_fn ( \"Nothing to do.\" ) \n        return_identifier_3 = False \n        return return_identifier_3 \n    return_identifier_4 = True \n    return return_identifier_4 "}
{"13650": "\ndef is_url ( url ) : \n    if '://' not in url : \n        return_identifier_0 = False \n        return return_identifier_0 \n    proto , addr = url . split ( '://' , 1 ) \n    if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : \n        return_identifier_1 = False \n        return return_identifier_1 \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"13651": "\ndef validate_url ( url ) : \n    if not isinstance ( url , basestring ) : \n        raise TypeError ( \"url must be a string, not %r\" % type ( url ) ) \n    url = url . lower ( ) \n    proto_addr = url . split ( '://' ) \n    assert len ( proto_addr ) == 2 , 'Invalid url: %r' % url \n    proto , addr = proto_addr \n    assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , \"Invalid protocol: %r\" % proto \n    pat = re . compile ( r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$' ) \n    if proto == 'tcp' : \n        lis = addr . split ( ':' ) \n        assert len ( lis ) == 2 , 'Invalid url: %r' % url \n        addr , s_port = lis \n        try : \n            port = int ( s_port ) \n        except ValueError : \n            raise AssertionError ( \"Invalid port %r in url: %r\" % ( port , url ) ) \n        assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url \n    else : \n        pass \n    return_identifier_0 = True \n    return return_identifier_0 "}
{"13672": "\ndef _ofind ( self , oname , namespaces = None ) : \n    oname = oname . strip ( ) \n    if not oname . startswith ( ESC_MAGIC ) and not oname . startswith ( ESC_MAGIC2 ) and not py3compat . isidentifier ( oname , dotted = True ) : \n        return dict ( found = False ) \n    alias_ns = None \n    if namespaces is None : \n        namespaces = [ ( 'Interactive' , self . user_ns ) , ( 'Interactive (global)' , self . user_global_ns ) , ( 'Python builtin' , builtin_mod . __dict__ ) , ( 'Alias' , self . alias_manager . alias_table ) , ] \n        alias_ns = self . alias_manager . alias_table \n    found = False ; \n    obj = None ; \n    ospace = None ; \n    ds = None ; \n    ismagic = False ; \n    isalias = False ; \n    parent = None \n    if ( oname == 'print' and not py3compat . PY3 and not ( self . compile . compiler_flags & __future__ . CO_FUTURE_PRINT_FUNCTION ) ) : \n        return_identifier_0 = { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n        return return_identifier_0 \n    oname_parts = oname . split ( '.' ) \n    oname_head , oname_rest = oname_parts [ 0 ] , oname_parts [ 1 : ] \n    for nsname , ns in namespaces : \n        try : \n            obj = ns [ oname_head ] \n        except KeyError : \n            continue \n        else : \n            for part in oname_rest : \n                try : \n                    parent = obj \n                    obj = getattr ( obj , part ) \n                except : \n                    break \n            else : \n                found = True \n                ospace = nsname \n                if ns == alias_ns : \n                    isalias = True \n                break \n    if not found : \n        obj = None \n        if oname . startswith ( ESC_MAGIC2 ) : \n            oname = oname . lstrip ( ESC_MAGIC2 ) \n            obj = self . find_cell_magic ( oname ) \n        elif oname . startswith ( ESC_MAGIC ) : \n            oname = oname . lstrip ( ESC_MAGIC ) \n            obj = self . find_line_magic ( oname ) \n        else : \n            obj = self . find_line_magic ( oname ) \n            if obj is None : \n                obj = self . find_cell_magic ( oname ) \n        if obj is not None : \n            found = True \n            ospace = 'IPython internal' \n            ismagic = True \n    if not found and oname_head in [ \"''\" , '\"\"' , '[]' , '{}' , '()' ] : \n        obj = eval ( oname_head ) \n        found = True \n        ospace = 'Interactive' \n    return_identifier_1 = { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n    return return_identifier_1 "}
{"13696": "\ndef run_ast_nodes ( self , nodelist , cell_name , interactivity = 'last_expr' ) : \n    if not nodelist : \n        return \n    if interactivity == 'last_expr' : \n        if isinstance ( nodelist [ - 1 ] , ast . Expr ) : \n            interactivity = \"last\" \n        else : \n            interactivity = \"none\" \n    if interactivity == 'none' : \n        to_run_exec , to_run_interactive = nodelist , [ ] \n    elif interactivity == 'last' : \n        to_run_exec , to_run_interactive = nodelist [ : - 1 ] , nodelist [ - 1 : ] \n    elif interactivity == 'all' : \n        to_run_exec , to_run_interactive = [ ] , nodelist \n    else : \n        raise ValueError ( \"Interactivity was %r\" % interactivity ) \n    exec_count = self . execution_count \n    try : \n        for i , node in enumerate ( to_run_exec ) : \n            mod = ast . Module ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"exec\" ) \n            if self . run_code ( code ) : \n                return_identifier_0 = True \n                return return_identifier_0 \n        for i , node in enumerate ( to_run_interactive ) : \n            mod = ast . Interactive ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"single\" ) \n            if self . run_code ( code ) : \n                return_identifier_1 = True \n                return return_identifier_1 \n        if softspace ( sys . stdout , 0 ) : \n            print \n    except : \n        self . showtraceback ( ) \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"13725": "\ndef last_error ( self ) : \n    if not len ( self . log ) : \n        raise RuntimeError ( 'Nothing executed' ) \n    try : \n        errs = [ l for l in self . log if l [ 1 ] != 0 ] \n        return errs [ - 1 ] [ 2 ] \n    except IndexError : \n        return_identifier_0 = 'no last error' \n        return return_identifier_0 "}
{"13738": "\ndef highlight_text ( needles , haystack , cls_name = 'highlighted' , words = False , case = False ) : \n    if not needles : \n        return haystack \n    if not haystack : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    if words : \n        pattern = r\"(%s)\" % \"|\" . join ( [ '\\\\b{}\\\\b' . format ( re . escape ( n ) ) for n in needles ] ) \n    else : \n        pattern = r\"(%s)\" % \"|\" . join ( [ re . escape ( n ) for n in needles ] ) \n    if case : \n        regex = re . compile ( pattern ) \n    else : \n        regex = re . compile ( pattern , re . I ) \n    i , out = 0 , \"\" \n    for m in regex . finditer ( haystack ) : \n        out += \"\" . join ( [ haystack [ i : m . start ( ) ] , '<span class=\"%s\">' % cls_name , haystack [ m . start ( ) : m . end ( ) ] , \"</span>\" ] ) \n        i = m . end ( ) \n    return mark_safe ( out + haystack [ i : ] ) "}
{"13739": "\ndef highlight ( string , keywords , cls_name = 'highlighted' ) : \n    if not keywords : \n        return string \n    if not string : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    include , exclude = get_text_tokenizer ( keywords ) \n    highlighted = highlight_text ( include , string , cls_name ) \n    return highlighted "}
{"13740": "\ndef highlight_words ( string , keywords , cls_name = 'highlighted' ) : \n    if not keywords : \n        return string \n    if not string : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    include , exclude = get_text_tokenizer ( keywords ) \n    highlighted = highlight_text ( include , string , cls_name , words = True ) \n    return highlighted "}
{"13748": "\ndef _find_optimal ( rlist , separator_size = 2 , displaywidth = 80 ) : \n    for nrow in range ( 1 , len ( rlist ) + 1 ) : \n        chk = map ( max , _chunks ( rlist , nrow ) ) \n        sumlength = sum ( chk ) \n        ncols = len ( chk ) \n        if sumlength + separator_size * ( ncols - 1 ) <= displaywidth : \n            break ; \n    return_identifier_0 = { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows_numbers' : nrow , 'columns_width' : chk } \n    return return_identifier_0 "}
{"13750": "\ndef compute_item_matrix ( items , empty = None , * args , ** kwargs ) : \n    info = _find_optimal ( map ( len , items ) , * args , ** kwargs ) \n    nrow , ncol = info [ 'rows_numbers' ] , info [ 'columns_numbers' ] \n    return_identifier_0 = ( [ [ _get_or_default ( items , c * nrow + i , default = empty ) for c in range ( ncol ) ] for i in range ( nrow ) ] , info ) \n    return return_identifier_0 "}
{"13813": "\ndef formatFailure ( self , test , err ) : \n    ec , ev , tb = err \n    tbinfo = inspect_traceback ( tb ) \n    test . tbinfo = tbinfo \n    return_identifier_0 = ( ec , '\\n' . join ( [ str ( ev ) , tbinfo ] ) , tb ) \n    return return_identifier_0 "}
{"13860": "\ndef price_options ( S = 100.0 , K = 100.0 , sigma = 0.25 , r = 0.05 , days = 260 , paths = 10000 ) : \n    import numpy as np \n    from math import exp , sqrt \n    h = 1.0 / days \n    const1 = exp ( ( r - 0.5 * sigma ** 2 ) * h ) \n    const2 = sigma * sqrt ( h ) \n    stock_price = S * np . ones ( paths , dtype = 'float64' ) \n    stock_price_sum = np . zeros ( paths , dtype = 'float64' ) \n    for j in range ( days ) : \n        growth_factor = const1 * np . exp ( const2 * np . random . standard_normal ( paths ) ) \n        stock_price = stock_price * growth_factor \n        stock_price_sum = stock_price_sum + stock_price \n    stock_price_avg = stock_price_sum / days \n    zeros = np . zeros ( paths , dtype = 'float64' ) \n    r_factor = exp ( - r * h * days ) \n    euro_put = r_factor * np . mean ( np . maximum ( zeros , K - stock_price ) ) \n    asian_put = r_factor * np . mean ( np . maximum ( zeros , K - stock_price_avg ) ) \n    euro_call = r_factor * np . mean ( np . maximum ( zeros , stock_price - K ) ) \n    asian_call = r_factor * np . mean ( np . maximum ( zeros , stock_price_avg - K ) ) \n    return_identifier_0 = ( euro_call , euro_put , asian_call , asian_put ) \n    return return_identifier_0 "}
{"13866": "\ndef mappable ( obj ) : \n    if isinstance ( obj , ( tuple , list ) ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    for m in arrayModules : \n        if isinstance ( obj , m [ 'type' ] ) : \n            return_identifier_1 = True \n            return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"13892": "\ndef allow ( self , record ) : \n    if not self : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return self . _allow ( record ) and not self . _deny ( record ) "}
{"13894": "\ndef formatError ( self , test , err ) : \n    test . capturedLogging = records = self . formatLogRecords ( ) \n    if not records : \n        return err \n    ec , ev , tb = err \n    return_identifier_0 = ( ec , self . addCaptureToErr ( ev , records ) , tb ) \n    return return_identifier_0 "}
{"13902": "\ndef init_parser ( ) : \n    usage = \"usage: %prog -u user -s secret -n name [-l label] \\[-t title] [-c callback] [TEXT]\" \n    parser = OptionParser ( usage , version = \"%prog \" + notifo . __version__ ) \n    parser . add_option ( \"-u\" , \"--user\" , action = \"store\" , dest = \"user\" , help = \"your notifo username\" ) \n    parser . add_option ( \"-s\" , \"--secret\" , action = \"store\" , dest = \"secret\" , help = \"your notifo API secret\" ) \n    parser . add_option ( \"-n\" , \"--name\" , action = \"store\" , dest = \"name\" , help = \"recipient for the notification\" ) \n    parser . add_option ( \"-l\" , \"--label\" , action = \"store\" , dest = \"label\" , help = \"label for the notification\" ) \n    parser . add_option ( \"-t\" , \"--title\" , action = \"store\" , dest = \"title\" , help = \"title of the notification\" ) \n    parser . add_option ( \"-c\" , \"--callback\" , action = \"store\" , dest = \"callback\" , help = \"callback URL to call\" ) \n    parser . add_option ( \"-m\" , \"--message\" , action = \"store_true\" , dest = \"message\" , default = False , help = \"send message instead of notification\" ) \n    ( options , args ) = parser . parse_args ( ) \n    return_identifier_0 = ( parser , options , args ) \n    return return_identifier_0 "}
{"13907": "\ndef html_tableify ( item_matrix , select = None , header = None , footer = None ) : \n    if not item_matrix : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    html_cols = [ ] \n    tds = lambda text : u'<td>' + text + u'  </td>' \n    trs = lambda text : u'<tr>' + text + u'</tr>' \n    tds_items = [ map ( tds , row ) for row in item_matrix ] \n    if select : \n        row , col = select \n        tds_items [ row ] [ col ] = u'<td class=\"inverted\">' + item_matrix [ row ] [ col ] + u'  </td>' \n    html_cols = map ( trs , ( u'' . join ( row ) for row in tds_items ) ) \n    head = '' \n    foot = '' \n    if header : \n        head = ( u'<tr>' + '' . join ( ( u'<td>' + header + u'</td>' ) * len ( item_matrix [ 0 ] ) ) + '</tr>' ) \n    if footer : \n        foot = ( u'<tr>' + '' . join ( ( u'<td>' + footer + u'</td>' ) * len ( item_matrix [ 0 ] ) ) + '</tr>' ) \n    html = ( u'<table class=\"completion\" style=\"white-space:pre\">' + head + ( u'' . join ( html_cols ) ) + foot + u'</table>' ) \n    return html "}
{"13987": "\ndef attr_matches ( self , text ) : \n    m = re . match ( r\"(\\S+(\\.\\w+)*)\\.(\\w*)$\" , text ) \n    if m : \n        expr , attr = m . group ( 1 , 3 ) \n    elif self . greedy : \n        m2 = re . match ( r\"(.+)\\.(\\w*)$\" , self . line_buffer ) \n        if not m2 : \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n        expr , attr = m2 . group ( 1 , 2 ) \n    else : \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    try : \n        obj = eval ( expr , self . namespace ) \n    except : \n        try : \n            obj = eval ( expr , self . global_namespace ) \n        except : \n            return_identifier_2 = [ ] \n            return return_identifier_2 \n    if self . limit_to__all__ and hasattr ( obj , '__all__' ) : \n        words = get__all__entries ( obj ) \n    else : \n        words = dir2 ( obj ) \n    try : \n        words = generics . complete_object ( obj , words ) \n    except TryNext : \n        pass \n    except Exception : \n        pass \n    n = len ( attr ) \n    res = [ \"%s.%s\" % ( expr , w ) for w in words if w [ : n ] == attr ] \n    return res "}
{"13989": "\ndef file_matches ( self , text ) : \n    if text . startswith ( '!' ) : \n        text = text [ 1 : ] \n        text_prefix = '!' \n    else : \n        text_prefix = '' \n    text_until_cursor = self . text_until_cursor \n    open_quotes = has_open_quotes ( text_until_cursor ) \n    if '(' in text_until_cursor or '[' in text_until_cursor : \n        lsplit = text \n    else : \n        try : \n            lsplit = arg_split ( text_until_cursor ) [ - 1 ] \n        except ValueError : \n            if open_quotes : \n                lsplit = text_until_cursor . split ( open_quotes ) [ - 1 ] \n            else : \n                return_identifier_0 = [ ] \n                return return_identifier_0 \n        except IndexError : \n            lsplit = \"\" \n    if not open_quotes and lsplit != protect_filename ( lsplit ) : \n        has_protectables = True \n        text0 , text = text , lsplit \n    else : \n        has_protectables = False \n        text = os . path . expanduser ( text ) \n    if text == \"\" : \n        return [ text_prefix + protect_filename ( f ) for f in self . glob ( \"*\" ) ] \n    m0 = self . clean_glob ( text . replace ( '\\\\' , '' ) ) \n    if has_protectables : \n        len_lsplit = len ( lsplit ) \n        matches = [ text_prefix + text0 + protect_filename ( f [ len_lsplit : ] ) for f in m0 ] \n    else : \n        if open_quotes : \n            matches = m0 \n        else : \n            matches = [ text_prefix + protect_filename ( f ) for f in m0 ] \n    matches = [ x + '/' if os . path . isdir ( x ) else x for x in matches ] \n    return matches "}
{"13990": "\ndef alias_matches ( self , text ) : \n    main_text = self . text_until_cursor . lstrip ( ) \n    if ' ' in main_text and not main_text . startswith ( 'sudo' ) : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    text = os . path . expanduser ( text ) \n    aliases = self . alias_table . keys ( ) \n    if text == '' : \n        return aliases \n    else : \n        return [ a for a in aliases if a . startswith ( text ) ] "}
{"13992": "\ndef _default_arguments ( self , obj ) : \n    if not ( inspect . isfunction ( obj ) or inspect . ismethod ( obj ) ) : \n        if inspect . isclass ( obj ) : \n            obj = ( getattr ( obj , '__init__' , None ) or getattr ( obj , '__new__' , None ) ) \n        elif hasattr ( obj , '__call__' ) : \n            obj = obj . __call__ \n    try : \n        args , _ , _1 , defaults = inspect . getargspec ( obj ) \n        if defaults : \n            return args [ - len ( defaults ) : ] \n    except TypeError : \n        pass \n    return_identifier_0 = [ ] \n    return return_identifier_0 "}
{"13995": "\ndef _match_one ( self , rec , tests ) : \n    for key , test in tests . iteritems ( ) : \n        if not test ( rec . get ( key , None ) ) : \n            return_identifier_0 = False \n            return return_identifier_0 \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"13998": "\ndef quiet ( self ) : \n    try : \n        cell = self . shell . history_manager . input_hist_parsed [ self . prompt_count ] \n        if cell . rstrip ( ) . endswith ( ';' ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    except IndexError : \n        pass \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14019": "\ndef ispackage ( path ) : \n    if os . path . isdir ( path ) : \n        end = os . path . basename ( path ) \n        if ident_re . match ( end ) : \n            for init in ( '__init__.py' , '__init__.pyc' , '__init__.pyo' ) : \n                if os . path . isfile ( os . path . join ( path , init ) ) : \n                    return_identifier_0 = True \n                    return return_identifier_0 \n            if sys . platform . startswith ( 'java' ) and os . path . isfile ( os . path . join ( path , '__init__$py.class' ) ) : \n                return_identifier_1 = True \n                return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"14022": "\ndef regex_last_key ( regex ) : \n    def k ( obj ) : \n        if regex . search ( obj ) : \n            return_identifier_0 = ( 1 , obj ) \n            return return_identifier_0 \n        return_identifier_1 = ( 0 , obj ) \n        return return_identifier_1 \n    return k "}
{"14027": "\ndef get_open_files ( self ) : \n    if self . pid == 0 : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    files = [ ] \n    rawlist = _psutil_osx . get_process_open_files ( self . pid ) \n    for path , fd in rawlist : \n        if isfile_strict ( path ) : \n            ntuple = nt_openfile ( path , fd ) \n            files . append ( ntuple ) \n    return files "}
{"14029": "\ndef user_has_group ( user , group , superuser_skip = True ) : \n    if user . is_superuser and superuser_skip : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return user . groups . filter ( name = group ) . exists ( ) "}
{"14044": "\ndef check_url_accessibility ( url , timeout = 10 ) : \n    if ( url == 'localhost' ) : \n        url = 'http://127.0.0.1' \n    try : \n        req = urllib2 . urlopen ( url , timeout = timeout ) \n        if ( req . getcode ( ) == 200 ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    except Exception : \n        pass \n    fail ( \"URL '%s' is not accessible from this machine\" % url ) "}
{"14047": "\ndef compare_content_type ( url , content_type ) : \n    try : \n        response = urllib2 . urlopen ( url ) \n    except : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return response . headers . type == content_type "}
{"14048": "\ndef compare_response_code ( url , code ) : \n    try : \n        response = urllib2 . urlopen ( url ) \n    except HTTPError as e : \n        return e . code == code \n    except : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return response . code == code "}
{"14055": "\ndef should_be_python ( self ) : \n    _ , ext = os . path . splitext ( self . filename ) \n    if ext . startswith ( '.py' ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    if not ext : \n        return_identifier_1 = True \n        return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"14073": "\ndef match ( self , fpath ) : \n    for d in self . dirs : \n        if fpath . startswith ( d ) : \n            if fpath == d : \n                return_identifier_0 = True \n                return return_identifier_0 \n            if fpath [ len ( d ) ] == os . sep : \n                return_identifier_1 = True \n                return return_identifier_1 \n    return_identifier_2 = False \n    return return_identifier_2 "}
{"14074": "\ndef match ( self , fpath ) : \n    for pat in self . pats : \n        if fnmatch . fnmatch ( fpath , pat ) : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14077": "\ndef loop_wx ( kernel ) : \n    import wx \n    from IPython . lib . guisupport import start_event_loop_wx \n    doi = kernel . do_one_iteration \n    poll_interval = int ( 1000 * kernel . _poll_interval ) \n    class TimerFrame ( wx . Frame ) : \n        def __init__ ( self , func ) : \n            wx . Frame . __init__ ( self , None , - 1 ) \n            self . timer = wx . Timer ( self ) \n            self . timer . Start ( poll_interval ) \n            self . Bind ( wx . EVT_TIMER , self . on_timer ) \n            self . func = func \n        def on_timer ( self , event ) : \n            self . func ( ) \n    class IPWxApp ( wx . App ) : \n        def OnInit ( self ) : \n            self . frame = TimerFrame ( doi ) \n            self . frame . Show ( False ) \n            return_identifier_0 = True \n            return return_identifier_0 \n    kernel . app = IPWxApp ( redirect = False ) \n    import signal \n    if not callable ( signal . getsignal ( signal . SIGINT ) ) : \n        signal . signal ( signal . SIGINT , signal . default_int_handler ) \n    start_event_loop_wx ( kernel . app ) "}
{"14087": "\ndef parse_step ( cls , ctxt , step_addr , step_conf ) : \n    if isinstance ( step_conf , six . string_types ) : \n        step_conf = { step_conf : None } \n    elif not isinstance ( step_conf , collections . Mapping ) : \n        raise ConfigError ( 'Unable to parse step configuration: expecting string or ' 'dictionary, not \"%s\"' % step_conf . __class__ . __name__ , step_addr , ) \n    action_item = None \n    mod_items = { } \n    kwargs = { } \n    for key , key_conf in step_conf . items ( ) : \n        if key in cls . schemas : \n            utils . schema_validate ( key_conf , cls . schemas [ key ] , ConfigError , key , step_addr = step_addr ) \n            kwargs [ key ] = key_conf \n        elif key in entry . points [ NAMESPACE_ACTION ] : \n            if action_item is not None : \n                raise ConfigError ( 'Bad step configuration: action \"%s\" specified, ' 'but action \"%s\" already processed' % ( key , action_item . name ) , step_addr , ) \n            action_item = StepItem ( entry . points [ NAMESPACE_ACTION ] [ key ] , key , key_conf ) \n        elif key in entry . points [ NAMESPACE_MODIFIER ] : \n            mod_class = entry . points [ NAMESPACE_MODIFIER ] [ key ] \n            mod_items . setdefault ( mod_class . priority , [ ] ) \n            mod_items [ mod_class . priority ] . append ( StepItem ( mod_class , key , key_conf ) ) \n        else : \n            raise ConfigError ( 'Bad step configuration: unable to resolve action ' '\"%s\"' % key , step_addr , ) \n    if action_item is None : \n        raise ConfigError ( 'Bad step configuration: no action specified' , step_addr , ) \n    action_type = ( Modifier . STEP if action_item . cls . step_action else Modifier . NORMAL ) \n    modifiers = [ ] \n    for mod_item in utils . iter_prio_dict ( mod_items ) : \n        if mod_item . cls . restriction & action_type == 0 : \n            raise ConfigError ( 'Bad step configuration: modifier \"%s\" is ' 'incompatible with the action \"%s\"' % ( mod_item . name , action_item . name ) , step_addr , ) \n        modifier = mod_item . init ( ctxt , step_addr ) \n        modifiers . append ( modifier ) \n        action_item . conf = modifier . action_conf ( ctxt , action_item . cls , action_item . name , action_item . conf , step_addr ) \n    action = action_item . init ( ctxt , step_addr ) \n    step = cls ( step_addr , action , modifiers , ** kwargs ) \n    if action_item . cls . step_action : \n        return step ( ctxt ) \n    return_identifier_0 = [ step ] \n    return return_identifier_0 "}
{"14108": "\ndef raw_input ( self , prompt = '' ) : \n    if self . has_readline : \n        self . set_readline_completer ( ) \n    prompt = py3compat . cast_bytes_py2 ( prompt ) \n    try : \n        line = py3compat . str_to_unicode ( self . raw_input_original ( prompt ) ) \n    except ValueError : \n        warn ( \"\\n********\\nYou or a %run:ed script called sys.stdin.close()\" \" or sys.stdout.close()!\\nExiting IPython!\\n\" ) \n        self . ask_exit ( ) \n        return_identifier_0 = \"\" \n        return return_identifier_0 \n    if self . autoindent : \n        if num_ini_spaces ( line ) > self . indent_current_nsp : \n            line = line [ self . indent_current_nsp : ] \n            self . indent_current_nsp = 0 \n    return line "}
{"14110": "\ndef _should_recompile ( self , e ) : \n    if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<BackgroundJob compilation>' , None ) : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        if ( self . autoedit_syntax and not self . ask_yes_no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : \n            return_identifier_1 = False \n            return return_identifier_1 \n    except EOFError : \n        return_identifier_2 = False \n        return return_identifier_2 \n    def int0 ( x ) : \n        try : \n            return int ( x ) \n        except TypeError : \n            return_identifier_3 = 0 \n            return return_identifier_3 \n    try : \n        self . hooks . fix_error_editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) \n    except TryNext : \n        warn ( 'Could not open editor' ) \n        return_identifier_4 = False \n        return return_identifier_4 \n    return_identifier_5 = True \n    return return_identifier_5 "}
{"14115": "\ndef info ( self ) : \n    return_identifier_0 = ( self . identity , self . url , self . pub_url , self . location ) \n    return return_identifier_0 "}
{"14128": "\ndef ensure_fromlist ( mod , fromlist , buf , recursive ) : \n    if not hasattr ( mod , '__path__' ) : \n        return \n    for item in fromlist : \n        if not hasattr ( item , 'rindex' ) : \n            raise TypeError ( \"Item in ``from list'' not a string\" ) \n        if item == '*' : \n            if recursive : \n                continue \n            try : \n                all = mod . __all__ \n            except AttributeError : \n                pass \n            else : \n                ret = ensure_fromlist ( mod , all , buf , 1 ) \n                if not ret : \n                    return_identifier_0 = 0 \n                    return return_identifier_0 \n        elif not hasattr ( mod , item ) : \n            import_submodule ( mod , item , buf + '.' + item ) "}
{"14153": "\ndef can_print_latex ( o ) : \n    import sympy \n    if isinstance ( o , ( list , tuple , set , frozenset ) ) : \n        return all ( can_print_latex ( i ) for i in o ) \n    elif isinstance ( o , dict ) : \n        return all ( ( isinstance ( i , basestring ) or can_print_latex ( i ) ) and can_print_latex ( o [ i ] ) for i in o ) \n    elif isinstance ( o , ( sympy . Basic , sympy . matrices . Matrix , int , long , float ) ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14178": "\ndef is_alive ( self ) : \n    if self . has_kernel : \n        if self . kernel . poll ( ) is None : \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            return_identifier_1 = False \n            return return_identifier_1 \n    elif self . _hb_channel is not None : \n        return self . _hb_channel . is_beating ( ) \n    else : \n        return_identifier_2 = True \n        return return_identifier_2 "}
{"14185": "\ndef pre_step ( self , ctxt , step , idx ) : \n    debugger = ExtensionDebugger ( 'pre_step' ) \n    for ext in self . exts : \n        with debugger ( ext ) : \n            if ext . pre_step ( ctxt , step , idx ) : \n                debugger . debug ( 3 , 'Skipping step %d' % idx ) \n                return_identifier_0 = True \n                return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14189": "\ndef scan_module ( egg_dir , base , name , stubs ) : \n    filename = os . path . join ( base , name ) \n    if filename [ : - 1 ] in stubs : \n        return_identifier_0 = True \n        return return_identifier_0 \n    pkg = base [ len ( egg_dir ) + 1 : ] . replace ( os . sep , '.' ) \n    module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] \n    if sys . version_info < ( 3 , 3 ) : \n        skip = 8 \n    else : \n        skip = 12 \n    f = open ( filename , 'rb' ) ; \n    f . read ( skip ) \n    code = marshal . load ( f ) ; \n    f . close ( ) \n    safe = True \n    symbols = dict . fromkeys ( iter_symbols ( code ) ) \n    for bad in [ '__file__' , '__path__' ] : \n        if bad in symbols : \n            log . warn ( \"%s: module references %s\" , module , bad ) \n            safe = False \n    if 'inspect' in symbols : \n        for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : \n            if bad in symbols : \n                log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) \n                safe = False \n    if '__name__' in symbols and '__main__' in symbols and '.' not in module : \n        if sys . version [ : 3 ] == \"2.4\" : \n            log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) \n            safe = False \n    return safe "}
{"14197": "\ndef pxrun_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    ipself = self . shell \n    with ipself . builtin_trap : \n        cell = ipself . prefilter_manager . prefilter_lines ( raw_cell ) \n        if store_history : \n            ipself . history_manager . store_inputs ( ipself . execution_count , cell , raw_cell ) \n        cell_name = ipself . compile . cache ( cell , ipself . execution_count ) \n        try : \n            ast . parse ( cell , filename = cell_name ) \n        except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n            ipself . showsyntaxerror ( ) \n            ipself . execution_count += 1 \n            return None \n        except NameError : \n            pass \n    if store_history : \n        ipself . history_manager . store_output ( ipself . execution_count ) \n        ipself . execution_count += 1 \n    if re . search ( r'get_ipython\\(\\)\\.magic\\(u?[\"\\']%?autopx' , cell ) : \n        self . _disable_autopx ( ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        try : \n            result = self . view . execute ( cell , silent = False , block = False ) \n        except : \n            ipself . showtraceback ( ) \n            return_identifier_1 = True \n            return return_identifier_1 \n        else : \n            if self . view . block : \n                try : \n                    result . get ( ) \n                except : \n                    self . shell . showtraceback ( ) \n                    return_identifier_2 = True \n                    return return_identifier_2 \n                else : \n                    with ipself . builtin_trap : \n                        result . display_outputs ( ) \n            return_identifier_3 = False \n            return return_identifier_3 "}
{"14219": "\ndef _try_passwordless_openssh ( server , keyfile ) : \n    if pexpect is None : \n        raise ImportError ( \"pexpect unavailable, use paramiko\" ) \n    cmd = 'ssh -f ' + server \n    if keyfile : \n        cmd += ' -i ' + keyfile \n    cmd += ' exit' \n    p = pexpect . spawn ( cmd ) \n    while True : \n        try : \n            p . expect ( '[Pp]assword:' , timeout = .1 ) \n        except pexpect . TIMEOUT : \n            continue \n        except pexpect . EOF : \n            return_identifier_0 = True \n            return return_identifier_0 \n        else : \n            return_identifier_1 = False \n            return return_identifier_1 "}
{"14220": "\ndef _try_passwordless_paramiko ( server , keyfile ) : \n    if paramiko is None : \n        msg = \"Paramiko unavaliable, \" \n        if sys . platform == 'win32' : \n            msg += \"Paramiko is required for ssh tunneled connections on Windows.\" \n        else : \n            msg += \"use OpenSSH.\" \n        raise ImportError ( msg ) \n    username , server , port = _split_server ( server ) \n    client = paramiko . SSHClient ( ) \n    client . load_system_host_keys ( ) \n    client . set_missing_host_key_policy ( paramiko . WarningPolicy ( ) ) \n    try : \n        client . connect ( server , port , username = username , key_filename = keyfile , look_for_keys = True ) \n    except paramiko . AuthenticationException : \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        client . close ( ) \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"14236": "\ndef wait ( self , jobs = None , timeout = - 1 ) : \n    tic = time . time ( ) \n    if jobs is None : \n        theids = self . outstanding \n    else : \n        if isinstance ( jobs , ( int , basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        theids = set ( ) \n        for job in jobs : \n            if isinstance ( job , int ) : \n                job = self . history [ job ] \n            elif isinstance ( job , AsyncResult ) : \n                map ( theids . add , job . msg_ids ) \n                continue \n            theids . add ( job ) \n    if not theids . intersection ( self . outstanding ) : \n        return_identifier_0 = True \n        return return_identifier_0 \n    self . spin ( ) \n    while theids . intersection ( self . outstanding ) : \n        if timeout >= 0 and ( time . time ( ) - tic ) > timeout : \n            break \n        time . sleep ( 1e-3 ) \n        self . spin ( ) \n    return len ( theids . intersection ( self . outstanding ) ) == 0 "}
{"14265": "\ndef wantFile ( self , file , package = None ) : \n    if self . coverInclusive : \n        if file . endswith ( \".py\" ) : \n            if package and self . coverPackages : \n                for want in self . coverPackages : \n                    if package . startswith ( want ) : \n                        return_identifier_0 = True \n                        return return_identifier_0 \n            else : \n                return_identifier_1 = True \n                return return_identifier_1 \n    return None "}
{"14270": "\ndef root_topic ( self ) : \n    if isinstance ( getattr ( self . engine , 'id' , None ) , int ) : \n        return \"engine.%i\" % self . engine . id \n    else : \n        return_identifier_0 = \"engine\" \n        return return_identifier_0 "}
{"14273": "\ndef formatError ( self , test , err ) : \n    test . capturedOutput = output = self . buffer \n    self . _buf = None \n    if not output : \n        return err \n    ec , ev , tb = err \n    return_identifier_0 = ( ec , self . addCaptureToErr ( ev , output ) , tb ) \n    return return_identifier_0 "}
{"14276": "\ndef hex_to_rgb ( color ) : \n    if color . startswith ( '#' ) : \n        color = color [ 1 : ] \n    if len ( color ) == 3 : \n        color = '' . join ( [ c * 2 for c in color ] ) \n    if len ( color ) != 6 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        r = int ( color [ : 2 ] , 16 ) \n        g = int ( color [ 2 : 4 ] , 16 ) \n        b = int ( color [ 4 : ] , 16 ) \n    except ValueError : \n        return_identifier_1 = False \n        return return_identifier_1 \n    else : \n        return r , g , b "}
{"14286": "\ndef _process_execute_payload ( self , item ) : \n    handler = self . _payload_handlers . get ( item [ 'source' ] ) \n    if handler is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        handler ( item ) \n        return_identifier_1 = True \n        return return_identifier_1 "}
{"14307": "\ndef nextset ( self ) : \n    if self . _executed : \n        self . fetchall ( ) \n    del self . messages [ : ] \n    db = self . _get_db ( ) \n    nr = db . next_result ( ) \n    if nr == - 1 : \n        return None \n    self . _do_get_result ( ) \n    self . _post_get_result ( ) \n    self . _warning_check ( ) \n    return_identifier_0 = 1 \n    return return_identifier_0 "}
{"14322": "\ndef wantFile ( self , file ) : \n    base = op_basename ( file ) \n    ignore_matches = [ ignore_this for ignore_this in self . ignoreFiles if ignore_this . search ( base ) ] \n    if ignore_matches : \n        log . debug ( '%s matches ignoreFiles pattern; skipped' , base ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    if not self . config . includeExe and os . access ( file , os . X_OK ) : \n        log . info ( '%s is executable; skipped' , file ) \n        return_identifier_1 = False \n        return return_identifier_1 \n    dummy , ext = op_splitext ( base ) \n    pysrc = ext == '.py' \n    wanted = pysrc and self . matches ( base ) \n    plug_wants = self . plugins . wantFile ( file ) \n    if plug_wants is not None : \n        log . debug ( \"plugin setting want %s to %s\" , file , plug_wants ) \n        wanted = plug_wants \n    log . debug ( \"wantFile %s? %s\" , file , wanted ) \n    return wanted "}
{"14323": "\ndef wantFunction ( self , function ) : \n    try : \n        if hasattr ( function , 'compat_func_name' ) : \n            funcname = function . compat_func_name \n        else : \n            funcname = function . __name__ \n    except AttributeError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    declared = getattr ( function , '__test__' , None ) \n    if declared is not None : \n        wanted = declared \n    else : \n        wanted = not funcname . startswith ( '_' ) and self . matches ( funcname ) \n    plug_wants = self . plugins . wantFunction ( function ) \n    if plug_wants is not None : \n        wanted = plug_wants \n    log . debug ( \"wantFunction %s? %s\" , function , wanted ) \n    return wanted "}
{"14324": "\ndef wantMethod ( self , method ) : \n    try : \n        method_name = method . __name__ \n    except AttributeError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if method_name . startswith ( '_' ) : \n        return_identifier_1 = False \n        return return_identifier_1 \n    declared = getattr ( method , '__test__' , None ) \n    if declared is not None : \n        wanted = declared \n    else : \n        wanted = self . matches ( method_name ) \n    plug_wants = self . plugins . wantMethod ( method ) \n    if plug_wants is not None : \n        wanted = plug_wants \n    log . debug ( \"wantMethod %s? %s\" , method , wanted ) \n    return wanted "}
{"14326": "\ndef _file_lines ( fname ) : \n    try : \n        outfile = open ( fname ) \n    except IOError : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    else : \n        out = outfile . readlines ( ) \n        outfile . close ( ) \n        return out "}
{"14348": "\ndef wantDirectory ( self , dirname ) : \n    if dirname in self . exclude_dirs : \n        log . debug ( \"excluded: %s\" % dirname ) \n        return_identifier_0 = False \n        return return_identifier_0 \n    else : \n        return None "}
{"14349": "\ndef links_to_dynamic ( self , ext ) : \n    libnames = dict . fromkeys ( [ lib . _full_name for lib in self . shlibs ] ) \n    pkg = '.' . join ( ext . _full_name . split ( '.' ) [ : - 1 ] + [ '' ] ) \n    for libname in ext . libraries : \n        if pkg + libname in libnames : \n            return_identifier_0 = True \n            return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14367": "\ndef _tab_pressed ( self ) : \n    text = self . _get_input_buffer_cursor_line ( ) \n    if text is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    complete = bool ( text [ : self . _get_input_buffer_cursor_column ( ) ] . strip ( ) ) \n    if complete : \n        self . _complete ( ) \n    return not complete "}
{"14369": "\ndef _event_filter_console_keypress ( self , event ) : \n    key = event . key ( ) \n    if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : \n        if key == QtCore . Qt . Key_C and self . _executing : \n            self . request_interrupt_kernel ( ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        elif key == QtCore . Qt . Key_Period : \n            self . request_restart_kernel ( ) \n            return_identifier_1 = True \n            return return_identifier_1 \n    elif not event . modifiers ( ) & QtCore . Qt . AltModifier : \n        if key == QtCore . Qt . Key_Backspace : \n            col = self . _get_input_buffer_cursor_column ( ) \n            cursor = self . _control . textCursor ( ) \n            if col > 3 and not cursor . hasSelection ( ) : \n                text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n                if text . endswith ( '    ' ) and not text . strip ( ) : \n                    cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4 ) \n                    cursor . removeSelectedText ( ) \n                    return_identifier_2 = True \n                    return return_identifier_2 \n    return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) "}
{"14385": "\ndef _call_tip ( self ) : \n    if not self . enable_calltips : \n        return_identifier_0 = False \n        return return_identifier_0 \n    cursor = self . _get_cursor ( ) \n    cursor . movePosition ( QtGui . QTextCursor . Left ) \n    if cursor . document ( ) . characterAt ( cursor . position ( ) ) != '(' : \n        return_identifier_1 = False \n        return return_identifier_1 \n    context = self . _get_context ( cursor ) \n    if not context : \n        return_identifier_2 = False \n        return return_identifier_2 \n    name = '.' . join ( context ) \n    msg_id = self . kernel_manager . shell_channel . object_info ( name ) \n    pos = self . _get_cursor ( ) . position ( ) \n    self . _request_info [ 'call_tip' ] = self . _CallTipRequest ( msg_id , pos ) \n    return_identifier_3 = True \n    return return_identifier_3 "}
{"14400": "\ndef check_if_exists ( self ) : \n    if self . req is None : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        self . satisfied_by = pkg_resources . get_distribution ( self . req ) \n    except pkg_resources . DistributionNotFound : \n        return_identifier_1 = False \n        return return_identifier_1 \n    except pkg_resources . VersionConflict : \n        existing_dist = pkg_resources . get_distribution ( self . req . project_name ) \n        if self . use_user_site : \n            if dist_in_usersite ( existing_dist ) : \n                self . conflicts_with = existing_dist \n            elif running_under_virtualenv ( ) and dist_in_site_packages ( existing_dist ) : \n                raise InstallationError ( \"Will not install to the user site because it will lack sys.path precedence to %s in %s\" % ( existing_dist . project_name , existing_dist . location ) ) \n        else : \n            self . conflicts_with = existing_dist \n    return_identifier_2 = True \n    return return_identifier_2 "}
{"14402": "\ndef cpu_percent ( interval = 0.1 , percpu = False ) : \n    global _last_cpu_times \n    global _last_per_cpu_times \n    blocking = interval is not None and interval > 0.0 \n    def calculate ( t1 , t2 ) : \n        t1_all = sum ( t1 ) \n        t1_busy = t1_all - t1 . idle \n        t2_all = sum ( t2 ) \n        t2_busy = t2_all - t2 . idle \n        if t2_busy <= t1_busy : \n            return_identifier_0 = 0.0 \n            return return_identifier_0 \n        busy_delta = t2_busy - t1_busy \n        all_delta = t2_all - t1_all \n        busy_perc = ( busy_delta / all_delta ) * 100 \n        return round ( busy_perc , 1 ) \n    if not percpu : \n        if blocking : \n            t1 = cpu_times ( ) \n            time . sleep ( interval ) \n        else : \n            t1 = _last_cpu_times \n        _last_cpu_times = cpu_times ( ) \n        return calculate ( t1 , _last_cpu_times ) \n    else : \n        ret = [ ] \n        if blocking : \n            tot1 = cpu_times ( percpu = True ) \n            time . sleep ( interval ) \n        else : \n            tot1 = _last_per_cpu_times \n        _last_per_cpu_times = cpu_times ( percpu = True ) \n        for t1 , t2 in zip ( tot1 , _last_per_cpu_times ) : \n            ret . append ( calculate ( t1 , t2 ) ) \n        return ret "}
{"14407": "\ndef get_cpu_percent ( self , interval = 0.1 ) : \n    blocking = interval is not None and interval > 0.0 \n    if blocking : \n        st1 = sum ( cpu_times ( ) ) \n        pt1 = self . _platform_impl . get_cpu_times ( ) \n        time . sleep ( interval ) \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n    else : \n        st1 = self . _last_sys_cpu_times \n        pt1 = self . _last_proc_cpu_times \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n        if st1 is None or pt1 is None : \n            self . _last_sys_cpu_times = st2 \n            self . _last_proc_cpu_times = pt2 \n            return_identifier_0 = 0.0 \n            return return_identifier_0 \n    delta_proc = ( pt2 . user - pt1 . user ) + ( pt2 . system - pt1 . system ) \n    delta_time = st2 - st1 \n    self . _last_sys_cpu_times = st2 \n    self . _last_proc_cpu_times = pt2 \n    try : \n        overall_percent = ( delta_proc / delta_time ) * 100 \n    except ZeroDivisionError : \n        return_identifier_1 = 0.0 \n        return return_identifier_1 \n    single_cpu_percent = overall_percent * NUM_CPUS \n    if os . name != 'posix' : \n        if single_cpu_percent > 100.0 : \n            return_identifier_2 = 100.0 \n            return return_identifier_2 \n    return round ( single_cpu_percent , 1 ) "}
{"14408": "\ndef get_memory_percent ( self ) : \n    rss = self . _platform_impl . get_memory_info ( ) [ 0 ] \n    try : \n        return ( rss / float ( TOTAL_PHYMEM ) ) * 100 \n    except ZeroDivisionError : \n        return_identifier_0 = 0.0 \n        return return_identifier_0 "}
{"14410": "\ndef is_running ( self ) : \n    if self . _gone : \n        return_identifier_0 = False \n        return return_identifier_0 \n    try : \n        return self . create_time == self . _platform_impl . get_process_create_time ( ) \n    except NoSuchProcess : \n        self . _gone = True \n        return_identifier_1 = False \n        return return_identifier_1 "}
{"14415": "\ndef _wire_kernel ( self ) : \n    self . gtk_main , self . gtk_main_quit = self . _hijack_gtk ( ) \n    gobject . timeout_add ( int ( 1000 * self . kernel . _poll_interval ) , self . iterate_kernel ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"14430": "\ndef prefilter_line ( self , line , continue_prompt = False ) : \n    self . shell . _last_input_line = line \n    if not line : \n        return_identifier_0 = '' \n        return return_identifier_0 \n    if not continue_prompt or ( continue_prompt and self . multi_line_specials ) : \n        line = self . transform_line ( line , continue_prompt ) \n    line_info = LineInfo ( line , continue_prompt ) \n    stripped = line . strip ( ) \n    normal_handler = self . get_handler_by_name ( 'normal' ) \n    if not stripped : \n        if not continue_prompt : \n            self . shell . displayhook . prompt_count -= 1 \n        return normal_handler . handle ( line_info ) \n    if continue_prompt and not self . multi_line_specials : \n        return normal_handler . handle ( line_info ) \n    prefiltered = self . prefilter_line_info ( line_info ) \n    return prefiltered "}
{"14441": "\ndef handle ( self , line_info ) : \n    normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n    line = line_info . line \n    try : \n        codeop . compile_command ( line ) \n    except SyntaxError : \n        if line [ 0 ] == ESC_HELP : \n            line = line [ 1 : ] \n        elif line [ - 1 ] == ESC_HELP : \n            line = line [ : - 1 ] \n        if line : \n            self . shell . magic ( 'pinfo %s' % line_info . ifun ) \n        else : \n            self . shell . show_usage ( ) \n        return_identifier_0 = '' \n        return return_identifier_0 \n    except : \n        raise \n        return normal_handler . handle ( line_info ) \n    else : \n        return normal_handler . handle ( line_info ) "}
{"14442": "\ndef eventFilter ( self , obj , event ) : \n    if obj == self . _text_edit : \n        etype = event . type ( ) \n        if etype == QtCore . QEvent . KeyPress : \n            key = event . key ( ) \n            if key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return ) : \n                self . hide ( ) \n            elif key == QtCore . Qt . Key_Escape : \n                self . hide ( ) \n                return_identifier_0 = True \n                return return_identifier_0 \n        elif etype == QtCore . QEvent . FocusOut : \n            self . hide ( ) \n        elif etype == QtCore . QEvent . Enter : \n            self . _hide_timer . stop ( ) \n        elif etype == QtCore . QEvent . Leave : \n            self . _leave_event_hide ( ) \n    return super ( CallTipWidget , self ) . eventFilter ( obj , event ) "}
{"14446": "\ndef show_tip ( self , tip ) : \n    text_edit = self . _text_edit \n    document = text_edit . document ( ) \n    cursor = text_edit . textCursor ( ) \n    search_pos = cursor . position ( ) - 1 \n    self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = False ) \n    if self . _start_position == - 1 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    self . setText ( tip ) \n    self . resize ( self . sizeHint ( ) ) \n    padding = 3 \n    cursor_rect = text_edit . cursorRect ( cursor ) \n    screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) \n    point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) \n    point . setY ( point . y ( ) + padding ) \n    tip_height = self . size ( ) . height ( ) \n    tip_width = self . size ( ) . width ( ) \n    vertical = 'bottom' \n    horizontal = 'Right' \n    if point . y ( ) + tip_height > screen_rect . height ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . y ( ) - tip_height < padding : \n            if 2 * point . y ( ) < screen_rect . height ( ) : \n                vertical = 'bottom' \n            else : \n                vertical = 'top' \n        else : \n            vertical = 'top' \n    if point . x ( ) + tip_width > screen_rect . width ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . x ( ) - tip_width < padding : \n            if 2 * point . x ( ) < screen_rect . width ( ) : \n                horizontal = 'Right' \n            else : \n                horizontal = 'Left' \n        else : \n            horizontal = 'Left' \n    pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) \n    point = text_edit . mapToGlobal ( pos ( ) ) \n    if vertical == 'top' : \n        point . setY ( point . y ( ) - tip_height - padding ) \n    if horizontal == 'Left' : \n        point . setX ( point . x ( ) - tip_width - padding ) \n    self . move ( point ) \n    self . show ( ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"14452": "\ndef virtualenv_no_global ( ) : \n    site_mod_dir = os . path . dirname ( os . path . abspath ( site . __file__ ) ) \n    no_global_file = os . path . join ( site_mod_dir , 'no-global-site-packages.txt' ) \n    if running_under_virtualenv ( ) and os . path . isfile ( no_global_file ) : \n        return_identifier_0 = True \n        return return_identifier_0 "}
{"14470": "\ndef already_used ( self , tok ) : \n    if tok in self . jwts : \n        return_identifier_0 = True \n        return return_identifier_0 \n    self . jwts [ tok ] = time . time ( ) \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14474": "\ndef eventFilter ( self , obj , event ) : \n    etype = event . type ( ) \n    if etype == QtCore . QEvent . KeyPress : \n        key = event . key ( ) \n        if self . _control_key_down ( event . modifiers ( ) ) and key in self . _ctrl_down_remap : \n            new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , self . _ctrl_down_remap [ key ] , QtCore . Qt . NoModifier ) \n            QtGui . qApp . sendEvent ( obj , new_event ) \n            return_identifier_0 = True \n            return return_identifier_0 \n        elif obj == self . _control : \n            return self . _event_filter_console_keypress ( event ) \n        elif obj == self . _page_control : \n            return self . _event_filter_page_keypress ( event ) \n    elif etype == QtCore . QEvent . MouseButtonRelease and event . button ( ) == QtCore . Qt . MidButton and obj == self . _control . viewport ( ) : \n        cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n        self . _control . setTextCursor ( cursor ) \n        self . paste ( QtGui . QClipboard . Selection ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    elif etype == QtCore . QEvent . Resize and not self . _filter_resize : \n        self . _filter_resize = True \n        QtGui . qApp . sendEvent ( obj , event ) \n        self . _adjust_scrollbars ( ) \n        self . _filter_resize = False \n        return_identifier_2 = True \n        return return_identifier_2 \n    elif etype == QtCore . QEvent . ShortcutOverride and self . override_shortcuts and self . _control_key_down ( event . modifiers ( ) ) and event . key ( ) in self . _shortcuts : \n        event . accept ( ) \n    elif etype == QtCore . QEvent . DragEnter and obj == self . _control . viewport ( ) and event . source ( ) == self . _control . viewport ( ) : \n        self . _filter_drag = True \n    elif etype == QtCore . QEvent . DragLeave and obj == self . _control . viewport ( ) and self . _filter_drag : \n        cursor = self . _control . textCursor ( ) \n        cursor . clearSelection ( ) \n        self . _control . setTextCursor ( cursor ) \n        self . _filter_drag = False \n    elif etype == QtCore . QEvent . Drop and obj == self . _control . viewport ( ) : \n        cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n        if self . _in_buffer ( cursor . position ( ) ) : \n            text = event . mimeData ( ) . text ( ) \n            self . _insert_plain_text_into_buffer ( cursor , text ) \n        QtGui . qApp . sendEvent ( obj , QtGui . QDragLeaveEvent ( ) ) \n        return_identifier_3 = True \n        return return_identifier_3 \n    elif etype in self . _pager_scroll_events and obj == self . _page_control : \n        self . _page_control . repaint ( ) \n        return_identifier_4 = True \n        return return_identifier_4 \n    return super ( ConsoleWidget , self ) . eventFilter ( obj , event ) "}
{"14477": "\ndef can_paste ( self ) : \n    if self . _control . textInteractionFlags ( ) & QtCore . Qt . TextEditable : \n        return bool ( QtGui . QApplication . clipboard ( ) . text ( ) ) \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"14498": "\ndef _event_filter_page_keypress ( self , event ) : \n    key = event . key ( ) \n    ctrl_down = self . _control_key_down ( event . modifiers ( ) ) \n    alt_down = event . modifiers ( ) & QtCore . Qt . AltModifier \n    if ctrl_down : \n        if key == QtCore . Qt . Key_O : \n            self . _control . setFocus ( ) \n            intercept = True \n    elif alt_down : \n        if key == QtCore . Qt . Key_Greater : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . End ) \n            intercepted = True \n        elif key == QtCore . Qt . Key_Less : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            intercepted = True \n    elif key in ( QtCore . Qt . Key_Q , QtCore . Qt . Key_Escape ) : \n        if self . _splitter : \n            self . _page_control . hide ( ) \n            self . _control . setFocus ( ) \n        else : \n            self . layout ( ) . setCurrentWidget ( self . _control ) \n        return_identifier_0 = True \n        return return_identifier_0 \n    elif key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return , QtCore . Qt . Key_Tab ) : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageDown , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return_identifier_1 = True \n        return return_identifier_1 \n    elif key == QtCore . Qt . Key_Backspace : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageUp , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return_identifier_2 = True \n        return return_identifier_2 \n    return_identifier_3 = False \n    return return_identifier_3 "}
{"14519": "\ndef copy_config_file ( self , config_file , path = None , overwrite = False ) : \n    dst = os . path . join ( self . location , config_file ) \n    if os . path . isfile ( dst ) and not overwrite : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if path is None : \n        path = os . path . join ( get_ipython_package_dir ( ) , u'config' , u'profile' , u'default' ) \n    src = os . path . join ( path , config_file ) \n    shutil . copy ( src , dst ) \n    return_identifier_1 = True \n    return return_identifier_1 "}
{"14524": "\ndef raw_input_multi ( header = '' , ps1 = '==> ' , ps2 = '..> ' , terminate_str = '.' ) : \n    try : \n        if header : \n            header += '\\n' \n        lines = [ raw_input ( header + ps1 ) ] \n    except EOFError : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    terminate = [ terminate_str ] \n    try : \n        while lines [ - 1 : ] != terminate : \n            new_line = raw_input ( ps1 ) \n            while new_line . endswith ( '\\\\' ) : \n                new_line = new_line [ : - 1 ] + raw_input ( ps2 ) \n            lines . append ( new_line ) \n        return lines [ : - 1 ] \n    except EOFError : \n        print ( ) \n        return lines "}
{"14535": "\ndef timeUnit ( elapsed , avg , est_end ) : \n    minute = 60 \n    hr = 3600 \n    day = 86400 \n    if elapsed <= 3 * minute : \n        unit_elapsed = ( elapsed , \"secs\" ) \n    if elapsed > 3 * minute : \n        unit_elapsed = ( ( elapsed / 60 ) , \"mins\" ) \n    if elapsed > 3 * hr : \n        unit_elapsed = ( ( elapsed / 3600 ) , \"hr\" ) \n    if avg <= 3 * minute : \n        unit_avg = ( avg , \"secs\" ) \n    if avg > 3 * minute : \n        unit_avg = ( ( avg / 60 ) , \"mins\" ) \n    if avg > 3 * hr : \n        unit_avg = ( ( avg / 3600 ) , \"hr\" ) \n    if est_end <= 3 * minute : \n        unit_estEnd = ( est_end , \"secs\" ) \n    if est_end > 3 * minute : \n        unit_estEnd = ( ( est_end / 60 ) , \"mins\" ) \n    if est_end > 3 * hr : \n        unit_estEnd = ( ( est_end / 3600 ) , \"hr\" ) \n    return_identifier_0 = [ unit_elapsed , unit_avg , unit_estEnd ] \n    return return_identifier_0 "}
{"14542": "\ndef is_archive_file ( name ) : \n    archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) \n    ext = splitext ( name ) [ 1 ] . lower ( ) \n    if ext in archives : \n        return_identifier_0 = True \n        return return_identifier_0 \n    return_identifier_1 = False \n    return return_identifier_1 "}
{"14557": "\ndef target_outdated ( target , deps ) : \n    try : \n        target_time = os . path . getmtime ( target ) \n    except os . error : \n        return_identifier_0 = 1 \n        return return_identifier_0 \n    for dep in deps : \n        dep_time = os . path . getmtime ( dep ) \n        if dep_time > target_time : \n            return_identifier_1 = 1 \n            return return_identifier_1 \n    return_identifier_2 = 0 \n    return return_identifier_2 "}
{"14598": "\ndef join_regex ( regexes ) : \n    if len ( regexes ) > 1 : \n        return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) \n    elif regexes : \n        return regexes [ 0 ] \n    else : \n        return_identifier_0 = \"\" \n        return return_identifier_0 "}
{"14664": "\ndef validateAttrib ( self , method , cls = None ) : \n    any = False \n    for group in self . attribs : \n        match = True \n        for key , value in group : \n            attr = get_method_attr ( method , cls , key ) \n            if callable ( value ) : \n                if not value ( key , method , cls ) : \n                    match = False \n                    break \n            elif value is True : \n                if not bool ( attr ) : \n                    match = False \n                    break \n            elif value is False : \n                if bool ( attr ) : \n                    match = False \n                    break \n            elif type ( attr ) in ( list , tuple ) : \n                if not str ( value ) . lower ( ) in [ str ( x ) . lower ( ) for x in attr ] : \n                    match = False \n                    break \n            else : \n                if ( value != attr and str ( value ) . lower ( ) != str ( attr ) . lower ( ) ) : \n                    match = False \n                    break \n        any = any or match \n    if any : \n        return None \n    return_identifier_0 = False \n    return return_identifier_0 "}
{"14665": "\ndef wantMethod ( self , method ) : \n    try : \n        cls = method . im_class \n    except AttributeError : \n        return_identifier_0 = False \n        return return_identifier_0 \n    return self . validateAttrib ( method , cls ) "}
{"14687": "\ndef tbsource ( tb , context = 6 ) : \n    lineno = tb . tb_lineno \n    frame = tb . tb_frame \n    if context > 0 : \n        start = lineno - 1 - context // 2 \n        log . debug ( \"lineno: %s start: %s\" , lineno , start ) \n        try : \n            lines , dummy = inspect . findsource ( frame ) \n        except IOError : \n            lines , index = [ '' ] , 0 \n        else : \n            all_lines = lines \n            start = max ( start , 1 ) \n            start = max ( 0 , min ( start , len ( lines ) - context ) ) \n            lines = lines [ start : start + context ] \n            index = lineno - 1 - start \n            if sys . version_info >= ( 2 , 5 ) and index > 0 : \n                while lines [ index - 1 ] . strip ( ) . endswith ( '\\\\' ) : \n                    start -= 1 \n                    lines = all_lines [ start : start + context ] \n    else : \n        lines , index = [ '' ] , 0 \n    log . debug ( \"tbsource lines '''%s''' around index %s\" , lines , index ) \n    return_identifier_0 = ( lines , index ) \n    return return_identifier_0 "}
{"14701": "\ndef module_list ( path ) : \n    if path == '' : \n        path = '.' \n    if os . path . isdir ( path ) : \n        folder_list = os . listdir ( path ) \n    elif path . endswith ( '.egg' ) : \n        try : \n            folder_list = [ f for f in zipimporter ( path ) . _files ] \n        except : \n            folder_list = [ ] \n    else : \n        folder_list = [ ] \n    if not folder_list : \n        return_identifier_0 = [ ] \n        return return_identifier_0 \n    isfile = os . path . isfile \n    pjoin = os . path . join \n    basename = os . path . basename \n    def is_importable_file ( path ) : \n        name , extension = os . path . splitext ( path ) \n        return import_re . match ( path ) and py3compat . isidentifier ( name ) \n    folder_list = [ p for p in folder_list if isfile ( pjoin ( path , p , '__init__.py' ) ) or is_importable_file ( p ) ] \n    return [ basename ( p ) . split ( '.' ) [ 0 ] for p in folder_list ] "}
{"14702": "\ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = False \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules += module_list ( path ) \n        if time ( ) - t >= TIMEOUT_STORAGE and not store : \n            store = True \n            print ( \"\\nCaching the list of root modules, please wait!\" ) \n            print ( \"(This will only be done once - type '%rehashx' to \" \"reset cache!)\\n\" ) \n            sys . stdout . flush ( ) \n        if time ( ) - t > TIMEOUT_GIVEUP : \n            print ( \"This is taking too long, we give up.\\n\" ) \n            ip . db [ 'rootmodules' ] = [ ] \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14704": "\ndef module_completion ( line ) : \n    words = line . split ( ' ' ) \n    nwords = len ( words ) \n    if nwords == 3 and words [ 0 ] == 'from' : \n        return_identifier_0 = [ 'import ' ] \n        return return_identifier_0 \n    if nwords < 3 and ( words [ 0 ] in [ 'import' , 'from' ] ) : \n        if nwords == 1 : \n            return get_root_modules ( ) \n        mod = words [ 1 ] . split ( '.' ) \n        if len ( mod ) < 2 : \n            return get_root_modules ( ) \n        completion_list = try_import ( '.' . join ( mod [ : - 1 ] ) , True ) \n        return [ '.' . join ( mod [ : - 1 ] + [ el ] ) for el in completion_list ] \n    if nwords >= 3 and words [ 0 ] == 'from' : \n        mod = words [ 1 ] \n        return try_import ( mod ) "}
{"14706": "\ndef cd_completer ( self , event ) : \n    ip = get_ipython ( ) \n    relpath = event . symbol \n    if event . line . endswith ( '-b' ) or ' -b ' in event . line : \n        bkms = self . db . get ( 'bookmarks' , None ) \n        if bkms : \n            return bkms . keys ( ) \n        else : \n            return_identifier_0 = [ ] \n            return return_identifier_0 \n    if event . symbol == '-' : \n        width_dh = str ( len ( str ( len ( ip . user_ns [ '_dh' ] ) + 1 ) ) ) \n        fmt = '-%0' + width_dh + 'd [%s]' \n        ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user_ns [ '_dh' ] ) ] \n        if len ( ents ) > 1 : \n            return ents \n        return_identifier_1 = [ ] \n        return return_identifier_1 \n    if event . symbol . startswith ( '--' ) : \n        return [ \"--\" + os . path . basename ( d ) for d in ip . user_ns [ '_dh' ] ] \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    relpath = relpath . replace ( '\\\\' , '/' ) \n    found = [ ] \n    for d in [ f . replace ( '\\\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : \n        if ' ' in d : \n            raise TryNext \n        found . append ( d ) \n    if not found : \n        if os . path . isdir ( relpath ) : \n            return_identifier_2 = [ compress_user ( relpath , tilde_expand , tilde_val ) ] \n            return return_identifier_2 \n        bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) \n        bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] \n        if bkmatches : \n            return bkmatches \n        raise TryNext \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in found ] "}
{"14721": "\ndef maybe_run ( self , job ) : \n    msg_id = job . msg_id \n    self . log . debug ( \"Attempting to assign task %s\" , msg_id ) \n    if not self . targets : \n        return_identifier_0 = False \n        return return_identifier_0 \n    if job . follow or job . targets or job . blacklist or self . hwm : \n        def can_run ( idx ) : \n            if self . hwm and self . loads [ idx ] == self . hwm : \n                return_identifier_1 = False \n                return return_identifier_1 \n            target = self . targets [ idx ] \n            if target in job . blacklist : \n                return_identifier_2 = False \n                return return_identifier_2 \n            if job . targets and target not in job . targets : \n                return_identifier_3 = False \n                return return_identifier_3 \n            return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) \n        indices = filter ( can_run , range ( len ( self . targets ) ) ) \n        if not indices : \n            if job . follow . all : \n                dests = set ( ) \n                relevant = set ( ) \n                if job . follow . success : \n                    relevant = self . all_completed \n                if job . follow . failure : \n                    relevant = relevant . union ( self . all_failed ) \n                for m in job . follow . intersection ( relevant ) : \n                    dests . add ( self . destinations [ m ] ) \n                if len ( dests ) > 1 : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return_identifier_4 = False \n                    return return_identifier_4 \n            if job . targets : \n                job . targets . difference_update ( job . blacklist ) \n                if not job . targets or not job . targets . intersection ( self . targets ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return_identifier_5 = False \n                    return return_identifier_5 \n            return_identifier_6 = False \n            return return_identifier_6 \n    else : \n        indices = None \n    self . submit_task ( job , indices ) \n    return_identifier_7 = True \n    return return_identifier_7 "}
{"14742": "\ndef notebook_exists ( self , notebook_id ) : \n    if notebook_id not in self . mapping : \n        return_identifier_0 = False \n        return return_identifier_0 \n    path = self . get_path_by_name ( self . mapping [ notebook_id ] ) \n    return os . path . isfile ( path ) "}
{"14756": "\ndef _classes_default ( self ) : \n    return_identifier_0 = [ InteractiveShellApp , self . __class__ , TerminalInteractiveShell , PromptManager , HistoryManager , ProfileDir , PlainTextFormatter , IPCompleter , ScriptMagics , ] \n    return return_identifier_0 "}
{"14762": "\ndef parse_notifier_name ( name ) : \n    if isinstance ( name , str ) : \n        return_identifier_0 = [ name ] \n        return return_identifier_0 \n    elif name is None : \n        return_identifier_1 = [ 'anytrait' ] \n        return return_identifier_1 \n    elif isinstance ( name , ( list , tuple ) ) : \n        for n in name : \n            assert isinstance ( n , str ) , \"names must be strings\" \n        return name "}
{"14769": "\ndef check ( self , completed , failed = None ) : \n    if len ( self ) == 0 : \n        return_identifier_0 = True \n        return return_identifier_0 \n    against = set ( ) \n    if self . success : \n        against = completed \n    if failed is not None and self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return self . issubset ( against ) \n    else : \n        return not self . isdisjoint ( against ) "}
{"14770": "\ndef unreachable ( self , completed , failed = None ) : \n    if len ( self ) == 0 : \n        return_identifier_0 = False \n        return return_identifier_0 \n    against = set ( ) \n    if not self . success : \n        against = completed \n    if failed is not None and not self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return not self . isdisjoint ( against ) \n    else : \n        return self . issubset ( against ) "}
{"14799": "\ndef _is_from_this_session ( self , msg ) : \n    session = self . _kernel_manager . session . session \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        return_identifier_0 = True \n        return return_identifier_0 \n    else : \n        return parent . get ( 'session' ) == session "}
{"14804": "\ndef extract_header ( msg_or_header ) : \n    if not msg_or_header : \n        return_identifier_0 = { } \n        return return_identifier_0 \n    try : \n        h = msg_or_header [ 'header' ] \n    except KeyError : \n        try : \n            h = msg_or_header [ 'msg_id' ] \n        except KeyError : \n            raise \n        else : \n            h = msg_or_header \n    if not isinstance ( h , dict ) : \n        h = dict ( h ) \n    return h "}
{"14807": "\ndef sign ( self , msg_list ) : \n    if self . auth is None : \n        return_identifier_0 = b'' \n        return return_identifier_0 \n    h = self . auth . copy ( ) \n    for m in msg_list : \n        h . update ( m ) \n    return str_to_bytes ( h . hexdigest ( ) ) "}
{"14843": "\ndef format2 ( self , raw , out = None , scheme = '' ) : \n    string_output = 0 \n    if out == 'str' or self . out == 'str' or isinstance ( self . out , StringIO . StringIO ) : \n        out_old = self . out \n        self . out = StringIO . StringIO ( ) \n        string_output = 1 \n    elif out is not None : \n        self . out = out \n    if scheme == 'NoColor' : \n        error = False \n        self . out . write ( raw ) \n        if string_output : \n            return raw , error \n        else : \n            return None , error \n    colors = self . color_table [ scheme ] . colors \n    self . colors = colors \n    self . raw = raw . expandtabs ( ) . rstrip ( ) \n    self . lines = [ 0 , 0 ] \n    pos = 0 \n    raw_find = self . raw . find \n    lines_append = self . lines . append \n    while 1 : \n        pos = raw_find ( '\\n' , pos ) + 1 \n        if not pos : \n            break \n        lines_append ( pos ) \n    lines_append ( len ( self . raw ) ) \n    self . pos = 0 \n    text = StringIO . StringIO ( self . raw ) \n    error = False \n    try : \n        for atoken in generate_tokens ( text . readline ) : \n            self ( * atoken ) \n    except tokenize . TokenError as ex : \n        msg = ex . args [ 0 ] \n        line = ex . args [ 1 ] [ 0 ] \n        self . out . write ( \"%s\\n\\n*** ERROR: %s%s%s\\n\" % ( colors [ token . ERRORTOKEN ] , msg , self . raw [ self . lines [ line ] : ] , colors . normal ) ) \n        error = True \n    self . out . write ( colors . normal + '\\n' ) \n    if string_output : \n        output = self . out . getvalue ( ) \n        self . out = out_old \n        return_identifier_0 = ( output , error ) \n        return return_identifier_0 \n    return_identifier_1 = ( None , error ) \n    return return_identifier_1 "}
{"14879": "\ndef analysis2 ( self , morf ) : \n    analysis = self . _analyze ( morf ) \n    return_identifier_0 = ( analysis . filename , sorted ( analysis . statements ) , sorted ( analysis . excluded ) , sorted ( analysis . missing ) , analysis . missing_formatted ( ) , ) \n    return return_identifier_0 "}
{"14907": "\ndef is_event_loop_running_qt4 ( app = None ) : \n    if app is None : \n        app = get_app_qt4 ( [ '' ] ) \n    if hasattr ( app , '_in_event_loop' ) : \n        return app . _in_event_loop \n    else : \n        return_identifier_0 = False \n        return return_identifier_0 "}
